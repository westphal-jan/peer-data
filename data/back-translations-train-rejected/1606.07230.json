{"id": "1606.07230", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Deep Learning Markov Random Field for Semantic Segmentation", "abstract": "Semantic segmentation tasks can be well modeled by Markov Random Field (MRF). This paper addresses semantic segmentation by incorporating high-order relations and mixture of label contexts into MRF. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN to model unary terms and additional layers are devised to approximate the mean field (MF) algorithm for pairwise terms. It has several appealing properties. First, different from the recent works that required many iterations of MF during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing models as its special cases. Furthermore, pairwise terms in DPN provide a unified framework to encode rich contextual information in high-dimensional data, such as images and videos. Third, DPN makes MF easier to be parallelized and speeded up, thus enabling efficient inference. DPN is thoroughly evaluated on standard semantic image/video segmentation benchmarks, where a single DPN model yields state-of-the-art segmentation accuracies on PASCAL VOC 2012, Cityscapes dataset and CamVid dataset.", "histories": [["v1", "Thu, 23 Jun 2016 08:52:39 GMT  (3408kb,D)", "http://arxiv.org/abs/1606.07230v1", "Extended version of our previous ICCV 2015 paper (arXiv:1509.02634)"], ["v2", "Tue, 8 Aug 2017 09:24:18 GMT  (3277kb,D)", "http://arxiv.org/abs/1606.07230v2", "To appear in IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2017. Extended version of our previous ICCV 2015 paper (arXiv:1509.02634)"]], "COMMENTS": "Extended version of our previous ICCV 2015 paper (arXiv:1509.02634)", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["ziwei liu", "xiaoxiao li", "ping luo", "chen change loy", "xiaoou tang"], "accepted": false, "id": "1606.07230"}, "pdf": {"name": "1606.07230.pdf", "metadata": {"source": "CRF", "title": "Deep Learning Markov Random Field for Semantic Segmentation", "authors": ["Ziwei Liu", "Xiaoxiao Li", "Ping Luo", "Chen Change Loy", "Xiaoou Tang"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Semantic Image / Video Segmentation, Markov Random Field, Convolutional Neural Network.F"}, {"heading": "1 INTRODUCTION", "text": "It is defined as a multi-label classification problem that aims to assign a label to each pixel. There are two widely used areas of research, namely semantic image segmentation [1] and semantic video segmentation [2], with static image and video sequences as input. The pro-pixel segmentation results obtained are extremely useful for multiple applications such as smart editing [3], scene understanding [4] and automated driving [5]. Since pixels in natural images or videos generally have a strong correlation, joint modeling of label distribution in all locations is desirable. To capture this contextual information, random fields (MRF) and conditional random fields (CRF) [6] are often used as classic frameworks for semantic segmentation. They model the common distribution of labels by defining both inconsistent terms and misleading terms."}, {"heading": "2 RELATED WORK", "text": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation either focus on constructing specific graph structures so that contextual information and long-term dependencies can be captured, or on designing suitable network architectures to harness the power of deep learning. Below, we summarize recent research advances in these two aspects. Markov Random Field (MRF) or conditional MRR Field (CRF) have achieved great successes in semantic image segmentation, which is one of the most difficult problems in the field of computer vision. Researchers have improved labeling accuracy by defining the pair-based functions, including extensive dependencies [7], [29], high-order potentials [8], and semantic label contexts."}, {"heading": "3 OUR APPROACH", "text": "Each node is3 (b) b (a) b (a) b (a) b (a) b (a) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c (c) c) c (c) c (c) c (c) c (c) c (c) c) c (c) c) c (c) c (c) c (c) c) c (c) c) c (c) c (c) c \"c\" c \"c\" c (c) c \"c\" c \"c\" c (c) c \"c\" c \"c (c) c (c) c c c c c c c c c c c c c c c c c c c (c) c c c c c c c c c c c c (c) c) c (c) c (c) c) c (c) c\" c \"c\" c (c) c \"c\" c (c) c \"c\" c (c) c \"c\" c (c) c \"c (c) c (c) c) c c c c c c c c c c c c c c c c c (c) c c c c c c c c c c c (c) c c c c c c c c c c c c c c c c c c (c) c) c c c c c c (c) c) c c c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c (c) c) c) c (c) c) c) c (c) c) c (c) c) c) c (c) c) c (c) c) c) c) c (c) c) c (c) c) c (c) c (c) (c) c) (c) c) (c) c) c (c) (c) c) (c) c) (c) (c) c) (c) c) (c) c) (c) (c) (c) (c) c) c) (c) (c) (c) (c) c) (c) ("}, {"heading": "4 DEEP PARSING NETWORK", "text": "This section describes the implementation of Eqn. (13) in a Deep Parsing Network (DPN). Hyper parameters of VGG16 and DPN are compared in Table 1. As noted in Table 1, the first line represents the name of the layer and \"x-y\" in the second line represents the size of the receptive field or step of the folding. \"3-1\" in the folding layer, for example, implies that the receptive field of each filter is 3 x-3 and is applied to each pixel of an input feature card, while \"2-2\" in the max pooling layer indicates that each feature card is structured over all other pixels within a 2 x-2 local layer."}, {"heading": "4.1 Modeling Unary Terms", "text": "To simplify the discussion, we take PASCAL VOC 2012 (VOC12) as an example. DPN can easily be adapted to all other semantic screens by changing its hyperparameters. VOC12 contains that each image is reactivated in training. DPN must predict a set of 512 \u00d7 512 screen jumpers, i.e., one label for each pixel. For this purpose, VOC12 extends to 512 \u00d7 512 screen jumpers."}, {"heading": "4.2 Modeling Smoothness Terms", "text": "The last four layers of DPN, i.e. from b12 to b15, are carefully designed to smooth out the unary labeling results. \u2022 b12. As listed in Table 1 (b), lconv'in b12 each has a 3D activation layer. A counterpart of this (i.e. 2D locally convolutional layer) is commonly used in face recognition [43], [44], [45] to capture different information from different 6TABLE 1: A comparison between the network architectures of VG16 and DPN. (a) VG16: 224 \u00b7 224 \u00b7 3 Input Image; 1 \u00d7 1000 Output Labels1 2 3 4 6 8 11 layers fi.-st. # ch. The size of the network architectures of VG16 and DPN. \"(a) VGG16: 224 \u00b7 224 \u00b7 224 \u00b7 3 Input Image; 1 Input Image; 1 \u00d7 1000 Output Labels1 2 3 3 3 3 3 3 3 3 3 3 4 5 8 11 8."}, {"heading": "4.3 Learning Algorithms", "text": "It is indeed the case that we are able to go in search of a solution that will enable us to move to another world in which we are in a position in which we are in."}, {"heading": "5 EXPERIMENTS", "text": "This year, we have the opportunity to go in search of a solution that gives the impression that solving the problems is only a matter of time. (...) This year, we have the opportunity to find a solution. (...) We have the opportunity to find a solution. (...) We have the opportunity to find a solution. (...) We have the opportunity to find a solution. (...) We have the opportunity to find a solution. (...) We have the opportunity to find a solution. (...) We have the opportunity to find a solution. (...) We have the opportunity to find a solution. (...) We have not found it. (...) We have found it. (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). (...). We have found it. (...). (...). (...). We have found it. (...). (...). (...). (...). (). (...). We have found it. (...). (...). (...). (). (). (). (). We have found it. (...). (...). (...). (...). (). (...). (). ().). (...). ().). (...). We have found it."}, {"heading": "5.1 Effectiveness of DPN", "text": "All models evaluated in this section are based on the validation of VOC12.3 Penalty. The receptive field of B12 shows the range of three relationships for each pixel. We examine various settings of the receptive fields, including \"10,\" and \"100,\" as shown in the table. They are the bottom-up boxes of the predicted regions, which are better than \"100,\" which implies that 50 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \"000\" 000 \""}, {"heading": "5.2 Effectiveness of Spatial-temporal DPN", "text": "In this section we will quantitatively examine the effectiveness of the paired term in spatio-temporal DPN. CamVid train are used for training and performance on CamVid test. Images in CamVid are taken from a moving vehicle, which results in certain time regulations. In spatio-temporal DPN we use the receptive field of b12 as 50 \u00d7 50 and b13 as a 3D conventional layer. In Table 4 we evaluate the performance of DPN. In Table 3 (a) we use the receptive field of b12 as 50 \u00d7 50 and b13 as a visual layer."}, {"heading": "5.3 Further Analysis", "text": "This year, it has reached the point where it will be able to take the lead, \"he said in an interview with the German Press Agency.\" We have never hesitated so long, \"he said.\" But we are not yet as far as we imagined. \""}, {"heading": "5.4 Benchmarks", "text": "We evaluate the performance of DPN and spatio-temporal DPN using several semantic standard segmentation benchmarks."}, {"heading": "5.4.1 Pascal VOC12", "text": "The VOC12 dataset is one of the most popular benchmarks for semantic image segmentation. This dataset contains 20 categories of indoor and outdoor objects and a background category. As already mentioned, we use 10 582 images for training, 1 449 images for validation and 1 456 images for testing. The results are given in Table 5 (b), we compare DPN with the best performing methods 5 based on two settings, 5. The results of these methods were presented either in the published papers or in arXiv preprints, i.e. with and without pre-training on COCO. The approaches trained on COCO are marked with \"\u2020.\" We evaluate DPN on several scales of the images and then average the results according to [13], [17]. DPN surpasses all existing methods trained on VOC12, but DPN only requires an MF iteration to solve MRF."}, {"heading": "5.4.2 Cityscapes", "text": "The Cityscapes dataset focuses on the segmentation of street scenes. All images are taken by a moving vehicle in different seasons and cities. This dataset is very different from VOC12. Due to the large image depth, the street scenes differ greatly even in a single image. It defines 19 object categories for evaluation. There are 2975 training sessions, 500 validations and 1525 test images with fine pixel annotations and an additional 20,000 images with core annotations. To make a fair comparison, we use the images in our experiments only with fine pixel annotations. As shown in Table 6, DPN achieves 66.8% for Cityscapes datasets, which is the second best method, and is close to first place 67.1% [52]. [52] adopts a multi-scale aggregation strategy that is easy to integrate into the Cityscapes dataset to further enhance performance."}, {"heading": "5.4.3 CamVid", "text": "CamVid data set consists of 367 training images and 233 test images with 11 classes annotations. All images are extracted from three video sequences at 1Hz. Similar to Cityscapes data set, these video sequences are also recorded by a moving vehicle. As shown in Table 7, DPN exceeds all existing methods, and the paired spatial term improves performance to 60.25%. Considering the relatively sparse sampling rate (one picture per second) and the strong unified term, our 3-D MRF can indeed use temporal context information for common conclusions. We can observe that DPN performs much better than other methods, especially on narrow and small objects (i.e. \"posts\" and \"signs\"), which are very difficult in the segmentation task. Since our triple penalty captures the appearance of pixels, we can predict more precise time limits on these objects. The spatial-time relationship between sky-time categories also improves the following pair-specific PN."}, {"heading": "5.5 Visual Quality Comparisons", "text": "Figure 12 shows the comparisons of DPN with FCN [11] and DeepLab [13]. We use the publicly released model6 to regenerate FCN label cards, while the results of DeepLab are extracted from their published papers. DPN generally provides more accurate predictions at both the image and instance levels. For example, in Figure 12 (line 2), DPN can detect all available persons while simultaneously restoring sharp boundaries of the aircraft. Even in the challenging case of Figure 12 (line 4), the integrity of the bus is maintained by complex reflection. Further examples of DPN label cards are in Figure 14. We observe that learning local label contexts helps distinguish confusing objects and learn triple penalties that facilitate the capture of intrinsic object boundaries."}, {"heading": "6 CONCLUSION", "text": "We proposed Deep Parsing Network (DPN) to address semantic image / video segmentation. DPN has several appealing features. First, DPN combines inference and the learning of unary terms and pairwise terms into a single revolutionary network. No iterative inference is required during retransmission. Second, high-order relationships and mixtures of label contexts are incorporated into the pairwise modeling of terms, making existing work a special case. Third, DPN builds on conventional CNN operations, making it easy to parallelise and accelerate. DPN achieves state-of-the-art performance on VOC12, Cityscapes, and CamVid datasets. Several valuable facts about semantic segmentation are revealed through extensive experiments, such as the interaction between tagging, localization, and boundary accuracy along different processing steps. Future guidelines include investigating the objectivity of large number of PN generalizations and boundary classes."}], "references": [{"title": "Semantic image segmentation via deep parsing network", "author": ["Z. Liu", "X. Li", "P. Luo", "C.-C. Loy", "X. Tang"], "venue": "ICCV, 2015, pp. 1377\u20131385.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Track to the future: Spatiotemporal video segmentation with long-range motion cues", "author": ["J. Lezama", "K. Alahari", "J. Sivic", "I. Laptev"], "venue": "CVPR, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Scene collaging: Analysis and synthesis of natural images with semantic layers", "author": ["P. Isola", "C. Liu"], "venue": "ICCV. IEEE, 2013, pp. 3048\u20133055.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "PAMI, vol. 35, no. 8, pp. 1915\u20131929, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1915}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "CVPR, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "An introduction to variational methods for graphical models", "author": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning, vol. 37, no. 2, pp. 183\u2013233, 1999.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1999}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Filter-based mean-field inference for random fields with higher-order terms and product label-spaces", "author": ["V. Vineet", "J. Warrell", "P.H. Torr"], "venue": "ECCV, 2012, pp. 31\u201344.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Context driven scene parsing with attention to rare classes", "author": ["J. Yang", "B. Price", "S. Cohen", "M.-H. Yang"], "venue": "CVPR, 2014, pp. 3294\u20133301.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, 2015, pp. 3431\u20133440.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR, 2009, pp. 248\u2013255.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L.-C. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "ICLR, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully connected deep structured networks", "author": ["A.G. Schwing", "R. Urtasun"], "venue": "arXiv:1503.02351v1, 9 Mar 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "From naive mean field theory to the tap equations", "author": ["M. Opper", "O. Winther"], "venue": "2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": "arXiv:1502.03240v2, 30 Apr 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I. Reid", "A. Hengel"], "venue": "arXiv:1504.01013v2, 23 Apr 2015.  12 (a) input image (b) ground truth (c) FCN (d) DeepLab (e) DPN Fig. 12: Visual quality comparison of different semantic image segmentation methods: (a) input image (b) ground truth (c) FCN [11] (d) DeepLab [13] and (e) DPN.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["M. Jaderberg", "A. Vedaldi", "A. Zisserman"], "venue": "BMVC, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "cudnn: Efficient primitives for deep learning", "author": ["S. Chetlur", "C. Woolley", "P. Vandermersch", "J. Cohen", "J. Tran", "B. Catanzaro", "E. Shelhamer"], "venue": "NIPS Deep Learning Workshop, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, vol. 88, no. 2, pp. 303\u2013338, 2010.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "Semantic object classes in video: A high-definition ground truth database", "author": ["G.J. Brostow", "J. Fauqueur", "R. Cipolla"], "venue": "Pattern Recognition Letters, 2008.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "PAMI, vol. 22, no. 8, pp. 888\u2013905, 2000.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning a classification model for segmentation", "author": ["X. Ren", "J. Malik"], "venue": "ICCV, 2003, pp. 10\u201317.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Efficient belief propagation for early vision", "author": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "venue": "IJCV, vol. 70, no. 1, pp. 41\u201354, 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning crfs using graph cuts", "author": ["M. Szummer", "P. Kohli", "D. Hoiem"], "venue": "ECCV, 2008, pp. 582\u2013595.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Class segmentation and object localization with superpixel neighborhoods", "author": ["B. Fulkerson", "A. Vedaldi", "S. Soatto"], "venue": "ICCV, 2009, pp. 670\u2013 677.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "PAMI, vol. 33, no. 5, pp. 898\u2013916, 2011.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "Feedforward semantic segmentation with zoom-out features", "author": ["M. Mostajabi", "P. Yadollahpour", "G. Shakhnarovich"], "venue": "CVPR, 2015, pp. 3376\u20133385.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Parameter learning and convergent inference for dense random fields", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "ICML, 2013, pp. 513\u2013521.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Posefield: An efficient mean-field based method for joint estimation of human pose, segmentation, and depth", "author": ["V. Vineet", "G. Sheasby", "J. Warrell", "P.H. Torr"], "venue": "Energy Minimization Methods in Computer Vision and Pattern Recognition. Springer, 2013, pp. 180\u2013194.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Nonparametric scene parsing via label transfer", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "PAMI, vol. 33, no. 12, pp. 2368\u20132382, 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "CVPR, 2014, pp. 891\u2013898.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Segmentation, ordering and multi-object tracking using graphical models.", "author": ["C. Wang", "M. de La Gorce", "N. Paragios"], "venue": "in ICCV,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Multiclass semantic video segmentation with objectlevel active inference", "author": ["B. Liu", "X. He"], "venue": "CVPR, 2015, pp. 4286\u20134294.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2015}, {"title": "Hierarchical face parsing via deep learning", "author": ["P. Luo", "X. Wang", "X. Tang"], "venue": "CVPR, 2012, pp. 2480\u20132487.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}, {"title": "Pedestrian parsing via deep decompositional network", "author": ["\u2014\u2014"], "venue": "ICCV, 2013, pp. 2648\u20132655.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Weakly-and semi-supervised learning of a dcnn for semantic image segmentation", "author": ["G. Papandreou", "L.-C. Chen", "K. Murphy", "A.L. Yuille"], "venue": "arXiv:1502.02734v2, 8 May 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Segnet: A deep convolutional encoder-decoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.00561, 2015.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning low-level vision", "author": ["W.T. Freeman", "E.C. Pasztor", "O.T. Carmichael"], "venue": "IJCV, vol. 40, no. 1, pp. 25\u201347, 2000.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2000}, {"title": "Semantic video segmentation: Exploring inference efficiency", "author": ["S. Tripathi", "S. Belongie", "Y. Hwang", "T. Nguyen"], "venue": "arXiv preprint arXiv:1509.02441, 2015.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Sift flow: Dense correspondence across scenes and its applications", "author": ["C. Liu", "J. Yuen", "A. Torralba"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 33, no. 5, pp. 978\u2013994, 2011.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012, pp. 1097\u20131105.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning face representation by joint identification-verification", "author": ["Y. Sun", "X. Wang", "X. Tang"], "venue": "NIPS, 2014.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "CVPR, 2014, pp. 1701\u20131708.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning face attributes in the wild", "author": ["Z. Liu", "P. Luo", "X. Wang", "X. Tang"], "venue": "ICCV, 2015, pp. 3730\u20133738.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2015}, {"title": "Distilling the knowledge in a neural network", "author": ["G.E. Hinton", "O. Vinyals", "J. Dean"], "venue": "NIPS Deep Learning Workshop, 2014.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast high-dimensional filtering using the permutohedral lattice", "author": ["A. Adams", "J. Baek", "M.A. Davis"], "venue": "Computer Graphics Forum, vol. 29, no. 2, 2010, pp. 753\u2013762.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2010}, {"title": "Segmentation and 13 (a) input image  (b) ground truth  (c) DPN Fig. 13: Failure cases: (a) input image (b) ground truth (c) DPN. recognition using structure from motion point clouds", "author": ["G.J. Brostow", "J. Shotton", "J. Fauqueur", "R. Cipolla"], "venue": "ECCV, 2008, pp. 44\u201357.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2008}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "ICCV, 2011, pp. 991\u2013998.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2011}, {"title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "arXiv:1503.01640v2, 18 May 2015.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2015}, {"title": "Superparsing: scalable nonparametric image parsing with superpixels", "author": ["J. Tighe", "S. Lazebnik"], "venue": "Computer Vision\u2013ECCV 2010. Springer, 2010, pp. 352\u2013365.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122, 2015.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2015}, {"title": "Associative hierarchical crfs for object class image segmentation", "author": ["C. Russell", "P. Kohli", "P.H. Torr"], "venue": "Computer Vision, 2009 IEEE 12th International Conference on. IEEE, 2009, pp. 739\u2013746.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2009}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, 2014, pp. 740\u2013755.  14 (a) input image (b) ground truth (c) DPN (a) input image (b) ground truth (c) DPN (a) input image  (b) ground truth  (c) DPN (a) input image  (b) ground truth  (c) DPN  Pa  sc  al  V  O  C  12 C  ity  sc  ap  es C  am  Vi d Fig. 14: Visual quality of DPN label maps: (a) input image (b) ground truth (white labels indicating ambiguous regions) and (c) DPN.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "There are two widely adopted research realms, namely semantic image segmentation [1] and semantic video segmentation [2], with static image and video sequence as input respectively.", "startOffset": 81, "endOffset": 84}, {"referenceID": 1, "context": "There are two widely adopted research realms, namely semantic image segmentation [1] and semantic video segmentation [2], with static image and video sequence as input respectively.", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "The obtained per-pixel segmentation results are extremely useful for several applications like smart editing [3], scene understanding [4] and automated driving [5].", "startOffset": 109, "endOffset": 112}, {"referenceID": 3, "context": "The obtained per-pixel segmentation results are extremely useful for several applications like smart editing [3], scene understanding [4] and automated driving [5].", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "The obtained per-pixel segmentation results are extremely useful for several applications like smart editing [3], scene understanding [4] and automated driving [5].", "startOffset": 160, "endOffset": 163}, {"referenceID": 5, "context": "To capture these contextual information, Markov random field (MRF) and conditional random field (CRF) [6] are commonly used as classic frameworks for semantic segmentation.", "startOffset": 102, "endOffset": 105}, {"referenceID": 6, "context": "[7] attained accurate segmentation boundary by inferring on a fully-connected graph.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] extended [7] by defining both high-order and long-range terms between pixels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] extended [7] by defining both high-order and long-range terms between pixels.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "Global or local semantic contexts between labels were also investigated by [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 9, "context": "As deep learning gradually takes over in many image recognition fields [10], researchers have also explored the possibility of designing effective deep architecture for semantic segmentation.", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "[11] transformed fully-connected layers", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "of CNN into convolutional layers, making accurate per-pixel classification possible using contemporary CNN architectures that were pre-trained on ImageNet [12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 12, "context": "[13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separate components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separate components.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "A recent advance was made in joint training CNN and MRF by passing the error of MRF inference backward into CNN [14].", "startOffset": 112, "endOffset": 116}, {"referenceID": 14, "context": "Nonetheless, an iterative inference of MRF such as the mean field algorithm (MF) [15] is required for each training image during the back-propagation (BP).", "startOffset": 81, "endOffset": 85}, {"referenceID": 15, "context": "[16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 9, "context": "Specifically, DPN models unary terms by extending the VGG-16 network (VGG16) [10] pre-trained on ImageNet, while additional layers are carefully designed to model complex pairwise terms.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": "The learning of these terms is transformed into deterministic endto-end computation by BP, instead of embedding MF into BP as [14], [17] did.", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "The learning of these terms is transformed into deterministic endto-end computation by BP, instead of embedding MF into BP as [14], [17] did.", "startOffset": 132, "endOffset": 136}, {"referenceID": 15, "context": "Although MF can be represented by a RNN [16], it needs to recurrently compute the forward pass so as to achieve good performance and thus the process is time-consuming, e.", "startOffset": 40, "endOffset": 44}, {"referenceID": 12, "context": "Furthermore, DPN is capable of representing multiple types of pairwise terms, making many previous works [13], [14], [16] as its special cases.", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "Furthermore, DPN is capable of representing multiple types of pairwise terms, making many previous works [13], [14], [16] as its special cases.", "startOffset": 111, "endOffset": 115}, {"referenceID": 15, "context": "Furthermore, DPN is capable of representing multiple types of pairwise terms, making many previous works [13], [14], [16] as its special cases.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "(3) DPN approximates MF with convolutional and pooling operations, which can be speeded up by low-rank approximation [18] and easily parallelized [19] in a Graphical Processing Unit (GPU).", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "(3) DPN approximates MF with convolutional and pooling operations, which can be speeded up by low-rank approximation [18] and easily parallelized [19] in a Graphical Processing Unit (GPU).", "startOffset": 146, "endOffset": 150}, {"referenceID": 15, "context": "(2) We show that multiple types of MRFs can be represented in DPN, making many previous works such as RNN [16] and DeepLab [13] as its special cases.", "startOffset": 106, "endOffset": 110}, {"referenceID": 12, "context": "(2) We show that multiple types of MRFs can be represented in DPN, making many previous works such as RNN [16] and DeepLab [13] as its special cases.", "startOffset": 123, "endOffset": 127}, {"referenceID": 19, "context": "We demonstrate the generalizability of DPN model by showing its state-of-the-art performance on several standard semantic image/video segmentation benchmarks, including PASCAL VOC 2012 [20], CityScapes dataset [5] and CamVid dataset [21].", "startOffset": 185, "endOffset": 189}, {"referenceID": 4, "context": "We demonstrate the generalizability of DPN model by showing its state-of-the-art performance on several standard semantic image/video segmentation benchmarks, including PASCAL VOC 2012 [20], CityScapes dataset [5] and CamVid dataset [21].", "startOffset": 210, "endOffset": 213}, {"referenceID": 20, "context": "We demonstrate the generalizability of DPN model by showing its state-of-the-art performance on several standard semantic image/video segmentation benchmarks, including PASCAL VOC 2012 [20], CityScapes dataset [5] and CamVid dataset [21].", "startOffset": 233, "endOffset": 237}, {"referenceID": 0, "context": "In comparison to our earlier version of this work [1], we propose a generic deep learning framework, Deep Parsing Network (DPN) to model and solve N -D high-order Markov Random Field (MRF).", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "Our previous study [1] only shows the possibility on 2-D image segmentation problem.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 17, "endOffset": 20}, {"referenceID": 10, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 22, "endOffset": 26}, {"referenceID": 21, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 28, "endOffset": 32}, {"referenceID": 22, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 40, "endOffset": 44}, {"referenceID": 24, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 46, "endOffset": 50}, {"referenceID": 25, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 58, "endOffset": 62}, {"referenceID": 27, "context": "Existing studies [4], [11], [22], [23], [24], [25], [26], [27], [28] on semantic segmentation focus on either constructing specific graph structure so that contextual information and long-term dependencies can be captured, or designing suitable network architecture to leverage the power of deep learning.", "startOffset": 64, "endOffset": 68}, {"referenceID": 6, "context": "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].", "startOffset": 137, "endOffset": 140}, {"referenceID": 28, "context": "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].", "startOffset": 142, "endOffset": 146}, {"referenceID": 7, "context": "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].", "startOffset": 170, "endOffset": 173}, {"referenceID": 29, "context": "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].", "startOffset": 175, "endOffset": 179}, {"referenceID": 8, "context": "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].", "startOffset": 209, "endOffset": 212}, {"referenceID": 30, "context": "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].", "startOffset": 214, "endOffset": 218}, {"referenceID": 31, "context": "Researchers improved labeling accuracy by exploring rich information to define the pairwise functions, including long-range dependencies [7], [29], high-order potentials [8], [30], and semantic label contexts [9], [31], [32].", "startOffset": 220, "endOffset": 224}, {"referenceID": 6, "context": "[7] attained accurate segmentation boundary by inferring on a fully-connected graph.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] extended [7] by defining both high-order and longrange terms between pixels.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] extended [7] by defining both high-order and longrange terms between pixels.", "startOffset": 13, "endOffset": 16}, {"referenceID": 8, "context": "Global or local semantic contexts between labels were also investigated by [9].", "startOffset": 75, "endOffset": 78}, {"referenceID": 32, "context": "[33] unified foreground object segmentation, tracking and occlusion reasoning into a carefully designed MRF model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Optical flow based long-term trajectories [2] were also exploited to discover moving objects.", "startOffset": 42, "endOffset": 45}, {"referenceID": 33, "context": "[34] employed fully-connected CRF augmented with object potentials for efficient multi-class inference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 32, "endOffset": 36}, {"referenceID": 12, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 38, "endOffset": 42}, {"referenceID": 13, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 44, "endOffset": 48}, {"referenceID": 15, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 50, "endOffset": 54}, {"referenceID": 16, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 56, "endOffset": 60}, {"referenceID": 27, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 68, "endOffset": 72}, {"referenceID": 35, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 74, "endOffset": 78}, {"referenceID": 36, "context": "With deep models, exising works [11], [13], [14], [16], [17], [28], [35], [36], [37] demonstrated encouraging segmentation results through using just simple definition of the pairwise function or even neglecting it.", "startOffset": 80, "endOffset": 84}, {"referenceID": 10, "context": "[11] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11] transformed fully-connected layers of CNN into convolutional layers, making accurate per-pixel classification possible using the contemporary CNN architectures that were pre-trained on ImageNet [12].", "startOffset": 199, "endOffset": 203}, {"referenceID": 12, "context": "[13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separated components.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[13] improved [11] by feeding the outputs of CNN into a MRF with simple pairwise potentials, but it treated CNN and MRF as separated components.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "A recent advance was obtained by [14], which jointly trained CNN and MRF by passing the error of MRF inference backward into CNN, but iterative inference of MRF such as the mean field algorithm (MF) [15] is required for each training image during the back-propagation (BP).", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "A recent advance was obtained by [14], which jointly trained CNN and MRF by passing the error of MRF inference backward into CNN, but iterative inference of MRF such as the mean field algorithm (MF) [15] is required for each training image during the back-propagation (BP).", "startOffset": 199, "endOffset": 203}, {"referenceID": 15, "context": "[16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] further showed that the procedure of MF inference can be represented as a Recurrent Neural Network (RNN), but their computational costs are similar to that of [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 37, "context": "Recent efforts in this direction include SegNet [38], which adopted an encoderdecoder architecture but did not take temporal relationships into consideration.", "startOffset": 48, "endOffset": 52}, {"referenceID": 38, "context": "MRF [39] is an undirected graph where each node represents a voxel in a video I, and each edge represents relation between voxels, as shown in Fig.", "startOffset": 4, "endOffset": 8}, {"referenceID": 39, "context": "Traditional approaches [40] usually define the edges E on rectangular grid in 3-D space.", "startOffset": 23, "endOffset": 27}, {"referenceID": 40, "context": "In the temporal domain, the neighboring voxels i = [i ti] and j = [j tj ] are defined as those lie on the same temporal trajectories \u2206i\u2192j, which can be estimated by standard optical flow techniques [41].", "startOffset": 198, "endOffset": 202}, {"referenceID": 12, "context": "This formulation has been adopted by most of the recent deep models [13], [14], [16] for semantic image segmentation.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "This formulation has been adopted by most of the recent deep models [13], [14], [16] for semantic image segmentation.", "startOffset": 74, "endOffset": 78}, {"referenceID": 15, "context": "This formulation has been adopted by most of the recent deep models [13], [14], [16] for semantic image segmentation.", "startOffset": 80, "endOffset": 84}, {"referenceID": 14, "context": "(1) can be obtained by the mean field (MF) algorithm [15], which estimates the joint distribution of MRF", "startOffset": 53, "endOffset": 57}, {"referenceID": 5, "context": "(9), which is denoted as free energy F (Q) [6].", "startOffset": 43, "endOffset": 46}, {"referenceID": 19, "context": "To simplify the discussions, we take PASCAL VOC 2012 (VOC12) [20] as an example.", "startOffset": 61, "endOffset": 65}, {"referenceID": 42, "context": "2D locally convolutional layer) is widely used in face recognition [43], [44], [45] to capture different information from different", "startOffset": 67, "endOffset": 71}, {"referenceID": 43, "context": "2D locally convolutional layer) is widely used in face recognition [43], [44], [45] to capture different information from different", "startOffset": 73, "endOffset": 77}, {"referenceID": 44, "context": "2D locally convolutional layer) is widely used in face recognition [43], [44], [45] to capture different information from different", "startOffset": 79, "endOffset": 83}, {"referenceID": 41, "context": "Moreover, \u2018relu\u2019, \u2018idn\u2019, \u2018soft\u2019, \u2018sigm\u2019, and \u2018lin\u2019 represent the activation functions, including rectified linear unit [42], identity, softmax, sigmoid, and linear, respectively.", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "Many existing deep models such as [13], [14], [16] employed Eqn.", "startOffset": 34, "endOffset": 38}, {"referenceID": 13, "context": "Many existing deep models such as [13], [14], [16] employed Eqn.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "Many existing deep models such as [13], [14], [16] employed Eqn.", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "During finetuning, all these stages solve the pixelwise softmax loss [11], but updating different sets of parameters.", "startOffset": 69, "endOffset": 73}, {"referenceID": 18, "context": "If we parallelize these operations using matrix multiplication on GPU as [19] did, the operation in b12 can be computed within 30ms.", "startOffset": 73, "endOffset": 77}, {"referenceID": 17, "context": "Note that convolutions in DPN can be further speeded up by low-rank decompositions [18] of the filters and model compressions [46].", "startOffset": 83, "endOffset": 87}, {"referenceID": 45, "context": "Note that convolutions in DPN can be further speeded up by low-rank decompositions [18] of the filters and model compressions [46].", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "In contrast, existing works [13], [16] employ fast Gaussian filtering [47] to accelerate the direct calculation of Eqn.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "In contrast, existing works [13], [16] employ fast Gaussian filtering [47] to accelerate the direct calculation of Eqn.", "startOffset": 34, "endOffset": 38}, {"referenceID": 46, "context": "In contrast, existing works [13], [16] employ fast Gaussian filtering [47] to accelerate the direct calculation of Eqn.", "startOffset": 70, "endOffset": 74}, {"referenceID": 6, "context": "For a mini-batch of ten 512\u00d7512 images, a recently optimized implementation [7] takes 12 seconds on CPU to compute one iteration of (13).", "startOffset": 76, "endOffset": 79}, {"referenceID": 19, "context": "We compare DPN with the state-of-the-art methods on PASCAL VOC 2012 (VOC12) [20] , Cityscapes [5] and CamVid", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "We compare DPN with the state-of-the-art methods on PASCAL VOC 2012 (VOC12) [20] , Cityscapes [5] and CamVid", "startOffset": 94, "endOffset": 97}, {"referenceID": 47, "context": "[48] datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "All existing works employed mean pixelwise intersection-over-union (denoted as mIoU) [11] to evaluate their performance.", "startOffset": 85, "endOffset": 89}, {"referenceID": 48, "context": "(3) For those objects that have been correctly localized, we compare the predicted object boundary with the ground truth boundary, measuring the precision of semantic boundary similar to [49].", "startOffset": 187, "endOffset": 191}, {"referenceID": 10, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 78, "endOffset": 82}, {"referenceID": 27, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 93, "endOffset": 97}, {"referenceID": 12, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 107, "endOffset": 111}, {"referenceID": 36, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 118, "endOffset": 122}, {"referenceID": 49, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 147, "endOffset": 151}, {"referenceID": 15, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 157, "endOffset": 161}, {"referenceID": 50, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 176, "endOffset": 180}, {"referenceID": 51, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 193, "endOffset": 197}, {"referenceID": 52, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 203, "endOffset": 207}, {"referenceID": 33, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 220, "endOffset": 224}, {"referenceID": 37, "context": "DPN is compared with the state-of-the-art segmentation methods, including FCN [11], Zoom-out [28], DeepLab [13], WSSL [37], BoxSup [50], Piecewise [17], RNN [16], SuperParsing [51], Dilation10 [52], ALE [53], Multiclass [34], and SegNet [38].", "startOffset": 237, "endOffset": 241}, {"referenceID": 53, "context": "Methods in the last group also employed MS-COCO [54] to pre-train deep models.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "Pairwise Terms DSN [14] DeepLab [13] DPN", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "Pairwise Terms DSN [14] DeepLab [13] DPN", "startOffset": 32, "endOffset": 36}, {"referenceID": 6, "context": "VGG16+denseCRF [7].", "startOffset": 15, "endOffset": 18}, {"referenceID": 15, "context": "DPN pairwise terms denseCRF [16] (a) (b)", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "A CRF [7] with dense pairwise edges needs more than 5 iterations to converge.", "startOffset": 6, "endOffset": 9}, {"referenceID": 12, "context": "Note that the existing deep models such as [13], [14], [16] required 5\u223c10 iterations to converge as well.", "startOffset": 43, "endOffset": 47}, {"referenceID": 13, "context": "Note that the existing deep models such as [13], [14], [16] required 5\u223c10 iterations to converge as well.", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": "Note that the existing deep models such as [13], [14], [16] required 5\u223c10 iterations to converge as well.", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "FCN [11] 76.", "startOffset": 4, "endOffset": 8}, {"referenceID": 27, "context": "2 Zoom-out [28] 85.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "6 Piecewise [17] 87.", "startOffset": 12, "endOffset": 16}, {"referenceID": 12, "context": "7 DeepLab [13] 84.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "6 RNN [16] 87.", "startOffset": 6, "endOffset": 10}, {"referenceID": 36, "context": "0 WSSL\u2020 [37] 89.", "startOffset": 8, "endOffset": 12}, {"referenceID": 15, "context": "9 RNN\u2020 [16] 90.", "startOffset": 7, "endOffset": 11}, {"referenceID": 49, "context": "7 BoxSup\u2020 [50] 89.", "startOffset": 10, "endOffset": 14}, {"referenceID": 53, "context": "The approaches pre-trained on COCO [54] are marked with \u2020.", "startOffset": 35, "endOffset": 39}, {"referenceID": 51, "context": "Dilation10 [52] no no no 97.", "startOffset": 11, "endOffset": 15}, {"referenceID": 16, "context": "1 Piecewise [17] no no no 97.", "startOffset": 12, "endOffset": 16}, {"referenceID": 10, "context": "3 FCN [11] no no no 97.", "startOffset": 6, "endOffset": 10}, {"referenceID": 36, "context": "3 WSSL [37] yes no 2 97.", "startOffset": 7, "endOffset": 11}, {"referenceID": 12, "context": "8 DeepLab [13] no no 2 97.", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "1 RNN [16] no no 2 96.", "startOffset": 6, "endOffset": 10}, {"referenceID": 52, "context": "ALE [53] 73.", "startOffset": 4, "endOffset": 8}, {"referenceID": 50, "context": "59 SuperParsing [51] 70.", "startOffset": 16, "endOffset": 20}, {"referenceID": 39, "context": "[40] 74.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "18 Liu and He [34] 66.", "startOffset": 14, "endOffset": 18}, {"referenceID": 37, "context": "2 SegNet [38] 68.", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": "We evaluate DPN on several scales of the images and then average the results following [13], [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "We evaluate DPN on several scales of the images and then average the results following [13], [17].", "startOffset": 93, "endOffset": 97}, {"referenceID": 15, "context": "Following [16], [50], we pre-train DPN with COCO, where 20 object categories that are also presented in VOC12 are selected for training.", "startOffset": 10, "endOffset": 14}, {"referenceID": 49, "context": "Following [16], [50], we pre-train DPN with COCO, where 20 object categories that are also presented in VOC12 are selected for training.", "startOffset": 16, "endOffset": 20}, {"referenceID": 51, "context": "1% [52].", "startOffset": 3, "endOffset": 7}, {"referenceID": 51, "context": "[52] takes a multi-scale aggregation strategy, which can be easily integrated into DPN to further boost the performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "12 demonstrates the comparisons of DPN with FCN [11] and DeepLab [13].", "startOffset": 48, "endOffset": 52}, {"referenceID": 12, "context": "12 demonstrates the comparisons of DPN with FCN [11] and DeepLab [13].", "startOffset": 65, "endOffset": 69}], "year": 2017, "abstractText": "Semantic segmentation tasks can be well modeled by Markov Random Field (MRF). This paper addresses semantic segmentation by incorporating high-order relations and mixture of label contexts into MRF. Unlike previous works that optimized MRFs using iterative algorithm, we solve MRF by proposing a Convolutional Neural Network (CNN), namely Deep Parsing Network (DPN), which enables deterministic end-to-end computation in a single forward pass. Specifically, DPN extends a contemporary CNN to model unary terms and additional layers are devised to approximate the mean field (MF) algorithm for pairwise terms. It has several appealing properties. First, different from the recent works that required many iterations of MF during back-propagation, DPN is able to achieve high performance by approximating one iteration of MF. Second, DPN represents various types of pairwise terms, making many existing models as its special cases. Furthermore, pairwise terms in DPN provide a unified framework to encode rich contextual information in high-dimensional data, such as images and videos. Third, DPN makes MF easier to be parallelized and speeded up, thus enabling efficient inference. DPN is thoroughly evaluated on standard semantic image/video segmentation benchmarks, where a single DPN model yields state-of-the-art segmentation accuracies on PASCAL VOC 2012, Cityscapes dataset and CamVid dataset.", "creator": "LaTeX with hyperref package"}}}