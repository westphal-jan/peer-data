{"id": "1606.07262", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "On the Theoretical Capacity of Evolution Strategies to Statistically Learn the Landscape Hessian", "abstract": "We study the theoretical capacity to statistically learn local landscape information by Evolution Strategies (ESs). Specifically, we investigate the covariance matrix when constructed by ESs operating with the selection operator alone. We model continuous generation of candidate solutions about quadratic basins of attraction, with deterministic selection of the decision vectors that minimize the objective function values. Our goal is to rigorously show that accumulation of winning individuals carries the potential to reveal valuable information about the search landscape, e.g., as already practically utilized by derandomized ES variants. We first show that the statistically-constructed covariance matrix over such winning decision vectors shares the same eigenvectors with the Hessian matrix about the optimum. We then provide an analytic approximation of this covariance matrix for a non-elitist multi-child $(1,\\lambda)$-strategy, which holds for a large population size $\\lambda$. Finally, we also numerically corroborate our results.", "histories": [["v1", "Thu, 23 Jun 2016 10:38:49 GMT  (447kb)", "http://arxiv.org/abs/1606.07262v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["ofer m shir", "jonathan roslund", "amir yehudayoff"], "accepted": false, "id": "1606.07262"}, "pdf": {"name": "1606.07262.pdf", "metadata": {"source": "CRF", "title": "On the Theoretical Capacity of Evolution Strategies to Statistically Learn the Landscape Hessian", "authors": ["Ofer M. Shir", "Jonathan Roslund", "Amir Yehudayoff"], "emails": ["ofersh@telhai.ac.il", "jroslund@lkb.upmc.fr", "amir.yehudayoff@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 262v 1 [cs.N E] 23 JuKeywords: Theory of evolutionary strategies, statistical learning, adaptation of the covariance matrix, Hessian landscape, boundary distributions of order statistics, extreme value distributions"}, {"heading": "1 Introduction", "text": "In fact, it is assumed that this learned covariance matrix, which defines the possibilities of variation in the search engine world, can also be applied in practice. It was supported by the logic that the localization of the global optimum can be supported by an ES with mutation steps corresponding to the actual landscape, or in other words that the optimal covariance distribution can provide mutation steps whose probability of equivalence corresponds to the levels of the landscape (maximizes the rate of progression in the same time). Additionally, it was argued that the reduction of a general problem to an isotropic square problem can be achieved by placing search points on a covariance matrix."}, {"heading": "2 Statistical Landscape Learning", "text": "We will outline the research question we are aiming at: What is the relationship between the statistically learned covariance matrix and the landscape of Hesse, if in each iteration a single winner is selected, assuming samples generated following an isotropic gauss (no adaptation)? We will focus on the a posteriori statistical construction of the covariance matrix of the decision variables when they reach the proximity of the optimum when they undergo the ES operation. In the following, we will formulate the problem, starting from a model, and present our notation."}, {"heading": "2.1 Problem Statement and Assumed Model", "text": "We assume that the objective function can be extended by the optimum. We model the n-dimensional basin of attraction by ~ x * using a square approximation. We assume that this expansion precisely J (~ x \u2212 x *) = J (~ x) = ~ xT \u00b7 H \u00b7 ~ x, (1) where H is the Hessian landscape around the optimum. The canonical, non-elitist ES search process works in the following way: The ES generates the search points ~ x1,., ~ xis in each iteration, based on Gaussian sample in relation to the given search point. We deal in particular with the canonical variation operator of ES, which is a normally distributed mutation ~ z \u00b2 N (~ 0, I).J is ~ J, ~ J is an individual search function (1 x)."}, {"heading": "2.2 Probability Functions of the ES Step", "text": "The length of a mutation vector, \u221a ~ zT ~ z, obeys the so-called \u03b7 distribution with n degrees of freedom. If we assume a square basin of attraction, its Hessian matrix forms the identity: H = I. In this case, the distribution of the Harry distributions is the standard Harry distributions and has the following cumulative distribution function (CDF), which takes into account the dimensionality of the search space n: F = 12n / 2\u0432 (n / 2) approximate 0 \u2212 1 Exp (\u2212 t 2) dt (4) dt (4) is the following cumulative distribution function (CDF), which takes into account the dimensionality of the search space n: F = 12n / 2\u0432 (n / 2) approximate 0 \u2212 1 Exp (\u2212 t 2) dt (4) dt (4), where the gamma function is defined by:"}, {"heading": "3 Covariance Matrix Formulation", "text": "In the current section, we formulate the covariance matrix on the basis of its definition density functions and then prove that it relates to the landscape of Hessian.By construction, the origin is determined at the parental search point, which is in the optimum. Analytically, the covariance elements are therefore reduced to the following expected values: Cij = surface compensation PDF ~ y (~ x) d ~ x, (10) where PDF ~ y (~ x) is a n-dimensional density function that characterizes the decision variables about the optimum. Essentially, the current study aims at understanding this expression in equation. 10. To this end, the disclosure of the nature of PDF ~ y is necessary for the interpretation of the covariance matrix. Important is the selection mechanism of heuristics blind to the position of candidate solutions in the search space, and its only criterion is the shunting functional values."}, {"heading": "4 Analytic Approximation", "text": "In this section, we provide an approximation for PDF\u03c9 (J (~ x)) and consequently for PDF ~ y (~ x) for the explicit calculation of the covariance matrix using Eq. 10.A non-elite multi-child selection, with a single individual deterministically selected from offspring in each iteration. However, in order to formulate the density of these winners, it is appropriate to first characterize the distribution function of the winning event among the candidates. (17) 1Gupta [7] showed that if the dimension n is equal, the distribution of the winners among the candidates enables 1: CDF\u03c9 (solvent 1) = Pr (1 \u2212 CDF\u0432) = 1 \u2212 (1 \u2212 CDF) distribution, which is clearly derived from the Eqta. (17) 1Gupta [7] 1Gupta [4] and the distribution of the Eqs (DF2) is equal, the distribution of the winners of the Eqf function (in the form CDF \u00b2) is isotropic (in the form CDF): (explicit)."}, {"heading": "4.1 Limit Distributions of Order Statistics", "text": "We treat the derived distribution of winners for large sample sizes, i.e., when the population size \u03bb = > q = infinity increases. We designate the CDF in Eq. 17 with a subscript \u03bb, L\u03bb (Phillips) = 1 \u2212 (1 \u2212 CDFductions (Phillips)) \u03bb, and consider the limit if it tends toward infinity: lim \u03bb \u2212 L\u03bb (\u043d) = {0 if CDF\u0432 (Phillips) = 0 \u2212 1 if CDFductions (Phillips)) > 0According to the Fisher-Tippett theory [6], also known as the extreme types theorem, the von Mises distribution family for minima (or the minimally generalized extreme value distributions (GEVDmin) are the only non-degenerated family of distributions that meets this limit. They are characterized as a unified family of distributions by the following CDFs."}, {"heading": "4.2 Covariance Derivation", "text": "Using the Weibull form as a characteristic density of PDF\u03c9 = ~ ~ \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b7 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 square \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 million \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2. \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2"}, {"heading": "5 Simulation Study", "text": "We implemented our model into a numerical procedure to compare the statistical parameters with the analytical calculations in practice, following algorithm 1, Cstat, which stamped statistically as described therein. Numerical validation occurs here for two aspects of our theoretical work: Theorem 1 and the analytical approximation for the covariance matrix A, Cstat versus H We generated a large set of random positively-defined matrices in different dimensions {n} with a spectrum of conditional numbers. For each experiment, the numerical procedure generated a random symmetric matrix A, diagonalized it into a series of orthonal property vectors U, drew n random positive numbers into a diagonal matrix D, and put H = UAU \u2212 1. We then applied algorithm 1 by comparing the essional matrix as a landscape Hessian. The resulting coximal matrices {and} Hessian matrix were calculated with the diagram D."}, {"heading": "5.2.1 Validating the Approximated \u03c72 Density fT \u03c72", "text": "Figure 1 shows the approximate density functions of the generalized \u03c72 distribution, fT \u03c72 (Eq. 9) for the four Hessian forms (H-1) - (H-4), which obviously represent solid approximates.5.2.2 The validation of the winning densities PDF\u03c9 and PDF GEVD \u03c9Figures 2 and 3 provide a validation of the winning density, which was accurately described by PDF\u03c9 in Eq. 18 and later approximated by PDFGEVD\u03c9 in Eq. 28 for large orders of magnitude. Interestingly, PDF\u03c9, which is realized here by the approximate generalized \u03b22 distribution FT \u03b22, exhibits a decreased accuracy in H1, H2 and H3. Obviously, it is highly sensitive to the approximation error of FT \u03b22, which is amplified by the exponent \u03b22. At the same time, PDFGEVD\u0438 exhibits a decreased accuracy in H2, H3 and H4, which is attributable to its additional size (see Figure 1)."}, {"heading": "5.2.3 Validating the Approximated Integral", "text": "Finally, we compared Cstat with the obtained analytical approximations. For the isotropic case, the result of Gl. 31 was successfully confirmed for a range of search space dimensions n. Thus, Gl. 31 received a value of 0.568 for the 100-dimensional case (H-4). For the general case, we considered the 3-dimensional case (H-1). Cstat was constructed with \u03bb = 20 and over 10,000 iterations, which are presented side by side with the numerical integration of Gl. 30 in Table 1. Additionally, their explicit eigenvectors were provided therein. H1 (n = 3) (Gl = 20, Niter = 10 5) 0 0.5 1 2.5 00.522.5Raw Sampling0 1 2 3 0.6 00.7 00.7 20,630.630.700.40.7\u03bb = 40.0000.4 (Gl."}, {"heading": "6 Conclusion", "text": "Our analytical work modeled passive ES learning processes, i.e. no incremental adjustment or covariance matrix adjustment was used in constructing a covariance matrix from winning decision vectors. We demonstrated that the statistically constructed covariance oscillates optimally with the Hessian landscape when assuming a square basin. The implication of this result is the enhanced ability of ESs to identify sensitive optimization directions by extracting this information from the learned covariance matrix. Subsequently, we derived an analytical approximation for the covariance matrix based on two assumptions - (i) the generalized density function (2) was approximated assuming moderate standard deviations of eigenvalues, and (ii) the distribution of the winners was shown to follow the Weibull distribution with a calculated tail index when the population size is large and adheres the boundary distributions of the order statistics, where multiple levels of accuracy were then discussed."}], "references": [{"title": "Contemporary Evolution Strategies", "author": ["T. B\u00e4ck", "C. Foussette", "P. Krause"], "venue": "Natural Computing Series. Springer-Verlag Berlin Heidelberg,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Evolution Strategies a Comprehensive Introduction", "author": ["H.-G. Beyer", "H.-P. Schwefel"], "venue": "Natural Computing: An International Journal, 1(1):3\u201352,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Extreme Value and Related Models with Applications in Engineering and Science", "author": ["E. Castillo", "A.S. Hadi", "N. Balakrishnan", "J.M. Sarabia"], "venue": "John Wiley and Sons,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Modelling Extremal Events for Insurance and Finance", "author": ["P. Embrechts", "C. Kl\u00fcppelberg", "T. Mikosch"], "venue": "Springer-Verlag,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "The distribution and properties of a weighted sum of chi squares", "author": ["A.H. Feiveson", "F.C. Delaney"], "venue": "Technical report, National Aeronautics and Space Administration,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1968}, {"title": "Limiting forms of the frequency distribution of the largest or smallest member of a sample", "author": ["R. Fisher", "L. Tippett"], "venue": "Proc. Cambridge Philos. Soc., 24:180\u2013190,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1928}, {"title": "Order Statistics from the Gamma Distribution", "author": ["S.S. Gupta"], "venue": "Technometrics, 2, May", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1960}, {"title": "Completely Derandomized Self-Adaptation in Evolution Strategies", "author": ["N. Hansen", "A. Ostermeier"], "venue": "Evolutionary Computation, 9(2):159\u2013195,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2001}, {"title": "On Correlated Mutations in Evolution Strategies", "author": ["G. Rudolph"], "venue": "Parallel Problem Solving from Nature - PPSN II, pages 105\u2013114, Amsterdam,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Convergence rates of evolutionary algorithms for a class of convex objective functions", "author": ["G. Rudolph"], "venue": "Control and Cybernetics, 26(3),", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Stochastic Global Optimization", "author": ["A. Zhigljavsky", "A. \u017dilinskas"], "venue": "Springer Optimization and Its Applications. Springer US,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction ESs [2], popular heuristics that excel in global optimization of continuous search landscapes, utilize a Gaussian-based update (variation) step with an evolving covariance matrix.", "startOffset": 19, "endOffset": 22}, {"referenceID": 8, "context": "It was supported by the rationale that locating the global optimum by an ES can be accommodated using mutation steps that fit the actual landscape, or in other words, that the optimal covariance distribution can offer mutation steps whose equidensity probability contours match the level sets of the landscape (maximizing the progress rate at the same time) [9].", "startOffset": 358, "endOffset": 361}, {"referenceID": 8, "context": "Rudolph [9] showed that ESs are capable of facilitating such learning and derived practical bounds on the population size toward the end of a successful learning period.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "Especially, accumulation of selected individuals is practically utilized by derandomized ES variants [8], and it has led to the formulation of a successful family of search heuristics [1].", "startOffset": 101, "endOffset": 104}, {"referenceID": 0, "context": "Especially, accumulation of selected individuals is practically utilized by derandomized ES variants [8], and it has led to the formulation of a successful family of search heuristics [1].", "startOffset": 184, "endOffset": 187}, {"referenceID": 4, "context": "The random variable \u03c8 = J(~z) now obeys a generalized \u03c72-distribution, whose exact distribution function is described as follows [5]: FH\u03c72(\u03c8) = \u222b \u221e 0 2 \u03c0 sin t\u03c8 2 t cos \uf8eb", "startOffset": 129, "endOffset": 132}, {"referenceID": 4, "context": "At the same time, this CDF is known to follow an approximation [5], FT \u03c72 (\u03c8) = \u03a5\u03b7 \u0393 (\u03b7) \u222b \u03c8 0 t exp (\u2212\u03a5t) dt, (7) with \u03a5 and \u03b7 accounting for matching the first two moments of ~zTH~z (and the subscript T marks the transformed distribution): \u03a5 = 1 2 \u2211n i=1 \u2206i \u2211n i=1\u2206 2 i , \u03b7 = 1 2 ( \u2211n i=1\u2206i) 2 \u2211n i=1 \u2206 2 i .", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "(9) The accuracy of this approximation depends upon the standard deviation of the eigenvalues [5], which is clearly related to the so-called condition number.", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "(17) Gupta [7] showed that when the dimension n is even, the distribution of the winners for the \u03c7 distribution (isotropic basin case) possesses a simple form:", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "Gupta [7] derived explicit order statistic results from the Gamma distribution, to which the \u03c72 distribution belongs, including the distribution function as well as moments of the kth order statistic.", "startOffset": 6, "endOffset": 9}, {"referenceID": 5, "context": "0 if CDF\u03c8 (\u03c8) = 0 1 if CDF\u03c8 (\u03c8) > 0 According to the Fisher-Tippett theorem [6], also known as the extremal types theorem, the vonMises family of distributions for minima (or the minimal generalized extreme value distributions (GEVDmin)) are the only non-degenerate family of distributions satisfying this limit.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "6 in [3] [pp.", "startOffset": 5, "endOffset": 8}, {"referenceID": 9, "context": "Note that Rudolph had already taken a related mathematical approach, which he termed asymptotic theory of extreme order statistics, to characterize convergence properties of ESs on a class of convex objective functions [10].", "startOffset": 219, "endOffset": 223}, {"referenceID": 10, "context": "Also, GEVD is introduced to the broad perspective of Stochastic Global Optimization in [11], a book which also constitutes a proper mathematical reference for this topic, yet in a slightly different light.", "startOffset": 87, "endOffset": 91}, {"referenceID": 2, "context": "(27) See [3] and [4] for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics.", "startOffset": 9, "endOffset": 12}, {"referenceID": 3, "context": "(27) See [3] and [4] for an overview on the family of generalized extreme value distributions and on the limit distributions of order statistics.", "startOffset": 17, "endOffset": 20}, {"referenceID": 2, "context": "1 in [3][p.", "startOffset": 5, "endOffset": 8}, {"referenceID": 10, "context": "3 in [11].", "startOffset": 5, "endOffset": 9}], "year": 2016, "abstractText": "We study the theoretical capacity to statistically learn local landscape information by Evolution Strategies (ESs). Specifically, we investigate the covariance matrix when constructed by ESs operating with the selection operator alone. We model continuous generation of candidate solutions about quadratic basins of attraction, with deterministic selection of the decision vectors that minimize the objective function values. Our goal is to rigorously show that accumulation of winning individuals carries the potential to reveal valuable information about the search landscape, e.g., as already practically utilized by derandomized ES variants. We first show that the statistically-constructed covariance matrix over such winning decision vectors shares the same eigenvectors with the Hessian matrix about the optimum. We then provide an analytic approximation of this covariance matrix for a non-elitist multi-child (1, \u03bb)-strategy, which holds for a large population size \u03bb. Finally, we also numerically corroborate our results.", "creator": "LaTeX with hyperref package"}}}