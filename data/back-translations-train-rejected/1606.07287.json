{"id": "1606.07287", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Picture It In Your Mind: Generating High Level Visual Representations From Textual Descriptions", "abstract": "In this paper we tackle the problem of image search when the query is a short textual description of the image the user is looking for. We choose to implement the actual search process as a similarity search in a visual feature space, by learning to translate a textual query into a visual representation. Searching in the visual feature space has the advantage that any update to the translation model does not require to reprocess the, typically huge, image collection on which the search is performed. We propose Text2Vis, a neural network that generates a visual representation, in the visual feature space of the fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis optimizes two loss functions, using a stochastic loss-selection method. A visual-focused loss is aimed at learning the actual text-to-visual feature mapping, while a text-focused loss is aimed at modeling the higher-level semantic concepts expressed in language and countering the overfit on non-relevant visual components of the visual loss. We report preliminary results on the MS-COCO dataset.", "histories": [["v1", "Thu, 23 Jun 2016 12:25:09 GMT  (2000kb,D)", "http://arxiv.org/abs/1606.07287v1", "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy"]], "COMMENTS": "Neu-IR '16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy", "reviews": [], "SUBJECTS": "cs.IR cs.CL cs.CV cs.NE", "authors": ["fabio carrara", "rea esuli", "tiziano fagni", "fabrizio falchi", "alejandro moreo fern\\'andez"], "accepted": false, "id": "1606.07287"}, "pdf": {"name": "1606.07287.pdf", "metadata": {"source": "CRF", "title": "Picture It In Your Mind: Generating High Level Visual Representations From Textual Descriptions", "authors": ["Fabio Carrara", "Andrea Esuli", "Tiziano Fagni", "Fabrizio Falchi", "Alejandro Moreo"], "emails": ["fabio.carrara@isti.cnr.it", "andrea.esuli@isti.cnr.it", "tiziano.fagni@isti.cnr.it", "fabrizio.falchi@isti.cnr.it", "alejandro.moreo@isti.cnr.it"], "sections": [{"heading": "Keywords", "text": "Image retrieval; cross-media retrieval; text display"}, {"heading": "1. INTRODUCTION", "text": "The actual retrieval process can be implemented in many ways, depending on how the shared search space between text and images is defined; the search space can be created on the basis of permission to make digital or hard copies of this work; for any other purpose, the copies are not made for profit or commercial advantage, and the full quotation is made on the first page. Copyrights for third-party components of this work must be rewarded; for all other uses, the owner / author is held responsible."}, {"heading": "2. RELATED WORK", "text": "In fact, it is the case that most people who are able to determine themselves are able to determine themselves what they want and what they want. In fact, it is the case that they are able to determine themselves what they want and what they want. In fact, it is the case that they are able to determine themselves what they want and what they want. In fact, it is the case that they are able to determine themselves what they want and what they want. In fact, it is the case that they are able to determine themselves what they want and what they want."}, {"heading": "3. GENERATING VISUAL REPRESENTATIONS OF TEXT", "text": "In fact, most people are able to determine for themselves what they want and what they don't want. (...) Most people in the world have no idea what they want. (...) Most of us have no idea what they want. (...) Most of us have no idea what they want. (...) Most of us have no idea what they want. (...) Most of us have no idea what they want. (...) Most of us have no idea what they want. (...) Most of us have no idea what they want. (...) Most of us have no idea what they want. (...) Most of us have no idea what they want. (...) Most of us have no idea what they want. (...) (...) (...) () () () () () () ()) () ()) () ()) () ()) () ()) () () ()) () () ()) () () () ()) () () () () ()) () () () () () () () () () () ()) () () ()) () () ()) ()) () ()) () ()) () ()) ()) () () ()) ()) () ()) ()) () ()) ()) () ()) ()) () ()) () ()) () ()) ()) () () ()) () () ()) () () ()) () () ()) () ()) () () () ()) () () () ()) () () () () () () () () () () () ()) () () ()) () () () () () () () ()) () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () ()"}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "We used the Microsoft COCO dataset (MsCOCO4 [17]). MsCOCO was originally proposed for image recognition, segmentation and caption generation. Although other datasets exist for image recovery (e.g. the one proposed in [11]), they are more oriented towards keyword-based queries. We believe that MsCOCO is better suited to the scenario we want to investigate, since the captions to the images are expressed in natural language, i.e. more semantically rich than a short list of keywords that compose a query. MsCOCO contains 82,783 training images (Train2014), 40,504 validation images (Val2014) and approximately 40K and 80K test images that correspond to two different competitions [3] (Test2014 and Test2015). Since MsCOCO was proposed for caption generation, captions are only available in4COlt1."}, {"heading": "4.2 Training", "text": "We solve the optimization problems of equations 5 and 6 using the Adam method [14] for stochastic optimization with standard parameters (learning rate \u03b1 = 0.001, \u03b21 = 0.9, \u03b22 = 0.999 and = 1e \u2212 0.8). Note that there are two independent instances of the Adam optimizer, one for Lt (equation 5) and another for Lv (equation 6). In this preliminary study, we have decided to set an equal probability of selection for both Lt and Lv for the SL; different distributions will be investigated in future research. We set the size of the training to 100 examples. We set the maximum number of iterations to 300,000, but apply an early stop when the model begins to be revised (which is reflected in the validation error)."}, {"heading": "4.3 Evaluation Measures", "text": "Image recovery is performed by searching for similarity in visual space using Euclidean distance on the l2-normalized visual vectors to generate a ranking of images sorted by proximity. We measure the retrieval effectiveness of the visual representations obtained from the text descriptions of our Text2Vis network using the Discounted Cumulative Gain (DCG [12]), defined as: DCGp = p \u2211 i = 1 2reli \u2212 1 log2 (i + 1) (7), reli quantifying the relevance of the element found in relation to the query and p being the rank at which the metric is calculated; we use p = 25 in our experiments, as in related research [11, 6]. Since the rel values are not provided in the MsCOCO, we estimate them based on the ROUGEL [16] query. ROUGEL is one of the evaluation measures for the MsCO caption that is relevant to the caption."}, {"heading": "4.4 Results", "text": "We compared the performance of Text2Vis1 and Text2VisN models with: Rank, a subordinate baseline that generates a random ranking of images for each query; VisSim, a direct similarity method that calculates the Euclidean distances using the original fc6 or fc7 characteristics for the image associated with the query signature in MsCOCO; and VisReg, the text-to-picture regressor described in Figure 1 Table 4.4, reflects the average DCG values achieved by the compared methods. These results show a significant improvement in our proposal for comparison methods. When using fc6 as visual space, Text2Vis1 achieves a relative improvement of 8.51% over VisSim and 1.40% over VisReg. The improvements of Text2VisN are respectively 8.08% and 0.94%. When using fc7 as visual space, Text2Vis1 achieves a relative improvement over Viscops / httth.com / Sgith.com compared to Sc7 visual space."}, {"heading": "4.5 Why Stochastic Loss?", "text": "Text2Vis uses two independent optimizers to optimize the visual (Lv) and textual (Lt) losses, based on stochastic selection for each iteration (SL, Section 3.1). Previous approaches to multimodal learning instead relied on a unique aggregated loss (typically in the form L = Lv + \u03bbLt), which is minimized by a single optimizer [8, 19]. We compared the two approaches in the case of equal relevance of the two losses (\u03bb = 1, uniform distribution for SL). SL optimizes the two losses better (Figure 4) and is less susceptible to overfulfillment. We believe that SL models the relative relevance of the various losses that are combined in a more natural way, i.e. by selecting the losses relative to the assigned relevance, while the numerical aggregation is influenced by the relative values of the losses and the differences in their optimization (for example, the improvement of the SL is also affected by the improvement of the improvement of the SL)."}, {"heading": "4.6 Visual comparison", "text": "In all cases, the results of the VisSim method are dominated by the main visual features of the images: a face for the first query, the contents of the screen for the second query, an exterior image with a slight lower part, plants, people and a bit of sky in the third query. The two text-based methods get results that 6More results at https: / / github.com / AlexMoreo / tensorflow-Tex2Vis"}, {"heading": "VisSim", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "VisReg", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Text2Vis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "VisSim", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "VisReg", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Text2Vis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "VisSim", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "VisReg", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Text2Vis", "text": "In the first query, Text2Vis fetches four out of five relevant images, one more than VisReg. In the other two queries, the results are quite similar: Text2Vis places an image in second place that matches the query perfectly, while VisReg places it in fifth place."}, {"heading": "5. CONCLUSIONS", "text": "The preliminary experiments suggest that our method produces more relevant rankings than those generated by looking for similarity directly on the visual characteristics of a search result. This is an indication that our text-image mapping produces better prototypes of the desired scene than the representation of a sample image itself. A simple explanation for this result is that the text descriptions strongly emphasize the relevant aspects of the scene that the user has in mind, while the visual characteristics directly extracted from the search result track all the information contained in that image, potentially confusing the similarity search by secondary elements of the scene. We also found that the Text2Vis model improved, but by a smaller distance from the VisReg model, which shows that automatic conversion on the network is useful to avoid overadjustment of visual characteristics. We also found that combing losses in a chastic way, rather than numerically, improves both the effectiveness of the system and the effectiveness."}, {"heading": "6. REFERENCES", "text": "[1] Y. Bai, W. Yu, T. Xiao, C. Xu, K. Yang, W.-Y. Ma, and T. Zhao. Multivisual Network for image retrieval. [2] S. Cappallo, T. Mensink, and C. G. Snoek. Image2emoji: Zero-shot emoji prediction for visual media. In Proceedings of the 23rd ACM International Conference on Multimedia, MM '15, S. Cappallo, T. Mensink, and C. G. Snoek. [3] X. Chen, H. Fang, T.-Y. Lin, R. Vedantam, S. Gupta, P. Dolla \u0430r, and C. L. Zitnick."}], "references": [{"title": "Bag-of-words based deep neural network for image retrieval", "author": ["Y. Bai", "W. Yu", "T. Xiao", "C. Xu", "K. Yang", "W.-Y. Ma", "T. Zhao"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 229\u2013232. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Image2emoji: Zero-shot emoji prediction for visual media", "author": ["S. Cappallo", "T. Mensink", "C.G. Snoek"], "venue": "Proceedings of the 23rd ACM International Conference on Multimedia, MM \u201915, pages 1311\u20131314, New York, NY, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J. Costa Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, 36(3):521\u2013535", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "arXiv preprint arXiv:1310.1531", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Word2VisualVec: Cross-Media Retrieval by Visual Feature Prediction", "author": ["J. Dong", "X. Li", "C.G.M. Snoek"], "venue": "ArXiv e-prints,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "From captions to visual concepts and back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1473\u20131482", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-modal retrieval with correspondence autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 7\u201316. ACM", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "et al", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "Devise: A deep visual-semantic embedding model. In Advances in Neural Information Processing Systems, pages 2121\u20132129", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Clickage: Towards bridging semantic and intent gaps via mining click logs of search engines", "author": ["X.-S. Hua", "L. Yang", "J. Wang", "J. Wang", "M. Ye", "K. Wang", "Y. Rui", "J. Li"], "venue": "Proceedings of the 21st ACM international conference on Multimedia, pages 243\u2013252. ACM", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS), 20(4):422\u2013446", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2004}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Computer Vision\u2013ECCV 2014, pages 740\u2013755. Springer", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "Proceedings of the 28th international conference on machine learning (ICML-11), pages 689\u2013696", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1312.5650", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u20131543", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 806\u2013813", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "Advances in neural information processing systems, pages 487\u2013495", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "In this paper we present the preliminary results on learning Text2Vis, a neural network model that converts textual descriptions into visual representations in the same space of those extracted from deep Convolutional Neural Networks (CNN) such as ImageNet [15].", "startOffset": 257, "endOffset": 261}, {"referenceID": 14, "context": "Deep Learning and Deep Convolutional Neural Networks (DCNNs) in particular, have recently shown impressive performance on a number of multimedia information retrieval tasks [15, 23, 10].", "startOffset": 173, "endOffset": 185}, {"referenceID": 22, "context": "Deep Learning and Deep Convolutional Neural Networks (DCNNs) in particular, have recently shown impressive performance on a number of multimedia information retrieval tasks [15, 23, 10].", "startOffset": 173, "endOffset": 185}, {"referenceID": 9, "context": "Deep Learning and Deep Convolutional Neural Networks (DCNNs) in particular, have recently shown impressive performance on a number of multimedia information retrieval tasks [15, 23, 10].", "startOffset": 173, "endOffset": 185}, {"referenceID": 4, "context": "As a result, the activation of the hidden layers has been used in the context of transfer learning and content-based image retrieval [5, 22] as high-level representations of the visual content.", "startOffset": 133, "endOffset": 140}, {"referenceID": 21, "context": "As a result, the activation of the hidden layers has been used in the context of transfer learning and content-based image retrieval [5, 22] as high-level representations of the visual content.", "startOffset": 133, "endOffset": 140}, {"referenceID": 17, "context": "Somewhat similarly, distributional semantic models, such as those produced by Word2Vec [18], or GloVe [21], have been found useful in modeling semantic similarities among words by establishing a correlation between word meaning and position in a vector space.", "startOffset": 87, "endOffset": 91}, {"referenceID": 20, "context": "Somewhat similarly, distributional semantic models, such as those produced by Word2Vec [18], or GloVe [21], have been found useful in modeling semantic similarities among words by establishing a correlation between word meaning and position in a vector space.", "startOffset": 102, "endOffset": 106}, {"referenceID": 3, "context": "Mapping into a common space: The idea of comparing texts and images in a shared space has been investigated by means of Cross-modal Factor Analysis and (Kernel) Canonical Correlation Analysis in [4].", "startOffset": 195, "endOffset": 198}, {"referenceID": 7, "context": ", from text-to-image and viceversa [8].", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "As will be seen, the architecture we are presenting here bears resemblance to one of the architectures investigated in [8], the so-called Correspondence full-modal autoencoder (which is inspired by the multimodal deep learning method [19]).", "startOffset": 119, "endOffset": 122}, {"referenceID": 18, "context": "As will be seen, the architecture we are presenting here bears resemblance to one of the architectures investigated in [8], the so-called Correspondence full-modal autoencoder (which is inspired by the multimodal deep learning method [19]).", "startOffset": 234, "endOffset": 238}, {"referenceID": 0, "context": "Mapping into the textual space: The BoWDNN method trains a deep neural network (DNN) to map images directly into a bag-of-words (BoW) space, where the cosine similarity between BoWs representations is used to generate the ranking [1].", "startOffset": 230, "endOffset": 233}, {"referenceID": 12, "context": ", [13, 7]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 6, "context": ", [13, 7]).", "startOffset": 2, "endOffset": 9}, {"referenceID": 8, "context": "Two other important examples along these lines are DeViSE [9] and ConSE [20].", "startOffset": 58, "endOffset": 61}, {"referenceID": 19, "context": "Two other important examples along these lines are DeViSE [9] and ConSE [20].", "startOffset": 72, "endOffset": 76}, {"referenceID": 14, "context": "Both methods build upon the higher layers of the convolutional neural network of [15]; the main difference lies on the way both methods treat the last layer of the net.", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "Mapping into the visual space: Our proposal Text2Vis belongs to this group where, to the best of our knowledge, the only example up to now was a method dubbed Word2VisualVec [6], which was reported just very recently.", "startOffset": 174, "endOffset": 177}, {"referenceID": 23, "context": "As the visual space we used the fc6 and fc7 layers of the Hybrid network [24] (i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": ", an AlexNet [15] trained on both ImageNet and Places datasets).", "startOffset": 13, "endOffset": 17}, {"referenceID": 5, "context": "We have also investigated the use of pre-trained word embeddings, representing the textual description as the average of the embeddings of the words composing the description (see Equation 1 in [6]), but we have not observed any improvement.", "startOffset": 194, "endOffset": 197}, {"referenceID": 1, "context": "For example, an 11% improvement in MAP is reported in [2] from learning embedding from Flickr tags compared to learning them from Wikipedia pages.", "startOffset": 54, "endOffset": 57}, {"referenceID": 14, "context": "Both predictions v\u2032 and t\u2032 are then confronted with the expected outputs (i) the visual representation v corresponding to the fc6 or fc7 layers of [15], and (ii) a textual descriptor tout that is semantically equivalent to tin.", "startOffset": 147, "endOffset": 151}, {"referenceID": 16, "context": "1 Datasets We used the Microsoft COCO dataset (MsCOCO [17]).", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": ", the one proposed in [11]), they are more oriented to keyword-based queries.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "504 validation images (Val2014 ), and about 40K and 80K test images corresponding to two different competitions [3] (Test2014 and Test2015 ).", "startOffset": 112, "endOffset": 115}, {"referenceID": 23, "context": "Given a \u3008I, C\u3009 pair, we define a labeled instance in our model as \u3008v, tin, tout\u3009, where v \u2208 R is the visual representation of the image I taken from the fc6 layer (or fc7, in separate experiments) of the Hybrid network [24]; tin and tout are two textual descriptors from C representing the input and output descriptors for the model, respectively.", "startOffset": 219, "endOffset": 223}, {"referenceID": 13, "context": "We solve the optimization problems of Equations 5 and 6, using the Adam method [14] for stochastic optimization, with default parameters (learning rate \u03b1 = 0.", "startOffset": 79, "endOffset": 83}, {"referenceID": 11, "context": "We measure the retrieval effectiveness of the visual representations produced from textual descriptions by our Text2Vis network by means of the Discounted Cumulative Gain (DCG [12]), defined as:", "startOffset": 176, "endOffset": 180}, {"referenceID": 10, "context": "where reli quantifies the relevance of the retrieved element at rank position i with respect to the query, and p is the rank at which the metric is computed; we set p = 25 in our experiments, as was done in related research [11, 6].", "startOffset": 224, "endOffset": 231}, {"referenceID": 5, "context": "where reli quantifies the relevance of the retrieved element at rank position i with respect to the query, and p is the rank at which the metric is computed; we set p = 25 in our experiments, as was done in related research [11, 6].", "startOffset": 224, "endOffset": 231}, {"referenceID": 15, "context": "Because the rel values are not provided in the MsCOCO, we estimate them by using theROUGEL [16] metric.", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "ROUGEL is one of the evaluation measures for the MsCOCO caption generation competition [3].", "startOffset": 87, "endOffset": 90}, {"referenceID": 7, "context": "Previous approaches to multimodal learning relied instead on a unique aggregated loss (typically of the form L = Lv +\u03bbLt) that is minimized by a single optimizer [8, 19].", "startOffset": 162, "endOffset": 169}, {"referenceID": 18, "context": "Previous approaches to multimodal learning relied instead on a unique aggregated loss (typically of the form L = Lv +\u03bbLt) that is minimized by a single optimizer [8, 19].", "startOffset": 162, "endOffset": 169}], "year": 2016, "abstractText": "In this paper we tackle the problem of image search when the query is a short textual description of the image the user is looking for. We choose to implement the actual search process as a similarity search in a visual feature space, by learning to translate a textual query into a visual representation. Searching in the visual feature space has the advantage that any update to the translation model does not require to reprocess the, typically huge, image collection on which the search is performed. We propose Text2Vis, a neural network that generates a visual representation, in the visual feature space of the fc6-fc7 layers of ImageNet, from a short descriptive text. Text2Vis optimizes two loss functions, using a stochastic loss-selection method. A visual-focused loss is aimed at learning the actual text-to-visual feature mapping, while a text-focused loss is aimed at modeling the higherlevel semantic concepts expressed in language and countering the overfit on non-relevant visual components of the visual loss. We report preliminary results on the MS-COCO dataset.", "creator": "LaTeX with hyperref package"}}}