{"id": "1606.07374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Multi-Stage Temporal Difference Learning for 2048-like Games", "abstract": "Szubert and Jaskowski successfully used temporal difference (TD) learning together with n-tuple networks for playing the game 2048. However, we observed a phenomenon that the programs based on TD learning still hardly reach large tiles. In this paper, we propose multi-stage TD (MS-TD) learning, a kind of hierarchical reinforcement learning method, to effectively improve the performance for the rates of reaching large tiles, which are good metrics to analyze the strength of 2048 programs. Our experiments showed significant improvements over the one without using MS-TD learning. Namely, using 3-ply expectimax search, the program with MS-TD learning reached 32768-tiles with a rate of 18.31%, while the one with TD learning did not reach any. After further tuned, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000 games, and one among these games even reached a 65536-tile, which is the first ever reaching a 65536-tile to our knowledge. In addition, MS-TD learning method can be easily applied to other 2048-like games, such as Threes. Based on MS-TD learning, our experiments for Threes also demonstrated similar performance improvement, where the program with MS-TD learning reached 6144-tiles with a rate of 7.83%, while the one with TD learning only reached 0.45%.", "histories": [["v1", "Thu, 23 Jun 2016 16:58:33 GMT  (1108kb)", "http://arxiv.org/abs/1606.07374v1", "The version under review of TCIAIG (The first version was sent on 23, October, 2015)"], ["v2", "Tue, 19 Jul 2016 18:36:49 GMT  (1106kb)", "http://arxiv.org/abs/1606.07374v2", "The version has been accepted by TCIAIG (The first version was sent on 23, October, 2015)"]], "COMMENTS": "The version under review of TCIAIG (The first version was sent on 23, October, 2015)", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kun-hao yeh", "i-chen wu", "chu-hsuan hsueh", "chia-chuan chang", "chao-chin liang", "han chiang"], "accepted": false, "id": "1606.07374"}, "pdf": {"name": "1606.07374.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Gabriele Cirulli"], "emails": ["icwu@csie.nctu.edu.tw)"], "sections": [{"heading": null, "text": "In this paper, we propose multi-level TD (MS-TD) learning methods, which are a kind of hierarchical reinforcement of learning methods to effectively improve the performance of the large tiles that have good metrics to analyze the strength of 2048 programs. Our experiments showed significant improvements in each area without using the MS-TD learning methods. Namely, using 3 times the expectation of finding the program with MS-TD learners, which reach a rate of 18.31%, while one with TD learners did not achieve further improvements. The program with MS-TD learners reached a rate of 31.75% in 10,000 games, and one of these games even reached 65536 tiles, which is the first attainment of a 65536 tile to our knowledge."}, {"heading": "II. BACKGROUND", "text": "In fact, it is the case that most of them will be able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that they are able to change the rules. (...) In fact, it is the case that they are able to change the rules. (...) In fact, it is the case that they do not change the rules. (...) In fact, it is the case that they are able to change the rules. (...) In fact, it is the case that they do not change the rules. (...) In fact, it is the case that the rules are not changed. (...)"}, {"heading": "B. Game Tree Search", "text": "A common search algorithm for wild trees used in 2048 programs is the expectation search [2] [13] [19]. Like most wild trees, the leaves are valued with values calculated by heuristic functions. An expectation tree contains two different types of nodes, maximum nodes and random nodes. At a maximum node, its value is the highest value of its children, if any. At a random node, its value is the expected value of its children, if any, weighted according to the probabilities of the children. An expectation search tree is shown in Fig. 2.Several types of characteristics have been used in heuristic functions for 2048 programs that use wild tree search [14] [30]. The first is the monotonicity of a board. Most high-ranking players tend to play 2048 with tiles arranged descending in several ways, as described in [19]. The second is the number of blank tiles that are wiped on board, the less likely the game will end in a few steps."}, {"heading": "C. Temporal Difference (TD) Learning", "text": "This year is the highest in the history of the country."}, {"heading": "III. MULTI-STAGE TD LEARNING ALGORITHM", "text": "eSi rf\u00fc ide nlrrteeeeeeeeeeeeeeteeeeeteeerteerteerrrr rf\u00fc ide nlrsrrrrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrteeeeeeeeeeteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "IV. EXPERIMENTS FOR 2048", "text": "In this section, experiments are conducted to analyze the performance of MS-TD learning for the year 2048 on machines with AMD Opteron 6174 x4, 128GB RAM, Linux. Firstly, in Section IV.A, we modify the N-tuple network from that of Szubert and Ja\u015bkowski [22] for subsequent experiments. Secondly, experiments for the simple 3-step strategy described above (as illustrated in Section III) are performed and analyzed in Section IV.B, and the expected search for the strategy is described in Section IV.C. Further splitting strategies are discussed in Section IV.D. Section IV.E for further methods to further improve MS-TD learning."}, {"heading": "A. New N-Tuple Network and Feature", "text": "In our experiments, we used (i) the tuples shown in Fig. 4 (b) and all of their rotated and mirrored tuples, and (ii) the large-format tuples, i.e. a set of features that represent all combinations of large tiles, namely -tiles, with 2048. The large-format feature for a game state is indexed by the tupel: (2048, 4096, 8, 16, 32), with the numbers 2048-tiles, 4096-tiles, 8192-tiles, 16384-tiles, and 32768-tiles, respectively. Characteristics in (ii) were used to indicate difficulties due to large tiles. In our implementation, a state is evaluated as the sum of all occurring features as Fig weighting."}, {"heading": "B. MS-TD Learning for the Simple 3-Stage Strategy", "text": "In the experiment for MS-TD learning, 5 million training games were performed at each level, and 100,000 boards were collected for the next level. All game weights of each level were set to zero. Fig. 7 (below) shows the learning curves of the average results (sampled every 100,000 games) in the three levels. The curve for level 1 is shown as Fig. 5. For a fair comparison, we used the following method to represent other curves. To illustrate, we first look at the curve for level 2. In level 2, we trained games, starting with the splitting points (16) of the 100,000 games collected from level 1. The results for games trained at level 2 included the results collected up to level 1. To compare the performance of the level 1 feature weights with level 2, we calculated the average score of the 100,000 games, the average score of the level 3 (in level 1)."}, {"heading": "C. MS-TD Learning Together with Expectimax Search", "text": "As described in subsection II.B., max nodes are the states in which players are allowed to move, and random nodes are the states in which new tiles are generated after moves. In TD learning, the learned poststate values can be used as heuristic values of leaves. Thus, selecting the maximum poststate values can be considered a one-time expectation search. Fig. 2 shows an example of double expectation search. Table 1 shows the results of running 10,000 games for the original TD learning for 1x to 3x expectation search, while Table 2 shows the results for MS-TD learning with three levels. Maximum values in the two tables are derived as follows. First, calculate the maximum score for each 100 games. Then, derive the average score of this maximum score for MS games for 1x to 3x expectation search. In addition, all the data in the two tables contain errors with 95% total intervals of learning steps divided by the total number of learning steps."}, {"heading": "D. More Splitting Strategies", "text": "In this area, we are in a position to develop strategies for the future, both for the future and for the future."}, {"heading": "E. Further Improvements", "text": "In this case, it is as if it were a mere red herring."}, {"heading": "V. EXPERIMENTS FOR THREES", "text": "The experiments for threes were similar to those for 2048. We used the same n-tuple network in Fig. 4 (b) and the features mentioned in Section IV.A, as well as some specific features for threes, such as the tip tiles. Experiments for a simple 3-stage strategy splitting at 1536 and 3072 are shown and analyzed in Section V.A., further splitting strategies and other improvements are experimented and analyzed in Section V.B."}, {"heading": "A. MS-TD Learning for a Simple 3-Stage Strategy", "text": "In our experiments for triple chains, which are similar to those for 2048, 5 million training games were performed at each level, and the averages and maximums were sampled every 100,000 games. Fig. 18 and Fig. 19 show the learning curves of averages and maximums in the three levels, respectively. The curves are presented in the same way as in Fig. IV.B. Both figures show that learning improved in Level 2 and Level 3, especially in the maximum score. In Fig. 19, the curve for MS-TD learning with the simple 3-level strategy shows more often by 250,000 than Level 1 and Level 2. Table 4 (below) shows the results of performing 10,000 games for the original TD learning along with a one- to three-fold expectation search, while Table 5 shows those for MS-TD learning with the simple 3-level strategy. For simplicity, both tables for achieving maximum scores contain only the results of large TD tiles, namely 3072-spikes and 146achles."}, {"heading": "B. More Splitting Strategies", "text": "Strategy 1 adds a further split time of \u20ac768 to the simple 3-step strategy mentioned above. Fig. 20 and Fig. 21 (below) show their learning curves for averages and maximums respectively. Strategy 2 adds a further split time of \u20ac3072 + 1536 instead. Fig. 22 and Fig. 23 show the learning curves for averages and maximums respectively. Table 6 (below) shows the experimental results of playing 10,000 games with triple expectation search for the two strategies mentioned above. Fig. 20 to Fig. 23 show the improvements in MS-TD learning for the learning curves. However, when included in the expectation search, these strategies did not fare better than the simple 3-step strategy, especially in terms of 6144-step strategies shown in Table 6. This also showed that the benefit of splitting in additional phases was lower, as observed in 2048."}, {"heading": "VI. CONCLUSIONS", "text": "This paper suggests multi-level TD learning (MS-TD), a kind of hierarchical learning method that effectively improves the performance of 2048-type games, especially for maximum values and achievement rates of large tiles. Our experiments in Sections IV and V showed significant improvements in the use of MS-TD learning for both 2048 and for threes. For 2048, when using 3x Expicitimax search, the 32768-tile attainment rate when using MS-TD learning with Strategy 3 was 18.31%, much higher than that of TD, which was 0%. After further improvements, our 2048 program reached 32768 tiles with a 31.75% probability in 10,000 games, and even reached a 65536-tile, the first ever to reach a 65536-tile of our knowledge. For Threes, when using 3x Expicitimax search, our program reached 368 tiles with a 27,736% probability and even reached a 55375-tile of our knowledge in 55375-55375."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by the NOVATEK Fellowship, the Ministry of Science and Technology of the Republic of China (Taiwan) under contract numbers MOST 104-2221-E-009-127-MY2 and 104-2221-E-009-074-MY2, and the National Center for High-Performance Computing (NCHC) for Computer Time and Equipment."}], "references": [{"title": "and W", "author": ["S. Bagheri", "M. Thill", "P. Koch"], "venue": "Konen, \u201cOnline adaptable learning rates for the game Connect-4,\u201d IEEE Trans. on Computational Intelligence and AI in Games, vol. PP, no. 99, pp. 1\u20131", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "The *-minimax search procedure for trees containing chance nodes,", "author": ["B.W. Ballard"], "venue": "Artificial Intelligence,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1983}, {"title": "Recent advances in hierarchical reinforcement learning,", "author": ["A.G. Barto", "S. Mahadevan"], "venue": "Discrete Event Dynamic Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Learning to play chess using temporal differences,", "author": ["J. Baxter", "A. Tridgell", "L. Weaver"], "venue": "Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "First results from using temporal difference learning in Shogi,", "author": ["D.F. Beal", "M.C. Smith"], "venue": "Proceedings of the First International Conference on Computers and Games (CG\u201998),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Experiments with multi-probcut and a new high-quality evaluation function for Othello,", "author": ["M. Buro"], "venue": "Proceedings of a Workshop on Game-Tree Search: Games in AI Research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Systematic n-tuple networks for position evaluation: Exceeding 90% in the Othello league,", "author": ["W. Ja\u015bkowski"], "venue": "http://arxiv.org/abs/1406.1509, Poznan University of Technology, Tech. Rep. RA-06/2014,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "An analysis of alpha-beta pruning,", "author": ["D.E. Knuth", "R.W. Moore"], "venue": "Artificial Intelligence,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1975}, {"title": "Learning to play Othello with n-tuple systems,", "author": ["S.M. Lucas"], "venue": "Australian Journal of Intelligent Information Processing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Fast Online Q(\u03bb),", "author": ["W. Marco", "S. J\u00fcrgen"], "venue": "Machine Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Optimal strategy in games with chance nodes,", "author": ["E. Melk\u00f3", "B. Nagy"], "venue": "Acta Cybernetica,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The solution for the branching factor of the alpha-beta pruning algorithm and its optimality,", "author": ["J. Pearl"], "venue": "Communications of ACM,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1982}, {"title": "Temporal difference learning applied to a high-performance game-playing program,", "author": ["J. Schaeffer", "M. Hlynka", "V. Jussila"], "venue": "Proceedings of the 17th International Joint Conference on Artificial Intelligence", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "Reinforcement learning and simulation-based search in computer Go,", "author": ["D. Silver"], "venue": "Ph.D. dissertation, Dept. Comput. Sci., Univ. Alberta,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Learning to predict by the methods of temporal differences,", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge, MA, USA: MIT Press", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "TD-Gammon, a self-teaching Backgammon program, achieves master-level play,", "author": ["G. Tesauro"], "venue": "Neural Computation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1994}, {"title": "Temporal difference learning with eligibility traces for the game Connect-4,", "author": ["M. Thill", "S. Bagheri", "P. Koch", "W. Konen"], "venue": "IEEE Conference on Computational Intelligence and Games (CIG),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Reinforcement learning with n-tuples on the game Connect-4,", "author": ["M. Thill", "P. Koch", "W. Konen"], "venue": "Proceedings of the 12th International Conference on Parallel Problem Solving from Nature - Volume Part I", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Temporal difference learning in Chinese chess,", "author": ["T.B. Trinh", "A.S. Bashi", "N. Deshpande"], "venue": "Proceedings of the 11th International Conference on Industrial and Engineering Applications of Artificial Intelligence and Expert Systems: Tasks and Methods in Applied Artificial Intelligence", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "Temporal difference learning for Connect6,", "author": ["I.-C. Wu", "H.-T. Tsai", "H.-H. Lin", "Y.-S. Lin", "C.-M. Chang", "P.-H. Lin"], "venue": "Proceedings of the 13th International Conference on Advances in Computer Games (ACG),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "Multi-stage temporal difference learning for 2048,", "author": ["I.-C. Wu", "K.-H. Yeh", "C.-C. Liang", "C.-C. Chang", "H. Chiang"], "venue": "Proceedings of the 19th International Conference on Technologies and Applications of Artificial Intelligence (TAAI),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "com/threes were alpha-beta search [10][16][19], a traditional game search method for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games.", "startOffset": 34, "endOffset": 38}, {"referenceID": 11, "context": "com/threes were alpha-beta search [10][16][19], a traditional game search method for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games.", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "com/threes were alpha-beta search [10][16][19], a traditional game search method for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games.", "startOffset": 125, "endOffset": 128}, {"referenceID": 10, "context": "com/threes were alpha-beta search [10][16][19], a traditional game search method for two-player games, and expectimax search [2][13][19], a common game search method for single-player stochastic games.", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Note that the preliminary version of this paper [32] did not include the following: MS-TD applied to Threes, more splitting strategies for both 2048 and Threes, and a demonstration of combining MS-TD with other techniques to improve 2048 and Threes programs.", "startOffset": 48, "endOffset": 52}, {"referenceID": 1, "context": "A common game tree search algorithm used in 2048 programs is expectimax search [2][13][19].", "startOffset": 79, "endOffset": 82}, {"referenceID": 10, "context": "A common game tree search algorithm used in 2048 programs is expectimax search [2][13][19].", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "Reinforcement learning is an important technique in training an agent to learn how to respond to a given environment [21].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "Temporal difference (TD) learning [20][21], a kind of reinforcement learning, is a method for adjusting state values from the subsequent evaluations.", "startOffset": 34, "endOffset": 38}, {"referenceID": 15, "context": "Temporal difference (TD) learning [20][21], a kind of reinforcement learning, is a method for adjusting state values from the subsequent evaluations.", "startOffset": 38, "endOffset": 42}, {"referenceID": 16, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 101, "endOffset": 104}, {"referenceID": 4, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 112, "endOffset": 115}, {"referenceID": 13, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 135, "endOffset": 139}, {"referenceID": 21, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 139, "endOffset": 143}, {"referenceID": 6, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 153, "endOffset": 156}, {"referenceID": 8, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 156, "endOffset": 160}, {"referenceID": 0, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 179, "endOffset": 182}, {"referenceID": 17, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 182, "endOffset": 186}, {"referenceID": 18, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 186, "endOffset": 190}, {"referenceID": 19, "context": "This method has been applied to several computer games such as Backgammon [24], Checkers [17], Chess [4], Shogi [5], Go [18], Connect6 [31][32], Othello [9][11][29], Connect four [1][25][26] and Chinese Chess [28].", "startOffset": 209, "endOffset": 213}, {"referenceID": 12, "context": ", Chinook [17], and TD-Gammon [24].", "startOffset": 10, "endOffset": 14}, {"referenceID": 16, "context": ", Chinook [17], and TD-Gammon [24].", "startOffset": 30, "endOffset": 34}, {"referenceID": 15, "context": "Note that in [21] V(st+1) is weighted by a discount factor, which is ignored in this paper for simplicity.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "For the general TD(\u03bb) as in [20][21][31], we adopted the forward view of value difference as mentioned (c.", "startOffset": 28, "endOffset": 32}, {"referenceID": 15, "context": "For the general TD(\u03bb) as in [20][21][31], we adopted the forward view of value difference as mentioned (c.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "For the general TD(\u03bb) as in [20][21][31], we adopted the forward view of value difference as mentioned (c.", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "2 of [21]):", "startOffset": 5, "endOffset": 9}, {"referenceID": 20, "context": "The function is usually modified into a linear combination of features [31] for TD learning, that is, V(s) = \u03c6(s) \u2219 \u03b8, where \u03c6(s) denotes a vector of feature occurrences in s, and \u03b8 denotes a vector of feature weights.", "startOffset": 71, "endOffset": 75}, {"referenceID": 6, "context": "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].", "startOffset": 98, "endOffset": 102}, {"referenceID": 0, "context": "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].", "startOffset": 124, "endOffset": 127}, {"referenceID": 17, "context": "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "In fact, n-tuple networks were also successfully applied to other applications such as Othello [9][11][29] and Connect four [1][25][26].", "startOffset": 131, "endOffset": 135}, {"referenceID": 2, "context": ", reaching 8192-tiles or 16384-tiles, as in hierarchical reinforcement learning (HRL) [3].", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "The concept of using different feature weights in multiple stages was also mentioned in the work [6] to evaluate game states and select different features according to different game stages, but not for reinforcement learning.", "startOffset": 97, "endOffset": 100}, {"referenceID": 14, "context": "We used the variant without eligibility traces [20][21] for the following reasons.", "startOffset": 47, "endOffset": 51}, {"referenceID": 15, "context": "We used the variant without eligibility traces [20][21] for the following reasons.", "startOffset": 51, "endOffset": 55}, {"referenceID": 9, "context": "We leave it open for better variants of TD(\u03bb) [12][28].", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "We leave it open for better variants of TD(\u03bb) [12][28].", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "Szubert and Ja\u015bkowski successfully used temporal difference (TD) learning together with n-tuple networks for playing the game 2048. However, we observed a phenomenon that the programs based on TD learning still hardly reach large tiles. In this paper, we propose multi-stage TD (MS-TD) learning, a kind of hierarchical reinforcement learning method, to effectively improve the performance for the rates of reaching large tiles, which are good metrics to analyze the strength of 2048 programs. Our experiments showed significant improvements over the one without using MS-TD learning. Namely, using 3-ply expectimax search, the program with MS-TD learning reached 32768-tiles with a rate of 18.31%, while the one with TD learning did not reach any. After further tuned, our 2048 program reached 32768-tiles with a rate of 31.75% in 10,000 games, and one among these games even reached a 65536-tile, which is the first ever reaching a 65536-tile to our knowledge. In addition, MS-TD learning method can be easily applied to other 2048-like games, such as Threes. Based on MS-TD learning, our experiments for Threes also demonstrated similar performance improvement, where the program with MS-TD learning reached 6144-tiles with a rate of 7.83%, while the one with TD learning only reached 0.45%. Keywords\u2014Stochastic Puzzle Game, 2048, Threes, Temporal Difference Learning, Expectimax.", "creator": "Microsoft\u00ae Word 2013"}}}