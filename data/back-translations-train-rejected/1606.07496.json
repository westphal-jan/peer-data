{"id": "1606.07496", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "abstract": "While textual reviews have become prominent in many recommendation-based systems, automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers. Suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. We propose a deep learning-based framework to automatically: (1) tag the images available in a review dataset, (2) generate a caption for each image that does not have one, and (3) enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer. We evaluate the proposed framework using the Yelp Challenge Dataset. While a subset of the images in this particular dataset are correctly captioned, the majority of the pictures do not have any associated text. Moreover, there is no mapping between reviews and images. Each image has a corresponding business-tag where the picture was taken, though. The overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge. Qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning, tagging, and recommendation for mapping reviews and images.", "histories": [["v1", "Thu, 23 Jun 2016 22:04:08 GMT  (4716kb,D)", "http://arxiv.org/abs/1606.07496v1", "10 pages, 11 figures, \"for associated results, see http://this http URL\" \"submitted to DLRS 2016 workshop\""]], "COMMENTS": "10 pages, 11 figures, \"for associated results, see http://this http URL\" \"submitted to DLRS 2016 workshop\"", "reviews": [], "SUBJECTS": "cs.CV cs.CL cs.IR cs.LG cs.NE", "authors": ["roberto camacho barranco", "laura m rodriguez", "rebecca urbina", "m shahriar hossain the university of texas at el paso)"], "accepted": false, "id": "1606.07496"}, "pdf": {"name": "1606.07496.pdf", "metadata": {"source": "CRF", "title": "Is a Picture Worth Ten Thousand Words in a Review Dataset?", "authors": ["Roberto Camacho", "Laura M. Rodriguez", "Rebecca Urbina", "Shahriar Hossain"], "emails": ["rcamachobarranco@utep.edu,", "rurbina5}@miners.utep.edu,", "mhossain@utep.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Information Systems \u2192 Data Mining; Document Topic Models; \u2022 Computer Methods \u2192 Neural Networks; Keywords Yelp Dataset, Review Enhancement, Recommendation Systems, Captions, Image Classification"}, {"heading": "1. INTRODUCTION", "text": "The usefulness of a review-based website (e.g. Yelp) largely depends on the quality of the digital or hard copies produced by the rePermission of all or part of this work for personal or class-specific use, provided that copies are not made or distributed for profit or commercial advantage, and that copies bear this message and the full quote on the first page. Copyrights for components of this work that belong to others other than ACM must be respected. Abstraction with the credit is permitted. Otherwise, copying or redistributing requires prior specific permission and / or a fee. Permissions @ acm.org.viewerts Permissions @ acm.org.viewerts The heterogeneous nature of these materials provides a tremendous opportunity to enhance the user experience. For example, distributing text reviews and images on lists requires prior specific permission and / or a fee."}, {"heading": "4.1 Image similarity", "text": "Measuring image similarity involves the complicated task of extracting minor features (such as color, texture, shape, etc.) Liu examines several content-based image retrieval systems (CBIR) that typically use segmentation techniques to identify the different regions within the image, and then measures the proximity between the regions in an image and another using metrics such as the Minkowsky Distance or Cosinal Coincidence. However, the problem with segmentation techniques is that they are mostly accurate when applied to nature images because they lack homogeneous colors and textures [4]. Riu and Huang present some of the most basic similarity measures that exist, including the histogram and RGB algorithms, which focus on the grayscale or color intensity of all pixels in an image. [5]"}, {"heading": "4.2 Object recognition", "text": "One of the goals we are trying to realize is to search for similar objects in an image to give better related images and tags. Unlike image similarity, which uses the attributes of photographs, object recognition is used to extract and classify objects in images; the use of object recognition to classify an image has previously been investigated by Li. [3] which system finds objects in a photo and comes up with a semantic label for them to classify what kind of synonym extraction is used. Wang and Hirst researched the extraction of synonyms and hyponyms using 3 di-erent methods and compared them with each other. Methods they used were: K-th nearest neighbor, dictionary graph, exploring patterns in dictionary definition."}, {"heading": "2. PROBLEM DESCRIPTION", "text": "Let D = {(i1, l1, c1, b1), (i2, l2, c2, b2),..., (iN, lN, cN, bN)} be an image record containing N images (i), along with the corresponding captions (l), captions (c), and business ID (b). A label can be a value from the following categories: {food, inside, outside, drink, menu, none}. An image caption should be a sentence, but can also be empty. Each image has exactly one business idea. Let R = {r1, r2,..., r | R |} be a text record containing all the ratings of a particular restaurant b. We are looking for an M mapping, so that in a review we can select r-R images (i, l, c, b), so that the image i is closely related to the r verification. To make such a mapping, we rely on a relationship between an image category and a summary that is not relevant to a frame in a main citation."}, {"heading": "3. METHODOLOGY", "text": "Our framework solves the problem of recommending images for each review in three main steps: First, we use an image classifier to predict a label for images that are categorized as none; second, we use a captioning algorithm using the image characteristics (determined in the first step) and existing captions as inputs; this generates captions for images that do not have captions; and finally, we apply theme modeling to the reviews and captions separately to create a probable mapping. The following subsections describe these steps."}, {"heading": "3.1 Image classification", "text": "The first step in our framework is to categorize each of the images that are labeled as none into one of the following categories: Food, Inside, Inside, Inside or Menu. Our preliminary data analysis shows that more than 25% of restaurant images are labeled as none. We use a Convolutional Neural1Available at: https: / / auto-captioning.herokuapp.com Network (CNN) image classifier to get class probabilities for all images in the test set (labeled as none).Convolutional Neural Network algorithms require that all images have the same dimension and are square shaped. We have resized the images so that the smallest dimension of the image is 64 or 224 pixels, and then cropped the image in the other dimension to get a 64-by-64-64-pixel or 224-by-224-pixel image."}, {"heading": "3.2 Image captioning", "text": "We use a lasagna-based implementation of the Neural Image Caption (NIC) Generator [35] to predict captions for images without captions. The NIC Generator uses a special form of a recurring neural network (RNN) called LongShort Term Memory (LSTM) network to sequentially generate a fixed-dimensional vector required by the variable length of input and output sets. LSTM networks are a special type of RNN that is able to learn long-term dependencies. LSTM networks apply weighted gates between input, output, and previous hidden states. By assigning different orders of magnitude to each gate (between 0 and 1), the information flow is modified so that the previous information is useful for the model, and if not, the model forgets the information. LSTM networks are able to automatically propagate the gates by reagation, train them with higher accuracy, and more robust."}, {"heading": "3.3 Topic modeling and review enhancement", "text": "We use Latent Dirichlet Allocation (LDA) [4] to model the themes of the reviews. For each review, we select the best topic and select the best representative terms of the topic, regardless of whether they appear in the review or not. For each review, we recommend the best images based on the presence of the representative terms in the review and in the captions of the images for which the review was written. An image is rated higher for a particular review if there is a representative term in both the caption and the review, compared to an image that contains the representative term only in the caption. Let's start by selecting images that use representative terms that exist both in the review and in the caption. If images cannot be found, we select images for which captions contain representative terms, but not in the review. This process ensures that the image selection is not determined by an overlap and not by an overlap, but by an overlap and not by an overlap."}, {"heading": "4. EVALUATION", "text": "We use different metrics to evaluate the quality of the results for each of the main components of the frame: q = q Confidence: q Confidence, caption and subject modeling. For image classification, we use the top-1 accuracy, which is the percentage of test images correctly classified as defined by Equation 1. We use only the top-1 accuracy due to the low number of possible labels. The BLEU method proposed by Panineni and al calculates the geometric mean of the n-gram precisions. Since the training set consists largely of short sentences (captions), we removed the Brevity penalty usually applied, which prevents very short sentences from having very high values."}, {"heading": "5. EXPERIMENTS", "text": "In this section, we try to answer the following questions about the proposed framework.1. How do different CNN architectures compare in terms of the accuracy of image classification? How do these results compare with a simpler method, such as SVM using feature pockets? (Section 5.1) 2. How does changing the hyperparameters of the model for the NIC generator affect the results in terms of trust and BLEU-4 score? How do different hyperparameters affect the perplexity of the resulting LDA model? (Section 5.4) 5. Can we link ratings with images based on the top words of the Yelp dataset? (Section 5.3) 4. How do different hyperparameters affect the perplexity of the resulting LDA model? (Section 5.4) 5. Can we relate ratings to the top words obtained through topic modeling? (Section 5.5) We focused on the 25,071 (of 77,445) restaurants discovered in the Yelp dataset."}, {"heading": "5.1 Image classification", "text": "We have tested different models to compare the accuracy achieved with each of them for image classification, and the architectures include: a simple CIFAR10 model (11 layers deep), VGG-16 [31] (16 layers deep), VGGG19 [31] (19 layers deep) and GoogleNet [33] (22 layers deep). We have also used the MATLABR model to divide the images into five different classes."}, {"heading": "5.2 Image captioning", "text": "The NIC generator has several hyperparameters that can affect the quality of the resulting captions: maximum sequence length (max _ seq _ len), batch size (batch _ sz), embedding size (embedding _ sz), learning rate (lr), and number of iterations (iters). To select the hyperparameters that led to the best confidence and BLEU-4 score, we created a first series of experiments that set max _ seq _ len to 6, batch _ sz to 50, and iters to 20,000. Figure 5 shows the results of embedding _ sz to {128, 256, 512, 768, 1024} and lr to {0.01, 0.001, 0.0001}. The Y axis represents the maximum value, over the iterations of a model, the addition of the median values of the confidence values for all generated records at the corpus level BLZ-4."}, {"heading": "5.3 Caption evaluation", "text": "To evaluate the quality of the captions generated, we asked five participants to manually assign a score to each caption generated for one hundred images (or less if there were repetitions); each caption was rated on a scale of 1 to 10, where 10 means that all five captions have terms associated with the image, and 1 means that no caption is related to the image. As a starting point, participants were asked to rate a caption with a score of 2 if only one of five captions contained a term related to the image; the hundred images used for this rating are the first hundred images that appear on our website2. This image subgroup includes 40 images labeled as food by our CNN predictor, 17 images labeled as beverage, 16 images as outside, 14 images as menu, and 13 images labeled as insiders."}, {"heading": "5.4 Topic modeling", "text": "The Gensim implementation [29] of the latent dirichlet allocation used in our framework has the following parameters: number of topics (n _ topics), words per topic, iterations (iters), \u03b1, \u03b7, \u0443 and \u03c40. For all experiments, we used the ten best words for each topic. As mentioned in Section 4, less perplexity is expected in a better model. In Figure 8, we show the effect of setting to {0.5, 0.6, 0.7, 2Available at: https: / / auto-captioning.herokuapp.com 0.8, 0.9, 1.0} and \u03c40 to {1, 64, 256, 1024} on the perplexity of the LDA model, while both \u03b1 and \u03b7 are set to symmetric values, n _ topics 20, and iters is 50th."}, {"heading": "5.5 Recommending images for reviews", "text": "In this section, we describe a few recommendations that our framework makes with respect to the image. In Figure 1 of Section 1, we show an example that our framework was able to recommend relevant images for a review of a burger dish. Another review, the most important current words and recommended images are in Figure 11. The review is from Yelp's contribution to the Mon Ami Gabi restaurant in Las Vegas. This example shows that the recommended compilation contains the images of the fountains of Hotel Bellagio across the street as described in the review. Summaries of five other reviews with the three best recommended images for each of these restaurants are in Table 5. The table shows that our framework is able to recommend images of main dishes as well as exterior features such as the Bellagio fountains that are relevant to the valuations.Recommended images for several thousand reviews of the Mon Ami Gabi restaurant can be found on our website3."}, {"heading": "6. RELATED WORK", "text": "To the best of our knowledge, the problem of image classification has been studied for at least half a century, with initial approaches focusing on manual extraction of textual characteristics [15]. Due to the difficulty of manual extraction of characteristics, several automatic algorithms have been developed, including histogram-based SVM classification [5], pyramid matches with sparse coding [37] and location-based linear coding [36]. Other methods use a bag-of-characteristics approach, followed by a classifier such as SVM [7]. Recently, there has been great interest in deep neural networks. In the field of image classification, an increase in relevant neural models (CNNs) has been observed."}, {"heading": "7. CONCLUSIONS", "text": "Part of the proposed methodology focuses on improving and improving existing data by providing additional information, i.e. categorizing images and predicting captions. One of the future directions of this work is to achieve further improvements by using multi-label classifications, where existing captions are used as labels. Another future goal is to develop models to track a company's performance and the mood in review and caption texts."}, {"heading": "8. REFERENCES", "text": "[1] A. Aker and R. Gaizauskas. Generating imagedescriptions using dependent Bach patterns. In Proc. 48th Annu. Meeting Assoc. Comput. Ling., pages 1250-1258. ACL, 2010. [2] L. AlSumait, D. Barbara, and C. Domeniconi. On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking. In Proc. 23rd Int. Conf. Mach. Learn., pages 113-120. ACM, 2006. [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent Dirichlet allocation. J. Mach. Learn. 3: 93-1022 P. 2003."}], "references": [{"title": "Generating image descriptions using dependency relational patterns", "author": ["A. Aker", "R. Gaizauskas"], "venue": "Proc. 48th Annu. Meeting Assoc. Comput. Ling., pages 1250\u20131258. ACL", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "On-line LDA: Adaptive topic models for mining text streams with applications to topic detection and tracking", "author": ["L. AlSumait", "D. Barbar\u00e1", "C. Domeniconi"], "venue": "In Proc. 8th IEEE Int. Conf. Data Mining,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2008}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Proc. 23rd Int. Conf. Mach. Learn., pages 113\u2013120. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Support vector machines for histogram-based image classification", "author": ["O. Chapelle", "P. Haffner", "V.N. Vapnik"], "venue": "IEEE Trans. Neural Netw.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual categorization with bags of keypoints", "author": ["G. Csurka", "C. Dance", "L. Fan", "J. Willamowski", "C. Bray"], "venue": "Workshop on Statistical Learn. in Comp. Vision, volume 1, pages 1\u20132. Prague", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "T.K. Landauer", "G.W. Furnas", "R.A. Harshman"], "venue": "JASIS, 41(6):391\u2013407", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1990}, {"title": "Imagenet: A large-scale hierarchical image  database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 248\u2013255. IEEE", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel", "E. Olson", "S. S\u00f8nderby", "D. Nouri", "D. Maturana", "M. Thoma", "E. Battenberg", "J. Kelly"], "venue": "Lasagne: First release. Zenodo: Geneva, Switzerland", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Image description using visual dependency representations", "author": ["D. Elliott", "F. Keller"], "venue": "Proc. Conf. Empirical Methods Natural Language Processing, pages 1292\u20131302. ACL", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "Proc. 11th European Conf. Comp. Vision, pages 15\u201329. Springer", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge representation for the generation of quantified natural language descriptions of vehicle traffic in image sequences", "author": ["R. Gerber", "H.-H. Nagel"], "venue": "Proc. Int. Conf. Image Processing, volume 1, pages 805\u2013808. IEEE", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1996}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18(5):602\u2013610", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2005}, {"title": "Textural features for image classification", "author": ["R.M. Haralick", "K. Shanmugam", "I. Dinstein"], "venue": "IEEE Trans. Syst. Man Cybern.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1973}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Online learning for latent Dirichlet allocation", "author": ["M. Hoffman", "F.R. Bach", "D.M. Blei"], "venue": "Advances in Neural Information Processing Systems 23, pages 856\u2013864. Curran Associates, Inc.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proc. 22nd Annu. Int. ACM SIGIR Conf. Res. Devel. Info. Retrieval, SIGIR \u201999, pages 50\u201357, New York, NY, USA", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1999}, {"title": "Widsets: A usability study of widget sharing", "author": ["K. Karvonen", "T. Kilinkaridis", "O. Immonen"], "venue": "Proc. Int. Conf. Human-Computer Interaction, volume 5727, pages 461\u2013464. Springer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T. Berg"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 35(12):2891\u20132903", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Collective generation of natural image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "A.C. Berg", "T.L. Berg", "Y. Choi"], "venue": "Proc. 50th Annu. Meeting Assoc. Comp. Ling.: Long Papers-Volume 1, pages 359\u2013368. ACL", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["P. Kuznetsova", "V. Ordonez", "T.L. Berg", "Y. Choi"], "venue": "Trans. ACL, 2(10):351\u2013362", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural autoregressive topic model", "author": ["H. Larochelle", "S. Lauly"], "venue": "Advances in Neural Information  Processing Systems 25, pages 2708\u20132716. Curran Associates, Inc.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "Composing simple image descriptions using web-scale n-grams", "author": ["S. Li", "G. Kulkarni", "T.L. Berg", "A.C. Berg", "Y. Choi"], "venue": "Proc. 15th Conf. Comp. Natural Language Learn., pages 220\u2013228. ACL", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Proc. 15th European Conf. Comp. Vision, pages 740\u2013755. Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "Proc. 13th Conf. European Chapter Assoc. Comp. Ling., pages 747\u2013756. ACL", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proc. of the 40th Annu. Meeting on Assoc. for Comp. Ling., pages 311\u2013318. ACL", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u030au\u0159ek", "P. Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Concurrent Inference of Topic Models and Distributed Vector Representations", "author": ["D. Shamanta", "S.M. Naim", "P. Saraf", "N. Ramakrishnan", "M.S. Hossain"], "venue": "pages 441\u2013457. Springer International Publishing, Cham", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "CoRR, abs/1409.1556", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["R. Socher", "A. Karpathy", "Q.V. Le", "C.D. Manning", "A.Y. Ng"], "venue": "Trans. ACL, 2:207\u2013218", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 1\u20139. IEEE", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proc. IEEE Conf. Comp. Vision and Pattern Recognition, pages 3156\u20133164. IEEE", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Locality-constrained linear coding for image classification", "author": ["J. Wang", "J. Yang", "K. Yu", "F. Lv", "T. Huang", "Y. Gong"], "venue": "In Proc. IEEE Conf. Comp. Vision and Pattern Recognition,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "Linear spatial pyramid matching using sparse coding for image classification", "author": ["J. Yang", "K. Yu", "Y. Gong", "T. Huang"], "venue": "In Proc. IEEE Conf. Comp. Vision and Pattern Recognition,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "I2T: image parsing to text description", "author": ["B.Z. Yao", "X. Yang", "L. Lin", "M.W. Lee", "S.C. Zhu"], "venue": "Proc. IEEE, 98(8):1485\u20131508", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 17, "context": "[19] show that visually prominent UI elements, such as images, play an important role in review-based decision making.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Therefore we researched more about each part of their methodology: Image similarity, object recognition and synonym extraction [7].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "Another approach that has been previously explored is that of unsupervised machine learning [5].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "The usage of object recognition to classify an image has been previously studied by Li [3] which system finds objects in a photo, and comes up with a semantic label for them, in order to classify what type of event is happening in the photo.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "A few problems with using object recognition have been classified as correspondence problems by Berg [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 1, "context": "This method blurs an object from di\u21b5erent points in its shape and gives an edge channel that can then be tested for correspondence with another image [2].", "startOffset": 150, "endOffset": 153}, {"referenceID": 0, "context": "It takes the distance of points in a subset, an image\u2019s shape context, to create a histogram that it then uses for comparison[1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 4, "context": "The third method was the most precise in simple environments whereas the dictionary graph using a machine learning algorithm worked best under context [6].", "startOffset": 151, "endOffset": 154}, {"referenceID": 0, "context": "[1] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] L.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "7 libraries based on the Theano deep-learning library [34]: Keras [6], and Lasagne [10].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "7 libraries based on the Theano deep-learning library [34]: Keras [6], and Lasagne [10].", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 50, "endOffset": 53}, {"referenceID": 7, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 116, "endOffset": 119}, {"referenceID": 29, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 128, "endOffset": 132}, {"referenceID": 29, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 141, "endOffset": 145}, {"referenceID": 31, "context": "One of these models was based on the CIFAR10 data [6] while the others were designed to work with the ImageNet data [9]: VGG-16 [31], VGG-19 [31] and GoogleNet [33].", "startOffset": 160, "endOffset": 164}, {"referenceID": 32, "context": "We leverage a Lasagne-based implementation of the Neural Image Caption (NIC) generator [35] to predict captions for images with no caption.", "startOffset": 87, "endOffset": 91}, {"referenceID": 12, "context": "model with higher accuracy [14].", "startOffset": 27, "endOffset": 31}, {"referenceID": 32, "context": "The detailed sampling method required for this inference model is described in [35].", "startOffset": 79, "endOffset": 83}, {"referenceID": 26, "context": "[28], computes the geometric mean of n-gram precisions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17]:", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "The architectures include: a simple CIFAR10 model (11-layer deep), VGG-16 [31] (16-layer deep), VGG19 [31] (19-layer deep) and GoogleNet [33] (22-layer deep).", "startOffset": 74, "endOffset": 78}, {"referenceID": 29, "context": "The architectures include: a simple CIFAR10 model (11-layer deep), VGG-16 [31] (16-layer deep), VGG19 [31] (19-layer deep) and GoogleNet [33] (22-layer deep).", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": "The architectures include: a simple CIFAR10 model (11-layer deep), VGG-16 [31] (16-layer deep), VGG19 [31] (19-layer deep) and GoogleNet [33] (22-layer deep).", "startOffset": 137, "endOffset": 141}, {"referenceID": 24, "context": "The latter value is significantly lower than that obtained using the Microsoft COCO dataset [26], 27.", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "The Gensim implementation [29] of Latent Dirichlet Allocation used in our framework has the following parameters: number of topics (n_topics), words per topic, iterations(iters), \u03b1, \u03b7, \u03ba and \u03c40.", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "The problem of image classification has been studied for at least half a century, with initial approaches focusing on manual extraction of textural features [15].", "startOffset": 157, "endOffset": 161}, {"referenceID": 3, "context": "Due to the difficulty of manual feature extraction, several automatic algorithms have been developed including histogram-based SVM classification [5], as well as pyramid matching with sparse coding [37] and locality-constrained linear coding [36].", "startOffset": 146, "endOffset": 149}, {"referenceID": 34, "context": "Due to the difficulty of manual feature extraction, several automatic algorithms have been developed including histogram-based SVM classification [5], as well as pyramid matching with sparse coding [37] and locality-constrained linear coding [36].", "startOffset": 198, "endOffset": 202}, {"referenceID": 33, "context": "Due to the difficulty of manual feature extraction, several automatic algorithms have been developed including histogram-based SVM classification [5], as well as pyramid matching with sparse coding [37] and locality-constrained linear coding [36].", "startOffset": 242, "endOffset": 246}, {"referenceID": 5, "context": "Other methods use a bag-of-features approach, followed by a classifier such as SVM [7].", "startOffset": 83, "endOffset": 86}, {"referenceID": 18, "context": "Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].", "startOffset": 53, "endOffset": 57}, {"referenceID": 29, "context": "Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 31, "context": "Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].", "startOffset": 79, "endOffset": 83}, {"referenceID": 14, "context": "Some of the most relevant CNN models include AlexNet [20], VGG [31], GoogLeNet [33] and ResNet [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 32, "context": "[35], combines a Convolutional Neural Network (CNN) with a special form of a Recurrent Neural Network (RNN) called Long Short-Term Memory (LSTM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 19, "endOffset": 27}, {"referenceID": 35, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 19, "endOffset": 27}, {"referenceID": 10, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 72, "endOffset": 84}, {"referenceID": 23, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 72, "endOffset": 84}, {"referenceID": 19, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 72, "endOffset": 84}, {"referenceID": 25, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 0, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 20, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 21, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 9, "context": "html Based systems [13, 38] or object detection combined with templates [12, 25, 21] or Language Models [27, 1, 22, 23, 11] for caption generation.", "startOffset": 104, "endOffset": 123}, {"referenceID": 30, "context": "Another alternative method for captioning and ranking of these captions is co-embedding images and text in the same vector space [32].", "startOffset": 129, "endOffset": 133}, {"referenceID": 6, "context": "Of particular interest are latent semantic indexing (LSI) [8] and probabilistic LSI (pLSI) [18], which map documents to a latent semantic space.", "startOffset": 58, "endOffset": 61}, {"referenceID": 16, "context": "Of particular interest are latent semantic indexing (LSI) [8] and probabilistic LSI (pLSI) [18], which map documents to a latent semantic space.", "startOffset": 91, "endOffset": 95}, {"referenceID": 2, "context": "Variations of this algorithm include dynamic topic modeling [3] and online LDA [2].", "startOffset": 60, "endOffset": 63}, {"referenceID": 1, "context": "Variations of this algorithm include dynamic topic modeling [3] and online LDA [2].", "startOffset": 79, "endOffset": 82}, {"referenceID": 22, "context": "Neural networks have also been used for topic modeling [24, 30].", "startOffset": 55, "endOffset": 63}, {"referenceID": 28, "context": "Neural networks have also been used for topic modeling [24, 30].", "startOffset": 55, "endOffset": 63}], "year": 2016, "abstractText": "While textual reviews have become prominent in many recommendation-based systems, automated frameworks to provide relevant visual cues against text reviews where pictures are not available is a new form of task confronted by data mining and machine learning researchers. Suggestions of pictures that are relevant to the content of a review could significantly benefit the users by increasing the effectiveness of a review. We propose a deep learning-based framework to automatically: (1) tag the images available in a review dataset, (2) generate a caption for each image that does not have one, and (3) enhance each review by recommending relevant images that might not be uploaded by the corresponding reviewer. We evaluate the proposed framework using the Yelp Challenge Dataset. While a subset of the images in this particular dataset are correctly captioned, the majority of the pictures do not have any associated text. Moreover, there is no mapping between reviews and images. Each image has a corresponding business-tag where the picture was taken, though. The overall data setting and unavailability of crucial pieces required for a mapping make the problem of recommending images for reviews a major challenge. Qualitative and quantitative evaluations indicate that our proposed framework provides high quality enhancements through automatic captioning, tagging, and recommendation for mapping reviews and images. CCS Concepts \u2022Information systems\u2192 Data mining; Document topic models; \u2022Computing methodologies\u2192 Neural networks;", "creator": "LaTeX with hyperref package"}}}