{"id": "1606.07565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Adaptability of Neural Networks on Varying Granularity IR Tasks", "abstract": "Recent work in Information Retrieval (IR) using Deep Learning models has yielded state of the art results on a variety of IR tasks. Deep neural networks (DNN) are capable of learning ideal representations of data during the training process, removing the need for independently extracting features. However, the structures of these DNNs are often tailored to perform on specific datasets. In addition, IR tasks deal with text at varying levels of granularity from single factoids to documents containing thousands of words. In this paper, we examine the role of the granularity on the performance of common state of the art DNN structures in IR.", "histories": [["v1", "Fri, 24 Jun 2016 04:40:48 GMT  (31kb)", "http://arxiv.org/abs/1606.07565v1", "4 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy"]], "COMMENTS": "4 pages, Neu-IR'16 SIGIR Workshop on Neural Information Retrieval, July 21, 2016, Pisa, Italy", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["daniel cohen", "qingyao ai", "w bruce croft"], "accepted": false, "id": "1606.07565"}, "pdf": {"name": "1606.07565.pdf", "metadata": {"source": "CRF", "title": "Adaptability of Neural Networks on Varying Granularity IR Tasks", "authors": ["Daniel Cohen", "Qingyao Ai", "W. Bruce Croft"], "emails": ["croft}@cs.umass.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 565v 1 [cs.I R] 24 Jun 20CCS Concepts \u2022 Information Systems \u2192 Retrieval Models and Ranking; Answering Questions; Document Structure; \u2022 Calculation Methods \u2192 Neural Networks; Keywords Deep Learning; Answering Questions; Ad-hoc Retrieval"}, {"heading": "1. INTRODUCTION", "text": "Conventional approaches are based on converting text into vectors consisting of lexical, semantic and syntactic features that capture the information contained in the text. This conversion depends on domain knowledge and is an independent step from the optimization process of the ranking method. As this process is separate from the loss function, potential information that negatively impacts performance can be lost. Deep learning has shown that internal representations are learned directly from the text in processing natural language and specific IR tasks that produce the state of the art. However, in-depth permission to make digital or printed copies of part or all of this work for personal or class-internal use is required, provided that copies are not made for profit or commercial advantage and that copies bear this note and full quotation on the first page. Copying rights for third-party components of this work must be respected. For any other tactical purposes the author must be converted."}, {"heading": "2. RELATED WORK", "text": "Significant improvements were achieved at each level of granularity through the introduction of different DNN structures. Convolutional Neural Networks (CNN) were used at different levels in the neural network, at the input level via word embedding demonstrated by Severyn and Musk [8], as an intermediate layer within one of Feng et al. [2], or as the penultimate stage above a recursive neural network (RNN) to provide more composite representations of the question and answer text of Tan et al. [11] Regardless of the position of the evolutionary layer, the motivation behind implementing a evolutionary layer was seen to extract the most prominent features from input to enable easier similarity comparisons in nature.Since language is sequential in nature, DSNs were shown to work extremely well for IR tasks. Wang and Nyberg show that the use of a bidirectional network-long memory is effective."}, {"heading": "3. GRANULARITY TASKS", "text": "We study the effectiveness of deep learning at three different levels of granularity. First, at the fine grain level, the query focuses on a particular word or treats a short sentence of text containing the relevant information. Second, at the medium granularity level, the query's information needs can no longer be covered by a single sentence and often requires multiple sentences to be relevant. Third, we address the coarse grain level, which we consider a complete query of documents commonly found in ad hoc retrieval tasks."}, {"heading": "3.1 Fine Granularity", "text": "The focus of this section is on the TREC-QA task. In this task, the length of individual documents is often no more than a single sentence, and the queries consist of short questions such as \"When did James Dean die?\" or \"What is Crips\" gang color. \"The relevant information in each document is one or two words that directly address the query's information needs. From a deep learning perspective, CNNs can adapt well to the fine-grained task, as they are able to identify key aspects of an input matrix. This ability has led to these networks becoming widespread in the computer vision task. The same principle can be applied at the sentence level by allowing evolutionary layers to extract the most important information about the embedding of a sentence. This approach has been used for the semantic sentence level, which coincides with Hu et al. [3]."}, {"heading": "3.2 Medium Granularity", "text": "The mean granularity level, consisting of passages, stands in stark contrast to the granularity of the preceding sentences. Instead of identifying specific words within a sentence, the passage task deals with information related to the query, which can extend over several sentences. However, relevance is not determined solely by the thematic similarity between document and query. Text in relevant passages may have only minor overlaps with the query, and conventional IR methods such as BM25 have reflected this in their performance. LSTM networks are uniquely suited for this task because of the range of relevant information on the length of candidates \"responses, as they are able to model syntactic and semantic dependencies between positions in a sequence and focus less on match than CNN."}, {"heading": "3.3 Coarse Granularity", "text": "This year it has come to be a reactionary, reactionary, reactionary, reactionary, reactionary, reactionary and reactionary party."}, {"heading": "4. CONCLUSION", "text": "We have demonstrated the effectiveness and inadequacies of common neural architectures at different levels of IR task granularity. If candidates \"responses are brief, CNNs and LSTM networks operate at the same level with differences that are due to attention methods and structural differences beyond the Convolutionary and LSTM layers. At the passage level, we show that LSTMs are capable of storing additional time information that an equivalent CNN cannot achieve. Finally, we will discuss the unique problem of ad hoc retrieval for neural networks and possible solutions to overcome these problems."}, {"heading": "5. ACKNOWLEDGMENT", "text": "This work has been supported in part by the Center for Intelligent Information Retrieval and in part by the NSF IIS-1160894. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor."}, {"heading": "6. REFERENCES", "text": "[1] C. N. dos Santos, M. Tan, B. Xiang, and B. Zhou.Attentive pooling networks. CoRR, abs / 1602.03609, 2016. [2] M. Feng, B. Xiang, M. R. Glass, L. Wang, and B. Zhou. Applying Deep Learning to Answer Selection: A Study and An Open Task. Applying Deep Learning to, aug 2015. [3] B. Hu, Z. Lu, H. Li, and Q. Chen. Convolutional Neural Network Architectures for Matching Natural Language Selection: A Study and An Open Task. [4]."}], "references": [{"title": "Applying Deep Learning to Answer Selection: A Study and An Open Task", "author": ["M. Feng", "B. Xiang", "M.R. Glass", "L. Wang", "B. Zhou"], "venue": "Applying Deep Learning to,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Convolutional neural network architectures for matching natural language sentences", "author": ["B. Hu", "Z. Lu", "H. Li", "Q. Chen"], "venue": "CoRR, abs/1503.03244,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["P.-S. Huang", "X. He", "J. Gao", "L. Deng", "A. Acero", "L. Heck"], "venue": "In Proceedings of the 22nd ACM international conference on Conference on information & knowledge management,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": "arXiv preprint arXiv:1405.4053,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Semantic modelling with long-short-term memory for information retrieval", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R. Ward"], "venue": "arXiv preprint arXiv:1412.6629,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval", "author": ["H. Palangi", "L. Deng", "Y. Shen", "J. Gao", "X. He", "J. Chen", "X. Song", "R.K. Ward"], "venue": "IEEE/ACM Trans. Audio, Speech & Language Processing,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "In SIGIR,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A latent semantic model with convolutional-pooling structure for information retrieval", "author": ["Y. Shen", "X. He", "J. Gao", "L. Deng", "G. Mesnil"], "venue": "In Proceedings of  the 23rd ACM International Conference on Conference on Information and Knowledge Management,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Learning to rank answers on large online qa collections", "author": ["M. Surdeanu", "M. Ciaramita", "H. Zaragoza"], "venue": "In ACL:HLT,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Lstm-based deep learning models for non-factoid answer selection", "author": ["M. Tan", "B. Xiang", "B. Zhou"], "venue": "CoRR, abs/1511.04108,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Monolingual and cross-lingual information retrieval models based on (bilingual) word embeddings", "author": ["I. Vuli\u0107", "M.-F. Moens"], "venue": "In Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "ABCNN: attention-based convolutional neural network for modeling sentence", "author": ["W. Yin", "H. Sch\u00fctze", "B. Xiang", "B. Zhou"], "venue": "pairs. CoRR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Statistical language models for information retrieval", "author": ["C. Zhai"], "venue": "Synthesis Lectures on Human Language Technologies,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}], "referenceMentions": [{"referenceID": 6, "context": "Convolutional neural networks (CNN) have been used at various layers in the neural net, at the input level over word embeddings demonstrated by Severyn and Moschitti [8], as an", "startOffset": 166, "endOffset": 169}, {"referenceID": 0, "context": "[2], or as a penultimate stage on top of a recurrent neural network (RNN) to provide more composite representations over the question and answer text by Tan et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[11].", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] demonstrate the use of a weakly supervised LSTM network to detemine answer sentence similarity and examine how individual cells attenuate information when processing query-answer pairs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4], DSSM uses a word hashing technique to project varied length text into fixed length vectors as the model\u2019s input and constructs a feed-", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] proposed a convolution network (CLSM) and Palangi et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] proposed a RNN-LSTM model with the same word hashing technique of DSSM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Severyn and Moschitti [8] also take advantage of the matching ability of CNNs by implementing a convolutional layer to extract the most salient features between answer and query sentences to compute similarity scores for ranking.", "startOffset": 22, "endOffset": 25}, {"referenceID": 11, "context": "[15] and Santos et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We demonstrate this on Yahoo\u2019s Webscope L4 CQA collection [10] of \u201cmanner\u201d type", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "These candidate answers are significantly longer than those found in WikiQA [2] or the TREC QA task with a mean length of 75.", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "[7] investigate the internal representation of text within a LSTM network.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] proposed a word hashing technique that aggregates n-grams of terms to produce a fixed length repre-", "startOffset": 0, "endOffset": 3}, {"referenceID": 10, "context": "For unsupervised models like WE [12] and the paragraph vector model (PV) [5], the embedding representations of documents are constructed to capture their main topics.", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "For unsupervised models like WE [12] and the paragraph vector model (PV) [5], the embedding representations of documents are constructed to capture their main topics.", "startOffset": 73, "endOffset": 76}, {"referenceID": 12, "context": "These representations lack discriminative ability at query time because we cannot distinguish the finer difference between semanticly related words and subtopics [16].", "startOffset": 162, "endOffset": 166}, {"referenceID": 10, "context": "WE [12] aggregates embeddings of words to form document representations and ranks documents according to their cosine similarities with queries.", "startOffset": 3, "endOffset": 7}, {"referenceID": 3, "context": "PV estimates a language model with paragraph vector model [5] and ranks documents according to the likelihood of queries given document models.", "startOffset": 58, "endOffset": 61}], "year": 2016, "abstractText": "Recent work in Information Retrieval (IR) using Deep Learning models has yielded state of the art results on a variety of IR tasks. Deep neural networks (DNN) are capable of learning ideal representations of data during the training process, removing the need for independently extracting features. However, the structures of these DNNs are often tailored to perform on specific datasets. In addition, IR tasks deal with text at varying levels of granularity from single factoids to documents containing thousands of words. In this paper, we examine the role of the granularity on the performance of common state of the art DNN structures in IR.", "creator": "LaTeX with hyperref package"}}}