{"id": "1606.07601", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Evaluation method of word embedding by roots and affixes", "abstract": "Word embedding has been shown to be remarkably effective in a lot of Natural Language Processing tasks. However, existing models still have a couple of limitations in interpreting the dimensions of word vector. In this paper, we provide a new approach---roots and affixes model(RAAM)---to interpret it from the intrinsic structures of natural language. Also it can be used as an evaluation measure of the quality of word embedding. We introduce the information entropy into our model and divide the dimensions into two categories, just like roots and affixes in lexical semantics. Then considering each category as a whole rather than individually. We experimented with English Wikipedia corpus. Our result show that there is a negative linear relation between the two attributes and a high positive correlation between our model and downstream semantic evaluation tasks.", "histories": [["v1", "Fri, 24 Jun 2016 08:35:01 GMT  (40kb)", "http://arxiv.org/abs/1606.07601v1", "7 pages,3 figures"]], "COMMENTS": "7 pages,3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kebin peng"], "accepted": false, "id": "1606.07601"}, "pdf": {"name": "1606.07601.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["kebinpeng@act.buaa.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 601v 1 [cs.C L] 24 Jun 2016The embedding of words has proved remarkably effective in many tasks of processing natural language. However, existing models still have some limitations in interpreting the dimensions of word vectors. In this essay, we offer a new approach - the root and affix model (RAAM) - to interpret from the intrinsic structures of natural language. In addition, it can be used as a yardstick for evaluating the quality of word embedding. We introduce the information tropy into our model and divide the dimensions into two categories, just like roots and affixes in lexical semantics. Then, we look at each category as a whole rather than separately. We experimented with the English Wikipedia corpus. Our result shows that there is a negative linear relationship between the two attributes and a high positive correlation between our model and downstream semantic assessment tasks."}, {"heading": "1 Introduction", "text": "In fact, it is so that most of us are able to outdo ourselves by outdoing ourselves. In fact, it is so that most of us are able to outdo ourselves. In the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century and in the second half of the 20th century, in the second half of the 20th century and in the second half of the 20th century, in the second half of the 20th century and in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century and in the second half of the 20th century, in the second half of the 20th century, in the second half of the 20th century and in the second half of the 20th century, in the second half of the 20th century and in the second half of the 20th century."}, {"heading": "2 Related works", "text": "In this section we will discuss some related work. Several interpretable embedding models have been developed to explain the meaning of dimensions in a word vector. They can be divided into two types of methods. Firstly, In (Tsvetkov et al., 2015) (Faruqui and Dyer, 2015), information at the word level is extracted from linguistic resources such as WordNet to construct word vectors, whose dimensions can be integer or decimal in the meantime. However, this representation cannot give meaning to all dimensions, since the length of the word vector is much longer than the attributes of a word in WordNet. Secondly, In (Murphy et al., 2012) distribution models that apply matrix factorization are introduced. In this model, word representations learned through non-negative thrift embedding can be sparse and interpretable."}, {"heading": "3 Word Vector Dimension Interpret Model", "text": "In this section we present the basic structure of our model. It consists of 3 phases. In the first phase we introduce the information entropy into our model and define two attributes for a dimension in the word vector: word entropy and sentence entropy. In this section we formally describe the model we use RAAM. The underlying hypothesis of RAAM is that the ith dimension in the word vector contributes to the ith dimension in the sentence vector or sentence vector. It is motivated for two reasons: First, sentence is a natural structure in the text rather than manual work. We guess the semantic meaning that could be reflected in this natural structure. Second, the explanation of a single dimension is problematic."}, {"heading": "4 Experiment", "text": "All codes are implemented in MATLAB. We select some state-of-the-art word vector models."}, {"heading": "4.1 Word vector models", "text": "The vector dimension is 50, 100, 150, 200, 250, 300, 350, 400 500.Skip-Gram (SG) and CBOW. The Word2Vec tool (Mikolov et al., 2013) (Mikolov et al., 2010) (Le and Mikolov, 2014) is very popular and effective in language processing in nature. This model uses Huffman code to represent the word, and then considers it as input for log classifiers. The word is predicted within a given context. Glove. GloVe is an unattended learning algorithm for obtaining word vector representations. The training is performed on the basis of aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the word vector space (Pennington al., 2014).GWlove + GDB + Paphuqui (+ the ganear), the database and the word structures (the data base)."}, {"heading": "4.2 Evaluation Benchmarks", "text": "We compare our RAAMX model with some standard semantic tasks. We will briefly introduce these tasks. Word Similarity. There are three different types of benchmarks: WS-353, MEN, SimLex-999. The WS-353 contains two sets of English word pairs along with human assigned similarity judgments. The collection can be used to train and / or test computer algorithms for implementing semantic similarity measurements (Finkelstein et al., 2001). MEN contains two sets of English word pairs (one for training and one for testing) together with human assigned similarity judgments. The collection can be used to train and / or test computer algorithms for implementing semantic similarity and kinship measurements (Bruni et al., 2014). SimLex-999 is a gold standard resource for evaluating models that learn the meaning of words and concepts (Hill et al., 2015). It can overcome the inconsistencies between S3m and S3m."}, {"heading": "5 Results", "text": "At the beginning, we show the relationship between the two hierarchies: word and sentence. From Figur1, 2, 3, we can draw a conclusion that there is a negative linear relationship between the two levels, and the linear relationship does not change between the different parameters. This conclusion supports our hypothesis that there are differences between the two hierarchies for one dimension. Later, we calculate according to (Tsvetkov et al., 2015) the Pearons correlation coefficient r between the RAAMX values and other word vector models. Our purpose is to verify the correctness between these models. First, we use the task called Senti to make a comparison. We set the vector dimensions in 300. As shown in Table 1, the Pearsons correlation between the RAAMX values and other models is r = 0.7903. Second, we show the correlation between two different train paths and RAAM waves in different datasets. From the table, we can see that the correlation between RAAM and RAAM is high."}, {"heading": "6 Conclusion", "text": "In this paper, we examine the problem of interpretable embedding and propose a new approach to interpreting the meaning of dimensions in a word vector. We design two attributes of a dimension in a word vector. According to these two attributes, we aggregate dimensions into two classes. We have experimented with several data sets. Our result suggests that the difference between two classes covers two semantic levels of a word. We believe that our approach provides valuable information and a good way to interpret the meaning of a dimension for future research."}, {"heading": "Acknowledgement", "text": "We would like to thank all anonymous reviewers for constructive feedback."}], "references": [{"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Non-distributional word vector representations. arXiv preprint arXiv:1506.05230", "author": ["Faruqui", "Dyer2015] Manaal Faruqui", "Chris Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay K Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A Smith"], "venue": "arXiv preprint arXiv:1411.4166", "citeRegEx": "Faruqui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2014}, {"title": "Placing search in context: The concept revisited", "author": ["Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Interpretable semantic vectors from a joint model of brain-and text-based meaning", "author": ["Fyshe et al.2014] Alona Fyshe", "Partha P Talukdar", "Brian Murphy", "Tom M Mitchell"], "venue": "In Proceedings of the conference. Association for Computational Linguistics. Meeting,", "citeRegEx": "Fyshe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fyshe et al\\.", "year": 2014}, {"title": "Ppdb: The paraphrase database", "author": ["Benjamin Van Durme", "Chris Callison-Burch"], "venue": "In HLT-NAACL,", "citeRegEx": "Ganitkevitch et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ganitkevitch et al\\.", "year": 2013}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Learning representations for weakly supervised natural language processing", "author": ["Huang et al.2014] Fei Huang", "Arun Ahuja", "Doug Downey", "Yi Yang", "Yuhong Guo", "Alexander Yates"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents. arXiv preprint arXiv:1405.4053", "author": ["Le", "Mikolov2014] Quoc V Le", "Tomas Mikolov"], "venue": null, "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["Chih-Jen Lin"], "venue": "Neural computation,", "citeRegEx": "Lin.,? \\Q2007\\E", "shortCiteRegEx": "Lin.", "year": 2007}, {"title": "Online learning of interpretable word embeddings", "author": ["Luo et al.2015] Hongyin Luo", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Luo et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luo et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning effective and interpretable semantic models using non-negative sparse embedding", "author": ["Murphy et al.2012] Brian Murphy", "Partha Pratim Talukdar", "Tom M Mitchell"], "venue": "In COLING,", "citeRegEx": "Murphy et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Murphy et al\\.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Alex Perelygin", "Jean Y Wu", "Jason Chuang", "Christopher D Manning", "Andrew Y Ng", "Christopher Potts"], "venue": "In Proceedings of the conference on empirical methods in natural language processing (EMNLP),", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Evaluation of word vector representations by subspace alignment", "author": ["Manaal Faruqui", "Wang Ling", "Guillaume Lample", "Chris Dyer"], "venue": null, "citeRegEx": "Tsvetkov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tsvetkov et al\\.", "year": 2015}, {"title": "Bilingual word embeddings for phrase-based machine translation", "author": ["Zou et al.2013] Will Y Zou", "Richard Socher", "Daniel M Cer", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Zou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zou et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 17, "context": "For example, bilingual word embedding for phrase-based machine translation (Zou et al., 2013), enhancing the coverage of POS tagging (Huang et al.", "startOffset": 75, "endOffset": 93}, {"referenceID": 7, "context": ", 2013), enhancing the coverage of POS tagging (Huang et al., 2014).", "startOffset": 47, "endOffset": 67}, {"referenceID": 12, "context": "Existing works such as CBOW and skip-gram in the toolbox word2vec (Mikolov et al., 2013) using large unlabeled corpora to train the model.", "startOffset": 66, "endOffset": 88}, {"referenceID": 16, "context": "There are some notable interpretation embedding model includes Subspace Alignment (Tsvetkov et al., 2015), Non-distributional Word Vector Representations (Faruqui and Dyer, 2015).", "startOffset": 82, "endOffset": 105}, {"referenceID": 13, "context": "Non-Negative Sparse Embedding (Murphy et al., 2012), Online Learning of Interpretable Word Embedding (Luo et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 10, "context": ", 2012), Online Learning of Interpretable Word Embedding (Luo et al., 2015) and so on.", "startOffset": 57, "endOffset": 75}, {"referenceID": 16, "context": "First, In (Tsvetkov et al., 2015) (Faruqui and Dyer, 2015), word level information from linguistic resources such as WordNet is extracted to construct word vectors.", "startOffset": 10, "endOffset": 33}, {"referenceID": 13, "context": "Second, In(Murphy et al., 2012), distributional models which apply matrix factorization are introduced.", "startOffset": 10, "endOffset": 31}, {"referenceID": 4, "context": "Such as citeluoonline(Fyshe et al., 2014)(Lin, 2007).", "startOffset": 21, "endOffset": 41}, {"referenceID": 9, "context": ", 2014)(Lin, 2007).", "startOffset": 7, "endOffset": 18}, {"referenceID": 12, "context": "Word2Vec tool (Mikolov et al., 2013) (Mikolov et al.", "startOffset": 14, "endOffset": 36}, {"referenceID": 11, "context": ", 2013) (Mikolov et al., 2010) (Le and Mikolov, 2014) is very popular in nature language processing and effectively.", "startOffset": 8, "endOffset": 30}, {"referenceID": 14, "context": "Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space(Pennington et al., 2014).", "startOffset": 193, "endOffset": 218}, {"referenceID": 5, "context": "We used the WordNet(WN), paraphrase database(PPDB) (Ganitkevitch et al., 2013) and (Faruqui et al.", "startOffset": 51, "endOffset": 78}, {"referenceID": 2, "context": ", 2013) and (Faruqui et al., 2014) to enrich the semantic information of the word vector.", "startOffset": 12, "endOffset": 34}, {"referenceID": 3, "context": "The collection can be utilized to train and/or test computer algorithms implementing semantic similarity measures(Finkelstein et al., 2001).", "startOffset": 113, "endOffset": 139}, {"referenceID": 0, "context": "The collection can be used to train and/or test computer algorithms implementing semantic similarity and relatedness measures(Bruni et al., 2014).", "startOffset": 125, "endOffset": 145}, {"referenceID": 6, "context": "SimLex-999 is a gold standard resource for the evaluation of models that learn the meaning of words and concepts(Hill et al., 2015).", "startOffset": 112, "endOffset": 131}, {"referenceID": 16, "context": "It can overcome the shortcomings of WS353 and contains 999 pairs of adjectives, nouns and verbs(Tsvetkov et al., 2015).", "startOffset": 95, "endOffset": 118}, {"referenceID": 15, "context": "Senti(Socher et al., 2013) is a classification task which between positive and negative movie reviews.", "startOffset": 5, "endOffset": 26}, {"referenceID": 16, "context": "Later, according to (Tsvetkov et al., 2015), we compute the Pearons correlation coefficient r between RAAMXs scores and other word vector models.", "startOffset": 20, "endOffset": 43}], "year": 2016, "abstractText": "Word embedding has been shown to be remarkably effective in a lot of Natural Language Processing tasks. However, existing models still have a couple of limitations in interpreting the dimensions of word vector. In this paper, we provide a new approach\u2014roots and affixes model(RAAM)\u2014to interpret it from the intrinsic structures of natural language. Also it can be used as an evaluation measure of the quality of word embedding. We introduce the information entropy into our model and divide the dimensions into two categories, just like roots and affixes in lexical semantics. Then considering each category as a whole rather than individually. We experimented with English Wikipedia corpus. Our result show that there is a negative linear relation between the two attributes and a high positive correlation between our model and downstream semantic evaluation tasks.", "creator": "LaTeX with hyperref package"}}}