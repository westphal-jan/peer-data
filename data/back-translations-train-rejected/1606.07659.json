{"id": "1606.07659", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Hybrid Recommender System based on Autoencoders", "abstract": "A standard model for Recommender Systems is the Matrix Completion setting: given partially known matrix of ratings given by users (rows) to items (columns), infer the unknown ratings. In the last decades, few attempts where done to handle that objective with Neural Networks, but recently an architecture based on Autoencoders proved to be a promising approach. In current paper, we enhanced that architecture (i) by using a loss function adapted to input data with missing values, and (ii) by incorporating side information. The experiments demonstrate that while side information only slightly improve the test error averaged on all users/items, it has more impact on cold users/items.", "histories": [["v1", "Fri, 24 Jun 2016 12:37:04 GMT  (39kb)", "http://arxiv.org/abs/1606.07659v1", "arXiv admin note: substantial text overlap witharXiv:1603.00806"], ["v2", "Fri, 2 Dec 2016 15:41:21 GMT  (36kb)", "http://arxiv.org/abs/1606.07659v2", "arXiv admin note: substantial text overlap witharXiv:1603.00806"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1603.00806", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["florian strub", "romaric gaudel", "j\\'er\\'emie mary"], "accepted": false, "id": "1606.07659"}, "pdf": {"name": "1606.07659.pdf", "metadata": {"source": "CRF", "title": "Hybrid Recommender System based on Autoencoders", "authors": ["Florian Strub", "Romaric Gaudel"], "emails": ["florian.strub@inria.fr", "jeremie.mary@inria.fr", "romaric.gaudel@inria.fr"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 659v 1 [cs.L G] 24 Jun 2016"}, {"heading": "1. INTRODUCTION", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "2. PRELIMINARIES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Matrix Completion", "text": "A default setting for CF is matrix completion [10]. Considering N users and M items, the rating rij is the rating given by the ith user for the jth item. It contains a matrix of ratings R, RN, M, for which only a few entries are known. The aim of the matrix completion is to infer the unknown value. Namely, the algorithm returns a matrix R, RN, M, which hopefully minimizes the reconstruction error L (R, R) = \u0445 (i, j) / \u0394K (R) (rij \u2212 r, ij) 2, where K (R) is the set of indices of known ratings by R."}, {"heading": "2.2 Denoising Autoencoders", "text": "The proposed approach is based on autoencoders, which are forward-facing neural networks popularized by Kramer [11]. They are unattended networks in which the output of the network is aimed at reconstructing the original input, and the network is trained by repropagating the square error loss during reconstruction. If the network is limited to a hidden layer, its output is given by nn (x) def = \u03c3 (W1- + b1) + b2), with x-RN the input, W1- Rk \u00b7 N and W2- R \u00b7 N \u00b7 k the weight matrices, b1- Rk and b2- R \u2212 N the preset vectors, and vice versa (.) a non-linear transmission function. The size k \u2011 N of the hidden layer is also known as the bottleneck of the autoencoder."}, {"heading": "2.3 Related Work", "text": "While deep learning has tremendous success in image and speech recognition [13], scant data has received less attention and remains a challenging problem for neural networks. Neural networks are nevertheless able to detect nonlinear latent variables with heterogeneous data [13], making them a promising tool for CF. [28, 30, 5] In our case, we train autoencoders directly to provide the best ratings. These methods generally provide excellent results. However, the cold start initialization problem is ignored. For example, AutoRec [28] replaces unpredictable ratings with an arbitrarily selected score. In our case, we apply a training loss designed for sparse rating inputs, and we integrate page information to reduce the cold start effects. Other contributions deal with this cold start problem by replacing neural networks with an arbitrarily selected code, and we integrate the page information to reduce the cold start effects."}, {"heading": "2.4 Notation", "text": "In the rest of the work we use the following notations: \u2022 ui, vj are the partially known rows / columns of R; \u2022 u, i, v, j are corrupt versions of ui, vj; \u2022 u, i, v, j are rows / columns of R, which is an estimate of each entry of R."}, {"heading": "3. AUTOENCODERS AND COLLABORATIVE FILTERING", "text": "For this we have to define two types of autoencoders: \u2022 U-CFN is defined as u-i = nn (ui), \u2022 V-CFN is defined as v-j = nn (vj). Note that CF is one of the few applications that derives missing values and does not only have to compress the available information."}, {"heading": "3.1 Sparse Inputs", "text": "There is no standard approach to the use of sparse vectors as neural mesh inputs. Most of the work dealing with sparse inputs circumvents itself by predicting an estimate of the missing values [32, 2]. In our case, we want the autoencoder to handle this problem of prediction itself. Such problems have already been investigated in the industry [22], where 5% of the values are missing. However, in cooperation with autoencoders, we are often confronted with datasets with more than 95% missing values. Furthermore, missing values are not known during the training in collaborative filtering, which makes the task even more difficult. Our approach includes three components for handling the training of sparse autoencoders: \u2022 Inhibition of the edges of the input coatings by eroding the values in the input \u2022 Inhibition of the edges of the output coatings by eroding the output coatings by using backheat-propagated values \u2022 Denoizing loss to emphasize the evaluation prediction over the restructuring."}, {"heading": "3.2 Integrating Side Information", "text": "We will show that this information helps in several ways: increasing predictive accuracy, accelerating training, increasing the robustness of the model, etc. Instead of adding only the page information to the first layer of the auto encoder, we propose to inject this information into each layer of the network inputs: If very little information about a user / object is available, Collaborative Filtering will have difficulty deriving its ratings. Instead of adding only the page information to the first layer of the auto encoder, we propose to inject this information into each layer of the network inputs. As an example, the U-CFN model will have difficulty deriving its ratings ({ui, zi}) = \u03c3 (W \u00b2 2 {\u043c (W \u00b2 1 {ui, zi} + b1), zi} + b2), where zi \u00b2 R P is the vector of the page information, W \u00b2 R (N \u00d7 N} (W \u00b2) and P are the two layers of each of the x1 information."}, {"heading": "P \u226a k \u226a N and Q \u226a k \u226a M.", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Benchmark Models", "text": "We designate CFN using two matrix factorization techniques widely used in the industry. Alternately, smallest squares with weighted \u03bb regularization (ALS-WR) [39] are used to solve a low matrix factorization problem by alternatively fixing U and V and solving the resulting linear problem. Experiments are performed using the Apache Mahout software. SVDFeature [4] is a machine learning toolkit for label-based cooperative filtering. It won the KDD Cup for two consecutive years. The evaluations are based on the following equation: r = N + P (u) p + M + Q \u00b2 qyqb (v) q + Q \u00b2 R \u00b2 rzrb (g) r + (N + P \u00b2 pxpup) T (M + Q \u00b2 qyqvq), where b (u) p + P (i) RM + Q (b), b \u00b2 R + K are the side of the model information."}, {"heading": "4.2 Data", "text": "The MovieLens-1M, MovieLens-10M and MovieLens-20M records each provide 1 / 10 / 20 million discrete ratings from 6 / 72 / 138 thousand users of 4 / 10 / 27 thousand movies. The ancillary information for MovieLens-1M is the age, gender and gender of the user and the movie category (action, thriller, etc.). The ancillary information for MovieLens-10 / 20M is a matrix of tags T in which Tij is the occurrence of the jth tag for the ith movie and the movie category. No ancillary information is provided to the user. The Douban Dataset [21] provides 17 million discrete ratings from 129 thousand users on 58 thousand movies. Page information is the bi-directional user / friend relationship for the user. The user / friend relationship is treated like the matrix of MovieLens tags."}, {"heading": "4.3 Error Function", "text": "The algorithms are compared on the basis of their respective Root Mean Square Error (RMSE) on the basis of test data. DenotingRtest is the matrix of test ratings and R \u0442 is the complete matrix returned by the learning algorithm, which is RMSE: RMSE (R, Rtest) = \u221a 1 | K (Rtest) | \u2211 (i, j) \u0435K (Rtest, ij \u2212 r, ij) 2, where | K (Rtest) | is the number of ratings in the test data set. Note that in the case of auto-encoders for fair comparison, the calculation is made from feeding the network with training data. As such, r-ij stands for nn (utrain, i) j for U-CFN and nn (vtrain, j) i for V-CFN."}, {"heading": "4.4 Training Settings", "text": "We train 2-layer autoencoders for MovieLens-1 / 10 / 20M and the Douban data sets. The layers have between 500 and 700 hidden neurons. The weights are initialized according to the fan-in rule [14]: Wij \u0445 U [\u2212 1 \u221a n, 1 \u221a n]. Transmission functions are hy-perbolic tangents. The neural network is optimized by stochastic backpropagation with minibatches of size 30 and a3. The number of eigenvalues is chosen arbitrarily. We do not focus on optimizing the quality of this representation. To regulate the weight decay is added. Hyperparameters 4 are tuned by a genetic algorithm already used by [31] in another context."}, {"heading": "4.5 General Results", "text": "Table 1 summarizes the RMSE on MovieLens and Douban datasets, with confidence intervals in the range of 95%. V-CFNs perform excellently in our experiments for each set of data we perform. They are competitive compared to the state-of-the-art cooperative filtering algorithms and significantly exceed them for MovieLens-10M. To our knowledge, the best published result for MovieLens10M (excluding ancillary information) is reported with [18] and [3] definitive RMSE of 0.7682 and 0.7769, respectively. However, these two methods require a recalculation of the full matrix for each new evaluation. CFN has the critical advantage of offering similar performance while being able to refute its prediction in advance for new evaluations. More generally, we are unaware that current work that both reach the state of the art, while successfully integrating ancillary information, produces results."}, {"heading": "4.6 Impact of Side Information", "text": "At first glance, the use of page information has only a limited impact on the RMSE. This statement needs to be tempered: since the repartition of known entries in the data set is not uniform, the estimates tend to include users and articles with high ratings. For these users and movies, the data set already contains a lot of information, which means additional information will have a marginal effect. Users and articles with low ratings should benefit more from some page information, but the estimate hides it. In order to show the usefulness of page information, we report to the RMSE in Table 2, subject to the number of missing values for items. As expected, the lower number of reviews for an item is all the more important the page information. A more careful analysis of the RMSE improvement in this environment shows that the improvement is distributed evenly across users, regardless of the number of ratings. This corresponds to the fact that the available page information consists only of items. This is very desirable for a real system: the use of the hyperparameters O of a page 13."}, {"heading": "5. REMARKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Source code", "text": "Torch is a powerful framework written in Lua to quickly prototype neural networks. It is a widely used (Facebook, Deep Mind) industry standard. However, Torch lacks some important basic tools to deal with sparse input. Therefore, we are developing several new modules to deal with DAE losses, sparse DAE losses and sparse input on both the CPU and the GPU. They can easily be plugged into existing code. An out-of-the-box tutorial is available for direct execution of the experiments. The code is available free of charge on Github and Luarocks5."}, {"heading": "5.2 Scalability", "text": "A major problem that most collaborative filtering needs to solve is scalability, as datasets often have hundreds of thousands of users and elements, and an efficient algorithm needs to be trained in a reasonable amount of time and provide rapid feedback during the evaluation period. [27, 28] Recent advances in GPU computation have managed to reduce the training time of neural networks by several orders of magnitude. However, this approach can lead to important synchronization latencies. In our case, we solve the problem by selectively condensing the input before sending it to the GPU cores without modifying the result of the computational complexity. It leads to an overhead on the computational complexity, but this implementation allows the GPUs to use their full power."}, {"heading": "6. CONCLUSION", "text": "Unlike other experiments with neural networks, this common network integrates page information and learns a nonlinear representation of users or objects in a unique neural network. This approach surpasses both the state of the art in CF and the cold start problem in MovieLens and Douban datasets. CFN is also scalable and robust in handling large datasets. We have made several assertions that autoencoders are closely linked to the low-level matrix factorization in Collaborative Filtering. Finally, a reusable source code is provided in Torch and hyperparameters are provided to reproduce the results."}, {"heading": "7. REFERENCES", "text": "[1] R. P. Adams and G. E. I. Murray. Incorporating side information in probabilistic matrix factorization with gaussian processes. arXiv pre-print arXiv: 1003.4944, 2010. [2] C. M. Bishop. Neural networks for pattern recognition. Oxford univ. press, 1995. [3] C. Chen, D. Li, Y. Zhao, Q. Lv, and L. Shang. Wemarec: Accurate and scalable commendation through weighed and ensemble matrix approximation. In Proc. of the International ACM SIGIR Conference on Research and Development in InformationRetrieval, pp. 303-312. ACM, 2015. T. Chen, W. Zhang, Q. Lu, K. Chen, Z. Zheng, and Y. Yong. Svdfeature: a toolkit for feature-based collaborative filtering."}], "references": [{"title": "Incorporating side information in probabilistic matrix factorization with gaussian processes", "author": ["R.P. Adams", "G.E.D.I. Murray"], "venue": "arXiv preprint arXiv:1003.4944", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": "Oxford univ. press", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Wemarec: Accurate and scalable recommendation through weighted and ensemble matrix approximation", "author": ["C. Chen", "D. Li", "Y. Zhao", "Q. Lv", "L. Shang"], "venue": "Proc. of the International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 303\u2013312. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Svdfeature: a toolkit for feature-based collaborative filtering", "author": ["T. Chen", "W. Zhang", "Q. Lu", "K. Chen", "Z. Zheng", "Y. Yong"], "venue": "JMLR, 13(1):3619\u20133622", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Neural network matrix factorization", "author": ["G. Dziugaite", "D. Roy"], "venue": "arXiv preprint arXiv:1511.06443", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "Proc. of AISTATS\u201910, pages 249\u2013256", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proc. of AISTATS\u201911, pages 315\u2013323", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "The netflix recommender system: Algorithms", "author": ["C. Gomez-Uribe", "N. Hunt"], "venue": "business value, and innovation. ACM Trans. Manage. Inf. Syst., 6(4):13:1\u201313:19", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Scalable variational bayesian matrix factorization with side information", "author": ["Y.-D. Kim", "S. Choi"], "venue": "Proc. of AISTATS\u201914, Reykjavik, Iceland", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R. Bell", "C. Volinsky"], "venue": "Computer, (8):30\u201337", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Nonlinear principal component analysis using autoassociative neural networks", "author": ["M.A. Kramer"], "venue": "AIChE journal, 37(2):233\u2013243", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1991}, {"title": "Social popularity based svd++ recommender system", "author": ["R. Kumar", "B.K. Verma", "S.S. Rastogi"], "venue": "International Journal of Computer Applications, 87(14)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, 521(7553):436\u2013444", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient backprop", "author": ["Y. LeCun", "L. Bottou", "G. Orr", "K. Muller"], "venue": "Neural networks: Tricks of the trade, pages 9\u201348. Springer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "Efficient sparse coding algorithms", "author": ["H. Lee", "A. Battle", "R. Raina", "A. Ng"], "venue": "Advances in neural information processing systems, pages 801\u2013808", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Local low-rank matrix approximation", "author": ["J. Lee", "S. Kim", "G. Lebanon", "Y. Singerm"], "venue": "Proc. of ICML\u201913, pages 82\u201390", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "A comparative study of collaborative filtering algorithms", "author": ["J. Lee", "M. Sun", "G. Lebanon"], "venue": "arXiv preprint arXiv:1205.3193", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Low-rank matrix approximation with stability", "author": ["D. Li", "C. Chen", "Q. Lv", "J. Yan", "L. Shang", "S. Chu"], "venue": "Proc. of ICML\u201916", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep collaborative filtering via marginalized denoising auto-encoder", "author": ["S. Li", "J. Kawale", "Y. Fu"], "venue": "Proc. of CIKM\u201915, pages 811\u2013820. ACM", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Content-based recommender systems: State of the art and trends", "author": ["P. Lops", "M.D. Gemmis", "G. Semeraro"], "venue": "Recommender systems handbook, pages 73\u2013105. Springer", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Recommender systems with social regularization", "author": ["H. Ma", "D. Zhou", "C. Liu", "M.R. Lyu", "I. King"], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining, WSDM \u201911, pages 287\u2013296, Hong Kong, China", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Reconstructing Missing Data in State Estimation With Autoencoders", "author": ["V. Miranda", "J. Krstulovic", "H. Keko", "C. Moreira", "J. Pereira"], "venue": "IEEE Transactions on Power Systems, 27(2):604\u2013611", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Probabilistic matrix factorization", "author": ["A. Mnih", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, pages 1257\u20131264", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Bayesian matrix factorization with side information and  dirichlet process mixtures", "author": ["I. Porteous", "M.W.A.U. Asuncion"], "venue": "Proc. of AAAI\u201910", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "Proc. of ICDM\u201910, pages 995\u20131000", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": "Proc. of ICML\u201908, pages 880\u2013887. ACM", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proc. of ICML\u201907, pages 791\u2013798. ACM", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2007}, {"title": "Autorec: Autoencoders meet collaborative filtering", "author": ["S. Sedhain", "A.K. Menon", "S. Sanner", "L. Xie"], "venue": "Proc. of Int. Conf. on World Wide Web Companion, pages 111\u2013112", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsk", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs", "author": ["F. Strub", "J. Mary"], "venue": "NIPS Workshop on Machine Learning for eCommerce, Montreal, Canada", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Active learning in regression", "author": ["O. Teytaud", "S. Gelly", "J. Mary"], "venue": "with application to stochastic dynamic programming. In A. International Conference On Informatics in Control and Robotics, editors, ICINCO and CAP, pages 373\u2013386", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "Training Neural Networks with Deficient Data", "author": ["V. Tresp", "S. Ahmad", "R. Neuneier"], "venue": "Advances in Neural Information Processing Systems 6, pages 128\u2013135", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1994}, {"title": "Ordinal boltzmann machines for collaborative filtering", "author": ["T.T. Truyen", "D. Phung", "S. Venkatesh"], "venue": "Proc. of UAI\u201909, pages 548\u2013556. AUAI Press", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Deep content-based music recommendation", "author": ["A. Van den Oord", "S. Dieleman", "B. Schrauwen"], "venue": "In Proc. of NIPS\u201913,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P. Manzagol"], "venue": "Jour. of Mach. Learn. Res., 11(3):3371\u20133408", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Collaborative deep learning for recommender systems", "author": ["H. Wang", "N. Wang", "D.Y. Yeung"], "venue": "arXiv preprint arXiv:1409.2944", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving content-based and hybrid music recommendation using deep learning", "author": ["H. Wang", "N. Wang", "D.Y. Yeung"], "venue": "Proceedings of the ACM International Conference on Multimedia, pages 627\u2013636. ACM", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative denoising auto-encoders for top-n recommender systems", "author": ["Y. Wu", "C. DuBois", "A. Zheng", "M. Ester"], "venue": "Proc. of WSDM\u201916, pages 153\u2013162. ACM", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Large-scale parallel collaborative filtering for the netflix prize", "author": ["Y. Zhou", "D. Wilkinson", "R. Schreiber", "R. Pan"], "venue": "Algorithmic Aspects in Information and Management, pages 337\u2013348. Springer", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2008}, {"title": "Representation learning via semi-supervised autoencoder for multi-task learning", "author": ["F. Zhuang", "D. Luo", "X. Jin", "H. Xiong", "P. Luo", "Q. He"], "venue": "Proc. of ICDM\u201915", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "For instance, 80% of movies watched on Netflix come from the recommender system of the company [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 19, "context": "DOI: on items to well perform [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent [10] or Regularized Alternating Least Square algorithm [39].", "startOffset": 131, "endOffset": 135}, {"referenceID": 38, "context": "Common latent factor techniques compute a low-rank rating matrix by applying Singular Value Decomposition through gradient descent [10] or Regularized Alternating Least Square algorithm [39].", "startOffset": 186, "endOffset": 190}, {"referenceID": 24, "context": "Newer algorithms were explored to face those constraints such as Factorization Machines [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "More recent works combine several low-rank matrices such as Local Low Rank Matrix Approximation [16] or WEMAREC [3] to enhance the recommendation.", "startOffset": 96, "endOffset": 100}, {"referenceID": 2, "context": "More recent works combine several low-rank matrices such as Local Low Rank Matrix Approximation [16] or WEMAREC [3] to enhance the recommendation.", "startOffset": 112, "endOffset": 115}, {"referenceID": 0, "context": "A successful approach [1, 24] extends the Bayesian Probabilistic Matrix Factorization Framework [26] to integrate side information.", "startOffset": 22, "endOffset": 29}, {"referenceID": 23, "context": "A successful approach [1, 24] extends the Bayesian Probabilistic Matrix Factorization Framework [26] to integrate side information.", "startOffset": 22, "endOffset": 29}, {"referenceID": 25, "context": "A successful approach [1, 24] extends the Bayesian Probabilistic Matrix Factorization Framework [26] to integrate side information.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "However, recent algorithms outperform them in the general case [17].", "startOffset": 63, "endOffset": 67}, {"referenceID": 34, "context": "In this paper we introduce a CF approach based on Stacked Denoising Autoencoders [35, 40] which tackles both challenges: learning a non-linear representation of users and items, and alleviating the cold start problem by integrating side information.", "startOffset": 81, "endOffset": 89}, {"referenceID": 39, "context": "In this paper we introduce a CF approach based on Stacked Denoising Autoencoders [35, 40] which tackles both challenges: learning a non-linear representation of users and items, and alleviating the cold start problem by integrating side information.", "startOffset": 81, "endOffset": 89}, {"referenceID": 26, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 27, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 29, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 4, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 37, "context": "Compared to previous attempts in that direction [27, 28, 30, 5, 38], our framework integrates the sparse matrix of ratings and side information in a unique Network.", "startOffset": 48, "endOffset": 67}, {"referenceID": 9, "context": "A standard setting for CF is Matrix Completion [10].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "The proposed approach builds upon Autoencoders which are feed-forward Neural Networks popularized by Kramer [11].", "startOffset": 108, "endOffset": 112}, {"referenceID": 5, "context": "Recent work in Deep Learning advocates to stack pretrained encoders to initialize Deep Neural Networks [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 34, "context": "[35] tackle this issue by corrupting inputs, pushing the Network to denoise the final outputs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "In a preliminary work, [27] tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow [33].", "startOffset": 23, "endOffset": 27}, {"referenceID": 32, "context": "In a preliminary work, [27] tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow [33].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "While Deep Learning has tremendous success in image and speech recognition [13], sparse data has received less attention and remains a challenging problem for Neural Networks.", "startOffset": 75, "endOffset": 79}, {"referenceID": 12, "context": "Nevertheless, Neural Networks are able to discover nonlinear latent variables with heterogeneous data [13] which makes them a promising tool for CF.", "startOffset": 102, "endOffset": 106}, {"referenceID": 27, "context": "[28, 30, 5] directly train Autoencoders to provide the best predicted ratings.", "startOffset": 0, "endOffset": 11}, {"referenceID": 29, "context": "[28, 30, 5] directly train Autoencoders to provide the best predicted ratings.", "startOffset": 0, "endOffset": 11}, {"referenceID": 4, "context": "[28, 30, 5] directly train Autoencoders to provide the best predicted ratings.", "startOffset": 0, "endOffset": 11}, {"referenceID": 27, "context": "For instance, AutoRec [28] replaces unpredictable ratings by an arbitrary selected score.", "startOffset": 22, "endOffset": 26}, {"referenceID": 22, "context": "Other contributions deal with this cold start problem by using Neural Networks properties for CBF: Neural Networks are first trained to learn a feature representation from the item which is then processed by a CF approach such as Probabilistic Matrix Factorization [23] to provide the final rating.", "startOffset": 265, "endOffset": 269}, {"referenceID": 6, "context": "For instance, [7, 36] respectively auto-encode bag-of-words from restaurant reviews and movie plots, [19] auto-encode heterogeneous side information from users and items.", "startOffset": 14, "endOffset": 21}, {"referenceID": 35, "context": "For instance, [7, 36] respectively auto-encode bag-of-words from restaurant reviews and movie plots, [19] auto-encode heterogeneous side information from users and items.", "startOffset": 14, "endOffset": 21}, {"referenceID": 18, "context": "For instance, [7, 36] respectively auto-encode bag-of-words from restaurant reviews and movie plots, [19] auto-encode heterogeneous side information from users and items.", "startOffset": 101, "endOffset": 105}, {"referenceID": 33, "context": "Finally, [34, 37] use Convolutional Networks on music samples.", "startOffset": 9, "endOffset": 17}, {"referenceID": 36, "context": "Finally, [34, 37] use Convolutional Networks on music samples.", "startOffset": 9, "endOffset": 17}, {"referenceID": 31, "context": "Most of the papers dealing with sparse inputs get around by pre-computing an estimate of the missing values [32, 2].", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "Most of the papers dealing with sparse inputs get around by pre-computing an estimate of the missing values [32, 2].", "startOffset": 108, "endOffset": 115}, {"referenceID": 21, "context": "Such problems have already been studied in industry [22] where 5% of the values are missing.", "startOffset": 52, "endOffset": 56}, {"referenceID": 26, "context": "This operation is equivalent to removing the neurons with missing values described in [27, 28].", "startOffset": 86, "endOffset": 94}, {"referenceID": 27, "context": "This operation is equivalent to removing the neurons with missing values described in [27, 28].", "startOffset": 86, "endOffset": 94}, {"referenceID": 14, "context": "Importantly, Autoencoders with sparse inputs differs from sparseAutoencoders [15] or Dropout regularization [29] in the sense that Sparse Autoencoders and Droupout inhibit the hidden neurons for regularization purpose.", "startOffset": 77, "endOffset": 81}, {"referenceID": 28, "context": "Importantly, Autoencoders with sparse inputs differs from sparseAutoencoders [15] or Dropout regularization [29] in the sense that Sparse Autoencoders and Droupout inhibit the hidden neurons for regularization purpose.", "startOffset": 108, "endOffset": 112}, {"referenceID": 38, "context": "Alternating Least Squares with Weighted-\u03bb-Regularization (ALS-WR) [39] solves a low-rank matrix factorization problem by alternatively fixing U and V and solving the resulting linear problem.", "startOffset": 66, "endOffset": 70}, {"referenceID": 3, "context": "SVDFeature [4] is a Machine Learning Toolkit for featurebased Collaborative Filtering.", "startOffset": 11, "endOffset": 14}, {"referenceID": 20, "context": "The Douban dataset [21] provides 17 million discrete ratings from 129 thousands users on 58 thousands movies.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "rule [14]: Wij \u223c U [ \u2212 1 \u221a n , 1 \u221a n ] .", "startOffset": 5, "endOffset": 9}, {"referenceID": 30, "context": "Hyperparameters are tuned by a genetic algorithm already used by [31] in a different context.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "To the best of our knowledge, the best result published regarding MovieLens10M (without side information) are reported by [18] and [3] with a final RMSE of respectively 0.", "startOffset": 122, "endOffset": 126}, {"referenceID": 2, "context": "To the best of our knowledge, the best result published regarding MovieLens10M (without side information) are reported by [18] and [3] with a final RMSE of respectively 0.", "startOffset": 131, "endOffset": 134}, {"referenceID": 8, "context": "For instance, [9, 12] reports a global RMSE above 0.", "startOffset": 14, "endOffset": 21}, {"referenceID": 11, "context": "For instance, [9, 12] reports a global RMSE above 0.", "startOffset": 14, "endOffset": 21}, {"referenceID": 26, "context": "[27, 28] face this sparsity constraint by building small dense Networks with shared weights.", "startOffset": 0, "endOffset": 8}, {"referenceID": 27, "context": "[27, 28] face this sparsity constraint by building small dense Networks with shared weights.", "startOffset": 0, "endOffset": 8}], "year": 2016, "abstractText": "A standard model for Recommender Systems is the Matrix Completion setting: given partially known matrix of ratings given by users (rows) to items (columns), infer the unknown ratings. In the last decades, few attempts where done to handle that objective with Neural Networks, but recently an architecture based on Autoencoders proved to be a promising approach. In current paper, we enhanced that architecture (i) by using a loss function adapted to input data with missing values, and (ii) by incorporating side information. The experiments demonstrate that while side information only slightly improve the test error averaged on all users/items, it has more impact on cold users/items.", "creator": "LaTeX with hyperref package"}}}