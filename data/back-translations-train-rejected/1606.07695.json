{"id": "1606.07695", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Fully DNN-based Multi-label regression for audio tagging", "abstract": "Acoustic event detection for content analysis in most cases relies on lots of labeled data. However, manually annotating data is a time-consuming task, which thus makes few annotated resources available so far. Unlike audio event detection, automatic audio tagging, a multi-label acoustic event classification task, only relies on weakly labeled data. This is highly desirable to some practical applications using audio analysis. In this paper we propose to use a fully deep neural network (DNN) framework to handle the multi-label classification task in a regression way. Considering that only chunk-level rather than frame-level labels are available, the whole or almost whole frames of the chunk were fed into the DNN to perform a multi-label regression for the expected tags. The fully DNN, which is regarded as an encoding function, can well map the audio features sequence to a multi-tag vector. A deep pyramid structure was also designed to extract more robust high-level features related to the target tags. Further improved methods were adopted, such as the Dropout and background noise aware training, to enhance its generalization capability for new audio recordings in mismatched environments. Compared with the conventional Gaussian Mixture Model (GMM) and support vector machine (SVM) methods, the proposed fully DNN-based method could well utilize the long-term temporal information with the whole chunk as the input. The results show that our approach obtained a 15% relative improvement compared with the official GMM-based method of DCASE 2016 challenge.", "histories": [["v1", "Fri, 24 Jun 2016 14:17:34 GMT  (92kb,D)", "http://arxiv.org/abs/1606.07695v1", "Submitted to DCASE2016 Workshop which is as a satellite event to the 2016 European Signal Processing Conference (EUSIPCO)"], ["v2", "Sat, 13 Aug 2016 10:39:29 GMT  (92kb,D)", "http://arxiv.org/abs/1606.07695v2", "Submitted to DCASE2016 Workshop which is as a satellite event to the 2016 European Signal Processing Conference (EUSIPCO)"]], "COMMENTS": "Submitted to DCASE2016 Workshop which is as a satellite event to the 2016 European Signal Processing Conference (EUSIPCO)", "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["yong xu", "qiang huang", "wenwu wang", "philip j b jackson", "mark d plumbley"], "accepted": false, "id": "1606.07695"}, "pdf": {"name": "1606.07695.pdf", "metadata": {"source": "CRF", "title": "FULLY DNN-BASED MULTI-LABEL REGRESSION FOR AUDIO TAGGING", "authors": ["Yong Xu", "Qiang Huang", "Wenwu Wang", "Philip J. B. Jackson", "Mark D. Plumbley"], "emails": ["m.plumbley}@surrey.ac.uk"], "sections": [{"heading": null, "text": "Index Terms - Audio Tagging, Deep Neural Networks, Multilabel Regression, Dropout, DCASE 2016"}, {"heading": "1. INTRODUCTION", "text": "In recent years, we have generated and uploaded a large amount of multimedia data to the Internet. These data, such as music, field sounds, news and TV broadcasts, have proven effective in many areas of application, but their effectiveness relies heavily on the quantity and quality of training data. In addition, it is very time consuming to get a grip on this problem. To address this problem, two types of methods have been developed that relate to a low level of audio words. \"Using unsupervised learning methods [1, 2, 4, 5]. The second type of work was supported by the Engineers and Physical Sciences Research Council (EPSRC) of the UK."}, {"heading": "2. RELATED WORK", "text": "Two basic methods, which are compared in our work, are briefly summarized in the following."}, {"heading": "2.1. Audio Tagging using Gaussian Mixture Models", "text": "Gaussian Mixture Models (GMMs) are a commonly used generative classifier. A GMM is parameterized in \u044b = {\u03c9m, \u00b5m, \u0435m}, m = {1, \u00b7 \u00b7 \u00b7, M}, where M is the number of mixtures and wm is the weight of the m-th mixture component. To implement multi-categorization with simple event days, a binary classifier is built that is associated with each audio event class in the training step. For a specific event class, all audio images in an audio block designated with that event are categorized into a positive class, while the remaining features are categorized into a negative class. At the classification level, an audio block Ci calculates the probability of each audio block xij (j, (j, {1 \u00b7 LCi}) for the two class models."}, {"heading": "2.2. Audio Tagging using Multiple Instance SVM", "text": "Multiple instance learning is described in terms of bags B. The jth instance in the ith bag, Bi, is defined as xij, where J-I = {1 \u00b7 \u00b7 \u00b7 li}, and li is the number of instances in Bi. Bi's label is Yi-1, 1}. If Yi = \u2212 1, then xij = \u2212 1 for all j. if Yi = 1, then at least one instance xij-Bi is a positive example of the underlying concept [8]. Since MI-SVM is the bag-level MIL support vector machine to maximize the bag margin, we define the functional margin of a bag in relation to a hyperplane as: \u03b3i = Yi max j-I (< w, xij > + b) (2) Using the above notion, MI-SVM can be defined as positive when the bag margin is designed as: min w, b, trit1-w2, and A-Zi (3)."}, {"heading": "3. PROPOSED FULLY DNN-BASED AUDIO TAGGING", "text": "DNN is a non-linear multi-layer model for extracting robust features associated with a specific classification [18] or regression [17] task. The purpose of the audio tagging task is to perform a multi-layer classification of audio chunks (i.e. assign zero or more labels to each audio chunck with a length of, say, four seconds in our experiments).This chunck has only redelevel labels without frame-level labels. Multiple events occur in many special frames, so the usual loss function based on frame-level entropy cannot be replicated. We suggest a method to encode the whole or almost all of the chunks."}, {"heading": "3.1. Fully DNN-based multi-label regression using sequence to sequence mapping", "text": "Fig. 1 shows the proposed fully DNN-based audio tagging framework using the deep pyramid structure. With the proposed framework, the entire or nearly complete audio features of the chunk are encoded in a regressive vector with values {0, 1}. Sigmoid was used as the output layer activation function to learn the presence probability of certain events. As an objective function, a stochastic gradient descendant algorithm was used, which is performed in mini batches with multiple epochs to improve learning convergence as follows: It = 1N N bias n = 1-X-chilling n (Yn + \u03c4n \u2212 \u03c4, W, b) \u2212 Xn tags can be regarded as mean squared errors, X-n (Yn + percared error) and Xn day points in relation to the size of the learning window (W, W)."}, {"heading": "3.2. Dropout for the over-fitting problem", "text": "This audio tagging task has only about four hours of training data with an unbalanced distribution of training data for each type of day. Dropout is a simple but effective way to alleviate this problem [27]. In each training iteration, the characteristic value of each input unit and the activation of each hidden unit are randomly removed with a predefined probability (e.g. \u03c1). These random disturbances effectively prevent the DNN from learning false dependencies. In the decryption phase, the DNN discounts all weights involved in the dropout training by (1 \u2212 \u03c1), which is considered a model averaging process [29]. In this task too, a mismatch problem may exist, and testing of audio segments may be completely different from existing audio segments due to the presence of many background noises. Therefore, dropout should be introduced to improve its robustness in order to generalize deviations in test segments."}, {"heading": "3.3. Background noise aware training", "text": "Different types of background noise in different recording environments could lead to the mismatch between the test pieces and the training pieces. To reduce this, we suggest a simple background noise-conscious training (or a method of adjusting background noise). To enable this noise awareness, the DNN is fed with the primary audio functions, which are supplemented by an estimate of background noise. In this way, the DNN can use additional online background noise information to better predict the expected tags. Background noise is estimated as follows: Vn = [Yn \u2212 \u03c4,..., Yn \u2212 1, Yn, Yn + 1,..., Yn + \u03c4, Z \u0435n] (7) Z-n = 1T T-T \u0445 t = 1 Yt (8), where background noise Z-N is fixed above the utterance and estimated from the first T-frames. Although this noise estimate is simple, it has been proven to be similar to speech-28 [an improved idea of DNN]."}, {"heading": "4. EXPERIMENTAL SETUP AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. DCASE2016 data set for audio tagging", "text": "The data we used for the evaluation is the Task4 dataset from DCASE 2016 [15]. The audio recordings are made in a home environment; the audio data is provided as 4-second chunks at two sampling rates (48kHz and 16kHz) with the 48kHz data in stereo and the 16kHz data in mono. The 16kHz chunks were obtained by downsampling the right channel of the 48kHz recordings. Each audio file corresponds to a single chunk [15]. For each chunk, multi-label annotations were initially obtained from each of the 3 annotators; the annotations are based on a series of 7 label classes as in Table 1. A detailed description of the annotation process is included in [25]. To reduce the uncertainty of the test data, the evaluation is based on those chunks in which 2 or more annotators have agreed on the label presence across all classes."}, {"heading": "4.2. Experimental Setup", "text": "In our experiments, following the original configuration of Task4 of DCASE 2016 [15], we use the same five folds as the evaluation from the given development dataset, and use the remaining audio recordings for training. We process each audio chunk by comparing it to a (80ms) sliding window with a size of 40ms, and convert each segment into 24-D MFCs. For each 4-second chunk, 99 frames of the MFCs are better found as input instead of the overall frame, as these relaxed input schemes can increase the overall training samples."}, {"heading": "5. RESULTS AND DISCUSSIONS", "text": "Figure 2 shows the results obtained with our approach and two baselines. It is clear that the fully DNN-based approach outperforms the two baselines in the five-fold evaluations, for the following two main reasons: First, our proposed approach can make good use of the long-term temporary information rather than treating this information independently; second, it can map the entire audio feature sequence into a multi-tag vector by acting as a coding function; and we find that two of the audio event classes, namely the adult's male speech (designation \"m\") and other identifiable sounds (designation \"o\"), have not been well used; and the GMM-based method results in a narrow performance over the proposed method only in the third assessment. We find that two of the audio event classes, namely the adult's speech (designation \"m\") and other identifiable sounds (designation \"o\"), can be well identified in this fold assessment."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we presented a fully DNN-based approach to handling audio tagging with weak labels, in the sense that only chunk-level instead of frame-level labels are available. This full DNN is considered to be an encoding function to regressively map the sequence of audio features to a multi-tag vector. In order to extract robust high-level features, a deep pyramid structure was designed to reduce most uncorrelated interfering features while maintaining the highly related features. Dropout and background noise-aware training methods were used to further improve its generalization capacity for new recordings in invisible environments. We tested our approach using Task4 data set of the DCASE 2016 Challenge and achieved significant improvements over two baselines, namely GMM and MI-SVM, which were given to CNN on average 0.85 baselines, compared to the GMM-SVM system used in the official DCASE 1785 challenge."}, {"heading": "7. REFERENCES", "text": "[1] G. Chen and B. Han, \"Improve k-means clustering for audio data by exploring a reasonable sampling rate,\" in Seventh International Conference on Fuzzy Systems and Knowledge Discovery, 2010, pp. 1639-1642. [2] M. Riley, E. Heinen, and J. Ghosh, \"A text retrieval approach to contnent-based audio retrieval,\" in Proceedings of International Conference on Music Information Retrieval, 2008, pp. 1639-1642. [3] X. Shao, C. Xu, and M. Kankanhalli, \"Unsupervised classification of music genre using hidden markov model,\" in Proceedings of International Conference on Music Information Retrieval, 2004, pp. 2023-2026. [4] R. Cai, L. Lu, and A. Hanjalic, \"Unsupervised content discovery in composite audio,\" in Proceedings of International Conference on Multida, 2005, pp."}], "references": [{"title": "Improve k-means clustering for audio data by exploring a reasonable sampling rate", "author": ["G. Chen", "B. Han"], "venue": "Seventh International Conference on Fuzzy Systems and Knowledge Discovery, 2010, pp. 1639\u20131642.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "A text retrieval approach to contnent-based audio retrieval", "author": ["M. Riley", "E. Heinen", "J. Ghosh"], "venue": "Proceedings of International Conference on Music Information Retrieval, 2008, pp. 1639\u20131642.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Unsupervised classification of music genre using hidden markov model", "author": ["X. Shao", "C. Xu", "M. Kankanhalli"], "venue": "Proceedings of International Conference on Multimedia and Expo, 2004, pp. 2023\u20132026.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Unsupervised content discovery in composite audio", "author": ["R. Cai", "L. Lu", "A. Hanjalic"], "venue": "Proceedings of International Conference on Multimeida, 2005, pp. 628\u2013637.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised audio segmentation using extended baum-welch transformations", "author": ["T. Sainath", "D. Kanevsky", "G. Ivengar"], "venue": "Proceedings of International Conference on Acoustic, Speech and Signal Processing, 2007, pp. 2009\u20132012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Audio event detection using weakly labeled data", "author": ["A. Kumar", "B. Raj"], "venue": "CoRR, vol. abs/1605.02401, 2016. [Online]. Available: http://arxiv.org/abs/1605.02401", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Solving the multiple-instance problem with axis-parallel rectangles", "author": ["T.G. Dietterich", "R.H. Lathrop", "T. Lozano-Perez"], "venue": "Artificial Intelligence, vol. 89, pp. 31\u201371, 1997.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "Support vector machines for multiple-instance learning", "author": ["S. Andrew", "I. Tsochantaridis", "T. Hofmann"], "venue": "Proceedings of Advances in Neural Information Processing Systems, 2003, pp. 557\u2013584.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "Multiple-instance learning for music information retrieval", "author": ["M. Mandel", "D. Ellis"], "venue": "The International Society of Music Information Retrieval, 2008, pp. 577\u2013582.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "Acoustic classification of multiple simultaneous bird species: A multi-instance multi-label approach", "author": ["F. Briggs", "B. Lakshminarayanan", "L. Neal", "X.Z. Fern", "R. Raich"], "venue": "Journal of Acoustic Society of America, vol. 131, pp. 4640\u20134650, June 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to recognize objects with little supervision", "author": ["P. Carbonetto", "G. Dorko", "C. Schmid", "H. Kuck", "N.D. Freitas"], "venue": "International Journal of Computer Vision, vol. 77, pp. 219\u2013 237, May 2008.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2008}, {"title": "Miles: Multiple-instance learning via embedded instance selection", "author": ["Y. Chen", "J. Bi", "J.Z. Wang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, pp. 1931\u20131947, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1931}, {"title": "Multiple instance learning from weakly labeled videos", "author": ["A. Ulges", "C. Schulze", "T.M. Breuel"], "venue": "Proceedings of the Workshop on Cross-Media Information Analysis, Extraction and Management, 2008, pp. 17\u201324.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Multimedia event detection using event-driven multiple instance learning", "author": ["S. Phan", "D.D. Le", "S. Satoh"], "venue": "Proceedings of the 23rd ACM international conference on Multimedia, 2015, pp. 1255\u20131258.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "IEEE Signal Processing Letters, vol. 21, no. 1, pp. 65\u201368, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["\u2014\u2014"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-R. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep neural networks for audio scene recognition", "author": ["Y. Petetin", "C. Laroche", "A. Mayoue"], "venue": "23rd European Signal Processing Conference (EUSIPCO), 2015, pp. 125\u2013129.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Polyphonic sound event detection using multi label deep neural networks", "author": ["E. Cakir", "T. Heittola", "H. Huttunen", "T. Virtanen"], "venue": "International Joint Conference on Neural Networks (IJCNN), 2015, pp. 1\u20137.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end learning for music audio", "author": ["S. Dieleman", "B. Schrauwen"], "venue": "ICASSP, 2014, pp. 6964\u20136968.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["K. Choi", "G. Fazekas", "M. Sandler"], "venue": "arXiv preprint arXiv:1606.00298, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "CHiME-home: A dataset for sound source recognition in a domestic environment", "author": ["P. Foster", "S. Sigtia", "S. Krstulovic", "J. Barker", "M. Plumbley"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, 2015, pp. 1\u20135.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431\u20133440.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),. IEEE, 2013, pp. 8609\u20138613.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic noise aware training for speech enhancement based on deep neural networks.", "author": ["Y. Xu", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "INTERSPEECH,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1207.0580, 2012.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "Murohy, Machine Learning: A Probabilistic Perspective", "author": ["P. K"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "One is to convert low-level acoustic features into \u201cbag of audio words\u201d using unsupervised learning methods [1, 2, 3, 4, 5].", "startOffset": 108, "endOffset": 123}, {"referenceID": 1, "context": "One is to convert low-level acoustic features into \u201cbag of audio words\u201d using unsupervised learning methods [1, 2, 3, 4, 5].", "startOffset": 108, "endOffset": 123}, {"referenceID": 2, "context": "One is to convert low-level acoustic features into \u201cbag of audio words\u201d using unsupervised learning methods [1, 2, 3, 4, 5].", "startOffset": 108, "endOffset": 123}, {"referenceID": 3, "context": "One is to convert low-level acoustic features into \u201cbag of audio words\u201d using unsupervised learning methods [1, 2, 3, 4, 5].", "startOffset": 108, "endOffset": 123}, {"referenceID": 4, "context": "One is to convert low-level acoustic features into \u201cbag of audio words\u201d using unsupervised learning methods [1, 2, 3, 4, 5].", "startOffset": 108, "endOffset": 123}, {"referenceID": 5, "context": "methods is based on only weakly labeled data [6], e.", "startOffset": 45, "endOffset": 48}, {"referenceID": 6, "context": "To overcome the lack of annotated training data, Multiple Instance Learning (MIL) is proposed in [7] as a variation of supervised learning for problems with incomplete knowledge about labels of training examples.", "startOffset": 97, "endOffset": 100}, {"referenceID": 7, "context": "[8] proposed a new formulation of MIL as a maximum margin problem, which had led to some further work [9, 10, 11, 12, 13] in audio and video processing using weakly labeled data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[8] proposed a new formulation of MIL as a maximum margin problem, which had led to some further work [9, 10, 11, 12, 13] in audio and video processing using weakly labeled data.", "startOffset": 102, "endOffset": 121}, {"referenceID": 9, "context": "[8] proposed a new formulation of MIL as a maximum margin problem, which had led to some further work [9, 10, 11, 12, 13] in audio and video processing using weakly labeled data.", "startOffset": 102, "endOffset": 121}, {"referenceID": 10, "context": "[8] proposed a new formulation of MIL as a maximum margin problem, which had led to some further work [9, 10, 11, 12, 13] in audio and video processing using weakly labeled data.", "startOffset": 102, "endOffset": 121}, {"referenceID": 11, "context": "[8] proposed a new formulation of MIL as a maximum margin problem, which had led to some further work [9, 10, 11, 12, 13] in audio and video processing using weakly labeled data.", "startOffset": 102, "endOffset": 121}, {"referenceID": 12, "context": "[8] proposed a new formulation of MIL as a maximum margin problem, which had led to some further work [9, 10, 11, 12, 13] in audio and video processing using weakly labeled data.", "startOffset": 102, "endOffset": 121}, {"referenceID": 8, "context": "Mandel and Ellis in [9] used clip-level tags to derive tags at the track, album, and artist granularities by formulating a number of music information related multiple-instance learning tasks and evaluated two MIL based algorithms on them.", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": "In [14], Phan et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Recently, [6] also presented a SVM based MIL system for audio tagging and event detection.", "startOffset": 10, "endOffset": 13}, {"referenceID": 14, "context": "Recently, deep learning technologies have obtained great successes in speech, image and video fields [16, 17, 18, 19] since Hinton and Salakhutdinov showed the insights using a greedy layerwise unsupervised learning procedure to train a deep model in 2006 [20].", "startOffset": 101, "endOffset": 117}, {"referenceID": 15, "context": "Recently, deep learning technologies have obtained great successes in speech, image and video fields [16, 17, 18, 19] since Hinton and Salakhutdinov showed the insights using a greedy layerwise unsupervised learning procedure to train a deep model in 2006 [20].", "startOffset": 101, "endOffset": 117}, {"referenceID": 16, "context": "Recently, deep learning technologies have obtained great successes in speech, image and video fields [16, 17, 18, 19] since Hinton and Salakhutdinov showed the insights using a greedy layerwise unsupervised learning procedure to train a deep model in 2006 [20].", "startOffset": 101, "endOffset": 117}, {"referenceID": 17, "context": "Recently, deep learning technologies have obtained great successes in speech, image and video fields [16, 17, 18, 19] since Hinton and Salakhutdinov showed the insights using a greedy layerwise unsupervised learning procedure to train a deep model in 2006 [20].", "startOffset": 101, "endOffset": 117}, {"referenceID": 18, "context": "Recently, deep learning technologies have obtained great successes in speech, image and video fields [16, 17, 18, 19] since Hinton and Salakhutdinov showed the insights using a greedy layerwise unsupervised learning procedure to train a deep model in 2006 [20].", "startOffset": 256, "endOffset": 260}, {"referenceID": 19, "context": "The deep learning methods were also investigated for related tasks, like acoustic scene classification [21] and acoustic event detection [22].", "startOffset": 103, "endOffset": 107}, {"referenceID": 20, "context": "The deep learning methods were also investigated for related tasks, like acoustic scene classification [21] and acoustic event detection [22].", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "For music tagging task, [23, 24] have also demonstrated the superiority of deep learning methods.", "startOffset": 24, "endOffset": 32}, {"referenceID": 22, "context": "For music tagging task, [23, 24] have also demonstrated the superiority of deep learning methods.", "startOffset": 24, "endOffset": 32}, {"referenceID": 23, "context": "However, to the best of our knowledge, the deep learning based methods have not been used for environmental audio tagging which is a newly proposed task in DCASE 2016 challenge based on the CHiME-home dataset [25].", "startOffset": 209, "endOffset": 213}, {"referenceID": 24, "context": "The fully neural network structure was also successfully used in image segmentation [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 25, "context": "Dropout [27] and background noise aware training [28] are adopted to further improve the tagging performance in the DNN-based framework.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "Dropout [27] and background noise aware training [28] are adopted to further improve the tagging performance in the DNN-based framework.", "startOffset": 49, "endOffset": 53}, {"referenceID": 7, "context": "If Yi = 1, then at least one instance xij \u2208 Bi is a positive example of the underlying concept [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 16, "context": "DNN is a non-linear multi-layer model for extracting robust features related to a specific classification [18] or regression [17] task.", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "DNN is a non-linear multi-layer model for extracting robust features related to a specific classification [18] or regression [17] task.", "startOffset": 125, "endOffset": 129}, {"referenceID": 25, "context": "Dropout is a simple but effective way to alleviate this problem [27].", "startOffset": 64, "endOffset": 68}, {"referenceID": 27, "context": "At the decoding stage, the DNN discounts all of the weights involved in the dropout training by (1\u2212 \u03c1), regarded as a model averaging process [29].", "startOffset": 142, "endOffset": 146}, {"referenceID": 15, "context": "Although this noise estimator is simple, a similar idea was shown to be effective in DNN-based speech enhancement [17, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 26, "context": "Although this noise estimator is simple, a similar idea was shown to be effective in DNN-based speech enhancement [17, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 23, "context": "A detailed description of the annotation procedure is provided in [25].", "startOffset": 66, "endOffset": 70}, {"referenceID": 28, "context": "EER is defined as the point of the graph of false negative rate (FNR) versus false positive rate (FPR) [30]", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "Acoustic event detection for content analysis in most cases relies on lots of labeled data. However, manually annotating data is a time-consuming task, which thus makes few annotated resources available so far. Unlike audio event detection, automatic audio tagging, a multi-label acoustic event classification task, only relies on weakly labeled data. This is highly desirable to some practical applications using audio analysis. In this paper we propose to use a fully deep neural network (DNN) framework to handle the multilabel classification task in a regression way. Considering that only chunk-level rather than frame-level labels are available, the whole or almost whole frames of the chunk were fed into the DNN to perform a multi-label regression for the expected tags. The fully DNN, which is regarded as an encoding function, can well map the audio features sequence to a multi-tag vector. A deep pyramid structure was also designed to extract more robust high-level features related to the target tags. Further improved methods were adopted, such as the Dropout and background noise aware training, to enhance its generalization capability for new audio recordings in mismatched environments. Compared with the conventional Gaussian Mixture Model (GMM) and support vector machine (SVM) methods, the proposed fully DNN-based method could well utilize the long-term temporal information with the whole chunk as the input. The results show that our approach obtained a 15% relative improvement compared with the official GMM-based method of DCASE 2016 challenge.", "creator": "LaTeX with hyperref package"}}}