{"id": "1606.07707", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Collective Semi-Supervised Learning for User Profiling in Social Media", "abstract": "The abundance of user-generated data in social media has incentivized the development of methods to infer the latent attributes of users, which are crucially useful for personalization, advertising and recommendation. However, the current user profiling approaches have limited success, due to the lack of a principled way to integrate different types of social relationships of a user, and the reliance on scarcely-available labeled data in building a prediction model. In this paper, we present a novel solution termed Collective Semi-Supervised Learning (CSL), which provides a principled means to integrate different types of social relationship and unlabeled data under a unified computational framework. The joint learning from multiple relationships and unlabeled data yields a computationally sound and accurate approach to model user attributes in social media. Extensive experiments using Twitter data have demonstrated the efficacy of our CSL approach in inferring user attributes such as account type and marital status. We also show how CSL can be used to determine important user features, and to make inference on a larger user population.", "histories": [["v1", "Fri, 24 Jun 2016 14:42:17 GMT  (2551kb,D)", "http://arxiv.org/abs/1606.07707v1", null]], "reviews": [], "SUBJECTS": "cs.SI cs.LG", "authors": ["richard j oentaryo", "ee-peng lim", "freddy chong tat chua", "jia-wei low", "david lo"], "accepted": false, "id": "1606.07707"}, "pdf": {"name": "1606.07707.pdf", "metadata": {"source": "CRF", "title": "Collective Semi-Supervised Learning for User Profiling in Social Media", "authors": ["Richard J. Oentaryo", "Ee-Peng Lim", "Freddy Chong", "Tat Chua", "Jia-Wei Low", "David Lo"], "emails": ["davidlo}@smu.edu.sg", "freddy.chua@hp.com"], "sections": [{"heading": null, "text": "Index terms - Convex optimization, collective learning, semi-supervised learning, social media, user profiling. F"}, {"heading": "1 INTRODUCTION", "text": "In recent years, we have seen a dramatic growth in the social interactions that take place on social media such as Twitter and Facebook. \u2022 These social media sites allow users to share content (e.g. text, images, videos or web links) and build social relationships, user communities and common interest groups. Social media also generate an enormous amount of digital data about user behavior. The availability of such data has triggered a desire to learn more about consumers / users, which in turn fuels the emergence of new services for peer interaction, marketing and content sharing. However, these services often do not provide users \"preferences and attributes in a way that supports personalization, advertising, and recommendations. [24], [28]. Despite the abundance of user-generated data, metadata about personal attributes that are directly useful for personalized services and recommendations, these are not available. At Twitter, for example, users rarely provide demographic information, such as gender, age, religion, or marital status. Such information can be used by Twitter or other organizations to make search segmentation, or machine-driven recommendations."}, {"heading": "1.1 Motivating Example", "text": "The example consists of six personal and organizational accounts whose names are known (ovals and round boxes), and two accounts with unknown names (dashed boxes); the top half of Bob Bob Bob shows two types of relationship: \"follow\" and \"retweet from,\" and each user's tweet content (words) is shown in the left table; the bottom half shows the user's bag-of-words property. Traditionally, the label can be derived using only the user's own content (default functions), but often the default functions alone are not indicative of the label, e.g., user Andy who never tweets. Augmenting social features from Andy's followees (Bob, Citibank and HSBC) can provide stronger cues for Andy's label.In this spirit, recent work [20], [Andy] has tried to integrate social features that largely stem from a single type of relationship between users."}, {"heading": "1.2 Proposal and Contributions", "text": "Generalizing the above scenario, we propose a new type of user profile by developing several types of social relationships and unlabeled data to better derive the (latent) user attributes. \u2022 How do we develop an efficient and robust profile method that can integrate multiple types of relationships and unlabeled data in a computational manner? \u2022 Can we understand the contributions of different characteristics and relationship types, as well as a larger number of user types? In this paper, we present a new calculation method called collective."}, {"heading": "1.3 Paper Outline", "text": "The rest of this paper is structured as follows: In Section 2, we first give an overview of related work. Section 3 explains the proposed CSL approach. In Section 4, we describe the user profiling tasks discussed in this paper, followed by the corresponding experimental results and analysis in Section 5. Finally, we conclude in Section 6."}, {"heading": "2 RELATED WORK", "text": "We first interview related work on user attribute profiling, semi-supervised learning, and multirelational learning, and then discuss how our approach differs from these."}, {"heading": "2.1 User Attribute Profiling", "text": "Rao et al. [28] proposed a set of network structure-based characteristics to determine the attributes of Twitter users, including gender, age, geographical origin and political preference, which were then fed into a stacked classifier to derive the attributes. Mislove et al. [24] used both methods of detecting global and local communities to find communities of users sharing common attribute values. Ikeda et al. [14] used social communities to derive demographic information from Twitter users. They developed a hybrid method that uses both text characteristics and characteristics of the network structure.3 Recently Kosinski et al. [18] showed that public attributes received from Facebook can be used to predict demographic attributes of Twitter users. By factoring a sparse matrix that pleases the user, which subject / topic pleases, a low-level relationship of user representation is then developed between the three-dimensional attributes and Li."}, {"heading": "2.2 Semi-Supervised Learning", "text": "The simplest form of SSL is bootstrapping, where a classifier is first trained on marked data and then applied to unmarked data to generate more labeled samples for the next rounds of training [1], [12]. Bootstrapping works on the simplistic assumption that the classifier's own (highly trustworthy) predictions are correct. Co-training is an extension of bootstrapping, in which two (or more) classifiers are trained on different, ideally fragmented characteristics and generate labeled samples to improve each other [5]. This method is less prone to error than bootstrapping, but requires that natural feature splits exist in the data.Another class of SSL methods uses low-density assumptions [6] and promotes decision limits to improve the results of generalization."}, {"heading": "2.3 Multi-Relational Learning", "text": "Multirelational learning (MRL) is applicable when the data is available in multiple structured formats and can be represented as multiple graphs (also known as multigraphs), i.e. a multigraph can be used in MRL to encode different types of relationships (edges) between entities (nodes). In [35], Xu et al. presented a groundbreaking work on multirelational Gaussian processes (MRGP) that uses a generative probabilistic model based on the Gaussian process. It combines covariance and random variables to model multiple relationships, which in turn provides support for multiple relational learning tasks with multiple types of entities and relations. In another task domain, Wang et al. [33] proposed an MRL method for video annotation that integrates multiple graphs into a regulatory framework to sufficiently exploit their complementation."}, {"heading": "2.4 Our Approach", "text": "Our CSL approach differs from existing work in several important areas that we have listed hereafter.4 Comparisons with existing profile methods. While many of the current profile methods use users \"social information for attribute predictions, they have focused on only one type of relationship (e.g., just the follow-up relationship in Twitter), and there is a lack of a systematic method to integrate different types of relationships. Second, existing profile methods only use described data, which is often very scarce. A more robust predictive model can also be achieved by leveraging a large pool of unused data. CSL offers these two capabilities in a unified and synergetic way that makes the most of our knowledge."}, {"heading": "3 PROPOSED FRAMEWORK", "text": "Our CSL framework works on the basis of two inputs: 1) partially labeled data, consisting of a feature matrix X with known designations YL and missing designations YU, and 2) multigraph, consisting of multiple directed graphs Gm encoding different types of social relationships. First, we describe our notations: Let G = {G1,..., Gm,..., GM} be a multigraph consisting of M graphs, with each graph Gm = (V, Em) encoding the nodes V and Em. Note that here we have a common set of nodes V, but different sets of edges Em. We refer to the feature matrix of nodes V as X-RN-J and the weighted adaptation matrix of edges Em as Wm-RN-N-N, where N and J are each the number of nodes and characteristics. We also refer to the designations of V as Y, which represent the attributes of the user of interest."}, {"heading": "3.1 Probabilistic Foundation", "text": "We outline here first the likely formulation of our CSL approach. Ultimately, our goal is to maximize the posteriore distribution of model parameters, since the labels Y, node features X and multigraph G. Here, the posterior can be calculated using the Bayes rule: p (1) (Y, X, G) p (1). In this work, we focus on partially labeled data (Y, X, G) p (1) p (Y, X, G) p (Y, G) p (X, G) p (1). In this work, we focus on partially labeled data, where only a few data instances have observed the labels YL, while the remaining instances are largely unlabeled, i.e., their labels YU, YU, YU, YU, YL, YL, YL, YL, YL, YL, YL, YL, YL, YL, YL, X, X, X and YE, YE, YU and YE, YU, YU, YE, YE, YE, YE, YU, YU, Y, YU, Y, Y, Y, YU, Y, YU, Y, Y, Y, YU, Y, Y, Y, YU, Y, Y, YU, Y, Y, YU, Y, YU, Y, YU, Y, YU, Y, YU, Y, Y, YU, Y, Y, YU, Y, Y, Y, YU, Y, Y, YU, Y, Y, Y, Y, Y, Y, YU, Y, Y, Y, YU, Y, Y, Y, Y, Y, YU, Y, Y, Y, Y, Y, Y, Y, Y, YU, YU, YU, YU, Y, Y, Y, Y, YU, Y, Y, Y, Y, Y, Y, Y, Y, YU, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y,"}, {"heading": "3.2 Base Model", "text": "The proposed CSL approach can be seen as a generalization of the contemporary logistic regression model [10]. Traditionally, logistic regression is learned solely on the basis of designated data YL (i.e., YU is not used), and multirelational information encoded as Multigraph G. That is, by excluding YU and G and assuming independent and identically distributed data instances (i.e., logistic regression essentially learns to minimize the following loss function: LL = \u2212 L \u2211 i = 1 ln (yi | xi \u2212 j) \u2212 J = 1 ln (p (p)) (6), where yi-YL and xi-X are the actual designation and function vector for the data instance i and L = | YL | is the 5 number of designated data instances, and the designated data instances is a model (i.e., weight parameters) that we want to learn."}, {"heading": "3.3 Multi-Relational Features", "text": "We extend the logistic regression model to integrate the multirelational information G by adding multirelational features (MRF). In particular, by integrating G into the parameterization of the base model in (8), we obtain an extended linear model f (xi, G, E): f (xi, G, E) = J (1) = 1 (1), j + M (1) = 1 (1), j (11) and accordingly (G, E): f (MRM)), where there are different characteristics of the data instance (i.e., node) i for graph, and Gm, j is the associated relational weight for Gm, and vice versa (G, E)."}, {"heading": "3.4 Convex Divergence Regularization", "text": "After constructing the MRF for all data instances (both labeled and unlabeled), CSL performs semi-supervised learning (SSL), using unlabeled data to improve generalization and robustness. To this end, we present the idea of convex divergence (CD) to regulate learning using unlabeled data (14). CD regularization results from the following definition of p (YU | X, B, D, E, D, E, E, E, B, D, D, D, D, D, D, D, D, D, D, D, E, is a normalizing constant, \u03b2 is a (non-negative) user-specified regulation parameter, and Df, E, is the f - divergence [2] between two distributions."}, {"heading": "3.5 Parameter Learning", "text": "We can now combine (17), (13) and (11) to derive the total loss function L for the CSL approach, which is given by: L = \u2212 L \u2211 i = 1 [yi ln (\u03c3i) + (1 \u2212 yi) ln (1 \u2212 \u03c3i) + \u03bb2 J = 1 (\u03b82j + M \u2211 m = 1 \u03b12, j) \u2212 \u03b2 \u00b2 L + U \u2211 i = L + 1 ln (\u03c3i) (20) with \u03c3i, 1 = 1 (f (xi, G)) and \u03c3i = 1 \u2212 2 (f (xi, L)). Before proceeding with the learning method to minimize L, we can first give a demonstration of the convexity of L. This is done by examining the convexity of L (i.e.) and the first derivative of L (i.e.) and the second derivative of L (i.e.) of L. We can now examine the convexity in relation to the convexity of L (i.e., we do the second derivative of L and i.e.)."}, {"heading": "4 USER PROFILING IN TWITTER", "text": "This section provides an overview of the Twitter record and the tasks of user profiling that we take into account in this work."}, {"heading": "4.1 Twitter Dataset", "text": "In our study, we used the Twitter data of Singapore users - hereinafter referred to as SGTwitter - collected from February 1-28, 2014. Starting with a group of Singapore users, we combed their network based on follow, retweet, and user mentions links. Next, we added followers, retweet sources, and mentioned users to our user base who specified Singapore as their profile location. Accordingly, we received a total of 130,142 public user accounts whose profiles can be accessed / studied. We then focused on active users who tweeted at least twice within a month, giving us the final number of 100,497 active users. Specifically, we use an implementation of the L-BFGS algorithm provided in the SciPy library: http: / / goo.gl / q2dfnZTable 1 summarizes the counting statistics of our SGTwitter data for various activities aggregated at the user level."}, {"heading": "4.2 User Attributes", "text": "In this paper, we consider the task of classifying two user attributes (i.e. labels): account type (i.e., personal vs. organizational accounts) and marital status (i.e. single vs. married) 3. The profiling of these attributes is a relatively new problem that has not been well studied so far, and this could bring advantages in terms of providing personalized services / support that meet the different needs of each user type. \"For example,\" corporate accounts might require a service to standardize the format of their content contributions or track feelings on their products, whereas personal accounts would likely benefit from personalized friendship and content recommendations. \"Similarly,\" married users would likely be more interested in family-related products or content than individual users. \"In order to derive the account type and marital status labels, we have defined several keywords / phrases that describe the respective labels. For the account type task, we have classified organizational accounts by identifying them with their URLs and remaining in them."}, {"heading": "4.3 Feature Extraction", "text": "Our main interest here is to investigate to what extent the content generated by a user can be used to derive its (latent) attributes. We use the term document to refer to a data instance i, which represents the collection of tweets posted by a user. In this context, our goal is to derive a user's attribute based on its Tweet documentation. To extract the text properties, we first converted the raw tweets into a vector of words from which we can derive an n-gram representation that is suitable for our CSL model. We summarize our extraction steps as follows: \u2022 Tokenization: We have broken up a tweet document into its constituent word marks, and then created pockets from word marks from which each bag has the frequency of tokens appearing in a document."}, {"heading": "4.4 Multi-Relational Information", "text": "In our study of the SGTwitter data, we look at the multirelational information G, which is derived from three directed graphs: the follow, mention, and retweet graph. The follow-up graph contains binary edge weights. That is, wm, i, i \"= 1 if a user i follows another user i\" and 0 otherwise. On the other hand, the weights of the mention / retweet graph refer to the number of times (number) in which a user i \"mentions / retweets. In this case, no edge is constructed for a zero number."}, {"heading": "5 EXPERIMENTAL RESULTS AND ANALYSIS", "text": "In this section, we present the results of our study on account type and marital status profiling of SGTwitter users. We aim to answer several research questions (RQs): \u2022 RQ1: How does CSL's performance compare with that of other SSL methods? \u2022 RQ2: How can the learned CSL models be generalized to invisible (unmarked) data? \u2022 RQ5: What can CSL's predictions say about a larger user population? Procedure. To address the above RQs, we consider two scenarios: evaluation using marked data (for RQ1-RQ3) and evaluation using unmarked data (for RQ4-RQ5). For the first scenario, we adopt a stratified 10-fold CV procedure."}, {"heading": "5.1 Performance Comparisons (RQ1)", "text": "training set. Specifically, for each fold, we train our CSL method using 90% of the available marked data (i.e., 90% of the 1,308 and 2,313 marked accounts for account and family status tasks).To facilitate comprehensive evaluations, we present the results for different MRF settings: no graph, no single graph, and all three graphs show the results for the two tasks, comparing the F1 values and training time of our CSL method with those of all other SSL methods. To facilitate comprehensive evaluations, we present the results for different MRF settings: no graph, single graph, and all three graphs show the values of the Wilcoxon test for comparing the respective MRF settings."}, {"heading": "5.2 Contribution of MRF and Unlabeled Data (RQ2)", "text": "In order to see the contributions of the MRF and CD formulations, further sensitivity studies were carried out by varying the graph configurations and the number of instances marked in the training data. We selected the number of instances marked L from {25, 50, 100, 200, 400, 800, \"full\"}, with \"full\" referring to 90% of all instances marked, as explained in Section 5.1. Fig. 2 and 3 consolidate the results of our studies for account and family status tasks. Note that the results at the right extremes of the numbers are those in Tables 3 and 4. We can see here that CD regulation in CSL provides more robust and consistent F1 values than ER and XR regularization, especially for small marked data sizes. Even if L = 25, CSL is able to achieve the formula 1 score not far from the one obtained with L = \"full,\" as other values, this is also confirmed by our Wilcoxon tests, which show all the characteristics before L, each of the 0.00L."}, {"heading": "5.3 Feature Importance Analysis (RQ3)", "text": "In the parameters of the trained CSL model, we can trace which functions are most useful for our user profiling tasks. In particular, we assess the importance of the individual functions by looking at the learned own weights."}, {"heading": "5.4 Generalization to Unseen Data (RQ4)", "text": "To this end, we used our trained CSL model to make predictions for all unmarked data, selecting the best K-positive instances with the highest predictive values \u03c3i and the best K-negative instances with the highest (1 \u2212 \u03c3i), and then examining all of these instances manually to determine how well the CSL predictions correspond to human judgments. To evaluate the robustness of the model, we varied K from 20 to 100. For each class designation, we recorded the number of correctly predicted instances (TP), the number of incorrectly predicted instances (FP) and the number of unclassifiable instances (UC). The UC refers to the case where we are unable to manually determine the actual designation of an instance (i.e., a \"do not know\" answer), and then calculated the precision at the top K (Prec) by excluding the non-classifiable entities, with the C being the results in the K and the K being in the rule."}, {"heading": "5.5 Inference on Larger User Population (RQ5)", "text": "The last part of our empirical studies consists of conclusions on a larger population of SGTwitter users. To this end, we performed quantitative and qualitative analyses of the predictions made for all unmarked data. Our quantitative study includes comparing the label distributions in the marked data set (according to Table 2) with the predicted distributions derived from CSL on the unmarked data set. Table 7 shows the results. We note that the predicted distributions are much more unbalanced than the distributions of the marked data. Although we cannot fully verify this observation (due to the need to label all 100K samples), it is reasonable to expect that the larger SGTwitter population would consist of more personal accounts than organizational data and more individual users than married ones. The distribution difference also suggests that the larger population contains new cases not previously recorded in the marked data, and our method is relatively generalizable to these cases."}, {"heading": "6 CONCLUSION", "text": "In this paper, we present a novel CSL approach to modeling / profiling the attributes of social media users. At the heart of the proposed CSL approach is the principal and efficient solution to the novel problem of simultaneously using multiple types of social relationships and a large pool of blank data in the creation of user profiles. At the heart of the proposed CSL approach is first to expand the entrance area by generalizing a set of MRF characteristics that capture different types of relationships, and then to perform CD regulation to establish convex semi-supervised learning based on blank data. Experimental results on Twitter users in Singapore have demonstrated the accuracy, robustness, efficiency, and interpretability features of our approach to user attribution profiling."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by the Singapore National Research Foundation through its International Research Centre @ Singapore Funding Initiative and managed by the IDM Programme Office, Media Development Authority (MDA)."}], "references": [{"title": "Understanding the Yarowsky algorithm", "author": ["S. Abney"], "venue": "Computational Linguistics, vol. 30, no. 3, pp. 365\u2013395, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "A General Class of Coefficients of Divergence of One Distribution from Another", "author": ["S.M. Ali", "S.D. Silvey"], "venue": "Journal of the Royal Statistical Society, vol. 28, no. 1, pp. 131\u2013142, 1966.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1966}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 2399\u2013 2434, 2006.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2006}, {"title": "Combining labeled and unlabeled data with co-training", "author": ["A. Blum", "T. Mitchell"], "venue": "Proceedings of the Annual Conference on Computational Learning Theory, 1998, pp. 92\u2013100.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1998}, {"title": "Semi-Supervised Learning", "author": ["O. Chapelle", "B. Schlkopf", "A. Zien"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Probabilistic models for incomplete multi-dimensional arrays", "author": ["W. Chu", "Z. Ghahramani"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics, 2009, pp. 89\u201396.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Inferring user demographics and social strategies in mobile social networks", "author": ["Y. Dong", "Y. Yang", "J. Tang", "Y. Yang", "N.V. Chawla"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2014, pp. 15\u201324.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Optimizing multi-relational factorization models for multiple target relations", "author": ["L.R. Drumond", "E. Diaz-Aviles", "L. Schmidt-Thieme", "W. Nejdl"], "venue": "Proceedings of the ACM International Conference on Information and Knowledge Management, 2014, pp. 191\u2013200.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "Journal of Machine Learning Research, vol. 9, pp. 1871\u20131874, 2008.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1871}, {"title": "Entropy regularization", "author": ["Y. Grandvalet", "Y. Bengio"], "venue": "Semi- Supervised Learning, O. Chapelle, B. Sch\u00f6lkopf, and A. Zien, Eds., 2006, pp. 151\u2013168.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Analysis of semi-supervised learning with the Yarowsky algorithm", "author": ["G. Haffari", "A. Sarkar"], "venue": "Proceedings of the Conference on Uncertainty in Artificial Intelligence, 2007, pp. 159\u2013166.  14", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "PARAFAC: Parallel factor analysis", "author": ["R.A. Harshman", "M.E. Lundy"], "venue": "Computational Statistics and Data Analysis, vol. 18, no. 1, pp. 39\u201372, 1994.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1994}, {"title": "Twitter user profiling based on text and community mining for market analysis", "author": ["K. Ikeda", "G. Hattori", "C. Ono", "H. Asoh", "T. Higashino"], "venue": "Knowledge-Based Systems, vol. 51, pp. 35\u201347, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "A latent factor model for highly multi-relational data", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "G. Obozinski"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 3176\u20133184.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Sur les fonctions convexes et les ingalits entre les valeurs moyennes", "author": ["J.L.W.V. Jensen"], "venue": "Acta Mathematica, vol. 30, no. 1, pp. 175\u2013 193, 1906.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1906}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "Proceedings of the International Conference on Machine Learning, 1999, pp. 200\u2013209.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1999}, {"title": "Private traits and attributes are predictable from digital records of human behavior", "author": ["M. Kosinski", "D. Stillwell", "T. Graepel"], "venue": "Proceedings of the National Academy of Sciences, vol. 110, no. 15, pp. 5802\u20135805, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "RCV1: A new benchmark collection for text categorization research", "author": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research, vol. 5, pp. 361\u2013397, 2004.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "Weakly supervised user profile extraction from Twitter", "author": ["J. Li", "A. Ritter", "E. Hovy"], "venue": "Proceedings of the Annual Meeting of the Association for Computational Linguistics, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "User profiling in an ego network: Co-profiling attributes and relationships", "author": ["R. Li", "C. Wang", "K.C.-C. Chang"], "venue": "Proceedings of the International World Wide Web Conference, 2014, pp. 819\u2013830.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "On the limited memory bfgs method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical Programming, vol. 45, no. 3, pp. 503\u2013528, 1989.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1989}, {"title": "Simple, robust, scalable semisupervised learning via expectation regularization", "author": ["G.S. Mann", "A. McCallum"], "venue": "Proceedings of the International Conference on Machine Learning, 2007, pp. 593\u2013 600.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "You are who you know: Inferring user profiles in online social networks", "author": ["A. Mislove", "B. Viswanath", "K.P. Gummadi", "P. Druschel"], "venue": "Proceedings of the ACM International Conference on Web Search and Data Mining, 2010, pp. 251\u2013260.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "The role of unlabeled data in supervised learning", "author": ["T. Mitchell"], "venue": "Language, Knowledge, and Representation, J. Larrazabal and L. Miranda, Eds. Springer, 2004, vol. 99, pp. 103\u2013111.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "Factorizing YAGO: scalable machine learning for linked data", "author": ["M. Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "Proceedings of the International World Wide Web Conference, 2012, pp. 271\u2013280.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Squared-loss mutual information regularization: A novel information-theoretic approach to semi-supervised learning", "author": ["G. Niu", "W. Jitkrittum", "B. Dai", "H. Hachiya", "M. Sugiyama"], "venue": "Proceedings of the International Conference on Machine Learning, 2013, pp. 10\u201318.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Classifying latent user attributes in Twitter", "author": ["D. Rao", "D. Yarowsky", "A. Shreevats", "M. Gupta"], "venue": "Proceedings of the International Workshop on Search and Mining User-Generated Contents, 2010.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Relational learning via collective matrix factorization", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "Proceedings of the ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2008, pp. 650\u2013658.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Foundations and Trends in Machine Learning, vol. 4, no. 4, pp. 267\u2013373, 2012.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "Mutual information estimation reveals global associations between stimuli and biological processes", "author": ["T. Suzuki", "M. Sugiyama", "T. Kanamori", "J. Sese"], "venue": "BMC Bioinformatics, vol. 10, 2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiple graph regularized protein domain ranking", "author": ["J. Wang", "H. Bensmail", "X. Gao"], "venue": "BMC Bioinformatics, vol. 13, no. 1, p. 307, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Unified video annotation via multigraph learning", "author": ["M. Wang", "X.-S. Hua", "R. Hong", "J. Tang", "G.-J. Qi", "Y. Song"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 19, no. 5, pp. 733\u2013 746, 2009.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Individual comparisons by ranking methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin, vol. 1, no. 6, pp. 80\u201383, 1945.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1945}, {"title": "Multi-relational learning with gaussian processes", "author": ["Z. Xu", "K. Kersting", "V. Tresp"], "venue": "Proceedings of the International Jont Conference on Artifical Intelligence, 2009, pp. 1309\u20131314.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 22, "context": "For these services, there is a need to profile user preferences and attributes so as to support personalization, advertising, and recommendation [24], [28].", "startOffset": 145, "endOffset": 149}, {"referenceID": 26, "context": "For these services, there is a need to profile user preferences and attributes so as to support personalization, advertising, and recommendation [24], [28].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "Recent studies [18], [20], [24], [28] have nonetheless shown that it is possible to use statistical means to profile the latent user attributes, based on public data (e.", "startOffset": 33, "endOffset": 37}, {"referenceID": 12, "context": "tion and social connectivity [14], [24], [28].", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "tion and social connectivity [14], [24], [28].", "startOffset": 35, "endOffset": 39}, {"referenceID": 26, "context": "tion and social connectivity [14], [24], [28].", "startOffset": 41, "endOffset": 45}, {"referenceID": 18, "context": "In this spirit, recent works [20], [21] have tried to incorporate social features, largely derived from a single type of relationship.", "startOffset": 29, "endOffset": 33}, {"referenceID": 19, "context": "In this spirit, recent works [20], [21] have tried to incorporate social features, largely derived from a single type of relationship.", "startOffset": 35, "endOffset": 39}, {"referenceID": 30, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 144, "endOffset": 148}, {"referenceID": 31, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 150, "endOffset": 154}, {"referenceID": 5, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 221, "endOffset": 224}, {"referenceID": 11, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 226, "endOffset": 230}, {"referenceID": 27, "context": "Deviating from existing multirelational learning methods, which either treat multirelational information as constraints to the learning process [32], [33] or rely on low-rank assumption to decompose multi-relational data [7], [13], [29], our MRF approach is more general and makes less restrictive assumption about the multi-relational information.", "startOffset": 232, "endOffset": 236}, {"referenceID": 20, "context": ", the Quasi-Newton algorithm in [22]).", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "[28] proposed a set of network structure-based features to infer the attributes of Twitter users, including gender, age, geographical origin, and political preference.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] used both global and local community detection methods in order to find communities of users who share common attribute values.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] used social communities to infer the demographic information of the Twitter users.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] showed that public information obtained from Facebook can be used to predict demographic attributes of users.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] devised a distant supervised learning method to infer the attributes of Twitter users by augmenting structured auxiliary data from the Facebook and Google+ networks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "In [21], Li et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "[8] presented a factor graph model to predict the demographic attributes of mobile phone users.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The simplest form of SSL is bootstrapping, whereby a classifier is first trained using labeled data, and then applied to unlabeled data so as to generate more labeled samples for the next rounds of training [1], [12].", "startOffset": 207, "endOffset": 210}, {"referenceID": 10, "context": "The simplest form of SSL is bootstrapping, whereby a classifier is first trained using labeled data, and then applied to unlabeled data so as to generate more labeled samples for the next rounds of training [1], [12].", "startOffset": 212, "endOffset": 216}, {"referenceID": 3, "context": "Co-training is an extension of bootstrapping in which two (or more) classifiers are trained on different, ideally disjoint sets of features, and generate labeled samples to improve each other [5], [25].", "startOffset": 192, "endOffset": 195}, {"referenceID": 23, "context": "Co-training is an extension of bootstrapping in which two (or more) classifiers are trained on different, ideally disjoint sets of features, and generate labeled samples to improve each other [5], [25].", "startOffset": 197, "endOffset": 201}, {"referenceID": 4, "context": "Another class of SSL methods uses the low-density assumption [6], encouraging the decision boundary to lie in low-density regions for improving generalization results.", "startOffset": 61, "endOffset": 64}, {"referenceID": 15, "context": "The most common way to achieve this is to use a maximum margin algorithm such as transductive support vector machine [17].", "startOffset": 117, "endOffset": 121}, {"referenceID": 9, "context": "Grandvalet and Bengio [11] devised an alternative method based on entropy regularization (ER).", "startOffset": 22, "endOffset": 26}, {"referenceID": 4, "context": "There are also active research works on graph-based SSL (GSSL) methods, which treat both labeled and unlabeled data as nodes in a graph and build edges between pairs of nodes weighted by their affinities (similarities) [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 2, "context": "Generalizations have been proposed under the umbrella of manifold regularization [4], [6].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "Generalizations have been proposed under the umbrella of manifold regularization [4], [6].", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "Extending the entropy regularization method [11], several information-theoretic SSL methods have been developed [23], [27].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "Extending the entropy regularization method [11], several information-theoretic SSL methods have been developed [23], [27].", "startOffset": 112, "endOffset": 116}, {"referenceID": 25, "context": "Extending the entropy regularization method [11], several information-theoretic SSL methods have been developed [23], [27].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Mann and McCallum [23] proposed the expectation regularization (XR) to build a simple and robust SSL method.", "startOffset": 18, "endOffset": 22}, {"referenceID": 25, "context": "[27] devised a squared-loss mutual information regularization (SMIR) method, which led to a convex SSL problem formulation guaranteed under a mild condition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "The key appeal of this approach is that an analytical (closed-form) solution can be computed to identify unique, globally optimal model parameters [27], [31].", "startOffset": 147, "endOffset": 151}, {"referenceID": 29, "context": "The key appeal of this approach is that an analytical (closed-form) solution can be computed to identify unique, globally optimal model parameters [27], [31].", "startOffset": 153, "endOffset": 157}, {"referenceID": 33, "context": "In [35], Xu et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "[33] proposed a MRL method for video annotation that integrates multiple graphs into a regularization framework, so as to sufficiently exploit their complementation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "A similar approach was used in [32] to tackle the task of protein domain ranking in structural biology.", "startOffset": 31, "endOffset": 35}, {"referenceID": 5, "context": "This can be naturally extended to MRL by stacking the matrices to be factorized and then applying tensor factorization methods [7], [13].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": "This can be naturally extended to MRL by stacking the matrices to be factorized and then applying tensor factorization methods [7], [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 24, "context": "Another natural extension to MRL is to share the common embedding or the entities across relations via collective matrix factorization [26], [29].", "startOffset": 135, "endOffset": 139}, {"referenceID": 27, "context": "Another natural extension to MRL is to share the common embedding or the entities across relations via collective matrix factorization [26], [29].", "startOffset": 141, "endOffset": 145}, {"referenceID": 24, "context": "This method has shown state-of-the-art performances on relational datasets [26], although the number of relation types is usually modest (less than 100).", "startOffset": 75, "endOffset": 79}, {"referenceID": 7, "context": "Extensions have recently been proposed in [9], [15] to handle multirelational data with a large number of relation types.", "startOffset": 42, "endOffset": 45}, {"referenceID": 13, "context": "Extensions have recently been proposed in [9], [15] to handle multirelational data with a large number of relation types.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "In contrast to conventional SSL methods such as bootstrapping [1] and co-training [5], our CSL approach does not rely on the assumption that the model\u2019s own (high-confidence) prediction is correct, or that natural feature splits exist in the data.", "startOffset": 62, "endOffset": 65}, {"referenceID": 3, "context": "In contrast to conventional SSL methods such as bootstrapping [1] and co-training [5], our CSL approach does not rely on the assumption that the model\u2019s own (high-confidence) prediction is correct, or that natural feature splits exist in the data.", "startOffset": 82, "endOffset": 85}, {"referenceID": 9, "context": "Our approach also provides a convex formulation of SSL that is more robust and computationally elegant than the ER method [11], whereby the learning procedure can be easily trapped to one of the (multiple) local optimal solutions.", "startOffset": 122, "endOffset": 126}, {"referenceID": 21, "context": "Finally, the CSL approach is more general than the state-of-the-art information-theoretic SSL methods such as XR [23] and SMIR [27].", "startOffset": 113, "endOffset": 117}, {"referenceID": 25, "context": "Finally, the CSL approach is more general than the state-of-the-art information-theoretic SSL methods such as XR [23] and SMIR [27].", "startOffset": 127, "endOffset": 131}, {"referenceID": 31, "context": "Our CSL approach compares favourably to the MRGP method [33] in several ways.", "startOffset": 56, "endOffset": 60}, {"referenceID": 28, "context": "First, CSL adopts a discriminative probabilistic model, which should in principle be more accurate than the generative model used in MRGP [30].", "startOffset": 138, "endOffset": 142}, {"referenceID": 30, "context": "CSL is also less restrictive than the MRL methods in [32], [33].", "startOffset": 53, "endOffset": 57}, {"referenceID": 31, "context": "CSL is also less restrictive than the MRL methods in [32], [33].", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 83, "endOffset": 86}, {"referenceID": 7, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 88, "endOffset": 91}, {"referenceID": 11, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 93, "endOffset": 97}, {"referenceID": 13, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 99, "endOffset": 103}, {"referenceID": 24, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 105, "endOffset": 109}, {"referenceID": 27, "context": "Finally, CSL is more generic/flexible than the matrix/tensor factorization methods [7], [9], [13], [15], [26], [29].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": ", their labels YU are assumed to be missing at random [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 6, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 120, "endOffset": 123}, {"referenceID": 12, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 125, "endOffset": 129}, {"referenceID": 18, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 131, "endOffset": 135}, {"referenceID": 19, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 137, "endOffset": 141}, {"referenceID": 22, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 143, "endOffset": 147}, {"referenceID": 26, "context": "It must be noted that the above formulation is new and different from the contemporary user attribute profiling methods [8], [14], [20], [21], [24], [28].", "startOffset": 149, "endOffset": 153}, {"referenceID": 8, "context": "The proposed CSL approach can be viewed as a generalization of the contemporary logistic regression model [10].", "startOffset": 106, "endOffset": 110}, {"referenceID": 8, "context": "Note that the regularization term \u03bb2 \u2211J j=1 \u03b8 2 j serves to penalize large values of the model parameters \u03b8j , thereby reducing the risk of data overfitting [10].", "startOffset": 157, "endOffset": 161}, {"referenceID": 30, "context": ", [32], [33], or rely only on latent features, e.", "startOffset": 2, "endOffset": 6}, {"referenceID": 31, "context": ", [32], [33], or rely only on latent features, e.", "startOffset": 8, "endOffset": 12}, {"referenceID": 5, "context": ", [7], [13], [26], our MRF formulation is more generic and makes less stringent assumption.", "startOffset": 2, "endOffset": 5}, {"referenceID": 11, "context": ", [7], [13], [26], our MRF formulation is more generic and makes less stringent assumption.", "startOffset": 7, "endOffset": 11}, {"referenceID": 24, "context": ", [7], [13], [26], our MRF formulation is more generic and makes less stringent assumption.", "startOffset": 13, "endOffset": 17}, {"referenceID": 28, "context": "It is also worth noting that our MRF formulation is different from that of conditional random field (CRF) [30].", "startOffset": 106, "endOffset": 110}, {"referenceID": 1, "context": "whereZ is a normalizing constant, \u03b2 is a (nonnegative) userspecified regularization parameter, and Df (\u03bc||\u03c1) is the f divergence [2] between two distributions \u03bc and \u03c1:", "startOffset": 129, "endOffset": 132}, {"referenceID": 14, "context": "Fortunately, we can use the Jensen inequality [16] in order to derive a convex upper bound of (17).", "startOffset": 46, "endOffset": 50}, {"referenceID": 21, "context": "The formulation in (19) is related to the XR approach [23], with some key differences.", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "Our formulation is also conceptually superior to that of the SMIR method [27], whose convexity is not guaranteed when the L2 regularization parameter (\u03bb in our notation) is not sufficiently large [27].", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "Our formulation is also conceptually superior to that of the SMIR method [27], whose convexity is not guaranteed when the L2 regularization parameter (\u03bb in our notation) is not sufficiently large [27].", "startOffset": 196, "endOffset": 200}, {"referenceID": 0, "context": "It is clear that the curvature (23) will always be positive for any positive \u03bb (since \u03c3i \u2208 [0, 1]).", "startOffset": 91, "endOffset": 97}, {"referenceID": 20, "context": "In this work, we use the limited memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno (L-BFGS) algorithm [22], a popular, efficient Quasi-Newton method for solving unconstrained optimization problems2.", "startOffset": 92, "endOffset": 96}, {"referenceID": 20, "context": "Notably, the memory/time complexity of the L-BFGS algorithm is linear in the problem size [22], and the convexity of our CSL formulation makes it possible to reach the optimum within a few iterations.", "startOffset": 90, "endOffset": 94}, {"referenceID": 17, "context": "We used the list of English stop-words in [19].", "startOffset": 42, "endOffset": 46}, {"referenceID": 10, "context": "The first baseline is bootstrapping [12], where we first train a logistic regression using labeled data, apply it to predict on unlabeled data, and then add into the labeled dataset those samples that have high prediction scores.", "startOffset": 36, "endOffset": 40}, {"referenceID": 9, "context": "Additionally, we compare our CSL approach with two state-of-the-art information theoretic SSL methods, namely entropy regularization (ER) [11] and expectation regularization (XR) [23].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "Additionally, we compare our CSL approach with two state-of-the-art information theoretic SSL methods, namely entropy regularization (ER) [11] and expectation regularization (XR) [23].", "startOffset": 179, "endOffset": 183}, {"referenceID": 32, "context": "To evaluate whether the performance difference between two methods is statistically significant, we perform the Wilcoxon signed-rank test [34] with a critical value of 0.", "startOffset": 138, "endOffset": 142}], "year": 2016, "abstractText": "The abundance of user-generated data in social media has incentivized the development of methods to infer the latent attributes of users, which are crucially useful for personalization, advertising and recommendation. However, the current user profiling approaches have limited success, due to the lack of a principled way to integrate different types of social relationships of a user, and the reliance on scarcely-available labeled data in building a prediction model. In this paper, we present a novel solution termed Collective Semi-Supervised Learning (CSL), which provides a principled means to integrate different types of social relationship and unlabeled data under a unified computational framework. The joint learning from multiple relationships and unlabeled data yields a computationally sound and accurate approach to model user attributes in social media. Extensive experiments using Twitter data have demonstrated the efficacy of our CSL approach in inferring user attributes such as account type and marital status. We also show how CSL can be used to determine important user features, and to make inference on a larger user population.", "creator": "LaTeX with hyperref package"}}}