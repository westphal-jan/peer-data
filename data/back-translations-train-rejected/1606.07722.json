{"id": "1606.07722", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Neural Network Based Next-Song Recommendation", "abstract": "Recently, the next-item/basket recommendation system, which considers the sequential relation between bought items, has drawn attention of researchers. The utilization of sequential patterns has boosted performance on several kinds of recommendation tasks. Inspired by natural language processing (NLP) techniques, we propose a novel neural network (NN) based next-song recommender, CNN-rec, in this paper. Then, we compare the proposed system with several NN based and classic recommendation systems on the next-song recommendation task. Verification results indicate the proposed system outperforms classic systems and has comparable performance with the state-of-the-art system.", "histories": [["v1", "Fri, 24 Jun 2016 15:25:55 GMT  (3621kb)", "http://arxiv.org/abs/1606.07722v1", "5 pages, 3 figures, the 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016)"]], "COMMENTS": "5 pages, 3 figures, the 1st Workshop on Deep Learning for Recommender Systems (DLRS 2016)", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.LG", "authors": ["kai-chun hsu", "szu-yu chou", "yi-hsuan yang", "tai-shih chi"], "accepted": false, "id": "1606.07722"}, "pdf": {"name": "1606.07722.pdf", "metadata": {"source": "CRF", "title": "Neural Network Based Next-Song Recommendation", "authors": ["Kai-Chun Hsu", "Szu-Yu Chou", "Yi-Hsuan Yang", "Tai-Shih Chi"], "emails": ["kch610596@gmail.com,", "yang}@citi.sinica.edu.tw,", "tschi@mail.nctu.edu.tw"], "sections": [{"heading": null, "text": "Categories and Theme Descriptions \u2022 Information Systems ~ Recommendation Systems \u2022 Calculation Methods ~ Neural NetworksTags Next Song Recommendation, Music Recommendation, Neural Network, Sequential Recommendator, Text Embedding"}, {"heading": "1. INTRODUCTION", "text": "There are several types of recommendations, and the task of the next item / shopping cart has been welcomed by us in recent years. This task is to anticipate the next item or shopping cart that a user will put up with. Therefore, it is important to record the user's behavior towards himself, especially the way in which he is able to identify himself, as well as the way in which he does it. It has been shown that the combination of the user's buying behavior and the user's general taste is more helpful than the sole use of the product or the user's general preference."}, {"heading": "2. PROPOSED AND COMPARED METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Formalization of next-song recommendation", "text": "Most recommenders of the next song predict the next song for a user based on the last two songs or a song he has listened to. Each user has an audio recording that can be expressed with = (,,...,), where is the index of the song the user has listened to at that time."}, {"heading": "2.2 Proposed CNN-rec", "text": "Figure 1 shows the calculation structure of the proposed CNNrec receiver. Inspired by [10-14], we adopted a revolutionary layer to capture the local relationship pattern between adjacent songs. CNN-rec can be customized to take into account any number of final songs listened to by the user. Here, we use the last J-songs as an example. First, we have converted the identity of the user and the last J-songs in his audiobook to one-dimensional representations that are highly economical vectors, using 1-of-N coding. Each user or song has a specific non-zero entity in the attribute vector. Such a type of representation is widely used in the field of NLP. [9-14, 16] We used the word embedding technique commonly used in NLP research to achieve a low-dimensional density."}, {"heading": "2.3 Compared methods", "text": "Here we describe a number of comparative methods. NN-rec: Inspired by the NN-based probabilistic language model in the area of NLP [9], the NN-rec was originally proposed as a recommender for the next shopping basket [8] with remarkable performance on two retail datasets [9]. WMF: Weighted Matrix Factorization (WMF) [18] is a state-of-the-art matrix factorization method that uses implicit feedback datasets. It estimates the confidence of preference by counting the number of requests for the same song. In this paper, this method was treated as representative of classical recommenders that only take into account the general preferences of users, but not their sequential patterns. Word2Vec: The Word2vec method uses either the continuous bag of words (CBOW) or the continuous skip-gram architecture [15]. Both architectures contain a generic sequential sequential method of performing this factorization construction and the insertion of words."}, {"heading": "3. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Data Pre-processing", "text": "In our experiments, we used the last.FM-1k [19] music dataset, widely used in music recommendation experiments, which is the only open music dataset with timestamps that can be used to rate recommendations for the next song. Its timestamps cover the period from February 2005 to May 2009. The dataset includes 992 users, 176,948 artists, 1,505,264 items and 19,150,868 listening recordings. In terms of data preprocessing, we first extracted the top 10,000 songs from the dataset to reduce noise, data economy and enormous computing requirements. Then, we mixed the data and divided them into training, validation and a test set at a ratio of 7: 1: 2. In order to further generalize the predictive results, we deleted from the validation and test set those records that contained songs listened to by the users in the training set. After the pre-song process, we received a subset of 81081 and 8487 users data."}, {"heading": "3.2 Validation Stage", "text": "During the validation phase, we refined the systems NN-rec and CNN-rec to achieve their best performance for fair comparisons.As for the structure of the NNs, the dimension of the embedding function was set to 60 and the last 5 songs were taken into account for predicting the next song. In the hidden layer, there were 300 neurons with ReLU activation function. In addition, the number of revolutionary filters of CNN-rec was set to 325; the size of each filter was 2; and the filter step was set to 1. The neurons in the revolutionary layer were also activated with ReLU. During the training, we used cross-entropy as a cost function and Adagrad [20] with backpropagation as an update rule for each method. The number of epochs was set to 25; the batch size was set to 50 and the learning rate was 0.01. We used glorot weight [21] for the initialization of the 7 and 0.out in the systems."}, {"heading": "3.3 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 Performance Comparison", "text": "For ratings, we used the ratings metrics suggested in [22], which are better suited to evaluating the Top N recommendation task. According to [4], the sequential relationship between songs interrupts as soon as continuous listening pauses for a while. Therefore, recommenders should only learn sequential listening patterns within a specified period of time. Therefore, we have divided the sequential listening into sessions where the interevent time is less than one hour. In our opinion, the 1-hour interval we choose corresponds to the music listening behavior of regular users. The performance of all systems compared is shown in Figure 2. Clearly, we see that all NN-based next song recommenders, i.e. NN-rec, CNN-rec and Word2Vec, are all non-NN based systems except for the call rate for k = 150, 200 and 500, where FPMC achieves higher performance (Recall rate = 0.635, 0.7378 and 0.96c are always better than Record-643 = Ve2c)."}, {"heading": "3.3.2 Impact of the Order of Markov chain", "text": "For example, FPMC [1], PME [5], PRME [7] and Bi-gram [24] are first-order Markov models, while HRM [2], Tribeflow [6], NN-rec [9] and our CNN-rec can predict the next item from the last j-items. But, what effect does the order number have on the music recommendation task? To answer the question, we conducted experiments with the same NN-rec with different order numbers and the results are shown in Figure 3.The results show that increasing the order number can improve performance, especially in situations where only a few items are recommended to users (i.e. if k is small). Nevertheless, we can see that performance gradually saturates as the order number increases."}, {"heading": "4. CONCLUSION", "text": "We are the first group to implement and evaluate NN-based Nextsong recommendation systems. We also propose CNN-rec, whose structure is transferred from NLP to capture local relationships between adjacent songs at a revolutionary level. Furthermore, we show that the NN-based recommendations for the next song, CNN-rec, NN-rec and Word2Vec, the non-NN-based recommendations for the next song, which combine the general preferences and sequential listening patterns of users, provide the highest performance. Although the proposed CNN-rec does not surpass the NN-rec, which is a flat NN, we believe that it can replace a deep NN-based recommender in situations where time and space resources are limited."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "This research is supported by the Taiwanese Ministry of Science and Technology under grant number MOST 103-2220-E-009-003."}, {"heading": "6. REFERENCES", "text": "[1] S. Rendle, C. Freudenthaler and L. Schmidt-Thieme.Factorizing personalized markov chains for next-basket recommendation. In WWW, pages 811 {820. ACM, 2010. [2] Wang, Pengfei, et al. \"Learning Hierarchical Representation Model for Next-Basket Recommendation.\" Proceedings of the 18th International ACM SIGIR Conference on Knowledge discovery and data mining. [4] Ji, Ke, et al. \"Playlist prediction via metric embedding.\" Proceedings of the 18th International ACM SIGKDD international conference on Knowledge discovery and data mining. [4] Ji, Ke, et al. \"Next-song commendation with temporal..\" Knowledge-Based Systems 88 (2015): 134-143."}], "references": [{"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "WWW, pages 811{820. ACM", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning Hierarchical Representation Model for Next-Basket Recommendation.", "author": ["Wang", "Pengfei"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Playlist prediction via metric embedding.", "author": ["Chen", "Shuo"], "venue": "Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Next-song recommendation with temporal dynamics.", "author": ["Ji", "Ke"], "venue": "Knowledge-Based Systems", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Personalized next-song recommendation in online", "author": ["Wu", "Xiang"], "venue": "karaokes.\"Proceedings of the 7th ACM conference on Recommender systems", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "TribeFlow: Mining & Predicting User Trajectories.\"arXiv preprint arXiv:1511.01032", "author": ["Figueiredo", "Flavio"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Personalized ranking metric embedding for next new POI recommendation.", "author": ["Feng", "Shanshan"], "venue": "Proceedings of the 24th International Conference on Artificial Intelligence. AAAI Press,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Next Basket Recommendation with Neural Networks.", "author": ["Wan", "Shengxian"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": "JMLR, 3", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning to rank short text pairs with convolutional deep neural networks.", "author": ["Severyn", "Aliaksei", "Alessandro Moschitti"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Classifying relations by ranking with convolutional neural networks.", "author": ["dos Santos", "C\u0131cero Nogueira", "Bing Xiang", "Bowen Zhou"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep Convolutional Neural Networks for Sentiment Analysis of Short Texts.\" COLING", "author": ["dos Santos", "C\u00edcero Nogueira", "Maira Gatti"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Relation Classification via Convolutional Deep Neural Network.\" COLING", "author": ["Zeng", "Daojian"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Convolutional neural network architectures for matching natural language sentences.", "author": ["Hu", "Baotian"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "word2vec explained: Deriving mikolov et al.'s negative-sampling word-embedding method.\" arXiv preprint arXiv:1402.3722(2014)", "author": ["Goldberg", "Yoav", "Omer Levy"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality.\" Advances in neural information processing", "author": ["Mikolov", "Tomas"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "From Word Embeddings to Item Recommendation.\"arXiv preprint arXiv:1601.01356", "author": ["Ozsoy", "Makbule Gulcin"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Collaborative filtering for implicit feedback datasets.", "author": ["Hu", "Yifan", "Yehuda Koren", "Chris Volinsky"], "venue": "Data Mining,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization.", "author": ["Duchi", "John", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks.", "author": ["Glorot", "Xavier", "Yoshua Bengio"], "venue": "Aistats. Vol", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "Performance of recommender algorithms on top-n recommendation tasks.", "author": ["Cremonesi", "Paolo", "Yehuda Koren", "Roberto Turrin"], "venue": "Proceedings of the fourth ACM conference on Recommender systems", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}, {"title": "Class-based n-gram models of natural language.\"Computational linguistics", "author": ["Brown", "Peter F"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}], "referenceMentions": [{"referenceID": 0, "context": "Existing algorithms for this task are mainly based on matrix/tensor factorization, Markov Chain (MC) or Markov Embedding [1-7].", "startOffset": 121, "endOffset": 126}, {"referenceID": 1, "context": "Existing algorithms for this task are mainly based on matrix/tensor factorization, Markov Chain (MC) or Markov Embedding [1-7].", "startOffset": 121, "endOffset": 126}, {"referenceID": 2, "context": "Existing algorithms for this task are mainly based on matrix/tensor factorization, Markov Chain (MC) or Markov Embedding [1-7].", "startOffset": 121, "endOffset": 126}, {"referenceID": 3, "context": "Existing algorithms for this task are mainly based on matrix/tensor factorization, Markov Chain (MC) or Markov Embedding [1-7].", "startOffset": 121, "endOffset": 126}, {"referenceID": 4, "context": "Existing algorithms for this task are mainly based on matrix/tensor factorization, Markov Chain (MC) or Markov Embedding [1-7].", "startOffset": 121, "endOffset": 126}, {"referenceID": 5, "context": "Existing algorithms for this task are mainly based on matrix/tensor factorization, Markov Chain (MC) or Markov Embedding [1-7].", "startOffset": 121, "endOffset": 126}, {"referenceID": 6, "context": "Existing algorithms for this task are mainly based on matrix/tensor factorization, Markov Chain (MC) or Markov Embedding [1-7].", "startOffset": 121, "endOffset": 126}, {"referenceID": 0, "context": "It has been shown that combining the sequential pattern of buying behavior and the general taste of the user is more helpful to system performance than solely utilizing the sequential pattern or the general preference of the user [1, 2].", "startOffset": 230, "endOffset": 236}, {"referenceID": 1, "context": "It has been shown that combining the sequential pattern of buying behavior and the general taste of the user is more helpful to system performance than solely utilizing the sequential pattern or the general preference of the user [1, 2].", "startOffset": 230, "endOffset": 236}, {"referenceID": 3, "context": "Empirically, people usually listen to songs of the same singer, album, music genre, lyricist, composer or record company in a listening session [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 3, "context": "This phenomenon has been verified by embedding the listened songs of a user in a session into the Euclidean space [4, 5].", "startOffset": 114, "endOffset": 120}, {"referenceID": 4, "context": "This phenomenon has been verified by embedding the listened songs of a user in a session into the Euclidean space [4, 5].", "startOffset": 114, "endOffset": 120}, {"referenceID": 7, "context": "Firstly, we modified the next-basket recommender NN-rec developed in [8] to the next-song recommender.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "The original idea of the NN-rec was inspired by natural language processing (NLP) techniques [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 7, "context": "The NN-rec has shown superior performance on two retail datasets [8], yet, it has not validated on music recommendation task.", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "Secondly, also inspired by NLP techniques [10], we proposed a novel convolutional neural network (CNN) based recommender, the CNN-rec.", "startOffset": 42, "endOffset": 46}, {"referenceID": 9, "context": "In [10], the original deep learning architecture was modified by including a convolutional layer to capture the relation between adjacent words in sentences for re-ranking short text pairs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "This technique has been corroborated in the field of NLP [10-14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 10, "context": "This technique has been corroborated in the field of NLP [10-14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 11, "context": "This technique has been corroborated in the field of NLP [10-14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 12, "context": "This technique has been corroborated in the field of NLP [10-14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 13, "context": "This technique has been corroborated in the field of NLP [10-14].", "startOffset": 57, "endOffset": 64}, {"referenceID": 14, "context": "Thirdly, we implemented the Word2Vec method [15], which is also based on NN, as one of the baseline methods.", "startOffset": 44, "endOffset": 48}, {"referenceID": 8, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 124, "endOffset": 134}, {"referenceID": 9, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 124, "endOffset": 134}, {"referenceID": 10, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 124, "endOffset": 134}, {"referenceID": 11, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 124, "endOffset": 134}, {"referenceID": 12, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 124, "endOffset": 134}, {"referenceID": 13, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 124, "endOffset": 134}, {"referenceID": 15, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 124, "endOffset": 134}, {"referenceID": 7, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 195, "endOffset": 202}, {"referenceID": 16, "context": "One of the core techniques of these three NNbased methods is word embedding, which has been widely used in the field of NLP [9-14, 16] and successfully adopted in recommendation systems recently [8, 17].", "startOffset": 195, "endOffset": 202}, {"referenceID": 9, "context": "Inspired by [10-14], we adopted a convolutional layer to capture the local relation pattern between adjacent songs.", "startOffset": 12, "endOffset": 19}, {"referenceID": 10, "context": "Inspired by [10-14], we adopted a convolutional layer to capture the local relation pattern between adjacent songs.", "startOffset": 12, "endOffset": 19}, {"referenceID": 11, "context": "Inspired by [10-14], we adopted a convolutional layer to capture the local relation pattern between adjacent songs.", "startOffset": 12, "endOffset": 19}, {"referenceID": 12, "context": "Inspired by [10-14], we adopted a convolutional layer to capture the local relation pattern between adjacent songs.", "startOffset": 12, "endOffset": 19}, {"referenceID": 13, "context": "Inspired by [10-14], we adopted a convolutional layer to capture the local relation pattern between adjacent songs.", "startOffset": 12, "endOffset": 19}, {"referenceID": 8, "context": "Such kind of representation is widely used in the field of NLP [9-14, 16].", "startOffset": 63, "endOffset": 73}, {"referenceID": 9, "context": "Such kind of representation is widely used in the field of NLP [9-14, 16].", "startOffset": 63, "endOffset": 73}, {"referenceID": 10, "context": "Such kind of representation is widely used in the field of NLP [9-14, 16].", "startOffset": 63, "endOffset": 73}, {"referenceID": 11, "context": "Such kind of representation is widely used in the field of NLP [9-14, 16].", "startOffset": 63, "endOffset": 73}, {"referenceID": 12, "context": "Such kind of representation is widely used in the field of NLP [9-14, 16].", "startOffset": 63, "endOffset": 73}, {"referenceID": 13, "context": "Such kind of representation is widely used in the field of NLP [9-14, 16].", "startOffset": 63, "endOffset": 73}, {"referenceID": 15, "context": "Such kind of representation is widely used in the field of NLP [9-14, 16].", "startOffset": 63, "endOffset": 73}, {"referenceID": 8, "context": "Secondly, we used the word embedding technique, commonly used in NLP research [9-16], to gain a low-dimensional dense feature.", "startOffset": 78, "endOffset": 84}, {"referenceID": 9, "context": "Secondly, we used the word embedding technique, commonly used in NLP research [9-16], to gain a low-dimensional dense feature.", "startOffset": 78, "endOffset": 84}, {"referenceID": 10, "context": "Secondly, we used the word embedding technique, commonly used in NLP research [9-16], to gain a low-dimensional dense feature.", "startOffset": 78, "endOffset": 84}, {"referenceID": 11, "context": "Secondly, we used the word embedding technique, commonly used in NLP research [9-16], to gain a low-dimensional dense feature.", "startOffset": 78, "endOffset": 84}, {"referenceID": 12, "context": "Secondly, we used the word embedding technique, commonly used in NLP research [9-16], to gain a low-dimensional dense feature.", "startOffset": 78, "endOffset": 84}, {"referenceID": 13, "context": "Secondly, we used the word embedding technique, commonly used in NLP research [9-16], to gain a low-dimensional dense feature.", "startOffset": 78, "endOffset": 84}, {"referenceID": 14, "context": "Secondly, we used the word embedding technique, commonly used in NLP research [9-16], to gain a low-dimensional dense feature.", "startOffset": 78, "endOffset": 84}, {"referenceID": 15, "context": "Secondly, we used the word embedding technique, commonly used in NLP research [9-16], to gain a low-dimensional dense feature.", "startOffset": 78, "endOffset": 84}, {"referenceID": 8, "context": "Detailed explanations of the embedding process can be accessed in [9].", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "model in the field of NLP [9], the NN-rec was originally proposed as a next-basket recommender [8] with remarkable performance on two retail datasets [9].", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "model in the field of NLP [9], the NN-rec was originally proposed as a next-basket recommender [8] with remarkable performance on two retail datasets [9].", "startOffset": 95, "endOffset": 98}, {"referenceID": 8, "context": "model in the field of NLP [9], the NN-rec was originally proposed as a next-basket recommender [8] with remarkable performance on two retail datasets [9].", "startOffset": 150, "endOffset": 153}, {"referenceID": 17, "context": "WMF: The weighted matrix factorization (WMF) [18] is a", "startOffset": 45, "endOffset": 49}, {"referenceID": 14, "context": "continuous bag-of-words (CBOW) or the continuous skip-gram architecture [15].", "startOffset": 72, "endOffset": 76}, {"referenceID": 16, "context": "Therefore, it can be used on recommendation [17].", "startOffset": 44, "endOffset": 48}, {"referenceID": 0, "context": "was proposed to combine the common Markov chain with the matrix factorization technique [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 18, "context": "During training, we used cross-entropy as the cost function and Adagrad [20] with backpropagation as the update rule for each method.", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "We used Glorot weight [21] for initialization and the dropout ratio of 0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 20, "context": "For evaluations, we used the evaluation metrics proposed in [22], which are more suitable for evaluating top-N recommendation task.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "According to [4], the sequential relation between songs breaks once the continual listening pauses for a while.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "For instance, FPMC [1], PME [5], PRME [7] and Bi-gram [24] are first-order Markov models while HRM [2], Tribeflow [6], NN-rec [9] and our CNN-rec can predict", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "For instance, FPMC [1], PME [5], PRME [7] and Bi-gram [24] are first-order Markov models while HRM [2], Tribeflow [6], NN-rec [9] and our CNN-rec can predict", "startOffset": 28, "endOffset": 31}, {"referenceID": 6, "context": "For instance, FPMC [1], PME [5], PRME [7] and Bi-gram [24] are first-order Markov models while HRM [2], Tribeflow [6], NN-rec [9] and our CNN-rec can predict", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "For instance, FPMC [1], PME [5], PRME [7] and Bi-gram [24] are first-order Markov models while HRM [2], Tribeflow [6], NN-rec [9] and our CNN-rec can predict", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "For instance, FPMC [1], PME [5], PRME [7] and Bi-gram [24] are first-order Markov models while HRM [2], Tribeflow [6], NN-rec [9] and our CNN-rec can predict", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "For instance, FPMC [1], PME [5], PRME [7] and Bi-gram [24] are first-order Markov models while HRM [2], Tribeflow [6], NN-rec [9] and our CNN-rec can predict", "startOffset": 126, "endOffset": 129}], "year": 2016, "abstractText": "Recently, the next-item/basket recommendation system, which considers the sequential relation between bought items, has drawn attention of researchers. The utilization of sequential patterns has boosted performance on several kinds of recommendation tasks. Inspired by natural language processing (NLP) techniques, we propose a novel neural network (NN) based next-song recommender, CNN-rec, in this paper. Then, we compare the proposed system with several NN based and classic recommendation systems on the next-song recommendation task. Verification results indicate the proposed system outperforms classic systems and has comparable performance with the state-ofthe-art system.", "creator": "\u00fe\u00ff"}}}