{"id": "1606.07767", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Sampling-based Gradient Regularization for Capturing Long-Term Dependencies in Recurrent Neural Networks", "abstract": "Vanishing (and exploding) gradients effect is a common problem for recurrent neural networks with nonlinear activation functions which use backpropagation method for calculation of derivatives. Deep feedforward neural networks with many hidden layers also suffer from this effect. In this paper we propose a novel universal technique that makes the norm of the gradient stay in the suitable range. We construct a way to estimate a contribution of each training example to the norm of the long-term components of the target function s gradient. Using this subroutine we can construct mini-batches for the stochastic gradient descent (SGD) training that leads to high performance and accuracy of the trained network even for very complex tasks. We provide a straightforward mathematical estimation of minibatch s impact on for the gradient norm and prove its correctness theoretically. To check our framework experimentally we use some special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies. Our network can detect links between events in the (temporal) sequence at the range approx. 100 and longer.", "histories": [["v1", "Fri, 24 Jun 2016 17:31:02 GMT  (3731kb,D)", "http://arxiv.org/abs/1606.07767v1", null], ["v2", "Tue, 31 Jan 2017 21:30:29 GMT  (3987kb,D)", "http://arxiv.org/abs/1606.07767v2", null], ["v3", "Mon, 13 Feb 2017 21:25:26 GMT  (3916kb,D)", "http://arxiv.org/abs/1606.07767v3", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["artem chernodub", "dimitri nowicki"], "accepted": false, "id": "1606.07767"}, "pdf": {"name": "1606.07767.pdf", "metadata": {"source": "CRF", "title": "Sampling-based Gradient Regularization for Capturing Long-Term Dependencies in Recurrent Neural Networks", "authors": ["Artem Chernodub", "Dimitri Nowicki"], "emails": ["a.chernodub@gmail.com", "nowicki@nnteam.ogr.ua"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. BACKPROPAGATION MECHANISM REVISITED", "text": "Consider a simple recursive network (classification or regression) that in each step k receives an external input u (k), previous internal state z (k \u2212 1) and produces output y (k): a (k) = u (k) win + z (k \u2212 1) wrec + b, z (k) = f (a (k), y (k + 1) = g (k) wout), (1) here win is a matrix of input weights, wrec is matrix of recurrent weight, wout is matrix of output weights, a (k) is known as \"presynaptic activations\" z (k) is a state, f (\u00b7) and g (\u00b7) are nonlinear activation functions for hidden output and layer. In this work we always use tanh function for hidden layer and optionally softmax or linear function depending on the target problem (classification or regression)."}, {"heading": "III. GRADIENT REGULARIZATION", "text": "In [22] one can find an approach called \"pseudo-regularization\" to force recurring neural networks to capture long-term behavior, the idea being to control the backward propagation flow during training, where the target cost function L (w) has been modified to perform multi-objective optimization by adding an additional term responsible for the size of the backward propagated gradients: L (w) = E (w) + 1 (w), where E (w) is a target error function. (w) A regulator that prevents the gradients the gradients from \"disappearing\" is a gradient regularization rate. This regulator was a function of deltas. (w) = E (w, k) is a target error function. (w, k) is a regulator that \"exceeds\" these gradients. (7) The goal was set to create a mean norm of deltas (w)."}, {"heading": "IV. DIFFERENTIATION OF THE GRADIENT\u2019S NORM", "text": "D D D D D (D D D D D D D (D D D D D D D D D D D D (D D D D D D D D). D D D D (D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D (D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D D"}, {"heading": "V. SAMPLING-BASED GRADIENT REGULARIZATION", "text": "It is well known that the formation of recurring networks is a rather erratic process. Modifying the cost function to maintain the standard of gradients causes difficulties, even if the gradient vectors are changed in a relevant direction.The formation of SRNs on sequences containing short-term dependencies with modified cost function (6) results in a lower accuracy than training with standard cost function. The difficulty of training RNNs is related to the complexity of the fault interface. An additional goal of optimization can only complicate the training process. Also, inaccurate gradients cause a natural for RNs so-called \"butterfly effect,\" where a little disturbance at the beginning leads to large divergences at the end of the produced sequences. This is one reason why the use of more popular for DNN regulation methods such as the dropout method does not work well for RNs."}, {"heading": "A. Computationally Efficient Cached Algorithm", "text": "So it is a way in which we estimate the complexity of the BPTT if we calculate it in a simple way. We see that G 2H has multiplications of the matrices, so it has the complexity of 2H \u00b7 O (N3w), where the size of the recurrent matrix is wrei. We see that G 2H has multiplications of the matrices, so it has the complexity of 2H \u00b7 O (N3w), where the size of the recurrent matrix is wrec Nw \u00b7 Nw. However, 50% of these matrices are diagonal matrices that have multiplication complexity O (N2w)."}, {"heading": "VI. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experimental tasks", "text": "We refer to [11] and [20] to describe the experiment; they used pathological synthetic test sets from [9] that require long-term correlations; we used four problems from this sentence: \"addition,\" \"multiplication,\" \"temporal order,\" \"temporal order 3-bit.\" In the \"addition problem,\" the input consists of a sequence of random numbers marked with two random positions (one at the beginning and one in the middle of the sequence); the model must predict the sum of the two random numbers after seeing the entire sequence; the first position is sampled with [1; T \u2032 / 10], while the second position is sampled with [T \u2032 / 10; T \u2032 / 2], where T'aus [T; 11 / 10T] is the length of the sequence in the paper; for a description of other problems, see [20]."}, {"heading": "B. Description of experiments", "text": "We use SRNs with 100 hidden units and Tanh activation function. For the linear activation function of the starting layer, only the regression and the Softmax function were used for classification. To make a better comparison, two sets of 10 networks were itized and stored by random values. Therefore, for training with different methods, we only varied the training algorithms, with the initial weights of the neural networks being the same. \"Safe\" range [QMIN; QMAX] was set to [\u2212 1; 1]. As an optimization algorithm, we use Stochastic Gradient Descent (SGD), training speed \u03b1 = 10 \u2212 5... 10 \u2212 3, impulse \u00b5 = 0.9, size of the mini-stack is 10. The data set contains 20,000 samples for training, 1000 samples for validation and 10,000 samples for testing. The training process consists of 2000 epochs, each epoch consists of 50 iterations, i.e. 100,000 corrections of the weights at all."}, {"heading": "C. Experimental results", "text": "\"It is very important that we actually implement the changes.\" \"It is very important that we implement the changes in practice.\" \"It is very important that we implement the changes in practice.\" \"It is very important that we implement the changes in practice.\" \"It is very important that we take the changes in practice into our own hands.\" \"It is very important.\" \"It is very important that we implement the changes in practice.\" \"It is very important that we follow the rules in practice.\" \"It is very important that we apply the rules in practice.\" \"It is very important that we apply the rules in practice.\" \"It is very important that we apply the rules in practice.\" \"It is very important that we apply the rules in practice.\" It is very important that we apply the rules in practice. \""}, {"heading": "VII. CONCLUSION", "text": "We have developed a novel solution to the problem of exploding and disappearing gradient effects applied to Simple Recurrent Networks. Based on the estimation of the differential of the gradient standard, we can predict the influence of each minibatch. Using this technique, we are building an algorithm that controls the size of the gradient working exclusively on the presence of minbatches in the training sequence. We have demonstrated the mathematical correctness of this algorithm and introduced negative feedback mechanisms that prevent self-oscillations within the training process. This framework has been tested on the basis of a comprehensive set of suitable benchmarks for long-term prediction, the resulting accuracy exceeding the most well-known SRN learning algorithms by 10-20%. This paradigm could be generalized to deep and multi-layered recursive networks, which is the subject of our future research."}, {"heading": "ACKNOWLEDGMENTS", "text": "We thank FlyElephant (http: / / flyelephant.net) and Dmitry Spodarets for the kindly provided computing resources for our experiments."}], "references": [{"title": "Computational capabilities of recurrent narx neural networks", "author": ["B.G. Horne H.T. Siegelmann"], "venue": "IEEE Transactions on Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1997}, {"title": "Recurrent neural networks are universal approximators", "author": ["H.G. Zimmermann A.M. Schfer"], "venue": "In Lecture Notes in Computer Science, International Conference on Artificial Neural Networks (ICANN 2006),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Advanced Methods for Time Series Prediction Using Recurrent Neural Networks, chapter Advanced Methods for Time Series Prediction Using Recurrent Neural Networks, page 1536", "author": ["H. Cardot R. Bone"], "venue": "Intech, Croatia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Toyota prius hev neurocontrol and diagnostics", "author": ["D.V. Prokhorov"], "venue": "Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Recurrent neural network based language model", "author": ["et. al T. Mikolov", "M. Karafiat"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["G. Hinton A. Graves", "A.R. Mohamed"], "venue": "In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "On the properties of neural machine translation: Encoderdecoder approaches", "author": ["D. Bahdanau K. Cho", "B. van Merrienboer"], "venue": "In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation (SSST-8),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["P. Frasconi Y. Bengio", "P. Simard"], "venue": "IEEE Trans. Neural Networks,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1994}, {"title": "Long short-term memory", "author": ["S.J. Schmidhuber S. Hochreiter"], "venue": "Neural Computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Kalman Filtering and Neural Networks", "author": ["S. Haykin", "editor"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Learning recurrent neural networks with hessianfree optimization", "author": ["I. Sutskever J. Martens"], "venue": "In Proceedings of the 28th International Conference on Machine Learning (ICML),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "The echo state approach to analysing and training recurrent neural networks-with an erratum note", "author": ["Herbert Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Long short-term memory in echo state networks: Details of a simulation study", "author": ["H. Jaeger"], "venue": "Technical Report 27, Jacobs University,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Multilevel assembly neural architecture and processing of sequences", "author": ["EM Kussul", "DA Rachkovskij"], "venue": "Neurocomputers and Attention: Connectionism and neurocomputers,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1991}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["L. Fei-Fei A. Karpathy"], "venue": "In CVPR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "An empirical exploration of recurrent network architectures", "author": ["I. Sutskever R. Jozefowicz", "W. Zaremba"], "venue": "In Proceedings of the 32-nd International Conference on Machine Learning,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Statistical Language Models based on Neural Networks", "author": ["T. Mikolov"], "venue": "Ph.d. thesis, Brno University of Technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Advances in optimizing recurrent networks", "author": ["R. Pascanu Y. Bengio", "N. Boulanger-Lewandowski"], "venue": "In Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Training Recurrent Neural Networks", "author": ["Ilya Sutskever"], "venue": "Ph.d. thesis, University of Toronto,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Y. Bengio R. Pascanu"], "venue": "Technical report, Universite de Montreal,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Deep Learning", "author": ["A. Courville. Y. Bengio", "I.J. Goodfellow"], "venue": "book in preparation", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Training neuroemulators using multicriteria extended kalman filter and pseudoregularization for model reference adaptive neurocontrol", "author": ["A.N. Chernodub"], "venue": "In Proceedings of IEEE IV International Congress on Ultra Modern Telecommunications and Control Systems (ICUMT),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Untersuchungen zu dynamischen neuronalen netzen", "author": ["S. Hochreiter"], "venue": "Master\u2019s thesis, TU Munich,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1991}, {"title": "Recurrent neural network regularization", "author": ["O. Vinyals W. Zaremba", "I. Sutskever"], "venue": "In ICLR 2015,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recurrent Neural Networks (RNNs) are known as universal approximators of dynamic systems [1], [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "Recurrent Neural Networks (RNNs) are known as universal approximators of dynamic systems [1], [2].", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 134, "endOffset": 137}, {"referenceID": 3, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 157, "endOffset": 160}, {"referenceID": 4, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 180, "endOffset": 183}, {"referenceID": 5, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 204, "endOffset": 207}, {"referenceID": 6, "context": "Since RNNs are able to simulate any open dynamical system, they have a broad spectrum of applications such as time series forecasting [3], control of plants [4], language modeling [5], speech recognition [6], neural machine translation [7] and other domains.", "startOffset": 236, "endOffset": 239}, {"referenceID": 7, "context": "However, in practice training of SRNs using first-order optimization methods is difficult [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "Hochreiter and Schmidhuber designed a set of special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies [9].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "The solution could be using more advanced second-order optimization algorithms such as Extended Kalman Filter [10], LBFGS, Hessian-Free optimization [11], but they require much more memory and computational resources for state-of-the-art networks.", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "The solution could be using more advanced second-order optimization algorithms such as Extended Kalman Filter [10], LBFGS, Hessian-Free optimization [11], but they require much more memory and computational resources for state-of-the-art networks.", "startOffset": 149, "endOffset": 153}, {"referenceID": 11, "context": "Echo State Networks (ESNs) proposed by Jaeger [12] may be considered as big reservoirs of sparsely connected neurons and randomly initialized weights which produces chaotic dynamics.", "startOffset": 46, "endOffset": 50}, {"referenceID": 12, "context": "Experiments show that this may be enough for capturing longterm dynamics [13].", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "We also mention such an alternative to temporal neural networks as hierarchical sequence processing with auto-associative memories [14] that use distributed coding.", "startOffset": 131, "endOffset": 135}, {"referenceID": 8, "context": "Another approach that was specially designed for catching the long-term dependencies is Long-Short Term Memory (LSTM) [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 5, "context": "Currently it is probably the most popular family of RNNs models that shows stateof-the-art performance in several domains including speech recognition [6], image captioning [15] and neural machine translation.", "startOffset": 151, "endOffset": 154}, {"referenceID": 14, "context": "Currently it is probably the most popular family of RNNs models that shows stateof-the-art performance in several domains including speech recognition [6], image captioning [15] and neural machine translation.", "startOffset": 173, "endOffset": 177}, {"referenceID": 6, "context": "An idea of using input/forgetting gates inspired a lot of followers, Gated Recurrent Units (GRU) networks is probably one of the most successful of them [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 15, "context": "Finally, the united team from Google and Facebook performed a grand experiment on finding the best architecture for RNNs [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "Recent research shows the ability to train SRNs for long term dependencies up to 100 time steps and more using several new techniques [17], [18]\u2013[20].", "startOffset": 134, "endOffset": 138}, {"referenceID": 17, "context": "Recent research shows the ability to train SRNs for long term dependencies up to 100 time steps and more using several new techniques [17], [18]\u2013[20].", "startOffset": 140, "endOffset": 144}, {"referenceID": 19, "context": "Recent research shows the ability to train SRNs for long term dependencies up to 100 time steps and more using several new techniques [17], [18]\u2013[20].", "startOffset": 145, "endOffset": 149}, {"referenceID": 19, "context": "One of the most common methods for preventing the vanishing gradients effect from this pool is known as \u201d\u2018gradient regularization\u201d\u2019 [20], [21]; also it was independently proposed in [22] as \u201d\u2018method of pseudoregularization\u201d\u2019.", "startOffset": 132, "endOffset": 136}, {"referenceID": 20, "context": "One of the most common methods for preventing the vanishing gradients effect from this pool is known as \u201d\u2018gradient regularization\u201d\u2019 [20], [21]; also it was independently proposed in [22] as \u201d\u2018method of pseudoregularization\u201d\u2019.", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "One of the most common methods for preventing the vanishing gradients effect from this pool is known as \u201d\u2018gradient regularization\u201d\u2019 [20], [21]; also it was independently proposed in [22] as \u201d\u2018method of pseudoregularization\u201d\u2019.", "startOffset": 182, "endOffset": 186}, {"referenceID": 19, "context": "In derivations below we use a framework very similar to [20] but based on studying evolution of local gradients \u03b4(k) for the backpropagation procedure.", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20].", "startOffset": 123, "endOffset": 126}, {"referenceID": 22, "context": "Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20].", "startOffset": 151, "endOffset": 155}, {"referenceID": 19, "context": "Now we can use an intuitive understanding of exploding/vanishing gradients problem that was deeply investigated in classic [8], [23] and modern papers [18], [20].", "startOffset": 157, "endOffset": 161}, {"referenceID": 19, "context": "In [20] the power iteration method was used to formally analyze product of Jacobian matrices and obtain tight conditions for when the", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [22] one can find an approach called \u201cpseudoregularization\u201d for forcing the recurrent neural networks to capture long-term behavior.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "Similar, but more advanced approach called \u201cgradient regularization\u201d [21] was independently proposed by [20].", "startOffset": 69, "endOffset": 73}, {"referenceID": 19, "context": "Similar, but more advanced approach called \u201cgradient regularization\u201d [21] was independently proposed by [20].", "startOffset": 104, "endOffset": 108}, {"referenceID": 23, "context": "This is a reason why using popular for DNN regularization methods like dropout doesn\u2019t work well for RNNs [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 10, "context": "We refer to [11] and [20] for descripton of experiment.", "startOffset": 12, "endOffset": 16}, {"referenceID": 19, "context": "We refer to [11] and [20] for descripton of experiment.", "startOffset": 21, "endOffset": 25}, {"referenceID": 8, "context": "They used pathological synthetic test set from [9] that requires long-term correlations.", "startOffset": 47, "endOffset": 50}, {"referenceID": 19, "context": "For description of other problems, please, see [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 10, "context": "3: An illustration of the addition problem [11], a typical problem with pathological long term dependencies.", "startOffset": 43, "endOffset": 47}, {"referenceID": 19, "context": "01 as in [20].", "startOffset": 9, "endOffset": 13}], "year": 2016, "abstractText": "Vanishing (and exploding) gradients effect is a common problem for recurrent neural networks with nonlinear activation functions which use backpropagation method for calculation of derivatives. Deep feedforward neural networks with many hidden layers also suffer from this effect. In this paper we propose a novel universal technique that makes the norm of the gradient stay in the suitable range. We construct a way to estimate a contribution of each training example to the norm of the long-term components of the target functions gradient. Using this subroutine we can construct mini-batches for the stochastic gradient descent (SGD) training that leads to high performance and accuracy of the trained network even for very complex tasks. We provide a straightforward mathematical estimation of minibatch\u2019s impact on for the gradient norm and prove its correctness theoretically. To check our framework experimentally we use some special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies. Our network can detect links between events in the (temporal) sequence at the range 100 and longer.", "creator": "LaTeX with hyperref package"}}}