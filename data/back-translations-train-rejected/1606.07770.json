{"id": "1606.07770", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Captioning Images with Diverse Objects", "abstract": "We propose Novel Object Captioner (NOC), a deep visual semantic captioning model that can describe a large number of object categories not present in existing image-caption datasets. Recent captioning models are limited in their ability to scale and describe concepts outside of paired image-text corpora. Our model takes advantage of external sources - labeled images from object recognition datasets, and semantic knowledge extracted from unannotated text - and combines them to generate descriptions about novel objects. We propose minimizing a joint objective which can learn from diverse data sources and leverage distributional semantic embeddings, enabling the model to generalize and describe novel objects outside of image-caption datasets. We demonstrate that our model exploits semantic information to generate captions for hundreds of object categories in the ImageNet object recognition dataset that are not observed in image-caption training data, as well as many categories that are observed very rarely.", "histories": [["v1", "Fri, 24 Jun 2016 17:53:45 GMT  (1873kb,D)", "http://arxiv.org/abs/1606.07770v1", "9 pages, 3 figures"], ["v2", "Thu, 1 Dec 2016 20:54:17 GMT  (8155kb,D)", "http://arxiv.org/abs/1606.07770v2", "16 pages, 12 figures, 7 tables. New empirical results, more qualitative and quantitative results. Includes human evaluations"], ["v3", "Thu, 20 Jul 2017 18:06:27 GMT  (9001kb,D)", "http://arxiv.org/abs/1606.07770v3", "CVPR 2017 Camera ready version. 17 pages (8 + 9 supplement), 12 figures, 8 tables. Includes project pagethis http URL"]], "COMMENTS": "9 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["subhashini venugopalan", "lisa anne hendricks", "marcus rohrbach", "raymond mooney", "trevor darrell", "kate saenko"], "accepted": false, "id": "1606.07770"}, "pdf": {"name": "1606.07770.pdf", "metadata": {"source": "CRF", "title": "Captioning Images with Diverse Objects", "authors": ["Subhashini Venugopalan", "Lisa Anne Hendricks", "Marcus Rohrbach"], "emails": ["vsub@cs.utexas.edu", "lisa_anne@berkeley.edu", "rohrbach@berkeley.edu", "mooney@cs.utexas.edu", "trevor@eecs.berkeley.edu", "saenko@cs.uml.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most people who are able to move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, explore, move, move, move, move, explore, move, move, move, explore, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move, move"}, {"heading": "2 Related Work", "text": "Visual description has seen many different approaches over the years (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014), and more recently, deeper models have gained popularity due to their high performance and potential for end-to-end training. Our in-depth visual description frames first describe an image in a fixed-length vector and then generate a description by either conditioning text generation to image characteristics (Vinyals et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image characteristics and previously generated words in a multimodal space (Mao et al., 2015a; Kiros et al., 2015) before predicting the next word. Although most models present images with an intermedia representation from a conventional neural network (CNN) (such as fc7 activations from an object identification number representing CNN as a vector)."}, {"heading": "3 Novel Object Captioner (NOC)", "text": "Our NOC model is illustrated in Fig. 2. It not only learns from paired picture-caption data, but also uses semantic information from uncommented text and integrates it with a visual recognition model. By introducing auxiliary functions (objectives) and training different components together on multiple data sources, we obtain a visual description model that simultaneously learns an independent object recognition model and a language model. We start by training a simple LSTM-based language model (LM) (Sundermeyer et al., 2010) for sentence generation. Our LM includes dense representations of words from distributional embedding (GloVe, Pennington et al. (2014))), pre-trained on external text bodies. At the same time, we train a state-of-the-art visual recognition network to establish trust in the vocabulary of an image."}, {"heading": "3.1 Auxiliary Training Objectives", "text": "Our motivation for introducing auxiliary targets is to learn how to describe images without losing the ability to detect more specific objects. Typically, caption models integrate a visual classifier that is pre-trained on a source domain (e.g. ImageNet dataset) and then match it to the target domain (dataset caption). However, important information from the source dataset can be suppressed if similar information is not available, causing the network to forget (override weights) for objects that are not in the target domain. This is problematic in our scenario where the model relies on the source datasets to learn a wide variety of visual concepts that are not present in the target datasets. However, with pre-training and the complementary auxiliary targets, the model maintains its ability to detect a wider variety of objects and is encouraged to describe objects that are not present in the target datasets."}, {"heading": "3.2 Language Model with Semantic Embeddings", "text": "Our language model consists of the following components: a continuous sub-dimensional embedding space for words (Wglove), a single recurring hidden layer (LSTM), and two linear transformation layers in which the second layer (W Tglove) maps the vectors to the size of the vocabulary. Finally, a Softmax activation function is used on the output layer to generate a normalized probability distribution. Cross-entropy loss corresponding to the maximum probability value is used as the training objective.In addition to our common goal (Eqn.4), we also use semantic embedding in our language model to generate sentences in describing novel objects. Specifically, the initial embedding of the space (Wglove) is used to represent the input of (uniform) words in semantically meaningful, dense fixed-length vectors. While the final transformation layer (W Tglove) reproduces the attribution of a word-type force similar to the one previously created by the word-image force."}, {"heading": "3.3 Visual Classifier", "text": "The other major component of our model is the visual classifier. As a visual recognition network for the image classifier, we use a newer Convolutionary Neural Network (VGG-16 (Simonyan and Zisserman, 2014)). We modify the last layers of the network to include multi-label loss (Eqn. 1) to predict visual reliability across multiple labels in the vocabulary, and the rest of the classification network remains unchanged."}, {"heading": "4 Datasets", "text": "In this section we describe the image description dataset as well as the external text and image datasets used in our experiments."}, {"heading": "4.1 External Text Corpus (WebCorpus)", "text": "We extract sentences from Gigaword, the British National Corpus (BNC), UkWaC, and Wikipedia. Stanford CoreNLP 3.4.2 (Manning et al., 2014) was used to extract tokenizations, and this data set was used to train the LSTM language model. We use GloVe (Pennington et al., 2014) for dense word representation on the network, pre-trained to 6B tokens from external corporations, including Gigaword and Wikipedia. To create our LM vocabulary, we identified the 80,000 most common tokens from the combined external corpora, and further refine this vocabulary to a set of 72,700 words that also had GloVe embedding."}, {"heading": "4.2 Image Caption data", "text": "To evaluate empirically whether NOC is able to describe new objects, we use the training and testing set by Hendricks et al. (2016), which consists of a subset of MSCOCO (Lin et al., 2014) that excludes all sentences describing one of the following eight objects: bottle, bus, couch, microwave, pizza, bat, suitcase, or zebra. Subsequently, we evaluate how well NOC can generate descriptions of these eight words."}, {"heading": "4.3 Image data", "text": "We also evaluate qualitative sentences generated by NOC on approximately 700 different ImageNet (Russakovsky et al., 2014) objects that are not present in the MSCOCO dataset. We select this sentence by identifying objects that are present both in ImageNet and in our language corpus (vocabulary) but are not present in MSCOCO. The selected words include a variety of categories, including fine-grained categories (e.g. \"bloodhound\" chrysanthemum \"), adjectives (e.g.\" chiffon \") and introductory words (e.g.\" toad \"). Rare objects To understand whether additional data will allow NOC to better generate sentences about rare words, we select 52 objects that are located in the ImageNet as well as in the MSCOCO vocabulary. Selected words appear in the MSCOCO training set with varying frequency, with an average of 52 menus (at 27 times), and only four times during such training sentences."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Empirical Evaluation on MSCOCO", "text": "In fact, it is as if most of us are not able to obey the rules that they have imposed on themselves. (...) It is not as if they were able to respect the rules. (...) It is not as if they were able to respect the rules. (...) It is not as if they were able to respect the rules. (...) It is not as if they were able to respect the rules. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they were doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is as if they are doing it. (...) It is if they are doing it. (...) It is if they are doing it. (...) It is if they are doing it. (... it is if they are doing it. (...) It is if they are doing it."}, {"heading": "5.2 Scaling to ImageNet", "text": "To demonstrate the scalability of our proposed architecture, we describe objects in ImageNet for which no paired image-set data exists. Table 4 compares models on 638 novel object categories with the following metrics: (i) The description of novel objects (%) refers to the percentage of selected ImageNet objects mentioned in descriptions, i.e. for each novel word (e.g. \"otter\") the model should include the word (\"otter\") in at least one description of an ImageNet image of the object (otter). While DCC is able to recognize and describe 56.85% (363) of the selected ImageNet objects in descriptions, NOC recognizes several other objects and is able to describe 91.27% (582 of 638) ImageNet objects (otter). (ii) Accuracy refers to the percentage of images from each category in which the model is able to correctly identify and describe the category."}, {"heading": "6 Conclusion", "text": "We present a consistent traceable architecture that includes additional training objectives and distribution semantics to generate descriptions for object classes that are not seen in paired caption data. Our model surpasses previous caption frameworks in detecting new objects and generates descriptions of comparable quality. We demonstrate the labeling capabilities of our model on a pre-set of MSCOCO objects as well as on several hundred ImageNet objects. We also present an analysis of the contributions of different network modules, training targets and data sources. In addition, our model extends directly to the generation of captions for ImageNet objects that are rarely mentioned in the caption corpora."}, {"heading": "Acknowledgements", "text": "Lisa Anne Hendricks is supported by the NDSEG Fellowship. Trevor Darrell was partially supported by DARPA; AFRL; DoD MURI award N000141110688; NSF awards IIS-1212798, IIS1427425, and IIS-1536003, and the Berkeley Artificial Intelligence Research (BAIR) Lab. Raymond Mooney and Kate Saenko are partially supported by DARPA under the AFRL grant FA875013-2-0026 and a Google Grant. Mooney is also supported by the ONR ATL Grant N00014-11-1-010."}], "references": [{"title": "Meteor universal: Language specific translation evaluation for any target", "author": ["Denkowski", "Lavie2014] Michael Denkowski", "Alon Lavie"], "venue": null, "citeRegEx": "Denkowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Denkowski et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Donahue et al.2015] Jeff Donahue", "Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Devise: A deep visualsemantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["He et al.2016] Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep compositional captioning: Describing novel object categories without paired training data", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Hendricks et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hendricks et al\\.", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li Fei-Fei"], "venue": null, "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Multimodal neural language models", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Rich Zemel"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Unifying visualsemantic embeddings with multimodal neural language models. TACL", "author": ["Kiros et al.2015] Ryan Kiros", "Ruslan Salakhutdinov", "Richard S. Zemel"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Treetalk: Composition and compression of trees for image descriptions", "author": ["Vicente Ordonez", "Tamara L Berg", "UNC Chapel Hill", "Yejin Choi"], "venue": "TACL", "citeRegEx": "Kuznetsova et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kuznetsova et al\\.", "year": 2014}, {"title": "Is this a wampimuk? cross-modal mapping between distributional semantics and the visual world", "author": ["Elia Bruni", "Marco Baroni"], "venue": null, "citeRegEx": "Lazaridou et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2014}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Manning et al.2014] C. Manning", "M Surdeanu", "J Bauer", "J Finkel", "S J Bethard", "Dx McClosky"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Lin-", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Mao et al.2015a] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille"], "venue": "In ICLR", "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Learning like a child: Fast novel visual concept learning from sentence descriptions of images", "author": ["Mao et al.2015b] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan L. Yuille"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Midge: Generating image descriptions from computer", "author": ["Jesse Dodge", "Amit Goyal", "Kota Yamaguchi", "Karl Stratos", "Xufeng Han", "Alyssa Mensch", "Alexander C. Berg", "Tamara L. Berg", "Hal Daum\u00e9 III"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2012}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Norouzi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Norouzi et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "Proceedings of the Empiricial Methods in Natural Language Processing", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences. TACL", "author": ["Andrej Karpathy", "Quoc V. Le", "Christopher D. Manning", "Andrew Y. Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "Lstm neural networks for language modeling", "author": ["Sundermeyer", "R. Schluter", "H. Ney."], "venue": "INTERSPEECH.", "citeRegEx": "Sundermeyer et al\\.,? 2010", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2010}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Corpusguided sentence generation of natural images", "author": ["Yang et al.2011] Yezhou Yang", "Ching Lik Teo", "Hal Daum\u00e9 III", "Yiannis Aloimonos"], "venue": null, "citeRegEx": "Yang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 3, "context": "Modern visual classifiers (He et al., 2016; Simonyan and Zisserman, 2014) can recognize thousands of object categories, some of which are basic or entry-level (e.", "startOffset": 26, "endOffset": 73}, {"referenceID": 4, "context": "Recent work from Hendricks et al. (2016) shows that, to incorporate the vast knowledge of current visual recognition models without explicit paired captions, models must learn from external sources and learn to compose sentences about visual concepts which are infrequent or non-existent in image-description corpora as in Fig.", "startOffset": 17, "endOffset": 41}, {"referenceID": 4, "context": "Compared to previous work (Hendricks et al., 2016) for the task, our model is end-to-end trainable and provides improved performance.", "startOffset": 26, "endOffset": 50}, {"referenceID": 21, "context": "Visual Description has seen many different approaches over the years (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014), and more recently deep models have gained popularity because of their high performance and their potential for end-to-end training.", "startOffset": 69, "endOffset": 136}, {"referenceID": 14, "context": "Visual Description has seen many different approaches over the years (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014), and more recently deep models have gained popularity because of their high performance and their potential for end-to-end training.", "startOffset": 69, "endOffset": 136}, {"referenceID": 8, "context": "Visual Description has seen many different approaches over the years (Yang et al., 2011; Mitchell et al., 2012; Kuznetsova et al., 2014), and more recently deep models have gained popularity because of their high performance and their potential for end-to-end training.", "startOffset": 69, "endOffset": 136}, {"referenceID": 20, "context": "Deep visual description frameworks first encode an image into a fixed length feature vector and then generate a description by either conditioning text generation on image features (Vinyals et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al.", "startOffset": 181, "endOffset": 253}, {"referenceID": 1, "context": "Deep visual description frameworks first encode an image into a fixed length feature vector and then generate a description by either conditioning text generation on image features (Vinyals et al., 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al.", "startOffset": 181, "endOffset": 253}, {"referenceID": 6, "context": ", 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al., 2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word.", "startOffset": 119, "endOffset": 178}, {"referenceID": 7, "context": ", 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al., 2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word.", "startOffset": 119, "endOffset": 178}, {"referenceID": 4, "context": "Though most models represent images with an intermediate representation from a convolutional neural network (CNN) (such as fc7 activations from an object recognition CNN), other models represent images as a vector of confidences over a fixed number of visual concepts (Hendricks et al., 2016; Fang et al., 2015).", "startOffset": 268, "endOffset": 311}, {"referenceID": 6, "context": "On the caption generation side, recurrent networks (RNNs) are a popular choice to model language, but log bilinear models (Kiros et al., 2014) and maximum entropy language models (Fang et al.", "startOffset": 122, "endOffset": 142}, {"referenceID": 1, "context": ", 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al., 2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word. Though most models represent images with an intermediate representation from a convolutional neural network (CNN) (such as fc7 activations from an object recognition CNN), other models represent images as a vector of confidences over a fixed number of visual concepts (Hendricks et al., 2016; Fang et al., 2015). Also, in almost all cases, the parameters of the visual pipeline are initialized with weights trained on the ImageNet classification task. On the caption generation side, recurrent networks (RNNs) are a popular choice to model language, but log bilinear models (Kiros et al., 2014) and maximum entropy language models (Fang et al., 2015) have also been explored. Our model is similar to the CNN-RNN frameworks in Mao et al. (2015a) and Hendricks et al.", "startOffset": 8, "endOffset": 979}, {"referenceID": 1, "context": ", 2015; Donahue et al., 2015; Karpathy and Fei-Fei, 2015) or embedding image features and previously generated words into a multimodal space (Mao et al., 2015a; Kiros et al., 2014; Kiros et al., 2015) before predicting the next word. Though most models represent images with an intermediate representation from a convolutional neural network (CNN) (such as fc7 activations from an object recognition CNN), other models represent images as a vector of confidences over a fixed number of visual concepts (Hendricks et al., 2016; Fang et al., 2015). Also, in almost all cases, the parameters of the visual pipeline are initialized with weights trained on the ImageNet classification task. On the caption generation side, recurrent networks (RNNs) are a popular choice to model language, but log bilinear models (Kiros et al., 2014) and maximum entropy language models (Fang et al., 2015) have also been explored. Our model is similar to the CNN-RNN frameworks in Mao et al. (2015a) and Hendricks et al. (2016).", "startOffset": 8, "endOffset": 1007}, {"referenceID": 10, "context": "Novel object captioning Recently Mao et al. (2015b) proposed an approach, that extends a model\u2019s capability to describe a small set of novel concepts (e.", "startOffset": 33, "endOffset": 52}, {"referenceID": 4, "context": "On the other hand, Hendricks et al. (2016) introduce a model that can describe many objects already existing in English corpora and object recognition datasets (ImageNet) but not in the caption corpora (e.", "startOffset": 19, "endOffset": 43}, {"referenceID": 4, "context": "On the other hand, Hendricks et al. (2016) introduce a model that can describe many objects already existing in English corpora and object recognition datasets (ImageNet) but not in the caption corpora (e.g. pheasant, otter). Our focus is on the latter case. Hendricks et al. (2016) integrate information from external text and visual sources, and explicitly transfer parameters from objects seen in image-caption data to unseen ImageNet objects to generate descriptions for the new objects.", "startOffset": 19, "endOffset": 283}, {"referenceID": 2, "context": "egories (Frome et al., 2013; Norouzi et al., 2013), as well as retrieval of images with text (Socher et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 15, "context": "egories (Frome et al., 2013; Norouzi et al., 2013), as well as retrieval of images with text (Socher et al.", "startOffset": 8, "endOffset": 50}, {"referenceID": 18, "context": ", 2013), as well as retrieval of images with text (Socher et al., 2014; Lazaridou et al., 2014).", "startOffset": 50, "endOffset": 95}, {"referenceID": 9, "context": ", 2013), as well as retrieval of images with text (Socher et al., 2014; Lazaridou et al., 2014).", "startOffset": 50, "endOffset": 95}, {"referenceID": 7, "context": ", wn\u22121 and an image are projected into a joint embedding space before the next word in a caption, wn, is generated (Mao et al., 2015a; Kiros et al., 2015).", "startOffset": 115, "endOffset": 154}, {"referenceID": 19, "context": "We start by first training a simple LSTMbased language model (LM) (Sundermeyer et al., 2010) for sentence generation.", "startOffset": 66, "endOffset": 92}, {"referenceID": 16, "context": "Our LM incorporates dense representations for words from distributional embeddings (GloVe, Pennington et al. (2014)) pre-trained on external text corpora.", "startOffset": 91, "endOffset": 116}, {"referenceID": 13, "context": "These distributional embeddings (Mikolov et al., 2013; Pennington et al., 2014) share the property that words that are semantically similar have", "startOffset": 32, "endOffset": 79}, {"referenceID": 16, "context": "These distributional embeddings (Mikolov et al., 2013; Pennington et al., 2014) share the property that words that are semantically similar have", "startOffset": 32, "endOffset": 79}, {"referenceID": 10, "context": "2 (Manning et al., 2014) was used to extract tokenizations.", "startOffset": 2, "endOffset": 24}, {"referenceID": 16, "context": "the dense word representation in the network, we use GloVe (Pennington et al., 2014) pre-trained on 6B tokens of external corpora including Gigaword and Wikipedia.", "startOffset": 59, "endOffset": 84}, {"referenceID": 4, "context": "To empirically evaluate the ability of NOC to describe new objects we use the training and test set from Hendricks et al. (2016). The dataset consists of a subset of MSCOCO (Lin et al.", "startOffset": 105, "endOffset": 129}, {"referenceID": 4, "context": "Table 1: COCO Compositional Captioning: F1 scores of NOC (our model) and DCC (Hendricks et al., 2016)", "startOffset": 77, "endOffset": 101}, {"referenceID": 4, "context": "We empirically evaluate the ability of our proposed model to describe novel objects by following the experimental setup of (Hendricks et al., 2016).", "startOffset": 123, "endOffset": 147}, {"referenceID": 4, "context": "This compares the F1 score achieved by NOC to the previously existing method, DCC (Hendricks et al., 2016), and demonstrates that NOC is able to integrate new vocabulary words into description better for every object in the held-out dataset except \u201ccouch\u201d and \u201cmicrowave\u201d, outperforming DCC considerably.", "startOffset": 82, "endOffset": 106}, {"referenceID": 1, "context": "(2016) (DCC), as well as a competitive image captioning model - LRCN (Donahue et al., 2015) trained only on the held-out MSCOCO dataset.", "startOffset": 69, "endOffset": 91}, {"referenceID": 3, "context": "We compare our NOC model to results from Hendricks et al. (2016) (DCC), as well as a competitive image captioning model - LRCN (Donahue et al.", "startOffset": 41, "endOffset": 65}, {"referenceID": 4, "context": "(Hendricks et al., 2016) on % of novel classes described,", "startOffset": 0, "endOffset": 24}], "year": 2016, "abstractText": "We propose Novel Object Captioner (NOC), a deep visual semantic captioning model that can describe a large number of object categories not present in existing image-caption datasets. Recent captioning models are limited in their ability to scale and describe concepts outside of paired image-text corpora. Our model takes advantage of external sources labeled images from object recognition datasets, and semantic knowledge extracted from unannotated text and combines them to generate descriptions about novel objects. We propose minimizing a joint objective which can learn from diverse data sources and leverage distributional semantic embeddings, enabling the model to generalize and describe novel objects outside of imagecaption datasets. We demonstrate that our model exploits semantic information to generate captions for hundreds of object categories in the ImageNet object recognition dataset that are not observed in imagecaption training data, as well as many categories that are observed very rarely.", "creator": "LaTeX with hyperref package"}}}