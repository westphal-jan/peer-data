{"id": "1606.07783", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding", "abstract": "We investigate the usage of convolutional neural networks (CNNs) for the slot filling task in spoken language understanding. We propose a novel CNN architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context. Moreover, it combines the information from the past and the future words for classification. Our proposed CNN architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an F1-score of 95.61% on the ATIS benchmark dataset without using any additional linguistic knowledge and resources.", "histories": [["v1", "Fri, 24 Jun 2016 18:35:56 GMT  (36kb,D)", "http://arxiv.org/abs/1606.07783v1", "Accepted at Interspeech 2016"]], "COMMENTS": "Accepted at Interspeech 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ngoc thang vu"], "accepted": false, "id": "1606.07783"}, "pdf": {"name": "1606.07783.pdf", "metadata": {"source": "CRF", "title": "Sequential Convolutional Neural Networks for Slot Filling in Spoken Language Understanding", "authors": ["Ngoc Thang Vu"], "emails": ["thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": "1. Introduction", "text": "In the sentence I would like to fly from Munich to Rome, an SLU system should mark Munich as the departure city of a journey and Rome as the arrival city. All other words that do not correspond to real slots are then given an artificial class O. Traditional approaches to this task are used generative models, such as hidden Markov models (HMM), or discriminatory models, such as contingent random fields (CRF) [2, 3]. More recently, neural network models (NN models) have been used, which successfully applied recursive neural networks (RNs) and revolutionary neural networks (CNNs) to this task [4, 5, 6, 8, 8]. Overall, RNNNNs outperform other NN models and achieved the results of the prediction on the ATIS benchmark."}, {"heading": "2. Related Work", "text": "Neural network models such as RNNs and CNNs have been used in a wide range of natural language processing tasks [12]. Another trend is to use distributed word representations [19, 20] as input for sequence labeling [14, 15] or for modelling larger units such as phrases [16] or sentences [17, 18]. In both models, distributed word representations [19, 20] were used as input. In the field of language understanding, neural networks were also used to determine intention or semantic classification of utterances [21, 22]. In the task of filling slots, RNNNs [4, 5] and their expansions [7, 8] were best used not only for traditional approaches, but also for other models of understanding neural networks [6] and defined the state-of-art results in ATIS benchmark data. Recently, it was shown that the results of this study [1, 9] are obtained in the evaluation of N1, N47, but only in the results of individual networks."}, {"heading": "3. Bi-directional Sequential CNN", "text": "This section describes the architecture of the bi-directional sequential CNN (bi-sCNN) presented in Figure 1. It contains insar Xiv: 160 6.07 783v 1 [cs.C L] 24 Jun 2016 three main components: a vanilla sequential CNN, an extended surrounding context and a bi-directional extension."}, {"heading": "3.1. Model", "text": "To avoid the boundary effect, future filler words are also embedded as input in the CNN context. There are several options that include the input matrix: applying 1D filters to each dimension independently of each other, or applying 2D filters that include some or all dimensions of word embedding. In this paper, we use 2D filters f |) that include all input matrix: applying 1D filters to each dimension independently of each other, or applying 2D filters that include some or all dimensions of word embedding, which is described by the following equation: (w-Filter f) (x, y) = d-Filter."}, {"heading": "3.2. Training objective function", "text": "It was shown in [9] that the use of ranking losses is more accurate than the cross entropy to train the model for this task. One reason might be that the network is not forced to learn a pattern for the O-class that actually does not exist. In this essay, we compare two different types of ranking loss functions. The first function is the well-known hinge loss function: L = max (0, 1 \u2212 s\u03b8 (wt) y + + s\u03b8 (wt) c \u2212) (5) with s\u03b8 (wt) y + and s\u03b8 (wt) c \u2212 as a result of the target class and the incorrectly predicted class of the model with the current word w or. This loss function maximizes the margin between these two classes. The second was proposed by Dos Santos et al. [23] and used in [9] to achieve the current best performance on the slot fill task. Instead of using the softmax activation function, we form a matrix containing the different classes from the sporters of the classes."}, {"heading": "L = log(1 + exp(\u03b3(m+ \u2212 s\u03b8(wt)y+)))", "text": "+ log (1 + exp (\u03b3 (m \u2212 + s\u03b8 (wt) c \u2212)))) (7) with s\u03b8 (wt) y + and s\u03b8 (wt) c \u2212 as values for classes y + and c \u2212. The parameter \u03b3 controls the punishment of prediction errors and m + and m \u2212 are margins for the correct and wrong classs. \u03b3, m + and m \u2212 are hyperparameters that can be adjusted to the development theorem. For class O, only the second sum of the equation 7 is calculated during the training, i.e. the model does not learn a pattern for class O, but nevertheless increases its difference to the best competitive etiquette. Moreover, it implicitly solves the problem of unbalanced data, since the number of data points in class O is much larger than in other classes. During the test, the model predicts class O if the values for all other classes < 0."}, {"heading": "3.3. Comparison with other neural models", "text": "The information flow of the proposed model is similar to a bidirectional RNN. Instead of using the recursive architecture to store the information from a long context, we use a Convolutionary Operator to scan all the n-grams in the contexts and find the most important features with maximum pooling. In each step, the most important features are then learned regardless of the previous time step. This is an advantage over bidirectional RNNNs when the previous word is a word of class O and the current word does not belong to class O, because the information used to predict class O is not helpful to predict other classes. Another difference is the integration of future information. In the backward RNN model, the sentence is scanned from right to left, which contradicts the nature of languages such as English. In contrast, CNN maintains the correct order of the sentence and searches for important n-grams. Another interpretation of this model is a common training of a forward NNN context with the current N as the input of a forward N takes the N for its context."}, {"heading": "4. Experimental Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data", "text": "To compare our work with methods already studied, we report on the results of the widely used ATIS dataset [24, 25]. This dataset comes from the field of air travel and consists of audio recordings of loudspeakers making travel bookings. All words are labeled with a semantic label in BIO format (B: start, I: inside, O: outside), e.g. New York contains two words New and York and is therefore labeled with B-from place name or I-from place name. Words that do not have semantic labels are labeled with O. Overall, the number of semantic labels is 127, including the class O. Training data consists of 4,978 sentences and 56,590 words. The test set consists of 893 sentences and 9,198 words. To evaluate our models, we used the script provided in the text CoNLL Shared Task 20001 in accordance with other related work."}, {"heading": "4.2. Model training", "text": "We used the Theano library [26] to implement the model. To train the model, stochastic gradient descent (SGD) was used. We performed a 5-fold cross-validation to adjust the hyperparameters, the learning rate was maintained constant during the first 10 eras, then we halved the learning rate after each epoch and stopped the training after 25 epochs. Note 1http: / / www.cnts.ua.ac.be / conll2000 / chunking / that using more advanced techniques such as AdaGrad [27] and AdaDelta [28] with the simple learning rate plan described above, we did not improve on SGD. As the learning plan does not require cross-validation, we trained the final best model with the full training dataset. Table 1 shows the hyperparameters used for all CNN models."}, {"heading": "4.3. Results", "text": "Five left context words, five right context words and the current word form the input of a forward-facing neural network with a hidden layer of size 100. We achieved an F1 score of 94.23% and 94.14% with this simple forward-facing network, respectively. Table 2 summarizes the performance of the ATIS test set with various CNN architectural constellations, and the results show that the context information from the past is more important than the future context. However, the future context seems to provide meaningful information as its combination leads to better results. Furthermore, the comparison between two different combinations of previous and future context (concatenation vs. addition) suggests that the information should not be confused by addition. Finally, the results in Table 2 also show that the use of the ranking loss function proposed in [23] exceeds the hinge loss function."}, {"heading": "5. Analysis", "text": "We analyzed the choice of context length, the effects of including the current word in its surrounding context, and the most important n-grams detected."}, {"heading": "5.1. Context length", "text": "The number of parameters remained unchanged when decreasing or increasing the context length. Short context means information loss, while a long context length potentially contributes to noise in the input of the model. Table 3 shows that F1Scores increased the context length from 5 to 9. However, increasing the context length to 10 and 11 slightly decreased the results, but the F1 scores remained fairly stable at 95.5%. This confirms our hypothesis that a longer context adds noise to the input, while the model is still able to extract the important information for the slot prediction."}, {"heading": "5.2. Surrounding context", "text": "Table 4 summarizes the F1 score without using the current word or context with different lengths of the surrounding contexts, showing the strong impact of including the current word with its surrounding context in the CNN final result. Excluding the current word, the F1 score dropped significantly to 92.01%. Successive addition of the current word and increasing its surrounding context to three left and three right adjacent words resulted in better performance. However, increasing the surrounding context to four lowered the F1 score, and the best F1 score was achieved with three left and three right adjacent words."}, {"heading": "5.3. Most important n-grams", "text": "We analyzed the most significant patterns for the four most common semantic slots in the test data. For each of them, we present up to three n-grams, which contributed the most to evaluating the correctly classified test points. To calculate the most important n-grams, we first determined the position of the maximum contribution to the point product and traced it back to the corresponding characteristic card. Based on the maximum pooling, we were able to trace and identify the n-grams used. To obtain the results shown in Table 5, we ranked the n-grams selected as the most important characteristics in all sentences by frequency and selected the most common ones. Table 5 shows that the model learned something significant for this task. For example, a pattern such as flights from A to B was used to predict the place name, while the model used only A to B or B to predict place names. Other examples include patterns such as afternoon, evening and night that appeared immediately after the departure date."}, {"heading": "6. Comparison with state of the art", "text": "Table 6 lists several previous results of the ATIS dataset, including our best results. The proposed R-bi-sCNN surpasses the previously best ranking of bidirectional RNN (R-bi-RNN). A more detailed comparison with R-bi-RNN shows that R-bisCNN performed just as well in the frequent semantic slots as R-bi-RNN in the rare slots. For example, rare slots such as toloc.country name, days code, period of day that appeared less than six times in the training data were correctly predicted with the R-bi-sCNN model, but not with R-bi-RNN."}, {"heading": "7. Conclusions", "text": "Our novel CNN architecture - bi-directional sequential CNN - takes into account past and future information with obtained order information and pays particular attention to the current word and its surrounding contexts. To train the model, we compared two different rank lens functions. Our results showed that it is helpful not to force the model to learn an O-class pattern to improve final performance. Finally, our bi-directional sequential CNN achieves state-of-the-art results with an F1 score of 95.61% on the ATIS benchmark dataset without using additional linguistic knowledge and resources. As a future work, we aim to evaluate the proposed model on other datasets (e.g. data presented in [29, 30])."}, {"heading": "8. Acknowledgements", "text": "This work was funded by the German Research Foundation (DFG), Collaborative Research Center 732 Incremental Specification in Context, Project A8, at the University of Stuttgart."}, {"heading": "9. References", "text": "[1] Y. Wang, L. Deng, and A. Acero. Spoken Language Understand-ing An Introduction to the Statistical Framework, IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 16-31, 2005. [2] J. Lafferty, A. McCallum, and F. P ereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data, in Proc. of ICML, 2001. [3] Y. Wang, L. Deng, and A. AceroSemantic Frame Based on the Language Language Language, in Chapter 3, Spoken Language Language Data, in XiXiXiXiantic Languages for Extracting Semantic Information from Speech, pp. 35-80, Wiley, 2011. [4] K. Yao, G. Zweig, M. Hwang, Y. Shi, and D. Yu, Recurrent neural Networks for Understanding, pp. 35-80, Wiley, 2011."}], "references": [{"title": "Spoken Language Understanding An Introduction to the Statistical Framework", "author": ["Y. Wang", "L. Deng", "A. Acero"], "venue": "IEEE Signal Processing Magazine, vol. 22, no. 5, pp. 16-31", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "and F", "author": ["J. Lafferty", "A. McCallum"], "venue": "P ereira. Conditional random fields: Probabilistic models for segmenting and labeling sequence data, in Proc. of ICML", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "and A", "author": ["Y. Wang", "L. Deng"], "venue": "AceroSemantic Frame Based Spoken Language Understanding, in Chapter 3, Spoken Language Understanding: Systems for Extracting Semantic Information from Speech, pp. 35-80, Wiley", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Recurrent neural networks for language understanding", "author": ["K. Yao", "G. Zweig", "M. Hwang", "Y. Shi", "D. Yu"], "venue": "Proc. of Interspeech", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["G. Mesnil", "Y. Dauphin", "K. Yao", "Y. Bengio", "L. Deng", "D. Hakkani- Tur", "X. He", "L. Heck", "G. Tur", "D. Yu", "G. Zweig"], "venue": "IEEE/ACM Trans. on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 530-539", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["P. Xu", "R. Sarikaya"], "venue": "Proc. of ASRU", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["K. Yao", "B. Peng", "Y. Zhang", "D. Yu", "G. Zweig", "Y. Shi"], "venue": "Proc. of SLT", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent Neural Networks with External Memory for Language Understanding", "author": ["B. Peng", "K. Yao"], "venue": "arXiv", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Bi-directional Recurrent Neural Network with Ranking Loss for Spoken Language Understanding", "author": ["N.T. Vu", "P. Gupta", "H. Adel", "H. Schuetze"], "venue": "Proc. of ICASSP", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient flow in recurrent nets: the difficulty of learning long-term dependencies", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "S. C. Kremer and J. F. Kolen, editors, A Field Guide to Dynamical Recurrent Neural Networks. IEEE Press", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735?1780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Stefan Kombrink", "author": ["T. Mikolov"], "venue": "Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur, Extensions of recurrent neural network based language model, in Proc. of ICASSP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "B", "author": ["K. Cho"], "venue": "van Merrienboer, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio Learning phrase representations using RNN encoder-decoder for statistical machine translation, in Proc. of EMNLP", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "A unified architecture for natural language processing: deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proc. of ICML", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research, vol. 12", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "MultiGranCNN: An Architecture for General Matching of Text Chunks on Multiple Levels of Granularity", "author": ["Y. Wenpeng", "H. Schtze"], "venue": "Proc. of ACL", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.2188", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "arXiv preprint arXiv:1408.5882", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "Proc. of NIPS", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2000}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Proc. of Workshop at ICLR", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Use of Kernel Deep Convex Networks and End-To-End Learning for Spoken Language Understanding", "author": ["L. Deng", "G. Tur", "X. He", "D. Hakkani-Tur"], "venue": "Proc. of SLT", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Towards Deeper Understanding Deep Convex Networks for Semantic Utterance Classification", "author": ["G. Tur", "L. Deng", "D. Hakkani-Tur", "X. He"], "venue": "Proc. of ICASSP", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Classifying relations by ranking with convolutional neural networks", "author": ["C.N. Dos Santos", "B. Xiang", "B. Zhou"], "venue": "Proc. of ACL", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "The ATIS spoken language systems pilot corpus", "author": ["C. Hemphill", "J. Godfrey", "G. Doddington"], "venue": "Proc. of the DARPA speech and natural language workshop", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1990}, {"title": "Evaluation of spoken language systems: The ATIS domain", "author": ["P. Price"], "venue": "Proc. of the Third DARPA Speech and Natural Language Workshop. Morgan Kaufmann", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1990}, {"title": "Y", "author": ["F. Bastien", "P. Lamblin", "R. Pascanu", "J. Bergstra", "I.J. Goodfellow", "A. Bergeron", "N. Bouchard"], "venue": "and Bengio, Y. Theano: new features and speed improvements, in Proc. of Deep Learning and Unsupervised Feature Learning NIPS Workshop", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research, vol. 12, pp. 2121-2159", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["M.D. Zeiler"], "venue": "CoRR, abs/1212.5701", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "What is left to be understood in ATIS", "author": ["G. Tur", "D. Hakkani-Tur", "L. Heck"], "venue": "Proc. of SLT", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Comparing stochastic approaches to spoken language understanding in multiple languages", "author": ["S. Hahn", "M. Dinarelli", "C. Raymond", "F. Lefevre", "P. Lehnen", "R.D. Mori", "A. Moschitti", "H. Ney", "G. Riccardi"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, pp. 1569-1583", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Traditional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random fields (CRF) [2, 3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 1, "context": "Traditional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random fields (CRF) [2, 3].", "startOffset": 167, "endOffset": 173}, {"referenceID": 2, "context": "Traditional approaches for this task used generative models, such as hidden markov models (HMM) [1], or discriminative models, such as conditional random fields (CRF) [2, 3].", "startOffset": 167, "endOffset": 173}, {"referenceID": 3, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 4, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 5, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 6, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 7, "context": "More recently, neural network (NN) models, such as recurrent neural networks (RNNs) and convolutional neural networks (CNNs) have been applied successfully to this task [4, 5, 6, 7, 8].", "startOffset": 169, "endOffset": 184}, {"referenceID": 8, "context": "Overall, RNNs outperformed other NN models and achieved the state-of-the-art results on the ATIS benchmark dataset [9].", "startOffset": 115, "endOffset": 118}, {"referenceID": 9, "context": "It is, however, well known that it is difficult to train an RNN due to the vanishing gradient problem [10].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "Introducing long shortterm memory (LSTM) [11] or other variants of LSTM such as the gated recurrent unit (GRU) can solve this problem but, in turn increases the number of parameters significantly.", "startOffset": 41, "endOffset": 45}, {"referenceID": 7, "context": "Previous results reported in [8] did not show any improvement on the ATIS data set using LSTM or GRU.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "Previous research in [6] showed promising results on the slot filling task.", "startOffset": 21, "endOffset": 24}, {"referenceID": 11, "context": "Vanilla RNNs or their extensions such as LSTMs or GRUs showed their success in many different tasks such as language modeling [12] or machine translation [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "Vanilla RNNs or their extensions such as LSTMs or GRUs showed their success in many different tasks such as language modeling [12] or machine translation [13].", "startOffset": 154, "endOffset": 158}, {"referenceID": 13, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 76, "endOffset": 84}, {"referenceID": 14, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 76, "endOffset": 84}, {"referenceID": 15, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 126, "endOffset": 130}, {"referenceID": 16, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 144, "endOffset": 152}, {"referenceID": 17, "context": "Another trend is to use convolutional neural networks for sequence labeling [14, 15] or modeling larger units such as phrases [16] or sentences [17, 18].", "startOffset": 144, "endOffset": 152}, {"referenceID": 18, "context": "For both models, distributed representations of words [19, 20] are used as input.", "startOffset": 54, "endOffset": 62}, {"referenceID": 19, "context": "For both models, distributed representations of words [19, 20] are used as input.", "startOffset": 54, "endOffset": 62}, {"referenceID": 20, "context": "In the spoken language understanding research area, neural networks have also been applied to intent determination or semantic utterance classification tasks [21, 22].", "startOffset": 158, "endOffset": 166}, {"referenceID": 21, "context": "In the spoken language understanding research area, neural networks have also been applied to intent determination or semantic utterance classification tasks [21, 22].", "startOffset": 158, "endOffset": 166}, {"referenceID": 3, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 32, "endOffset": 38}, {"referenceID": 4, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 32, "endOffset": 38}, {"referenceID": 6, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 60, "endOffset": 66}, {"referenceID": 7, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 60, "endOffset": 66}, {"referenceID": 5, "context": "For the slot filling task, RNNs [4, 5] and their extensions [7, 8] outperformed not only traditional approaches but also other neural network related models [6] and defined the state-of-the-art results on the ATIS benchmark data set.", "startOffset": 157, "endOffset": 160}, {"referenceID": 8, "context": "Recently it was shown in [9] that applying ranking loss to train the model is effective for tasks that involve an artificial class like O.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "The only previous study using convolutional neural networks was presented in [6] showing promising results.", "startOffset": 77, "endOffset": 80}, {"referenceID": 8, "context": "As reported in [9], information not only from the past but also from the future contributes to the recognition accuracy.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "It was shown in [9] that using ranking loss is more accurate than cross entropy to train the model for this task.", "startOffset": 16, "endOffset": 19}, {"referenceID": 22, "context": "[23] and used in [9] to achieve the current best performance on the slot filling task till now.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[23] and used in [9] to achieve the current best performance on the slot filling task till now.", "startOffset": 17, "endOffset": 20}, {"referenceID": 8, "context": "We use the same ranking loss function as in [9] to train the CNNs.", "startOffset": 44, "endOffset": 47}, {"referenceID": 14, "context": "This is an advantage of this model over the CNN model proposed in [15] which has problems identifying the current word for labeling.", "startOffset": 66, "endOffset": 70}, {"referenceID": 23, "context": "To compare our work with previously studied methods, we report results on the widely used ATIS dataset [24, 25].", "startOffset": 103, "endOffset": 111}, {"referenceID": 24, "context": "To compare our work with previously studied methods, we report results on the widely used ATIS dataset [24, 25].", "startOffset": 103, "endOffset": 111}, {"referenceID": 25, "context": "We used the Theano library [26] to implement the model.", "startOffset": 27, "endOffset": 31}, {"referenceID": 26, "context": "be/conll2000/chunking/ that with more advanced techniques like AdaGrad [27] and AdaDelta [28] we did not achieve improvements over SGD with the described simple learning rate schedule.", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "be/conll2000/chunking/ that with more advanced techniques like AdaGrad [27] and AdaDelta [28] we did not achieve improvements over SGD with the described simple learning rate schedule.", "startOffset": 89, "endOffset": 93}, {"referenceID": 14, "context": "We adopted the window approach proposed in [15] as the baseline system.", "startOffset": 43, "endOffset": 47}, {"referenceID": 22, "context": "Finally, results in Table 2 also reveal that using the ranking loss function proposed in [23] outperforms the hinge loss function.", "startOffset": 89, "endOffset": 93}, {"referenceID": 4, "context": "Methods F1-score CRF [5] 92.", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "94 simple RNN [4] 94.", "startOffset": 14, "endOffset": 17}, {"referenceID": 5, "context": "11 CNN [6] 94.", "startOffset": 7, "endOffset": 10}, {"referenceID": 6, "context": "35 LSTM [7] 94.", "startOffset": 8, "endOffset": 11}, {"referenceID": 7, "context": "85 RNN-EM [8] 95.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "25 R-bi-RNN [9] 95.", "startOffset": 12, "endOffset": 15}, {"referenceID": 28, "context": "data presented in [29, 30]).", "startOffset": 18, "endOffset": 26}, {"referenceID": 29, "context": "data presented in [29, 30]).", "startOffset": 18, "endOffset": 26}], "year": 2016, "abstractText": "We investigate the usage of convolutional neural networks (CNNs) for the slot filling task in spoken language understanding. We propose a novel CNN architecture for sequence labeling which takes into account the previous context words with preserved order information and pays special attention to the current word with its surrounding context. Moreover, it combines the information from the past and the future words for classification. Our proposed CNN architecture outperforms even the previously best ensembling recurrent neural network model and achieves state-of-the-art results with an F1-score of 95.61% on the ATIS benchmark dataset without using any additional linguistic knowledge and resources.", "creator": "LaTeX with hyperref package"}}}