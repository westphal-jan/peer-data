{"id": "1606.07786", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Jun-2016", "title": "Precise deep neural network computation on imprecise low-power analog hardware", "abstract": "There is an urgent need for compact, fast, and power-efficient hardware implementations of state-of-the-art artificial intelligence. Here we propose a power-efficient approach for real-time inference, in which deep neural networks (DNNs) are implemented through low-power analog circuits. Although analog implementations can be extremely compact, they have been largely supplanted by digital designs, partly because of device mismatch effects due to fabrication. We propose a framework that exploits the power of Deep Learning to compensate for this mismatch by incorporating the measured variations of the devices as constraints in the DNN training process. This eliminates the use of mismatch minimization strategies such as the use of very large transistors, and allows circuit complexity and power-consumption to be reduced to a minimum. Our results, based on large-scale simulations as well as a prototype VLSI chip implementation indicate at least a 3-fold improvement of processing efficiency over current digital implementations.", "histories": [["v1", "Thu, 23 Jun 2016 18:32:43 GMT  (533kb,D)", "http://arxiv.org/abs/1606.07786v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["jonathan binas", "daniel neil", "giacomo indiveri", "shih-chii liu", "michael pfeiffer"], "accepted": false, "id": "1606.07786"}, "pdf": {"name": "1606.07786.pdf", "metadata": {"source": "CRF", "title": "Precise deep neural network computation on imprecise low-power analog hardware", "authors": ["Jonathan Binas", "Daniel Neil", "Giacomo Indiveri", "Shih-Chii Liu", "Michael Pfeiffer"], "emails": ["jbinas@ini.ethz.ch."], "sections": [{"heading": null, "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "1 Results", "text": "A deep neural network processes input signals in a number of successive layers of neurons, with each neuron computing a weighted sum of its inputs, followed by a nonlinearity, such as sigmoid or rectification. Specifically, the output of a neuron i is given by xi = f (\u2211 j wijxj), where f is nonlinearity, and wij is the weight of the connection from neuron j to neuron i. Thus, the basic operations that encompass a neural network are summation, multiplication by scalars, and simple nonlinear transformations. All of these operations can be performed very efficiently in analog electronic circuits, i.e. with very few transistors, where numerical values are represented by actual voltage or current values, rather than by a digital code. Analog circuits are affected by a fabrication imbalance, i.e. small fluctuations in the manufacturing process, resulting in fixed distortions of the neural properties of each of the same elements, as well as minor differences in noise properties for each device."}, {"heading": "1.1 Training with heterogeneous transfer functions", "text": "The weights of multi-layer networks are typically learned from labeled training data using the back propagation algorithm [44], which minimizes the training error by calculating error patterns and guiding them backwards through the layers. [16] For this to work in practice, the transmission function f must be differentiated at least in part, as is the case with the frequently used rectified linear unit (ReLU). [16] Although it is common practice in neural network training, it is not necessary for all neurons to have identical activation functions f. In fact, the presence of different activation functions makes no difference to backpropagation as long as their derivatives can be calculated. Here, this principle is utilized by transferring the heterogeneous but measured transmission curves f: i from a physical analog neural network implementation into the training algorithm (with the aim of finding weight parameters tailored to a specific system)."}, {"heading": "1.2 Analog circuit implementation", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "1.3 Handwritten and spoken digit classification", "text": "In fact, most of them are able to move to another world, in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "1.4 VLSI implementation", "text": "As a demonstration of our framework, we designed a prototype of a VLSI chip and trained it for a classification task. A design based on the three-layer, seven-neuron circuit shown in Fig. 2 was manufactured using 180 nm CMOS technology. After characterizing the individual neuron circuits by measurements as described in Fig. 1.2, we trained a 4 \u2212 7 \u2212 3 network on 80% of the iris diaphragm dataset [15], programmed the device with the found parameters and used the remaining 20% of the data to test the classification performance. By implementing the hardware, 100% of the test data could be correctly classified (see Fig. 5e for network output)."}, {"heading": "2 Discussion", "text": "In this context, it should be noted that this project is a project, which is primarily a project."}, {"heading": "3 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Description of the example circuit", "text": "The exemplary networks described in Section 1.2 were implemented on the basis of the circuits shown in Figure 2. With M0 as diode-connected nFET, the circuit essentially leads to a rectification of the input current Iin. Furthermore, the current is converted into the M1 and M3, also into the M2 together with the connected synapse circuits, and into the M4 together with the connected synapse circuits, generating the scaled copies of the rectified input currents Iin. The scaling factor is determined by the dimensions of the M10 to M15. Transistors M16 to M20 operate as switches and are controlled by the digital signals."}, {"heading": "3.2 Circuit simulation details", "text": "All circuits were simulated using NGSPICE Release 26 and BSIM3 Version 3.3.0 models of a TSMC 180 nm process; the SPICE netlist for a particular network was generated using custom Python software and then passed to NGSPICE for DC and transient simulations; input patterns were provided to the input layer by current sources fixed to their respective values; the parameters of Tab. 2 were used in all simulations and Vdd was configured to 1.8 V. Synapses were configured by applying their respective configuration bits w \u00b1, w0, w1 and w2 to either Vdd or Boden to emulate a digital memory element that is identical; the parasitic capacities and resistances found in an implementation of our circuits were estimated by post-layout simulations of individual soma and synapse cells; the main slowdown of the circuits can be attributed to the aptic capacities of the synapse."}, {"heading": "3.4 Training and evaluation details", "text": "The 196 \u2212 100 \u2212 10 networks were trained on the MNIST and TIDIGITS datasets with the ADAM Optimizer [27] and the mean square error as a loss function. The low precision of the training (three significant bits per synapse) was performed with high-precision memory and low precision activities in the manner in which the training was performed. An L1 activation scheme was applied to negative weights only to reduce the number of negative inputs to neurons as they would slow down the circuits. Keras software toolkit was used to perform the training."}, {"heading": "3.5 Performance measurements", "text": "The accuracy of the abstract software model was determined after training by running the respective test patterns through the network. Due to the prohibitively long simulation times, only subsets of the respective test sets were used to determine the accuracy of the SPICE simulated circuits. Specifically, the first 500 samples of the MNIST test set and 500 randomly selected samples from the TIDIGITS test set were used to obtain an estimate of the classification accuracy of the simulated circuits. Specifically, the time to output a particular pattern was calculated by connecting a (random) input pattern from the test set to the Iin nodes of the input layer, and then, once the circuit was converted to a steady state, replaced by the input pattern to be tested. The more realistic scenario of a transition between two patterns was calculated by using an (random) input pattern from the test set and then, once the output current was determined to a steady state, the input time was determined by the V."}, {"heading": "3.6 VLSI prototype implementation", "text": "A 7 \u2212 7 \u2212 7 network, consisting of 21 neurons and 98 synapses, was manufactured using 180 nm CMOS technology (AMS 1P6M) and the input currents were provided by custom bias generators optimized for operation below the threshold [12]. Custom current-frequency converters were used to read the outputs of the neurons and send them from the chip at intervals between events. Weight parameters were stored on the device in latches directly connected to the configuration lines of the synapse circuits. Custom digital logic was implemented on the chip to program distortions, weights and monitors. Furthermore, the chip was connected to a Xilinx Spartan 6 FPGA, which included a custom interface logic and a Cypress FX2 device, which provided a USB interface to allow the user to communicate with the chip and was implemented."}], "references": [{"title": "A neuromorphic VLSI learning system", "author": ["J. Alspector", "R.B. Allen"], "venue": "P. Losleben, editor, Proceedings of the 1987 Stanford Conference on Advanced Research in VLSI, pages 313\u2013 349, Cambridge, MA, USA,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "Spike-timing dependent plasticity in a transistor-selected resistive switching memory", "author": ["S Ambrogio", "S Balatti", "F Nardi", "S Facchinetti", "D Ielmini"], "venue": "Nanotechnology, 24(38):384012,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Current-mode subthreshold MOS circuits for analog VLSI neural systems", "author": ["Andreas G Andreou", "Kwabena Boahen", "Philippe O Pouliquen", "Aleksandra Pavasovic", "Robert E Jenkins", "Kim Strohbehn"], "venue": "IEEE Transactions on neural networks,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Can programming be liberated from the von neumann style?: A functional style and its algebra of programs", "author": ["John Backus"], "venue": "Commun. ACM,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1978}, {"title": "Programmable current-mode neural network for implementation in analogue MOS VLSI", "author": ["T.H. Borgstrom", "M Ismail", "S.B. Bibyk"], "venue": "IEE Proceedings G, 137(2):175\u2013184,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1990}, {"title": "Origami: A convolutional network accelerator", "author": ["Lukas Cavigelli", "David Gschwend", "Christoph Mayer", "Samuel Willi", "Beat Muheim", "Luca Benini"], "venue": "In Proceedings of the 25th edition on Great Lakes Symposium on VLSI,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "eyeriss: An energyefficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Yu-Hsin Chen", "Tushar Krishna", "Joel Emer", "Vivienne Sze"], "venue": "IEEE International Solid-State Circuits Conference (ISSCC),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun"], "venue": "In Microarchitecture,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Low precision arithmetic for deep learning", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "arXiv preprint arXiv:1412.7024,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "32-bit configurable bias current generator with sub-off-current capability", "author": ["Tobi Delbruck", "Raphael Berner", "Patrick Lichtsteiner", "Carlos Dualibe"], "venue": "In Proceedings of 2010 IEEE International Symposium on Circuits and Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "DeCAF: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In ICML,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Comparison between frame-constrained fix-pixel-value and frame-free spiking-dynamic-pixel convnets for visual processing", "author": ["Cl\u00e9ment Farabet", "R Paz-Vicente", "JA P\u00e9rez-Carrasco", "Carlos Zamarre\u00f1o-Ramos", "Alejandro Linares-Barranco", "Yann LeCun", "Eugenio Culurciello", "Teresa Serrano-Gotarredona", "Bernabe Linares-Barranco"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "A 240 g-ops/s mobile coprocessor for deep neural networks", "author": ["Vinayak Gokhale", "Jonghoon Jin", "Aysegul Dundar", "Ben Martini", "Eugenio Culurciello"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "An 11-million transistor neural network execution engine", "author": ["Matthew Griffin", "Gary Tahara", "Kurt Knorpp", "Ray Pinkham", "Bob Riley"], "venue": "In Solid-State Circuits Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Finding a roadmap to achieve large neuromorphic hardware systems", "author": ["Jennifer Hasler", "Bo Marr"], "venue": "Frontiers in neuroscience,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv preprint arXiv:1503.02531,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Neuromorphic architectures for spiking deep neural networks", "author": ["Giacomo Indiveri", "Federico Corradi", "Ning Qiao"], "venue": "In IEEE International Electron Devices Meeting (IEDM),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Neuromorphic silicon neuron circuits", "author": ["Giacomo Indiveri", "Bernabe Linares-Barranco", "Tara Julia Hamilton", "Andr\u00e9 van Schaik", "Ralph Etienne-Cummings", "Tobi Delbruck", "Shih-Chii Liu", "Piotr Dudek", "Philipp H\u00e4fliger", "Sylvie Renaud", "Johannes Schemmel", "Gert Cauwenberghs", "John Arthur", "Kai Hynna", "Fopefolu Folowosele", "Sylvain SA\u00cfGHI", "Teresa Serrano-Gotarredona", "Jayawan Wijekoon", "Yingxue Wang", "Kwabena Boahen"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Memory and information processing in neuromorphic systems", "author": ["Giacomo Indiveri", "Shih-Chii Liu"], "venue": "Proceedings of the IEEE,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A functional hybrid memristor crossbar-array/CMOS system for data storage and neuromorphic applications", "author": ["Kuk-Hwan Kim", "Siddharth Gaba", "Dana Wheeler", "Jose M Cruz-Albrecht", "Tahir Hussain", "Narayan Srinivasa", "Wei Lu"], "venue": "Nano letters,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Characterisation and modeling of mismatch in MOS transistors for precision analog design", "author": ["Kadaba R Lakshmikumar", "Robert Hadaway", "Miles Copeland"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1986}, {"title": "A high-speed analog neural processor", "author": ["Peter Masa", "Klaas Hoen", "Hans Wallinga"], "venue": "Micro, IEEE,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1994}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Paul A Merolla", "John V Arthur", "Rodrigo Alvarez-Icaza", "Andrew S Cassidy", "Jun Sawada", "Filipp Akopyan", "Bryan L Jackson", "Nabil Imam", "Chen Guo", "Yutaka Nakamura", "Bernard Brezzo", "Ivan Vo", "Steven K Esser", "Rathinakumar Appuswamy", "Brian Taba", "Arnon Amir", "Myron D Flickner", "William P Risk", "Rajit Manohar", "Dharmendra S Modha"], "venue": "Science, 345(6197):668\u2013673,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Learning to be efficient: Algorithms for training low-latency, low-compute deep spiking neural networks", "author": ["Daniel Neil", "Michael Pfeiffer", "Shih-Chii Liu"], "venue": "In ACM Symposium on Applied Computing,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}, {"title": "Impact of process variations on emerging memristor", "author": ["Dimin Niu", "Yiran Chen", "Cong Xu", "Yuan Xie"], "venue": "In Design Automation Conference (DAC),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Realtime classification and sensor fusion with a spiking deep belief network", "author": ["Peter O\u2019Connor", "Daniel Neil", "Shih-Chii Liu", "Tobi Delbruck", "Michael Pfeiffer"], "venue": "Frontiers in Neuromorphic Engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "An energy-efficient and scalable deep learning/inference processor with tetra-parallel MIMD architecture for big data applications", "author": ["SW Park", "J Park", "K Bong", "D Shin", "J Lee", "S Choi", "HJ Yoo"], "venue": "IEEE transactions on biomedical circuits and systems,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2016}, {"title": "Transistor matching in analog CMOS applications", "author": ["Marcel JM Pelgrom", "Hans P Tuinhout", "Maarten Vertregt"], "venue": "IEDM Tech. Dig, pages 915\u2013918,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1998}, {"title": "Welbers. Matching properties of MOS transistors", "author": ["M.J.M. Pelgrom", "Aad C.J. Duinmaijer", "A.P.G"], "venue": "IEEE Journal of Solid-State Circuits,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1989}, {"title": "Training and operation of an integrated neuromorphic network based on metal-oxide memristors", "author": ["Mirko Prezioso", "Farnood Merrikh-Bayat", "BD Hoskins", "GC Adam", "Konstantin K Likharev", "Dmitri B Strukov"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "CNN features off-the-shelf: an astounding baseline for recognition", "author": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "venue": "In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "The perceptron: a probabilistic model for information storage and organization in the brain", "author": ["F. Rosenblatt"], "venue": "Psychological review, 65(6):386\u2013408, nov", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1958}, {"title": "Parallel distributed processing: Explorations in the microstructure of cognition, vol", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "1. chapter Learning Internal Representations by Error Propagation, pages 318\u2013362. MIT Press, Cambridge, MA, USA,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1986}, {"title": "A reconfigurable VLSI neural network", "author": ["S. Satyanarayana", "Y.P. Tsividis", "H.P. Graf"], "venue": "IEEE Journal of Solid-State Circuits, 27(1):67\u201381, Jan", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1992}, {"title": "Accelerating large-scale convolutional neural networks with parallel graphics multiprocessors", "author": ["Dominik Scherer", "Hannes Schulz", "Sven Behnke"], "venue": "In Artificial Neural Networks\u2013ICANN", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2010}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks, 61:85\u2013 117, January", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2015}, {"title": "Robustness of spiking deep belief networks to noise and reduced bit precision of neuro-inspired hardware platforms", "author": ["Evangelos Stromatias", "Daniel Neil", "Michael Pfeiffer", "Francesco Galluppi", "Steve B Furber", "Shih-Chii Liu"], "venue": "Frontiers in neuroscience,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2015}, {"title": "Analog VLSI implementation of neural networks", "author": ["E.A. Vittoz"], "venue": "Proc. IEEE Int. Symp. Circuit and Systems, pages 2524\u20132527, New Orleans,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1990}], "referenceMentions": [{"referenceID": 3, "context": "In particular, due to the separation of memory and processing elements, traditional computing systems experience a bottleneck when dealing with problems involving great amounts of highdimensional data [4, 25], such as image processing, object recognition, probabilistic inference, or speech recognition.", "startOffset": 201, "endOffset": 208}, {"referenceID": 22, "context": "In particular, due to the separation of memory and processing elements, traditional computing systems experience a bottleneck when dealing with problems involving great amounts of highdimensional data [4, 25], such as image processing, object recognition, probabilistic inference, or speech recognition.", "startOffset": 201, "endOffset": 208}, {"referenceID": 40, "context": "These problems can often best be tackled by conceptually simple but powerful and highly parallel methods, such as deep neural networks (DNNs), which in recent years have delivered state-of-the-art performance on exactly those applications [29, 47].", "startOffset": 239, "endOffset": 247}, {"referenceID": 39, "context": "processing units (GPUs) [46].", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 199, "endOffset": 211}, {"referenceID": 18, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 199, "endOffset": 211}, {"referenceID": 9, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 199, "endOffset": 211}, {"referenceID": 5, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 256, "endOffset": 266}, {"referenceID": 14, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 256, "endOffset": 266}, {"referenceID": 7, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 256, "endOffset": 266}, {"referenceID": 12, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 30, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 27, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 20, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 28, "context": "The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34].", "startOffset": 319, "endOffset": 339}, {"referenceID": 5, "context": "With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38].", "startOffset": 179, "endOffset": 193}, {"referenceID": 15, "context": "With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38].", "startOffset": 179, "endOffset": 193}, {"referenceID": 6, "context": "With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38].", "startOffset": 179, "endOffset": 193}, {"referenceID": 31, "context": "With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38].", "startOffset": 179, "endOffset": 193}, {"referenceID": 36, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 0, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 42, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 4, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 2, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 38, "context": "An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45].", "startOffset": 362, "endOffset": 383}, {"referenceID": 17, "context": "By directly representing neural network operations in the physical properties of silicon transistors, such analog implementations can outshine their digital counterparts in terms of simplicity, allowing for significant advances in speed, size, and power consumption [20, 32].", "startOffset": 266, "endOffset": 274}, {"referenceID": 26, "context": "By directly representing neural network operations in the physical properties of silicon transistors, such analog implementations can outshine their digital counterparts in terms of simplicity, allowing for significant advances in speed, size, and power consumption [20, 32].", "startOffset": 266, "endOffset": 274}, {"referenceID": 33, "context": "The main reason why engineers have been discouraged from following this approach is that the properties of analog circuits are affected by the physical imperfections inherent to any chip fabrication process, which can lead to significant functional differences between individual devices [40].", "startOffset": 288, "endOffset": 292}, {"referenceID": 37, "context": "The weights of multi-layered networks are typically learned from labeled training data using the backpropagation algorithm [44], which minimizes the training error by computing error gradients and passing them backwards through the layers.", "startOffset": 123, "endOffset": 127}, {"referenceID": 13, "context": "In order for this to work in practice, the transfer function f needs to be at least piece-wise differentiable, as is the case for the commonly used rectified linear unit (ReLU) [16].", "startOffset": 177, "endOffset": 181}, {"referenceID": 24, "context": "After simulating measurements and parameterizing the transfer characteristics of the circuits as described previously, software networks were trained on the MNIST dataset of handwritten digits [30] and the TIDIGITS dataset of spoken digits [31] by means of the ADAM training method [27].", "startOffset": 282, "endOffset": 286}, {"referenceID": 41, "context": "In order to optimize the network for the use of discrete weights in the synaptic circuits dual-copy rounding [48, 10] was used (see Methods).", "startOffset": 109, "endOffset": 117}, {"referenceID": 8, "context": "In order to optimize the network for the use of discrete weights in the synaptic circuits dual-copy rounding [48, 10] was used (see Methods).", "startOffset": 109, "endOffset": 117}, {"referenceID": 5, "context": "Without major optimizations to either process or implementation, this leads to an efficiency of around 8 TOp/J, to our knowledge a performance at least four times greater than that achieved by digital single-purpose neural network accelerators in similar scenarios [6, 38].", "startOffset": 265, "endOffset": 272}, {"referenceID": 31, "context": "Without major optimizations to either process or implementation, this leads to an efficiency of around 8 TOp/J, to our knowledge a performance at least four times greater than that achieved by digital single-purpose neural network accelerators in similar scenarios [6, 38].", "startOffset": 265, "endOffset": 272}, {"referenceID": 36, "context": "The theory of analog neural networks and electronic realizations thereof have a substantial history that goes back to the1950s [43, 1].", "startOffset": 127, "endOffset": 134}, {"referenceID": 0, "context": "The theory of analog neural networks and electronic realizations thereof have a substantial history that goes back to the1950s [43, 1].", "startOffset": 127, "endOffset": 134}, {"referenceID": 5, "context": "Instead, digital designs have flourished in the interim and almost all current deep network designs are implemented in digital form [6, 7, 38].", "startOffset": 132, "endOffset": 142}, {"referenceID": 6, "context": "Instead, digital designs have flourished in the interim and almost all current deep network designs are implemented in digital form [6, 7, 38].", "startOffset": 132, "endOffset": 142}, {"referenceID": 31, "context": "Instead, digital designs have flourished in the interim and almost all current deep network designs are implemented in digital form [6, 7, 38].", "startOffset": 132, "endOffset": 142}, {"referenceID": 1, "context": "For example, the memristive computing technology which is currently being pursued for implementing large-scale cognitive neuromorphic and other technologies still suffers from the mismatch of fabricated devices [2, 26, 41].", "startOffset": 211, "endOffset": 222}, {"referenceID": 23, "context": "For example, the memristive computing technology which is currently being pursued for implementing large-scale cognitive neuromorphic and other technologies still suffers from the mismatch of fabricated devices [2, 26, 41].", "startOffset": 211, "endOffset": 222}, {"referenceID": 34, "context": "For example, the memristive computing technology which is currently being pursued for implementing large-scale cognitive neuromorphic and other technologies still suffers from the mismatch of fabricated devices [2, 26, 41].", "startOffset": 211, "endOffset": 222}, {"referenceID": 29, "context": "The proposed training method in this work can be used to account for device non-idealities in this technology [35].", "startOffset": 110, "endOffset": 114}, {"referenceID": 21, "context": "For example, spike-based neuromorphic systems [24] often have configurable weights between neurons.", "startOffset": 46, "endOffset": 50}, {"referenceID": 19, "context": "In principle, even recurrent architectures such as LSTM networks [22] can be trained using the same methods, where not only the static properties of the circuit are taken into account but also their dynamics.", "startOffset": 65, "endOffset": 69}, {"referenceID": 11, "context": "However, there is increasing evidence that neural networks pretrained on large datasets such as ImageNet provide excellent generic feature detectors [13, 42], which means that fast and efficient analog input pre-processors could be used as an important building blocks for a large variety of applications.", "startOffset": 149, "endOffset": 157}, {"referenceID": 35, "context": "However, there is increasing evidence that neural networks pretrained on large datasets such as ImageNet provide excellent generic feature detectors [13, 42], which means that fast and efficient analog input pre-processors could be used as an important building blocks for a large variety of applications.", "startOffset": 149, "endOffset": 157}], "year": 2016, "abstractText": "There is an urgent need for compact, fast, and power-efficient hardware implementations of state-of-the-art artificial intelligence. Here we propose a power-efficient approach for real-time inference, in which deep neural networks (DNNs) are implemented through low-power analog circuits. Although analog implementations can be extremely compact, they have been largely supplanted by digital designs, partly because of device mismatch effects due to fabrication. We propose a framework that exploits the power of Deep Learning to compensate for this mismatch by incorporating the measured variations of the devices as constraints in the DNN training process. This eliminates the use of mismatch minimization strategies such as the use of very large transistors, and allows circuit complexity and powerconsumption to be reduced to a minimum. Our results, based on large-scale simulations as well as a prototype VLSI chip implementation indicate at least a 3-fold improvement of processing efficiency over current digital implementations. Modern information technology requires increasing computational power to process massive amounts of data in real time. This rapidly growing need for computing power has led to the exploration of computing technologies beyond the predominant von Neumann architecture. In particular, due to the separation of memory and processing elements, traditional computing systems experience a bottleneck when dealing with problems involving great amounts of highdimensional data [4, 25], such as image processing, object recognition, probabilistic inference, or speech recognition. These problems can often best be tackled by conceptually simple but powerful and highly parallel methods, such as deep neural networks (DNNs), which in recent years have delivered state-of-the-art performance on exactly those applications [29, 47]. DNNs are characterized by stereotypical and simple operations at each unit, of which many can be performed in parallel. For this reason they map favorably e.g. onto the processing style of graphics 1 ar X iv :1 60 6. 07 78 6v 1 [ cs .N E ] 2 3 Ju n 20 16 processing units (GPUs) [46]. The large computational demands of DNNs have simultaneously sparked interest in methods that make neural network inference faster and more power efficient, whether through new algorithmic inventions [19, 21, 11], dedicated digital hardware implementations [6, 17, 8], or by taking inspiration from real nervous systems [14, 37, 33, 23, 34]. With synchronous digital logic being the established standard of the electronics industry, first attempts towards hardware deep network accelerators have focused on this approach [6, 18, 7, 38]. However, the massively parallel style of computation of neural networks is not reflected in the mostly serial and time-multiplexed nature of digital systems. An arguably more natural way of developing a hardware neural network emulator is to implement its computational primitives as multiple physical and parallel instances of analog computing nodes, where memory and processing elements are co-localized, and state variables are directly represented by analog currents or voltages, rather than being encoded digitally [43, 1, 49, 5, 3, 45]. By directly representing neural network operations in the physical properties of silicon transistors, such analog implementations can outshine their digital counterparts in terms of simplicity, allowing for significant advances in speed, size, and power consumption [20, 32]. The main reason why engineers have been discouraged from following this approach is that the properties of analog circuits are affected by the physical imperfections inherent to any chip fabrication process, which can lead to significant functional differences between individual devices [40]. In this work we propose a new approach, whereby rather than brute-force engineering more homogeneous circuits (e.g. by increasing transistor sizes and burning more power), we employ neural network training methods as an effective optimization framework to automatically compensate for the device mismatch effects of analog VLSI circuits. We use the diverse measured characteristics of individual VLSI devices as constraints in an off-line training process, to yield network configurations that are tailored to the particular analog device used, thereby compensating the inherent variability of chip fabrication. Finally, the network parameters, in particular the synaptic weights found during the training phase can be programmed in the network, and the analog circuits can be operated at run-time in the sub-threshold region for significantly lower power consumption. In this article, in addition to introducing a novel training method for both device and network, we also propose compact and low-power candidate VLSI circuits. A closed-loop demonstration of the framework is shown, based on a fabricated prototype chip, as well as detailed, large-scale simulations. The resulting analog electronic neural network performs as well as an ideal network, while offering at least a threefold lower power consumption over its digital counterpart.", "creator": "LaTeX with hyperref package"}}}