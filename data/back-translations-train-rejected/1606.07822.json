{"id": "1606.07822", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Efficient Parallel Learning of Word2Vec", "abstract": "Since its introduction, Word2Vec and its variants are widely used to learn semantics-preserving representations of words or entities in an embedding space, which can be used to produce state-of-art results for various Natural Language Processing tasks. Existing implementations aim to learn efficiently by running multiple threads in parallel while operating on a single model in shared memory, ignoring incidental memory update collisions. We show that these collisions can degrade the efficiency of parallel learning, and propose a straightforward caching strategy that improves the efficiency by a factor of 4.", "histories": [["v1", "Fri, 24 Jun 2016 20:05:53 GMT  (39kb)", "http://arxiv.org/abs/1606.07822v1", "ICML 2016 Machine Learning workshop"]], "COMMENTS": "ICML 2016 Machine Learning workshop", "reviews": [], "SUBJECTS": "cs.CL cs.DC", "authors": ["jeroen b p vuurens", "carsten eickhoff", "arjen p de vries"], "accepted": false, "id": "1606.07822"}, "pdf": {"name": "1606.07822.pdf", "metadata": {"source": "META", "title": "Efficient Parallel Learning of Word2Vec", "authors": [], "emails": ["J.B.P.VUURENS@TUDELFT.NL", "CARSTEN.EICKHOFF@INF.ETHZ.CH", "ARJEN@ACM.ORG"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 822v 1 [cs.C L] 2"}, {"heading": "1. Introduction", "text": "Traditional NLP approaches predominantly use simple word representations of documents and sentences, but newer approaches that use a distributed representation of words by constructing a so-called \"word embedding\" have proven effective in many different NLP tasks (Weston et al., 2014). Mikolov et al. have introduced efficient strategies to learn embedding from text corpora, collectively known as Word2Vec (Mikolov et al., 2013). They have shown that simply increasing the volume of training data improves semantic and syntactic generalizations that are automatically encoded in the embedding space, and therefore efficiency is the key to increasing the potential of this technique."}, {"heading": "2. Related Work", "text": "Bengio et al. propose to learn a distributed representation for words in an embedding space based on the words that immediately precede a word in the natural language (Bengio et al., 2006). They use a flat neural network architecture to learn a vector space representation for words similar to those in latent semantic indexing. Recently, Mikolov et al. proposed a very efficient way to learn embedding via large text corpora called Word2Vec (Mikolov et al., 2013), which leads to results in various tasks such as word similarity, word analogy tasks, named entity recognition, machine translation, and question-answering (Weston et al., 2014). Word2vec uses stochastic gradient descension to optimize vector representations by learning iteratively on words that appear jointly in the text."}, {"heading": "3. Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Analysis of inefficiencies for learning Word2Vec", "text": "One strategy is to map the input in separate batches and aggregate the results in a reduced step. Alternatively, multithreading can be used on a single model in shared memory used by the original Word2Vec implementation in C and the widely used Gensim package for Python. We analyzed how learning times extend over the number of cores used with Word2Vec and found that total runtime appears optimal when using about 8 cores at which efficiency decreases (see Figure 1). Ji et al. analyzes this problem and concludes that multiple threads can cause conflicts when trying to update the same memory (Ji et al., 2016). Current Word2Vec implementations simply ignore memory conflicts between threads, but simultaneous attempts to read and update the same vocabulary increase memory access latency."}, {"heading": "3.2. Caching frequently used vectors", "text": "For the efficient learning of texts with negative sampling, Ji et al., we propose a mini-batch strategy that uses a level-3 BLAS function to multiply a matrix consisting of words that have the same context word (e.g. with a window size of 5 to 10 words), with a matrix containing the context word and a common set of negative samples (e.g.)."}, {"heading": "4. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experiment setup", "text": "To compare the efficiency of the proposed caching strategy with the existing implementations, we used the Skip-1http: / / cythnn.github.ioAlgorithm 1 Cached Skipgram HS1: for each wordid v in text do 2: for each wordid w within window of v do 3: for each node n, turn t in treepath to v do 4: if isFrequent (n) then 5: if not cached [n] then 6: scopy (weight [n], cachedweight [n] 7: scopy (cachedweight [n], original [n]) 8: cached [n] = True 9: end if 10: usedweight = cachedweight [n] 11: else 12: usedweight = weight [n] 13: end if 14: f = sigmoid (sdot (sdot (sdot), usedweight [w], usedached n), usedached each [n]) 15: graached each cached for each cachedweight [n] (for 1 - 1 usedweight layer) for each cached n do (usable for each cached n: cached n: cached n: each layer), cached each cached n: (usable for each layer)."}, {"heading": "4.2. Efficiency", "text": "In Figure 1, we compare the changes in total runtime when we add more cores between the original Word2Vec C implementation, Gensim, Cythnn without caching (c0), and Cythnn when we cache the 31 most commonly used nodes in the Huffmann tree (c31). The first observation is that without caching, Word2Vec and Cythnn have degrading efficiencies beyond the use of 8 cores, while Gensim seems more optimized to consume up to 16 cores. In Figure 2, we compare the execution time of Cythnn, which caches the 31 top vectors (157 sec), up to 3.9x faster than the fastest C run (630 sec) and up to 2.5 x faster than the fastest Gensim run (385 sec)."}, {"heading": "4.3. Effectiveness", "text": "Ji et al. compared the effectiveness between their approach and the original Word2Vec implementation and report that processing in mini batches has a marginal but noticeable negative effect on the accuracy of the embedded learning. In Table 1, we show the average accuracy of each system over the runs of 2-28 cores, with the accuracy calculated using the evaluation tool and the question word test set packed with Word2Vec3. Results show a higher accuracy for Gensim, but we have not found any documentation to explain this difference. With Cythnn, caching does not result in lower effectiveness, as reported by (Ji et al., 2016), with a difference that our caching approach always uses updated vectors within a thread, while the BLAS-3 solution uses no updated vectors within a mini-batch.When we select the cache update frequency of the word vary, we expect this difference to vary according to our model balance of practice, with a compromise of the number of words being selected according to 1."}, {"heading": "5. Conclusion", "text": "In this study, we analyzed the scalability of parallel learning of a single model in shared memory, in particular learning Word2Vec using a Skipgram architecture against 3http: / / word2vec.googlecode.com"}, {"heading": "W2V C 33.73", "text": "The original Word2Vec implementation is most efficient when it is trained with 8 cores, and becomes less efficient when more cores are added. We propose a simple caching strategy that caches the most commonly used weight vectors and updates their change to main memory after a short delay, reducing simultaneous access to shared memory. We compared the efficiency of this strategy with existing Word2Vec implementations and showed up to a fourfold increase in efficiency."}, {"heading": "Acknowledgment", "text": "This work was carried out on the national infrastructure of the Netherlands with the support of the SURF Foundation."}], "references": [{"title": "Neural probabilistic language models", "author": ["Bengio", "Yoshua", "Schwenk", "Holger", "Sen\u00e9cal", "JeanS\u00e9bastien", "Morin", "Fr\u00e9deric", "Gauvain", "Jean-Luc"], "venue": "In Innovations in Machine Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Parallelizing word2vec in shared and distributed memory", "author": ["Ji", "Shihao", "Satish", "Nadathur", "Li", "Sheng", "Dubey", "Pradeep"], "venue": null, "citeRegEx": "Ji et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg S", "Dean", "Jeff"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Recht", "Benjamin", "Re", "Christopher", "Wright", "Stephen", "Niu", "Feng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Recht et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Recht et al\\.", "year": 2011}, {"title": "tagspace: Semantic embeddings from hashtags", "author": ["Weston", "Jason", "Chopra", "Sumit", "Adams", "Keith"], "venue": "In Proceedings of EMNLP 2014,", "citeRegEx": "Weston et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "Traditional NLP approaches dominantly use simple bagof-word representations of documents and sentences, but recent approaches that use distributed representation of words, by constructing a so-called \u201dword embedding\u201d, have been shown effective across many different NLP tasks (Weston et al., 2014).", "startOffset": 276, "endOffset": 297}, {"referenceID": 2, "context": "introduced efficient strategies to learn embeddings from text corpora, which are collectively known as Word2Vec (Mikolov et al., 2013).", "startOffset": 112, "endOffset": 134}, {"referenceID": 1, "context": "argue that the current implementations of Word2Vec do not scale well over the number of used cores, and propose a solution that uses higherlevel linear algebra functions to improve the efficiency of Word2Vec using negative sampling (Ji et al., 2016).", "startOffset": 232, "endOffset": 249}, {"referenceID": 0, "context": "propose to learn a distributed representation for words in an embedding space based on the words that immediately precede a word in natural language (Bengio et al., 2006).", "startOffset": 149, "endOffset": 170}, {"referenceID": 2, "context": "proposed a very efficient way to learn embeddings over large text corpora called Word2Vec (Mikolov et al., 2013), which produces stateof-the-art results on various tasks, such as word similarity, word analogy tasks, named entity recognition, machine translation and question answering (Weston et al.", "startOffset": 90, "endOffset": 112}, {"referenceID": 4, "context": ", 2013), which produces stateof-the-art results on various tasks, such as word similarity, word analogy tasks, named entity recognition, machine translation and question answering (Weston et al., 2014).", "startOffset": 180, "endOffset": 201}, {"referenceID": 3, "context": "The original Word2Vec implementation uses a Hogwild strategy of allowing processors lock-free access to shared memory, in which they can update at will (Recht et al., 2011).", "startOffset": 152, "endOffset": 172}, {"referenceID": 1, "context": "notice that for training Word2Vec this strategy does not use computational resources efficiently (Ji et al., 2016).", "startOffset": 97, "endOffset": 114}, {"referenceID": 1, "context": "analyze this problem and conclude that multiple threads can cause conflicts when they attempt to update the same memory (Ji et al., 2016).", "startOffset": 120, "endOffset": 137}, {"referenceID": 1, "context": "with a window size of 5 up to 10 words would train against the same context term), with a matrix that contains the context word and a shared set of negative samples (Ji et al., 2016).", "startOffset": 165, "endOffset": 182}, {"referenceID": 1, "context": "The solution that is presented by (Ji et al., 2016) uses level-3 BLAS functions to perform matrix-matrix multiplications, which implicitly lowers the number of times shared memory is accessed and updated.", "startOffset": 34, "endOffset": 51}, {"referenceID": 1, "context": "Using Cythnn, caching does not result in lower effectiveness as reported by (Ji et al., 2016), a difference being that our caching approach always uses updated vectors within a thread whereas the BLAS-3 solution does not use updated vectors within a mini-batch.", "startOffset": 76, "endOffset": 93}], "year": 2016, "abstractText": "Since its introduction, Word2Vec and its variants are widely used to learn semantics-preserving representations of words or entities in an embedding space, which can be used to produce state-of-art results for various Natural Language Processing tasks. Existing implementations aim to learn efficiently by running multiple threads in parallel while operating on a single model in shared memory, ignoring incidental memory update collisions. We show that these collisions can degrade the efficiency of parallel learning, and propose a straightforward caching strategy that improves the efficiency by a factor of 4.", "creator": "LaTeX with hyperref package"}}}