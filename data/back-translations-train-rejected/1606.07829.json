{"id": "1606.07829", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Jun-2016", "title": "Unsupervised Topic Modeling Approaches to Decision Summarization in Spoken Meetings", "abstract": "We present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify \"summary-worthy\" words. Concretely, a series of unsupervised topic models is explored and experimental results show that fine-grained topic models, which discover topics at the utterance-level rather than the document-level, can better identify the gist of the decision-making process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary.", "histories": [["v1", "Fri, 24 Jun 2016 20:17:44 GMT  (69kb,D)", "http://arxiv.org/abs/1606.07829v1", "SIGDIAL 2012"]], "COMMENTS": "SIGDIAL 2012", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "claire cardie"], "accepted": false, "id": "1606.07829"}, "pdf": {"name": "1606.07829.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Topic Modeling Approaches to Decision Summarization in Spoken Meetings", "authors": ["Lu Wang", "Claire Cardie"], "emails": ["luwang@cs.cornell.edu", "cardie@cs.cornell.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the fact is that most of them are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"\" I don't think they will be able to change the world. \""}, {"heading": "2 Related Work", "text": "A primary goal for most language summary systems is to take into account the specifics of dialogue. Early work in this area examined monitored learning methods, including maximum entropy, conditional random fields (CRFs) and supporting vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008). However, for unattended methods, maximum marginal relevance (MMR) is examined in (Zechner, 2002) and (Xie and Liu, 2010). Gillick et al. (2009) introduces a concept-based global optimization framework using Integrated Linear Programming (ILP). Only in very recent work is decision summary addressed (Ferna \u2012 ndez et al., 2008), (Bui et al., 2009) and (Wang and Cardie)."}, {"heading": "3 Summarization Frameworks", "text": "In this section, we first introduce our proposed token-level decsumization framework - DomSum - which uses latent topic structures in statements to extract words from Dominant Topic (see details in Section 3.1) to form summaries. In Section 3.2, we describe four existing sentence evaluation metrics, called OneTopic, MultiTopic, TMMSum and KLSum, which are also based on latent topic distributions."}, {"heading": "3.1 Token-level Summarization Framework", "text": "Domsum takes as its input the clusters of DRDAs (with or without additional context DAs), the topic distribution for each DA, and the word distribution for each topic. The output is a set of topic-coherent summary words that can be used directly as a summary or to continue to generate an abstract summary. We introduce DomSum in two steps after its input: by using clusters of DRDAs as input and with additional context information.DRDAs Only. Given the clusters of DRDAs, we use Algorithm 1 to produce the token-level summary for each cluster. Generally, Algorithm 1 selects the topic with the highest probability of being the dominant topic in the face of the dialogue act (DA). Then it collects the words with a high common probability for the dominant topic from this DA.Input: Cluster C = {DAi}, P (Tj | DAi), P (Tj | DAi), P (wk | Tj) Output: Summary Summary DAs empty (DAI)."}, {"heading": "3.2 Utterance-level Summarization Metrics", "text": "Although they are developed on different topic models, since the desired topic distributions are used as input, they can evaluate the statements according to their importance and provide summaries at the statement level for comparison. In (Bhandari et al., 2008) several statement functions are introduced based on probabilistic latent semantic indexing. We assume two metrics, which are OneTopic and MultiTopic. For OneTopic, the topic T is selected with the highest probability per cluster. (T) The topic T (T) is selected as the central topic per cluster. The value for DA in C is: P (DA | T)."}, {"heading": "4 Topic Models", "text": "In this section, we briefly describe the three fine-grained topic models for calculating the latent distribution of topics at the expression level in theme meetings. According to the input of algorithm 1, we are interested in estimating the distribution of topics for each DA P (T | DA) and the distribution of words for each topic P (f | T). For MG-LDA, P (T | DA) is calculated as the expectation of local distribution of topics in view of the window distribution."}, {"heading": "4.1 Local LDA", "text": "Local LDA (LocalLDA) (Brody and Elhadad, 2010) use almost the same probabilistic generative model as latent Dirichlet allocation (LDA) (Blei et al., 2003), except that it treats each sentence as a separate document2. Each DA d is generated as follows: 1. For each topic choose k: (a) word distribution: \u03c6k \u0445 Dir (\u03b2) 2. For each DA d: (a) topic distribution: \u03b8d \u0445 Dir (\u03b1) (b) For each word w in DA choose d: i. topic: zd, w \u0432d i."}, {"heading": "4.2 Multi-grain LDA", "text": "Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) can model the meeting-specific topics (e.g. the construction of a remote control) as well as various concrete aspects (e.g. the costs or the functionality). The generative process is: 1. Select a global topic distribution: Successful-Dir (\u03b1gl) 2. Select for each sliding window v the size T: (a) Select local topic distribution: Phenomenlocm, v-Dir (\u03b1loc) (b) Select granularity mix: \u03c0m, v-Beta (\u03b1mix) 3. Select for each DA d: (a) Window distribution: \u0445m, d-Dir (\u03b3) 4. Select for each word w in DA d of the meeting m: (a) Select sliding window: vm, w-Dir, w-Dir-Dir: zm, w-Dir-Dir-Dir-Dir (\u03b1loc) (Select Dir-Dir): Dir-Dir-Dir-Dir: (Dir) Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir: (Dir) Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir: (Dir): (Dir-Dir-Dir-Dir) Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir: (Dir-Dir-Dir-Dir): (Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir): (Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Dir-Di"}, {"heading": "4.3 Segmented Topic Model", "text": "The last model we use is the Segmented Topic Model (STM) (Du et al., 2010), which models latent themes at the documentation and sentence level using a two-parameter Poisson-Dirichlet process (PDP). Given the parameters \u03b1, \u03b3, \u03a6 and PDP parameters a, b is the generative process: 1. Select the distribution of themes: \u03b8m \u0445 Dir (\u03b1) 2. For each dialog act d: 2For each word w in the dialog act d: (a) select the DAs in the same session the document, so that \"every DA\" in the generative process from LocalLDA is changed to \"every meeting.\" (a) Select the distribution of themes: \u03b8m \u0445 Dir (\u03b1) 3. For each word the LDA form the DAs in the same session the document, so that \"every DA\" in the generative process from LocalLDA is changed to \"every meeting.\" (a) Select the distribution of themes: \u03b8m \u0445 Dir, PDP (\u03b1) 3. For each word the DAs in the dialog d: 3. For each word in the dialog act d: (a) Select the theme: zm, c, c, c, c, c, c, c, c, w, w, w, w, w, c, w, c, c, w, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c, c"}, {"heading": "5 Experimental Setup", "text": "We evaluate our approach to the AMI conference corpus (Carletta et al., 2005), which consists of 140 multi-party conferences, with 129 scenario-driven conferences involving four participants who play different roles in a design team. A short (typically one-sentence) summary is created manually to summarize each decision discussed in the session and use it in our experiments as gold-standard summaries. System inputs. Our conference system requires a split of DRDAs as input according to the decision (s) that support each decision (i.e., one cluster of DRDAs per decision). As mentioned above, for all experiments we assume that the DRDAs for each meeting have been identified. To evaluate, we consider two system input settings. In the True Clustering setting, we use the AMI notes to create perfect partitions of the DRDAs input."}, {"heading": "5.1 Baselines and Comparisons", "text": "We compare our Token-Level Summarization Framework based on the fine-grained topic models with (1) two unattended baselines, (2) Token-Level Summarization by LDA, (3) utterance-Level Summarization by Topical Mixture Model (TMM) (Chen and Chen, 2008), (4) utterance-Level Summarization based on the fine-grained topic models using existing metrics (Section 3.2), (5) two monitored methods, and (6) an upper bound derived from the AMI Gold Standard abstracts. (1) and (6) are described below, others are discussed in Section 6.The LONGEST DA Baseline. As in (Riedhammer et al., 2010) and (Wang and Cardie, 2011), this baseline simply selects the longest DRDA in each cluster as a summary. Thus, it executes an utterance-Level Composure Gap Decision Summarization by Summary."}, {"heading": "6 Results and Discussion", "text": "In fact, most of them are able to set out in search of a solution that originates in the real world."}, {"heading": "6.2 System Clusterings", "text": "The results using the system clusters (Table 2) show similar results, although all system and initial values are lower. By adding context information, the token-level summation approaches based on fine-grained topic models compare favorably with the monitored methods in F-scores and also receive the best ROUGE-1 recalls."}, {"heading": "6.3 Sample System Summaries", "text": "To better illustrate the summaries generated by different systems, Table 3 shows that extractive summaries at the statement level (Longest DA, Prototype DA, TMM) yield more coherent but still far from concise and compact abstracts. On the other hand, the monitored methods (SVM, CRF) that produce extracts at the token level better identify the overall content of the decision abstracts. Unfortunately, they require human annotation during the training phase. By comparison, the output of fine-grained topic models can cover the most useful information."}, {"heading": "7 Conclusion", "text": "We propose a token-level summary framework based on theme models and show that modeling the theme structure at the utterance level identifies relevant words and phrases better than document-level models. The role of context is also examined and shown to be able to identify additional summary words."}], "references": [{"title": "Generic text summarization using probabilistic latent semantic indexing", "author": ["Harendra Bhandari", "Takahiko Ito", "Masashi Shimbo", "Yuji Matsumoto."], "venue": "Proceedings of IJCNLP, pages 133\u2013140.", "citeRegEx": "Bhandari et al\\.,? 2008", "shortCiteRegEx": "Bhandari et al\\.", "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "J. Mach. Learn. Res., 3:993\u20131022, March.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "An unsupervised aspect-sentiment model for online reviews", "author": ["Samuel Brody", "Noemie Elhadad."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT \u201910,", "citeRegEx": "Brody and Elhadad.,? 2010", "shortCiteRegEx": "Brody and Elhadad.", "year": 2010}, {"title": "Extracting decisions from multi-party dialogue using directed graphical models and semantic similarity", "author": ["Trung H. Bui", "Matthew Frampton", "John Dowding", "Stanley Peters."], "venue": "Proceedings of the SIGDIAL 2009 Conference, pages 235\u2013243.", "citeRegEx": "Bui et al\\.,? 2009", "shortCiteRegEx": "Bui et al\\.", "year": 2009}, {"title": "Automatic summarization of meeting data: A feasibility study", "author": ["Anne Hendrik Buist", "Wessel Kraaij", "Stephan Raaijmakers."], "venue": "Proc. Meeting of Computational Linguistics in the Netherlands (CLIN).", "citeRegEx": "Buist et al\\.,? 2004", "shortCiteRegEx": "Buist et al\\.", "year": 2004}, {"title": "Methods for Mining and Summarizing Text Conversations", "author": ["Giuseppe Carenini", "Gabriel Murray", "Raymond Ng."], "venue": "Morgan & Claypool Publishers.", "citeRegEx": "Carenini et al\\.,? 2011", "shortCiteRegEx": "Carenini et al\\.", "year": 2011}, {"title": "A hybrid hierarchical model for multi-document summarization", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 815\u2013824, Stroudsburg, PA, USA. Associa-", "citeRegEx": "Celikyilmaz and Hakkani.Tur.,? 2010", "shortCiteRegEx": "Celikyilmaz and Hakkani.Tur.", "year": 2010}, {"title": "Extractive spoken document summarization for information retrieval", "author": ["Berlin Chen", "Yi-Ting Chen."], "venue": "Pattern Recogn. Lett., 29:426\u2013437, March.", "citeRegEx": "Chen and Chen.,? 2008", "shortCiteRegEx": "Chen and Chen.", "year": 2008}, {"title": "A segmented topic model based on the two-parameter poisson-dirichlet process", "author": ["Lan Du", "Wray Buntine", "Huidong Jin."], "venue": "Mach. Learn., 81:5\u201319, October.", "citeRegEx": "Du et al\\.,? 2010", "shortCiteRegEx": "Du et al\\.", "year": 2010}, {"title": "Identifying relevant phrases to summarize decisions in spoken meetings", "author": ["Raquel Fern\u00e1ndez", "Matthew Frampton", "John Dowding", "Anish Adukuzhiyil", "Patrick Ehlen", "Stanley Peters."], "venue": "INTERSPEECH-2008, pages 78\u201381.", "citeRegEx": "Fern\u00e1ndez et al\\.,? 2008", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2008}, {"title": "Real-time decision detection in multi-party dialogue", "author": ["Matthew Frampton", "Jia Huang", "Trung Huu Bui", "Stanley Peters."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, pages 1133\u20131141.", "citeRegEx": "Frampton et al\\.,? 2009", "shortCiteRegEx": "Frampton et al\\.", "year": 2009}, {"title": "A skip-chain conditional random field for ranking meeting utterances by importance", "author": ["Michel Galley."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 364\u2013 372.", "citeRegEx": "Galley.,? 2006", "shortCiteRegEx": "Galley.", "year": 2006}, {"title": "A global optimization framework for meeting summarization", "author": ["Dan Gillick", "Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-Tur."], "venue": "Proceedings of the 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP", "citeRegEx": "Gillick et al\\.,? 2009", "shortCiteRegEx": "Gillick et al\\.", "year": 2009}, {"title": "Generic text summarization using relevance measure and latent semantic analysis", "author": ["Yihong Gong", "Xin Liu."], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, SIGIR \u201901, pages 19\u2013", "citeRegEx": "Gong and Liu.,? 2001", "shortCiteRegEx": "Gong and Liu.", "year": 2001}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers."], "venue": "Proceedings of the National Academy of Sciences, 101(Suppl. 1):5228\u20135235, April.", "citeRegEx": "Griffiths and Steyvers.,? 2004", "shortCiteRegEx": "Griffiths and Steyvers.", "year": 2004}, {"title": "Exploring content models for multi-document summarization", "author": ["Aria Haghighi", "Lucy Vanderwende."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Haghighi and Vanderwende.,? 2009", "shortCiteRegEx": "Haghighi and Vanderwende.", "year": 2009}, {"title": "Latent topic modeling for audio corpus summarization", "author": ["Timothy J. Hazen."], "venue": "INTERSPEECH, pages 913\u2013916.", "citeRegEx": "Hazen.,? 2011", "shortCiteRegEx": "Hazen.", "year": 2011}, {"title": "Text categorization with Support Vector Machines: Learning with many relevant features", "author": ["Thorsten Joachims."], "venue": "Claire N\u00e9dellec and C\u00e9line Rouveirol, editors, Machine Learning: ECML-98, volume 1398, chapter 19, pages 137\u2013142. Berlin/Heidelberg.", "citeRegEx": "Joachims.,? 1998", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Improved spoken document summarization using probabilistic latent semantic analysis (plsa)", "author": ["Sheng-Yi Kong", "Lin shan Leek."], "venue": "Proceedings of the 2006 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP \u201906.", "citeRegEx": "Kong and Leek.,? 2006", "shortCiteRegEx": "Kong and Leek.", "year": 2006}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["John D. Lafferty", "Andrew McCallum", "Fernando C.N. Pereira."], "venue": "Proceedings of the Eighteenth International Conference on Machine Learning, ICML \u201901, pages", "citeRegEx": "Lafferty et al\\.,? 2001", "shortCiteRegEx": "Lafferty et al\\.", "year": 2001}, {"title": "A risk minimization framework for extractive speech summarization", "author": ["Shih-Hsiang Lin", "Berlin Chen."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL \u201910, pages 79\u201387. Association for Computational Linguis-", "citeRegEx": "Lin and Chen.,? 2010", "shortCiteRegEx": "Lin and Chen.", "year": 2010}, {"title": "Automatic evaluation of summaries using n-gram co-occurrence statistics", "author": ["Chin-Yew Lin", "Eduard Hovy."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technol-", "citeRegEx": "Lin and Hovy.,? 2003", "shortCiteRegEx": "Lin and Hovy.", "year": 2003}, {"title": "Leveraging kullback-leibler divergence measures and informationrich cues for speech summarization", "author": ["S.-H. Lin", "Y.-M. Yeh", "B. Chen"], "venue": null, "citeRegEx": "Lin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2010}, {"title": "Unsupervised approaches for automatic keyword extraction using meeting transcripts", "author": ["Feifan Liu", "Deana Pennell", "Fei Liu", "Yang Liu."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the As-", "citeRegEx": "Liu et al\\.,? 2009", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "A supervised framework for keyword extraction from meeting transcripts", "author": ["Fei Liu", "Feifan Liu", "Yang Liu."], "venue": "IEEE Transactions on Audio, Speech & Language Processing, 19(3):538\u2013548.", "citeRegEx": "Liu et al\\.,? 2011", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Multi-aspect sentiment analysis with topic models", "author": ["Bin Lu", "Myle Ott", "Claire Cardie", "Benjamin Tsou."], "venue": "Workshop on Sentiment Elicitation from Natural Text for Information Retrieval and Extraction.", "citeRegEx": "Lu et al\\.,? 2011", "shortCiteRegEx": "Lu et al\\.", "year": 2011}, {"title": "Comparing Lexical, Acoustic/Prosodic, Structural and Discourse Features for Speech Summarization", "author": ["Sameer Maskey", "Julia Hirschberg."], "venue": "Proc. European Conference on Speech Communication and Technology (Eurospeech).", "citeRegEx": "Maskey and Hirschberg.,? 2005", "shortCiteRegEx": "Maskey and Hirschberg.", "year": 2005}, {"title": "Towards online speech summarization", "author": ["Gabriel Murray", "Steve Renals."], "venue": "INTERSPEECH, pages 2785\u20132788.", "citeRegEx": "Murray and Renals.,? 2007", "shortCiteRegEx": "Murray and Renals.", "year": 2007}, {"title": "Extractive summarization of meeting recordings", "author": ["Gabriel Murray", "Steve Renals", "Jean Carletta."], "venue": "in Proceedings of the 9th European Conference on Speech Communication and Technology, pages 593\u2013 596.", "citeRegEx": "Murray et al\\.,? 2005", "shortCiteRegEx": "Murray et al\\.", "year": 2005}, {"title": "Interpretation and transformation for abstracting conversations", "author": ["Gabriel Murray", "Giuseppe Carenini", "Raymond Ng."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguis-", "citeRegEx": "Murray et al\\.,? 2010a", "shortCiteRegEx": "Murray et al\\.", "year": 2010}, {"title": "Generating and validating abstracts of meeting conversations: a user study", "author": ["Gabriel Murray", "Giuseppe Carenini", "Raymond T. Ng."], "venue": "INLG\u201910.", "citeRegEx": "Murray et al\\.,? 2010b", "shortCiteRegEx": "Murray et al\\.", "year": 2010}, {"title": "Long story short - global unsupervised models for keyphrase based meeting summarization", "author": ["Korbinian Riedhammer", "Benoit Favre", "Dilek Hakkani-T\u00fcr."], "venue": "Speech Commun., 52(10):801\u2013815, October.", "citeRegEx": "Riedhammer et al\\.,? 2010", "shortCiteRegEx": "Riedhammer et al\\.", "year": 2010}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["Ivan Titov", "Ryan McDonald."], "venue": "Proceeding of the 17th international conference on World Wide Web, WWW \u201908, pages 111\u2013120. ACM.", "citeRegEx": "Titov and McDonald.,? 2008", "shortCiteRegEx": "Titov and McDonald.", "year": 2008}, {"title": "Summarizing decisions in spoken meetings", "author": ["Lu Wang", "Claire Cardie."], "venue": "Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages, pages 16\u201324, Portland, Oregon, June. Association for Computational Linguis-", "citeRegEx": "Wang and Cardie.,? 2011", "shortCiteRegEx": "Wang and Cardie.", "year": 2011}, {"title": "Using confusion networks for speech summarization", "author": ["Shasha Xie", "Yang Liu."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, HLT \u201910, pages 46\u201354. Associ-", "citeRegEx": "Xie and Liu.,? 2010", "shortCiteRegEx": "Xie and Liu.", "year": 2010}, {"title": "Evaluating the effectiveness of features and sampling in extractive meeting summarization", "author": ["Shasha Xie", "Yang Liu", "Hui Lin."], "venue": "Proc. of IEEE Spoken Language Technology (SLT).", "citeRegEx": "Xie et al\\.,? 2008", "shortCiteRegEx": "Xie et al\\.", "year": 2008}, {"title": "Automatic summarization of open-domain multiparty dialogues in diverse genres", "author": ["Klaus Zechner."], "venue": "Comput. Linguist., 28:447\u2013485, December.", "citeRegEx": "Zechner.,? 2002", "shortCiteRegEx": "Zechner.", "year": 2002}], "referenceMentions": [{"referenceID": 36, "context": "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).", "startOffset": 132, "endOffset": 232}, {"referenceID": 26, "context": "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).", "startOffset": 132, "endOffset": 232}, {"referenceID": 11, "context": "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).", "startOffset": 132, "endOffset": 232}, {"referenceID": 20, "context": "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).", "startOffset": 132, "endOffset": 232}, {"referenceID": 29, "context": "Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a).", "startOffset": 132, "endOffset": 232}, {"referenceID": 5, "context": "Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.", "startOffset": 0, "endOffset": 23}, {"referenceID": 7, "context": "However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings.", "startOffset": 85, "endOffset": 119}, {"referenceID": 16, "context": "However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings.", "startOffset": 85, "endOffset": 119}, {"referenceID": 3, "context": "They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern\u00e1ndez et al.", "startOffset": 89, "endOffset": 107}, {"referenceID": 3, "context": "They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern\u00e1ndez et al. (2008), Frampton et al.", "startOffset": 89, "endOffset": 132}, {"referenceID": 3, "context": "They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern\u00e1ndez et al. (2008), Frampton et al. (2009). ilmaz and Hakkani-Tur, 2010).", "startOffset": 89, "endOffset": 156}, {"referenceID": 30, "context": "\u2022 As a step towards creating the abstractive summaries that people prefer when dealing with spoken language (Murray et al., 2010b), we propose a token-level rather than sentence-level framework", "startOffset": 108, "endOffset": 130}, {"referenceID": 2, "context": "We investigate three topic models \u2014 Local LDA (LocalLDA) (Brody and Elhadad, 2010), Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) and Segmented Topic Model (STM) (Du et al.", "startOffset": 57, "endOffset": 82}, {"referenceID": 32, "context": "We investigate three topic models \u2014 Local LDA (LocalLDA) (Brody and Elhadad, 2010), Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) and Segmented Topic Model (STM) (Du et al.", "startOffset": 109, "endOffset": 135}, {"referenceID": 8, "context": "We investigate three topic models \u2014 Local LDA (LocalLDA) (Brody and Elhadad, 2010), Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) and Segmented Topic Model (STM) (Du et al., 2010) \u2014 which can utilize the latent topic structure on utterance level instead of document level.", "startOffset": 168, "endOffset": 185}, {"referenceID": 27, "context": "tion (Murray and Renals, 2007), we investigate the role of context in our token-level summarization framework.", "startOffset": 5, "endOffset": 30}, {"referenceID": 4, "context": "Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008).", "startOffset": 162, "endOffset": 214}, {"referenceID": 11, "context": "Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008).", "startOffset": 162, "endOffset": 214}, {"referenceID": 35, "context": "Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008).", "startOffset": 162, "endOffset": 214}, {"referenceID": 36, "context": "methods, maximal marginal relevance (MMR) is investigated in (Zechner, 2002) and (Xie and Liu, 2010).", "startOffset": 61, "endOffset": 76}, {"referenceID": 34, "context": "methods, maximal marginal relevance (MMR) is investigated in (Zechner, 2002) and (Xie and Liu, 2010).", "startOffset": 81, "endOffset": 100}, {"referenceID": 12, "context": "Gillick et al. (2009) introduce a conceptbased global optimization framework by using integer linear programming (ILP).", "startOffset": 0, "endOffset": 22}, {"referenceID": 9, "context": "Only in very recent works has decision summarization been addressed in (Fern\u00e1ndez et al., 2008), (Bui et al.", "startOffset": 71, "endOffset": 95}, {"referenceID": 3, "context": ", 2008), (Bui et al., 2009) and (Wang and Cardie, 2011).", "startOffset": 9, "endOffset": 27}, {"referenceID": 33, "context": ", 2009) and (Wang and Cardie, 2011).", "startOffset": 12, "endOffset": 35}, {"referenceID": 9, "context": "(Fern\u00e1ndez et al., 2008) and (Bui et al.", "startOffset": 0, "endOffset": 24}, {"referenceID": 3, "context": ", 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases.", "startOffset": 12, "endOffset": 30}, {"referenceID": 3, "context": ", 2008), (Bui et al., 2009) and (Wang and Cardie, 2011). (Fern\u00e1ndez et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both utterance- and token- level.", "startOffset": 10, "endOffset": 396}, {"referenceID": 0, "context": "Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010).", "startOffset": 72, "endOffset": 197}, {"referenceID": 15, "context": "Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010).", "startOffset": 72, "endOffset": 197}, {"referenceID": 6, "context": "Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010).", "startOffset": 72, "endOffset": 197}, {"referenceID": 6, "context": "Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010).", "startOffset": 72, "endOffset": 197}, {"referenceID": 7, "context": "There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011).", "startOffset": 86, "endOffset": 146}, {"referenceID": 16, "context": "There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011).", "startOffset": 86, "endOffset": 146}, {"referenceID": 23, "context": "meetings (Liu et al., 2009; Liu et al., 2011) and keyphrase based summarization (Riedhammer et al.", "startOffset": 9, "endOffset": 45}, {"referenceID": 24, "context": "meetings (Liu et al., 2009; Liu et al., 2011) and keyphrase based summarization (Riedhammer et al.", "startOffset": 9, "endOffset": 45}, {"referenceID": 31, "context": ", 2011) and keyphrase based summarization (Riedhammer et al., 2010).", "startOffset": 42, "endOffset": 67}, {"referenceID": 0, "context": "In (Bhandari et al., 2008), several sentence scoring functions are introduced based on Probabilistic Latent Semantic Indexing.", "startOffset": 3, "endOffset": 26}, {"referenceID": 7, "context": "Chen and Chen (2008) propose a Topical Mixture Model (TMM) for speech summariza-", "startOffset": 0, "endOffset": 21}, {"referenceID": 13, "context": "(VSM) (Gong and Liu, 2001), Latent Semantic Analysis (LSA) (Gong and Liu, 2001) and Maximum Marginal Relevance (MMR) (Murray et al.", "startOffset": 6, "endOffset": 26}, {"referenceID": 13, "context": "(VSM) (Gong and Liu, 2001), Latent Semantic Analysis (LSA) (Gong and Liu, 2001) and Maximum Marginal Relevance (MMR) (Murray et al.", "startOffset": 59, "endOffset": 79}, {"referenceID": 28, "context": "(VSM) (Gong and Liu, 2001), Latent Semantic Analysis (LSA) (Gong and Liu, 2001) and Maximum Marginal Relevance (MMR) (Murray et al., 2005).", "startOffset": 117, "endOffset": 138}, {"referenceID": 15, "context": "Kullback-Lieber (KL) divergence is explored for summarization in (Haghighi and Vanderwende, 2009) and (Lin et al.", "startOffset": 65, "endOffset": 97}, {"referenceID": 22, "context": "Kullback-Lieber (KL) divergence is explored for summarization in (Haghighi and Vanderwende, 2009) and (Lin et al., 2010), where it is used to measure the distance of distributions between the document and the summary.", "startOffset": 102, "endOffset": 120}, {"referenceID": 2, "context": "Local LDA (LocalLDA) (Brody and Elhadad, 2010) uses almost the same probabilistic generative model as Latent Dirichlet Allocation (LDA) (Blei et al.", "startOffset": 21, "endOffset": 46}, {"referenceID": 1, "context": "Local LDA (LocalLDA) (Brody and Elhadad, 2010) uses almost the same probabilistic generative model as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), except that it treats each sentence as a separate document2.", "startOffset": 136, "endOffset": 155}, {"referenceID": 8, "context": "The last model we utilize is Segmented Topic Model (STM) (Du et al., 2010), which jointly models document- and sentence-level latent topics using a two-parameter Poisson Dirichlet Process (PDP).", "startOffset": 57, "endOffset": 74}, {"referenceID": 33, "context": "tions to create perfect partitionings of the DRDAs as the input; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011).", "startOffset": 196, "endOffset": 219}, {"referenceID": 33, "context": "tions to create perfect partitionings of the DRDAs as the input; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011). The Wang and Cardie (2011) cluster-", "startOffset": 197, "endOffset": 248}, {"referenceID": 21, "context": "To evaluate the performance of various summarization approaches, we use the widely accepted ROUGE (Lin and Hovy, 2003) metrics.", "startOffset": 98, "endOffset": 118}, {"referenceID": 21, "context": "To evaluate the performance of various summarization approaches, we use the widely accepted ROUGE (Lin and Hovy, 2003) metrics. We use the stemming option of the ROUGE software at http://berouge.com/ and remove stopwords from both the system and gold-standard summaries, same as Riedhammer et al. (2010) do.", "startOffset": 99, "endOffset": 304}, {"referenceID": 25, "context": "Inference and Hyperparameters We use the implementation from (Lu et al., 2011) for the three topic models in Section 4.", "startOffset": 61, "endOffset": 78}, {"referenceID": 14, "context": "The collapsed Gibbs Sampling approach (Griffiths and Steyvers, 2004) is exploited for inference.", "startOffset": 38, "endOffset": 68}, {"referenceID": 2, "context": "Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al.", "startOffset": 40, "endOffset": 65}, {"referenceID": 32, "context": "Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al.", "startOffset": 67, "endOffset": 93}, {"referenceID": 8, "context": "Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al., 2010).", "startOffset": 98, "endOffset": 115}, {"referenceID": 32, "context": "And the number of local topic is set as the same number of global topic as discussed in (Titov and McDonald, 2008).", "startOffset": 88, "endOffset": 114}, {"referenceID": 7, "context": "1 Baselines and Comparisons We compare our token-level summarization framework based on the fine-grained topic models to (1) two unsupervised baselines, (2) token-level summarization by LDA, (3) utterance-level summarization by Topical Mixture Model (TMM) (Chen and Chen, 2008), (4) utterance-level summarization based on the fine-grained topic models using existing metrics (Section 3.", "startOffset": 256, "endOffset": 277}, {"referenceID": 31, "context": "As in (Riedhammer et al., 2010) and (Wang and Cardie, 2011), this baseline simply selects the longest DRDA in each cluster as the summary.", "startOffset": 6, "endOffset": 31}, {"referenceID": 33, "context": ", 2010) and (Wang and Cardie, 2011), this baseline simply selects the longest DRDA in each cluster as the summary.", "startOffset": 12, "endOffset": 35}, {"referenceID": 33, "context": "Following Wang and Cardie (2011), the second baseline selects the", "startOffset": 10, "endOffset": 33}, {"referenceID": 7, "context": "How do fine-grained topic models compare to basic topic models or baselines? Figure 2 demonstrates that by using the DomSum token-level summarization framework, the three fine-grained topic models uniformly outperform the two non-trivial baselines and TMM (Chen and Chen, 2008) (reimplemented by us) that generates utterance-level summaries.", "startOffset": 256, "endOffset": 277}, {"referenceID": 17, "context": "We use Support Vector Machines (Joachims, 1998) with RBF kernel and order1 Conditional Random Fields (Lafferty et al.", "startOffset": 31, "endOffset": 47}, {"referenceID": 19, "context": "We use Support Vector Machines (Joachims, 1998) with RBF kernel and order1 Conditional Random Fields (Lafferty et al., 2001) \u2014 trained with the same features as (Wang and Cardie, 2011) to identify the summary-worthy tokens to include in the abstract.", "startOffset": 101, "endOffset": 124}, {"referenceID": 33, "context": ", 2001) \u2014 trained with the same features as (Wang and Cardie, 2011) to identify the summary-worthy tokens to include in the abstract.", "startOffset": 44, "endOffset": 67}], "year": 2016, "abstractText": "We present a token-level decision summarization framework that utilizes the latent topic structures of utterances to identify \u201csummaryworthy\u201d words. Concretely, a series of unsupervised topic models is explored and experimental results show that fine-grained topic models, which discover topics at the utterance-level rather than the document-level, can better identify the gist of the decisionmaking process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary.", "creator": "TeX"}}}