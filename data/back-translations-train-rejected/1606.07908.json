{"id": "1606.07908", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2016", "title": "Label Tree Embeddings for Acoustic Scene Classification", "abstract": "We present in this paper an efficient approach for acoustic scene classification by exploring the structure of class labels. Given a set of class labels, a category taxonomy is automatically learned by collectively optimizing a clustering of the labels into multiple meta-classes in a tree structure. An acoustic scene instance is then embedded into a low-dimensional feature representation which consists of the likelihoods that it belongs to the meta-classes. We demonstrate state-of-the-art results on two different datasets for the acoustic scene classification task, including the DCASE 2013 and LITIS Rouen datasets.", "histories": [["v1", "Sat, 25 Jun 2016 12:57:44 GMT  (151kb,D)", "https://arxiv.org/abs/1606.07908v1", "to appear in the Proceedings of ACM Multimedia 2016"], ["v2", "Tue, 26 Jul 2016 11:42:20 GMT  (156kb,D)", "http://arxiv.org/abs/1606.07908v2", "to appear in the Proceedings of ACM Multimedia 2016 (ACMMM 2016)"]], "COMMENTS": "to appear in the Proceedings of ACM Multimedia 2016", "reviews": [], "SUBJECTS": "cs.MM cs.AI cs.SD", "authors": ["huy phan", "lars hertel", "marco maass", "philipp koch", "alfred mertins"], "accepted": false, "id": "1606.07908"}, "pdf": {"name": "1606.07908.pdf", "metadata": {"source": "META", "title": "Label Tree Embeddings for Acoustic Scene Classification", "authors": ["Huy Phan", "Lars Hertel", "Marco Maass", "Philipp Koch", "Alfred Mertins"], "emails": ["phan@isip.uni-luebeck.de", "hertel@isip.uni-luebeck.de", "maass@isip.uni-luebeck.de", "koch@isip.uni-luebeck.de", "mertins@isip.uni-luebeck.de", "permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords acoustic scene classification; embedding of label trees; spectral clustering"}, {"heading": "1. INTRODUCTION", "text": "In fact, it is such that most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to move, to, to, to move, to, to,"}, {"heading": "2. THE PROPOSED APPROACH", "text": "In this section, we first introduce the framework to learn the label trees and label tree embedding for feature mapping, and then develop various label tree embedding that are derived from the framework and the final classification step."}, {"heading": "2.1 Learning a Label Tree", "text": "In fact, we will be able to hold our own, we will be able to hold our own, we will be able to put ourselves in the lead."}, {"heading": "2.3 Scene and Speech LTEs", "text": "Using the frame described above, we examine the following LTEs to handle the acoustic scene classification: (1) the LTE derived from a target scene database itself (SceneLTE), (2) the LTE learned from an external voice channel (Speech-LTE), and (3) its combination (Fusion-LTE). Scene LTE resulting from a target scene database (e.g. LITIS Rouen) is formed according to the frame. However, we do not consider the entire 30-second snippet of an acoustic scene instance as an example. Instead, to capture significant events in a scene whose length is in the order of hundreds of milliseconds, we use segments of the length 500 ms with an overlap of 250 ms as samples for further processing. Each segment is broken down into 50 ms frames with 50% overlap, each of which is described by M = 128 coefficients."}, {"heading": "2.4 Final Acoustic Scene Classification", "text": "An overview of the final classification schemes is shown in Figure 2. For a test scene, we obtained their representations using the Scene LTE and SpeechLTE as described above. To extract representations for the training instances, we performed a 10-fold cross validation of the training data. Finally, we trained the final scene classification systems using one-on-one support vector machines (SVM) with different cores, including linear, \u03c72, histogram intersections (hist for short) and radial base functions (RBF). For Fusion LTE, we used non-linear SVMs with the extended Gaussian core specified in (7). Hyper parameters of the SVMs were tuned via a 10-fold cross validation."}, {"heading": "3. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Datasets", "text": "We used the following two datasets in our experiments: DCASE 2013 dataset [2, 24]. This dataset was used in the DCASE 2013 Challenge [24]. It consists of ten scene categories recorded at different locations in London at different times. The dataset has two subsets: public and private subsets, each containing 100-second scene instances with ten examples for each class. The former was published during the challenge for participants to optimize their classification systems, the latter was used to evaluate the submissions and was also published after the challenge. The submitted systems were evaluated with a five-layered cross-validation on the private subset [24]. We follow the cross-validation setting, but combined each time the public set and the training folds of the private set to create the training data. LITIS Rouen dataset [23]. This dataset contains 3026 30-second examples of 19 urban scenario categories recorded with a total duration of 1500 minutes."}, {"heading": "3.2 Experimental Results", "text": "The classification performance obtained from Scene-LTE, Speech-LTE and Fusion-LTE is even better than the selected Tables 1, 2 and 3. For the DCASE datasets, the performance in terms of classification accuracy is presented as in the DCASE 2013 Challenge [24], whereas we use an average class-by-class F1 score for the LITIS datasets as they exhibit significant imbalances in the number of samples per class. With Scene-LTE, our systems achieve an accuracy of 86% and an F1 score of 94.9% for the DCASE and LITIS datasets, outperforming the best reported performance on the DCASE dataset (85% in terms of accuracy), while we only marginally below the best performance on the LITIS datasets (95.6% for the DCASE datasets and LITIS datasets)."}, {"heading": "4. CONCLUSIONS", "text": "In this paper, we present efficient schemes for acoustic classification of scenes. We examine the structure of class labels by automatically learning class labeling hierarchies, and then the labeling hierarchies to map scene instances into the semantic space that underlies the class hierarchy. We examine both the labeling tree learned intrinsically from scene data and that learned from external TIMIT language data, both of which show good empirical performance of experimental datasets, including DCASE 2013 and LITIS Rouen datasets. Moreover, merging with a simple scheme results in state-of-the-art performance."}, {"heading": "5. ACKNOWLEDGMENTS", "text": "This work was supported by the Graduate School of Computing in Medicine and Life Sciences, which is funded by the German Excellence Initiative [DFG GSC 235 / 1]."}, {"heading": "6. REFERENCES", "text": "[1] S. Ag caer, A. Schlesinger, F.-M. Hoffmann, andR. Martin Martin. Optimization of amplitude modulation features for low-resource acoustic scene classification. In Proc. European Signal Processing Conference (EUSIPCO), pp. 2556-2560, 2015. [2] D. Barchiesi, D. Giannoulis, D. Stowell, and M. Plumbley. Acoustic scene classification: Classifying environments from the sounds they produce. IEEE Signal Processing Magazine, 32 (3): 16-34, 2015. [3] S. Bengio, J. Weston, and D. Grangier. Label embedding trees for large multi-class tasks. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 163-171, 2010. [4] V. Bisot, S. Essid, and G. Richard."}], "references": [{"title": "Optimization of amplitude modulation features for low-resource acoustic scene classification", "author": ["S. A\u011fcaer", "A. Schlesinger", "F.-M. Hoffmann", "R. Martin"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), pages 2556\u20132560", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic scene classification: Classifying environments from the sounds they produce", "author": ["D. Barchiesi", "D. Giannoulis", "D. Stowell", "M. Plumbley"], "venue": "IEEE Signal Processing Magazine, 32(3):16\u201334", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Label embedding trees for large multi-class tasks", "author": ["S. Bengio", "J. Weston", "D. Grangier"], "venue": "Proc. Advances in Neural Information Processing Systems (NIPS), pages 163\u2013171", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "HOG and subband power distribution image features for acoustic scene classification", "author": ["V. Bisot", "S. Essid", "G. Richard"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), pages 719\u2013723", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic scene classification with matrix factorization for unsupervised feature learning", "author": ["V. Bisot", "R. Serizel", "S. Essid", "G. Richard"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6445\u20136449", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Random forest", "author": ["L. Breiman"], "venue": "Machine Learning, 45:5\u201332", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Co-clustering for auditory scene categorization", "author": ["R. Cai", "L. Lu", "A. Hanjalic"], "venue": "IEEE Trans. Multimedia, 10(4):596\u2013606", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Where am I? Scene recognition for mobile robots using audio features", "author": ["S. Chu", "S. Narayanan", "C.-C.J. Kuo", "M.J. Mataric"], "venue": "Proc. IEEE International Conference on Multimedia and Expo (ICME), pages 885\u2013888", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Robust minimum statistics project coefficients feature for acoustic environment recognition", "author": ["S. Deng", "J. Han", "C. Zhang", "T. Zheng", "G. Zheng"], "venue": "Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8232\u20138236", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Audio-based context recognition", "author": ["A.J. Eronen", "V.T. Peltonen", "J.T. Tuomi", "A.P. Klapuri", "S. Fagerlund", "T. Sorsa", "G. Lorho", "J. Huopaniemi"], "venue": "IEEE Trans. Audio, Speech, and Language Processing, 14(1):321\u2013329", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "The DARPA speech recognition research database: Specifications and status", "author": ["W. Fisher", "G. Doddington", "K. Goudie-Marshall"], "venue": "Proc. DARPA Workshop on Speech Recognition, pages 93\u201399", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1986}, {"title": "Context-dependent sound event detection", "author": ["T. Heittola", "A. Mesaros", "A. Eronen", "T. Virtanen"], "venue": "EURASIP Journal on Audio, Speech, and Music Processing", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Audio context recognition using audio event histogram", "author": ["T. Heittola", "A. Mesaros", "A.J. Eronen", "T. Virtanen"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), pages 1272\u20131276", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "M", "author": ["I. Laptev"], "venue": "Marsza lek, C. Schmid, and B. Rozenfeld. Learning realistic human actions from movies. In Proc CVPR, pages 1\u20138", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Machine hearing: An emerging field", "author": ["R.F. Lyon"], "venue": "IEEE Signal Processing Magazine, 27(5):131\u2013139", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Noise-robust environmental sound classification method based on combination of  ICA and MP features", "author": ["R. Mogi", "H. Kasaii"], "venue": "Artificial Intelligence Research, 2(1):107\u2013121", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "On spectral clustering: Analysis and an algorithm", "author": ["A.Y. Ng", "M.I. Jordan", "Y. Weiss"], "venue": "Proc. NIPS, pages 849\u2013856", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Deep neural networks for audio scene recognition", "author": ["Y. Petetin", "C. Laroche", "A. Mayoue"], "venue": "Proc. European Signal Processing Conference (EUSIPCO), pages 125\u2013129", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Representing nonspeech audio signals through speech classification models", "author": ["H. Phan", "L. Hertel", "M. Maass", "R. Mazur", "A. Mertins"], "venue": "Proc. Interspeech, pages 3441\u20133445", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning representations for nonspeech audio events through their similarities to speech patterns", "author": ["H. Phan", "L. Hertel", "M. Maass", "R. Mazur", "A. Mertins"], "venue": "IEEE/ACM Trans. Audio, Speech, and Language Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Audio analysis for surveillance applications", "author": ["R. Radhakrishnan", "A. Divakaran", "P. Smaragdis"], "venue": "Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pages 158\u2013161", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Histogram of gradients of time-frequency representations for audio scene classification", "author": ["A. Rakotomamonjy", "G. Gasso"], "venue": "IEEE/ACM Trans. Audio, Speech, and Language Processing, 23(1):142\u2013153", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Detection and classification of acoustic scenes and events", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M.D. Plumbley"], "venue": "IEEE Trans. Multimedia, 17(10):1733\u20131746", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Gammatone cepstral coefficients: biologically inspired features fro non-speech audio classification", "author": ["X. Valero", "F. A\u013a\u0131as"], "venue": "IEEE Trans. Multimedia, 17(6):1684\u20131689", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Computational Auditory Scene Analysis: Principles", "author": ["D. Wang", "G.J. Brown"], "venue": "Algorithms, and Applications. Wiley-IEEE Press", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Intelligent Wearable Interfaces", "author": ["Y. Xu", "W.J. Li", "K.K. Lee"], "venue": "Hoboken, NJ: Wiley", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Acoustic scene classification based on sound textures and events", "author": ["J. Ye", "T. Kobayashi", "M. Murakawa", "T. Higuchi"], "venue": "Proc. ACM Multimedia, pages 1291\u20131294", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 24, "context": "Acoustic scene classification (ASC) is an important problem of computational auditory scene analysis [26, 16].", "startOffset": 101, "endOffset": 109}, {"referenceID": 14, "context": "Acoustic scene classification (ASC) is an important problem of computational auditory scene analysis [26, 16].", "startOffset": 101, "endOffset": 109}, {"referenceID": 20, "context": "Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 7, "context": "Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11].", "startOffset": 202, "endOffset": 205}, {"referenceID": 25, "context": "Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11].", "startOffset": 234, "endOffset": 242}, {"referenceID": 9, "context": "Solving this problem will allow a device to recognize a surrounding environment via the sound it captures, and hence, enables a wide range of applications, such as surveillance [22], robotic navigation [8], and context-aware services [27, 11].", "startOffset": 234, "endOffset": 242}, {"referenceID": 11, "context": "A recognized scene can also be used as a prior information to improve the performance of sound event detection [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 17, "context": "Different features adapted from the related problems, such as speech recognition and audio event classification, have been used to characterize an acoustic scene, for instance MFCC [19, 24] and Gammatone filters [25].", "startOffset": 181, "endOffset": 189}, {"referenceID": 22, "context": "Different features adapted from the related problems, such as speech recognition and audio event classification, have been used to characterize an acoustic scene, for instance MFCC [19, 24] and Gammatone filters [25].", "startOffset": 181, "endOffset": 189}, {"referenceID": 23, "context": "Different features adapted from the related problems, such as speech recognition and audio event classification, have been used to characterize an acoustic scene, for instance MFCC [19, 24] and Gammatone filters [25].", "startOffset": 212, "endOffset": 216}, {"referenceID": 21, "context": "2967268 [23, 4, 28] and Gabor dictionaries [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 3, "context": "2967268 [23, 4, 28] and Gabor dictionaries [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 26, "context": "2967268 [23, 4, 28] and Gabor dictionaries [17].", "startOffset": 8, "endOffset": 19}, {"referenceID": 15, "context": "2967268 [23, 4, 28] and Gabor dictionaries [17].", "startOffset": 43, "endOffset": 47}, {"referenceID": 1, "context": "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].", "startOffset": 52, "endOffset": 62}, {"referenceID": 12, "context": "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].", "startOffset": 52, "endOffset": 62}, {"referenceID": 6, "context": "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].", "startOffset": 52, "endOffset": 62}, {"referenceID": 8, "context": "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].", "startOffset": 81, "endOffset": 84}, {"referenceID": 26, "context": "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].", "startOffset": 108, "endOffset": 112}, {"referenceID": 1, "context": "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].", "startOffset": 161, "endOffset": 171}, {"referenceID": 12, "context": "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].", "startOffset": 161, "endOffset": 171}, {"referenceID": 6, "context": "At a higher semantic level, foreground sound events [2, 14, 7], background noise [9], and their combination [28] can be used as a footprint to represent a scene [2, 14, 7].", "startOffset": 161, "endOffset": 171}, {"referenceID": 18, "context": "We study the class hierarchy learned from the acoustic scene data themselves as well as the one learned from external speech data [20, 21].", "startOffset": 130, "endOffset": 138}, {"referenceID": 19, "context": "We study the class hierarchy learned from the acoustic scene data themselves as well as the one learned from external speech data [20, 21].", "startOffset": 130, "endOffset": 138}, {"referenceID": 22, "context": "In addition, combining them with a simple fusion scheme leads to state-of-the-art performance on both target datasets: DCASE 2013 [24] and LITIS Rouen datasets [23].", "startOffset": 130, "endOffset": 134}, {"referenceID": 21, "context": "In addition, combining them with a simple fusion scheme leads to state-of-the-art performance on both target datasets: DCASE 2013 [24] and LITIS Rouen datasets [23].", "startOffset": 160, "endOffset": 164}, {"referenceID": 2, "context": "In order to explore the structure of class labels, we learn a label tree similar to [3].", "startOffset": 84, "endOffset": 87}, {"referenceID": 21, "context": "Figure 1: A subtree extracted from the label tree learned from the LITIS Rouen dataset [23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "We train the multi-class classifier M using random forest classification [6] with 200 trees using S train and then evaluate it on the evaluation set S eval to obtain the confusion matrix A \u2208 R|l|\u00d7|l|.", "startOffset": 73, "endOffset": 76}, {"referenceID": 16, "context": "We apply spectral clustering [18] on the matrix \u0100 to solve a relaxed version of the optimization problem in (3).", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "We demonstrate in Figure 1 a subtree extracted from the label tree learned from the LITIS Rouen dataset [23] (more details in Section 2.", "startOffset": 104, "endOffset": 108}, {"referenceID": 5, "context": "Here, P (negative|x,Mi) and P (positive|x,Mi) are the classification probabilities outputted by Mi when evaluating on x, thanks to the probability support of the random forest classification [6].", "startOffset": 191, "endOffset": 194}, {"referenceID": 23, "context": "Each segment is decomposed into 50 ms frames with 50% overlap, each of which is described by M = 128 Gammatone cepstral coefficients [25, 10] in the frequency range of 1-11025 Hz.", "startOffset": 133, "endOffset": 141}, {"referenceID": 18, "context": "Speech signals have been shown to bear potential to serve as a generic representation for nonspeech audio events [20, 21].", "startOffset": 113, "endOffset": 121}, {"referenceID": 19, "context": "Speech signals have been shown to bear potential to serve as a generic representation for nonspeech audio events [20, 21].", "startOffset": 113, "endOffset": 121}, {"referenceID": 19, "context": "The Speech-LTE is learned from a set of phone triplets [21] selected from TIMIT speech data [12].", "startOffset": 55, "endOffset": 59}, {"referenceID": 10, "context": "The Speech-LTE is learned from a set of phone triplets [21] selected from TIMIT speech data [12].", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "Note that, instead of the Gammatone cepstral coefficients used in the Scene-LTE, we utilized the same low-level feature set in [21] for the Speech-LTE.", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "Scene-LTE and SpeechLTE, we combine them using the extended Gaussian-\u03c7 kernel [15] given by", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "We employed the following two datasets in our experiments: DCASE 2013 dataset [2, 24].", "startOffset": 78, "endOffset": 85}, {"referenceID": 22, "context": "We employed the following two datasets in our experiments: DCASE 2013 dataset [2, 24].", "startOffset": 78, "endOffset": 85}, {"referenceID": 22, "context": "This dataset was used in the DCASE 2013 challenge [24].", "startOffset": 50, "endOffset": 54}, {"referenceID": 22, "context": "The submitted systems were evaluated with five-fold stratified cross validation on the private subset [24].", "startOffset": 102, "endOffset": 106}, {"referenceID": 21, "context": "LITIS Rouen dataset [23].", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "We follow the standard training/testing splits in [23] (for more details, please refer to [23]) and report average performances over 20 splits of the data.", "startOffset": 50, "endOffset": 54}, {"referenceID": 21, "context": "We follow the standard training/testing splits in [23] (for more details, please refer to [23]) and report average performances over 20 splits of the data.", "startOffset": 90, "endOffset": 94}, {"referenceID": 22, "context": "For the DCASE dataset, the performance is reported in terms of classification accuracy as in the DCASE 2013 challenge [24] whereas we used average class-wise F1-score for the LITIS dataset since it exhibits significant imbalance in the numbers of samples per class.", "startOffset": 118, "endOffset": 122}, {"referenceID": 0, "context": "These results surpass the best reported performance on the DCASE dataset (85% in terms of accuracy [1]) while being just marginally below the best performance on the LITIS dataset (95.", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "6% in terms of F1-score [5]).", "startOffset": 24, "endOffset": 27}, {"referenceID": 21, "context": "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.", "startOffset": 29, "endOffset": 37}, {"referenceID": 17, "context": "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.", "startOffset": 29, "endOffset": 37}, {"referenceID": 3, "context": "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.", "startOffset": 67, "endOffset": 73}, {"referenceID": 4, "context": "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.", "startOffset": 67, "endOffset": 73}, {"referenceID": 3, "context": "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.", "startOffset": 96, "endOffset": 103}, {"referenceID": 26, "context": "average class-wise precision [23, 19], average class-wise F1-score [4, 5], and overall accuracy [4, 28], we provide our performance on all of these metrics to make a proper comparison.", "startOffset": 96, "endOffset": 103}, {"referenceID": 22, "context": "0 RNH [24] 76.", "startOffset": 6, "endOffset": 10}, {"referenceID": 22, "context": "0 MV [24] 77.", "startOffset": 5, "endOffset": 9}, {"referenceID": 1, "context": "Human [2] 75.", "startOffset": 6, "endOffset": 9}, {"referenceID": 21, "context": "HOG [23] 76.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "0 AMS+LDA [1] 85.", "startOffset": 10, "endOffset": 13}, {"referenceID": 21, "context": "4 HOG [23] 91.", "startOffset": 6, "endOffset": 10}, {"referenceID": 3, "context": "7 \u2212 \u2212 HOG+SPD [4] 93.", "startOffset": 14, "endOffset": 17}, {"referenceID": 17, "context": "4 DNN+MFCC [19] 92.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "2 \u2212 \u2212 Sparse NMF [5] \u2212 94.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "1 \u2212 Convolutive NMF [5] \u2212 94.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "5 \u2212 Kernel PCA [5] \u2212 95.", "startOffset": 15, "endOffset": 18}, {"referenceID": 26, "context": "6 \u2212 HOG+ProbSVM [28] \u2212 \u2212 96.", "startOffset": 16, "endOffset": 20}, {"referenceID": 22, "context": "RNH [24]) with a large margin of about 10% and also outrun the best reported performance in [1] from 1% to 2%.", "startOffset": 4, "endOffset": 8}, {"referenceID": 0, "context": "RNH [24]) with a large margin of about 10% and also outrun the best reported performance in [1] from 1% to 2%.", "startOffset": 92, "endOffset": 95}], "year": 2016, "abstractText": "We present in this paper an efficient approach for acoustic scene classification by exploring the structure of class labels. Given a set of class labels, a category taxonomy is automatically learned by collectively optimizing a clustering of the labels into multiple meta-classes in a tree structure. An acoustic scene instance is then embedded into a low-dimensional feature representation which consists of the likelihoods that it belongs to the meta-classes. We demonstrate state-of-the-art results on two different datasets for the acoustic scene classification task, including the DCASE 2013 and LITIS Rouen datasets.", "creator": "LaTeX with hyperref package"}}}