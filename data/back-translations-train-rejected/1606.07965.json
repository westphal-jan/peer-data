{"id": "1606.07965", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Jun-2016", "title": "Summarizing Decisions in Spoken Meetings", "abstract": "This paper addresses the problem of summarizing decisions in spoken meetings: our goal is to produce a concise {\\it decision abstract} for each meeting decision. We explore and compare token-level and dialogue act-level automatic summarization methods using both unsupervised and supervised learning frameworks. In the supervised summarization setting, and given true clusterings of decision-related utterances, we find that token-level summaries that employ discourse context can approach an upper bound for decision abstracts derived directly from dialogue acts. In the unsupervised summarization setting,we find that summaries based on unsupervised partitioning of decision-related utterances perform comparably to those based on partitions generated using supervised techniques (0.22 ROUGE-F1 using LDA-based topic models vs. 0.23 using SVMs).", "histories": [["v1", "Sat, 25 Jun 2016 20:45:14 GMT  (29kb)", "http://arxiv.org/abs/1606.07965v1", "ACL Workshop on Automatic Summarization for Different Genres, Media, and Languages, 2011"]], "COMMENTS": "ACL Workshop on Automatic Summarization for Different Genres, Media, and Languages, 2011", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lu wang", "claire cardie"], "accepted": false, "id": "1606.07965"}, "pdf": {"name": "1606.07965.pdf", "metadata": {"source": "CRF", "title": "Summarizing Decisions in Spoken Meetings", "authors": ["Lu Wang"], "emails": ["luwang@cs.cornell.edu", "cardie@cs.cornell.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.07 965v 1 [cs.C L] 25 Jun 20"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to play by the rules they have imposed on themselves."}, {"heading": "2 Related Work", "text": "There is much previous research on automatic summarization of texts using corpus-based, knowledge-based, or statistical methods (Mani, 1999; Marcu, 2000), but dialog summarization methods generally attempt to take into account the specific characteristics of language. Among the early work in this subfield, Zechner (2002) investigates the summary of speeches based on maximum marginal relevance (MMR) and the linking of information. Popular verified methods for summarizing language - including maximum entropy, conditional random fields (CRFs), and support for vector machines (SVMs) - are examined in Buist et al. (2004), Xie et al. (2008), and Galley (2006). Techniques for determining semantic similarity are considered relevant for the selection of utterances in Gurevych and Strube (2004).Studies in Banerjee et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al. 2005 are considered to show that DRP decisions are the most important results of DRP decisions."}, {"heading": "3 Clustering Decision-Related Dialogue Acts", "text": "We design a pre-processing step that facilitates the summary of decisions by clustering all decision-related dialog actions according to the decision (s) it supports. Since it is not clear how many decisions are made in a meeting, we use a hierarchical agglomerative cluster algorithm (instead of techniques that require a priori knowledge of the number of clusters) and select the correct stop conditions. In particular, we use methods of the average link method: For each iteration, we merge the two clusters with the maximum pair-wise similarity of their DRDAs. In the following subsections, we present uncontrolled and monitored methods for measuring the pair-wise DRDA similarity."}, {"heading": "3.1 DRDA Similarity: Unsupervised Methods", "text": "We look at two unattended similarity models - one based on the TF-IDF score of the Information Retrieval Research Community and a second based on latent Dirichlet allocation topic models.TF-IDF similarity metrics have proven to be a measure of document similarity, so we use them as a measure to measure the similarity of two DRDAs. Suppose there are different types of words in the corpus. We treat any decision-related dialgy as a document and present it as an L-dimensional feature vector. \u2212 FVi = (xi1, xi2,..., xiL), where xik is the word wk's tf \u00b7 idf score for DAi. Then, the (average) similarity of clusters Cm and cluster Cn, Sim TFIDF (Cm, Cn), is defined as a distribution model."}, {"heading": "3.2 DRDA Similarity: Supervised Techniques", "text": "In addition to the unattended methods of clustering DRDAs, we are also investigating an approach based on pairwise supervised learning: We are developing a classifier that determines whether a pair of DRDAs supports the same decision or not. So each training and test example is a feature vector that is a function of two DRDAs: for DAi and DAj, the feature vector is \u2212 \u2192 FVij = f (DAi, DAj) = {fv 1 ij, fv 2 ij,..., fv k ij}. Table 2 contains a full list of features used. Since the annotations for the time information and dialog types of DAs are available from the corpus, we are using features such as time difference of paired DAs, relative position5, and whether they3We cannot readily associate each topic with a decision, because the number of decisions is not known as Priority 4 Parameter Estimation and Conclusion \u2192 DDA Conclusion \u2212 because the relative position of the SVN is known as a pairwise one."}, {"heading": "3.3 Experiments", "text": "In this context, it should be noted that the results of the study are not results, but results that are linked to the results of the study. (...) It is assumed that all decision-relevant DAs have been identified (but not with the decision (...) that they support. (...) Two cluster baselines are used for comparison. (...) A baseline represents all decision-relevant DAs for the meeting in a single partition and assumes that all decision-relevant DAs have been identified (...) but are not linked to the decision (...). (...) The second phase is linked from the second phase to the final phase of decision-making."}, {"heading": "4 Decision Summarization", "text": "In this section, we turn to summarizing decisions - and extract a brief description of each decision based on the decision-related DAs in each cluster. We explore options for creating an extract-based summary consisting of a single DRDA and an abstract summary containing keywords that describe the decision. For both types of summaries, we use standard techniques from the summary of texts, but also examine the use of dialog-specific features and the use of discourse contexts."}, {"heading": "4.1 DA-Level Summarization Based on Unsupervised Methods", "text": "We use two unguarded methods to summarize the DRDAs in each \"decision cluster\": the first method simply delivers the longest DRDA in the6The MUC Scorer is popular in coreference evaluation, but it is flawed in measuring the singleton clusters that predominate in the AMI corpus. Therefore, we do not use it in this working cluster as a summary (LONGEST DA); the second approach delivers the decision cluster prototype, i.e. the DRDA that is most similar between TF and IDF to the cluster centroid (PROTOTYPE DA). Although important decision-related information can be distributed across multiple DRDAs, both unguarded methods allow us to determine the quality of summaries when summaries are limited to a single statement."}, {"heading": "4.2 DA-Level and Token-Level Summarization Using Supervised Learning", "text": "Since the AMI corpus contains a decision abstract for every decision made at the meeting, we can use this supervisory information to train classifiers who can identify informative DRDAs (for DA-level summaries) or informative tokens (for token-level summaries). Previous research (e.g. Murray et al. (2005), Galley (2006), Gurevych and Strube (2004) has shown that an extractive summary at the DRDA level can be effective when viewed as a binary classification task. To implement this approach, we assume that the DRDA to be extracted for the summary is the vocabulary that overlaps with the gold standard decision abstract of the cluster. This DA-level summary method has the advantage that the summary maintains good legibility without a natural language generation component."}, {"heading": "4.3 Dialogue Cues for Decision Summarization", "text": "Contrary to the text, dialogs have some notable features that we expect to be useful for finding informative, decision-based statements. In this section, some of the dialogue-based features used in our classifiers are described; the full list of features is in Table 4 and Table 5. Structural information: Adjacency Pairs. An Adjacency Pair (AP) is an important dialogical analytical concept; APs are considered to be the basic unit of dialogical organization (Schegloff and Sacks, 1973). In the AMI corpus, an AP pair consists of a source utterance and a target utterance produced by different speakers. The source precedes the target, but they are not necessarily side by side. We include features to indicate whether two DAs APs specify QUESTION + ANSWER or POSITIVE FEEDBACK. For these features, we use the gold standard AP annotations."}, {"heading": "5 Experiments", "text": "In fact, most people are able to survive themselves, \"he told the German Press Agency.\" I don't think people are able to survive themselves, \"he said.\" I don't think they're able to survive themselves. \"He added,\" I don't think they're able to survive themselves. \"He added,\" I don't think they're able to survive themselves. \"He added,\" I don't think they're able to survive themselves. \"He added,\" I don't think they're able to survive themselves, and they're able to survive themselves. \""}, {"heading": "5.1 Sample Decision Summaries", "text": "Here we show sample summaries created using our methods (Table 8). We select one of the clusters created by LDA, which consists of four DAs that support two decisions and use SVMs as a supervised summary method. We remove function words and special markers such as \"[Disfmarker]\" from the DAs. Results indicate that either the longest DA or the prototype DA contains some of the decisions in this \"mixed\" cluster. Adding a discourse context refines the summaries at both the DA and token levels."}, {"heading": "6 Conclusion", "text": "In this paper, we examine methods for producing decision summaries from spoken meetings at both DA and token levels. We show that clus-10obtain summaries of F1s of 0.1954 and 0.1329 using abstracts without function words and using LDA-generated clusterings and CRFs at DA and token levels, which is slightly better than the corresponding 0.1935 and 0.1307 in Table 7. Similarly, when using DA- and token-level SVMs, we obtain summaries of 0.2367 and 0.1861 instead of 0.2349 and 0.1843. All other results achieve marginally small increases in F1.tering DRDAs before determining informative content to improve the quality of the summary. We also find that uncontrolled clustering of DRDAs (using LDA-based topic models) can generate summaries of comparable quality generated by verified DRDA clusters."}], "references": [{"title": "Algorithms for scoring coreference chains", "author": ["Amit Bagga", "Breck Baldwin."], "venue": "In The First International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference, pages 563\u2013566.", "citeRegEx": "Bagga and Baldwin.,? 1998", "shortCiteRegEx": "Bagga and Baldwin.", "year": 1998}, {"title": "The necessity of a meet", "author": ["Satanjeev Banerjee", "Carolyn Penstein Ros\u00e9", "Alexander I. Rudnicky"], "venue": null, "citeRegEx": "Banerjee et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Banerjee et al\\.", "year": 2005}, {"title": "A maximum entropy approach to natural language processing", "author": ["Adam L. Berger", "Vincent J. Della Pietra", "Stephen A. Della Pietra."], "venue": "Comput. Linguist., 22:39\u201371, March.", "citeRegEx": "Berger et al\\.,? 1996", "shortCiteRegEx": "Berger et al\\.", "year": 1996}, {"title": "Latent dirichlet allocation", "author": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "J. Mach. Learn. Res., 3:993\u20131022, March.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Extracting decisions from multi-party dialogue using directed graphical models and semantic similarity", "author": ["Trung H. Bui", "Matthew Frampton", "John Dowding", "Stanley Peters."], "venue": "Proceedings of the SIGDIAL 2009 Conference, pages 235\u2013243.", "citeRegEx": "Bui et al\\.,? 2009", "shortCiteRegEx": "Bui et al\\.", "year": 2009}, {"title": "Automatic summarization of meeting data: A feasibility study", "author": ["Anne Hendrik Buist", "Wessel Kraaij", "Stephan Raaijmakers."], "venue": "in Proc. Meeting of Computational Linguistics in the Netherlands (CLIN.", "citeRegEx": "Buist et al\\.,? 2004", "shortCiteRegEx": "Buist et al\\.", "year": 2004}, {"title": "Advances in domain independent linear text segmentation", "author": ["Freddy Y.Y. Choi."], "venue": "Proceedings of the 1st North American chapter of the Association for Computational Linguistics conference, pages 26\u201333.", "citeRegEx": "Choi.,? 2000", "shortCiteRegEx": "Choi.", "year": 2000}, {"title": "Identifying relevant phrases to summarize decisions in spoken meetings", "author": ["Raquel Fern\u00e1ndez", "Matthew Frampton", "John Dowding", "Anish Adukuzhiyil", "Patrick Ehlen", "Stanley Peters."], "venue": "INTERSPEECH-2008, pages 78\u201381.", "citeRegEx": "Fern\u00e1ndez et al\\.,? 2008a", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2008}, {"title": "Modelling and detecting decisions in multi-party dialogue", "author": ["Raquel Fern\u00e1ndez", "Matthew Frampton", "Patrick Ehlen", "Matthew Purver", "Stanley Peters."], "venue": "Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 156\u2013163.", "citeRegEx": "Fern\u00e1ndez et al\\.,? 2008b", "shortCiteRegEx": "Fern\u00e1ndez et al\\.", "year": 2008}, {"title": "Real-time decision detection in multi-party dialogue", "author": ["Matthew Frampton", "Jia Huang", "Trung Huu Bui", "Stanley Peters."], "venue": "Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3 - Volume 3, pages 1133\u20131141.", "citeRegEx": "Frampton et al\\.,? 2009", "shortCiteRegEx": "Frampton et al\\.", "year": 2009}, {"title": "A skip-chain conditional random field for ranking meeting utterances by importance", "author": ["Michel Galley."], "venue": "Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 364\u2013 372.", "citeRegEx": "Galley.,? 2006", "shortCiteRegEx": "Galley.", "year": 2006}, {"title": "Semantic similarity applied to spoken dialogue summarization", "author": ["Iryna Gurevych", "Michael Strube."], "venue": "Proceedings of the 20th international conference on Computational Linguistics.", "citeRegEx": "Gurevych and Strube.,? 2004", "shortCiteRegEx": "Gurevych and Strube.", "year": 2004}, {"title": "Text categorization with Sup", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Advances in Automatic Text Sum", "author": ["Inderjeet Mani"], "venue": null, "citeRegEx": "Mani.,? \\Q1999\\E", "shortCiteRegEx": "Mani.", "year": 1999}, {"title": "The Theory and Practice", "author": ["Daniel Marcu"], "venue": null, "citeRegEx": "Marcu.,? \\Q2000\\E", "shortCiteRegEx": "Marcu.", "year": 2000}, {"title": "Opening up clos", "author": ["E.A. Schegloff", "H. Sacks"], "venue": null, "citeRegEx": "Schegloff and Sacks.,? \\Q1973\\E", "shortCiteRegEx": "Schegloff and Sacks.", "year": 1973}, {"title": "Automatic summarization", "author": ["Klaus Zechner"], "venue": null, "citeRegEx": "Zechner.,? \\Q2002\\E", "shortCiteRegEx": "Zechner.", "year": 2002}], "referenceMentions": [{"referenceID": 6, "context": "While there has been some previous work in summarizing meetings and conversations, very little work has focused on decision summarization: Fern\u00e1ndez et al. (2008a) and Bui et al.", "startOffset": 139, "endOffset": 164}, {"referenceID": 4, "context": "(2008a) and Bui et al. (2009) investigate the use of a semantic parser and machine learning methods for phrase- and token-level decision summarization.", "startOffset": 12, "endOffset": 30}, {"referenceID": 4, "context": "These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern\u00e1ndez et al.", "startOffset": 90, "endOffset": 108}, {"referenceID": 4, "context": "These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern\u00e1ndez et al. (2008a), Frampton et al.", "startOffset": 90, "endOffset": 134}, {"referenceID": 4, "context": "These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern\u00e1ndez et al. (2008a), Frampton et al. (2009). The latter refer to all DAs", "startOffset": 90, "endOffset": 158}, {"referenceID": 3, "context": ", 1997) and LDA topic modeling (Blei et al., 2003)) and (pairwise) supervised clustering procedures (using SVMs and MaxEnt) for partitioning DRDAs according to the decision each supports.", "startOffset": 31, "endOffset": 50}, {"referenceID": 13, "context": "There exists much previous research on automatic text summarization using corpus-based, knowledgebased or statistical methods (Mani, 1999; Marcu, 2000).", "startOffset": 126, "endOffset": 151}, {"referenceID": 14, "context": "There exists much previous research on automatic text summarization using corpus-based, knowledgebased or statistical methods (Mani, 1999; Marcu, 2000).", "startOffset": 126, "endOffset": 151}, {"referenceID": 10, "context": "There exists much previous research on automatic text summarization using corpus-based, knowledgebased or statistical methods (Mani, 1999; Marcu, 2000). Dialogue summarization methods, however, generally try to account for the special characteristics of speech. Among early work in this subarea, Zechner (2002) investigates speech summarization based on maximal marginal relevance (MMR) and cross-speaker linking of information.", "startOffset": 127, "endOffset": 311}, {"referenceID": 5, "context": "Popular supervised methods for summarizing speech \u2014 including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) \u2014 are investigated in Buist et al. (2004), Xie et al.", "startOffset": 170, "endOffset": 190}, {"referenceID": 5, "context": "Popular supervised methods for summarizing speech \u2014 including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) \u2014 are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006).", "startOffset": 170, "endOffset": 209}, {"referenceID": 5, "context": "Popular supervised methods for summarizing speech \u2014 including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) \u2014 are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006). Techniques for determining semantic similarity are used for selecting relevant utterances in Gurevych and Strube (2004).", "startOffset": 170, "endOffset": 227}, {"referenceID": 5, "context": "Popular supervised methods for summarizing speech \u2014 including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) \u2014 are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006). Techniques for determining semantic similarity are used for selecting relevant utterances in Gurevych and Strube (2004).", "startOffset": 170, "endOffset": 348}, {"referenceID": 1, "context": "Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings.", "startOffset": 11, "endOffset": 34}, {"referenceID": 1, "context": "Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in recent years, there has been much research on detecting decisionrelated DAs. Hsueh and Moore (2008), for example, propose maximum entropy classification techniques to identify DRDAs in meetings; Fern\u00e1ndez et al.", "startOffset": 11, "endOffset": 229}, {"referenceID": 1, "context": "Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in recent years, there has been much research on detecting decisionrelated DAs. Hsueh and Moore (2008), for example, propose maximum entropy classification techniques to identify DRDAs in meetings; Fern\u00e1ndez et al. (2008b) develop a model of decision-making dialogue structure and detect decision DAs based on it; and Frampton et al.", "startOffset": 11, "endOffset": 349}, {"referenceID": 1, "context": "Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in recent years, there has been much research on detecting decisionrelated DAs. Hsueh and Moore (2008), for example, propose maximum entropy classification techniques to identify DRDAs in meetings; Fern\u00e1ndez et al. (2008b) develop a model of decision-making dialogue structure and detect decision DAs based on it; and Frampton et al. (2009) implement a real-time decision detection system.", "startOffset": 11, "endOffset": 467}, {"referenceID": 4, "context": "(2008a) and Bui et al. (2009), however, might be the most relevant previous work to ours.", "startOffset": 12, "endOffset": 30}, {"referenceID": 3, "context": "Here we use the Latent Dirichlet Allocation (LDA) topic models of Blei et al. (2003) \u2014 unsuper-", "startOffset": 66, "endOffset": 85}, {"referenceID": 12, "context": "We employ Support Vector Machines (SVMs) and Maximum Entropy (MaxEnt) as our learning methods, because SVMs are shown to be effective in text categorization (Joachims, 1998) and MaxEnt has been applied in many natural language processing tasks (Berger et al.", "startOffset": 157, "endOffset": 173}, {"referenceID": 2, "context": "We employ Support Vector Machines (SVMs) and Maximum Entropy (MaxEnt) as our learning methods, because SVMs are shown to be effective in text categorization (Joachims, 1998) and MaxEnt has been applied in many natural language processing tasks (Berger et al., 1996).", "startOffset": 244, "endOffset": 265}, {"referenceID": 6, "context": "The second uses the text segmentation software of Choi (2000) to partition the decision-related DAs (ordered according to time) into several topic-based groups (CHOISEGMENT).", "startOffset": 50, "endOffset": 62}, {"referenceID": 0, "context": "Our evaluation metrics include b (also called Bcubed) (Bagga and Baldwin, 1998), which is a com-", "startOffset": 54, "endOffset": 79}, {"referenceID": 8, "context": "(2005), Galley (2006), Gurevych and Strube (2004)) has shown that DRDA-level extractive summarization can be effective when viewed as a binary classification task.", "startOffset": 8, "endOffset": 22}, {"referenceID": 8, "context": "(2005), Galley (2006), Gurevych and Strube (2004)) has shown that DRDA-level extractive summarization can be effective when viewed as a binary classification task.", "startOffset": 8, "endOffset": 50}, {"referenceID": 7, "context": "We follow the method of Fern\u00e1ndez et al. (2008a), but use a larger set of features and different learning methods.", "startOffset": 24, "endOffset": 49}, {"referenceID": 15, "context": "An Adjacency Pair (AP) is an important conversational analysis concept; APs are considered the fundamental unit of conversational organization (Schegloff and Sacks, 1973).", "startOffset": 143, "endOffset": 170}, {"referenceID": 4, "context": "As in Bui et al. (2009) and Fern\u00e1ndez", "startOffset": 6, "endOffset": 24}, {"referenceID": 7, "context": "Finally, we considered comparing our systems to the earlier similar work of (Fern\u00e1ndez et al., 2008a) and (Bui et al.", "startOffset": 76, "endOffset": 101}, {"referenceID": 4, "context": ", 2008a) and (Bui et al., 2009), but found that it would be quite difficult because they employ a different notion from DRDAs which is Decision Dialogue Acts(DDAs).", "startOffset": 13, "endOffset": 31}], "year": 2016, "abstractText": "This paper addresses the problem of summarizing decisions in spoken meetings: our goal is to produce a concise decision abstract for each meeting decision. We explore and compare token-level and dialogue act-level automatic summarization methods using both unsupervised and supervised learning frameworks. In the supervised summarization setting, and given true clusterings of decisionrelated utterances, we find that token-level summaries that employ discourse context can approach an upper bound for decision abstracts derived directly from dialogue acts. In the unsupervised summarization setting,we find that summaries based on unsupervised partitioning of decision-related utterances perform comparably to those based on partitions generated using supervised techniques (0.22 ROUGE-F1 using LDA-based topic models vs. 0.23 using SVMs).", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}