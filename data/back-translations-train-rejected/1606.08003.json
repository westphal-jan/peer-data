{"id": "1606.08003", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2016", "title": "Functional Distributional Semantics", "abstract": "Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning. In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms. We describe an implementation of this framework using a combination of Restricted Boltzmann Machines and feedforward neural networks. Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets.", "histories": [["v1", "Sun, 26 Jun 2016 07:44:08 GMT  (96kb,D)", "http://arxiv.org/abs/1606.08003v1", "Published at Representation Learning for NLP workshop at ACL 2016,this https URL"]], "COMMENTS": "Published at Representation Learning for NLP workshop at ACL 2016,this https URL", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["guy emerson", "ann copestake"], "accepted": false, "id": "1606.08003"}, "pdf": {"name": "1606.08003.pdf", "metadata": {"source": "CRF", "title": "Functional Distributional Semantics", "authors": ["Guy Emerson"], "emails": ["gete2@cam.ac.uk", "aac10@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Current approaches to distributional semantics usually involve the representation of words as points in a high-dimensional vector space. However, vectors do not offer \"natural\" compositional processes that have clear analogies to operations in formal semantics, which makes it difficult to draw conclusions or grasp various aspects of meaning that have been studied by semanticists, regardless of whether the vectors are constructed using a counter approach (e.g. Turney and Pantel, 2010) or an embedding approach (e.g. Mikolov et al., 2013), and indeed Levy and Goldberg (2014b) showed that there are close links between them. Even the tensorial approach described by Coecke et al. (2010) and Baroni et al. (2014), which naturally grasps the argument structure, does not allow for obvious consideration of context dependence or logical inference. In this paper, we build on insights from formal semantics to attempt to integrate and the natural structure in this attempt."}, {"heading": "2 Formal Framework of Functional Distributional Semantics", "text": "In this section, we describe our framework, explain the connections to formal semantics, and define our probabilistic model. First, we motivate the representation of predicates with functions, and then explain how these functions can be integrated into a representation for a complete enunciation."}, {"heading": "2.1 Semantic Functions", "text": "We start by starting from an extended model structure, as the standard in formal semantics (Kamp and Reyle, 1993; Cann, 1993; Allan, 2001). In the simplest case, a model contains a series of entities that can distinguish predicate from representation of entities. Models can, of course, be endowed with additional structure, such as for majorities (Link, 2002), although we will not discuss such details here. The important point is that we should separate the representation of a predicate from the representations of entities that exist. We generalize this formalization of predicates by treating truth values as random variables, the step of replacing absolute truth values with probabilities being parallels in much computational logic work. For example, Garrette et al. (2011) we integrate distribution information into a Markov Logic Network (Richardson and Domingos, 2006)."}, {"heading": "2.2 Incorporation with Dependency Minimal Recursion Semantics", "text": "We use the dependence Minimal Recursion Semantics (DMRS) (Copestake et al., 2005; Copestake, 2009), which represents a meaning as a directed acyclic graph: nodes represent predicates / entities (based on a one-to-one correspondence between them) and nodes (edges) represent argumentation structure and scopal constraints. Note: We assume that we have a neo-Davidonian approach (Davidson, 1967), where events are also treated as entities that allow a better representation of adverbs, among other phenomena. For example (simplifying a little) to represent the dogs \"we have three nodes for predicting dogs, dogs and barks, and two nodes: an ARG1 connection from bark to dog, and an RSTR connection from dog to dog."}, {"heading": "3 Implementation", "text": "In the previous section we described a general framework for probabilistic semantics. Here we give details on how such a framework for distribution semantics can be implemented to keep the architecture as simple as possible."}, {"heading": "3.1 Network Architecture", "text": "We take the semantic spaceX as a set of binaryrated vectors, 4 {0, 1} n. A situation s then consists of entity vectors x (1), \u00b7 \u00b7 \u00b7 \u00b7 (K) \u00b7 X (where the number of entities K may vary), along with connections between entities. We designate a connection from x (n) to x (m) with labeling l as: x (n) l \u2212 \u2212 x (m). We define the background distribution over situations using a Restricted Boltzmann Machine4We use the term vector in computer science meaning a linear array, rather than in the mathematical sense of a point in a vector space. (RBM) (1986; Hinton et al., 2006), but instead of having connections between hidden and visible entities, we have connections between components of entities, according to the links.The probability that the network in the respective configuration depends on the energy shown b."}, {"heading": "3.2 Learning Algorithm", "text": "To train this model, we aim to maximize the probability of observing the training data - in Bayesian terminology, this is a constant estimate. As described in Section 2.2, each data point is a lexicalized DMRS graph, while our model defines distributions over lexical distributions of graphs. In other words, we take as an example the observed distribution over abstract graph structures (where there are linkages but nodes are not labeled) and try to optimize how the model generates predicates predicates (using the parameters W (l) ij, bi, W (c) i (c) i (c))). For the family of optimization algorithms based on gradient descent, we need to know the degree of probability relative to the model parameters given in (8), where x (X) is a latent entity, and c (V) is an observed predicate."}, {"heading": "4 Training and Initial Experiments", "text": "In this section we report on the first experiments carried out in our framework."}, {"heading": "4.1 Training Data", "text": "In particular, we used WikiWoods, an automatically analyzed version of the full English Wikipedia from July 2008, distributed by DELPH-IN5. This resource was produced by Flickinger et al. (2010) using the English resource Grammar (ERG; Flickinger et al., 2000) and implemented with the PET parser (Callmeier, 2001; Toutanova et al., 2005). To prepare the corpus, we used the python packages pydelphin6 (developed by Michael Goodman et al., 2009) and pydmrs7 (Copestake et al., 2016). For simplicity, we limited our attention to subject-verb object (SVO) three-dimensional, although we should stress that this is not an inherent limitation of our model, which is based on GERs et al al al al al al, 2016)."}, {"heading": "4.2 Evaluation", "text": "The goal of this evaluation was simply to verify whether the model has learned anything reasonable at all, and not whether it is a model that illustrates the strengths of our model, because we need more tasks to exploit its full expressiveness. (2001) The datasets we selected aim to assess the similarity, which is divided by Agirre et al. (2009) into similarity and relativization. (2015) \"We have untuned hyperparameters. The results will also be in Table 2. We have described Mikolov et al. (2013)\" s Word2Vec model on the SVO database to give a direct comparison between the models. \"The results are also in Table 2. We have Mikolov et al. (2013)\" s Word2Vec model on the SVO database."}, {"heading": "5 Related Work", "text": "As mentioned above, Coecke et al. (2010) and Baroni et al. (2014) have introduced a tensor-based framework that incorporates argumentation structures through tensor contraction, but for logical conclusions we need to know how a vector can result in a different situation. However, Grefenstette (2013) investigates a method to do this; they do not show that this approach can be learned from distributional information, and furthermore they prove that quantifiers cannot be expressed with tensors. Balk\u0131r (2014), working within the tensor framework, uses the quantum mechanical notion of a \"mixed state\" to double the uncertainty of the model. However, this doubles the number of tensor indices, i.e. quadratically the number of dimensions (e.g. vectors become matrices). In the original framework, we already have expressions with multiple arguments of high dimensionality (e.g. the representation of which becomes a fifth order by this problem, tensor and matrices)."}, {"heading": "6 Conclusion", "text": "We have demonstrated how this approach can capture semantic phenomena that are challenging for traditional vector space models. We have explained how our framework can be implemented and trained using a corpus of DMRS diagrams. Finally, our initial evaluation of similarity data sets demonstrates the feasibility of this approach and shows that thematically related words do not receive similar representations. In future work, we plan to use more extensive tasks that exploit the meaningfulness of the model."}, {"heading": "Acknowledgments", "text": "This work was funded by a Schiff Foundation Studentship. We would also like to thank Yarin Gal who gave useful feedback on the specification of our generative model."}, {"heading": "Appendix: Derivation of Gradients", "text": "In this section we derive the equation (8). \u2212 juli \u2212 juli \u2212 juli \u2212 juli, p \u2212 juli, p \u2212 juli, p \u2212 juli, p \u2212 juli, p \u2212 juli, p \u2212 juli, p \u2212 juli, p \u2212 juli, b (juli, c) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (juli) juli (((juli)) juli (juli) juli (juli) juli ((juli)) juli (juli) juli (juli) juli ((juli) juli) juli ((juli) juli ((juli) juli) juli ((juli) juli) juli ((juli) juli) juli (((juli) juli)) juli (juli) juli ((juli)) juli) juli ((juli) juli) juli (juli) juli) juli (((juli) juli) juli) juli ((("}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Vector space models have become popular in distributional semantics, despite the challenges they face in capturing various semantic phenomena. We propose a novel probabilistic framework which draws on both formal semantics and recent advances in machine learning. In particular, we separate predicates from the entities they refer to, allowing us to perform Bayesian inference based on logical forms. We describe an implementation of this framework using a combination of Restricted Boltzmann Machines and feedforward neural networks. Finally, we demonstrate the feasibility of this approach by training it on a parsed corpus and evaluating it on established similarity datasets.", "creator": "TeX"}}}