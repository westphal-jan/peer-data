{"id": "1606.08051", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2016", "title": "Training LDCRF model on unsegmented sequences using Connectionist Temporal Classification", "abstract": "Many machine learning problems such as speech recognition, gesture recognition, and handwriting recognition are concerned with simultaneous segmentation and labeling of sequence data. Latent-dynamic conditional random field (LDCRF) is a well-known discriminative method that has been successfully used for this task. However, LDCRF can only be trained with pre-segmented data sequences in which the label of each frame is available. In the realm of neural networks, the invention of Connectionist Temporal Classification (CTC) made it possible to train Recurrent Neural Networks on unsegmented sequences with great success. In this paper, we use CTC to train LDCRF model on unsegmented sequences. Experimental results on gesture recognition tasks show that our proposed method can outperform LDCRF, Hidden Markov Models, and Conditional Random Fields.", "histories": [["v1", "Sun, 26 Jun 2016 16:26:19 GMT  (491kb)", "http://arxiv.org/abs/1606.08051v1", null], ["v2", "Tue, 28 Jun 2016 14:23:06 GMT  (491kb)", "http://arxiv.org/abs/1606.08051v2", null], ["v3", "Tue, 6 Sep 2016 11:57:06 GMT  (509kb)", "http://arxiv.org/abs/1606.08051v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["amir ahooye atashin", "kamaledin ghiasi-shirazi", "ahad harati"], "accepted": false, "id": "1606.08051"}, "pdf": {"name": "1606.08051.pdf", "metadata": {"source": "CRF", "title": "Training LDCRF model on unsegmented sequences using Connectionist Temporal Classification", "authors": [], "emails": ["1amir.atashin@stu.um.ac.ir", "2k.ghiasi@um.ac.ir", "3a.harati@um.ac.ir"], "sections": [{"heading": null, "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "A. Temporal Classification", "text": "The temporal classification is the abolition of the sequence classification, where each input sequence is associated with a sequence of labels and the alignment between them is unknown. Let = {(,),..., (,)} be a set of training samples which each training samples = (,) in S consist of a pair an input data sequence (=,,,...,) with length and corresponding label sequence =,,..., with which length of the class label sequence is most long as data sequence (i.e. \u2264). each is d-dimensional feature vector and is a member of set which is set of all possible class labels.The goal in the temporal classification tasks is training a map between input data sequences and their corresponding label sequences, which should reduce error on a set of test sequences."}, {"heading": "B. Latent-dynamic conditional random field (LDCRF)", "text": "LDCRF is an undirected graphical model designed for the specific case of sequence classification problems where the goal is to predict a label for each frame of a given data sequence (Figure 1. (b)) Let = {(,)... (,)} be a set of n training samples, each sample = (,) in is a pair of input data sequences =,,..., and the corresponding class identifier sequence =,..., both of which have the same length. There is also a sequence of hidden variables (=,,...,) that are ignored. Each data point is a d-dimensional input vector (i.e."}, {"heading": "C. Connectionist Temporal Classification", "text": "Connetionist Temporal Classification (CTC) [11] is suggested, as the name suggests, for use in time classification tasks. It is used as an output layer of RNNs for time classification problems.Let be a training set (as explained in Section II.A), CTC Objective is defined as the sum of negative log probabilities for each sample = (,) in: = \u2212 log (|) \u0442 (4) For each frame of the input data sequence by the size of, CTC receives a probability table of all class names. Therefore, it receives a probability table by the size of \u00d7 | as input and then efficiently calculates Log (|) by means of dynamic programming. In addition, it creates an error table for each unit of its input data table. III. PROPOSED METHOD In the first section, we talked about the limitation of the LDCRF model, which can only be applied to sequence classification framework with the next section of the label."}, {"heading": "A. Our Framework overview", "text": "We propose a discriminatory framework for unsegmented sequence learning tasks. As shown in Figure 2, it consists of two levels: the model layer that LDCRF has placed in it. CTC objective level. In the first step, LDCRF takes a data sequence as input and calculates the label probability for each frame. CTC then calculates both the LDCRF output and the desired label sequence to calculate objective function in (5), as described in section (II.C). CTC also calculates objective function gradients relative to the label probability for each node of LDCRF. Like the familiar method of backpropagation, we use CTC gradients to obtain update rules for LDCRF parameters. Finally, the sequencing technique is used to optimize the CTC target."}, {"heading": "B. CTC-LDCRF Learning algorithm", "text": "Our training set consists of n pairs (,) - = 1... of a data sequence and the corresponding label sequence with unknown orientation between them (as explained in Section II.A.) We use the following target to learn the LDCRF parameter: = \u2212 (|) + () (5) The first term in (5) is the CTC target (4) and calculates the \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 in the second level of our frame. The second term is a regulation function for the model parameters. We use the gradient decently to find optimal parameter values of the LDCRF model. With the following chain rule, we can get the \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 gradient of the CTC object with respect to the LDCRF parameters: = (|) \u00d7 (|) For a specific training sample (,) we assume that the parameter, the parameter of the node in the LDCRF that is associated with the function (), so we have served the gradient in relation to the first part (=), in relation to the gradient."}, {"heading": "C. LDCRF-CTC inference", "text": "For testing, we want to estimate the most likely labeling sequence that maximizes the conditional model: \u043a = (|, \u0445) (8), where \u043c are parameter values learned during the training phase.IV. EXPERIENCE In this section, we describe the datasets and methodology used in our experiments to evaluate our model performance.Avatar Eye Data Set: This dataset consisted of an eye stare gesture roughly calculated from 6 human participants who interacted with an avatar [17]. The goal is to recognize eye stare gestures compared to all other types of eye gestures from unsegmented video sequences.The input functions for each video sequence are two-dimensional eye stare gestures obtained using the view-based appearance model. Each image of a video sequence in the dataset was marked either as an \"eye stare version\" or \"other eye sequences.\""}, {"heading": "A. Models", "text": "We compare three configurations of our proposed method with LDCRF, CRF and HMM on ArmGesture continuities and Avatar-Eye datasets. Conditional Random Field: we trained a single chain CRF with its standard lens function and regulated the term, we varied the window size from 0 to 2.Latent Dynamic Conditional Random Field: we trained a single chain LDCRF model with the lens function described in [5]. We differentiated the number of hidden states per label from 2 to 6 and the window size from 0 to 2.Hidden Markov Model: we used one HMM for each class. Each HMM was trained with segmented subsequences, with the frames of each subsequence all assigned to the same class. This training set contained the same number of frames as the one used to train the CRF and LDCRF model."}, {"heading": "B. Methodology", "text": "We use K = 5 for the ArmGesture continuities and K = 2 for the Avatar Eye datasets. We compare the results of the three configurations of our LDCRF CTC learning algorithm on ArmGesture continuities and Avatar Eye datasets on each frame of each test sequence. We compare the results of the three configurations of our LDCRF CTC learning algorithm on ArmGesture continuities and Avatar Eye datasets on each frame: = \u2211 # \u00d7 100 (9) V. RESULTS AND DISCUSSION. We compare the results of the three configurations of our LDCRF CTC learning algorithm on ArmGesture continuities and Avatar Eye datasets with standard LDCRF, Linear CRF and HMM models on our experiments. We use (9) the ROC curve for the experiment on Avatarye datasets."}, {"heading": "ACKNOWLEDGMENT", "text": "We would like to thank Mostafa Rafiee, MSc student at Ferdowsi University in Mashhad, for his remarkable discussions and his useful support in implementing and conducting experiments."}], "references": [{"title": "A brief survey on sequence classification", "author": ["Z. Xing", "J. Pei", "E. Keogh"], "venue": "ACM SIGKDD Explorations Newsletter, vol. 12, pp. 40-48, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": "MIT press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F.C. Pereira"], "venue": "2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "A tutorial on hidden Markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, pp. 257-286, 1989.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Latent-dynamic discriminative models for continuous gesture recognition", "author": ["L.-P. Morency", "A. Quattoni", "T. Darrell"], "venue": "Computer Vision and Pattern Recognition, 2007. CVPR'07. IEEE Conference on, 2007, pp. 1-8.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Hidden conditional random fields", "author": ["A. Quattoni", "S. Wang", "L.-P. Morency", "M. Collins", "T. Darrell"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, pp. 1848-1852, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1848}, {"title": "Latent pose estimator for continuous action recognition", "author": ["H. Ning", "W. Xu", "Y. Gong", "T. Huang"], "venue": "Computer Vision\u2013ECCV 2008, ed: Springer, 2008, pp. 419-433.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Conditional sequence model for context-based recognition of gaze aversion", "author": ["L.-P. Morency", "T. Darrell"], "venue": "Machine Learning for Multimodal Interaction, ed: Springer, 2007, pp. 11-23.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-view latent variable discriminative models for action recognition", "author": ["Y. Song", "L.-P. Morency", "R. Davis"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, 2012, pp. 2120-2127.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Sign language spotting with a threshold model based on conditional random fields", "author": ["H.-D. Yang", "S. Sclaroff", "S.-W. Lee"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, pp. 1264-1277, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369-376.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural Networks", "author": ["A. Graves"], "venue": "Supervised Sequence Labelling with Recurrent Neural Networks, ed: Springer, 2012, pp. 15-35.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "A novel connectionist system for unconstrained handwriting recognition", "author": ["A. Graves", "M. Liwicki", "S. Fern\u00e1ndez", "R. Bertolami", "H. Bunke", "J. Schmidhuber"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 31, pp. 855-868, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Offline handwriting recognition with multidimensional recurrent neural networks", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2009, pp. 545-552.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Towards End-To-End Speech Recognition with Recurrent Neural Networks", "author": ["A. Graves", "N. Jaitly"], "venue": "ICML, 2014, pp. 1764-1772.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognizing gaze aversion gestures in embodied conversational discourse", "author": ["L.-P. Morency", "C.M. Christoudias", "T. Darrell"], "venue": " Proceedings of the 8th international conference on Multimodal interfaces, 2006, pp. 287-294.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep Dynamic Neural Networks for Multimodal Gesture Segmentation and Recognition", "author": ["D. Wu", "L. Pigou", "P.-J. Kindermans", "L. Nam", "L. Shao", "J. Dambre"], "venue": "2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional neural random fields for action recognition", "author": ["C. Liu", "J. Liu", "Z. He", "Y. Zhai", "Q. Hu", "Y. Huang"], "venue": "Pattern Recognition, 2016.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION Labeling and segmenting of data sequences is a common problem in sequence classification tasks [1].", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "Probabilistic Graphical Models (PGMs) are the type of probabilistic models which are commonly used in machine learning problems [2].", "startOffset": 128, "endOffset": 131}, {"referenceID": 2, "context": "(a)) [3] is a variant of MRFs, and Hidden Markov Models (HMMs) [4] is a variant of BNs, are two well-known models which are widely used in sequence classification problems.", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "(a)) [3] is a variant of MRFs, and Hidden Markov Models (HMMs) [4] is a variant of BNs, are two well-known models which are widely used in sequence classification problems.", "startOffset": 63, "endOffset": 66}, {"referenceID": 4, "context": "Latent-dynamic conditional random field (LDCRF) [5] is a type of CRF which used for structured prediction problems.", "startOffset": 48, "endOffset": 51}, {"referenceID": 5, "context": "Similar to the Hidden Conditional Random Fields (HCRFs) [6], LDCRF uses hidden variables to learn both extrinsic and intrinsic relations for various structures of labels.", "startOffset": 56, "endOffset": 59}, {"referenceID": 6, "context": "It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].", "startOffset": 138, "endOffset": 144}, {"referenceID": 7, "context": "It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].", "startOffset": 138, "endOffset": 144}, {"referenceID": 8, "context": "It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].", "startOffset": 138, "endOffset": 144}, {"referenceID": 9, "context": "It is a proper model for human-computer interaction (HCI) problems, mostly applied in sign language spotting and human action recognition [7-10].", "startOffset": 138, "endOffset": 144}, {"referenceID": 10, "context": "introduced a method called \u201cConnectionist Temporal Classification\u201d (CTC) [11, 12] which can be joint with RNNs and provides a framework that is able to train, directly on unsegmented sequences.", "startOffset": 73, "endOffset": 81}, {"referenceID": 11, "context": "introduced a method called \u201cConnectionist Temporal Classification\u201d (CTC) [11, 12] which can be joint with RNNs and provides a framework that is able to train, directly on unsegmented sequences.", "startOffset": 73, "endOffset": 81}, {"referenceID": 12, "context": "Experimented results showed that their proposed method got a significant improvement in classification performance over earlier methods for dealing with those problems [13-16].", "startOffset": 168, "endOffset": 175}, {"referenceID": 13, "context": "Experimented results showed that their proposed method got a significant improvement in classification performance over earlier methods for dealing with those problems [13-16].", "startOffset": 168, "endOffset": 175}, {"referenceID": 14, "context": "Experimented results showed that their proposed method got a significant improvement in classification performance over earlier methods for dealing with those problems [13-16].", "startOffset": 168, "endOffset": 175}, {"referenceID": 10, "context": "Connectionist Temporal Classification Connectionist Temporal Classification (CTC) [11], as its name would suggest, is proposed for use in temporal classification tasks.", "startOffset": 82, "endOffset": 86}, {"referenceID": 15, "context": "Avatar-Eye Dataset: This dataset consisted of eye gaze gesture roughly calculated from 6 human contributors interacting with an avatar [17].", "startOffset": 135, "endOffset": 139}, {"referenceID": 4, "context": "Latent-dynamic Conditional Random Field: we trained a single chain LDCRF model with the objective function described in [5].", "startOffset": 120, "endOffset": 123}, {"referenceID": 16, "context": "For the future works we plan to extend our framework and use a neural network such as LSTM and Convolutional Neural Network (CNN) as a feature extractor and make it possible to apply our framework on more complex and raw real-world data sets [18, 19].", "startOffset": 242, "endOffset": 250}, {"referenceID": 17, "context": "For the future works we plan to extend our framework and use a neural network such as LSTM and Convolutional Neural Network (CNN) as a feature extractor and make it possible to apply our framework on more complex and raw real-world data sets [18, 19].", "startOffset": 242, "endOffset": 250}], "year": 2016, "abstractText": "Many machine learning problems such as speech recognition, gesture recognition, and handwriting recognition are concerned with simultaneous segmentation and labeling of sequence data. Latent-dynamic conditional random field (LDCRF) is a well-known discriminative method that has been successfully used for this task. However, LDCRF can only be trained with pre-segmented data sequences in which the label of each frame is available. In the realm of neural networks, the invention of Connectionist Temporal Classification (CTC) made it possible to train Recurrent Neural Networks on unsegmented sequences with great success. In this paper, we use CTC to train LDCRF model on unsegmented sequences. Experimental results on gesture recognition tasks show that our proposed method can outperform LDCRF, Hidden Markov Models, and Conditional Random Fields.", "creator": null}}}