{"id": "1606.08061", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Jun-2016", "title": "Exact gradient updates in time independent of output size for the spherical loss family", "abstract": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200,000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the $D \\times d$ output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in $O(d^{2})$ per example instead of $O(Dd)$, remarkably without ever computing the D-dimensional output. The proposed algorithm yields a speedup of up to $D/4d$ i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.", "histories": [["v1", "Sun, 26 Jun 2016 17:57:36 GMT  (1642kb,D)", "http://arxiv.org/abs/1606.08061v1", "Expanded journal version of our NIPS-2015 conference paperarXiv:1412.7091with full algorithm generalized to the spherical family"]], "COMMENTS": "Expanded journal version of our NIPS-2015 conference paperarXiv:1412.7091with full algorithm generalized to the spherical family", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["pascal vincent", "alexandre de br\\'ebisson", "xavier bouthillier"], "accepted": false, "id": "1606.08061"}, "pdf": {"name": "1606.08061.pdf", "metadata": {"source": "CRF", "title": "Exact gradient updates in time independent of output size for the spherical loss family", "authors": ["Pascal Vincent", "Alexandre de Br\u00e9bisson", "Xavier Bouthillier"], "emails": [], "sections": [{"heading": null, "text": "This year, the time has come to put yourself in a position to be at the forefront in order to find the way to the future."}], "references": [{"title": "A neural probabilistic language model", "author": ["Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent"], "venue": "In NIPS\u201900,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Large-scale learning of embeddings with reconstruction sampling", "author": ["Y. Dauphin", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th International Conference on Machine learning,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": "In ACL-IJCNLP\u20192015,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyvarinen"], "venue": "In Proceedings of The Thirteenth International Conference on Artificial Intelligence and Statistics (AISTATS\u201910),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Learning word embeddings efficiently with noisecontrastive estimation", "author": ["Andriy Mnih", "Koray Kavukcuoglu"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS\u20192013,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS)", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Deep networks with large output spaces", "author": ["Sudheendra Vijayanarasimhan", "Jonathon Shlens", "Rajat Monga", "Jay Yagnik"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Hierarchical probabilistic neural network language model", "author": ["Frederic Morin", "Yoshua Bengio"], "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2005}, {"title": "Learning representations by backpropagating", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "errors. Nature,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1986}, {"title": "Une proc\u00e9dure d\u2019apprentissage pour R\u00e9seau \u00e0 seuil assym\u00e9trique", "author": ["Yann LeCun"], "venue": "In Cognitiva 85: A la Frontie\u0300re de l\u2019Intelligence Artificielle, des Sciences de la Connaissance et des Neurosciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1985}, {"title": "Learning processes in an asymmetric threshold network", "author": ["Yann LeCun"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1985}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Blocks and Fuel: Frameworks for deep learning", "author": ["B. van Merri\u00ebnboer", "D. Bahdanau", "V. Dumoulin", "D. Serdyuk", "D. Warde-Farley", "J. Chorowski", "Y. Bengio"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "[1] first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[1] first proposed using a neural network for learning a language model, in which case the computed output vector represents the probability of the next word and is the size of the considered vocabulary, which is becoming increasingly large in modern applications [2].", "startOffset": 264, "endOffset": 267}, {"referenceID": 2, "context": "[3], the efficient use of biased importance sampling in Jean et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "[4], the use of Noise Contrastive Estimation [5] in Mnih and Kavukcuoglu [6] and Mikolov et al.", "startOffset": 73, "endOffset": 76}, {"referenceID": 6, "context": "[7] all fall under this category.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset.", "startOffset": 118, "endOffset": 124}, {"referenceID": 8, "context": "As does the more recent use of approximate Maximum Inner Product Search based on Locality Sensitive Hashing techniques[8, 9] to select a good candidate subset.", "startOffset": 118, "endOffset": 124}, {"referenceID": 9, "context": "\u2022 Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 30}, {"referenceID": 6, "context": "\u2022 Hierarchical softmax [10, 7] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 30}, {"referenceID": 10, "context": "[11], LeCun [12, 13] to efficiently compute the gradients.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[11], LeCun [12, 13] to efficiently compute the gradients.", "startOffset": 12, "endOffset": 20}, {"referenceID": 12, "context": "[11], LeCun [12, 13] to efficiently compute the gradients.", "startOffset": 12, "endOffset": 20}, {"referenceID": 0, "context": "in Neural Language Models [1] with large vocabulary size (e.", "startOffset": 26, "endOffset": 29}, {"referenceID": 1, "context": "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.", "startOffset": 72, "endOffset": 75}, {"referenceID": 3, "context": "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.", "startOffset": 79, "endOffset": 85}, {"referenceID": 4, "context": "Reconstruction sampling [2] and the use of Noise Contrastive Estimation [3] in [4, 5] fall under this category.", "startOffset": 79, "endOffset": 85}, {"referenceID": 5, "context": "\u2023 Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 29}, {"referenceID": 3, "context": "\u2023 Hierarchical softmax [6, 4] imposes a heuristically defined hierarchical tree structure for the computation of the normalized probability of the target class.", "startOffset": 23, "endOffset": 29}, {"referenceID": 0, "context": "[1] Bengio, Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] Dauphin, Y.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] Mnih, A.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[6] Morin, F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] Gutmann, M.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] Mikolov, T.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "We would like to thank the developers of Theano [14, 15] and Blocks [16].", "startOffset": 48, "endOffset": 56}, {"referenceID": 14, "context": "We would like to thank the developers of Theano [14, 15] and Blocks [16].", "startOffset": 48, "endOffset": 56}, {"referenceID": 15, "context": "We would like to thank the developers of Theano [14, 15] and Blocks [16].", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of sizeD (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating theD\u00d7 d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efficient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d) per example instead of O(Dd), remarkably without ever computing theD-dimensional output. The proposed algorithm yields a speedup of D 4d , i.e. two orders of magnitude for typical sizes, for that critical part of the computations that often dominates the training time in this kind of network architecture.", "creator": "LaTeX with hyperref package"}}}