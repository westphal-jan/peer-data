{"id": "1606.08117", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Improved Recurrent Neural Networks for Session-based Recommendations", "abstract": "Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNNbased models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.", "histories": [["v1", "Mon, 27 Jun 2016 03:06:44 GMT  (183kb,D)", "http://arxiv.org/abs/1606.08117v1", null], ["v2", "Fri, 16 Sep 2016 09:41:10 GMT  (184kb,D)", "http://arxiv.org/abs/1606.08117v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yong kiam tan", "xinxing xu", "yong liu"], "accepted": false, "id": "1606.08117"}, "pdf": {"name": "1606.08117.pdf", "metadata": {"source": "CRF", "title": "Improved Recurrent Neural Networks for Session-based Recommendations", "authors": ["Yong Kiam Tan", "Xinxing Xu", "Yong Liu"], "emails": ["tanyongkiam@gmail.com", "xuxinx@ihpc.a-star.edu.sg", "liuyong@ihpc.a-star.edu.sg", "Recall@20", "Rank@20"], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Computer Methods \u2192 Guided Learning; Neural Networks; \u2022 Information Systems \u2192 Recommendation Systems; Keywords Recursive Neural Networks; Recommendation Systems; Session-Based Recommendations"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to stir, to"}, {"heading": "2. RELATED WORK", "text": "Matrix factorization methods [15, 30] are based on the sparse user-item interaction matrix, in which the recommendation problem is formulated as a matrix completion task. After decomposing the matrix, each user and each item is represented by a latent factor vector, and the missing value of the user item matrix can then be filled by multiplying the appropriate user and item vectors. As this requires us to identify both the user and the item vectors, matrix factorization methods are not directly suitable for session-based recommendations where the users are unknown. One way to solve this cold-start problem is the pair preference regression [20]. Neighborhood-based methods [22, 14] utilize the similarities between item and user purchase history; they can be applied to session-based recommendations."}, {"heading": "3. PROPOSED APPROACHES", "text": "In this section, we will explain the use of RNNs for the session-based recommendation problem (3.1), followed by our proposed methods of data enlargement (3.2), our approach to managing time shifts (3.3), an explanation of the application of LUPI (3.4), and finally an alternative model based on embedding trading model accuracy for speed and storage requirements (3.5)."}, {"heading": "3.1 RNNs for session-based recommendations", "text": "The session-based recommendation problem can be formulated as a sequence-based prediction problem as follows: Let [x1, x2, xn \u2212 1, xn] be a click session, in the clicking-R (1 \u2264 i \u2264 n) the index of a clicked item from a total number of m-items is. We are looking for a model M such that for each given prefix click sequence of session x = [x1, x2..., xr \u2212 1, xr], 1 r < n, we obtain an output y = M (x), in which [y1,...,] a prefix click sequence of a session x = [x1, xr], 1 r < n, we obtain an output y = M (x), in which [y1,...,] a session-based recommendation problem can be formulated as a sequence-based prediction problem."}, {"heading": "3.2 Data augmentation", "text": "Click sessions often vary in length: some users may take a long time to find their desired object, while others can find it with just a few clicks. One goal of the recommendation system should be to provide accurate predictions regardless of the current session duration. Data enlargement techniques are often used to improve image-based models [16]. Here, we propose two methods to extend click sequences, the first being to apply the sequence pre-processing method proposed in [5]. All prefixes of the original input sessions are treated as new training sequences. In the face of an input viewing session [x1, x2,..., xn] we generate the sequences and the corresponding labels ([x1], V (x2), (x2], V (x3)), (x1, x2,..), (xn \u2212 1], V (xn)."}, {"heading": "3.3 Adapting to temporal changes", "text": "An important assumption of many machine learning models is that input is independent and equally distributed, which is not entirely true in setting item recommendations, since new products will only appear in sessions collected after the release of this product, and user behavior / preferences may change over time. Furthermore, the purpose of the recommender system is to make predictions about new sequences, i.e. those that arise from recent user behavior. Therefore, learning a recommendation model over the entire data set can lead to poorer performance, because the model ultimately focuses on some outdated properties that are irrelevant to the latest sequences. One way to deal with this is to define a time threshold and discard click sequences that are older than the threshold when creating the model. However, this reduces the amount of training data available for our models to learn from it. We propose a simple solution to get the best out of worlds by first aligning the two pre-training sessions on the whole data set."}, {"heading": "3.4 Use of privileged information", "text": "The item sequence clicked by users after an item may also contain information about that item (highlighted in Figure 3). This information cannot be used for predictions, as we cannot look at the future sequences when making recommendations. However, we can use these future sequences as privileged information [28] to provide soft labels for regulating and training our models, using the general distillation framework [17]. Formally, considering a sequence [x1, x2,.., xr] labeled xr + 1 from a session, we define the privileged sequence as x = [xn, xn, xn \u2212 1,.., xr + 2] where n is the length of the original session prior to our pre-processing. The privileged sequence is simply the reverse future sequence that occurs after the smallest item item. We can now match a teacher model to the same teacher sequence on the 1st grade M labels by matching the miniature M labels with the small M)."}, {"heading": "3.5 Output embeddings for faster predictions", "text": "One problem with the models we have described so far is the size of the output layer. The output layer 2 is typically fully connected to the previous hidden layer - which means that the number of parameters to be matched in these two layers alone is H \u043c N, where H is the number of nodes in the hidden layer and N is the number of candidates for the prediction. In addition to memory requirements, this also slows down the prediction, since the model needs to perform an additional large matrix multiplication. A similar problem has also been investigated in natural language processing, where the output vocabulary can be enormous. Typical approaches include using a hierarchical Softmax layer [19] and scanning only the most common items.The hierarchical Softmax approach does not directly apply in our case, as we need to make a top-k prediction instead of just making a top-1 prediction, we consider the next item to inspire the HOT from a lower space, instead."}, {"heading": "4. EXPERIMENTS", "text": "We evaluate our proposed enhancements to the basic RNN model using the RecSys Challenge 2015 dataset. The dataset is broken down as follows [10]: Last-day sessions are placed in the test set, and everything else is placed in the training set, resulting in 7966257 sessions in the training set, 15234 sessions in the test set, and 37483 candidates for prediction. We have 236770981 training sequences and 55898 test sequences after pre-processing the sessions. To better evaluate some of our models, such as privileged information and pre-training, we sort the training sequences by time and report our results to models trained on newer breaks (1 256, 1 64, 1 16, 1 4, 1 1) of the training sequences as well. To better evaluate some of our models, such as privileged information and pre-training, we used evaluation metrics such as Recall @ 20 and Mean Reciprocal Rank (MRR) @ 20; each of the test sequences, i.e. the original presentation containing a maximum temperature parameter, can also be used to set the unit temperature."}, {"heading": "4.1 Experimental setup", "text": "All of our models used 50-dimensional element embedding, with 25% of the original training sessions containing dropouts. Optimization was done with Adam [13], setting the mini-frame size to 512. We truncated the BPTT in 19 time steps, as 99% of the original training sessions had lengths of less than or equal to 19. The number of eras was set by not using 10% of the training data as a validation set for each model. We used a recurring (GRU) layer in all of our models, as we found that additional layers did not improve performance. The GRU was set to 100 and 1000 hidden units for each model. Models are defined and trained in Keras [3] and Theano [26] on a GeForce GTX Titan Black GPU. The specifications of each model (along with its labels) read as follows: M1 The RNN model with softmax data outputs, sequencing and recurrent results are fully associated with this output and the recurrence of the recurrent 1."}, {"heading": "4.2 Experimental results", "text": "3This can be efficiently calculated on the GPU as an additional theano expression."}, {"heading": "M (GRU Size) Prediction time (s) Parameters", "text": "The performance of each model on the evaluation metrics is summarized in Figure 4. Overall, M1 and M2 yielded strong performance gains over the reported RNN models. We also see from the results of M1 that training on the entire data set yields slightly worse results than training on newer fractions of the data set, suggesting that our recommendation models must take into account changes in user behavior over time. Our most powerful models are listed in Table 1. We also list the baseline results recorded in [10], including their best RNN-based models (i.e. TOP1 and BPR) and two traditional algorithms (i.e. S-POP and Item-KNN)."}, {"heading": "5. CONCLUSION", "text": "We have presented and empirically evaluated several proposed enhancements to a basic RNN model. We have shown that it is possible to improve the performance of recurring models for session-based recommendation systems by using appropriate techniques for data amplification and taking into account time shifts in user behavior. Guidelines for future work include investigating the trade-offs of the embedded model and using known features of the elements in our models."}, {"heading": "6. REFERENCES", "text": "[1] Baidu Research. Deep speech 2: End-to-end speech prediction in English and Mandarin Yabs]. CoRR, abs / 1512.02595, 2015. [2] K. Chatfield, K. Simonyan, A. Vedaldi, and A. Zisserman, 2015. [4] J. Chung, C: Gu'lc, K. Cho, and Y. Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs / 1412.3555, 2014. [5] A. de Bre. bisson, E. Simon, A. Auvolat, P. Vincent, and Y. Bengio. Artificial neural networks on sequence modeling. CoRR, abs / 1412.3555, 2014."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Baidu Research"], "venue": "CoRR, abs/1512.02595,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Return of the devil in the details:  Delving deep into convolutional nets", "author": ["K. Chatfield", "K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "British Machine Vision Conference", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "venue": "CoRR, abs/1412.3555", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Artificial neural networks applied to taxi destination prediction", "author": ["A. de Br\u00e9bisson", "\u00c9. Simon", "A. Auvolat", "P. Vincent", "Y. Bengio"], "venue": "CoRR, abs/1508.00021,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "A theoretically grounded application of dropout in recurrent neural networks", "author": ["Y. Gal"], "venue": "CoRR, abs/1512.05287", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Distilling the knowledge in a neural network", "author": ["H. Geoffrey", "V. Oriol", "D. Jeff"], "venue": "arXiv:1511.03643", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A. Mohamed", "G.E. Hinton"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2013, Vancouver, BC, Canada, May 26-31, 2013, pages 6645\u20136649", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CoRR, abs/1512.03385", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Session-based recommendations with recurrent neural networks", "author": ["B. Hidasi", "A. Karatzoglou", "L. Baltrunas", "D. Tikk"], "venue": "CoRR, abs/1511.06939", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation, 9(8):1735\u20131780", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 448\u2013456", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "International Conference on Learning Representations", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Factorization meets the neighborhood: a multifaceted collaborative filtering model", "author": ["Y. Koren"], "venue": "Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Las Vegas, Nevada, USA, August 24-27, 2008, pages 426\u2013434", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Matrix factorization techniques for recommender systems", "author": ["Y. Koren", "R.M. Bell", "C. Volinsky"], "venue": "IEEE Computer, 42(8):30\u201337", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, pages 1106\u20131114", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unifying distillation and privileged information", "author": ["D. Lopez-Paz", "B. Sch\u00f6lkopf", "L. Bottou", "V. Vapnik"], "venue": "International Conference on Learning Representations", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "27th Annual Conference on Neural Information Processing Systems, pages 3111\u20133119", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors, Advances in Neural Information Processing Systems 21, pages 1081\u20131088. Curran Associates, Inc.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Pairwise preference regression for cold-start recommendation", "author": ["S.-T. Park", "W. Chu"], "venue": "Proceedings of the Third ACM Conference on Recommender Systems, RecSys \u201909, pages 21\u201328, New York, NY, USA", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Restricted boltzmann machines for collaborative filtering", "author": ["R. Salakhutdinov", "A. Mnih", "G. Hinton"], "venue": "Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 791\u2013798, New York, NY, USA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Item-based collaborative filtering recommendation algorithms", "author": ["B.M. Sarwar", "G. Karypis", "J.A. Konstan", "J. Riedl"], "venue": "Proceedings of the Tenth International World Wide Web Conference, WWW 10, Hong Kong, China, May 1-5, 2001, pages 285\u2013295", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Recommender systems in e-commerce", "author": ["J.B. Schafer", "J. Konstan", "J. Riedl"], "venue": "Proceedings of the 1st ACM Conference on Electronic Commerce, EC \u201999, pages 158\u2013166, New York, NY, USA", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks", "author": ["R. Socher", "C.C. Lin", "A.Y. Ng", "C.D. Manning"], "venue": "Proceedings of the 26th International Conference on Machine Learning (ICML)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15:1929\u20131958", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning using privileged information: Similarity control and knowledge transfer", "author": ["V. Vapnik", "R. Izmailov"], "venue": "Journal of Machine Learning Research, 16:2023\u20132049", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "A new learning paradigm: Learning using privileged information", "author": ["V. Vapnik", "A. Vashist"], "venue": "Neural Networks, 22(5-6):544\u2013557", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative deep learning for recommender systems", "author": ["H. Wang", "N. Wang", "D.-Y. Yeung"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201915, pages 1235\u20131244, New York, NY, USA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum margin matrix factorization for collaborative ranking", "author": ["M. Weimer", "A. Karatzoglou", "Q.V. Le", "A.J. Smola"], "venue": "NIPS, pages 1593\u20131600", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Sequential click prediction for sponsored search with recurrent neural networks", "author": ["Y. Zhang", "H. Dai", "C. Xu", "J. Feng", "T. Wang", "J. Bian", "B. Wang", "T.-Y. Liu"], "venue": "C. E. Brodley and P. Stone, editors, AAAI, pages 1369\u20131375. AAAI Press", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 14, "context": "Collaborative filtering approaches [15, 14, 21] can define useruser similarity based on their history of purchases, or they", "startOffset": 35, "endOffset": 47}, {"referenceID": 13, "context": "Collaborative filtering approaches [15, 14, 21] can define useruser similarity based on their history of purchases, or they", "startOffset": 35, "endOffset": 47}, {"referenceID": 20, "context": "Collaborative filtering approaches [15, 14, 21] can define useruser similarity based on their history of purchases, or they", "startOffset": 35, "endOffset": 47}, {"referenceID": 22, "context": "An alternative to relying on historical data is to make session-based recommendations [23].", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "Recurrent Neural Networks (RNNs) were recently proposed in [10] for the session-based recommendation task.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Matrix factorization methods [15, 30] are based on the sparse user-item interaction matrix, where the recommendation problem is formulated as a matrix completion task.", "startOffset": 29, "endOffset": 37}, {"referenceID": 28, "context": "Matrix factorization methods [15, 30] are based on the sparse user-item interaction matrix, where the recommendation problem is formulated as a matrix completion task.", "startOffset": 29, "endOffset": 37}, {"referenceID": 19, "context": "One way to solve this cold-start problem is to use pairwise preference regression [20].", "startOffset": 82, "endOffset": 86}, {"referenceID": 21, "context": "Neighbourhood based methods [22, 14] utilize the similarities between item and user purchase history; they can be applied to session-based recommendations by comparing session similarity.", "startOffset": 28, "endOffset": 36}, {"referenceID": 13, "context": "Neighbourhood based methods [22, 14] utilize the similarities between item and user purchase history; they can be applied to session-based recommendations by comparing session similarity.", "startOffset": 28, "endOffset": 36}, {"referenceID": 15, "context": "Deep learning has recently been applied very successfully in areas such as image recognition [16, 9], speech recognition [8, 1] and natural language processing [24].", "startOffset": 93, "endOffset": 100}, {"referenceID": 8, "context": "Deep learning has recently been applied very successfully in areas such as image recognition [16, 9], speech recognition [8, 1] and natural language processing [24].", "startOffset": 93, "endOffset": 100}, {"referenceID": 7, "context": "Deep learning has recently been applied very successfully in areas such as image recognition [16, 9], speech recognition [8, 1] and natural language processing [24].", "startOffset": 121, "endOffset": 127}, {"referenceID": 0, "context": "Deep learning has recently been applied very successfully in areas such as image recognition [16, 9], speech recognition [8, 1] and natural language processing [24].", "startOffset": 121, "endOffset": 127}, {"referenceID": 23, "context": "Deep learning has recently been applied very successfully in areas such as image recognition [16, 9], speech recognition [8, 1] and natural language processing [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 27, "context": "They have also been used for collaborative filtering [29, 21].", "startOffset": 53, "endOffset": 61}, {"referenceID": 20, "context": "They have also been used for collaborative filtering [29, 21].", "startOffset": 53, "endOffset": 61}, {"referenceID": 9, "context": "In [10], RNNs were proposed for session-based recommendations.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "In [31], the authors also use RNNs for click sequence prediction; they consider historical user behaviours as well as hand engineered features for each user and item.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Popular approaches include data augmentation [16], dropout [25, 6], batch normalization [12] and residual connections [9].", "startOffset": 45, "endOffset": 49}, {"referenceID": 24, "context": "Popular approaches include data augmentation [16], dropout [25, 6], batch normalization [12] and residual connections [9].", "startOffset": 59, "endOffset": 66}, {"referenceID": 5, "context": "Popular approaches include data augmentation [16], dropout [25, 6], batch normalization [12] and residual connections [9].", "startOffset": 59, "endOffset": 66}, {"referenceID": 11, "context": "Popular approaches include data augmentation [16], dropout [25, 6], batch normalization [12] and residual connections [9].", "startOffset": 88, "endOffset": 92}, {"referenceID": 8, "context": "Popular approaches include data augmentation [16], dropout [25, 6], batch normalization [12] and residual connections [9].", "startOffset": 118, "endOffset": 121}, {"referenceID": 26, "context": "The learning using privileged information (LUPI) framework [28, 27] was proposed to utilize the additional feature representations that are only available during training but not during testing.", "startOffset": 59, "endOffset": 67}, {"referenceID": 25, "context": "The learning using privileged information (LUPI) framework [28, 27] was proposed to utilize the additional feature representations that are only available during training but not during testing.", "startOffset": 59, "endOffset": 67}, {"referenceID": 26, "context": "When there is a limited amount of training data, the use of such information has been found to be helpful [28].", "startOffset": 106, "endOffset": 110}, {"referenceID": 6, "context": "In the generalized distillation approach [7], a student model learns from soft labels provided by a teacher model.", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "Other outputs are possible: the models in [10] output ranking scores for each item, and they are trained with ranking losses.", "startOffset": 42, "endOffset": 46}, {"referenceID": 3, "context": "For the recurrent layers, we use the Gated Recurrent Unit (GRU) [4] as it was found in [10] that they outperformed the Long-term Short Memory (LSTM) [11] units.", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "For the recurrent layers, we use the Gated Recurrent Unit (GRU) [4] as it was found in [10] that they outperformed the Long-term Short Memory (LSTM) [11] units.", "startOffset": 87, "endOffset": 91}, {"referenceID": 10, "context": "For the recurrent layers, we use the Gated Recurrent Unit (GRU) [4] as it was found in [10] that they outperformed the Long-term Short Memory (LSTM) [11] units.", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "Data augmentation techniques have been widely used to enhance image-based models [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 4, "context": "The first is an application of the sequence preprocessing method proposed in [5].", "startOffset": 77, "endOffset": 80}, {"referenceID": 5, "context": "Embedding dropout is a form of regularization applied to input sequences [6].", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "In this way, it resembles the fine-tuning process used in training of imagebased networks [2], where the models are typically initialized by pre-training on ImageNet (a large image classification dataset) before the weights are fine-tuned on a smaller image dataset in the desired domain.", "startOffset": 90, "endOffset": 93}, {"referenceID": 26, "context": "We can, however, utilize these future sequences as privileged information [28] in order to provide soft labels for regularizing and training our models.", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "We use the generalized distillation framework [17] for this purpose.", "startOffset": 46, "endOffset": 50}, {"referenceID": 0, "context": "loss function of the form: (1 \u2212 \u03bb) \u2217 L(M(x), V (xn)) + \u03bb \u2217 L(M(x),M\u2217(x\u2217)), where \u03bb \u2208 [0, 1] is a tradeoff parameter between the two sets of labels.", "startOffset": 85, "endOffset": 91}, {"referenceID": 18, "context": "Typical approaches include the use of a hierarchical softmax layer [19], and sampling only the most frequent items.", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "This approach is inspired by the distributed representations of words [18], where similar words have embeddings that are closer in cosine distance.", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "The dataset is split following [10], where sessions in the last day are placed in the test set, and everything else is placed in the training set.", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "S-POP (-) [10] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 9, "context": "1775 Item-KNN (-) [10] 0.", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "2048 TOP1 (1000) [10] 0.", "startOffset": 17, "endOffset": 21}, {"referenceID": 9, "context": "2693 BPR (1000) [10] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 9, "context": "prefixes of sessions, is given equal weight [10].", "startOffset": 44, "endOffset": 48}, {"referenceID": 12, "context": "Optimization was done using Adam [13], with mini-batch size fixed at 512.", "startOffset": 33, "endOffset": 37}, {"referenceID": 2, "context": "The models are defined and trained in Keras [3] and Theano [26] on a GeForce GTX Titan Black GPU.", "startOffset": 44, "endOffset": 47}, {"referenceID": 9, "context": "B This refers to the best results reported in [10].", "startOffset": 46, "endOffset": 50}, {"referenceID": 9, "context": "We also list the baseline results reported in [10], including their best RNN based models (i.", "startOffset": 46, "endOffset": 50}, {"referenceID": 16, "context": "This is consistent with the use of privileged information in [17], and suggests that it might be useful in settings where little data is available.", "startOffset": 61, "endOffset": 65}], "year": 2017, "abstractText": "Recurrent neural networks (RNNs) were recently proposed for the session-based recommendation task. The models showed promising improvements over traditional recommendation approaches. In this work, we further study RNNbased models for session-based recommendations. We propose the application of two techniques to improve model performance, namely, data augmentation, and a method to account for shifts in the input data distribution. We also empirically study the use of generalised distillation, and a novel alternative model that directly predicts item embeddings. Experiments on the RecSys Challenge 2015 dataset demonstrate relative improvements of 12.8% and 14.8% over previously reported results on the Recall@20 and Mean Reciprocal Rank@20 metrics respectively.", "creator": "LaTeX with hyperref package"}}}