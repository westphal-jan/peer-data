{"id": "1606.08165", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Supervised learning based on temporal coding in spiking neural networks", "abstract": "Gradient descent training techniques are remarkably successful in training analog-valued artificial neural networks (ANNs). Such training techniques, however, do not transfer easily to spiking networks due to the spike generation hard non-linearity and the discrete nature of spike communication. We show that in a feedforward spiking network that uses a temporal coding scheme where information is encoded in spike times instead of spike rates, the network input-output relation is differentiable almost everywhere. Moreover, this relation is locally linear after a transformation of variables. Methods for training ANNs thus carry directly to the training of such spiking networks as we show when training on the permutation invariant MNIST task. In contrast to rate-based spiking networks that are often used to approximate the behavior of ANNs, the networks we present spike much more sparsely and their behavior can not be directly approximated by conventional ANNs. Our results highlight a new approach for controlling the behavior of spiking networks with realistic temporal dynamics, opening up the potential for using these networks to process spike patterns with complex temporal information.", "histories": [["v1", "Mon, 27 Jun 2016 08:58:29 GMT  (214kb,D)", "http://arxiv.org/abs/1606.08165v1", null], ["v2", "Wed, 16 Aug 2017 16:15:20 GMT  (321kb,D)", "http://arxiv.org/abs/1606.08165v2", "Extended the discussion and introduction. Clarified the training parameters"]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["hesham mostafa"], "accepted": false, "id": "1606.08165"}, "pdf": {"name": "1606.08165.pdf", "metadata": {"source": "CRF", "title": "Supervised learning based on temporal coding in spiking neural networks", "authors": ["Hesham Mostafa"], "emails": ["hesham@ini.uzh.ch"], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "2 Network model", "text": "In fact, it is a matter of a way in which people are able to determine for themselves how they have behaved. (...) It is the time in which people are able to behave. (...) It is the time in which they are able to retaliate. (...) It is the time in which they are able to retaliate. (...) It is the time in which they are able to retaliate. (...) It is the time in which they are able to retaliate. (...) It is the time in which they are able to change the world. (...)"}, {"heading": "3 Training", "text": "We look at the neural networks in which the neural and synaptic dynamics must take place at a certain time, as if they are not affected by any other factors. (The neurons are able to move in a plane.) (The neurons are processed exclusively in the z-domain. (The procedure is described in Algorithm 1.) All indices are 1-based, vectors are in the following layer, and the entry of a vector is one [i]. At each layer, the causal set for each neuron is determined from the causal set function. Neural output is then evaluated according to Eq. 6 The causal set function is shown in Algorithm 2. It first sorts the input time and then looks at larger sets of early input spikes until it finds a series of input spikes that cause the neurons to spike."}, {"heading": "4 Results", "text": "We trained the network to solve two classification tasks: an XOR task and the permutation invariant MNIST task [11]. We used fully networked feedback networks where the top layer had as many neurons as the number of classes (2 in the XOR task and 10 in the MNIST task).The goal is to train the network so that the neuron corresponding to the right class fires first among the top neurons. We used cross-entropy loss and interpreted the value of a neuron in the top layer as a negative of its peak time (in the z domain).By maximizing the value of the right class neuron, the training of this neuron effectively brings this neuron to fire earlier than the neurons representing the wrong classes. Thus, for an output spike times vector zL and a target class index g, the loss function is given by L (g), zieg = lieg (L \u2212 n = L \u2212 xp]."}, {"heading": "4.1 XOR task", "text": "In the XOR task, two spike sources each send a spike to the network. Each of the two input spikes can occur at a time of 0.0 (earlier spike) or 2.0 (later spike); the two input spike sources project onto a hidden layer of 4 neurons and the hidden neurons project onto two output neurons; the first output neuron must peak before the second output neuron, if exactly one input spike is an early spike; the network is shown in Figure 2a together with the 4 input patterns; to investigate whether the network can solve the XOR task in a robust manner, we repeated the training process 1,000 times each time based on random starting weights; in each of these 1000 training studies, we used as many training siterations as were required for the training to convergence. Each training siteration included the presentation of the four input patterns 100 times maximum for the number of constants required during the 48 studies."}, {"heading": "4.2 MNIST classification task", "text": "The rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the r"}, {"heading": "5 Discussion", "text": "We presented a form of spiking of neural networks that can be effectively trained with gradient pedigree techniques, avoiding many difficulties in the formation of spike networks such as the discontinuous spike generation mechanism and the discrete nature of spike numbers. Network input-output ratio is locally linear after a transformation of time variants. As input spike times, the causal input sets of neurons change, which in turn change the shape of the linear input-output relationship, analogous to the behavior of networks with rectified linear units (ReLUs), where changes in the input setup situation occur. ReLUs do not produce the linear input-output relationship (Fig)."}], "references": [{"title": "Parallel distributed processing: explorations in the microstructure of cognition. Volume 1. Foundations", "author": ["D.E. Rumelhart", "J.L. McClelland"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Error-backpropagation in temporally encoded networks of spiking", "author": ["S.M. Bohte", "J.N. Kok", "Han La-Poutre"], "venue": "neurons. Neurocomputing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Real-time classification and sensor fusion with a spiking deep belief network", "author": ["P. O\u2019Connor", "D. Neil", "S.-C. Liu", "T. Delbruck", "M. Pfeiffer"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Fast-classifying, high-accuracy spiking deep networks through weight and threshold balancing", "author": ["Peter U Diehl", "Daniel Neil", "Jonathan Binas", "Matthew Cook", "Shih-Chii Liu", "Michael Pfeiffer"], "venue": "In International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Spiking deep convolutional neural networks for energy-efficient object recognition", "author": ["Yongqiang Cao", "Yang Chen", "Deepak Khosla"], "venue": "International Journal of Computer Vision,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A re-configurable on-line learning spiking neuromorphic processor comprising 256 neurons and 128k synapses", "author": ["Ning Qiao", "Hesham Mostafa", "Federico Corradi", "Marc Osswald", "Fabio Stefanini", "Dora Sumislawska", "Giacomo Indiveri"], "venue": "Frontiers in Neuroscience,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Minitaur, an event-driven FPGA-based spiking network accelerator", "author": ["D. Neil", "S.-C. Liu"], "venue": "Very Large Scale Integration (VLSI) Systems, IEEE Transactions on,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Neurogrid: A mixed-analog-digital multichip system for large-scale neural simulations", "author": ["Ben Varkey Benjamin", "Peiran Gao", "Emmett McQuinn", "Swadesh Choudhary", "Anand R Chandrasekaran", "J Bussat", "R Alvarez-Icaza", "JV Arthur", "PA Merolla", "K Boahen"], "venue": "Proceedings of the IEEE,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "G.E. Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Le Cun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "David Warde-Farley", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5590,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Theano: a cpu and gpu math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for scientific computing conference (SciPy),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1929}, {"title": "Fast readout of object identity from macaque inferior temporal cortex", "author": ["C.P. Hung", "Gabriel Kreiman", "Tomaso Poggio", "J.J. DiCarlo"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Spike-based strategies for rapid processing", "author": ["S. Thorpe", "A. Delorme", "R. Van Rullen"], "venue": "Neural networks,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "The idea of a distributed network of simple neuron elements that adaptively adjusts its connection weights based on training examples is partially inspired by the operation of biological spiking networks [2].", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "An approach that bears some similarities to ours is the SpikeProp algorithm [3] that can be used to train spiking networks to produce output spikes at specific times.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "Many approaches for training spiking networks first train conventional feedforward ANNs and then translate the trained weights to spiking networks developed to approximate the behavior of the original ANNs [4, 5, 6].", "startOffset": 206, "endOffset": 215}, {"referenceID": 3, "context": "Many approaches for training spiking networks first train conventional feedforward ANNs and then translate the trained weights to spiking networks developed to approximate the behavior of the original ANNs [4, 5, 6].", "startOffset": 206, "endOffset": 215}, {"referenceID": 4, "context": "Many approaches for training spiking networks first train conventional feedforward ANNs and then translate the trained weights to spiking networks developed to approximate the behavior of the original ANNs [4, 5, 6].", "startOffset": 206, "endOffset": 215}, {"referenceID": 5, "context": "Compared to rate-based networks, the networks we present can be implemented more efficiently on neuromorphic architectures where power consumption decreases as spike rates are reduced [7, 8, 9].", "startOffset": 184, "endOffset": 193}, {"referenceID": 6, "context": "Compared to rate-based networks, the networks we present can be implemented more efficiently on neuromorphic architectures where power consumption decreases as spike rates are reduced [7, 8, 9].", "startOffset": 184, "endOffset": 193}, {"referenceID": 7, "context": "Compared to rate-based networks, the networks we present can be implemented more efficiently on neuromorphic architectures where power consumption decreases as spike rates are reduced [7, 8, 9].", "startOffset": 184, "endOffset": 193}, {"referenceID": 8, "context": "Indeed, many feedforward ANNs use activation functions with a discontinuous first derivative such as rectified linear units (ReLUs) [10] while still being effectively trainable.", "startOffset": 132, "endOffset": 136}, {"referenceID": 9, "context": "We trained the network to solve two classification tasks: an XOR task, and the permutation invariant MNIST task [11].", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "Training was done using Theano [12, 13].", "startOffset": 31, "endOffset": 39}, {"referenceID": 11, "context": "Training was done using Theano [12, 13].", "startOffset": 31, "endOffset": 39}, {"referenceID": 12, "context": "We experimented with dropout [14] where we randomly removed neurons from the network during training.", "startOffset": 29, "endOffset": 33}, {"referenceID": 8, "context": "This is analogous to the behavior of networks using rectified linear units (ReLUs) [10] where changes in the input change the set of ReLUs producing non-zero output, thus changing the linear transformation implemented by the network.", "startOffset": 83, "endOffset": 87}, {"referenceID": 13, "context": "Recordings from higher visual areas in the brain indicate these areas encode information about abstract features of visual stimuli as early as 125ms after stimulus onset [15].", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "Given the typical firing rate of cortical neurons and delays across synaptic stages, this indicates rapid visual processing is mostly a feedforward process where neurons get to spike at most once [16].", "startOffset": 196, "endOffset": 200}], "year": 2016, "abstractText": "Gradient descent training techniques are remarkably successful in training analogvalued artificial neural networks (ANNs). Such training techniques, however, do not transfer easily to spiking networks due to the spike generation hard nonlinearity and the discrete nature of spike communication. We show that in a feedforward spiking network that uses a temporal coding scheme where information is encoded in spike times instead of spike rates, the network input-output relation is differentiable almost everywhere. Moreover, this relation is locally linear after a transformation of variables. Methods for training ANNs thus carry directly to the training of such spiking networks as we show when training on the permutation invariant MNIST task. In contrast to rate-based spiking networks that are often used to approximate the behavior of ANNs, the networks we present spike much more sparsely and their behavior can not be directly approximated by conventional ANNs. Our results highlight a new approach for controlling the behavior of spiking networks with realistic temporal dynamics, opening up the potential for using these networks to process spike patterns with complex temporal information.", "creator": "LaTeX with hyperref package"}}}