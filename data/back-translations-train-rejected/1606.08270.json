{"id": "1606.08270", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Evaluating Informal-Domain Word Representations With UrbanDictionary", "abstract": "Existing corpora for intrinsic evaluation are not targeted towards tasks in informal domains such as Twitter or news comment forums. We want to test whether a representation of informal words fulfills the promise of eliding explicit text normalization as a preprocessing step. One possible evaluation metric for such domains is the proximity of spelling variants. We propose how such a metric might be computed and how a spelling variant dataset can be collected using UrbanDictionary.", "histories": [["v1", "Mon, 27 Jun 2016 13:39:54 GMT  (21kb)", "http://arxiv.org/abs/1606.08270v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["naomi saphra", "adam lopez"], "accepted": false, "id": "1606.08270"}, "pdf": {"name": "1606.08270.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["n.saphra@ed.ac.uk", "alopez@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.08 270v 1 [cs.C L] 27 Jun 2016"}, {"heading": "1 Introduction", "text": "In recent years, there has been an increase in interest in effective models for informal areas such as Twitter or discussion forums. Several new papers therefore target social media platforms by learning word representations that are specific to such areas (Tang et al., 2014); (Benton et al., 2016).Traditional NLP techniques often rely on text normalization methods when applied to informal areas. For example, \"u want 2 chill wit us 2nite\" can be transcribed as \"you want to chill with us tonight,\" and normalized transcription would be used as input for a word processing system. This method facilitates the application of models that are successful in formal language to informal language. However, there are several drawbacks to this method. Building a precise text normalization component for a word processing pipeline can require considerable technical effort, and collecting manual training data can be a challenge even for text normalization models."}, {"heading": "2 Evaluating By Spelling Variants", "text": "A natural question, therefore, is whether a representation of words in social media texts would place the spelling variants of the same word close together. For example, while the representation of \"ur\" in a formal domain such as Wikipedia may appear close to \"babylon\" and \"mesopotamia,\" on Twitter we should note the relative placement of each word pair in our dictionary closer to \"yours.\" We can judge these representations by the proximity of spelling variants. Given a corpus of common spelling variants (an informal variant and a formal one), we will reject the relative placement of each word pair in our dictionary. So, for example, we can consider (ur, yours) as such a pair. To evaluate this pair, we classify the words in our vocabulary by formal variants that are ulary."}, {"heading": "3 Gathering Spelling Variants", "text": "If we have an informal text corpus, we can use it to generate a set of probable spelling variants that we validate by hand. An existing unattended method of doing so is outlined as part of the text normalization pipeline provided by (Gouws et al., 2011).This technique requires a formal vocabulary corpus such as Wikipedia as well as a social media corpus such as Twitter. They begin by exhaustively ranking all word pairs by their distributional similarity in Wikipedia and Twitter. However, the word pairs that are distributed are not considered as candidates for spelling variants in Wikipedia. These candidates are then uncovered by lexical similarity by providing a list of probable spelling variants. This method is inappropriate when we collect datasets for the purpose of evaluation."}, {"heading": "4 Experiments", "text": "By limiting ourselves to ASCII word entries, we identified 5289 incorrect definitions in UrbanDictionary containing the string \"spelling.\" Many entries explicitly describe a word as a spelling variant of another \"correctly\" spelled word, as in the following definition of \"neice\": 1 parentheses point to a link to another page of definitions, in this case for \"yours.\" 2https: / / github.com / nsaphra / urbandic-scraperspelling [...] (? P < variant >\\ w +) (\\ 1) Figure 1: Regular expression to identify spelling variants. Neice is a common misspelling of the word niece, meaning the daughter of a brother or sister. Correct spelling is nothing.We variant is niece.Even this relative wide net measures many definitions that identify a variant, including this variant \"definitively.\""}, {"heading": "4.1 Filtering by a Formal Vocabulary List", "text": "Some symbols UrbanDictionary deems appropriate may not appear in the formal corpus. For example, UrbanDictionary considers the top definition of \"braj\" to be: pronounced as it is spelled. Means bro or dude. Developed over many times by misspellings [brah] over texts and online chats. Both \"braj\" and \"brah\" are spellings of \"bro,\" itself an abbreviation of \"brother.\" If we extract (braj, brah) as a potential spelling pair based on this definition, we cannot rate if brah does not appear in the formal corpus. The representations of these words should probably reflect their similarity, but using the method described in Section 2, we cannot rate spelling pairs of two informal words."}, {"heading": "4.2 Results on GloVe", "text": "As a test, we evaluated embeddings trained with GloVe (Pennington et al., 2014) on a 121GB English Twitter corpus. We used a formal vocabulary list based on English Wikipedia. We found that 146 (24%) of the informal word representations from the pairs of words in our data set contained the formal target word in the 20 most similar formal words from the vocabulary. Only 70 (11%) of the informal word representations had the formal target word as the most similar formal word. Word pairs with far apart representations often featured an informal word that appeared closer to words that were thematically related but not similar in meaning. The representation of \"Orgasim\" was closer to a number of medical terms, including \"abscess,\" \"hysterectomy,\" \"hematoma\" and \"cochlear\" when it was \"orgasm,\" other words that were \"pairing,\" were not punished. \""}, {"heading": "5 Biases and Drawbacks", "text": "For example, a writer can use more casual spelling to convey sarcasm: I see women who support Trump or Brock Turner, and I'm like \"wow u r such a good example for our daughter lol not poor bitch\" (Twitter, Jun 18, 2016) or whimsy: * Take a personal test that I knew I should have studied harder for (Twitter, Jun 6, 2016). An intrinsic measure of spelling similarity will not address these aspects. Some of the drawbacks of metrics based on cosmic similarity, as discussed in Faruqui et al. (2016), apply here too. Specifically, we don't know whether performance would correlate well with extrinsic metrics."}, {"heading": "6 Conclusions", "text": "It is possible to collect more pairs of spelling variants by selecting more commonly used patterns (such as the more than 5,000 entries that contain the string \"spelling\") to select candidate definitions. We could then use more complex rules, a learned model, or human participants to extract the spelling variants from the definitions. However, the simplicity of our system, which requires minimal human labor, makes it a handy option for evaluating specialized word embeddings for social media texts. Our experiments with GloVe suggest that models based solely on the distributional similarity of words may be limited in their ability to represent the semantics of online language. Some recent work has learned depictions of Twitter embeddings that use both string and distributional information (Dhingra et al., 2016); (Vosoughi et al., 2016). These models should have a significant advantage if the spelling variants are likely to rely on metrics."}], "references": [{"title": "Learning multiview embeddings of twitter", "author": ["Benton et al.2016] Adrian Benton", "Raman Arora", "Mark Dredze"], "venue": null, "citeRegEx": "Benton et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Benton et al\\.", "year": 2016}, {"title": "Tweet2vec: Character-based distributed representations for social media", "author": ["Zhong Zhou", "Dylan Fitzpatrick", "Michael Muehl", "William W Cohen"], "venue": "In Proceedings of ACL", "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Phonological factors in social media writing", "author": ["Jacob Eisenstein"], "venue": "In Proc. of the Workshop on Language Analysis in Social Media,", "citeRegEx": "Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "What to do about bad language on the internet", "author": ["Jacob Eisenstein"], "venue": "In HLT-NAACL,", "citeRegEx": "Eisenstein.,? \\Q2013\\E", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "Problems with evaluation of word embeddings using word similarity tasks. In RepEval", "author": ["Yulia Tsvetkov", "Pushpendre Rastogi", "Chris Dyer"], "venue": null, "citeRegEx": "Faruqui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2016}, {"title": "Intrinsic evaluations of word embeddings: What can we do better? In RepEval", "author": ["Gladkova", "Drozd2016] Anna Gladkova", "Aleksandr Drozd"], "venue": null, "citeRegEx": "Gladkova et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gladkova et al\\.", "year": 2016}, {"title": "Unsupervised mining of lexical variants from noisy text", "author": ["Gouws et al.2011] Stephan Gouws", "Dirk Hovy", "Donald Metzler"], "venue": "In Proceedings of the First Workshop on Unsupervised Learning in NLP,", "citeRegEx": "Gouws et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2011}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Learning sentiment-specific word embedding for twitter sentiment classification", "author": ["Tang et al.2014] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": "ACL", "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "Tweet2vec: Learning tweet embeddings using character-level cnn-lstm encoder-decoder", "author": ["Prashanth Vijayaraghavan", "Deb Roy"], "venue": null, "citeRegEx": "Vosoughi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vosoughi et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Several new works have thus targeted social media platforms by learning word representations specific to such domains (Tang et al., 2014); (Benton et al.", "startOffset": 118, "endOffset": 137}, {"referenceID": 0, "context": ", 2014); (Benton et al., 2016).", "startOffset": 9, "endOffset": 30}, {"referenceID": 0, "context": "They can extract persona or dialect information while handling the semantic or syntactic features of words (Benton et al., 2016).", "startOffset": 107, "endOffset": 128}, {"referenceID": 7, "context": "A similar method is common in assessing performance on analogical reasoning tasks (Mikolov et al., 2013).", "startOffset": 82, "endOffset": 104}, {"referenceID": 6, "context": "pipeline described by (Gouws et al., 2011).", "startOffset": 22, "endOffset": 42}, {"referenceID": 1, "context": "When we rely on lexical similarity to find variants, we also offer an unfair advantage to representations that include character-level similarity as part of the model, such as (Dhingra et al., 2016).", "startOffset": 176, "endOffset": 198}, {"referenceID": 8, "context": "As a test, we performed an evaluation on embeddings trained with GloVe (Pennington et al., 2014) on a 121GB English Twitter corpus.", "startOffset": 71, "endOffset": 96}, {"referenceID": 4, "context": "based on cosine similarity, as discussed in Faruqui et al. (2016), apply here as well.", "startOffset": 44, "endOffset": 66}, {"referenceID": 1, "context": "Some recent work has learned representations of embeddings for Twitter using character sequences as well as distributional information (Dhingra et al., 2016); (Vosoughi et al.", "startOffset": 135, "endOffset": 157}, {"referenceID": 10, "context": ", 2016); (Vosoughi et al., 2016).", "startOffset": 9, "endOffset": 32}], "year": 2016, "abstractText": "Existing corpora for intrinsic evaluation are not targeted towards tasks in informal domains such as Twitter or news comment forums. We want to test whether a representation of informal words fulfills the promise of eliding explicit text normalization as a preprocessing step. One possible evaluation metric for such domains is the proximity of spelling variants. We propose how such a metric might be computed and how a spelling variant dataset can be collected using UrbanDictionary.", "creator": "LaTeX with hyperref package"}}}