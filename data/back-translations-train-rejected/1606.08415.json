{"id": "1606.08415", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "abstract": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic process which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "histories": [["v1", "Mon, 27 Jun 2016 19:20:40 GMT  (435kb,D)", "http://arxiv.org/abs/1606.08415v1", null], ["v2", "Fri, 8 Jul 2016 18:32:46 GMT  (608kb,D)", "http://arxiv.org/abs/1606.08415v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["dan hendrycks", "kevin gimpel"], "accepted": false, "id": "1606.08415"}, "pdf": {"name": "1606.08415.pdf", "metadata": {"source": "CRF", "title": "Bridging Nonlinearities and Stochastic Regularizers with Gaussian Error Linear Units", "authors": ["Dan Hendrycks", "Kevin Gimpel"], "emails": ["dan@ttic.edu", "kgimpel@ttic.edu"], "sections": [{"heading": "1 Introduction", "text": "The sigmoid function was once the most popular nonlinear activation function for neural networks. Although we have a probabilistic interpretation, it has fallen out of favor due to slow and inaccurate convergence. In contrast, widespread ReLU activation generally shows superior convergence, but it lacks its likely interpretation [8]. Both nonlinearities increase monotonously and therefore yield greater results for larger input factors - their utility lies in how they affect their input.Some neural network regulation strategies, on the other hand, we act independently of a neuron value. For example, dropout sets neural values to zero, and zoneout preserves randomly calculated neuron values. [10, 5] Dropout can act as a stochastic zero card, and zoneout can act as a stochastic identity card. The ReLU is a deterministic zero card or identity card, depending on the input value."}, {"heading": "3 Experiments", "text": "We evaluate the tasks GELU, ReLU and ELU according to the MNIST classification (grayscale images with 10 classes, 60k training examples and 10k test examples), MNIST autocoding and CIFAR-10 (color images with 10 classes, 50k training examples and 10k test examples). We do not evaluate the LReLU because of its similarity to ReLUs (see [6] for a description of the LReLUs)."}, {"heading": "3.1 MNIST Classification", "text": "We train a fully connected neural network with GELUs (\u00b5 = 0, \u03c3 = 1), ReLUs and ELUs (\u03b1 = 1). Each 7-layer, 128-neuron wide neural network is trained for 50 epochs with a batch size of 128. We use the Adam Optimizer and its suggested learning rate of 0.001 [4]. As a result of our reasoning in the previous section, weights are uniformly initialized on the hypersphere of the unit; this has positive effects on the performance of any nonlinearity [7, 9]. Figure 2 shows that the GELU tends to have the lowest median training log loss."}, {"heading": "3.2 MNIST Autoencoder", "text": "To achieve this, we use a network with layers of width 1000, 500, 250, 30, 250, 500, 1000, in this order. We use again the Adam optimizer and a lot size of 64. Our loss is the mean square 0 50 100 150 epoch10 2Re cons truct ion Erro rGELU ReLU ELU0 50 100 150 epoch10 2GELU ReLU ELUFigure 3: MNIST Autoencoding Results. Each curve is the median of three passes, and we plot each on a logarithmic scale, because the mean square losses are small. On the left are loss curves for a learning rate of 10 \u2212 3, the middle figure has a learning rate of 10 \u2212 5. We vary the learning rate from 10 \u2212 3 \u2212 10% to these significant square losses, or the two figures of ELU \u2212 3 are stable."}, {"heading": "3.3 CIFAR-10", "text": "Next, we show that for more complex architectures, GELU nonlinearity with dropout outperforms other nonlinearity. We use a convoluted architecture in which the stacks (2 x 3 x 32), (2 x 3 x 64) represent the number of layers, the receiver field, or the number of filters. We then feed this output through a two-layer network of 512 or 256 neurons for the two layers, respectively. We apply max pooling after each stack and conduct two experiments: we do not apply dropout or drop-out rates of 0.25 after the first stack, 0.25 after the second stack, and 0.5 before the last fully connected layer. As always, we use the Adam Optimizer with a learning rate of 0.001 and initialize weights on the unit hypersphere (each filter has a \"2 standard of one). Figure 4 shows the results. In both situations, GELUs offer a faster and more inspiring process, though it is GELU-inspired by a different one."}, {"heading": "4 Discussion", "text": "The MNIST experiments are similar to those carried out by the architects of the ELU, but unlike their experiments, we use Adam instead of ordinary gradient lineage, because Adam1 After that we have tried a learning rate of 10 \u2212 2, but all solutions are poorly converged, GELUs are tied to ReLUs and ELUs diverge. This better reflects what is normally used in practice for training neural networks. Interestingly, this change results in ELUs occasionally performing worse than ReLUs. In addition, we find that an optimizer with dynamics is important to allow GELUs to achieve good convergence, whereas earlier nonlinearity does not have this (slight) disadvantage. This suggests that GELUs produce an error surface that is different from the error surfaces generated by ReLUs."}, {"heading": "5 Conclusion", "text": "This nonlinearity has a probable interpretation and could therefore lead to a deeper understanding of the feed process as a whole. Other future research approaches include exploring the LaLU, xFL (x), L \u0445 Laplace (0, 1), as this could promote neuron sparseness. Another way is to find a meaningful way to apply the SOI map to many different tasks, as it remains unclear how best to change the extent to which the SOI map regulates. Fortunately, the GELU does not require an obvious adjustment to exceed the accuracy of previous nonlinearities."}], "references": [{"title": "Learning with pseudo-ensembles", "author": ["Philip Bachman", "Ouais Alsharif", "Doina Precup"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)", "author": ["Djork-Arne Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Andrew L. Maas", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "All You Need Is a Good Init", "author": ["Dmytro Mishkin", "Jiri Matas"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["Vinod Nair", "Geoffrey E. Hinton"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M. Saxe", "James L. McClelland", "Surya Ganguli"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "In contrast, the widely-used ReLU activation usually demonstrates superior convergence yet is lacking in terms of its probabilistic interpretation [8].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "For example, dropout randomly sets neuron values to zero, and zoneout randomly preserves previously computed neuron values [10, 5].", "startOffset": 123, "endOffset": 130}, {"referenceID": 0, "context": "But this determinism of the ReLU is limiting; the stochasticity of dropout and zoneout may allow for a pseudo-ensemble [1] of networks with stochastic width or stochastic depth, respectively, and this leads to improved performance [3, 11].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "The GELU bears semblance to the ReLU and ELU (see [2] for an ELU description).", "startOffset": 50, "endOffset": 53}, {"referenceID": 3, "context": "We do not evaluate the LReLU because of its similarity to ReLUs (see [6] for a description of LReLUs).", "startOffset": 69, "endOffset": 72}, {"referenceID": 2, "context": "001 [4].", "startOffset": 4, "endOffset": 7}, {"referenceID": 4, "context": "The weights are uniformly initialized on the unit hypersphere as a consequence of our argument in the previous section; this has positive impact on each nonlinearity\u2019s performance [7, 9].", "startOffset": 180, "endOffset": 186}, {"referenceID": 6, "context": "The weights are uniformly initialized on the unit hypersphere as a consequence of our argument in the previous section; this has positive impact on each nonlinearity\u2019s performance [7, 9].", "startOffset": 180, "endOffset": 186}], "year": 2017, "abstractText": "We propose the Gaussian Error Linear Unit (GELU), a high-performing neural network activation function. The GELU nonlinearity is the expected transformation of a stochastic process which randomly applies the identity or zero map, combining the intuitions of dropout and zoneout while respecting neuron values. This connection suggests a new probabilistic understanding of nonlinearities. We perform an empirical evaluation of the GELU nonlinearity against the ReLU and ELU activations and find performance improvements across all tasks.", "creator": "LaTeX with hyperref package"}}}