{"id": "1606.08495", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Network-Efficient Distributed Word2vec Training System for Large Vocabularies", "abstract": "Word2vec is a popular family of algorithms for unsupervised training of dense vector representations of words on large text corpuses. The resulting vectors have been shown to capture semantic relationships among their corresponding words, and have shown promise in reducing a number of natural language processing (NLP) tasks to mathematical operations on these vectors. While heretofore applications of word2vec have centered around vocabularies with a few million words, wherein the vocabulary is the set of words for which vectors are simultaneously trained, novel applications are emerging in areas outside of NLP with vocabularies comprising several 100 million words. Existing word2vec training systems are impractical for training such large vocabularies as they either require that the vectors of all vocabulary words be stored in the memory of a single server or suffer unacceptable training latency due to massive network data transfer. In this paper, we present a novel distributed, parallel training system that enables unprecedented practical training of vectors for vocabularies with several 100 million words on a shared cluster of commodity servers, using far less network traffic than the existing solutions. We evaluate the proposed system on a benchmark dataset, showing that the quality of vectors does not degrade relative to non-distributed training. Finally, for several quarters, the system has been deployed for the purpose of matching queries to ads in Gemini, the sponsored search advertising platform at Yahoo, resulting in significant improvement of business metrics.", "histories": [["v1", "Mon, 27 Jun 2016 22:00:21 GMT  (144kb,D)", "http://arxiv.org/abs/1606.08495v1", "10 pages, 2 figures"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["erik ordentlich", "lee yang", "y feng", "peter cnudde", "mihajlo grbovic", "nemanja djuric", "vladan radosavljevic", "gavin owens"], "accepted": false, "id": "1606.08495"}, "pdf": {"name": "1606.08495.pdf", "metadata": {"source": "CRF", "title": "Network\u2013Efficient Distributed Word2vec Training System for Large Vocabularies", "authors": ["Erik Ordentlich", "Lee Yang", "Andy Feng", "Peter Cnudde", "Mihajlo Grbovic", "Nemanja Djuric", "Vladan Radosavljevic", "Gavin Owens"], "emails": ["mihajlo}@yahoo-inc.com"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "2. SPONSORED SEARCH USE CASE", "text": "This year is the highest in the history of the country."}, {"heading": "3. THE WORD2VEC TRAINING PROBLEM", "text": "In this paper, we focus on the Skipgram approach with random negative examples proposed in [23]. It was found that the best results among the proposed variants come down to a variety of semantic tests of the resulting vectors [19, 23]. Given a corpus consisting of a sequence of words s1, s2, sn of each sequence of words si = 1, wi, 2, etc., the goal is to maximize the log probability of words."}, {"heading": "4. EXISTING WORD2VEC SYSTEMS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Single machine", "text": "Several existing word2vec training systems are limited to operation on a single machine, but several parallel execution strands are used on different segments of the training data, including the original open source implementation of word2vec [23], as well as those of Medallia [22] and Rehurek [28]. As mentioned in the introduction, these systems would require much larger memory configurations than those available on typical merchandise-scale servers."}, {"heading": "4.2 Distributed data-parallel", "text": "A similar disadvantage applies to distributed data-parallel training systems such as those available in Apache Spark MLLib [8] and Deeplearning4j [12]. In the former, the Spark driver sends the latest vectors to all Spark exporters in each iteration. Each executor modifies its local vector copy based on its partition of the training data set, and the driver then combines local vector modifications to update the global vectors. It requires that all vectors are stored in the memory of all Spark exporters, and is therefore not suitable for large vocabularies, much like the individual machines. The Deeplearning4j system takes a similar approach and therefore suffers from the same limitations, although it allows the use of GPUs to speed up training on each machine."}, {"heading": "4.3 Parameter servers", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "5. NETWORK-EFFICIENT DISTRIBUTED WORD2VEC TRAINING SYSTEM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Architecture", "text": "We're not able to focus on the system we're proposing. \u2022 We're not able to focus on the system we're proposing. \u2022 We're not able to focus on the system we're proposing. \u2022 We're not able to focus on the system we're proposing. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus. \u2022 We're not able to focus."}, {"heading": "5.2 Network bandwidth analysis", "text": "Using the same notation as in (2), and if S indicates the number of shards, the average bytes transmitted by all PS shards for each dotprod call are transmitted by b \u00b7 (w \u00b7 (n + 1) \u00b7 S \u00b7 4. (6) That is, each shark transmits the subset results between the input vector of each minibatch word and all context words (there is no more than an average of w per minibatch word) and negative examples (there is no more than n per context per minibatch word or n \u00b7 w per minibatch word). It is not difficult to see that this is exactly the number of bytes transmitted to all PS shards for the vector component of each adjust call. That is, there are two linear vector updates for each vector pair per minibatch word or n per minibatch word."}, {"heading": "6. IMPLEMENTATION ON HADOOP", "text": "We have implemented the system described in Section 5 in Java and Scala on a planned Hadoop YARN cluster using Slider [6] and Spark [7]. Our end-to-end training implementation comprises four steps: vocabulary generation, data pre-processing, training and vector export. Next, we will review the details of these steps. All data, including the initial training data and its pre-processed version, will be stored in the Hadoop Distributed File System (HDFS) for all exported vectors. We note that although our calculation environment is currently based on Hadoop and Spark, other distributed calculation frameworks such as the recently released TensorFlow could also serve as a platform for implementing the proposed system.7"}, {"heading": "6.1 Main steps", "text": "This year is the highest in the history of the country."}, {"heading": "6.2 Training step throughput", "text": "To get an idea of the type of training we can achieve with this system, a configuration must be found that requires the sponsored application in our Hadoop cluster: 882000 + nodes with 128 GB of memory, 5 negative examples, 7 theses per day, 7 theses per day, 7 theses per day, 7 theses per day, 8 theses per day, 8 theses per day, 7 theses per day, 8 theses per day, 7 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day, 8 theses per day."}, {"heading": "7. EVALUATION & DEPLOYMENT", "text": "In this section, we provide evidence that the vectors trained by the proposed distributed system are of high quality, even with fairly aggressive parallelism during training. We also show bucket test results in live web search traffic that compare the performance of our large vocabulary model with the performance of our model, which was implemented with a single machine, leading to the decision to use the proposed system in production at the end of 2015."}, {"heading": "7.1 Benchmark data set", "text": "To compare the proposed distributed systems, we have set vectors to a publicly available data set derived from the script \"demo-train-big-model-v1-compute-only.sh,\" which collects a large number of publicly available text passages and processes them with the algorithm described in it. The resulting data has about 8 billion words and yields a vocabulary of about 7 million words and phrases."}, {"heading": "7.2 Sponsored Search data set", "text": "Figure 4 shows the ten most similar and least similar of the 800 most similar queries, and we note that the ten least similar queries can still be considered relatively semantically similar. This particular set of vectors was trained for a vocabulary of 200 million generalized words using the 300 dimension vector, 15 PS Shard settings described in Section 6.2. We found that the vector quality shown in Figure 4 is the norm based on inspections of similar matchings of query vectors with a number of ad vectors. We also compared the cosmic similarities for pairs of vectors trained using the proposed distributed system and for corresponding vector pairs trained using the open source implementation of word vectors with a number of ad vectors."}, {"heading": "7.3 Online A/B tests", "text": "After successful offline evaluation of the proposed distributed system, in the following series of experiments, we conducted tests on live web search traffic. We conducted two bucket tests, each on 5% of the search traffic, where we compared queryad matches produced by training queries and ad vectors that spanned search session data over 9 months. One model was performed using the implementation of [23] and the other was trained with the proposed distributed system. Both buckets were compared against control buckets that employed a collection of various broad matching techniques used at the time of the test. Each of the online tests was performed for 10 days, one at a time, more than a month apart. Results of the tests were compared in terms of query coverage (part of the queries for which ads were shown), Auction Depth (number of ads per query that made it to an auction) click rate (CTR, or number of ad clicks divided by the number of ad impressions)."}, {"heading": "8. CONCLUSION", "text": "In this paper, we presented a novel scalable word2vec training system that, unlike available systems, can produce semantically accurate vectors for hundreds of millions of vocabulary words with training latency and network bandwidth usage that are suitable for regular training on raw material clusters. We motivated the usefulness of word2vec training with a large vocabulary with a sponsored search application that includes generalized \"words\" that correspond to queries, ads and hyperlinks for which the proposed system was used in production. Results from both benchmark data sets and online A / B tests strongly suggest the benefits of the proposed approach."}, {"heading": "9. REFERENCES", "text": "[1] M. Abadi, et. al., TensorFlow: Large-scale machinelearning on heterogeneous systems, 2015. http: / / tensorflow.org / [2] A. Ahmed, M. Li, J. Gonzalez, S. Narayanamurthy and A.J. Smola, Scalable Inference in Latent Variable Models, WSDM '12 [3] Apache Hadoop, http: / / hadoop.apache.org. [4] Apache Hadoop YARN, http: / / hadoop.apache.org in Latent Variable Models / current / hadoop-yarn."}], "references": [{"title": "et", "author": ["M. Abadi"], "venue": "al., TensorFlow: Large-scale machine learning on heterogeneous systems", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring session context using distributed representations of queries and reformulations", "author": ["M. Bhaskar"], "venue": "Proc. SIGIR, 3\u201312", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["A. Bordes", "N. Usunier", "A. Garcia-Duran", "J. Weston", "O. Yakhnenko"], "venue": "Proc. NIPS, 2787\u20132795", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E.Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "ACM Trans. on Inform. Sys.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Context-and content-aware embeddings for query rewriting in sponsored search", "author": ["M. Grbovic", "N. Djuric", "V. Radosavljevic", "F. Silvestri", "N. Bhamidipati"], "venue": "Proc. SIGIR, 383\u2013392. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Sponsored search: An overview of the concept", "author": ["B.J. Jansen", "T. Mullen"], "venue": "history, and technology, International Journal of Electronic Business, 6(2):114\u2013131", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "A multiplicative model for learning distributed text-based attribute representations", "author": ["R. Kiros", "R.S. Zemel", "R. Salakhutdinov"], "venue": "arXiv:1406.2710", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal neural language models", "author": ["R. Kiros", "R. Zemel", "R. Salakhutdinov"], "venue": "Proc. ICML", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Trans. of the Assoc. for Comp. Linguistics 3, 211\u2013225", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Scaling Distributed Machine Learning with the Parameter Server", "author": ["M. Li", "D.G. Andersen", "J.W. Park", "A.J. Smola", "A. Ahmed", "V. Josifovski", "J. Long", "E.J. Shekita", "B. Su"], "venue": "OSDI", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality, in Proc. NIPS 2013, source code at https://code.google.com/p/word2vec", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "HOGWILD!: A lock-free approach to parallelizing stochastic gradient descent", "author": ["F. Niu", "B. Recht", "C. Re", "S.J. Wright"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "GloVe: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proc. Emp. Methods in Nat. Lang. Proc. (EMNLP)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepwalk: Online learning of social representations", "author": ["B. Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "arXiv:1403.6652", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep learning with Word2Vec and gensim", "author": ["R. Rehurek"], "venue": "http://rare-technologies.com/ deep-learning-with-word2vec-and-gensim/", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Factorbird- a parameter server approach to distributed factorization", "author": ["S. Schelter", "V. Satuluri", "R.B. Zadeh"], "venue": "Proc. NIPS Workshop on Distributed Machine Learning and Matrix Computations", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Swivel: Improving embeddings by noticing what\u2019s missing", "author": ["N. Shazeer", "R. Doherty", "C. Evans", "C. Waterson"], "venue": "arXiv:1602.02215", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "An architecture for parallel topic models, in Proc", "author": ["A.J. Smola", "S. Narayanamurthy"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["R. Socher", "D. Chen", "C.D. Manning", "A. Ng"], "venue": "Proc. NIPS, pages 926\u2013934", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "S", "author": ["E.P. Xing", "Q. Ho", "W. Dai", "J.K. Kim", "J. Wei"], "venue": "Lee, X., Zheng, P. Xie, A. Kumar and Y. Yu, Petuum: A new platform for distributed machine learning on big data, IEEE Trans. Big Data 1(2): 49\u201367 ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Word2vec [23] is a recently proposed family of algorithms for training such vector representations from unstructured text data via shal-", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "The geometry of the resulting vectors was shown in [23] to capture word semantic similarity through the cosine similarity of the corresponding vectors as well as more complex semantic relationships through vector differences, such as vec(\u201cMadrid\u201d) - vec(\u201cSpain\u201d) + vec(\u201cFrance\u201d) \u2248 vec(\u201cParis\u201d).", "startOffset": 51, "endOffset": 55}, {"referenceID": 2, "context": "These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32], general text-based attributes [17], descriptive text of images [18], nodes in graph structure of networks [27], and queries [15], to name a few.", "startOffset": 167, "endOffset": 175}, {"referenceID": 18, "context": "These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32], general text-based attributes [17], descriptive text of images [18], nodes in graph structure of networks [27], and queries [15], to name a few.", "startOffset": 167, "endOffset": 175}, {"referenceID": 6, "context": "These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32], general text-based attributes [17], descriptive text of images [18], nodes in graph structure of networks [27], and queries [15], to name a few.", "startOffset": 207, "endOffset": 211}, {"referenceID": 7, "context": "These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32], general text-based attributes [17], descriptive text of images [18], nodes in graph structure of networks [27], and queries [15], to name a few.", "startOffset": 240, "endOffset": 244}, {"referenceID": 13, "context": "These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32], general text-based attributes [17], descriptive text of images [18], nodes in graph structure of networks [27], and queries [15], to name a few.", "startOffset": 283, "endOffset": 287}, {"referenceID": 4, "context": "These powerful ideas from the NLP community have been adapted by researchers from other domains to tasks beyond representation of words, including relational entities [10, 32], general text-based attributes [17], descriptive text of images [18], nodes in graph structure of networks [27], and queries [15], to name a few.", "startOffset": 301, "endOffset": 305}, {"referenceID": 13, "context": "For example, the number of unique nodes in a social network [27] or the number of unique queries in a search engine [15] can easily reach few hundred million, a scale that is not achievable using existing word2vec implementations.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "For example, the number of unique nodes in a social network [27] or the number of unique queries in a search engine [15] can easily reach few hundred million, a scale that is not achievable using existing word2vec implementations.", "startOffset": 116, "endOffset": 120}, {"referenceID": 10, "context": "In word2vec, each vocabulary word has two associated d-dimensional vectors which must be trained, respectively referred to as input and output vectors, each of which is represented as an array of d single precision floating point numbers [23].", "startOffset": 238, "endOffset": 242}, {"referenceID": 5, "context": "Sponsored search is a popular advertising model [16] used by web search engines, such as Google, Microsoft, and Yahoo, in which advertisers sponsor the top web search results in order to redirect user\u2019s attention from organic search results to ads that are highly relevant to the entered query.", "startOffset": 48, "endOffset": 52}, {"referenceID": 8, "context": "We leave the suitability and scalability of the more recent \u201ccount\u201d based embedding algorithms that operate on word pair co-occurrence counts [19, 26, 30] to the data sets and vocabulary sizes of interest here as open questions, noting only that the vocabularies considered in published experiments involving these alternatives is at most 500,000 words.", "startOffset": 142, "endOffset": 154}, {"referenceID": 12, "context": "We leave the suitability and scalability of the more recent \u201ccount\u201d based embedding algorithms that operate on word pair co-occurrence counts [19, 26, 30] to the data sets and vocabulary sizes of interest here as open questions, noting only that the vocabularies considered in published experiments involving these alternatives is at most 500,000 words.", "startOffset": 142, "endOffset": 154}, {"referenceID": 16, "context": "We leave the suitability and scalability of the more recent \u201ccount\u201d based embedding algorithms that operate on word pair co-occurrence counts [19, 26, 30] to the data sets and vocabulary sizes of interest here as open questions, noting only that the vocabularies considered in published experiments involving these alternatives is at most 500,000 words.", "startOffset": 142, "endOffset": 154}, {"referenceID": 1, "context": "The idea of using word2vec to train query representations is not new and has been suggested by several researchers in the past [9, 15].", "startOffset": 127, "endOffset": 134}, {"referenceID": 4, "context": "The idea of using word2vec to train query representations is not new and has been suggested by several researchers in the past [9, 15].", "startOffset": 127, "endOffset": 134}, {"referenceID": 4, "context": "The results of [15] were based on training the largest vocabulary that could fit into the large memory of a special purpose server, which resulted in learned vector representations for about 45 million words.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "random negative examples proposed in [23].", "startOffset": 37, "endOffset": 41}, {"referenceID": 8, "context": "This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19, 23].", "startOffset": 130, "endOffset": 138}, {"referenceID": 10, "context": "This has been found to yield the best results among the proposed variants on a variety of semantic tests of the resulting vectors [19, 23].", "startOffset": 130, "endOffset": 138}, {"referenceID": 10, "context": "\u2022 window sizes bi,j are randomly selected so that each inner sum includes between 1 and a maximumB terms, as in [23] and its open\u2013source implementation;", "startOffset": 112, "endOffset": 116}, {"referenceID": 10, "context": "\u2022 negative examples Ni,j,k associated with positive output word wi,k are selected randomly according to a probability distribution suggested in [23];", "startOffset": 144, "endOffset": 148}, {"referenceID": 10, "context": "We follow [23] for setting V and select words occurring in the corpus a sufficient number of times (e.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "We further also assume a randomized version of (1) according to the subsampling technique of [23], which removes some occurrences of frequent words.", "startOffset": 93, "endOffset": 97}, {"referenceID": 10, "context": "The algorithm for maximizing (1) advocated in [23], and implemented in its open\u2013source counterpart, is a minibatch stochastic gradient descent (SGD).", "startOffset": 46, "endOffset": 50}, {"referenceID": 10, "context": "Throughout, it is assumed that words not in the vocabulary or words omitted due to the subsampling of frequent words, following [23], do not count towards window or context size.", "startOffset": 128, "endOffset": 132}, {"referenceID": 8, "context": "That is, we assume \u201cdirty\u201d contexts using the terminology of [19], consistent with the open\u2013source version of [23].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "That is, we assume \u201cdirty\u201d contexts using the terminology of [19], consistent with the open\u2013source version of [23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "also based on minibatch SGD optimization of (1), however, as described in Section 5, it is carried out in a distributed fashion in a manner quite different from the implementation of [23].", "startOffset": 183, "endOffset": 187}, {"referenceID": 10, "context": "These include the original open source implementation of word2vec [23], as well as those of Medallia [22], and Rehurek [28].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "These include the original open source implementation of word2vec [23], as well as those of Medallia [22], and Rehurek [28].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "As observed and partially theoretically justified in [25] (see also [11]), in many applications involving sparse training data characterized by low average overlap between the model parameters associated with different mini-", "startOffset": 53, "endOffset": 57}, {"referenceID": 0, "context": "The parameter server paradigm has been applied successfully to the training of very large models for logistic regression, deep learning, and factorization machines, and to sampling from the posterior topic distribution in largescale Latent Dirichlet Allocation [1, 2, 11, 20, 21, 29, 30, 31, 33].", "startOffset": 261, "endOffset": 295}, {"referenceID": 9, "context": "The parameter server paradigm has been applied successfully to the training of very large models for logistic regression, deep learning, and factorization machines, and to sampling from the posterior topic distribution in largescale Latent Dirichlet Allocation [1, 2, 11, 20, 21, 29, 30, 31, 33].", "startOffset": 261, "endOffset": 295}, {"referenceID": 15, "context": "The parameter server paradigm has been applied successfully to the training of very large models for logistic regression, deep learning, and factorization machines, and to sampling from the posterior topic distribution in largescale Latent Dirichlet Allocation [1, 2, 11, 20, 21, 29, 30, 31, 33].", "startOffset": 261, "endOffset": 295}, {"referenceID": 16, "context": "The parameter server paradigm has been applied successfully to the training of very large models for logistic regression, deep learning, and factorization machines, and to sampling from the posterior topic distribution in largescale Latent Dirichlet Allocation [1, 2, 11, 20, 21, 29, 30, 31, 33].", "startOffset": 261, "endOffset": 295}, {"referenceID": 17, "context": "The parameter server paradigm has been applied successfully to the training of very large models for logistic regression, deep learning, and factorization machines, and to sampling from the posterior topic distribution in largescale Latent Dirichlet Allocation [1, 2, 11, 20, 21, 29, 30, 31, 33].", "startOffset": 261, "endOffset": 295}, {"referenceID": 19, "context": "The parameter server paradigm has been applied successfully to the training of very large models for logistic regression, deep learning, and factorization machines, and to sampling from the posterior topic distribution in largescale Latent Dirichlet Allocation [1, 2, 11, 20, 21, 29, 30, 31, 33].", "startOffset": 261, "endOffset": 295}, {"referenceID": 10, "context": "For w = 10, n = 10, d = 500, values within the ranges recommended in [23], this works out to r(10, 10, 500) \u2248 200, 000 bytes transferred per word with each get and put.", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "The vectors are initialized in the parameter server shards as in [23].", "startOffset": 65, "endOffset": 69}, {"referenceID": 10, "context": "\u2022 dotprod: Select negative examples w\u0303 in (4) according to a probability distribution derived from the vocabulary histogram proposed in [23], but with the client thread supplied seed initializing the random number generation, and then return all partial dot products required to evaluate the gradient (4) for all positive output, negative output, and input word vectors associated with the minibatch, wherein the partial dot products involve those vector components stored on the designated shard: usv T s .", "startOffset": 136, "endOffset": 140}, {"referenceID": 10, "context": "The data set is iterated over multiple times and after each iteration, the learning rate \u03b1 is reduced in a manner similar to the open source implementation of [23].", "startOffset": 159, "endOffset": 163}, {"referenceID": 11, "context": "The lack of synchronization introduces many approximations into the overall SGD computation, similar in spirit to the HOGWILD [25] and Downpour SGD [11] distributed optimization schemes.", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "sh\u2019 from the open-source package of [23].", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "This script collects a variety of publicly available text corpuses and processes them using the algorithm described in [23] to coalesce sufficiently co-occurring words into phrases.", "startOffset": 119, "endOffset": 123}, {"referenceID": 3, "context": "txt\u2019 file and also evaluated Spearman\u2019s rank correlation with respect to the editorial evaluation of semantic relatedness of pairs of words in the well known wordsim-353 collection [14].", "startOffset": 181, "endOffset": 185}, {"referenceID": 10, "context": "The first column shows results for the single machine implementation of [23], the second for a \u2019low parallelism\u2019 configuration of our system using 50 Spark executors, minibatch size of 1, and 1 thread per executor, and the third column for a \u2019high parallelism\u2019 configuration again with 50 executors, but with minibatch size increased to 50 and 8 threads per executor.", "startOffset": 72, "endOffset": 76}, {"referenceID": 10, "context": "The various systems were run using the skipgram variant with 500 dimensional vectors, maximum window size of 20 (10 in each direction), 5 negative examples, subsample ratio of 1e-6 (see [23]), initial learning rate of 0.", "startOffset": 186, "endOffset": 190}, {"referenceID": 10, "context": "We are unsure why our system yields better results than the implementation of [23] on the wordsim test, yet worse scores on the analogies test.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "sh\u2019 of [23].", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "We also compared the cosine similarities for pairs of vectors trained using the proposed distributed system and for corresponding vector pairs trained using the open\u2013source implementation of [23], again on a large search session data set.", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "One model was trained using implementation from [23] and the other was trained using the proposed distributed system.", "startOffset": 48, "endOffset": 52}], "year": 2016, "abstractText": "Word2vec is a popular family of algorithms for unsupervised training of dense vector representations of words on large text corpuses. The resulting vectors have been shown to capture semantic relationships among their corresponding words, and have shown promise in reducing a number of natural language processing (NLP) tasks to mathematical operations on these vectors. While heretofore applications of word2vec have centered around vocabularies with a few million words, wherein the vocabulary is the set of words for which vectors are simultaneously trained, novel applications are emerging in areas outside of NLP with vocabularies comprising several 100 million words. Existing word2vec training systems are impractical for training such large vocabularies as they either require that the vectors of all vocabulary words be stored in the memory of a single server or suffer unacceptable training latency due to massive network data transfer. In this paper, we present a novel distributed, parallel training system that enables unprecedented practical training of vectors for vocabularies with several 100 million words on a shared cluster of commodity servers, using far less network traffic than the existing solutions. We evaluate the proposed system on a benchmark dataset, showing that the quality of vectors does not degrade relative to nondistributed training. Finally, for several quarters, the system has been deployed for the purpose of matching queries to ads in Gemini, the sponsored search advertising platform at Yahoo, resulting in significant improvement of business metrics.", "creator": "TeX"}}}