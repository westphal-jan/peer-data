{"id": "1606.08501", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "Symmetric and antisymmetric properties of solutions to kernel-based machine learning problems", "abstract": "A particularly interesting instance of supervised learning with kernels is when each training example is associated with two objects, as in pairwise classification (Brunner et al., 2012), and in supervised learning of preference relations (Herbrich et al., 1998). In these cases, one may want to embed additional prior knowledge into the optimization problem associated with the training of the learning machine, modeled, respectively, by the symmetry of its optimal solution with respect to an exchange of order between the two objects, and by its antisymmetry. Extending the approach proposed in (Brunner et al., 2012) (where the only symmetric case was considered), we show, focusing on support vector binary classification, how such embedding is possible through the choice of a suitable pairwise kernel, which takes as inputs the individual feature vectors and also the group feature vectors associated with the two objects. We also prove that the symmetry/antisymmetry constraints still hold when considering the sequence of suboptimal solutions generated by one version of the Sequential Minimal Optimization (SMO) algorithm, and we present numerical results supporting the theoretical findings. We conclude discussing extensions of the main results to support vector regression, to transductive support vector machines, and to several kinds of graph kernels, including diffusion kernels.", "histories": [["v1", "Mon, 27 Jun 2016 22:34:14 GMT  (386kb,D)", "https://arxiv.org/abs/1606.08501v1", null], ["v2", "Fri, 28 Oct 2016 02:13:31 GMT  (391kb,D)", "http://arxiv.org/abs/1606.08501v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["giorgio gnecco"], "accepted": false, "id": "1606.08501"}, "pdf": {"name": "1606.08501.pdf", "metadata": {"source": "CRF", "title": "Symmetric and antisymmetric properties of solutions to kernel-based machine learning problems", "authors": ["Giorgio Gnecco"], "emails": ["giorgio.gnecco@imtlucca.it"], "sections": [{"heading": null, "text": "Keywords: Kernel Methods, Symmetry and Antisymmetry, Pairwise Support Vector Machines, Sequential Minimal Optimization, Optimum and Suboptimal Solutions"}, {"heading": "1. Introduction", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "2. Symmetry and antisymmetry constraints", "text": "In this section we provide a definition of symmetry and antisymmetry constraints that will be examined in the thesis (together with an overview of the type of problems that will be investigated).In the following we will examine the symbols of each character and the symbol of the three types of character (each designated with a vector of group characteristics that do not change at all (changes only in characters, respectively) after an exchange of order between the two objects of the ordered characteristics. Particularly simple examples of the three types of character are respectively position vectors, their euclidean distance and the relative position of the second object of the ordered pair with respect to the first. We refer to (Dard et al, 2016) for some more differentiated examples of such individual and group characteristics that we refer to."}, {"heading": "3. Preliminaries: some typologies of pairwise kernels", "text": "To impose symmetry and antisymmetry constraints (3) and (4), we need to introduce some basic notations on the following types of nuclei (2015), which will be widely used in the next sections: (a) pair-by-pair kernels: one (possibly infinite dimensional) Euclidean space E (attribute space) and a nonlinear mapping structure: Rn \u2192 E, the paired kernel K: Rn \u00b7 Rn \u2192 R represents the inner productivity of attribute E between the images of the vectors in Rn, while by the following definition: K (Xab, Xcd)."}, {"heading": "4. Two methods to impose symmetry and antisymmetry constraints on the", "text": "In the following we explain two methods to impose symmetry and antisymmetry constraints on the optimal solution of a nuclear-based machine learning problem (Shawe-Taylor and Christianini, 2004, Section 7.2.2) (see Section 7 for extensions)."}, {"heading": "4.1 Use of symmetric/antisymmetric training sets", "text": "In the following, we assume that every time the training workload includes the ordered pair (a, b), it also includes the ordered pair (b, a), the ordered pair (b, a). Furthermore, we assume that the ordered pair (b, a) does not contain ordered pairs of the form (a, a). For C > 0 and an order invariant co-associated with the mapping margin E, the primary optimization problem that classifies the formation of the corresponding l1 soft margin binary SVM: minimizew."}, {"heading": "4.2 Use of balanced/skew-balanced kernels", "text": "An alternative way to solve the symmetry / antisymmetry conditions from theorem 1 to an optimal solution of the primary optimization problem (12) consists in solving the following two variants (a) and (b) of the dual optimization problem (13). Below, for each disordered pair of different objects a, b, one of the two possible optimization problems (say, either (a, b) or (b, a) is presented as an optimization problem, and we designate the set of ordered pairs obtained by this procedure. Let Ko be any order invariant and consider the following optimization problems. (a) Case of symmetrical labels: minimize {\u03b2ab, b) H b), b)."}, {"heading": "5. Analysis of the Sequential Minimal Optimization algorithm from the point of", "text": "In this section, we focus on examining how it is possible to obtain either symmetrical or antisymmetrical solutions to the primary and dual optimization problems (12) and (13), focusing on the behavior of a commonly used algorithm to solve the dual problem (13), which is the sequential Minimal Optimization (SMO) algorithm (Platt, 1999). We remember that in each of its iterations, both primary and dual feasibility are guaranteed, whereas the Karush-KuKuKuhn-Tucker conditions (KT) can be violated in the case of the suboptimality of the current solution. In each iteration, this algorithm fixes all the dual variables to the previous values, except for two of them, and then resolves the resulting two-step optimization problem (in closed form, which is one of the main advantages of the algorithm over others) that arise in a new dual solution."}, {"heading": "6. Numerical results", "text": "In a first scenario of numerical simulations, we tested the obtained theoretical results for solving the dual problem (13) with LIBSVM, based on a set of artificial datas10. As regards the real applications of such properties, we refer, for example, to our current work (Dardard et al., 2016) on an application with real motion data. Other real world applications are described in Section 8. In both cases, we will consider a small number of training examples, only to report in the successive tables the optimal dual variables associated with all support vectors (i.e.), in the context of the work, the ordered pairs of training examples are associated with non-optimal dual dual variables associated with non-optimal dual variables that we associate with non-optimal dual variables that we associate with all support vectors (i.e.), with non-optimal dual variables that we associate with training examples that we associate with non-zero optimal dual variables that we associate with, at the same time we keep small in size (a)."}, {"heading": "7. Extensions", "text": "Below we will discuss some extensions of the results obtained in the publication.15. For the sake of simplicity, in these scenarios the group characteristics were generated in a very simple way using the individual characteristics of the objects of the pair (see, however, note 1 for more complex situations).16. In this case, due to the balance of the two classes, test accuracy is a good criterion for evaluating the performance of the classifiers. Otherwise, other criteria such as the area below the receiver characteristic (AUC) may be used in other cases (Fawcett, 2006)."}, {"heading": "7.1 Extension to support vector regression", "text": "Lemma 1 and Theorem 1 in section 4.1 can be extended to support vector regression with the linear \u03b5-insensitive loss function (described, e.g., in (Shawe-Taylor and Christianini, 2004, section 7.3.3). In this case, for given C, \u03b5 > 0, the primary and dual optimization problems have, respectively, the formal and dual optimization problems, the formal and dual optimization problems have, respectively, the formal and dual optimization problems have, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively, qualitatively,"}, {"heading": "7.3 Extension to feature vectors associated with multiple pairs of nodes in a graph", "text": "As noted earlier, the results obtained in the work have application in the context of the following machine learning problem: a diagram with a possibly large number of arcs (some of which are monitored) is given (a possible relationship between a set of characteristics that are associated with the nodes, connected by the same arc (i.e., ordered pair of nodes) and the (possibly binary, in the case of binary classification, otherwise real value, in the case of regression) weight of this arc, among the additional knowledge provided by the symmetry (or antisymmetry) of the relationship to be discovered. Now, let us consider the following extension, which takes the structure of the diagram more deeply into account (another way to do this is reported at the end of this section).One such extension consists in that the characteristics that are associated not only with the nodes, but also with their neighbors: e.g. a graph in the case of an h-regular graph."}, {"heading": "7.4 Extension to diffusion kernels on an auxiliary graph", "text": "We conclude from the description of the following other extension of our analysis to the problem of arc classification on graphs, taking us deeper into the graph structure. The aim is to show how to construct a diffusion core (Condor and Lafferty, 2002), (Shawe-Taylor and Cristianini, 2004, Section 9.4) on an auxiliary graph whose nodes are a selection of the directed arcs of the original graph and provide sufficient conditions under which such a diffusion core is balanced or skew-balanced (see next theorem 5), a numerical comparison with other (unbalanced or non-skew-balanced) graphs (Vishwanathan et al, 2010) (including other diffusion nuclei), and with the multiple-pair kernels introduced in Section 7.3, is postponed to a future investigation. Below, likewise in Section 4.1, we assume that we are proposing a number of pairs, and that there are a number of different objects."}, {"heading": "8. Conclusions", "text": "The question that arises is whether this is a way in which such symmetry / anti-symmetry properties can be imposed, which, depending on the specific machine learning problem, can form an additional a-priori knowledge. Results show that it is possible to establish such symmetry / anti-symmetry properties that, depending on the specific machine learning process, form a symmetrical structure that carries within itself the reference to the inventory system of (Kira). 2014, the first part of the analysis specializes in what has been done in (Brunner et al), 2012 associates to a certain transformation of feature vectors associated with a pair of objects. In addition, we have detailed an extension of the anti-symmetrical cases that have not been investigated."}, {"heading": "Appendix: Proofs", "text": "The idea of the proof is similar to the problem of (Brunner et al., 2012, Lemma et al., with some non-trivial changes in the antisymmetric case. We report here only on the problem of the case (b), since the problem of the case (a) follows directly through specialization (Brunner et al., 2012, Lemma 1) on the definition of the orderly invariant kernel (which also includes the function vector transformation associated with the operator T). Given optimal solution of the dual optimization problem (13) under condition (b), one constructs another solution defined by the definition of another kernel: = the function vector transformation associated with the operator T (13). Given optimal solution of the dual optimization problem (13) under condition (b), one constructs an optimal solution of the problem defined by another."}], "references": [{"title": "Boosting margin based distance functions for clustering", "author": ["A. Bar-Hillel", "T. Hertz", "D. Weinshall"], "venue": "Social Network Data Analytics,", "citeRegEx": "Bar.Hillel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bar.Hillel et al\\.", "year": 2011}, {"title": "Pairwise support vector machines and their application to large scale problems", "author": ["C. Brunner", "A. Fischer", "K. Luig", "T. Thies"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Brunner et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Brunner et al\\.", "year": 2012}, {"title": "Uniqueness of the SVM solution", "author": ["C.J.C. Burges", "D.J. Crisp"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "Burges and Crisp.,? \\Q2000\\E", "shortCiteRegEx": "Burges and Crisp.", "year": 2000}, {"title": "LIBSVM: a library for support vector machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "Chang and Lin.,? \\Q2011\\E", "shortCiteRegEx": "Chang and Lin.", "year": 2011}, {"title": "A study on SMO-type decomposition methods for support vector machines", "author": ["P.-H. Chen", "R.-E. Fan", "C.-J. Lin"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Chen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2006}, {"title": "On the mathematical foundations of learning", "author": ["F. Cucker", "S. Smale"], "venue": "Bulletin of the American Mathematical Society,", "citeRegEx": "Cucker and Smale.,? \\Q2002\\E", "shortCiteRegEx": "Cucker and Smale.", "year": 2002}, {"title": "Automatic classification of leading interactions in a string quartet", "author": ["F. Dardard", "G. Gnecco", "D. Glowinski"], "venue": "ACM Transactions on Interactive Intelligent Systems,", "citeRegEx": "Dardard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dardard et al\\.", "year": 2016}, {"title": "Statistical comparison of classifiers over multiple data sets", "author": ["J. Dem\u0161ar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dem\u0161ar.,? \\Q2006\\E", "shortCiteRegEx": "Dem\u0161ar.", "year": 2006}, {"title": "Bridgning logic and kernel machines", "author": ["M. Diligenti", "M. Gori", "M. Maggini", "L. Rigutini"], "venue": "Machine Learning,", "citeRegEx": "Diligenti et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Diligenti et al\\.", "year": 2012}, {"title": "Linear Algebra in Action", "author": ["H. Dym"], "venue": "AMS,", "citeRegEx": "Dym.,? \\Q2007\\E", "shortCiteRegEx": "Dym.", "year": 2007}, {"title": "An introduction to ROC analysis", "author": ["T. Fawcett"], "venue": "Pattern Recognition Letters,", "citeRegEx": "Fawcett.,? \\Q2006\\E", "shortCiteRegEx": "Fawcett.", "year": 2006}, {"title": "Learning with boundary conditions", "author": ["G. Gnecco", "M. Gori", "M. Sanguineti"], "venue": "Neural Computation,", "citeRegEx": "Gnecco et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gnecco et al\\.", "year": 2013}, {"title": "A theoretical framework for supervised learning from regions", "author": ["G. Gnecco", "M. Gori", "S. Melacci", "M. Sanguineti"], "venue": null, "citeRegEx": "Gnecco et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gnecco et al\\.", "year": 2014}, {"title": "Foundations of support constraint machines", "author": ["G. Gnecco", "M. Gori", "S. Melacci", "M. Sanguineti"], "venue": "Neural Computation,", "citeRegEx": "Gnecco et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gnecco et al\\.", "year": 2015}, {"title": "Learning with mixed hard/soft pointwise constraints", "author": ["G. Gnecco", "M. Gori", "S. Melacci", "M. Sanguineti"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "Gnecco et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gnecco et al\\.", "year": 2015}, {"title": "Supervised and semi-supervised classifiers for the detection of flood-prone areas", "author": ["G. Gnecco", "R. Morisi", "G. Roth", "M. Sanguineti", "A.C. Taramasso"], "venue": "Soft Computing,", "citeRegEx": "Gnecco et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gnecco et al\\.", "year": 2016}, {"title": "Supervised learning of preference relations", "author": ["R. Herbrich", "T. Graepel", "P. Bollmann-Sdorra", "K. Obermayer"], "venue": "In Proceedings Fachgruppentreffen Maschinelles Lernen,", "citeRegEx": "Herbrich et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Herbrich et al\\.", "year": 1998}, {"title": "Transductive inference for text classification using support vector machines", "author": ["T. Joachims"], "venue": "In Proceedings of the 16th International Conference on Machine Learning (ICML", "citeRegEx": "Joachims.,? \\Q1999\\E", "shortCiteRegEx": "Joachims.", "year": 1999}, {"title": "Improvements to Platt\u2019s SMO algorithm for SVM classifier design", "author": ["S.S. Keerthi", "S. Shevade", "C. Bhattacharyya", "K. Murthy"], "venue": "Neural Computation,", "citeRegEx": "Keerthi et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Keerthi et al\\.", "year": 2001}, {"title": "Learning with algebraic invariances, and the invariant kernel trick. arXiv: 1411.7817v1 [stat.ML", "author": ["F.J. Kir\u00e1ly", "A. Ziehe", "K.-R. M\u00fcller"], "venue": null, "citeRegEx": "Kir\u00e1ly et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kir\u00e1ly et al\\.", "year": 2014}, {"title": "Diffusion kernels on graphs and other discrete input spaces", "author": ["R.I. Kondor", "J.D. Lafferty"], "venue": "In Proceedings of the 19th International Conference on Machine Learning (ICML", "citeRegEx": "Kondor and Lafferty.,? \\Q2002\\E", "shortCiteRegEx": "Kondor and Lafferty.", "year": 2002}, {"title": "On the convergence of the decomposition method for support vector machines", "author": ["C.-J. Lin"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Lin.,? \\Q2001\\E", "shortCiteRegEx": "Lin.", "year": 2001}, {"title": "Asymptotic convergence of an SMO algorithm without any assumptions", "author": ["C.-J. Lin"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Lin.,? \\Q2002\\E", "shortCiteRegEx": "Lin.", "year": 2002}, {"title": "Laplacian Support Vector Machines Trained in the Primal", "author": ["S. Melacci", "M. Belkin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Melacci and Belkin.,? \\Q2011\\E", "shortCiteRegEx": "Melacci and Belkin.", "year": 2011}, {"title": "Spectral analysis of symmetric and antisymmetric pairwise kernels. arXiv:1506.05950v1 [cs.LG", "author": ["T. Pahikkala", "M. Viljanen", "A. Airola", "W. Waegeman"], "venue": null, "citeRegEx": "Pahikkala et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pahikkala et al\\.", "year": 2015}, {"title": "Kernel methods in system identification, machine learning and function estimation: a survey", "author": ["G. Pillonetto", "F. Dinuzzo", "T. Chen", "G. De Nicolao", "L. Ljung"], "venue": null, "citeRegEx": "Pillonetto et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pillonetto et al\\.", "year": 2014}, {"title": "Fast training of support vector machines using sequential minimal optimization. In Advances in Kernel Methods: Support Vector Learning, pages 185\u2013208", "author": ["J.C. Platt"], "venue": null, "citeRegEx": "Platt.,? \\Q1999\\E", "shortCiteRegEx": "Platt.", "year": 1999}, {"title": "Kernel methods for pattern analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "Shawe.Taylor and Cristianini.,? \\Q2004\\E", "shortCiteRegEx": "Shawe.Taylor and Cristianini.", "year": 2004}, {"title": "Large scale semi-supervised linear SVMs", "author": ["V. Sindhwani", "S.S. Keerthi"], "venue": "In Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "citeRegEx": "Sindhwani and Keerthi.,? \\Q2006\\E", "shortCiteRegEx": "Sindhwani and Keerthi.", "year": 2006}, {"title": "Learning equivariant structured output SVM regressors", "author": ["A. Vedaldi", "M. Blaschko", "A. Zisserman"], "venue": "In Proceedings of the 2011 IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Vedaldi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vedaldi et al\\.", "year": 2011}, {"title": "A new pairwise kernel for biological network inference with support vector machines", "author": ["J.P. Vert", "J. Qiu", "W. Noble"], "venue": "In BMC Bioinformatics,", "citeRegEx": "Vert et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Vert et al\\.", "year": 2007}, {"title": "A tutorial on spectral clustering", "author": ["U. von Luxburg"], "venue": "Statistics and Computing,", "citeRegEx": "Luxburg.,? \\Q2007\\E", "shortCiteRegEx": "Luxburg.", "year": 2007}, {"title": "A kernel-based framework for learning graded relations from data", "author": ["W. Waegeman", "T. Pahikkala", "A. Airola", "T. Salakoski", "M. Stock", "B. De Baets"], "venue": "IEEE Transactions on Fuzzy Systems,", "citeRegEx": "Waegeman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Waegeman et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "Abstract A particularly interesting instance of supervised learning with kernels is when each training example is associated with two objects, as in pairwise classification (Brunner et al., 2012), and in supervised learning of preference relations (Herbrich et al.", "startOffset": 173, "endOffset": 195}, {"referenceID": 16, "context": ", 2012), and in supervised learning of preference relations (Herbrich et al., 1998).", "startOffset": 60, "endOffset": 83}, {"referenceID": 1, "context": "Extending the approach proposed in (Brunner et al., 2012) (where the only symmetric case was considered), we show, focusing on support vector binary classification, how such embedding is possible through the choice of a suitable pairwise kernel, which takes as inputs the individual feature vectors and also the group feature vectors associated with the two objects.", "startOffset": 35, "endOffset": 57}, {"referenceID": 27, "context": "In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al.", "startOffset": 47, "endOffset": 83}, {"referenceID": 5, "context": "In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al.", "startOffset": 126, "endOffset": 150}, {"referenceID": 23, "context": "In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al.", "startOffset": 206, "endOffset": 257}, {"referenceID": 25, "context": "In the context of the so-called kernel methods (Shawe-Taylor and Cristianini, 2004), examples of such problems are supervised (Cucker and Smale, 2002), unsupervised (von Luxburg, 2007), and semi-supervised (Belkin and Niyogi, 2006; Melacci and Belkin, 2011) learning, and identification of models of dynamical systems (Pillonetto et al., 2014).", "startOffset": 318, "endOffset": 343}, {"referenceID": 1, "context": ", in recognizing whether two face images belong to the same person or not) (Brunner et al., 2012), and in supervised learning of preference relations (i.", "startOffset": 75, "endOffset": 97}, {"referenceID": 16, "context": ", in learning an order among objects through supervised pairs of examples) (Herbrich et al., 1998).", "startOffset": 75, "endOffset": 98}, {"referenceID": 1, "context": "The starting point of this part of our investigation is the work (Brunner et al., 2012), in which the only case of symmetry constraints was studied.", "startOffset": 65, "endOffset": 87}, {"referenceID": 1, "context": "It is worth mentioning that, according to (Brunner et al., 2012), that work is the first one in which the symmetry of the optimal solution to the training problem of an l1-soft margin binary Support Vector Machine (SVM) classifier with a symmetric training set has been rigorously proved, for a quite general class of kernels.", "startOffset": 42, "endOffset": 64}, {"referenceID": 19, "context": "Taking inspiration from the invariance framework of (Kir\u00e1ly et al., 2014), we specialize this result to the presence of both individual features and two kinds of group features, then we extend it to the case of an antisymmetric training set, showing the antisymmetry of the optimal solution.", "startOffset": 52, "endOffset": 73}, {"referenceID": 27, "context": "theory in optimization (Bertsekas, 1999) and on representer theorems for SVMs (Shawe-Taylor and Cristianini, 2004).", "startOffset": 78, "endOffset": 114}, {"referenceID": 2, "context": "We also employ existence and uniqueness results from the theory of SVMs (Burges and Crisp, 2000), and we introduce a kernel (called in the paper skew-balanced kernel), related specifically with the antisymmetry constraint.", "startOffset": 72, "endOffset": 96}, {"referenceID": 26, "context": "Additionally, in Section 5, we investigate how to impose the symmetry and antisymmetry constraints when looking for suboptimal solutions to the optimization problems investigated in the paper, analyzing the behavior of a specific algorithm proposed in the literature (Platt, 1999) from this viewpoint (i.", "startOffset": 267, "endOffset": 280}, {"referenceID": 6, "context": "2), whose application in (Dardard et al., 2016) inspired part of the theoretical investigations of the present work, providing numerical results that showed the occurrence of the antisymmetry property, for a specific choice of the kernel (the linear kernel), and to which we refer for real-world examples of individual and group features in a motion-capture context.", "startOffset": 25, "endOffset": 47}, {"referenceID": 6, "context": "No such theoretical analysis was performed in (Dardard et al., 2016), where the antisymmetry property was only observed a-posteriori, after training the learning machine.", "startOffset": 46, "endOffset": 68}, {"referenceID": 1, "context": "Finally, we outline some extensions of our analysis to the problem of arc classification on graphs, a problem for which the application of pairwise kernels was not considered in (Brunner et al., 2012).", "startOffset": 178, "endOffset": 200}, {"referenceID": 1, "context": "Besides the work (Brunner et al., 2012) mentioned above, a related paper is (Kir\u00e1ly et al.", "startOffset": 17, "endOffset": 39}, {"referenceID": 19, "context": ", 2012) mentioned above, a related paper is (Kir\u00e1ly et al., 2014), which provides the general structure of kernels incorporating prior knowledge represented by algebraic invariances.", "startOffset": 44, "endOffset": 65}, {"referenceID": 19, "context": "Moreover, even though it is related to the G-invariant kernels of (Kir\u00e1ly et al., 2014), the order-invariant kernel considered in this paper is not a particular case of the former, since, in the context of the present paper, the symmetry property for a symmetric training set is not associated only with the choice of the kernel, but also with the existence of optimal symmetric dual variables.", "startOffset": 66, "endOffset": 87}, {"referenceID": 29, "context": "Another related work is (Vedaldi et al., 2011), where a novel approach, based on a convex relaxation of a more general nonconvex optimization problem, is proposed to incorporate invariance and equivariance in the SVM training problem.", "startOffset": 24, "endOffset": 46}, {"referenceID": 32, "context": "Likewise in the present paper, symmetric and antisymmetric pairwise kernels have been also considered in (Waegeman et al., 2012), which presents sufficient conditions under which", "startOffset": 105, "endOffset": 128}, {"referenceID": 1, "context": "However, neither the extensions of the results of (Brunner et al., 2012) mentioned above are provided in (Waegeman et al.", "startOffset": 50, "endOffset": 72}, {"referenceID": 32, "context": ", 2012) mentioned above are provided in (Waegeman et al., 2012), nor an investigation of the issue of symmetry/antisymmetry preservation of suboptimal solutions to the learning problem.", "startOffset": 40, "endOffset": 63}, {"referenceID": 16, "context": "For what regards the antisymmetry constraint, the present paper extends the setting of (Herbrich et al., 1998), by allowing for more general kernels, and providing several additional theoretical results (e.", "startOffset": 87, "endOffset": 110}, {"referenceID": 6, "context": "We refer to (Dardard et al., 2016), for some more sophisticated examples of such individual and group features, i.", "startOffset": 12, "endOffset": 34}, {"referenceID": 19, "context": "It is worth remarking that, expressing the conditions (3) and (4) in terms of the operator T , is closely related to the invariance framework of (Kir\u00e1ly et al., 2014).", "startOffset": 145, "endOffset": 166}, {"referenceID": 1, "context": "As an example, in Section 4, taking the hint from (Brunner et al., 2012) (where the only condition (3) was considered), we consider two ways to impose condition (4) to the trained binary SVM classifier: the use of an antisymmetric training set (together with the choice of a suitable \u201corder-invariant\u201d kernel, see Section 4.", "startOffset": 50, "endOffset": 72}, {"referenceID": 1, "context": "For completeness, in that section we report also analogous results (stated in terms of order-invariant and balanced kernels) related to the symmetry condition (3), which, for the particular problem considered therein, are specializations of the results obtained in (Brunner et al., 2012) to the case in which the", "startOffset": 265, "endOffset": 287}, {"referenceID": 27, "context": "Similarly, if a bag-of-features (also called bag-of-words (Shawe-Taylor and Cristianini, 2004)) approach is used (i.", "startOffset": 58, "endOffset": 94}, {"referenceID": 1, "context": ", the tensor pairwise learning kernel reported later in Section 3), one can map individual feature vectors to group feature vectors in the feature spaces E associated with such kernels, which makes some examples of kernels reported in (Brunner et al., 2012) useful to deal with group feature vectors constructed starting from individual feature vectors.", "startOffset": 235, "endOffset": 257}, {"referenceID": 6, "context": ", the case of the motion-related group features considered in (Dardard et al., 2016), which were obtained through several time-series analysis).", "startOffset": 62, "endOffset": 84}, {"referenceID": 27, "context": "Case (a) is just the application of the standard machine-learning definition of (symmetric and positive semi-definite) kernel (Shawe-Taylor and Cristianini, 2004) to the situation in which each feature vector represents an ordered pair of objects (rather than, e.", "startOffset": 126, "endOffset": 162}, {"referenceID": 19, "context": ", a single object), whereas cases (b), (c), and (d) impose additional invariance properties (cases (b), and (d) also follow as special cases of the invariance framework considered in (Kir\u00e1ly et al., 2014)).", "startOffset": 183, "endOffset": 204}, {"referenceID": 1, "context": "Moreover, cases (a), (b) and (d) are specifications (due to the inclusion of both individual and group features, and the use of the operator T ) of the corresponding cases investigated in (Brunner et al., 2012)3.", "startOffset": 188, "endOffset": 210}, {"referenceID": 24, "context": ", whereas case (c) extends in a similar way the one recently introduced independently in (Pahikkala et al., 2015)4.", "startOffset": 89, "endOffset": 113}, {"referenceID": 1, "context": "As already observed in (Brunner et al., 2012), it follows from the definitions of pairwise kernels, balanced kernels, and order-invariant kernels, that every balanced kernel is also an order-invariant kernel.", "startOffset": 23, "endOffset": 45}, {"referenceID": 1, "context": "We have used the term order-invariant for the case (d), for which no specific term was used in (Brunner et al., 2012).", "startOffset": 95, "endOffset": 117}, {"referenceID": 24, "context": "The term permutation invariant is used in (Pahikkala et al., 2015).", "startOffset": 42, "endOffset": 66}, {"referenceID": 24, "context": "In (Pahikkala et al., 2015), this kernel has been introduced for a different investigation than ours, as it is related to spectral properties of a suitable linear operator associated with the kernel.", "startOffset": 3, "endOffset": 27}, {"referenceID": 1, "context": "It is worth remarking that linear, polynomial and Gaussian kernels are also considered both in (Brunner et al., 2012) and in (Kir\u00e1ly et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 19, "context": ", 2012) and in (Kir\u00e1ly et al., 2014).", "startOffset": 15, "endOffset": 36}, {"referenceID": 1, "context": "Likewise in (Brunner et al., 2012), starting from any order-invariant kernel Ko (associated with the mapping \u03c6) and exploiting its symmetry, it is possible to define a balanced kernel Kb in the following way:", "startOffset": 12, "endOffset": 34}, {"referenceID": 1, "context": "In the following sections, to simplify the notation, in a similar way as in (Brunner et al., 2012), we use the shortcuts Ko ab,cd := K o (Xab, Xcd) , K b ab,cd := K b (Xab, Xcd) , and K s ab,cd := K s (Xab, Xcd) .", "startOffset": 76, "endOffset": 98}, {"referenceID": 30, "context": "KTL ((xa, xb) , (xc, xd)) := 1 2 (k1(xa, xc)k1(xb, xd) + k1(xa, xd)k1(xb, xc)) , KML ((xa, xb) , (xc, xd)) := 1 4 (k1(xa, xc)\u2212 k1(xa, xd)\u2212 k1(xb, xc) + k1(xb, xd)) , KTM ((xa, xb) , (xc, xd)) := KTL ((xa, xb) , (xc, xd)) +KML ((xa, xb) , (xc, xd)) (called, respectively, tensor learning pairwise kernel (Vert et al., 2007), metric learning pairwise kernel (Vert et al.", "startOffset": 303, "endOffset": 322}, {"referenceID": 30, "context": ", 2007), metric learning pairwise kernel (Vert et al., 2007), and tensor metric learning pairwise kernel (Brunner et al.", "startOffset": 41, "endOffset": 60}, {"referenceID": 1, "context": ", 2007), and tensor metric learning pairwise kernel (Brunner et al., 2012)).", "startOffset": 52, "endOffset": 74}, {"referenceID": 1, "context": "One difference in our formulation of the dual optimization problem with respect to (Brunner et al., 2012) is the absence of the constraint 0 \u2264 \u03b1aa \u2264 2C, since the set I does not contain elements of the form (a, a).", "startOffset": 83, "endOffset": 105}, {"referenceID": 27, "context": ", (Shawe-Taylor and Cristianini, 2004)).", "startOffset": 2, "endOffset": 38}, {"referenceID": 27, "context": ", (Shawe-Taylor and Cristianini, 2004)).", "startOffset": 2, "endOffset": 38}, {"referenceID": 1, "context": "To save memory space, one can cache the individual features (likewise in (Brunner et al., 2012)), then compute the kernel values every time they are needed, only if the group features can be easily computed from the pairs of individual features.", "startOffset": 73, "endOffset": 95}, {"referenceID": 26, "context": "In this section, we focus on the investigation of how it is possible to obtain either symmetric or antisymmetric suboptimal solutions to the primal and dual optimization problems (12) and (13), focusing on the behavior of a commonly-used algorithm to solve the dual problem (13), which is the Sequential Minimal Optimization (SMO) algorithm (Platt, 1999).", "startOffset": 341, "endOffset": 354}, {"referenceID": 18, "context": "A modified version of such an algorithm, based on a suitable selection of the two dual variables to be re-optimized at each iteration (Keerthi et al., 2001), is guaranteed to converge asymptotically to an optimal solution of the dual problem (13) (Lin, 2001, 2002) and has been implemented in the software LIBSVM (Chang and Lin, 2011).", "startOffset": 134, "endOffset": 156}, {"referenceID": 3, "context": ", 2001), is guaranteed to converge asymptotically to an optimal solution of the dual problem (13) (Lin, 2001, 2002) and has been implemented in the software LIBSVM (Chang and Lin, 2011).", "startOffset": 164, "endOffset": 185}, {"referenceID": 4, "context": "For instance, (Chen et al., 2006) proved linear convergence of SMO under certain assumptions.", "startOffset": 14, "endOffset": 33}, {"referenceID": 6, "context": ", to our recent work (Dardard et al., 2016) for an application with real motion capture data.", "startOffset": 21, "endOffset": 43}, {"referenceID": 1, "context": "The whole procedure (which used for training all the available ordered pairs of objects, likewise in the simulations performed in (Brunner et al., 2012)) was repeated 20 times, each time considering different realizations of P, c1, c2, p1, p2, and of the training/test objects.", "startOffset": 130, "endOffset": 152}, {"referenceID": 7, "context": "Moreover, for both subtables, the difference in performance was statistically significant, according to a two-sided Wilcoxon signed-rank test (Dem\u0161ar, 2006) at confidence level \u03b1 = 0.", "startOffset": 142, "endOffset": 156}, {"referenceID": 10, "context": "Otherwise, in other cases, other criteria such as the Area under the Receiving Operating Characteristic Curve (AUC) can be used (Fawcett, 2006).", "startOffset": 128, "endOffset": 143}, {"referenceID": 1, "context": "In more details, one obtains the following extension of Lemma 1 to the regression case (for both the symmetric/antisymmetric cases, such an extension was not detailed in (Brunner et al., 2012)).", "startOffset": 170, "endOffset": 192}, {"referenceID": 17, "context": "Then, the l1-soft margin transductive SVM training problem for binary classification (Joachims, 1999), applied here to the case of the classification of ordered pairs using an order-invariant kernel Ko, is formulated as the following optimization problem17:", "startOffset": 85, "endOffset": 101}, {"referenceID": 28, "context": "Moreover, we have removed from the formulation an equality constraint, which is present in the formulation of the problem given in (Sindhwani and Keerthi, 2006), but absent in the original formulation of (Joachims, 1999) (however, see also Remark 7 for an extension to this case).", "startOffset": 131, "endOffset": 160}, {"referenceID": 17, "context": "Moreover, we have removed from the formulation an equality constraint, which is present in the formulation of the problem given in (Sindhwani and Keerthi, 2006), but absent in the original formulation of (Joachims, 1999) (however, see also Remark 7 for an extension to this case).", "startOffset": 204, "endOffset": 220}, {"referenceID": 17, "context": "In order to find a local minimum for large-scale instances of the problem above, the following label-switching algorithm was proposed in (Joachims, 1999) (here formulated in the case of ordered pairs, and with an order-invariant kernel).", "startOffset": 137, "endOffset": 153}, {"referenceID": 17, "context": "The algorithm has been proved in (Joachims, 1999) to converge in a finite number of iterations to a local minimum of (45), and has been extended in (Sindhwani and Keerthi, 2006) to a multiple label-switching algorithm, which has similar performance guarantees.", "startOffset": 33, "endOffset": 49}, {"referenceID": 28, "context": "The algorithm has been proved in (Joachims, 1999) to converge in a finite number of iterations to a local minimum of (45), and has been extended in (Sindhwani and Keerthi, 2006) to a multiple label-switching algorithm, which has similar performance guarantees.", "startOffset": 148, "endOffset": 177}, {"referenceID": 28, "context": "In more details, given a positive integer S, the multiple label-switching algorithm of (Sindhwani and Keerthi, 2006) and its selection rule at the iteration t work as follows: 1.", "startOffset": 87, "endOffset": 116}, {"referenceID": 28, "context": "This is just the additional equality constraint considered in the transductive SVM formulation reported in (Sindhwani and Keerthi, 2006), which was used to obtain the numerical results of (Dardard et al.", "startOffset": 107, "endOffset": 136}, {"referenceID": 6, "context": "This is just the additional equality constraint considered in the transductive SVM formulation reported in (Sindhwani and Keerthi, 2006), which was used to obtain the numerical results of (Dardard et al., 2016).", "startOffset": 188, "endOffset": 210}, {"referenceID": 6, "context": "Since the kernel used in that paper was the linear kernel (which is order-invariant), and the initial labels were antisymmetric, we conclude that Theorem 4 (b) provides a justification for the observed antisymmetry of the classifier, which was obtained in (Dardard et al., 2016).", "startOffset": 256, "endOffset": 278}, {"referenceID": 20, "context": "The goal is to show how one can construct a diffusion kernel (Kondor and Lafferty, 2002), (Shawe-Taylor and Cristianini, 2004, Section 9.", "startOffset": 61, "endOffset": 88}, {"referenceID": 19, "context": "For the symmetric case, taking the hint from the invariance framework of (Kir\u00e1ly et al., 2014), the first part of the analysis specializes the one made in (Brunner et al.", "startOffset": 73, "endOffset": 94}, {"referenceID": 1, "context": ", 2014), the first part of the analysis specializes the one made in (Brunner et al., 2012) to a particular transformation of feature vectors associated with a pair of objects, when one exchanges their order.", "startOffset": 68, "endOffset": 90}, {"referenceID": 1, "context": "Moreover, we have also detailed an extension to the antisymmetric case, which was not investigated in (Brunner et al., 2012).", "startOffset": 102, "endOffset": 124}, {"referenceID": 18, "context": "Additionally, we have also investigated how a classical algorithm from the literature (one version of the SMO algorithm (Keerthi et al., 2001)) is able to generate a sequence of suboptimal solutions having the same symmetry/antisymmetry properties.", "startOffset": 120, "endOffset": 142}, {"referenceID": 16, "context": "The results contribute to fill a current gap in the literature about kernel methods, providing an additional theoretical investigation (besides the ones provided in (Herbrich et al., 1998), (Brunner et al.", "startOffset": 165, "endOffset": 188}, {"referenceID": 1, "context": ", 1998), (Brunner et al., 2012), (Kir\u00e1ly et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 19, "context": ", 2012), (Kir\u00e1ly et al., 2014) and the other references cited in the Introduction) of symmetry and antisymmetry properties of the optimal solutions to machine learning problems modeled by kernel methods.", "startOffset": 9, "endOffset": 30}, {"referenceID": 16, "context": "2012) and (Herbrich et al., 1998) for the two cases, respectively.", "startOffset": 10, "endOffset": 33}, {"referenceID": 16, "context": "2) deals with a face recognition problem for which the symmetry property arises naturally (the goal therein is to recognize whether two face images belong to the same person or not), while (Herbrich et al., 1998) considers the case in which one wants to learn an order among objects through supervised examples, which naturally leads to the antisymmetric property (this learning framework has applications, e.", "startOffset": 189, "endOffset": 212}, {"referenceID": 6, "context": "We also refer to the recent work (Dardard et al., 2016) for an application with real motion capture data (incidentally, the antisymmetry property empirically observed when solving numerically the binary classification problem studied in (Dardard et al.", "startOffset": 33, "endOffset": 55}, {"referenceID": 6, "context": ", 2016) for an application with real motion capture data (incidentally, the antisymmetry property empirically observed when solving numerically the binary classification problem studied in (Dardard et al., 2016)22 was the source of inspiration for the theoretical investigation of the antisymmetry constraint made in the present work23).", "startOffset": 189, "endOffset": 211}, {"referenceID": 6, "context": "In more details, the application considered in (Dardard et al., 2016) concerns the analysis of leading interactions in a group of individuals (specifically, musicians in a string quartet).", "startOffset": 47, "endOffset": 69}, {"referenceID": 6, "context": "In the specific case considered in (Dardard et al., 2016), the weigths of the ordered arcs are obtained by training, as a binary classifier, a transductive SVM with linear kernel (Sindhwani and Keerthi, 2006), whose primal problem has some similarities with the l1-soft margin binary SVM classifier.", "startOffset": 35, "endOffset": 57}, {"referenceID": 28, "context": ", 2016), the weigths of the ordered arcs are obtained by training, as a binary classifier, a transductive SVM with linear kernel (Sindhwani and Keerthi, 2006), whose primal problem has some similarities with the l1-soft margin binary SVM classifier.", "startOffset": 129, "endOffset": 158}], "year": 2016, "abstractText": "A particularly interesting instance of supervised learning with kernels is when each training example is associated with two objects, as in pairwise classification (Brunner et al., 2012), and in supervised learning of preference relations (Herbrich et al., 1998). In these cases, one may want to embed additional prior knowledge into the optimization problem associated with the training of the learning machine, modeled, respectively, by the symmetry of its optimal solution with respect to an exchange of order between the two objects, and by its antisymmetry. Extending the approach proposed in (Brunner et al., 2012) (where the only symmetric case was considered), we show, focusing on support vector binary classification, how such embedding is possible through the choice of a suitable pairwise kernel, which takes as inputs the individual feature vectors and also the group feature vectors associated with the two objects. We also prove that the symmetry/antisymmetry constraints still hold when considering the sequence of suboptimal solutions generated by one version of the Sequential Minimal Optimization (SMO) algorithm, and we present numerical results supporting the theoretical findings. We conclude discussing extensions of the main results to support vector regression, to transductive support vector machines, and to several kinds of graph kernels, including diffusion kernels.", "creator": "LaTeX with hyperref package"}}}