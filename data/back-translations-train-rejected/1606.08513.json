{"id": "1606.08513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Jun-2016", "title": "SelQA: A New Benchmark for Selection-based Question Answering", "abstract": "This paper presents a new dataset to benchmark selection-based question answering. Our dataset contains contexts drawn from the ten most prevalent topics in the English Wikipedia. For the generation of a large, diverse, and challenging dataset, a new annotation scheme is proposed. Our annotation scheme involves a series of crowdsourcing tasks that can be easily followed by any researcher. Several systems are compared on the tasks of answer sentence selection and answer triggering, providing strong baseline results for future work to improve upon. We hope that providing a large corpus will enable researchers to work towards more effective open-domain question answering.", "histories": [["v1", "Mon, 27 Jun 2016 23:48:16 GMT  (291kb,D)", "https://arxiv.org/abs/1606.08513v1", null], ["v2", "Wed, 12 Oct 2016 16:36:02 GMT  (334kb,D)", "http://arxiv.org/abs/1606.08513v2", null], ["v3", "Fri, 28 Oct 2016 01:20:19 GMT  (335kb,D)", "http://arxiv.org/abs/1606.08513v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tomasz jurczyk", "michael zhai", "jinho d choi"], "accepted": false, "id": "1606.08513"}, "pdf": {"name": "1606.08513.pdf", "metadata": {"source": "CRF", "title": "SelQA: A New Benchmark for Selection-based Question Answering", "authors": ["Tomasz Jurczyk", "Michael Zhai", "Jinho D. Choi"], "emails": ["tomasz.jurczyk@emory.edu", "michael.zhai@emory.edu", "jinho.choi@emory.edu"], "sections": [{"heading": null, "text": "I. INTRODUCTIONSelection-based question answering is the task of selecting a segment of text, or interchangeably a context, from a provided set of contexts that best answers a posed question. Let's define a context as a single document section, a group of contiguous sentences, or a single sentence. Selection-based question answering is divided into answer sentence selection and answer trigasing. Answer-selection is defined as a ranking sentence that answers a question that is higher than the irrelevant sentences in which there is at least a single sentence answering the question in a set of candidate sets. Answer-triggering is defined as selecting any number (n > = 0) of sentences from a set of candidate sets that may or may not contain sentences that answer the question. Several corporations were created for these tasks [1], [2], [3] which allow researchers to answer effective questions by developing systems to answer questions that may or may not contain sentences that answer the question [5], [6] which improve understanding of the understanding of the question."}, {"heading": "II. RELATED WORK", "text": "The TREC-QA contest datasets were a popular choice for evaluation of answer set selections.2 [1] combined the TREC [8-12] datasets for training and shared the TREC-13 dataset for development and evaluation. This dataset, known as QASent, was used as the default benchmark for selecting answer sets, although it is rather small (277 questions with manually selected answer contexts).2 Introduced a single dataset, WikiQA, consisting of questions collected from the Bing search engine user logs. Our corpus is similar to WikiQA, but covers more diverse topics, consists of a larger number of questions (about 6 times larger for selecting answer sets and 2.5 times larger for triggering answers), and uses more contexts by extracting contexts from the entire article rather than just the abstract dataset [3], including a distributed data set in insurance domains."}, {"heading": "III. CORPUS", "text": "Our annotation scheme provides each researcher with a framework to create a large, diverse, pragmatic, and challenging dataset for selecting and triggering responses, while keeping crowdsourcing costs low."}, {"heading": "A. Data Collection", "text": "A total of 486 articles are consistently sampled from the following 10 topics in Wikipedia published on 8 August 2014: art, country, food, historical events, movies, music, science, sports, travel, television. These are the most common topics categorised by DBPedia.3 The original data is pre-processed into smaller chunks. First, each article is dumped based on the original iteration.4 Each section is segmented by the open source toolkit NLP4J.5. In our corpus, documents refer to individual sections in the Wikipedia articles. 3http: / / dbpedia.org 4https: / dumps.wikimedia.org / enwiki 5https: / / github.com / emorynlp / nlp4j"}, {"heading": "B. Annotation Scheme", "text": "In fact, most of them will be able to abide by the rules that they have imposed on themselves, and that they will be able to abide by the rules that they have imposed on themselves. (...) Most of them are able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...) Most of them are not able to abide by the rules. (...)"}, {"heading": "C. Corpus Analysis", "text": "The entire note took about 130 hours and cost a total of $770; each Turkish job took, on average, about a minute and cost about $10. A total of 7,904 questions were generated from Papers 1-4, of which 92.2% found their answers in individual sentences. It is clear that Paper 3 was effective in reducing the percentage of overlapping words between question and answer pairs (about 4%; f. Table III). Questions from Paper 3 can also be used to develop paraphrasing models. Several pilot studies on different tasks were conducted to analyze quality and cost; Papers 1-4 proved to be the most effective in the pilot studies. Subsequently [18] we paid incentives to those who submitted outstanding work that improved the overall quality of our annotations.Our corpus could be compared with WikiQA, which was created with the intention of providing a challenging dataset for answering selection-based questions."}, {"heading": "IV. SYSTEMS", "text": "Two models using revolutionary neural networks are being developed, one is our replication of the best model in [2], and the other is an improved model using sub-tree matching (Section IV-A), two more models are being developed using recurrent neural networks, one is our replication of the model of attentive pooling in [17], and the other is a simpler model with one-sided attention (Section IV-B)."}, {"heading": "A. Convolutional Neural Networks", "text": "The image consists of lines, each representing consecutive words in two sentences, the question (q) and the answer candidate (a), in which the words are represented by their embedding (19). In our experiments, we use the image of 80 lines (40 for question and answer). If one of the questions or answers is longer than 40 lines, the rest of the input is cut off. Next, the maximum pooling function is applied, 7 and the sentence vectors for q and a are generated. Unlike [2], which the dot product performed between these two vectors, we have added another hidden layer to learn their weights. Finally, the sigmoid activation function is applied and the entire network is trained using binary cross-entropy."}, {"heading": "B. Recurrent Neural Network", "text": "Our RNN model is based on the bidirectional Long ShortTerm Memory (LSTM), which draws attention to itself (LSTM), except for the fact that our network uses a gated recurrent unit (GRU; [21]) instead of LSTM. From our preliminary experiments, we have found that GRU converges faster than LSTM, while achieving similar performance for these tasks. Let's wqi q, waj, where q is the question and a is the answer candidate, and e (w) returns the embedding of a word w. Embedding is encoded by a single bidirectional GRU g, which consists of the forward (\u2212 g) and backward (\u2012 g) GRUs, each with h hidden units."}, {"heading": "V. EXPERIMENTS", "text": "Our systems are evaluated for the selection of answer sets (Section V-B) and tasks that trigger answers (Section V-C), both on WikiQA and on our corpus."}, {"heading": "A. SelQA: Selection-based QA Corpus", "text": "Table IV shows the distributions of our corpus, the so-called SelQA. Our corpus is divided into training (70%), development (10%) and evaluation rates (20%). Due to the additional sections of Chapter 5 (Section III-B), the data triggering the response (AT) are significantly larger than the selection data of the response rates (ASD)."}, {"heading": "B. Answer Sentence Selection", "text": "It turns out that it is a purely mental game, in which it is a matter of giving people the opportunity to develop, and that it is a purely mental game."}, {"heading": "C. Answer Triggering", "text": "In fact, it is the case that most of us are able to abide by the rules that they have imposed on themselves. (...) It is not the case that they are able to understand the rules that they have imposed on themselves. (...) It is not the case that they are able to understand the rules. (...) It is not the case that they are able to understand the rules of the rules that they have imposed on themselves. (...) It is not the case that they are able to understand the rules. (...) It is as if they are able to break the rules of the rules that break the rules. \"(...). (...) It is not as if they are able to observe the rules, to observe the rules.\" (...) It is not as if they are able to observe the rules. \"(...)"}, {"heading": "VI. CONCLUSION", "text": "In this paper, we present a new benchmark for two important tasks for answering questions: selecting answer sets and triggering answers. To analyze our corpus, several systems have been developed that use neural networks. Our analysis shows various aspects of current QA approaches that are beneficial for further improvement.Researchers working on relatively small datasets show useful characteristics of the question answer tasks. Techniques that lead to improvements in smaller datasets are often significantly reduced with larger datasets.Current hardware trends and the availability of larger datasets make answering questions on a large scale accessible. We plan to continue our work on providing large corpora for answering questions in open domains."}, {"heading": "ACKNOWLEDGEMENT", "text": "We appreciate the support of Infosys Ltd. All content in this material is that of the authors and does not necessarily reflect the views of Infosys Ltd."}], "references": [{"title": "What is the Jeopardy Model? A Quasi-Synchronous Grammar for QA", "author": ["M. Wang", "N.A. Smith", "T. Mitamura"], "venue": "Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, ser. EMNLP-CoNLL\u201907, 2007, pp. 22\u201332.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "WIKIQA: A Challenge Dataset for Open-Domain Question Answering", "author": ["Y. Yang", "W.-t. Yih", "C. Meek"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP\u201915, 2015, pp. 2013\u20132018.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Applying Deep Learning to Answer Selection: A Study and An Open Task", "author": ["M. Feng", "B. Xiang", "M.R. Glass", "L. Wang", "B. Zhou"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding, 2015, pp. 813\u2013820.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep Learning for Answer Sentence Selection", "author": ["L. Yu", "K.M. Hermann", "P. Blunsom", "S. Pulman"], "venue": "Proceedings of the NIPS Deep Learning Workshop, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A Long Short-Term Memory Model for Answer Sentence Selection in Question Answering", "author": ["D. Wang", "E. Nyberg"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, ser. ACL\u201915, 2015, pp. 707\u2013712.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to Rank Short Text Pairs with Convolutional Deep Neural Networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval, ser. SIGIR \u201915, 2015, pp. 373\u2013382.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning for answer sentence selection", "author": ["L. Yu", "K.M. Hermann", "P. Blunsom", "S. Pulman"], "venue": "arXiv preprint arXiv:1412.1632, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J. Boyd-Graber", "L. Claudino", "R. Socher", "H. Daum\u00e9 III"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2014, pp. 633\u2013644.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Question answering over freebase with multi-column convolutional neural networks", "author": ["L. Dong", "F. Wei", "M. Zhou", "K. Xu"], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, vol. 1, 2015, pp. 260\u2013269.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs", "author": ["W. Yin", "H. Sch\u00fctze", "B. Xiang", "B. Zhou"], "venue": "arXiv preprint arXiv:1512.05193, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic parsing for single-relation question answering", "author": ["W.-t. Yih", "X. He", "C. Meek"], "venue": "Proceedings of ACL, 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["P. Blunsom", "E. Grefenstette", "N. Kalchbrenner"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Tree Edit Models for Recognizing Textual Entailments, Paraphrases, and Answers to Questions", "author": ["M. Heilman", "N.A. Smith"], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, ser. HLT\u201910, 2010, pp. 1011\u20131019.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic Tree-Edit Models with Structured Latent Variables for Textual Entailment and Question Answering", "author": ["M. Wang", "C. Manning"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, ser. COLING\u201910, 2010, pp. 1164\u20131172.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic Feature Engineering for Answer Selection and Extraction", "author": ["A. Severyn", "A. Moschitti"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP\u201913, 2013, pp. 458\u2013467.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "LSTM-based Deep Learning Models for Non-factoid Answer Selection", "author": ["M. Tan", "B. Xiang", "B. Zhou"], "venue": "arXiv, vol. arXiv:1511.04108, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Attentive pooling networks", "author": ["C.N. d. Santos", "M. Tan", "B. Xiang", "B. Zhou"], "venue": "CoRR, vol. abs/1602.03609, 2016. [Online]. Available: http://arxiv.org/abs/1602.03609", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Incentivize High Quality Crowdwork", "author": ["C.-J. Ho", "A. Slivkins", "S. Suri", "J.W. Vaughan"], "venue": "Proceedings of the 24th World Wide Web Conference, ser. WWW\u201915, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Proceedings of Advances in Neural Information Processing Systems 26, ser. NIPS\u201913, 2013, pp. 3111\u20133119.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Transition-based Dependency Parsing with Selectional Branching", "author": ["J.D. Choi", "A. McCallum"], "venue": "Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ser. ACL\u201913, 2013, pp. 1052\u20131062.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing, ser. EMNLP\u201914, 2014, pp. 1724\u20131734.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Effective Approaches to Attention-based Neural Machine Translation", "author": ["T. Luong", "H. Pham", "C.D. Manning"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, ser. EMNLP\u201915, 2015, pp. 1412\u20131421.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural Variational Inference for Text Processing", "author": ["Y. Miao", "L. Yu", "P. Blunsom"], "venue": "arXiv, vol. arXiv:1511.06038, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "ABCNN: Attention-Based Convolutional Neural Network for Modeling Sentence Pairs", "author": ["W. Yin", "H. Sch\u00fctze", "B. Xiang", "B. Zhou"], "venue": "arXiv, vol. arXiv:1512.05193, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Sentence Similarity Learning by Lexical Decomposition and Composition", "author": ["Z. Wang", "H. Mi", "A. Ittycheriah"], "venue": "arXiv, vol. arXiv:1602.07019, 2016.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 50, "endOffset": 53}, {"referenceID": 1, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 55, "endOffset": 58}, {"referenceID": 2, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 60, "endOffset": 63}, {"referenceID": 3, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 132, "endOffset": 135}, {"referenceID": 4, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "Several corpora have been created for these tasks [1], [2], [3], allowing researchers to build effective question answering systems [4], [5], [6] with the aim of improving reading comprehension through understanding and reasoning of natural language.", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "In addition, our systems are evaluated on another dataset, WikiQA [2], for a fair comparison to previous work.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 8, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 9, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 10, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 11, "context": "2 [1] combined the TREC-[8-12] datasets for training and divided the TREC-13 dataset for development and evaluation.", "startOffset": 24, "endOffset": 30}, {"referenceID": 1, "context": "[2] introduced a lager dataset, WikiQA, consisting of questions collected from the user logs of the Bing search engine.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] distributed another dataset, InsuranceQA, including questions in the insurance", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] proposed a convolutional neural network with a", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 115, "endOffset": 118}, {"referenceID": 8, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 125, "endOffset": 129}, {"referenceID": 10, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 267, "endOffset": 271}, {"referenceID": 13, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 273, "endOffset": 277}, {"referenceID": 14, "context": "Further, more convolutional neural network based frameworks have been proposed as solutions for question answering [8], [9], [10], [11], [12] Our convolutional neural network model is inspired by the previous work utilizing the tree-edit distance and the tree kernel [13], [14], [15], although we introduce a different way of performing subtree matching facilitating word embeddings.", "startOffset": 279, "endOffset": 283}, {"referenceID": 15, "context": "Our recurrent neural network models with attention are based on established state-of-the-art systems for answer sentence selection [16], [17].", "startOffset": 131, "endOffset": 135}, {"referenceID": 16, "context": "Our recurrent neural network models with attention are based on established state-of-the-art systems for answer sentence selection [16], [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 17, "context": "Following [18], we paid incentives to those who submitted outstanding work, which improved the overall quality of our annotation.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "Our corpus could be compared to WikiQA that was created with the intent of providing a challenging dataset for selectionbased question answering [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "Two models using convolutional neural networks are developed, one is our replication of the best model in [2], and the other is an improved model using subtree matching (Section IV-A).", "startOffset": 106, "endOffset": 109}, {"referenceID": 16, "context": "Two more models using recurrent neural networks are developed, one is our replication of the attentive pooling model in [17], and the other is a simpler model using one-way attention (Section IV-B).", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "Our CNN model is motivated by [2].", "startOffset": 30, "endOffset": 33}, {"referenceID": 18, "context": "The image consists of rows standing for consecutive words in two sentences, the question (q) and the answer candidate (a), where the words are represented by their embeddings [19].", "startOffset": 175, "endOffset": 179}, {"referenceID": 1, "context": "Unlike [2] who performed the dot product between these two vectors, we added another hidden layer to learn their weights.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "7We also experimented with the average pooling as [2], which led to a marginally lower accuracy.", "startOffset": 50, "endOffset": 53}, {"referenceID": 0, "context": "append(fc(s q j , s a k)) end end Sdep[1]\u2190 Sdep[1] + fm(vals) C i \u2190getChildren(D q i ) C i \u2190getChildren(D i ) vals\u2190 [] foreach child cqj in C q i do foreach child ck in C i do vals.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "append(fc(s q j , s a k)) end end Sdep[1]\u2190 Sdep[1] + fm(vals) C i \u2190getChildren(D q i ) C i \u2190getChildren(D i ) vals\u2190 [] foreach child cqj in C q i do foreach child ck in C i do vals.", "startOffset": 47, "endOffset": 50}, {"referenceID": 1, "context": "append(fc(c q j , c a k)) end end Sdep[2]\u2190 Sdep[2] + fm(vals) end Algorithm 1: Algorithm of our subtree matching mechanism", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "append(fc(c q j , c a k)) end end Sdep[2]\u2190 Sdep[2] + fm(vals) end Algorithm 1: Algorithm of our subtree matching mechanism", "startOffset": 47, "endOffset": 50}, {"referenceID": 19, "context": "All sentences are automatically parsed by the NLP4J dependency parser [20].", "startOffset": 70, "endOffset": 74}, {"referenceID": 16, "context": "Our RNN model is based on the bidirectional Long ShortTerm Memory (LSTM) using attentive pooling introduced by [17], except that our network uses a gated recurrent unit (GRU; [21]) instead of LSTM.", "startOffset": 111, "endOffset": 115}, {"referenceID": 20, "context": "Our RNN model is based on the bidirectional Long ShortTerm Memory (LSTM) using attentive pooling introduced by [17], except that our network uses a gated recurrent unit (GRU; [21]) instead of LSTM.", "startOffset": 175, "endOffset": 179}, {"referenceID": 21, "context": "[22].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We did not use the one-way attention from [16] to avoid deviating the attention mechanism significantly.", "startOffset": 42, "endOffset": 46}, {"referenceID": 1, "context": "CNN0 is our replication of the best model in [2].", "startOffset": 45, "endOffset": 48}, {"referenceID": 1, "context": "[2] - 65.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17] - 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23] - 68.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] - 69.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[25] - 70.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "Thus, the F1-score on the question level was proposed by [2] as the evaluation for this task, which we follow.", "startOffset": 57, "endOffset": 60}, {"referenceID": 1, "context": "[2] - - - 27.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "8 RNN1 showed a very similar result to [2], which was surprising since it performed so much better for answer sentence selection.", "startOffset": 39, "endOffset": 42}, {"referenceID": 1, "context": "On the other hand, RNN1 shows a similar score to [2] as it does on WikiQA.", "startOffset": 49, "endOffset": 52}], "year": 2016, "abstractText": "This paper presents a new selection-based question answering dataset, SelQA. The dataset consists of questions generated through crowdsourcing and sentence length answers that are drawn from the ten most prevalent topics in the English Wikipedia. We introduce a corpus annotation scheme that enhances the generation of large, diverse, and challenging datasets by explictly aiming to reduce word co-occurrences between the question and answers. Our annotation scheme is composed of a series of crowdsourcing tasks with a view to more effectively utilize crowdsourcing in the creation of question answering datasets in various domains. Several systems are compared on the tasks of answer sentence selection and answer triggering, providing strong baseline results for future work to improve upon.", "creator": "LaTeX with hyperref package"}}}