{"id": "1606.08733", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Recurrent Neural Networks for Dialogue State Tracking", "abstract": "This paper discusses models for dialogue state tracking using recurrent neural networks (RNN). We present experiments on the standard dialogue state tracking (DST) dataset, DSTC2 [6]. On the one hand, RNN models became state of the art in DST, on the other hand, most state-of-the-art models are only turn-based and require dataset-specific preprocessing (e.g. DSTC2-specific) in order to achieve state-of-the-art results. We implemented two architectures which can be used in incremental settings and require almost no preprocessing. We compare their performance to the benchmarks on DSTC2 and discuss their properties. With only trivial preprocessing, the performance of our models is close to the state-of-the-art results.", "histories": [["v1", "Tue, 28 Jun 2016 14:33:29 GMT  (48kb,D)", "https://arxiv.org/abs/1606.08733v1", "preprint"], ["v2", "Wed, 13 Jul 2016 21:42:58 GMT  (49kb,D)", "http://arxiv.org/abs/1606.08733v2", "Accepted to slo-nlp 2016"]], "COMMENTS": "preprint", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ond\\v{r}ej pl\\'atek", "petr b\\v{e}lohl\\'avek", "vojt\\v{e}ch hude\\v{c}ek", "filip jur\\v{c}\\'i\\v{c}ek"], "accepted": false, "id": "1606.08733"}, "pdf": {"name": "1606.08733.pdf", "metadata": {"source": "CRF", "title": "Recurrent Neural Networks for Dialogue State Tracking", "authors": ["Ond\u0159ej Pl\u00e1tek", "Petr B\u011blohl\u00e1vek", "Vojt\u011bch Hude\u010dek", "Filip Jur\u010d\u00ed\u010dek"], "emails": ["oplatek@ufal.mff.cuni.cz,", "jurcicek@ufal.mff.cuni.cz,", "me@petrbel.cz,", "vojta.hudecek@gmail.com,"], "sections": [{"heading": "1 Introduction", "text": "These agents play the role of domain experts in a narrow area, and users ask for information through conversations in natural language (see the example system and user reactions in Figure 1).A State Tracker dialog summarizes the dialog history and maintains a probability distribution over the (possible) user objectives (see note in Figure 1).The dialog agents decide on the next steps based on the State Distribution dialog by the tracker. The user's objectives are expressed in a formal language that is typically presented as dialogue points (see Section 2) and the tracker updates the probability for each element. The State dialog is latently variable [20] and one must mark the conversations to train the State Tracker dialog."}, {"heading": "2 Dialogue state tracking on DSTC2 dataset", "text": "In the DSTC2 dataset, history is captured by dialog files and their probabilities. A dialog actpoint is a triple of the following form (actionType, slotName, slotValue).The DSTC2 dataset is a standard DST dataset, and most state-of-the-art systems in DST have their pro-2The slots Requested and Method have accuracies of 0.95 and 0.95 on the state-of-the-art test set [19].ar Xiv: 160 6.08 733v 2 [cs.C L] 13 July 2 016formance on this dataset [7]. The full dataset has been freely available since January 2014 and contains 1612 dialogs in the training database, 506 dialogs in the development database, and 1117 dialogs in the test database.3 The conversations are expressed covertly at the restaurant level, which are hidden at the restaurant level."}, {"heading": "3 Models", "text": "The models are all based on an RNN encoder [17]. The models update their hidden states after processing each word similar to the \u017dilka and Jurc RNN encoder, which represents the entire dialog history up to the current word. We use a Gated Recurrent Unit (GRU) as an update function instead of a simple RNN cell because it does not suffer from the disappearing gradient problem. [10] The model optimizes its parameters, including word embedding."}, {"heading": "3.1 Independent classifiers for each label", "text": "The independent model (see Figure 3.1) consists of three models that independently predict state, area, and price range based on the last hidden hT. Independent slot prediction, which uses one classifier per slot, is easy to implement, but the model introduces an unrealistic assumption of uncorrelated slot properties. In the case of DSTC2 and the Cambridge restaurant domain, for example, it is hard to believe that the slot area and price range are not correlated. We also experimented with a single classifier that predicts the labels together (see Figure 3), but it suffers from the data sparseness of the predicted tuples, so we focused only on independent label prediction and encoder decoder models."}, {"heading": "3.2 Encoder-decoder framework", "text": "We throw out the slot prediction problem as a sequence-to-equation prediction task and use an encoder decoder model with attention [2] to learn this representation along with slot predictions (see Figure 4). To our knowledge, we are the first to use this model for tracking the dialog state. [2] The model is successfully used in machine translation, where it is able to handle long sequences with good accuracy [2]. In DST, it easily captures the correlation between the decrypted slots. By introducing the encoder decoder architecture, we aim to overcome the problem of data sparity and false assumptions of independence. XT Slot1 Slot2 Slot3 EOSWe use the encoder sequence RNN cell, which captures the history of the dialog, which is presented as a sequence of words from the user and the system. The words are introduced to the encode model as the user responds to the dialog, where they interact with the system."}, {"heading": "4 Experiments", "text": "The results are reported on the basis of the standard DSTC2 data division, in which we used 516 dialogs as validation set for early stopping [14] and the remaining 1612 dialogs for training. We use 1-best automatic speech recognition (ASR) transcriptions of the conversation history of the input and measure the accuracy of the articulated slots. The models are evaluated on the basis of the recommended measurement accuracy [7] with Scheme 2, which skips the first turns in which the faith tracker does not track values. In addition, our models are also evaluated on the basis of a randomly distributed DSTC2 data set (see Section 4.3). For all our experiments we train word embedding of size 100 and use the encoder state size 100, together with an dropout we maintain a probability of 0.7 for both encoder inputs and outputs. These parameters are selected by a grid search using the hyperparameters of the development data."}, {"heading": "4.1 Training", "text": "The training sequence minimizes the cross-entropy loss function with the help of the Adam Optimizer [12] with a lot size of 10. We train the prediction of the target slot values for each lap. We treat each dialog round as a separate training example by feeding the entire dialog history up to the current lap into the encoder and predicting the slot labels for the current lap. We use early pausing with patience [14], validating the development determined by each epoch and stopping when the three top models do not change for four epochs. The predicted labels in the DST task depend not only on the last lap, but also on the entire dialog history. Since the length of the dialog history varies many times and we stack our inputs, we have divided the dialogs into ten buckets according to their length to ensure computational acceleration."}, {"heading": "4.2 Comparing models", "text": "The common prediction of the labels is quite a challenge, as the distribution of the labels is distorted, as shown in Figure 5. Some of the label combinations are very rare, and they occur only in development and in the test set, so the common model is not able to predict them. In initial informal experiments, the common model performed poorly due to the scarcity of data from slot triples. Furthermore, we focus on a model with independent classifiers and encoder architectures.The model with independent label prediction is a strong baseline, which was used in the work of \u017dilka and Jurc Picek, among others. [16] The model suffers less from dataset7The maximum length of the dialogue history is 487 words and 95% percentiles is 205 words for the training set.8The prediction has been conditioned to the full history, but we have propagated the error only in words within the last round."}, {"heading": "4.3 Data preparation experiments", "text": "The data for the DSTC2 test set were collected using a different configuration of the spoken dialog system than the data for the validation and training set. [7] We wanted to investigate the influence of the complexity of the task, so we combined all the DSTC2 data and created splits of 80%, 10% and 10% for the training, development and testing sets. The results in Table 2 show that the complexity of the task has significantly decreased. 9We could have modified the decoder so that three symbols would always have been predicted for our three slots, but our experiments showed that the encoder decoder architecture does not make mistakes in predicting the sequence of the three slots and the EOS symbol. 10The best model weights were found after 18 to 23 epochs for all model architectures."}, {"heading": "5 Related work", "text": "Since there are numerous systems that reported on the DSTC2 dataset, we will only discuss the systems that use RNNs. Generally speaking, the RNN systems achieved excellent results. Our system is related to the \u017dilka and Jurc H\u00edc H\u00ed\u010dek RNN tracker [16], which reported almost state-of-the-art results on the DSTC2 dataset and introduced the first incremental system that was able to update the dialogue word-for-word with such accuracy. In contrast to the work of \u017dilka and Jurc H\u00edc H\u00ed\u010d\u00ed\u010dek [16], we do not use an abstraction of slot values. Instead, we add the additional features as described in Section 3. The first system that used a neural for dialogue state tracking [6] used a feed-forward network and more than 10 manually constructed features across different levels of abstraction of the user input system, including language-specific outputs."}, {"heading": "6 Conclusion", "text": "To our knowledge, we are the first to use an encoder decoder model for the dialog-tracking task, and we encourage others to do so because it competes with the standard RNN model.11 The models presented are comparable to the state-of-the-art models. We evaluate the models based on the DSTC2 dataset, which contains task-oriented dialogs in the restaurant domain.The experiments presented are published under https: / / github. com / oplatek / e2end / under Apache license.Informal experiments were conducted during the Charles University Statistical Dialogue Systems Course (see http: / / github.com / oplatek / sds-tracker).The models are only practiced using the ASR 1-best transcriptions and task-specific lexical features defined by the task database. We note that the state-tracking task is much easier to determine than the DC2 task."}], "references": [{"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "A Neural probabilistic language model", "author": ["Yoshua Bengio", "Rejean Ducharme", "Pascal Vincent"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Learning End-to-End Goal-Oriented Dialog", "author": ["Antoine Bordes", "Jason Weston"], "venue": "arXiv preprint arXiv:1605.07683,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "CoRR, abs/1406.1078,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Deep neural network approach for the dialog state tracking challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young"], "venue": "Proceedings of the SIG- DIAL 2013 Conference,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "The second dialog state tracking challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Jason Williams"], "venue": "In 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "The third dialog state tracking challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Jason D Williams"], "venue": "In Spoken Language Technology Workshop (SLT), 2014 IEEE,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Word-based dialog state tracking with recurrent neural networks", "author": ["Matthew Henderson", "Blaise Thomson", "Steve Young"], "venue": "In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Reinforcement learning for parameter estimation in statistical spoken dialogue systems", "author": ["Filip Jur\u010d\u00ed\u010dek", "Blaise Thomson", "Steve Young"], "venue": "Computer Speech & Language,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Dialog History Construction with Long-Short Term Memory for Robust Generative Dialog State Tracking", "author": ["Byung-Jun Lee", "Kee-Eung Kim"], "venue": "Dialogue & Discourse,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Early stopping-but when? In Neural Networks: Tricks of the trade, pages 55\u201369", "author": ["Lutz Prechelt"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1998}, {"title": "Hybrid Dialog State Tracker", "author": ["Miroslav Vodol\u00e1n", "Rudolf Kadlec", "Jan Kleindienst"], "venue": "CoRR, abs/1510.03710,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Incremental LSTM-based dialog state tracker", "author": ["Luk\u00e1s \u017dilka", "Filip Jur\u010d\u00ed\u010dek"], "venue": "arXiv preprint arXiv:1507.03471,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Paul J Werbos"], "venue": "Proceedings of the IEEE,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1990}, {"title": "The dialog state tracking challenge", "author": ["Jason Williams", "Antoine Raux", "Deepak Ramachandran", "Alan Black"], "venue": "In Proceedings of the SIGDIAL 2013 Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Web-style ranking and SLU combination for dialog state tracking", "author": ["Jason D Williams"], "venue": "In 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "The hidden information state model: A practical framework for POMDP-based spoken dialogue management", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Simon Keizer", "Fran\u00e7ois Mairesse", "Jost Schatzmann", "Blaise Thomson", "Kai Yu"], "venue": "Computer Speech & Language,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "We present experiments on the standard dialogue state tracking (DST) dataset, DSTC2 [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 17, "context": "Dialogue state tracking (DST) is a standard and important task for evaluating task-oriented conversational agents [18, 7, 8].", "startOffset": 114, "endOffset": 124}, {"referenceID": 6, "context": "Dialogue state tracking (DST) is a standard and important task for evaluating task-oriented conversational agents [18, 7, 8].", "startOffset": 114, "endOffset": 124}, {"referenceID": 7, "context": "Dialogue state tracking (DST) is a standard and important task for evaluating task-oriented conversational agents [18, 7, 8].", "startOffset": 114, "endOffset": 124}, {"referenceID": 19, "context": "Dialogue agents as introduced in [20] decide about the next action based on the dialogue state distribution given by the tracker.", "startOffset": 33, "endOffset": 37}, {"referenceID": 19, "context": "The dialogue state is a latent variable [20] and one needs to label the conversations in order to train a dialogue state tracker using supervised learning.", "startOffset": 40, "endOffset": 44}, {"referenceID": 10, "context": "It was shown that with a better dialogue state tracker, conversation agents achieve better success rate in overall completion of the their task [11].", "startOffset": 144, "endOffset": 148}, {"referenceID": 6, "context": "We also experiment with re-splitting of the DSTC2 data because there are considerable differences between the standard train and test datasets [7].", "startOffset": 143, "endOffset": 146}, {"referenceID": 18, "context": "95 on the test set according to the state-of-the-art [19].", "startOffset": 53, "endOffset": 57}, {"referenceID": 6, "context": "formance on this dataset [7].", "startOffset": 25, "endOffset": 28}, {"referenceID": 16, "context": "Our models are all based on a RNN encoder [17].", "startOffset": 42, "endOffset": 46}, {"referenceID": 15, "context": "The models update their hidden states h after processing each word similarly to the RNN encoder of \u017dilka and Jur\u010d\u00ed\u010dek [16].", "startOffset": 118, "endOffset": 122}, {"referenceID": 4, "context": "We use a Gated Recurrent Unit (GRU) cell [5] as the update function instead of a simple RNN cell because it does not suffer from the vanishing gradient problem [10].", "startOffset": 41, "endOffset": 44}, {"referenceID": 9, "context": "We use a Gated Recurrent Unit (GRU) cell [5] as the update function instead of a simple RNN cell because it does not suffer from the vanishing gradient problem [10].", "startOffset": 160, "endOffset": 164}, {"referenceID": 2, "context": "The model optimizes its parameters including word embeddings [3] during training.", "startOffset": 61, "endOffset": 64}, {"referenceID": 0, "context": "The models were implemented using the TensorFlow [1] framework.", "startOffset": 49, "endOffset": 52}, {"referenceID": 6, "context": "5Accuracy measure with schedule 2 on slot f ood, area and pricerange about which users can inform the system is a featured metric for DSTC2 challenge [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 1, "context": "We cast the slot predictions problem as a sequence-tosequence predictions task and we use a encoder-decoder model with attention [2] to learn this representation together with slot predictions (see Figure 4).", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "The model is successfully used in machine translation where it is able to handle long sequences with good accuracy [2].", "startOffset": 115, "endOffset": 118}, {"referenceID": 13, "context": "The results are reported on the standard DSTC2 data split where we used 516 dialogues as a validation set for early stopping [14] and the remaining 1612 dialogues for train-", "startOffset": 125, "endOffset": 129}, {"referenceID": 6, "context": "The models are evaluated using the recommended measure accuracy [7] with schedule 2 which skips the first turns where the believe tracker does not track any values.", "startOffset": 64, "endOffset": 67}, {"referenceID": 11, "context": "The training procedure minimizes the cross-entropy loss function using the Adam optimizer [12] with a batch size of 10.", "startOffset": 90, "endOffset": 94}, {"referenceID": 13, "context": "We use early stopping with patience [14], validating on the development set after each epoch and stopping if the three top models does not change for four epochs.", "startOffset": 36, "endOffset": 40}, {"referenceID": 15, "context": "The model with independent label prediction is a strong baseline which was used, among others, in work of \u017dilka and Jur\u010d\u00ed\u010dek [16].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "[15] - 0.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "\u017dilka and Jur\u010d\u00ed\u010dek [16] 0.", "startOffset": 19, "endOffset": 23}, {"referenceID": 5, "context": "[6] - 0.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "DSTC2 stacking ensemble [7] - 0.", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "Our system is related to the RNN tracker of \u017dilka and Jur\u010d\u00ed\u010dek [16], which reported near state-of-the art results on the DSTC2 dataset and introduced the first incremental system which was able to update the dialogue state wordby-word with such accuracy.", "startOffset": 63, "endOffset": 67}, {"referenceID": 15, "context": "In contrast to work of \u017dilka and Jur\u010d\u00ed\u010dek [16], we use no abstraction of slot values.", "startOffset": 42, "endOffset": 46}, {"referenceID": 5, "context": "The first system which used a neural network for dialogue state tracking [6] used a feed-forward network and more than 10 manually engineered features across different levels of abstraction of the user input, including the outputs of the spoken language understanding component (SLU).", "startOffset": 73, "endOffset": 76}, {"referenceID": 8, "context": "[9] achieves the stateof-the-art results and, similarly to our system, it predicts the dialogue state from words by employing a RNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Another dialogue state tracker with LSTM was used in the reinforcement setting but the authors also used information from the SLU pipeline [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 14, "context": "[15], who combine a rule-based and a machine learning based approach.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "It is worth noting that there are first attempts to train an end-to-end dialogue system even without explicitly modeling the dialogue state [4], which further simplifies the architecture of a dialogue system.", "startOffset": 140, "endOffset": 143}], "year": 2016, "abstractText": "This paper discusses models for dialogue state tracking using recurrent neural networks (RNN). We present experiments on the standard dialogue state tracking (DST) dataset, DSTC2 [7]. On the one hand, RNN models became the state of the art models in DST, on the other hand, most state-of-the-art DST models are only turn-based and require dataset-specific preprocessing (e.g. DSTC2-specific) in order to achieve such results. We implemented two architectures which can be used in an incremental setting and require almost no preprocessing. We compare their performance to the benchmarks on DSTC2 and discuss their properties. With only trivial preprocessing, the performance of our models is close to the state-ofthe-art results.1", "creator": "LaTeX with hyperref package"}}}