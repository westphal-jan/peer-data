{"id": "1606.08777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "\"Show me the cup\": Reference with Continuous Representations", "abstract": "One of the most basic functions of language is to refer to objects in a shared scene. Modeling reference with continuous representations is challenging because it requires individuation, i.e., tracking and distinguishing an arbitrary number of referents. We introduce a neural network model that, given a definite description and a set of objects represented by natural images, points to the intended object if the expression has a unique referent, or indicates a failure, if it does not. The model, directly trained on reference acts, is competitive with a pipeline manually engineered to perform the same task, both when referents are purely visual, and when they are characterized by a combination of visual and linguistic properties.", "histories": [["v1", "Tue, 28 Jun 2016 16:31:50 GMT  (6442kb,D)", "http://arxiv.org/abs/1606.08777v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["gemma boleda", "sebastian pad\\'o", "marco baroni"], "accepted": false, "id": "1606.08777"}, "pdf": {"name": "1606.08777.pdf", "metadata": {"source": "CRF", "title": "\u201cShow me the cup\u201d: Reference with Continuous Representations", "authors": ["Gemma Boleda", "Sebastian Pad\u00f3", "Marco Baroni"], "emails": ["firstname.lastname@unitn.it", "sebastian.pado@ims.uni-stuttgart.de"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is not as if it were an attempt to get a grip on the situation, but an attempt to get a grip on the situation. (1) It is not as if it were an attempt. (1) It is as if it were an attempt. (3) It is as if it were an attempt. (4) It is as if it were an attempt. (4) It is as if it were an attempt. (4) It is as if it were an attempt. (4) It is as if it were an attempt. (4) It is as if it were an attempt. (4) It is as if it were an attempt. (4) It is as if it were an attempt. (4) It is as if it were an attempt. (5) It is as if it were an attempt."}, {"heading": "2 Models", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before."}, {"heading": "3 Data", "text": "In fact, most of them will be able to play by the rules they have set themselves in order to play by the rules."}, {"heading": "4 Experiments", "text": "These models, as well as TRPoP, use 4096-dimensional vectors as input visual representations produced solely by passing images through the pre-formed VGG network. The same pre-formed network was used to generate the labels of our CNN competitor model. PoP / TRPoP and the maxmargin embeddings pipeline parameters are estimated by online stochastic gradients of the two datasets. For pipeline, we extract all possible pairs of positive and negative query object tuples from each reference to the relevant data."}, {"heading": "5 Related work", "text": "It's not like you're going to be able to behave in a certain way, like you've done in the past. It's not like you're going to behave in a certain way, like you did. It's like you're going to behave in a different way. It's like you're going to behave in a different way. It's like you're going to behave in a different way. It's like you're going to behave in a different way. It's like you're going to behave in a different way. It's like you're going to behave in a different way. It's like you're going to behave in a different way. It's like you're going to behave in a different way."}, {"heading": "6 Conclusion and outlook", "text": "PoP is a neural network model that, faced with a linguistic expression and a set of objects represented by natural images, either resolves references by pointing to the object designated by the expression, or marks the reference act as anomalous if the linguistic expression is not adequate. It consists of an integrated and generic architecture that can be trained directly on examples of successful and failed reference acts. It is competitive with a pipeline that has been manually developed to perform the same task. PoP successfully takes characterization into account because it is able to relate entity properties (visually and linguistically transmitted) to linguistic expressions through its distributed representations."}, {"heading": "Acknowledgments", "text": "We thank Elia Bruni for the CNN basic idea and Angeliki Lazaridou for providing the visual vectors used in the work. This project was funded under the European Union's Horizon 2020 research and innovation programme under Marie Sklodowska-Curie funding contract No. 655577 (LOVe), the ERC 2011 Starting Independent Research Grant No. 283554 (COMPOSES), the DFG (SFB 732, Project D10) and the Spanish MINECO (FFI201341301-P)."}, {"heading": "1 Data Creation for the Object-Only Dataset (Experiment 1)", "text": "We start with an empty sequence and take the length of the sequence evenly at random from the permitted sequence lengths (l. 2). We fill the sequence with randomly captured objects and images (l. 4 / 5). Without losing generality, we assume that the object we ask for is q (l. 6). Then we check whether the current sequence should be an anomaly (l. 7). If it should be an anomaly (i.e. no match for the query), we overwrite the target object and the image with a new random move from the pool (l. 9 / 10). If we decide to turn it into a multipleanomaly (i.e., with multiple matches for the query), we randomly select a different position in the sequence and overwrite it with the query object and a new image (l. 12 / 13) so that the sequence is assigned."}, {"heading": "2 Data Creation for the Object+Attribute Dataset (Experiment 2)", "text": "Figure 1 shows the intuition for scanning the object + attribute dataset. Arrows indicate compatibility limitations during scanning. We start with the query pair (object 1 - attribute 1), then sample two additional attributes, both compatible with object algorithm 1. Creation of the object dataset. Input: sequence length of query interval [i \u2265 2, j]; sequence of objectsO = {o1,.., to} and sequence of associated images I (o) for each object o; probability of missing anomalies P0; probability of multiple anomalies Pm.Output: < object query q, object image sequence S > 1: S attribute I (i, j) 3: for k = 1 to l: Figure 4 (l)."}, {"heading": "3 Statistics on the Datasets", "text": "Table 1 shows statistics on the data set. < The first row covers the object-only data set < < < < < < < several objects in the test group are seen during the training, 23% of the images are not. Due to random sample creation, a minimum number of sequences are repeated (5 sequences occur twice in the test group, 1 four times) and are split between the candidates (1 sequence); all other sequences occur only once; the second row covers the object + attribute data set; the average frequencies for objects and object images reflect those in the object group fairly accurately; the new columns on object attribute (O + A) and object attribute image (O + A + I) show that object combinations occur relatively rarely (any object and many attributes)."}, {"heading": "4 Hyperparameter Tuning", "text": "We have set the following hyperparameters based on the object validation set and reused without further adjustment for object + attribute (with the exception of the thresholds for pipeline heuristics). The selected values are given in brackets. \u2022 PoP: Multimodal embedding size (300), anomaly sensor size (100), nonlinearity \u03d1 (relative) and \u03c6 (sigmoid), learning rate (0.09), number of epochs (14). \u2022 TRPoP: same settings, except number of epochs (36). \u2022 Pipeline: multimodal embedding size (300), margin size (0.5), learning rate (0.09), maximum similarity threshold (0.1 for object-only, 0.4 for object + attribute), threshold for two similarity differences (0.05 and 0.07). Momentum has been set to 0.09, learning rate to 1E-4 for all models based on informal preliminary experiments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR Conference Track,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Marco Baroni", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs. contextpredicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of ACL,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Grounding distributional semantics in the visual world", "author": ["Marco Baroni"], "venue": "Language and Linguistics Compass,", "citeRegEx": "Baroni.,? \\Q2016\\E", "shortCiteRegEx": "Baroni.", "year": 2016}, {"title": "Wide-coverage semantic representations from a CCG parser", "author": ["Bos et al.2004] Johan Bos", "Stephen Clark", "Mark Steedman", "James R. Curran", "Julia Hockenmaier"], "venue": "In Proceedings of the COLING,", "citeRegEx": "Bos et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bos et al\\.", "year": 2004}, {"title": "Concreteness ratings for 40 thousand generally known English word lemmas", "author": ["Amy Beth Warriner", "Victor Kuperman"], "venue": "Behavior Research Methods,", "citeRegEx": "Brysbaert et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Brysbaert et al\\.", "year": 2014}, {"title": "Image retrieval: ideas, influences, and trends of the new age", "author": ["Datta et al.2008] Ritendra Datta", "Dhiraj Joshi", "Jia Li", "James Wang"], "venue": "ACM Computing Surveys,", "citeRegEx": "Datta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Datta et al\\.", "year": 2008}, {"title": "DeViSE: A deep visual-semantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Greg Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Marc\u2019Aurelio Ranzato", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "Visual Turing test for computer vision systems", "author": ["Geman et al.2015] Donald Geman", "Stuart Geman", "Neil Hallonquist", "Laurent Younes"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Geman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Geman et al\\.", "year": 2015}, {"title": "Grounded semantic composition for visual scenes", "author": ["Gorniak", "Roy2004] Peter Gorniak", "Deb Roy"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Gorniak et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Gorniak et al\\.", "year": 2004}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Joulin", "Mikolov2015] Armand Joulin", "Tomas Mikolov"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Joulin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Joulin et al\\.", "year": 2015}, {"title": "ReferItGame: Referring to objects in photographs of natural scenes", "author": ["Vicente Ordonez", "Mark Matten", "Tamara Berg"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Kazemzadeh et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kazemzadeh et al\\.", "year": 2014}, {"title": "Simple learning and compositional application of perceptually grounded word meanings for incremental reference resolution", "author": ["Kennington", "Schlangen2015] Casey Kennington", "David Schlangen"], "venue": "In Proceedings of ACL,", "citeRegEx": "Kennington et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kennington et al\\.", "year": 2015}, {"title": "Computational generation of referring expressions: A survey", "author": ["Krahmer", "van Deemter2012] Emiel Krahmer", "Kees van Deemter"], "venue": "Computational Linguistics,", "citeRegEx": "Krahmer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krahmer et al\\.", "year": 2012}, {"title": "Formal semantics for perceptual classification", "author": ["Staffan Larsson"], "venue": "Journal of Logic and Computation,", "citeRegEx": "Larsson.,? \\Q2015\\E", "shortCiteRegEx": "Larsson.", "year": 2015}, {"title": "2015a. Hubness and pollution: Delving into cross-space mapping for zero-shot learning", "author": ["Georgiana Dinu", "Marco Baroni"], "venue": "In Proceedings of ACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of NAACL,", "citeRegEx": "Lazaridou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["Malinowski", "Fritz2014] Mateusz Malinowski", "Mario Fritz"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Malinowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2014}, {"title": "Generation and comprehension of unambiguous object descriptions", "author": ["Mao et al.2016] Junhua Mao", "Jonathan Huang", "Alexander Toshev", "Oana Camburu", "Alan Yuille", "Kevin Murphy"], "venue": "In Proceedings of CVPR,", "citeRegEx": "Mao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2016}, {"title": "Learning from unscripted deictic gesture and language for human-robot interactions", "author": ["Liefeng Bo", "Luke Zettlemoyer", "Dieter Fox"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Matuszek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Matuszek et al\\.", "year": 2014}, {"title": "Neural Networks and Deep Learning. Determination Press, New York. Published online: http:// neuralnetworksanddeeplearning.com", "author": ["Michael Nielsen"], "venue": null, "citeRegEx": "Nielsen.,? \\Q2015\\E", "shortCiteRegEx": "Nielsen.", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Simonyan", "Zisserman2015] Karen Simonyan", "Andrew Zisserman"], "venue": "In Proceedings of ICLR Conference Track,", "citeRegEx": "Simonyan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2015}, {"title": "Look, some green circles!\u201d", "author": ["Sandro Pezzelle", "Angeliki Lazaridou", "Aur\u00e9lie Herbelot", "Gemma Boleda", "Raffa Bernardi"], "venue": null, "citeRegEx": "Sorodoc et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sorodoc et al\\.", "year": 2016}, {"title": "Coordinating perceptually grounded categories through language: A case study for colour", "author": ["Steels", "Belpaeme2005] Luc Steels", "Tony Belpaeme"], "venue": "Behavioral and Brain Sciences,", "citeRegEx": "Steels et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Steels et al\\.", "year": 2005}, {"title": "Endto-end memory networks. http://arxiv.org/ abs/1503.08895", "author": ["Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Refer efficiently: Use less informative expressions for more predictable meanings", "author": ["Tily", "Piantadosi2009] Harry Tily", "Steven Piantadosi"], "venue": "In Proceedings of the CogSci Workshop on the Production of Referring Expressions,", "citeRegEx": "Tily et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tily et al\\.", "year": 2009}, {"title": "From Frequency to Meaning: Vector Space Models of Semantics", "author": ["Turney", "Pantel2010] Peter D Turney", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "WSABIE: Scaling up to large vocabulary image annotation", "author": ["Weston et al.2011] Jason Weston", "Samy Bengio", "Nicolas Usunier"], "venue": "In Proceedings of IJCAI,", "citeRegEx": "Weston et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2011}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Xu et al.2015] Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio"], "venue": "In Proceedings of ICML,", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "fundamental devices to ground linguistic symbols in extralinguistic reality (Harnad, 1990).", "startOffset": 76, "endOffset": 90}, {"referenceID": 4, "context": "The converse holds for logics-based approaches (Bos et al., 2004).", "startOffset": 47, "endOffset": 65}, {"referenceID": 21, "context": ", Nielsen (2015). possible referent (missing-referent anomaly) or if there is more than one (multiple-referent anomaly).", "startOffset": 2, "endOffset": 17}, {"referenceID": 7, "context": "This has been shown to produce excellent multimodal embeddings (Frome et al., 2013; Lazaridou et al., 2015a; Weston et al., 2011).", "startOffset": 63, "endOffset": 129}, {"referenceID": 29, "context": "This has been shown to produce excellent multimodal embeddings (Frome et al., 2013; Lazaridou et al., 2015a; Weston et al., 2011).", "startOffset": 63, "endOffset": 129}, {"referenceID": 16, "context": "The objects and images are sampled uniformly at random from a set of 2,000 objects and 50 ImageNet5 images per object, itself sampled from a larger dataset used in Lazaridou et al. (2015b). As the examples show, we use natural objects and images, which makes the task very challenging (even humans might wonder which image in the second row depicts", "startOffset": 164, "endOffset": 189}, {"referenceID": 5, "context": "We thus filter verbs through the concreteness norms of Brysbaert et al. (2014), retaining only those with a concreteness score of at most 2.", "startOffset": 55, "endOffset": 79}, {"referenceID": 1, "context": "Method PoP and Pipeline\u2019s input word representations are 400-dimensional cbow embeddings from Baroni et al. (2014), trained on about 2.", "startOffset": 94, "endOffset": 115}, {"referenceID": 15, "context": "Various studies in this area have proposed multimodal approaches jointly handling vision and language (Gorniak and Roy, 2004; Larsson, 2015; Matuszek et al., 2014; Steels and Belpaeme, 2005, a.o.). These papers focus on aspects of the resolution process we are not currently addressing, such as full compositionality or gesture, but they work with very limited perceptual input, such as simple shapes and colours. Probably the most relevant study in this area is the one by Kennington and Schlangen (2015). They consider visual scenes with more objects than our sequences, but more limited in nature (tables with 36 puzzle pieces).", "startOffset": 126, "endOffset": 506}, {"referenceID": 12, "context": "Some recent efforts collect and analyze large corpora of referring expressions for multimodal tasks (Kazemzadeh et al., 2014; Tily and Piantadosi, 2009).", "startOffset": 100, "endOffset": 152}, {"referenceID": 8, "context": "Our task is more distantly related to visual question answering (Geman et al., 2015; Malinowski and Fritz, 2014; Ren et al., 2015), in the sense that we model one specific type of question that could easily be asked about an image.", "startOffset": 64, "endOffset": 130}, {"referenceID": 22, "context": "Our task is more distantly related to visual question answering (Geman et al., 2015; Malinowski and Fritz, 2014; Ren et al., 2015), in the sense that we model one specific type of question that could easily be asked about an image.", "startOffset": 64, "endOffset": 130}, {"referenceID": 6, "context": "Our task can finally also be seen as a special case of the much broader problem of content-based image retrieval (Datta et al., 2008).", "startOffset": 113, "endOffset": 133}, {"referenceID": 3, "context": "See Baroni (2016) for a discussion of how the problem of reference is addressed in that line of work.", "startOffset": 4, "endOffset": 18}, {"referenceID": 24, "context": ", corresponding to all X or many X (Sorodoc et al., 2016).", "startOffset": 35, "endOffset": 57}], "year": 2016, "abstractText": "One of the most basic functions of language is to refer to objects in a shared scene. Modeling reference with continuous representations is challenging because it requires individuation, i.e., tracking and distinguishing an arbitrary number of referents. We introduce a neural network model that, given a definite description and a set of objects represented by natural images, points to the intended object if the expression has a unique referent, or indicates a failure, if it does not. The model, directly trained on reference acts, is competitive with a pipeline manually engineered to perform the same task, both when referents are purely visual, and when they are characterized by a combination of visual and linguistic properties.", "creator": "LaTeX with hyperref package"}}}