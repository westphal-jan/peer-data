{"id": "1606.08808", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Adaptive Training of Random Mapping for Data Quantization", "abstract": "Data quantization learns encoding results of data with certain requirements, and provides a broad perspective of many real-world applications to data handling. Nevertheless, the results of encoder is usually limited to multivariate inputs with the random mapping, and side information of binary codes are hardly to mostly depict the original data patterns as possible. In the literature, cosine based random quantization has attracted much attentions due to its intrinsic bounded results. Nevertheless, it usually suffers from the uncertain outputs, and information of original data fails to be fully preserved in the reduced codes. In this work, a novel binary embedding method, termed adaptive training quantization (ATQ), is proposed to learn the ideal transform of random encoder, where the limitation of cosine random mapping is tackled. As an adaptive learning idea, the reduced mapping is adaptively calculated with idea of data group, while the bias of random transform is to be improved to hold most matching information. Experimental results show that the proposed method is able to obtain outstanding performance compared with other random quantization methods.", "histories": [["v1", "Tue, 28 Jun 2016 18:15:32 GMT  (64kb)", "https://arxiv.org/abs/1606.08808v1", "6 pages, 5 figures, 15.8"], ["v2", "Fri, 26 May 2017 15:24:26 GMT  (66kb)", "http://arxiv.org/abs/1606.08808v2", "6 pages, 5 figures, 15.8"]], "COMMENTS": "6 pages, 5 figures, 15.8", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["miao cheng", "ah chung tsoi"], "accepted": false, "id": "1606.08808"}, "pdf": {"name": "1606.08808.pdf", "metadata": {"source": "CRF", "title": "Adaptive Training of Random Mapping for Data Quantization", "authors": ["Miao Cheng", "Ah Chung Tsoi"], "emails": ["cheng@outlook.com", "actsoi@must.edu.cn"], "sections": [{"heading": null, "text": "This year it is more than ever before."}, {"heading": "A. Adaptive Training of Linear Mapping", "text": "Regarding linear transformation in the preform of linear mapping, however, the handling of the first step is ignored, so that a smooth coding could occur. However, it is significant whether the bilateral groups can be preserved in the reduced characteristics as possible. \u2212 In the literature, the original problem was widely accepted to search for basic components of global data distribution, and is able to obtain the main information of the original data [3]. Furthermore, it is competent to learn the ideal data groups of characteristic structures as a data decomposition method, and informative contents of data models can be preserved. In order to improve the adaptive formation of the linear form, the 2-way groups based on objective function, the ideal mapping of the original form is elaborated."}, {"heading": "B. Adaptive Training of Offset", "text": "In this subset, the choice of the offset constant of linear transformation in cosmic random mapping is required to be inspired so that an appropriate rotation of the angular effects is possible to achieve acceptable results of binary embedding. In order to get the best knowledge from us, there is no existing work that performs such a topic, and a random constant is usually referred to as having a weak effect on final quantization as a normal concept. However, the randomly selected offset cannot correspond to the optimal projective direction with cycle curves in general, as shown in the figure. Since the original bias of cosmic function is determined by randomly selected offsets, the final hashing output can be placed on an unbalanced hyperplane as a result. Then, the resulting mapping can represent the projective values in the binary embedding as possible. In other words, the random biases are not able to make an optimal decision on the side information for the input."}], "references": [{"title": "Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions", "author": ["A. Andoni", "P. Indyk"], "venue": "Communications of The ACM, vol. 51, no. 1, pp. 117\u2013122, 2008.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 1, pp. 117\u2013128, 2011.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Eigenfaces for recognition", "author": ["M. Turk", "A. Pentland"], "venue": "Journal of Cognitive Neuroscience, vol. 3, no. 1, pp. 71\u201386, 1991.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni"], "venue": "Proceedings of the ACM Symposium on Computational Geometry, 2004.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2004}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["M.S. Charikar"], "venue": "STOC, 2002, pp. 380\u2013388.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "Neural Information Processing Systems, vol. 21, 2009, pp. 1753\u20131760.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "IEEE International Conference on Computer Vision and Pattern Recognition, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Neural Information Processing Systems, 2007.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Locality-sensitive binary codes from shift-invariant kernels", "author": ["M. Raginsky", "S. Lazebnik"], "venue": "Neural Information Processing Systems, 2009.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Randomized nonlinear component analysis", "author": ["D. Lopez-Paz", "S. Sra", "A.J. Smola", "Z. Ghahramani", "B. Sch\u00f6lkopf"], "venue": "Proceedings of International Conference on Machine Learning, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "K-means clustering via principal component analysis", "author": ["C. Ding", "X. He"], "venue": "Proceedings of International Conference on Machine Learning, 2004.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "Learning multi-view neighborhood preserving projections", "author": ["N. Quadrianto", "C.H. Lampert"], "venue": "Proceedings of International Conference on Machine Learning, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Nonnegative class-specific entropy component analysis with adaptive step search criterion", "author": ["M. Cheng", "C.-M. Pun", "Y.Y. Tang"], "venue": "Pattern Analysis and Applications, vol. 17, no. 1, pp. 113\u2013127, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Iterative Methods for Optimization, ser", "author": ["C.T. Kelley"], "venue": "Frountiers in Applied Mathematics. SIAM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "On the solution of large quadratic programming problems with bound constraints", "author": ["J.J. Mor\u00e9", "G. Toraldo"], "venue": "SIAM Journal of Optimization, vol. 1, no. 1, pp. 93\u2013113, 1991.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1991}, {"title": "Nonnegative matrix factorization with constrained second-order optimization", "author": ["R. Zdunek", "A. Cichocki"], "venue": "Signal Processing, vol. 87, no. 8, pp. 1904\u20131916, 2007.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1904}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky"], "venue": "University of Toronto, Tech. report, 2009.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, 1998, pp. 2278\u20132324.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Modeling the shape of the scene: a holistic representation of the spatial envelope", "author": ["A. Oliva", "A. Torralba"], "venue": "International Journal of Computer Vision, vol. 42, no. 3, pp. 145\u2013175, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, vol. 60, no. 2, pp. 91\u2013110, 2004.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "To fast retrieve required information, data quantization methods have been widely applied to find matched data with binary codes, and plays a key bridge between query and feedback in systems [1][2].", "startOffset": 191, "endOffset": 194}, {"referenceID": 1, "context": "To fast retrieve required information, data quantization methods have been widely applied to find matched data with binary codes, and plays a key bridge between query and feedback in systems [1][2].", "startOffset": 194, "endOffset": 197}, {"referenceID": 2, "context": "In the literature, many quantization method adopts wellknown principal component analysis (PCA) [3] to find the appropriate projections for binary embedding.", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "As a popular hashing method, locality sensitive hashing (LSH) [4][5][1] learns the low-dimensional representation of coming data with randomly generated projected directions.", "startOffset": 62, "endOffset": 65}, {"referenceID": 4, "context": "As a popular hashing method, locality sensitive hashing (LSH) [4][5][1] learns the low-dimensional representation of coming data with randomly generated projected directions.", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "As a popular hashing method, locality sensitive hashing (LSH) [4][5][1] learns the low-dimensional representation of coming data with randomly generated projected directions.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "Spectral Hashing (SH) [6] encodes input data into low-dimensional binary ones by preserving of local structures.", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "And the similar idea is also used to find the best quantization with iterative procedures in other methods [7].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "To address the binarization of arbitrarily given data, random function based hashing is devised to avoid fussy deduction and calculation burden of deterministic results [8][9].", "startOffset": 169, "endOffset": 172}, {"referenceID": 8, "context": "To address the binarization of arbitrarily given data, random function based hashing is devised to avoid fussy deduction and calculation burden of deterministic results [8][9].", "startOffset": 172, "endOffset": 175}, {"referenceID": 9, "context": "And it is believed that, such random mapping holds an intrinsic relationship with Fourier features [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 8, "context": "where sgn (\u00b7) and cos (\u00b7) respectively denotes the sign function and the cosine function, w denotes the random mapping vector, while b denotes the linear offset [9].", "startOffset": 161, "endOffset": 164}, {"referenceID": 9, "context": "The inspired motivation behind such items can be referred to shift-invariant transform [10], e.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "In certain works, it is also regard as a specifical hashing function with sift-invariant mapping [9], if each feature is reformed into two angular transforms.", "startOffset": 97, "endOffset": 100}, {"referenceID": 2, "context": "In the literature, PCA has been widely adopted to seek for principal components of global distribution of data, and is able to preserve the main information of original data [3].", "startOffset": 174, "endOffset": 177}, {"referenceID": 10, "context": "The original problem conducts the 2-way clustering of data, and seeks for solutions with principal decomposition of data [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 11, "context": "And also, there have been some works involving iterative approach to solve optimization problem of pattern analysis [12][13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 12, "context": "And also, there have been some works involving iterative approach to solve optimization problem of pattern analysis [12][13].", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": ", Newton method [15][14], is that it does not require to calculate the second-order gradient \u2207J, and optimized steps and directions are adaptively searched in iterations [13].", "startOffset": 16, "endOffset": 20}, {"referenceID": 12, "context": ", Newton method [15][14], is that it does not require to calculate the second-order gradient \u2207J, and optimized steps and directions are adaptively searched in iterations [13].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "In the literature, gradient based stopping rule has been a successful tool for stopping iterative search, it is mainly applied to quadratic optimization, while diseased learning problem is usually unavoidable in general objective functions [13].", "startOffset": 240, "endOffset": 244}, {"referenceID": 14, "context": "To tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13].", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "To tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13].", "startOffset": 70, "endOffset": 74}, {"referenceID": 15, "context": "To tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 12, "context": "To tackle the general objective, the numerical stopping criterion [16][13][17] is applied in this work, which is found much more effective than standard gradient based ones and diseased learning problem can be avoidable [13].", "startOffset": 220, "endOffset": 224}, {"referenceID": 0, "context": "In this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ.", "startOffset": 184, "endOffset": 187}, {"referenceID": 3, "context": "In this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ.", "startOffset": 187, "endOffset": 190}, {"referenceID": 5, "context": "In this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ.", "startOffset": 195, "endOffset": 198}, {"referenceID": 6, "context": "In this section, the performance of proposed ATQ method is evaluated and compared with several the-state-of-arts random quantization methods for binary pattern matching, including LSH [1][4], SH [6], ITQ [7], and CQ.", "startOffset": 204, "endOffset": 207}, {"referenceID": 16, "context": "Two data sets, CIFAR10 image dataset [18] and MINIST digit database [19], are used in these experiments.", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Two data sets, CIFAR10 image dataset [18] and MINIST digit database [19], are used in these experiments.", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "For images in CIFAR-10 dataset, a set of 512 dimensional GIST descriptors [20] and 128 dimensional SIFT descriptors [21] are learned from every tiny image, so that each data is represented as a 640 dimensional sample vector.", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "For images in CIFAR-10 dataset, a set of 512 dimensional GIST descriptors [20] and 128 dimensional SIFT descriptors [21] are learned from every tiny image, so that each data is represented as a 640 dimensional sample vector.", "startOffset": 116, "endOffset": 120}], "year": 2017, "abstractText": "Data quantization learns encoding results of data with certain requirements, and provides a broad perspective of many real-world applications to data handling. Nevertheless, the results of encoder is usually limited to multivariate inputs with the random mapping, and side information of binary codes are hardly to mostly depict the original data patterns as possible. In the literature, cosine based random quantization has attracted much attentions due to its intrinsic bounded results. Nevertheless, it usually suffers from the uncertain outputs, and information of original data fails to be fully preserved in the reduced codes. In this work, a novel binary embedding method, termed adaptive training quantization (ATQ), is proposed to learn the ideal transform of random encoder, where the limitation of cosine random mapping is tackled. As an adaptive learning idea, the reduced mapping is adaptively calculated with idea of data group, while the bias of random transform is to be improved to hold most matching information. Experimental results show that the proposed method is able to obtain outstanding performance compared with other random quantization methods.", "creator": "LaTeX with hyperref package"}}}