{"id": "1606.08866", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Technical Report: Towards a Universal Code Formatter through Machine Learning", "abstract": "There are many declarative frameworks that allow us to implement code formatters relatively easily for any specific language, but constructing them is cumbersome. The first problem is that \"everybody\" wants to format their code differently, leading to either many formatter variants or a ridiculous number of configuration options. Second, the size of each implementation scales with a language's grammar size, leading to hundreds of rules.", "histories": [["v1", "Tue, 28 Jun 2016 20:04:07 GMT  (203kb,D)", "http://arxiv.org/abs/1606.08866v1", null]], "reviews": [], "SUBJECTS": "cs.PL cs.AI cs.LG", "authors": ["terence parr", "jurgin vinju"], "accepted": false, "id": "1606.08866"}, "pdf": {"name": "1606.08866.pdf", "metadata": {"source": "CRF", "title": "Technical Report: Towards a Universal Code Formatter through Machine Learning", "authors": ["Terence Parr", "Jurgen Vinju"], "emails": ["parrt@cs.usfca.edu", "Jurgen.Vinju@cwi.nl"], "sections": [{"heading": null, "text": "In this paper, we solve the problem of format construction using a novel approach that automatically derives formatting for any language without the need for language experts to intervene. We introduce a code formatter called CODEBUFF that uses machine learning to abstract formatting rules from a representative corpus using a carefully designed set of features. Our experiments with Java, SQL, and ANTLR grammars show that CODEBUFF is efficient, has excellent accuracy, and is grammatically invariant for a particular language. It also generalizes to a fourth language that has been tested during manuscript preparation. Categories and Subject Descriptors D.2.3 [Software Engineering]: Coding - Pretty printersKeywords Formatting algorithms, Pretprinters"}, {"heading": "1. Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2. Sample Formatting", "text": "This year, it has reached the stage where it is a type of disease that is able to retaliate."}, {"heading": "3. The Design of an AI for Formatting", "text": "In fact, most of them are able to move to another world, where they can move to another world, where they can find their way to another world."}, {"heading": "3.1 Capturing whitespace as directives", "text": "To reproduce a specific style, the format guidelines for hpos / hpos / hpos / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples ples / hples / hples / hples / hples / hples / hples / hples / hples / hples / hples ples / hples / hples / hples / hples / hples ples / hples / hples / hples / hples ples / hples / hples ples / hples / hples ples / hples / hples / hples ples / hples / hples / hples / hples / hples ples / hples / hples / hples / hples / hples ples / hples / hples / hples / hples ples / hples / hples ples / hples / hples / hples / hples / hples / hples / hples / hples /"}, {"heading": "3.2 How Directives Refer to Earlier Tokens", "text": "The way in which training identifies preceding tokens for hpos policies is critical to the successful formatting of documents and is one of the most important contributions of this paper. The goal is to define a \"token locator\" that is as generic as possible but uses the least specific information. The more generic the locator, the more previous token policies can be identified, but the more specific the locator, the less applicable it is in other contexts. Consider the given positions within the following Java fragments, in which the first tokens of the previous function are arguments.f (x, y) f (x + 1, y) f (x + 1, y, \u2212 c) The absolute token index within a document is so specific that it is not applicable to other documents or even other positions within the same document."}, {"heading": "3.3 Token Context\u2014Feature Vectors", "text": "For each character that we try out in the context of development to some degree, we seem to link the properties of the context with a context in which the individual characteristics must be captured from the individual characteristics in order to be able to distinguish between different formatting, but not so specifically that the classification functions can recognize any context. The shorter the feature vector, the more situations in which each copy is applied also has the potential to confuse the classifications that require a combination of intuition and exhaustive experiments, we have achieved a small number of characteristics that work well. There are 22 context characteristics that are calculated for each token during training, but only 11 of them and hpos are used. (The classification function knows which subsets are used.) The characteristics that characterize the context of the languages that we have tested during development to some degree, but the characteristics seem to be used only by them and hpos."}, {"heading": "3.3.1 Token type and matching token features", "text": "On the token index i within each document, the context feature vector Xi contains the following properties related to previous tokens (two): < type I, token type of the previous token 2. ti, token type of the current token 3. Is ti \u2212 1 the first token type on a line? 4. Is the token type for ti the first on a line? 5. Is the token type for the last on a line? Function # 3 allows the model to distinguish between the following two different ANTLR grammar rule styles on a line when ti = DIGIT, using two different contexts. DECIMAL: DI \u2191 a GIT +; \u2191 bDECIMAL: a model for the two cases are: (X = [:, RULEREF, false, false, false,.], w = (sp, 1), h = none."}, {"heading": "3.3.2 List membership features", "text": "Most computer languages have lists of repeated elements separated or terminated by a token word, such as statement lists, formal parameter lists = = w = column lists. The next set of attributes indicates whether ti is a component of a list construct and whether this list is split over several lines (\"oversize\").6 Is leftancestor (ti) a component of an oversized list? 7. leftancestor (ti) component type within the list of {prefix token, first member, first delimiter, member, suffix token}.With these two attributes, context vectors not only capture two different generic styles for short and oversized lists, but how the various elements within these two types of lists are formatted. Here is an example oversized Java formal parameter list with list type annotations: Prefix first component types."}, {"heading": "3.3.3 Identifying oversize lists during formatting", "text": "As in the training, the formatter performs a pre-processing pass to identify the tokens of the list phrases. While the training simply identifies oversized lists as those scattered across lines, the formatting sees documents with all the squeezed whitespace. For each (r, c, sep) encountered during the pre-processing pass, the formatter consults a mini-classifier to predict whether or not that list is oversized based on the list length, ll. The mini-classifier compares the mean square of ll with the median for regular lists and the median for oversized lists, and adjusts these distances according to the probability of regular vs. oversized lists. The probability that a list is regular p (reg) = nreg / (nreg + nbig), which represents an appropriate distance to the regular type list, is vague."}, {"heading": "3.3.4 Parse-tree context features", "text": "The last features of the parseo context are: 8. child index ti) 9. rightancestor (ti \u2212 1) 10. leftancestor (ti) 11. childancestor (ti) 12. parentancestor (ti))) 16. parentestor (ti) 17. parentancestor (ti)) 18. parentestor (ti) 18. parentestor (ti) 18. parentestor (ti). parentestor (ti) 20. parentestor (ti) 20. parentestor (ti) 20. parent5 (leftancestor) 21. childancestor (leftancestor (ti)))). Here is the child index (p)."}, {"heading": "3.4 Predicting Formatting Directives", "text": "CODEBUFF's kNN classifier uses a fixed k = 11 (experimentally selected in section 4) and an L0 distance function (ratio of the number of components that differ from the vector length), but with a twist on classic kNN that highlights the attribute vector spacing in a nonlinear manner. To make predictions, a classic kNN classifier calculates the distance from the unknown attribute vector X to each Xj vector in the examples (X, Y) and predicts the category y that occurs most frequently among the k examples closest to X. The classic approach works very well in Euclidean space with quantitative attribute vectors, but not so well with an L0 distance that measures how similar two code phrases contexts are. As the L0 distance increases, the similarity of two context vectors closest to X drops dramatically."}, {"heading": "3.5 Formatting a Document", "text": "To format the document d, the formatting function 6 first squeezes all whitespace tokens and row / column information from the tokens of d, and then iterates through the remaining tokens of d, deciding which whitespace to inject before each token. On each token, the formatting software calculates a feature vector for that context and asks the model to predict a formatting statement (whereas the training examines the whitespace to determine the directive); the formatting statement uses the information in the formatting statement to calculate the number of line and space characters to be injected; the formater treats the statements as byte code statements for a simple virtual machine: {(nl, n), (sp, n), none (align, ancestor, child), (inward, ancestor, child) dent, align, inline, child, inject, child, and spatify the old information to be available for the spatification function (the spalin.whinace, the spatification, the spatification, the spalin.inace)."}, {"heading": "4. Empirical results", "text": "The main question when evaluating a code format is whether it consistently delivers high-quality results, and we begin by experimenting to show that CODEBUFF does so. Next, we examine the key factors that affect the statistical model of CODEBUFF and indirectly the quality of formatting: the way a grammar describes a language, the body size / consistency, and the parameter k of the kNN model. We conclude with a discussion of the complexity and performance of CODEBUFF."}, {"heading": "4.1 Research Method: Quantifying formatting quality", "text": "We need to quantify code format quality without human evaluation; a metric helps to isolate problems with the model (and subsequently improve it) and to measure its effectiveness in an objective way. We propose the following unit of measurement: error rate that is fully consistent formatted, CODEBUFF should generate the identity transformation for each document that d), line: = col: d: 0; d: d with whitespace tokens, line / column info; foreach ti d do with any comments to the left of ti; Xi: = compute context vector at ti; ws: = predictive directive using Xi and X, W; newlines: = sp: = (nl, n) then:. \""}, {"heading": "4.2 Corpora", "text": "We selected three very different languages - ANTLR grammars, Java and SQL - and used the following corpora (stored in CODEBUFF's [11] corpus directory). \u2022 antlr. A subset of 12 grammars from the ANTLR grammar repository that we manually formatted. \u2022 st. All 59 Java source files for StringTemplate. \u2022 guava. All 511 Java source files for Google's Guava. \u2022 sql. The same 36 SQL files formatted with IntellijIDE; some manual formatting interventions were performed to fix Intellij formatting errors."}, {"heading": "4.3 Formatting quality results", "text": "Our first experiment shows that CODEBUFF can faithfully reproduce the style found in a consistent corpus. Details of reproducing all results are available in a README.md [11]. Figure 7 shows the formatting error rate as described above. Lower mean error rates correspond with higher quality formatting, meaning that the formatted files are closer to the original. Manual checking of the corpus confirms that consistently formatted corpus actually delivers better results. For example, the median error rates (17% and 19%) are higher when using the two grammars on the noisy corpus of sql than on the cleaned sql corpus. The Guava corpus has an extremely consistent style because it is programmatically enforced and CODEBUFF is therefore able to match the style with 1 https: / / githubess.com / mmano with Java SL11 / 30acy either / 11."}, {"heading": "4.4 Grammar invariance", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves, and they will be able to play by the rules they have imposed on themselves."}, {"heading": "4.5 Effects of corpus size", "text": "We conducted an experiment to determine: (i) how many files are needed to achieve the mean overall error rate and (ii) whether adding more and more files confuses the kNN classifier. Figure 8 summarizes the results of an experiment comparing the mean error rate for randomly selected subsets of corpus of different sizes across different corpus sizes and grammars. Each data point represents 50 attempts at a given corpus size. The error rate quickly decreases after about 5 files and then approaches asymptotically that of Figure 7. This graph suggests a minimum corpus size of about 10 files and provides evidence that adding more (uniformly formatted) files neither confuses nor significantly improves the classifier. 12 2016 / 6 / 30"}, {"heading": "4.6 Optimization and stability of model parameters", "text": "Choosing a k for a kNN model is more of an art, but k = \u221a N forN examples are commonly used. Instead, through extensive manual review of formatted files, we arrived at a fixed k = 11 and then tested its suitability through experiments. Figure 9 shows the impact of different k on the mean error rate for a selection of corpora and grammars; k was between 1 and 99. This graph supports the conclusion that a formater can make accurate decisions by comparing the context surrounding ti with very few model examples. Moreover, the accuracy of formatting for k is very stable; even large changes in k do not change the error rate very much."}, {"heading": "4.7 Worst-case complexity", "text": "The first pass performs the parse tree for each d'eau, and the number of tokens in the document is limited, so the sum of tokens per token has a tree depth of no more than three meters. To make a pass over all the parse trees for the entire corpus, the time complexity must be divided into three levels, so that inO (N) has a tree depth of no more than four meters for the entire token, and the second pass goes through all the d'eau trees that represent the entire corpus."}, {"heading": "4.8 Expected performance", "text": "CODEBUFF is instrumented to report one-thread CPU runtime for training and formatting purposes. We report an average training time of 1.5 s for the (by an order of magnitude oversized) guava corpus (511 files, 143k lines) after parsing documents with Java grammar and a training time of 1.8 s with Java grammar. 2 The antlr corpus of only 12 files is trained within 72 ms. Since kNN uses the full list of examples as internal representation, the memory footprint of CODEBUFF is significant. For each of the N tokens in the corpus, we follow m characteristics and two formatting guidelines (each packed into a single word).The size complexity is O (N), but with a non-trivial constant of the CODEBUFF corpus."}, {"heading": "5. Test of Generality", "text": "As a test of generality, we asked for a grammar and corpus for an unknown language, Quorum3 by Andreas Stefik, 2 experiments run 20 times on an iMac17.1 OS 10.11.5, 4GHz Intel Core i7 with Java 8, heap / stack size 4G / 1M; we ignored the first 5 measurements to take warm-up time into account. Details that should be reproduced in github repo. 3 https: / / www.quorumlanguage.com13 2016 / 6 / 30 and trained CODEBUFF on this corpus. The first look at the grammar and corpus for both the authors and for the model occurred during the preparation of this manuscript. We dropped about half of the corpus files randomly to simulate a more reasonable training corpus, but did not change any data. Most of the files in the corpus result in a plausibly formatted code, but there are a few fancy files, such as function would not be evaluated as table.For example, tabletop ="}, {"heading": "6. Related work", "text": "This year is the highest in the history of the country."}, {"heading": "7. Conclusion", "text": "CODEBUFF is a step towards a universal code format that uses machine learning to abstract formatting rules from a representative corpus. Current approaches require complex formatting rules written by a language expert, while entering them into CODEBUFF is just a grammar, a corpus and an indentation size. Experiments show that the training requires about 1014 2016 / 6 / 30 files and that the resulting formats for three languages are fast and highly precise. Tests on a previously invisible language and a corpus also show that CODEBUFF has been generalized to a fourth language without modification. Formatting results are largely insensitive to linguistic changes in grammar and our kNN classifier is highly stable in terms of changes to the model parameter k. Based on these results, we look forward to many more applications of machine learning in software language development."}], "references": [{"title": "A pretty good formatting pipeline", "author": ["A.H. Bagge", "T. Hasu"], "venue": "In International Conferance on Software Language Engineering (SLE\u201913),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Pretty printing with lazy dequeues", "author": ["O. Chitil"], "venue": "ACM TOPLAS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2005}, {"title": "The Box", "author": ["J. Coutaz"], "venue": "a layout abstraction for user interface toolkits. CMU-CS-84-167, Carnegie Mellon University", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1984}, {"title": "Pretty-printing for software reengineering", "author": ["M. de Jonge"], "venue": "In Proceedings of ICSM", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Pretty-printing for software reengineering", "author": ["M. de Jonge"], "venue": "ICSM", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "Technological spaces: An initial appraisal", "author": ["I. Kurtev", "J. B\u00e9zivin", "M. Aksit"], "venue": "International Symposium on Distributed Objects and Applications, DOA 2002", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Binary Codes Capable of Correcting Deletions, Insertions and Reversals", "author": ["V.I. Levenshtein"], "venue": "Soviet Physics Doklady,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1966}, {"title": "Code Complete", "author": ["S. McConnel"], "venue": "Microsoft Press", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "Program indentation and comprehensibility", "author": ["R.J. Miara", "J.A. Musselman", "J.A. Navarro", "B. Shneiderman"], "venue": "ACM, 26 (11):861\u2013867", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1983}, {"title": "Prettyprinting", "author": ["D.C. Oppen"], "venue": "ACM TOPLAS, 2(4):465\u2013483", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1980}, {"title": "Generation of formatters for context-free languages", "author": ["M. van den Brand", "E. Visser"], "venue": "ACM Trans. Softw. Eng. Methodol.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "A language independent framework for contextsensitive formatting", "author": ["M.G. van den Brand", "A.T. Kooiker", "J.J. Vinju", "N.P. Veerman"], "venue": "CSMR", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Generation of formatters for context-free languages", "author": ["M.G.J. van den Brand", "E. Visser"], "venue": "ACM Trans. Softw. Eng. Methodol.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1996}, {"title": "The ASF+SDF Meta-Environment: a Component-Based Language Development Environment", "author": ["M.G.J. van den Brand", "A. van Deursen", "J. Heering", "H.A. de Jong", "M. de Jonge", "T. Kuipers", "P. Klint", "P.A. Olivier", "J. Scheerder", "J.J. Vinju", "E. Visser", "J. Visser"], "venue": "In CC \u201901,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2001}, {"title": "Analysis and Transformation of Source Code by Parsing and Rewriting", "author": ["J. Vinju"], "venue": "PhD thesis, U. van Amsterdam", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "High-fidelity c/c++ code transformation", "author": ["D.G. Waddington", "B. Yao"], "venue": "Electron. Notes Theor. Comput. Sci.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "A prettier printer", "author": ["P. Wadler"], "venue": "Journal of Functional Programming, pages 223\u2013244. Palgrave Macmillan", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 8, "context": "The way source code is formatted has a significant impact on its comprehensibility [9], and manually reformatting code is just not an option [8, p.", "startOffset": 83, "endOffset": 86}, {"referenceID": 11, "context": "The emergent behavior is not always obvious, there exists interdependency between options, and the tools cannot take context information into account [13].", "startOffset": 150, "endOffset": 154}, {"referenceID": 2, "context": "Rule-based formatting systems let programmers specify phrase-formatting pairs, such as the following sample specification for formatting the COBOL MOVE statement using ASF+SDF [3, 12, 13, 15].", "startOffset": 176, "endOffset": 191}, {"referenceID": 10, "context": "Rule-based formatting systems let programmers specify phrase-formatting pairs, such as the following sample specification for formatting the COBOL MOVE statement using ASF+SDF [3, 12, 13, 15].", "startOffset": 176, "endOffset": 191}, {"referenceID": 11, "context": "Rule-based formatting systems let programmers specify phrase-formatting pairs, such as the following sample specification for formatting the COBOL MOVE statement using ASF+SDF [3, 12, 13, 15].", "startOffset": 176, "endOffset": 191}, {"referenceID": 13, "context": "Rule-based formatting systems let programmers specify phrase-formatting pairs, such as the following sample specification for formatting the COBOL MOVE statement using ASF+SDF [3, 12, 13, 15].", "startOffset": 176, "endOffset": 191}, {"referenceID": 0, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 1, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 2, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 3, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 4, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 5, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 6, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 7, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 8, "context": ": [0-9]", "startOffset": 2, "endOffset": 7}, {"referenceID": 6, "context": "A naive similarity measure is the edit distance (Levenshtein Distance [7]) between d\u2032 and d, but it is expensive to compute and will over-accentuate minor differences.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "Let operator d1 d2 be the normalized edit distance between documents d1 and d2 defined by the Levenshtein Distance [7] divided by max(len(d1), len(d2)).", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "The state of the art in language-parametric formatting stems from two sources: Coutaz [3] and Oppen [10].", "startOffset": 86, "endOffset": 89}, {"referenceID": 9, "context": "The state of the art in language-parametric formatting stems from two sources: Coutaz [3] and Oppen [10].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "Coutaz introduced the \u201cBox\u201d as a basic two-dimensional layout mechanism and all derived work elaborates on specifying the mapping from syntax (parse) trees to Box expressions more effectively (in terms of meta-program size and expressiveness) [4, 12\u201314].", "startOffset": 243, "endOffset": 253}, {"referenceID": 10, "context": "Coutaz introduced the \u201cBox\u201d as a basic two-dimensional layout mechanism and all derived work elaborates on specifying the mapping from syntax (parse) trees to Box expressions more effectively (in terms of meta-program size and expressiveness) [4, 12\u201314].", "startOffset": 243, "endOffset": 253}, {"referenceID": 11, "context": "Coutaz introduced the \u201cBox\u201d as a basic two-dimensional layout mechanism and all derived work elaborates on specifying the mapping from syntax (parse) trees to Box expressions more effectively (in terms of meta-program size and expressiveness) [4, 12\u201314].", "startOffset": 243, "endOffset": 253}, {"referenceID": 12, "context": "Coutaz introduced the \u201cBox\u201d as a basic two-dimensional layout mechanism and all derived work elaborates on specifying the mapping from syntax (parse) trees to Box expressions more effectively (in terms of meta-program size and expressiveness) [4, 12\u201314].", "startOffset": 243, "endOffset": 253}, {"referenceID": 0, "context": "Derivative and later work [1, 2, 18] elaborates on specifying how and when to inject these tokens for different languages in different ways and in different technological spaces [6].", "startOffset": 26, "endOffset": 36}, {"referenceID": 1, "context": "Derivative and later work [1, 2, 18] elaborates on specifying how and when to inject these tokens for different languages in different ways and in different technological spaces [6].", "startOffset": 26, "endOffset": 36}, {"referenceID": 16, "context": "Derivative and later work [1, 2, 18] elaborates on specifying how and when to inject these tokens for different languages in different ways and in different technological spaces [6].", "startOffset": 26, "endOffset": 36}, {"referenceID": 5, "context": "Derivative and later work [1, 2, 18] elaborates on specifying how and when to inject these tokens for different languages in different ways and in different technological spaces [6].", "startOffset": 178, "endOffset": 181}, {"referenceID": 0, "context": "PGF [1] by Bagge and Hasu is a rule-based system to inject pretty printing directives into a stream.", "startOffset": 4, "endOffset": 7}, {"referenceID": 14, "context": "Aside from the actual functionality of formatting text, related work has extended formatting with \u201chigh fidelity\u201d (not losing comments) [16, 17] and partial formatting (introducing a new source text in an already formatted context) [5].", "startOffset": 136, "endOffset": 144}, {"referenceID": 15, "context": "Aside from the actual functionality of formatting text, related work has extended formatting with \u201chigh fidelity\u201d (not losing comments) [16, 17] and partial formatting (introducing a new source text in an already formatted context) [5].", "startOffset": 136, "endOffset": 144}, {"referenceID": 4, "context": "Aside from the actual functionality of formatting text, related work has extended formatting with \u201chigh fidelity\u201d (not losing comments) [16, 17] and partial formatting (introducing a new source text in an already formatted context) [5].", "startOffset": 232, "endOffset": 235}], "year": 2016, "abstractText": "There are many declarative frameworks that allow us to implement code formatters relatively easily for any specific language, but constructing them is cumbersome. The first problem is that \u201ceverybody\u201d wants to format their code differently, leading to either many formatter variants or a ridiculous number of configuration options. Second, the size of each implementation scales with a language\u2019s grammar size, leading to hundreds of rules. In this paper, we solve the formatter construction problem using a novel approach, one that automatically derives formatters for any given language without intervention from a language expert. We introduce a code formatter called CODEBUFF that uses machine learning to abstract formatting rules from a representative corpus, using a carefully designed feature set. Our experiments on Java, SQL, and ANTLR grammars show that CODEBUFF is efficient, has excellent accuracy, and is grammar invariant for a given language. It also generalizes to a 4th language tested during manuscript preparation.", "creator": "LaTeX with hyperref package"}}}