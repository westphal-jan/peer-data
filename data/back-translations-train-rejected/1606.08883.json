{"id": "1606.08883", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Jun-2016", "title": "Defending Non-Bayesian Learning against Adversarial Attacks", "abstract": "This paper addresses the problem of non-Bayesian learning over multi-agent networks, where agents repeatedly collect partially informative observations about an unknown state of the world, and try to collaboratively learn the true state. We focus on the impact of the adversarial agents on the performance of consensus-based non-Bayesian learning, where non-faulty agents combine local learning updates with consensus primitives. In particular, we consider the scenario where an unknown subset of agents suffer Byzantine faults -- agents suffering Byzantine faults behave arbitrarily. Two different learning rules are proposed.", "histories": [["v1", "Tue, 28 Jun 2016 20:50:08 GMT  (45kb)", "http://arxiv.org/abs/1606.08883v1", null]], "reviews": [], "SUBJECTS": "cs.DC cs.LG", "authors": ["lili su", "nitin h vaidya"], "accepted": false, "id": "1606.08883"}, "pdf": {"name": "1606.08883.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Lili Su", "Nitin H. Vaidya"], "emails": ["nhv}@illinois.edu", "(lilisu3@illinois.edu)"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.08 883v 1 [cs.D C] 28 June 20Agents repeatedly collect partial informative observations about an unknown state of the world and try to learn the true state together. We focus on the effects of opposing agents on the performance of consensus-based non-Bayesian learning processes in which non-flawed agents combine local learning updates with consensus primitives. In particular, we consider the scenario in which an unknown subset of agents suffers Byzantine errors - agents who suffer Byzantine errors behave arbitrarily. We propose two rules of learning. - In our first rule of updating, each agent updates his local beliefs as (until normalization) the product of cumulative private signals and (2) the weighted geometric average of the beliefs of his future neighbors and himself."}, {"heading": "1 Introduction", "text": "In fact, most of them are unable to abide by the rules they have imposed on themselves."}, {"heading": "2 Problem Formulation", "text": "We are looking at a synchronous system, including the results of the network. A collection of n agents (also referred to as knots) are connected by a network connected to G (V, E), 4 \u00b7 V = [1],.. \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "3 Byzantine Consensus", "text": "The Byzantine consensus has attracted considerable attention [25,26,29,27,30,31]. Whereas previous work has focused mainly on scalar inputs, the more general vector (or multidimensional) inputs have recently been studied [31,29,28]. Complete communication networks are considered in [31,29], where each agent is only allowed to exchange information about his state with his neighbors. In particular, our learning algorithms are built on byz-itter algorithms proposed in [28] and a simple algorithm proposed in [30] for iterative Byzantine consensus algorithms with vector inputs and scalar inputs."}, {"heading": "3.3 Matrix Representation [28]", "text": "Suppose that the state updates performed by the non-faulty agents in the t-th iteration (t-th iteration) can be expressed in such a way that it is a m-dimensional reduced graph Hm [t] with adjacent matrix Hm [t] v-1, (2) where A [t] ver-or (n-th iteration) \u00b7 (n-th iteration) is a series of stochastic graphs for which it is a m-dimensional reduced graph Hm [t] with adjacent matrix Hm [t] v-1, [t] such that an A [t] ver mHm [t], [t] ver [t], where 0 < \u03b2m \u2264 1 is a constant that depends only on G (V, E)."}, {"heading": "3.4 Tight Topological Condition for Scalar Iterative Byzantine Consensus", "text": "The above analysis shows that assumption 1 is sufficient to reach a byzantine consensus iteratively. In the specific case where m = 1 (i.e. the inputs of individual error-free agents are scalars), [30] it was shown that assumption 1 is also necessary. Theorem 4. [30] For scalar inputs, an iterative approximate byzantine consensus is attainable among non-defective agents if and only if each one-dimensionally reduced graph of G (V, E) contains only one source component.In addition, the following simple algorithm (algorithm 3) works under assumption 1, if m = 1. In addition, it has been shown that the dynamics of the non-defective agents allow the same matrix representation as in Section 3.3, with the reduced graph being a one-dimensionally reduced graph defined in Definition 1.With the above background on the byzantine vector consensus defined, we are now ready to present our first algorithm and its analysis."}, {"heading": "4 Byzantine Fault-Tolerant Non-Bayesian Learning (BFL)", "text": "In this section, we present our first learning rule, whose values have not been removed. [...] In this section, we present our first learning rule. [...] In this section, we present our first learning rule. [...] In this section, we present our first learning rule. [...] In this section, we present our first learning rule. [...] In this section, we state that each agent has a set of beliefs. [...] Each agent has a consensus. [...] [...] Sort the values obtained. [...] Sort the values obtained. [...] Sort the values obtained. [...] Sort the values obtained. [...] Sort the values obtained. [...] For each outgoing link. [...] Received messages about all incoming links. [...] These message values. [...] These message values. [...] These message values form a multisetRi form of multisetRi of size. [...] Sort the values obtained. [...] For each incoming link. [...] These message values are not decreasing in a sequence of the smallest values [...]."}, {"heading": "4.1 Identifiability", "text": "In the absence of failure to act [3], it is sufficient to assume that G (V, E) is strongly interconnected and that we are universally identifiable, which means that for all action options designated by D (V), there is a node j (FL), so that the Kullback-Leiber divergence between the true borderline consensus j (V, E) and the marginal state j (V) designated by D (V) exists. This means that a precise action component j (V), however, contains a consensus that is not zero; it is equivalent that the action constellation j (V) is free."}, {"heading": "4.2 Convergence Results", "text": "Our proofs have parallels to the structure of evidence in which we represent the dynamics of proof i. However, we have some substantial differences that we must take into account in order to take into account our updating rule for the faith vector. (For each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n), each (n, each (n), each (n), each (n, each (n), each (n), each (n), each (n), each (n), each (n, each (n), each (n), each (n, each (n), each (n), each (n, each (n), each (n), each (n), each (n, each (n), each (n), each (n), each (n), each (n, each (n), each (n), each (n), each (n, each (n), each (n), each (n, each (n), each (n), each (n), each (n, each (n), each (n), each (n), each (n), each (n), each (n), each (n, n), each (n), each (n), each (n, each (n), each (n), each (n, n), each (n, n), each (n), each (n), each (n), each (n), n (n), each (n), each (n)."}, {"heading": "5 BFL in the absence of Byzantine Agents, i.e., f = 0", "text": "In this section we present BFL for the specific case where there are no unambiguous results, i.e., f = 0, designated failless BFL. Since f = 0, all actors in the network are cooperative, no trimming is required. In fact, the BFL for f = 0 is a simple modification of the algorithm proposed in [14]. (28) 16Algorithm 5: Failless BFL1 Transmit current faith vector it is \u2212 1 on all outgoing edges. (2) Wait until a private signal is observed and faith vectors are received by all incoming neighbors. (28) 16Algorithm 5: Failless BFL1 Transmit current faith vector it is \u2212 1. (2) Wait until a private signal is observed and faith vectors are received by all incoming neighbors."}, {"heading": "5.1 Finite-Time Analysis of Failure-Free BFL", "text": "In this subsection, we represent the convergence rate that can be achieved in finite time with a high probability. Our proof is similar to the proof that can be achieved in [14,17].Lemma 4. Let us determine the convergence rate that can be achieved in finite time with a high probability, as defined in (9). Then, for each agent, we have a random answer [1 \u2212 (1 \u2212 (1 n). (1 \u2212 (n). (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212) t (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212 (1 \u2212 (n). (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212). (1 \u2212). (1). (1). (1. (.). (. (.). (1. (.). (1. (.). (. (.). (1. (.). (1. (.). (1. (.). (.). (1. (.). (1. (.). (. (.). (1. (.). (. (.). (1). (. (.). (1. (.). (. (.). (.). (1). (.). (. (1). (.). (1). (.). (. (.). (1). (.). (.). (.).). (1).). (.). (1. (.). (. (.). (1). (.). (. (.). (.).).). (1. (.). (.).). (.). (. (1.). (.).). (1.). (. (.). (1.). (.).). (1.).). (. (.).). (.).).). (1. (.).). (.).). ("}, {"heading": "6 Modified BFL and Minimal Network Identifiability", "text": "To reduce the complexity of algorithms, we only need a consensus between the consensus agents. (...) The minimal (narrow) global identifiability of the network for all the consensus-based learning rule of interest to learn the true state, we must propose a modification of the above learning rule that works under much weaker network topology and global identifiability. (...) We break down the m-Ratio-Ratio-Test-Problem into m (m \u2212 1) (ordered) binary hypotheses-test problems. (...) Each pair of hypotheses-1 and 2, each non-error-agent updates the probability-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-Ratio-follows. (...)"}, {"heading": "7 Conclusion", "text": "This paper deals with the problem of consensus-based non-Bayean learning via multi-agent networks, although an unknown subset of agents can be counterproductive (Byzantine).We propose two rules of learning and characterize the narrow identification condition for each consensus-based rule of learning of interest. In our first rule of updating, each agent updates his local beliefs as (until normalization) the product of (1) cumulative private signals and (2) the weighted geometric average of the beliefs of his incoming neighbors and himself. Under reasonable assumptions about the underlying network structure and the global identification capability of the network, we show that all non-faulty agents correspond asymptomatically to the true state. In the case that each agent is failure-free, we show that (most likely) the beliefs of each agent are based on the false hypothesis at which rate O (exp) decreases."}], "references": [{"title": "Decentralized detection in sensor networks", "author": ["J.-F. Chamberland", "V.V. Veeravalli"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Bayesian learning in social networks", "author": ["D. Gale", "S. Kariv"], "venue": "Games and Economic Behavior,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "Non-bayesian social learning", "author": ["A. Jadbabaie", "P. Molavi", "A. Sandroni", "A. Tahbaz-Salehi"], "venue": "Games and Economic Behavior,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Reaching approximate byzantine consensus with multi-hop communication", "author": ["L. Su", "N.H. Vaidya"], "venue": "In Proceedings of International Symposium on Stabilization, Safety, and Security of Distributed Systems (SSS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Decentralized detection by a large number of sensors", "author": ["J. Tsitsiklis"], "venue": "Mathematics of Control, Signals and Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1988}, {"title": "Decentralized detection", "author": ["J.N. Tsitsiklis"], "venue": "Advances in Statistical Signal Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "Distributed Detection and Data Fusion", "author": ["P.K. Varshney"], "venue": "Springer Science & Business Media,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Stochastic Processes in Engineering Systems", "author": ["E. Wong", "B. Hajek"], "venue": "Springer Science & Business Media,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Weak ergodicity in non-homogeneous markov chains", "author": ["J. Hajnal", "M. Bartlett"], "venue": "In Mathematical Proceedings of the Cambridge Philosophical Society,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1958}, {"title": "Information heterogeneity and the speed of learning in social networks", "author": ["A. Jadbabaie", "P. Molavi", "A. Tahbaz-Salehi"], "venue": "Columbia Business School Research Paper,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Social learning and distributed hypothesis testing", "author": ["A. Lalitha", "A. Sarwate", "T. Javidi"], "venue": "In Information Theory (ISIT),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Distributed Algorithms", "author": ["N.A. Lynch"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1996}, {"title": "Foundations of non-bayesian social learning", "author": ["P. Molavi", "A. Jadbabaie"], "venue": "Columbia Business School Research Paper,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Nonasymptotic convergence rates for cooperative learning over timevarying directed graphs", "author": ["A. Nedi\u0107", "A. Olshevsky", "C.A. Uribe"], "venue": "arXiv preprint arXiv:1410.1977,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Distributed parameter estimation in networks", "author": ["K.R. Rad", "A. Tahbaz-Salehi"], "venue": "In IEEE Conference on Decision and Control (CDC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Exponentially fast parameter estimation in networks using distributed dual averaging", "author": ["S. Shahrampour", "A. Jadbabaie"], "venue": "In IEEE Conference on Decision and Control (CDC),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Distributed detection: Finite-time analysis and impact of network topology", "author": ["S. Shahrampour", "A. Rakhlin", "A. Jadbabaie"], "venue": "arXiv preprint arXiv:1409.8606,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Finite-time analysis of the distributed detection problem", "author": ["S. Shahrampour", "A. Rakhlin", "A. Jadbabaie"], "venue": "arXiv preprint arXiv:1512.09311,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Products of indecomposable, aperiodic, stochastic matrices", "author": ["J. Wolfowitz"], "venue": "In Proceedings of the American Mathematical Society,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1963}, {"title": "Matrix Representation of Iterative Approximate Byzantine Consensus in Directed Graphs", "author": ["N.H. Vaidya"], "venue": "arXiv 1203.1888,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Large deviations performance of consensus+ innovations distributed detection with non-gaussian observations", "author": ["D. Bajovi\u0107", "D. Jakoveti\u0107", "J.M. Moura", "J. Xavier", "B. Sinopoli"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Distributed detection over adaptive networks using diffusion adaptation", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1917}, {"title": "Distributed detection over noisy networks: Large deviations analysis", "author": ["D. Jakoveti\u0107", "J.M. Moura", "J. Xavier"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Reaching agreement in the presence of faults", "author": [], "venue": "J. ACM", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1980}, {"title": "Reaching approximate agreement in the presence of faults", "author": ["D. Dolev", "N.A. Lynch", "S.S. Pinter", "E.W. Stark", "W.E. Weihl"], "venue": "J. ACM,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1986}, {"title": "Asymptotically optimal algorithms for approximate agreement", "author": ["A.D. Fekete"], "venue": "Distributed Computing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1990}, {"title": "Consensus of multi-agent networks in the presence of adversaries using only local information", "author": ["H.J. LeBlanc", "H. Zhang", "S. Sundaram", "X. Koutsoukos"], "venue": "In Proceedings of the 1st International Conference on High Confidence Networked Systems, HiCoNS", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Iterative byzantine vector consensus in incomplete graphs", "author": ["N.H. Vaidya"], "venue": "In Distributed Computing and Networking,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Byzantine vector consensus in complete graphs", "author": ["N.H. Vaidya", "V.K. Garg"], "venue": "In Proceedings of the 2013 ACM symposium on Principles of distributed computing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "Iterative approximate byzantine consensus in arbitrary directed graphs", "author": ["N.H. Vaidya", "L. Tseng", "G. Liang"], "venue": "In Proceedings of the 2012 ACM symposium on Principles of distributed computing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Multidimensional approximate agreement in byzantine asynchronous systems", "author": ["H. Mendes", "M. Herlihy"], "venue": "In Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Sigron. A generalization of Tverberg\u2019s", "author": ["M.M.A. Perles"], "venue": "theorem. arXiv", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Network Independent Rates in Distributed Learning", "author": ["A. Nedi\u0107", "A. Olshevsky", "C.A. Uribe"], "venue": "arXiv preprint arXiv:1509.08574,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Asynchronous Distributed Hypothesis Testing in the Presence of Crash Failures University of Illinois at Urbana-Champaign", "author": ["L. Su", "N.H. Vaidya"], "venue": "Tech. Rep,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "2 1 Introduction Decentralized hypothesis testing (learning) has received significant amount of attention [1,2,3,5,6,7,8].", "startOffset": 106, "endOffset": 121}, {"referenceID": 1, "context": "2 1 Introduction Decentralized hypothesis testing (learning) has received significant amount of attention [1,2,3,5,6,7,8].", "startOffset": 106, "endOffset": 121}, {"referenceID": 2, "context": "2 1 Introduction Decentralized hypothesis testing (learning) has received significant amount of attention [1,2,3,5,6,7,8].", "startOffset": 106, "endOffset": 121}, {"referenceID": 4, "context": "2 1 Introduction Decentralized hypothesis testing (learning) has received significant amount of attention [1,2,3,5,6,7,8].", "startOffset": 106, "endOffset": 121}, {"referenceID": 5, "context": "2 1 Introduction Decentralized hypothesis testing (learning) has received significant amount of attention [1,2,3,5,6,7,8].", "startOffset": 106, "endOffset": 121}, {"referenceID": 6, "context": "2 1 Introduction Decentralized hypothesis testing (learning) has received significant amount of attention [1,2,3,5,6,7,8].", "startOffset": 106, "endOffset": 121}, {"referenceID": 7, "context": "2 1 Introduction Decentralized hypothesis testing (learning) has received significant amount of attention [1,2,3,5,6,7,8].", "startOffset": 106, "endOffset": 121}, {"referenceID": 4, "context": "The traditional decentralized detection framework consists of a collection of spatially distributed sensors and a fusion center [5,6,7].", "startOffset": 128, "endOffset": 135}, {"referenceID": 5, "context": "The traditional decentralized detection framework consists of a collection of spatially distributed sensors and a fusion center [5,6,7].", "startOffset": 128, "endOffset": 135}, {"referenceID": 6, "context": "The traditional decentralized detection framework consists of a collection of spatially distributed sensors and a fusion center [5,6,7].", "startOffset": 128, "endOffset": 135}, {"referenceID": 1, "context": "Distributed hypothesis testing in the absence of fusion center is considered in [2,22,23,21].", "startOffset": 80, "endOffset": 92}, {"referenceID": 21, "context": "Distributed hypothesis testing in the absence of fusion center is considered in [2,22,23,21].", "startOffset": 80, "endOffset": 92}, {"referenceID": 22, "context": "Distributed hypothesis testing in the absence of fusion center is considered in [2,22,23,21].", "startOffset": 80, "endOffset": 92}, {"referenceID": 20, "context": "Distributed hypothesis testing in the absence of fusion center is considered in [2,22,23,21].", "startOffset": 80, "endOffset": 92}, {"referenceID": 1, "context": "In particular, Gale and Kariv [2] studied the distributed hypothesis testing problem in the context of social learning, where fully Bayesian belief update rule is studied.", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 38, "endOffset": 63}, {"referenceID": 13, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 38, "endOffset": 63}, {"referenceID": 14, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 38, "endOffset": 63}, {"referenceID": 15, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 38, "endOffset": 63}, {"referenceID": 10, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 38, "endOffset": 63}, {"referenceID": 17, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 38, "endOffset": 63}, {"referenceID": 16, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 38, "endOffset": 63}, {"referenceID": 12, "context": "[3], and has attracted much attention [10,14,15,16,11,18,17,13].", "startOffset": 38, "endOffset": 63}, {"referenceID": 2, "context": "[3] considered the general setting where external signals are observed during each iteration of the algorithm execution.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "It is shown [3] that, under this learning rule, each agent learns the true state almost surely.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 19, "endOffset": 22}, {"referenceID": 9, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 263, "endOffset": 288}, {"referenceID": 13, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 263, "endOffset": 288}, {"referenceID": 14, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 263, "endOffset": 288}, {"referenceID": 15, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 263, "endOffset": 288}, {"referenceID": 10, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 263, "endOffset": 288}, {"referenceID": 17, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 263, "endOffset": 288}, {"referenceID": 16, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 263, "endOffset": 288}, {"referenceID": 12, "context": "The publication of [3] has inspired significant efforts in designing and analyzing non-Bayesian learning rules with a particular focus on refining the fusion strategies and analyzing the (asymptotic and/or finite time) convergence rates of the refined algorithms [10,14,15,16,11,18,17,13].", "startOffset": 263, "endOffset": 288}, {"referenceID": 14, "context": "In this paper we are particularly interested in the log-linear form of the update rule, in which, essentially, each agent updates its belief as the geometric average of the local Bayesian update and its neighbors\u2019 beliefs [15,10,14,16,11,18,17,13].", "startOffset": 222, "endOffset": 247}, {"referenceID": 9, "context": "In this paper we are particularly interested in the log-linear form of the update rule, in which, essentially, each agent updates its belief as the geometric average of the local Bayesian update and its neighbors\u2019 beliefs [15,10,14,16,11,18,17,13].", "startOffset": 222, "endOffset": 247}, {"referenceID": 13, "context": "In this paper we are particularly interested in the log-linear form of the update rule, in which, essentially, each agent updates its belief as the geometric average of the local Bayesian update and its neighbors\u2019 beliefs [15,10,14,16,11,18,17,13].", "startOffset": 222, "endOffset": 247}, {"referenceID": 15, "context": "In this paper we are particularly interested in the log-linear form of the update rule, in which, essentially, each agent updates its belief as the geometric average of the local Bayesian update and its neighbors\u2019 beliefs [15,10,14,16,11,18,17,13].", "startOffset": 222, "endOffset": 247}, {"referenceID": 10, "context": "In this paper we are particularly interested in the log-linear form of the update rule, in which, essentially, each agent updates its belief as the geometric average of the local Bayesian update and its neighbors\u2019 beliefs [15,10,14,16,11,18,17,13].", "startOffset": 222, "endOffset": 247}, {"referenceID": 17, "context": "In this paper we are particularly interested in the log-linear form of the update rule, in which, essentially, each agent updates its belief as the geometric average of the local Bayesian update and its neighbors\u2019 beliefs [15,10,14,16,11,18,17,13].", "startOffset": 222, "endOffset": 247}, {"referenceID": 16, "context": "In this paper we are particularly interested in the log-linear form of the update rule, in which, essentially, each agent updates its belief as the geometric average of the local Bayesian update and its neighbors\u2019 beliefs [15,10,14,16,11,18,17,13].", "startOffset": 222, "endOffset": 247}, {"referenceID": 12, "context": "In this paper we are particularly interested in the log-linear form of the update rule, in which, essentially, each agent updates its belief as the geometric average of the local Bayesian update and its neighbors\u2019 beliefs [15,10,14,16,11,18,17,13].", "startOffset": 222, "endOffset": 247}, {"referenceID": 9, "context": "The log-linear form (geometric averaging) update rule is shown to converge exponentially fast [10,16].", "startOffset": 94, "endOffset": 101}, {"referenceID": 15, "context": "The log-linear form (geometric averaging) update rule is shown to converge exponentially fast [10,16].", "startOffset": 94, "endOffset": 101}, {"referenceID": 12, "context": "Taking an axiomatic approach, the geometric averaging fusion is proved to be optimal [13].", "startOffset": 85, "endOffset": 89}, {"referenceID": 15, "context": "An optimization-based interpretation of this rule is presented in [16], using dual averaging method with properly chosen proximal functions.", "startOffset": 66, "endOffset": 70}, {"referenceID": 13, "context": "Finite-time convergence rates are investigated independently in [14,11,17].", "startOffset": 64, "endOffset": 74}, {"referenceID": 10, "context": "Finite-time convergence rates are investigated independently in [14,11,17].", "startOffset": 64, "endOffset": 74}, {"referenceID": 16, "context": "Finite-time convergence rates are investigated independently in [14,11,17].", "startOffset": 64, "endOffset": 74}, {"referenceID": 13, "context": "Both [14] and [18] consider time-varying networks, with slightly different network models.", "startOffset": 5, "endOffset": 9}, {"referenceID": 17, "context": "Both [14] and [18] consider time-varying networks, with slightly different network models.", "startOffset": 14, "endOffset": 18}, {"referenceID": 13, "context": "Specifically, [14] assumes that the union of every consecutive B networks is strongly connected, while [18] considers random networks.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "Specifically, [14] assumes that the union of every consecutive B networks is strongly connected, while [18] considers random networks.", "startOffset": 103, "endOffset": 107}, {"referenceID": 2, "context": "Thus, this paper focuses on the fault-tolerant version the non-Bayesian framework proposed in [3].", "startOffset": 94, "endOffset": 97}, {"referenceID": 33, "context": "An alternative fault model, where some agents may unexpectedly cease computing and communicate with each other asynchronously, is considered in our companion work [34].", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "[24] and has attracted intensive attention from researchers [25,26,27,30,28,31].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[24] and has attracted intensive attention from researchers [25,26,27,30,28,31].", "startOffset": 60, "endOffset": 79}, {"referenceID": 25, "context": "[24] and has attracted intensive attention from researchers [25,26,27,30,28,31].", "startOffset": 60, "endOffset": 79}, {"referenceID": 26, "context": "[24] and has attracted intensive attention from researchers [25,26,27,30,28,31].", "startOffset": 60, "endOffset": 79}, {"referenceID": 29, "context": "[24] and has attracted intensive attention from researchers [25,26,27,30,28,31].", "startOffset": 60, "endOffset": 79}, {"referenceID": 27, "context": "[24] and has attracted intensive attention from researchers [25,26,27,30,28,31].", "startOffset": 60, "endOffset": 79}, {"referenceID": 30, "context": "[24] and has attracted intensive attention from researchers [25,26,27,30,28,31].", "startOffset": 60, "endOffset": 79}, {"referenceID": 9, "context": "The existing non-Bayesian learning algorithms [10,11,13,14,15,16,17,18] are not robust to Byzantine agents, since the malicious messages sent by the Byzantine agents are indiscriminatingly utilized in the local belief updates.", "startOffset": 46, "endOffset": 71}, {"referenceID": 10, "context": "The existing non-Bayesian learning algorithms [10,11,13,14,15,16,17,18] are not robust to Byzantine agents, since the malicious messages sent by the Byzantine agents are indiscriminatingly utilized in the local belief updates.", "startOffset": 46, "endOffset": 71}, {"referenceID": 12, "context": "The existing non-Bayesian learning algorithms [10,11,13,14,15,16,17,18] are not robust to Byzantine agents, since the malicious messages sent by the Byzantine agents are indiscriminatingly utilized in the local belief updates.", "startOffset": 46, "endOffset": 71}, {"referenceID": 13, "context": "The existing non-Bayesian learning algorithms [10,11,13,14,15,16,17,18] are not robust to Byzantine agents, since the malicious messages sent by the Byzantine agents are indiscriminatingly utilized in the local belief updates.", "startOffset": 46, "endOffset": 71}, {"referenceID": 14, "context": "The existing non-Bayesian learning algorithms [10,11,13,14,15,16,17,18] are not robust to Byzantine agents, since the malicious messages sent by the Byzantine agents are indiscriminatingly utilized in the local belief updates.", "startOffset": 46, "endOffset": 71}, {"referenceID": 15, "context": "The existing non-Bayesian learning algorithms [10,11,13,14,15,16,17,18] are not robust to Byzantine agents, since the malicious messages sent by the Byzantine agents are indiscriminatingly utilized in the local belief updates.", "startOffset": 46, "endOffset": 71}, {"referenceID": 16, "context": "The existing non-Bayesian learning algorithms [10,11,13,14,15,16,17,18] are not robust to Byzantine agents, since the malicious messages sent by the Byzantine agents are indiscriminatingly utilized in the local belief updates.", "startOffset": 46, "endOffset": 71}, {"referenceID": 17, "context": "The existing non-Bayesian learning algorithms [10,11,13,14,15,16,17,18] are not robust to Byzantine agents, since the malicious messages sent by the Byzantine agents are indiscriminatingly utilized in the local belief updates.", "startOffset": 46, "endOffset": 71}, {"referenceID": 13, "context": "In contrast to the existing algorithms [14,11], where only the current private signal is used in the update, our proposed algorithm relies on the cumulative private signals.", "startOffset": 39, "endOffset": 46}, {"referenceID": 10, "context": "In contrast to the existing algorithms [14,11], where only the current private signal is used in the update, our proposed algorithm relies on the cumulative private signals.", "startOffset": 39, "endOffset": 46}, {"referenceID": 9, "context": "Thus, our proposed rule may be of independent interest for the failure-free setting considered in [10,11,13,14,15,16,17,18].", "startOffset": 98, "endOffset": 123}, {"referenceID": 10, "context": "Thus, our proposed rule may be of independent interest for the failure-free setting considered in [10,11,13,14,15,16,17,18].", "startOffset": 98, "endOffset": 123}, {"referenceID": 12, "context": "Thus, our proposed rule may be of independent interest for the failure-free setting considered in [10,11,13,14,15,16,17,18].", "startOffset": 98, "endOffset": 123}, {"referenceID": 13, "context": "Thus, our proposed rule may be of independent interest for the failure-free setting considered in [10,11,13,14,15,16,17,18].", "startOffset": 98, "endOffset": 123}, {"referenceID": 14, "context": "Thus, our proposed rule may be of independent interest for the failure-free setting considered in [10,11,13,14,15,16,17,18].", "startOffset": 98, "endOffset": 123}, {"referenceID": 15, "context": "Thus, our proposed rule may be of independent interest for the failure-free setting considered in [10,11,13,14,15,16,17,18].", "startOffset": 98, "endOffset": 123}, {"referenceID": 16, "context": "Thus, our proposed rule may be of independent interest for the failure-free setting considered in [10,11,13,14,15,16,17,18].", "startOffset": 98, "endOffset": 123}, {"referenceID": 17, "context": "Thus, our proposed rule may be of independent interest for the failure-free setting considered in [10,11,13,14,15,16,17,18].", "startOffset": 98, "endOffset": 123}, {"referenceID": 3, "context": "2 Problem Formulation Network Model: Our network model is similar to the model used in [4,30].", "startOffset": 87, "endOffset": 93}, {"referenceID": 29, "context": "2 Problem Formulation Network Model: Our network model is similar to the model used in [4,30].", "startOffset": 87, "endOffset": 93}, {"referenceID": 11, "context": "The faulty agents may collaborate with each other adaptively [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "Observation Model: Our observation model is identical the model used in [3,11,18].", "startOffset": 72, "endOffset": 81}, {"referenceID": 10, "context": "Observation Model: Our observation model is identical the model used in [3,11,18].", "startOffset": 72, "endOffset": 81}, {"referenceID": 17, "context": "Observation Model: Our observation model is identical the model used in [3,11,18].", "startOffset": 72, "endOffset": 81}, {"referenceID": 24, "context": "Byzantine consensus has attracted significant amount of attention [25,26,29,27,30,28,31].", "startOffset": 66, "endOffset": 88}, {"referenceID": 25, "context": "Byzantine consensus has attracted significant amount of attention [25,26,29,27,30,28,31].", "startOffset": 66, "endOffset": 88}, {"referenceID": 28, "context": "Byzantine consensus has attracted significant amount of attention [25,26,29,27,30,28,31].", "startOffset": 66, "endOffset": 88}, {"referenceID": 26, "context": "Byzantine consensus has attracted significant amount of attention [25,26,29,27,30,28,31].", "startOffset": 66, "endOffset": 88}, {"referenceID": 29, "context": "Byzantine consensus has attracted significant amount of attention [25,26,29,27,30,28,31].", "startOffset": 66, "endOffset": 88}, {"referenceID": 27, "context": "Byzantine consensus has attracted significant amount of attention [25,26,29,27,30,28,31].", "startOffset": 66, "endOffset": 88}, {"referenceID": 30, "context": "Byzantine consensus has attracted significant amount of attention [25,26,29,27,30,28,31].", "startOffset": 66, "endOffset": 88}, {"referenceID": 30, "context": "While the past work mostly focus on scalar inputs, the more general vector (or multi-dimensional) inputs have been studied recently [31,29,28].", "startOffset": 132, "endOffset": 142}, {"referenceID": 28, "context": "While the past work mostly focus on scalar inputs, the more general vector (or multi-dimensional) inputs have been studied recently [31,29,28].", "startOffset": 132, "endOffset": 142}, {"referenceID": 27, "context": "While the past work mostly focus on scalar inputs, the more general vector (or multi-dimensional) inputs have been studied recently [31,29,28].", "startOffset": 132, "endOffset": 142}, {"referenceID": 30, "context": "Complete communication networks are considered in [31,29], where tight conditions on the number of agents are identified.", "startOffset": 50, "endOffset": 57}, {"referenceID": 28, "context": "Complete communication networks are considered in [31,29], where tight conditions on the number of agents are identified.", "startOffset": 50, "endOffset": 57}, {"referenceID": 27, "context": "Incomplete communication networks are studied in [28].", "startOffset": 49, "endOffset": 53}, {"referenceID": 27, "context": "In particular, our learning algorithms build upon Byz-Iter algorithm proposed in [28] and a simple algorithm proposed in [30] for iterative Byzantine consensus with vector inputs and scalar inputs, respectively, in incomplete networks.", "startOffset": 81, "endOffset": 85}, {"referenceID": 29, "context": "In particular, our learning algorithms build upon Byz-Iter algorithm proposed in [28] and a simple algorithm proposed in [30] for iterative Byzantine consensus with vector inputs and scalar inputs, respectively, in incomplete networks.", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "A matrix representation of the non-faulty agents\u2019 states evolution under Byz-Iter algorithm is provided by [28], which also captures the dynamics of the simple algorithm with scalar inputs in [30].", "startOffset": 107, "endOffset": 111}, {"referenceID": 29, "context": "A matrix representation of the non-faulty agents\u2019 states evolution under Byz-Iter algorithm is provided by [28], which also captures the dynamics of the simple algorithm with scalar inputs in [30].", "startOffset": 192, "endOffset": 196}, {"referenceID": 27, "context": "1 Algorithm Byz-Iter [28] Algorithm Byz-Iter is based on Tverberg\u2019s Theorem [32].", "startOffset": 21, "endOffset": 25}, {"referenceID": 31, "context": "1 Algorithm Byz-Iter [28] Algorithm Byz-Iter is based on Tverberg\u2019s Theorem [32].", "startOffset": 76, "endOffset": 80}, {"referenceID": 31, "context": "[32] Let f be a nonnegative integer.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Algorithm 2: Algorithm Byz-Iter [28]: t-th iteration at agent i 1 v t \u2190 One-Iter(v t\u22121); Remark 1.", "startOffset": 32, "endOffset": 36}, {"referenceID": 27, "context": "2 Correctness of Algorithm Byz-Iter We briefly summarize the aspects of correctness proof of Algorithm 2 from [28] that are necessary for our subsequent discussion.", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "[28] shows that the effective communication network thus obtained can be characterized by a \u201creduced graph\u201d of G(V, E), defined below.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Assumption 1 below states a condition that is sufficient for reaching approximate Byzantine vector consensus using Algorithm 1 [28].", "startOffset": 127, "endOffset": 131}, {"referenceID": 27, "context": "[28] Suppose Assumption 1 holds for a given m \u2265 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "3 Matrix Representation [28] Let |F| = \u03c6 (thus, 0 \u2264 \u03c6 \u2264 f).", "startOffset": 24, "endOffset": 28}, {"referenceID": 27, "context": "[28] Suppose Assumption 1 holds for a given m \u2265 1.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "Using prior work on coefficients of ergodicity [9], under Assumption 1, it has been shown [28,19] that lim t\u2265r, t\u2192\u221e \u03a6(t, r) = 1\u03c0(r), (3) where \u03c0(r) \u2208 Rn\u2212\u03c6 is a row stochastic vector, and 1 is the column vector with each entry being 1.", "startOffset": 47, "endOffset": 50}, {"referenceID": 27, "context": "Using prior work on coefficients of ergodicity [9], under Assumption 1, it has been shown [28,19] that lim t\u2265r, t\u2192\u221e \u03a6(t, r) = 1\u03c0(r), (3) where \u03c0(r) \u2208 Rn\u2212\u03c6 is a row stochastic vector, and 1 is the column vector with each entry being 1.", "startOffset": 90, "endOffset": 97}, {"referenceID": 18, "context": "Using prior work on coefficients of ergodicity [9], under Assumption 1, it has been shown [28,19] that lim t\u2265r, t\u2192\u221e \u03a6(t, r) = 1\u03c0(r), (3) where \u03c0(r) \u2208 Rn\u2212\u03c6 is a row stochastic vector, and 1 is the column vector with each entry being 1.", "startOffset": 90, "endOffset": 97}, {"referenceID": 27, "context": "[28] For all t \u2265 r \u2265 1, it holds that |\u03a6ij(t, r)\u2212 \u03c0j(r)| \u2264 (1 \u2212 \u03b2 m) t\u2212r+1 \u03bd \u2309, where \u03bd , \u03c7m(n\u2212 \u03c6).", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The next lemma is a consequence of the results in [28].", "startOffset": 50, "endOffset": 54}, {"referenceID": 27, "context": "[28] For any r \u2265 1, there exists a reduced graph H[r] with source component Sr such that \u03c0i(r) \u2265 \u03b2\u03c7m(n\u2212\u03c6) m for each i \u2208 Sr.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": ", the inputs provided at individual non-faulty agents are scalars) it has been shown [30] that Assumption 1 is also necessary.", "startOffset": 85, "endOffset": 89}, {"referenceID": 29, "context": "[30] For scalar inputs, iterative approximate Byzantine consensus is achievable among non-faulty agents if and only if every 1-dimensional reduced graph of G(V, E) contains only one source component.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "8 Algorithm 3: Algorithm Scalar Byzantine Consensus: iteration t \u2265 1 [30] 1 Transmit vi[t\u2212 1] on all outgoing links; 2 Receive messages on all incoming links.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "BFL is a modified version of the geometric averaging update rule that has been investigated in previous work [14,15,11,17].", "startOffset": 109, "endOffset": 122}, {"referenceID": 14, "context": "BFL is a modified version of the geometric averaging update rule that has been investigated in previous work [14,15,11,17].", "startOffset": 109, "endOffset": 122}, {"referenceID": 10, "context": "BFL is a modified version of the geometric averaging update rule that has been investigated in previous work [14,15,11,17].", "startOffset": 109, "endOffset": 122}, {"referenceID": 16, "context": "BFL is a modified version of the geometric averaging update rule that has been investigated in previous work [14,15,11,17].", "startOffset": 109, "endOffset": 122}, {"referenceID": 13, "context": "Algorithm 4: BFL: Iteration t \u2265 1 at agent i 1 \u03b7 t \u2190 One-Iter(log \u03bct\u22121); 2 Observe st; 3 for \u03b8 \u2208 \u0398 do 4 li(s i 1,t|\u03b8) \u2190 li(st|\u03b8) li(s1,t\u22121|\u03b8); 5 \u03bct(\u03b8) \u2190 li(s1,t|\u03b8) exp(\u03b7 t(\u03b8)) \u2211m p=1 li(s i 1,t|\u03b8p) exp(\u03b7 t(\u03b8p)) ; 6 end The main difference of Algorithm 4 with respect to the algorithms in [14,15,11,17] is that (i) our algorithm uses a Byzantine consensus iteration as a primitive (in line 1), and (ii) li(s i 1,t|\u03b8) used in line 5 is the likelihood for observations from iteration 1 to t (the previous algorithms instead use li(s i t|\u03b8) here).", "startOffset": 288, "endOffset": 301}, {"referenceID": 14, "context": "Algorithm 4: BFL: Iteration t \u2265 1 at agent i 1 \u03b7 t \u2190 One-Iter(log \u03bct\u22121); 2 Observe st; 3 for \u03b8 \u2208 \u0398 do 4 li(s i 1,t|\u03b8) \u2190 li(st|\u03b8) li(s1,t\u22121|\u03b8); 5 \u03bct(\u03b8) \u2190 li(s1,t|\u03b8) exp(\u03b7 t(\u03b8)) \u2211m p=1 li(s i 1,t|\u03b8p) exp(\u03b7 t(\u03b8p)) ; 6 end The main difference of Algorithm 4 with respect to the algorithms in [14,15,11,17] is that (i) our algorithm uses a Byzantine consensus iteration as a primitive (in line 1), and (ii) li(s i 1,t|\u03b8) used in line 5 is the likelihood for observations from iteration 1 to t (the previous algorithms instead use li(s i t|\u03b8) here).", "startOffset": 288, "endOffset": 301}, {"referenceID": 10, "context": "Algorithm 4: BFL: Iteration t \u2265 1 at agent i 1 \u03b7 t \u2190 One-Iter(log \u03bct\u22121); 2 Observe st; 3 for \u03b8 \u2208 \u0398 do 4 li(s i 1,t|\u03b8) \u2190 li(st|\u03b8) li(s1,t\u22121|\u03b8); 5 \u03bct(\u03b8) \u2190 li(s1,t|\u03b8) exp(\u03b7 t(\u03b8)) \u2211m p=1 li(s i 1,t|\u03b8p) exp(\u03b7 t(\u03b8p)) ; 6 end The main difference of Algorithm 4 with respect to the algorithms in [14,15,11,17] is that (i) our algorithm uses a Byzantine consensus iteration as a primitive (in line 1), and (ii) li(s i 1,t|\u03b8) used in line 5 is the likelihood for observations from iteration 1 to t (the previous algorithms instead use li(s i t|\u03b8) here).", "startOffset": 288, "endOffset": 301}, {"referenceID": 16, "context": "Algorithm 4: BFL: Iteration t \u2265 1 at agent i 1 \u03b7 t \u2190 One-Iter(log \u03bct\u22121); 2 Observe st; 3 for \u03b8 \u2208 \u0398 do 4 li(s i 1,t|\u03b8) \u2190 li(st|\u03b8) li(s1,t\u22121|\u03b8); 5 \u03bct(\u03b8) \u2190 li(s1,t|\u03b8) exp(\u03b7 t(\u03b8)) \u2211m p=1 li(s i 1,t|\u03b8p) exp(\u03b7 t(\u03b8p)) ; 6 end The main difference of Algorithm 4 with respect to the algorithms in [14,15,11,17] is that (i) our algorithm uses a Byzantine consensus iteration as a primitive (in line 1), and (ii) li(s i 1,t|\u03b8) used in line 5 is the likelihood for observations from iteration 1 to t (the previous algorithms instead use li(s i t|\u03b8) here).", "startOffset": 288, "endOffset": 301}, {"referenceID": 2, "context": "1 Identifiability In the absence of agent failures [3], for the networked agents to detect the true hypothesis \u03b8\u2217, it is sufficient to assume that G(V, E) is strongly connected, and that \u03b8\u2217 is globally identifiable.", "startOffset": 51, "endOffset": 54}, {"referenceID": 13, "context": "2 Convergence Results Our proof parallels the structure of a proof in [14], but with some key differences to take into account our update rule for the belief vector.", "startOffset": 70, "endOffset": 74}, {"referenceID": 13, "context": "(16) As it can be seen later, the proof of Lemma 3 is significantly different from the analogous lemma in [14].", "startOffset": 106, "endOffset": 110}, {"referenceID": 13, "context": "Our convergence proof has similar structure as the analysis in [14].", "startOffset": 63, "endOffset": 67}, {"referenceID": 13, "context": "Indeed, the BFL for f = 0 is a simple modification of the algorithm proposed in [14].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "As mentioned before, the non-Bayesian learning rules [14,15,11,17] are consensus-based learning algorithms, wherein agents are required to reach a common decision asymptotically.", "startOffset": 53, "endOffset": 66}, {"referenceID": 14, "context": "As mentioned before, the non-Bayesian learning rules [14,15,11,17] are consensus-based learning algorithms, wherein agents are required to reach a common decision asymptotically.", "startOffset": 53, "endOffset": 66}, {"referenceID": 10, "context": "As mentioned before, the non-Bayesian learning rules [14,15,11,17] are consensus-based learning algorithms, wherein agents are required to reach a common decision asymptotically.", "startOffset": 53, "endOffset": 66}, {"referenceID": 16, "context": "As mentioned before, the non-Bayesian learning rules [14,15,11,17] are consensus-based learning algorithms, wherein agents are required to reach a common decision asymptotically.", "startOffset": 53, "endOffset": 66}, {"referenceID": 13, "context": "Our proof is similar to the proof presented in [14,17].", "startOffset": 47, "endOffset": 54}, {"referenceID": 16, "context": "Our proof is similar to the proof presented in [14,17].", "startOffset": 47, "endOffset": 54}, {"referenceID": 13, "context": "Similar to [14,17], we also use McDiarmid\u2019s Inequality.", "startOffset": 11, "endOffset": 18}, {"referenceID": 16, "context": "Similar to [14,17], we also use McDiarmid\u2019s Inequality.", "startOffset": 11, "endOffset": 18}, {"referenceID": 13, "context": "Our analysis for the special when f = 0 also works for time-varying networks [14].", "startOffset": 77, "endOffset": 81}, {"referenceID": 19, "context": "By [20], we know that for each pair of hypotheses \u03b81 and \u03b82, there exists a row-stochastic matrix M1,2[t] \u2208 R(n\u2212\u03c6)\u00d7(n\u2212\u03c6) such that r t(\u03b81, \u03b82) = n\u2212\u03c6 \u2211", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "Abstract This paper addresses the problem of non-Bayesian learning over multi-agent networks, where agents repeatedly collect partially informative observations about an unknown state of the world, and try to collaboratively learn the true state. We focus on the impact of the adversarial agents on the performance of consensus-based non-Bayesian learning, where non-faulty agents combine local learning updates with consensus primitives. In particular, we consider the scenario where an unknown subset of agents suffer Byzantine faults \u2013 agents suffering Byzantine faults behave arbitrarily.", "creator": "LaTeX with hyperref package"}}}