{"id": "1606.08928", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs", "abstract": "In this paper, we present subgraph2vec, a novel approach for learning latent representations of rooted subgraphs from large graphs inspired by recent advancements in Deep Learning and Graph Kernels. These latent representations encode semantic substructure dependencies in a continuous vector space, which is easily exploited by statistical models for tasks such as graph classification, clustering, link prediction and community detection. subgraph2vec leverages on local information obtained from neighbourhoods of nodes to learn their latent representations in an unsupervised fashion. We demonstrate that subgraph vectors learnt by our approach could be used in conjunction with classifiers such as CNNs, SVMs and relational data clustering algorithms to achieve significantly superior accuracies. Also, we show that the subgraph vectors could be used for building a deep learning variant of Weisfeiler-Lehman graph kernel. Our experiments on several benchmark and large-scale real-world datasets reveal that subgraph2vec achieves significant improvements in accuracies over existing graph kernels on both supervised and unsupervised learning tasks. Specifically, on two realworld program analysis tasks, namely, code clone and malware detection, subgraph2vec outperforms state-of-the-art kernels by more than 17% and 4%, respectively.", "histories": [["v1", "Wed, 29 Jun 2016 01:05:36 GMT  (137kb,D)", "http://arxiv.org/abs/1606.08928v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CR cs.SE", "authors": ["annamalai narayanan", "mahinthan chandramohan", "lihui chen", "yang liu", "santhoshkumar saminathan"], "accepted": false, "id": "1606.08928"}, "pdf": {"name": "1606.08928.pdf", "metadata": {"source": "CRF", "title": "subgraph2vec: Learning Distributed Representations of Rooted Sub-graphs from Large Graphs", "authors": ["Annamalai Narayanan", "Mahinthan Chandramohan", "Lihui Chen", "Yang Liu", "Santhoshkumar Saminathan"], "emails": ["annamala002@e.ntu.edu.sg,", "mahinthan@ntu.edu.sg,", "elhchen@ntu.edu.sg,", "yangliu@ntu.edu.sg,", "santhosh.kumar@yahoo.com"], "sections": [{"heading": null, "text": "Keywords Graph Kernel, Deep Learning, Representation Learning"}, {"heading": "1. INTRODUCTION", "text": "Graphs provide a rich, generic, and natural way to present structured data. In domains such as computer biology, chemoinformatics, social networking analysis, and program analysis, we are often interested in calculating similarities between graphs to take into account domain-specific applications such as protein function prediction, drug toxicity prediction, and malware detection. Graph kernels are one of the popu-ACM ISBN 978-1-4503-2138-9.DOI: 10.1145 / 1235lar and widely used approaches to measure similarities between graphs [3, 4, 6, 7, 14]. A graph kernel measures the similarity between a pair of graphs by recursively splitting them into atomic substructures (e.g. aisle [3], shortest paths [4], graphs [7], etc.) and defining a similarity function over graphs (e.g. substructures)."}, {"heading": "1.1 Limitations of Existing Graph Kernels", "text": "(L1) Substructure similarity. Substructures used to calculate the kernel matrix are not independent. To illustrate this, consider the Weisfeiler-Lehman (WL) ar Xiv: 160 6.08 928v 1 [cs.L G] 29 Jun 2016kernel [6], which splits graphs into rooted substructures. (These subgraphs, however, include the neighborhood of a certain degree around the root node. Understandably, these subgraphs have strong relationships with each other. That is, a subgraph with second-degree neighbors of the root node could be achieved by adding some nodes and edges to its first-degree counterpart. We explain this with an example shown in Figure 1. The figure illustrates APIdependency subgraphs from a well-known Android malware called DroidKungFu (Db). (DF) [18] Subgraph."}, {"heading": "1.2 Existing Solution: Deep Graph Kernels", "text": "To mitigate these issues, Yanardag and Vishwanathan [7], recently proposed alternative kernel phrases designated as Deep Graph Kernel (DGK), were used. In contrast to eq. (1), DGK captures the similarities between the substructures with the following phrases: K (G), (G), (2), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), (S), ((S) (() () () () () () () () () () () (()) (()) ((())) ((())) (((()))) (((())))) (((()))) (((()))) (((())))) (((()))) ((())) ((())))) (((())))) (\"(\" (\"(\" (\"(\"), \"(\" (\"(\" (), \"(\" (), \"(\" (), \"(\" ((), \"(((),\" (\"(((),\" () ((((()) (((((())) (((((((((()))))) ((((((((((((((()))))))) ((((((((((((((((()))))))))))) (((((((((((((((\")))))))))) (((((\" ((((((\"((\" (((\"((\" ((\"((\" (\")))))), ((\" (((\"(\" ("}, {"heading": "1.3 Our Approach", "text": "We have made two major contributions through our subgraph2vec framework to solve these problems: \u2022 We extend the WL renaming strategy [6] (which is used to relativize the nodes in a graph that covers its width first) to define an appropriate context for a given subgraph. \u2022 We consider all entrenched subgraphs (to some extent) as neighbors of r as context of sg (d) r."}, {"heading": "2. RELATED WORK", "text": "The work closest to our paper is Deep Graph Kernels [7]. As we discussed it in detail in paragraph 1, we will refrain from discussing it here. Recently, there has been considerable interest in the research community learning representations of nodes and other substructures from graphs. We list the most prominent such work in Table 1 and show how our work compares in principle with them. Deep Walk [8] and node2vec [10] intend to learn node embeddings by generating random walks in a single graph. Both work rely on the existence of node labels for at least a small portion of the nodes and adopt a semi-supervised approach to learn node embeddings. Recently proposed Patchy-san [9] learns node and subgraph embeddings from a supervised evolutionary neural network level (CNN-based approach to learn the core embeddings), as opposed to three subgraph embeddings."}, {"heading": "3. PROBLEM STATEMENT", "text": "Formally, G = (V, E, \u03bb) is considered a graph in which V is a set of nodes and E (V \u00b7 V) a set of edges. Graph G is labeled 2 if there is a function that \u03bb: V \u2192 \"that assigns a unique alphabet label to each node v \u00b2 V. Given G = (V, E, \u03bb) and sg = (Vsg, Esg, \u03bbsg), sg is a sub-graph of G iff, there is an injective mapping: Vsg \u2192 V such that (v1, v2)\" Esg iff (v1) and sg = (Vsg, Esg, \u03bbsg). \"Given a set of graphs G = {G1, G2,..., Gn} and a positive integer D, we intend to extract a vocabulary of all (uprooted) representations."}, {"heading": "4. BACKGROUND: LANGUAGE MODELS", "text": "Our next goal is to learn the distributed representations of subgraphs that extend the recently proposed methods of learning representation and language modeling for multirelational data. In this section, we will review the associated background in language modeling. Traditional language models. Considering a corpus, traditional language models determine the likelihood that a sequence of words will appear in it. For example, for a word sequence {w1, w2,..., wT}, n-gram language models2For graphs without a node designation, we will follow the procedure and label nodes mentioned in [6] with their origin. Goals to maximize the following probability: Pr (wt | w1,..., wt \u2212 1) (3) Meaning, they estimate the probability of observing the target word if n previous words (w1,..., wt \u2212 1) are given. The goal of these models is to exploit the target word c."}, {"heading": "4.1 Skip Gram", "text": "The skipgram model maximizes the probability of simultaneous occurrence between words appearing within a given context window. Specify a context window of size c and the target word wt, the skipgram model attempts to predict the words appearing in the context of the target word (wt \u2212 c,..., wt \u2212 c). Specifically, the goal of the skipgram model is to maximize the following log probability, T \u2211 t = 1 log Pr (wt \u2212 c,..., wt + c | wt) (5), with the probability Pr (wt \u2212 c,..., wt + c) being calculated as independent. Furthermore, Pr (wt + j | wt) is defined as: exp (wt \u2212 c), j 6 = 0Pr (wt + j | wt) (6) Here the contextual words and the current word are assumed to be independent."}, {"heading": "4.2 Negative Sampling", "text": "The posterior probability in eq. (6) could be learned in several ways. For example, a beginner's approach is the use of a classifier such as logistic regression, which is prohibitive if the vocabulary is very extensive. Negative sampling is an efficient algorithm that is used to alleviate this problem and train the skipgram model. Negative sampling selects the words that are not randomly in the context rather than taking into account all the words in the vocabulary. In other words, if a word appears in the context of another word w, then the vector of embedding w \u00b2 is closer to that of w \u00b2 compared to any other randomly selected word in the vocabulary. Once the skipgram training converges, semantically similar words are taken into account to closer positions in the embedding of w \u00b2 algorithm 1: subgraph2vec \u00b2 algorithm (G, D, \u03b4, E) input: G = Gk, Gk =... Gi = Daren."}, {"heading": "5. METHOD: LEARNING SUB-GRAPH REPRESENTATIONS", "text": "In this section we discuss the main components of our subgraph2vec algorithm (\u00a7 5.2), how it enables a deep learning variant of the WL kernel (\u00a7 5.3) and some of its use cases in detail (\u00a7 5.4)."}, {"heading": "5.1 Overview", "text": "Similar to the convention on language modeling, the only input required is a corpus and vocabulary of subgraphs for subgraph2vec to learn representations. Faced with a dataset of graphs, subgraph2vec considers all neighborhoods of rooted subgraphs around each rooted subgraph (to some extent) to be its corpus and a set of all rooted subgraphs around each node in each graph as its vocabulary. Subsequently, after the language modeling process with the subgraphs and their contexts, subgraph2vec learns the intended subgraph embeddings."}, {"heading": "5.2 Algorithm: subgraph2vec", "text": "The algorithm consists of two main components: firstly, a procedure for creating rooted subgraphs around each node in a given graph (\u00a7 5.2.1) and secondly, the procedure for embedding these subgraphs (\u00a7 5.2.2). As shown in Algorithm 1, we intend to learn the three-dimensional embedding of subgraphs (up to degree D) from all graphs in record G in e-epochs. We begin with the construction of a vocabulary of all subgraphs, SGvocab (line 2) (using algorithm 2: GetWLSubgraph (v, G, d) Input: v: Node, which is the root of the subgraph G = (V, E, \u03bb): Graph, from which the subgraph must be extracted d: degree of neighbors to be taken into account for extracting subgraphs: sg (d): sv: (verted root of the subgraph by {d = 1 if) the node of the grade (d = 1)"}, {"heading": "4 else", "text": "5 Nv: = {v, v, v, v, E} 6 M (d) v: = {GetWLSubgraph (v, G, d \u2212 1) | v, g (d) v: = sg (d) v [GetWLSubgraph (v, G, d \u2212 1) [M (d) v] 8 return sg (d) vAlgorithm 3: RadialSkipGram (\u03a6, sg (d) v, G, D]"}, {"heading": "1 begin", "text": "In fact, it is as if most people are able to understand themselves and understand what they are doing. (...) It is not as if they are able to understand themselves. (...) It is not as if they are able to understand themselves. (...) It is not as if they are able to understand themselves. (...) It is as if they are able to understand themselves. (...) It is not as if they are able to understand themselves. (...) It is not as if they are able to understand themselves. (...) It is not as if they are able to understand themselves. (...) It is as if they are able. (...) It is as if they are able to understand themselves. (...) It is not as if they are able. (...) It is not as if they are able. (...) It is not as if they are in the world. (...) It is not as if they are in the world. (...)"}, {"heading": "5.3 Relation to Deep WL kernel", "text": "As already mentioned, each subgraph in SGvocab is determined using the WL labeling strategy and thus represents the WL neighborhood labels of a node. Therefore, learning latent representations of such subgraphs amounts to learning WL neighborhood labels. Thus, once the embedding of all subgraphs in SGvocab is learned using algorithm 1, one could build the deep learning variant of the WL core between the graphs in G. For example, we could calculate the M matrix in such a way that each entry Mij is calculated as < \u03a6i, \u03a6j >, where \u03a6i corresponds to the learned B-dimensional embedding of the subgraph i (or \u03a6j). Thus, MatrixM represents nothing else than the paired similarities of all substructures used by the WL core. Therefore, matrix M could be inserted directly into eq. (2) to get to the deep WL graph across the entire G kernel."}, {"heading": "5.4 Use cases", "text": "Once we calculate the embedding of the subgraphs, they could be used in several practical applications. Here, we list some prominent use cases: (1) Graph classification. Given G, a series of graphs, and Y, the set of corresponding class names, graph classification is the task in which we learn a model H, so that H: G \u2192 Y. To this end, one could feed the embedding of subgraph2vec into a deep learning classifier such as CNN (as in [9]) to learn H. Alternatively, however, one could follow a kernel-based classification. That is, one could achieve a deep WL kernel by discussing the embedding of the subgraphs as in \u00a7 5.3, and use a nuclearized learning algorithm such as SVM to perform the classification. (2) Graph clustering. Given G, the task in clustering is to group similar graphs (noc)."}, {"heading": "6. EVALUATION", "text": "We evaluate the accuracy and efficiency of subgraph2vec for both supervised and unsupervised learning tasks. In addition to experimenting with benchmark data sets, we also evaluate subgraph2vec for real-world program analysis tasks such as malware and code clone detection for large-scale Android malware and clone data sets. In particular, we intend to answer the following research questions: (1) How does subgraph2vec compare existing graph classification tasks in terms of accuracy and efficiency for benchmark data sets, (2) How does subgraph2vec compare itself to state graph cores for a real-world, unsupervised learning task, namely code cloning detection (3) How does subgraph2vec compare modern graph cores with a real-world, supervised learning task, namely malware detectory.All experiments were conducted on a server with 36 CPPU cores (black-of-the-art processor NVIGUGT4-14GB)."}, {"heading": "6.1 Classification on benchmark datasets", "text": "This year it has come to the point where it will be able to put itself at the top of the leaderboard, \"he said in an interview with the Deutsche Presse-Agentur.\" We have to put ourselves at the top, \"he said.\" We are able to change the world, \"he said.\" We have to be able to change the world. \""}, {"heading": "6.2 Clone Detection", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6.3 Malware Detection", "text": "In the case of the Android platform, many existing works such as [11] benign and malware apps collected from 2010 to 2012. We collected 5000 benign top-selling apps from Google Play [2], which were released around the same time and which, together with the Drebin apps, trained the malware detection model. We refer to this collection of data as Train10K. To evaluate the performance of the model, we use a newer set of 5000 malware samples from Google Play [3] that were collected from 2010 to 2014 by virus-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-scripts-"}, {"heading": "7. CONCLUSION", "text": "In this paper, we introduced subgraph2vec, an unattended learning method for embedding entrenched subgraphs that exist in large graphs. By conducting large-scale experiments with benchmark and real graph classification and cluster data sets, we show that Subgraph2vec embeddings learned through our approach can be used in conjunction with classifiers such as CNNs, SVMs, and cluster algorithms to achieve significantly superior accuracies. In real-world applications with large graphs, subgraph2vec significantly outperforms modern graph cores without compromising overall performance efficiency. We provide all the code and data used in this work at: https: / / sites.google.com / site / subgraph2vec."}, {"heading": "8. REFERENCES", "text": "[1] Virus Share Malware dataset: http: / / virusshare.com [2] Google Play Store: https: / / play.google.com / store [3] Vishwanathan, S. V. N., et al. \"Graph kernels.\" The Journal of Machine Learning Research 11 (2010): 1201-1242. [4] Borgwardt, Karsten M., and Hans-Peter Kriegel. \"Shortest-path kernels on graphs.\" Data Mining, Fifth IEEE International Conference on. IEEE, 2005. [5] Shervashidze, et al. \"Efficient graphlet kernels for large graph comparison.\" [6] Shervashidze, et al. \"Weisfeiler-graph kernels.\" \"The Journal of Machine Learning Research 12 (2011): 2539-2561."}], "references": [{"title": "Graph kernels.", "author": ["Vishwanathan", "S.V. N"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Shortest-path kernels on graphs.", "author": ["Borgwardt", "Karsten M", "Hans-Peter Kriegel"], "venue": "Data Mining, Fifth IEEE International Conference on. IEEE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Efficient graphlet kernels for large graph comparison.", "author": ["Shervashidze", "Nino"], "venue": "International conference on artificial intelligence and statistics", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Weisfeiler-lehman graph kernels.", "author": ["Shervashidze", "Nino"], "venue": "The Journal of Machine Learning Research", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Deep graph kernels.", "author": ["Yanardag", "Pinar", "S.V.N. Vishwanathan"], "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Deepwalk: Online learning of social representations.", "author": ["Perozzi", "Bryan"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning Convolutional Neural Networks for Graphs.", "author": ["Niepert", "Mathias"], "venue": "Proceedings of the 33rd annual international conference on machine learning", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "node2vec: Scalable Feature Learning for Networks.", "author": ["Grover", "Aditya", "Leskovec", "Jure"], "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Contextual Weisfeiler-Lehman Graph Kernel For Malware Detection.", "author": ["Narayanan", "Annamalai"], "venue": "The 2016 International Joint Conference on Neural Networks (IJCNN)", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Cyclic pattern kernels for predictive graph mining", "author": ["Horv\u00e1th", "Tam\u00e1s"], "venue": "In Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2004}, {"title": "Graph kernel benchmark data sets are trivial!", "author": ["Orlova", "Yuliia"], "venue": "ICML Workshop on Features and Structures FEATS", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "A Structural Smoothing Framework For Robust Graph Comparison.", "author": ["Yanardag", "Pinar", "S.V.N. Vishwanathan"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Efficient estimation of word representations in vector space.", "author": ["Mikolov", "Tomas"], "venue": "ICLR Workshop,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Clustering by passing messages between data points.", "author": ["Frey", "Brendan J", "Delbert Dueck"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Achieving accuracy and scalability simultaneously in detecting application clones on android markets.", "author": ["Chen", "Kai"], "venue": "Proceedings of the 36th International Conference on Software Engineering", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Drebin: Effective and explainable detection of android malware in your pocket.", "author": ["Arp", "Daniel"], "venue": "Proceedings of the Annual Symposium on Network and Distributed System Security (NDSS)", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "lar and widely adopted approaches to measure similarities among graphs [3, 4, 6, 7, 14].", "startOffset": 71, "endOffset": 87}, {"referenceID": 1, "context": "lar and widely adopted approaches to measure similarities among graphs [3, 4, 6, 7, 14].", "startOffset": 71, "endOffset": 87}, {"referenceID": 3, "context": "lar and widely adopted approaches to measure similarities among graphs [3, 4, 6, 7, 14].", "startOffset": 71, "endOffset": 87}, {"referenceID": 4, "context": "lar and widely adopted approaches to measure similarities among graphs [3, 4, 6, 7, 14].", "startOffset": 71, "endOffset": 87}, {"referenceID": 11, "context": "lar and widely adopted approaches to measure similarities among graphs [3, 4, 6, 7, 14].", "startOffset": 71, "endOffset": 87}, {"referenceID": 0, "context": ", walk [3], shortest paths [4], graphlets [7] etc.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": ", walk [3], shortest paths [4], graphlets [7] etc.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": ", walk [3], shortest paths [4], graphlets [7] etc.", "startOffset": 42, "endOffset": 45}, {"referenceID": 4, "context": "However, as noted in [7, 14], the representation in eq.", "startOffset": 21, "endOffset": 28}, {"referenceID": 11, "context": "However, as noted in [7, 14], the representation in eq.", "startOffset": 21, "endOffset": 28}, {"referenceID": 3, "context": "kernel [6] which decomposes graphs into rooted subgraphs.", "startOffset": 7, "endOffset": 10}, {"referenceID": 15, "context": "The figure illustrates APIdependency subgraphs from a well-known Android malware called DroidKungFu (DKF) [18].", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "To alleviate these problems Yanardag and Vishwanathan [7], recently proposed an alternative kernel formulation termed as Deep Graph Kernel (DGK).", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "In DGK [7], the authors used representation learning (deep learning) techniques inspired by the work of Mikolov et al.", "startOffset": 7, "endOffset": 10}, {"referenceID": 12, "context": "[15] to learn vector representations (aka embeddings) of substructures.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "In order to facilitate unsupervised representation learning on graph substructures, the authors of [7] defined a notion of context among these substructures.", "startOffset": 99, "endOffset": 102}, {"referenceID": 4, "context": "} is a multi-set of all degree d subgraphs in graph G, [7] assumes that any two subgraphs sg (d) i , sg (d)", "startOffset": 55, "endOffset": 58}, {"referenceID": 4, "context": "1, [7] assumes that only subgraphs (a) and (d) are in the same context and are possibly similar as they both are degree-1 subgraphs.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Hence, [7] incorrectly biases them to be dissimilar.", "startOffset": 7, "endOffset": 10}, {"referenceID": 4, "context": "Through our thorough analysis and experiments we observe that these assumptions led [7] to building relatively low quality subgraph embeddings.", "startOffset": 84, "endOffset": 87}, {"referenceID": 4, "context": "Consequently, this reduces the classification and clustering accuracies when [7]\u2019s deep WL kernel is deployed.", "startOffset": 77, "endOffset": 80}, {"referenceID": 3, "context": "\u2022 We extend the WL relabeling strategy [6] (used to relabel the nodes in a graph encompassing its breadth-first neigh-", "startOffset": 39, "endOffset": 42}, {"referenceID": 12, "context": "This renders the existing representation learning models such as the skipgram model [15] (which captures fixed-length linear contexts) unusable in a straight-forward manner to learn the representations of subgraphs using its context, thus formed.", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "\u2022 We develop a modified version of the skipgram language model [15] which is capable of modeling varying length", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "[7]) on graph classification and clustering tasks (\u00a76).", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "The closest work to our paper is Deep Graph Kernels [7].", "startOffset": 52, "endOffset": 55}, {"referenceID": 5, "context": "Deep Walk [8] and node2vec [10] intend to learn node embeddings by generating random walks in a single graph.", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "Deep Walk [8] and node2vec [10] intend to learn node embeddings by generating random walks in a single graph.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Recently proposed Patchy-san [9] learns node and subgraph embeddings using a supervised convolutional neural network (CNN) based approach.", "startOffset": 29, "endOffset": 32}, {"referenceID": 5, "context": "Deep Walk [8] Semi-sup Fixed-length random walks", "startOffset": 10, "endOffset": 13}, {"referenceID": 7, "context": "node2vec [10] Semi-sup Fixed-Length biased random walks", "startOffset": 9, "endOffset": 13}, {"referenceID": 6, "context": "Patchy-san [9] Sup Receptive field of sequence of neighbours of nodes", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "Deep Graph Kernels [7] Unsup Subgraphs occurring at same degree", "startOffset": 19, "endOffset": 22}, {"referenceID": 9, "context": "In general, from a substructure analysis point of view, research on graph kernel could be grouped into three major categories: kernels for limited-size subgraphs [12], kernels based on subtree patterns [6] and kernels based on walks [3] and paths [4].", "startOffset": 162, "endOffset": 166}, {"referenceID": 3, "context": "In general, from a substructure analysis point of view, research on graph kernel could be grouped into three major categories: kernels for limited-size subgraphs [12], kernels based on subtree patterns [6] and kernels based on walks [3] and paths [4].", "startOffset": 202, "endOffset": 205}, {"referenceID": 0, "context": "In general, from a substructure analysis point of view, research on graph kernel could be grouped into three major categories: kernels for limited-size subgraphs [12], kernels based on subtree patterns [6] and kernels based on walks [3] and paths [4].", "startOffset": 233, "endOffset": 236}, {"referenceID": 1, "context": "In general, from a substructure analysis point of view, research on graph kernel could be grouped into three major categories: kernels for limited-size subgraphs [12], kernels based on subtree patterns [6] and kernels based on walks [3] and paths [4].", "startOffset": 247, "endOffset": 250}, {"referenceID": 3, "context": ", wT }, n-gram language model For graphs without node labels, we follow the procedure mentioned in [6] and label nodes with their degree.", "startOffset": 99, "endOffset": 102}, {"referenceID": 12, "context": "Next, we discuss one such a method that we extend in our subgraph2vec framework, namely Skipgram models [15].", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "To extract these subgraphs, we follow the well-known WL relabeling process [6] which lays the basis for the WL kernel and WL test of graph isomorphism [6, 7].", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "To extract these subgraphs, we follow the well-known WL relabeling process [6] which lays the basis for the WL kernel and WL test of graph isomorphism [6, 7].", "startOffset": 151, "endOffset": 157}, {"referenceID": 4, "context": "To extract these subgraphs, we follow the well-known WL relabeling process [6] which lays the basis for the WL kernel and WL test of graph isomorphism [6, 7].", "startOffset": 151, "endOffset": 157}, {"referenceID": 5, "context": "For instance, Deep Walk [8] uses a similar approach to learn a target node\u2019s representation by generating random walks around it.", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "To this end, one could feed subgraph2vec\u2019s embeddings to a deep learning classifier such as CNN (as in [9]) to learn H.", "startOffset": 103, "endOffset": 106}, {"referenceID": 13, "context": "Subsequently, relational data clustering algorithms such as Affinity Propagation (AP) [16] and Hierarchical Clustering could be used to cluster the graphs.", "startOffset": 86, "endOffset": 90}, {"referenceID": 5, "context": "degree 0 are considered, subgraph2vec provides node embeddings similar to Deep Walk [8] and node2vec [10]).", "startOffset": 84, "endOffset": 87}, {"referenceID": 7, "context": "degree 0 are considered, subgraph2vec provides node embeddings similar to Deep Walk [8] and node2vec [10]).", "startOffset": 101, "endOffset": 105}, {"referenceID": 3, "context": "All these datasets are made available in [6, 7].", "startOffset": 41, "endOffset": 47}, {"referenceID": 4, "context": "All these datasets are made available in [6, 7].", "startOffset": 41, "endOffset": 47}, {"referenceID": 3, "context": "We compare subgraph2vec against the WL kernel [6] and Yanardag and Vishwanathan\u2019s formulation of deep WL kernel [7] (denoted as Deep WLYV).", "startOffset": 46, "endOffset": 49}, {"referenceID": 4, "context": "We compare subgraph2vec against the WL kernel [6] and Yanardag and Vishwanathan\u2019s formulation of deep WL kernel [7] (denoted as Deep WLYV).", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "WL [6] 80.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "34 Deep WLYV [7] 82.", "startOffset": 13, "endOffset": 16}, {"referenceID": 14, "context": "Clone260 [17] 260 100 9829.", "startOffset": 9, "endOffset": 13}, {"referenceID": 10, "context": "In fact, by using trivial features such as number of nodes in the graph, [13] achieved comparable ac-", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "Android apps are cloned across different markets by unscrupulous developers for reasons such as stealing advertisement revenue [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 3, "context": "Kernel WL [6] Deep WLYV [7] subgraph2vec Pre-training duration 421.", "startOffset": 10, "endOffset": 13}, {"referenceID": 4, "context": "Kernel WL [6] Deep WLYV [7] subgraph2vec Pre-training duration 421.", "startOffset": 24, "endOffset": 27}, {"referenceID": 14, "context": "We acquired a dataset of 260 apps collected from the authors of a recent clone detection work, 3D-CFG [17].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "ground truth clusters) are identified by the authors of [17].", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Finally, we use AP clustering algorithm [16] over the kernel matrix to obtain clusters of similar ICFGs representing clone apps.", "startOffset": 40, "endOffset": 44}, {"referenceID": 3, "context": "We compare subgraph2vec\u2019s accuracy on the clone detection task against the WL [6] and Deep WLYV [7] kernels.", "startOffset": 78, "endOffset": 81}, {"referenceID": 4, "context": "We compare subgraph2vec\u2019s accuracy on the clone detection task against the WL [6] and Deep WLYV [7] kernels.", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "For more details on app representations, we refer to [11].", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "Train10K Malware Drebin [18] 5600 9590.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "In the case of Android platform, many existing works such as [11], represent benign and malware apps as ICFGS and cast malware detection as a graph classification problem.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Drebin [18] provides a collection of 5,560 Android malware apps collected from 2010 to 2012.", "startOffset": 7, "endOffset": 11}, {"referenceID": 3, "context": "Classifier WL [6] Deep WLYV [7] subgraph2vec Pre-training duration 2631.", "startOffset": 14, "endOffset": 17}, {"referenceID": 4, "context": "Classifier WL [6] Deep WLYV [7] subgraph2vec Pre-training duration 2631.", "startOffset": 28, "endOffset": 31}], "year": 2016, "abstractText": "In this paper, we present subgraph2vec, a novel approach for learning latent representations of rooted subgraphs from large graphs inspired by recent advancements in Deep Learning and Graph Kernels. These latent representations encode semantic substructure dependencies in a continuous vector space, which is easily exploited by statistical models for tasks such as graph classification, clustering, link prediction and community detection. subgraph2vec leverages on local information obtained from neighbourhoods of nodes to learn their latent representations in an unsupervised fashion. We demonstrate that subgraph vectors learnt by our approach could be used in conjunction with classifiers such as CNNs, SVMs and relational data clustering algorithms to achieve significantly superior accuracies. Also, we show that the subgraph vectors could be used for building a deep learning variant of Weisfeiler-Lehman graph kernel. Our experiments on several benchmark and large-scale real-world datasets reveal that subgraph2vec achieves significant improvements in accuracies over existing graph kernels on both supervised and unsupervised learning tasks. Specifically, on two realworld program analysis tasks, namely, code clone and malware detection, subgraph2vec outperforms state-of-the-art kernels by more than 17% and 4%, respectively.", "creator": "LaTeX with hyperref package"}}}