{"id": "1606.08954", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs", "abstract": "We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008--9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.", "histories": [["v1", "Wed, 29 Jun 2016 05:01:56 GMT  (188kb,D)", "http://arxiv.org/abs/1606.08954v1", "13 pages, 5 figures, accepted to CoNLL 2016"]], "COMMENTS": "13 pages, 5 figures, accepted to CoNLL 2016", "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["swabha swayamdipta", "miguel ballesteros", "chris dyer", "noah a smith"], "accepted": false, "id": "1606.08954"}, "pdf": {"name": "1606.08954.pdf", "metadata": {"source": "CRF", "title": "Greedy, Joint Syntactic-Semantic Parsing with Stack LSTMs", "authors": ["Swabha Swayamdipta", "Miguel Ballesteros", "Chris Dyer", "Noah A. Smith"], "emails": ["swabha@cs.cmu.edu,", "miguel.ballesteros@upf.edu,", "cdyer@cs.cmu.edu,", "nasmith@cs.washington.edu"], "sections": [{"heading": null, "text": "Our greedy inference algorithm has linear time, including feature extraction. For CoNLL 2008-9 English Shared Tasks, we achieve the best published parsing performance among models that learn syntax and semantics together."}, {"heading": "1 Introduction", "text": "We introduce a new syntactic and semantic dependency parser. Our parser draws on the algorithmic insights of Henderson et al. \"s incremental structure approach (2008), with two major differences. First, it learns representations of the entire algorithmic state of the parser, not just the top positions on the stack or the most recent parser states; in fact, it does not use any expert-created properties at all. Second, it uses completely greedy conclusions instead of beam searching. We find that it surpasses all previous common parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) shared tasks at CoNLL 2008 and 2009 (English). Our parser multilingual results are comparable to the top systems in CoNLL 2009.Common models like ours."}, {"heading": "2 Joint Syntactic and Semantic Dependency Parsing", "text": "We largely follow the transition-based synchronized algorithm developed by Henderson et al. (2013) to predict common parse structures. Input to the algorithm is a sentence commented on with part-of-speech tags. Output consists of a labeled syntactic dependency tree and a directed SRL graph in which a subset of words in a sentence are selected as predicates, uniquely defined in a particular sense, and linked to their semantic arguments by labeled, directed edges. Figure 1 shows an example."}, {"heading": "2.1 Transition-Based Procedure", "text": "The state of the parsing algorithm is represented by three stack data structures: a syntactic stack St, a semantic stack Mt - each containing partially constructed structures - and a buffer of input words Bt. Our algorithm also places partially syntactic and semantic parse structures on the front of the buffer so that it is also implemented as a stack. Each arc in the output corresponds to a transition (or \"action\") selected on the basis of the current state; each transition changes the state by updating St, Mt, andBt to St + 1, Mt + 1 or Bt + 1, respectively. While each state can license multiple valid actions, each action has a deterministic effect on the state of the algorithm. Originally, S0 and M0 contain the input set with the first word at the front of the root symbol and Bt at the top of the special execution structure ending in a single ending."}, {"heading": "2.2 Transitions for Joint Parsing", "text": "There are separate sets of syntactic and semantic transitions (to allow access to the semantic operations); there are separate sets of syntactic and semantic transitions; the former manipulation S andB, the latterM and B. All are formally defined in Table 1; the syntactic transitions are from Nivre's \"arc-eager\" algorithm (2008); they include: \u2022 S-SHIFT, which pushes an element off the front of B and pushes it to S. \u2022 S-REDUCE pops, an element of S.1https: / / github.com / clab-lstm-parser 2This works better for the arc-eager algorithm (Ballesteros and Nivre, 2013), unlike Henderson et al. (2013), which pushes the point on the front with root in the original arc-eager algorithm (Nivre, 2008), SHIFT and RIGHT-ARC actions."}, {"heading": "2.3 Constraints on Transitions", "text": "To ensure that the parser never gets into an invalid state, the sequence of transitions according to Henderson et al. (2013) is limited. Actions that copy or move elements from the buffer (S-SHIFT, S-RIGHT, and M-SHIFT) are prohibited when the buffer is empty. Actions that jump out of a stack (S-REDUCE and M-REDUCE) are prohibited when this stack is empty. We prohibit actions that correspond to the same dependency or predicate to be repeated in sequence. Repeated M-SWAP transitions are prohibited to avoid an infinite exchange. Finally, as mentioned above, we limit the parser to syntactic actions until it must move an element from B to S, after which it can only perform semantic actions until it performs an M-SHIFT.Asymptotic runtime complexity of this greedy algorithm."}, {"heading": "3 Statistical Model", "text": "The transitions in \u00a7 2 describe the execution paths that our algorithm can take; as in the past, we use a statistical classifier to decide which transition to make given the current state at each time step. What's new about our model is that it learns a finite vector representation of the entire state of the common parser (S, M and B) to make this decision."}, {"heading": "3.1 Stack Long Short-Term Memory (LSTM)", "text": "LSTMs are recurrent neural networks that, in addition to a hidden state (Hochreiter and Schmidhuber, 1997; Graves, 2013), are equipped with special memory components to model sequences. Stack LSTMs (Dyer et al., 2015) are LSTMs that allow stack operations: query, push, and pop. A \"stack pointer\" is retained that determines which cell in the LSTM provides the memory and hidden units when calculating the new memory cell contents. Query returns a summary of the stack in a single fixed-length vector. Push adds an element to the top of the stack, resulting in a new summary. Pop, which does not correspond to a conventional LSTM process, shifts the stack pointer to the previous timestep, resulting in a stack summary as it was before the element was observed. Implementation details (Dyer, 2015 and Goldberg) were made public."}, {"heading": "3.2 Stack LSTMs for Joint Parsing", "text": "Like Dyer et al. (2015), we use a fourth stack LSTM, A, for the history of the actions - A is never jumped out, but just pushed out. Figure 4 illustrates the architecture. The state of the algorithm at the time t t is encoded by the four vectors that summarize the four stacks LSTMs, and this is the input to the classifier, which at this time selects between the allowable transitions. Let st, mt, bt, and at this time allow the summaries of St, Mt, Bt, and At. Letters At = Allowed (St, Mt, Bt, At) denote the allowed transitions taking into account the current stacks and buffers. The parser state is at this time represented by a aretificated linear unit (Namet and Hinton, 2010) in vector yt: yt, Bt, At) denoting the allowed transitions taking into account the current stacks and buffers."}, {"heading": "3.3 Composition Functions", "text": "To use stack LSTMs, we need vector representations of the elements stored in the stacks. Specifically, we need vector representations of atoms (words, possibly with part-of-speech tags) and parse fragments. Word vectors can be pre-trained or learned directly; we consider a concatenation of the two in our experiments; part-of-speech vectors are learned and associated with the same one. To obtain vector representations of parse fragments, we use neural networks that recursively compute representations of complex structured output (Dyer et al., 2015). The tree structures here are always ternary trees, with the three children of each internal node having a head, a dependent and a label. The vectors for leaves are word vectors and vectors that correspond to syntactical and sector relations (Dyer et al., 2015)."}, {"heading": "3.4 Training", "text": "The formation of the classifier requires the conversion of each training instance (of a common parse) into a transition sequence, a deterministic operation within the framework of our transition set. Given a collection of algorithmic states at the time t and correct classification decisions \u03c4t, we minimize the sum of log loss terms given (for a time step) by: \u2212 log exp (q\u03c4t + \u03b8\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043d\u043dstost exp) (5) with respect to the classifier and the LSTM parameters. Note that the loss is differentiable in terms of parameters; gradients are calculated by means of backpropagation. We apply chastic network descendent with dropout for all neural parameters."}, {"heading": "3.5 Pretrained Embeddings", "text": "Following Dyer et al. (2015), \"structured skip diagrams\" were embedded (Ling et al., 2015), trained in English (AFP section), German, Spanish, and Chinese with a size 5 window; training was stopped after 5 epochs. For words outside the vocabulary, a randomly initialized vector of the same dimension was used."}, {"heading": "3.6 Predicate Sense Disambiguation", "text": "The predicate sense disambiguation is handled within the model (M-PRED transitions), but since the senses are lexeme-specific, we need a way to deal with invisible predicates at the test time. If a predicate is encountered at the test time that was not observed in training, our system constructs a predicate from the predicted dim of the word at that position and uses the \"01\" sense, which is correct for 91.22% of predicates by type in the English CoNLL 2009 training data."}, {"heading": "4 Experimental Setup", "text": "Our model is evaluated on the basis of the common tasks of the CoNLL with regard to common syntactic and semantic dependency analyses in 2008 (Surdeanu et al., 2008) and 2009 (Hajic and al., 2009). According to the common task guidelines, automatically predicted POS days and lemmats were used in the data sets for all experiments. Pseudo-projectification of the syntactic trees (Nivre et al., 2007) was used as a pre-processing step, which enabled an accurate conversion of even the non-projective syntactic trees into syntactic transitions. However, the oracle conversion of semantic parses in transitions is not perfect despite the use of the M-SWAP action, due to the presence of several intersections. 7The standard evaluation metrics include the syntactically designated Attachment Score (LAS), the semantic F1 score in both the domain (WJ systems) and the macro domain (s domain) of which was defined differently in each case."}, {"heading": "4.1 CoNLL 2008", "text": "The CoNLL 2008 dataset contains notes from the Penn Treebank (Marcus et al., 1993), PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). The shared task evaluated predicate identification systems in addition to predicate sense disambiguations and SRL. To identify predicates, we trained a bi-directional LSTM two-class classifier. As input to the classifier, we used learned representations of word terminations and POS tags. This model achieves an F1 score of 91.43% when marking words as predicates (or not).Hyper parameter The input representation for a word consists of pre-trained embeddings (size 100 for English, 80 for Chinese, 64 for German and Spanish), associated with additional learned word and POS tag embeddings (size 32 and 12) of 0.1 and 0.1 respectively."}, {"heading": "4.2 CoNLL 2009", "text": "Regarding the task CoNLL 2008 (above), the most important change in 2009 is that predicates are identified in advance and systems are evaluated only on predicate sense disambiguation (not identification). Therefore, the bidirectional LSTM classifier is not used here. Pre-processing for projectivity and selection of hyperparameters is the same as in \u00a7 4.1. In addition to the common approach described in the preceding sections, we are experimenting here with several variants: semantics-only: the set of syntactic transitions S, the syntactic stack S and the syntactic composition function are discarded. Consequently, the set of constraints on transitions is a subset of the complete set of constraints in \u00a7 2.3. Effectively, this model does not use syntactic characteristics S and the syntactic composition S. (2011) and Zhou and Xu."}, {"heading": "5 Results and Discussion", "text": "CoNLL 2008 (Table 2) Our common model significantly exceeds the common model of Henderson et al. (2008), from which our series of transitions is derived, and shows the benefit of learning a representation for the entire algorithmic state. Several other common learning models have been proposed (Llu\u0131 dem and Ma rquez, 2008; Johansson et al. (2008), Ciaramita et al., 2009) for the same task; our common model exceeds the performance of all of these models. The best reported systems on the CoNLL 2008 task are Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008), Ciaramita et al. (2008), all pipeline syntax and semantics; our system performance is comparable to these. We fall behind only Johansson and Nugues, whose success has been attributed."}, {"heading": "6 Related Work", "text": "Other approaches to joint modeling that are not taken into account in our experiments are noteworthy. Llui's et al. (2013) propose a graph-based common model that uses dual decomposition to match syntax and semantics, but does not perform competitively in the CoNLL 2009 task. Lewis et al. (2015) proposed an efficient common model for CCG syntax and SRL that works better than a pipeline model. However, their training does not require CCG comments, ours does. Furthermore, their evaluation metric rewards semantic dependencies, regardless of where they are within the argument span given by a PropBank constituent, which makes a direct comparison to our evaluation unfeasible. Krishnamurthy and Mitchell (2014) demonstrate a common CCG analysis and relation extraction model that improves semantic dependencies, but their task is different from our one based on NL, which is also based on common modules and not on common NL."}, {"heading": "7 Conclusion", "text": "We presented an incremental, greedy parser for analyzing syntactic and semantic dependencies. Our model surpasses the performance of previous common models in the English tasks of CoNLL 2008 and 2009 without using the expert-built, expensive functions of the full syntactic parser."}, {"heading": "Acknowledgments", "text": "The authors thank Sam Thomson, Lingpeng Kong, Mark Yatskar, Eunsol Choi, George Mulcaire, and Luheng He, as well as anonymous reviewers, for many useful comments. This research was supported in part by DARPA funding FA8750-12-2-0342, funded under the DEFT program, and by the U.S. Army Research Office under grant number W911NF-10-1-0533. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the author (s) and do not necessarily reflect the views of the U.S. Army Research Office or the U.S. Government. Miguel Ballesteros was supported by the European Commission under contract numbers FP7-ICT-610411 (Project MULTISENSOR) and H2020-RIA-645012 (Project KRISTINA)."}], "references": [{"title": "Going to the roots of dependency parsing", "author": ["Ballesteros", "Nivre2013] Miguel Ballesteros", "Joakim Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Ballesteros et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2013}, {"title": "Improved transition-based parsing by modeling characters instead of words with LSTMs", "author": ["Chris Dyer", "Noah A. Smith"], "venue": "In Proc. of EMNLP", "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Multilingual semantic role labeling", "author": ["Love Hafdell", "Pierre Nugues"], "venue": "In Proc. of CoNLL", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2009}, {"title": "A highperformance syntactic and semantic dependency parser", "author": ["Bernd Bohnet", "Love Hafdell", "Pierre Nugues"], "venue": "In Proc. of COLING", "citeRegEx": "Bj\u00f6rkelund et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bj\u00f6rkelund et al\\.", "year": 2010}, {"title": "Introduction to the CoNLL-2005 shared task: Semantic role labeling", "author": ["Carreras", "M\u00e0rquez2005] Xavier Carreras", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "In Proc. of CoNLL", "citeRegEx": "Carreras et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Carreras et al\\.", "year": 2005}, {"title": "A cascaded syntactic and semantic dependency parsing system", "author": ["Che et al.2008] Wanxiang Che", "Zhenghua Li", "Yuxuan Hu", "Yongqiang Li", "Bing Qin", "Ting Liu", "Sheng Li"], "venue": "In Proc. of CoNLL", "citeRegEx": "Che et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Che et al\\.", "year": 2008}, {"title": "Multilingual dependency-based syntactic and semantic parsing", "author": ["Che et al.2009] Wanxiang Che", "Zhenghua Li", "Yongqiang Li", "Yuhang Guo", "Bing Qin", "Ting Liu"], "venue": "In Proc. of CoNLL", "citeRegEx": "Che et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Che et al\\.", "year": 2009}, {"title": "DeSRL: A linear-time semantic role labeling system", "author": ["Giuseppe Attardi", "Felice Dell\u2019Orletta", "Mihai Surdeanu"], "venue": "In Proc. of CoNLL", "citeRegEx": "Ciaramita et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "In Proc. of ACL", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Semantic role labelling with neural network factors", "author": ["Oscar T\u00e4ckstr\u00f6m", "Kuzman Ganchev", "Dipanjan Das"], "venue": "In Proc. of EMNLP", "citeRegEx": "FitzGerald et al\\.,? \\Q2015\\E", "shortCiteRegEx": "FitzGerald et al\\.", "year": 2015}, {"title": "Dependencybased semantic role labeling using convolutional neural networks", "author": ["Foland", "Martin2015] William R. Foland", "James Martin"], "venue": "In Proc. of *SEM", "citeRegEx": "Foland et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Foland et al\\.", "year": 2015}, {"title": "A latent variable model of synchronous syntactic-semantic parsing for multiple languages", "author": ["James Henderson", "Paola Merlo", "Ivan Titov"], "venue": "In Proc. of CoNLL", "citeRegEx": "Gesmundo et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Gesmundo et al\\.", "year": 2009}, {"title": "Automatic labeling of semantic roles", "author": ["Gildea", "Jurafsky2002] Daniel Gildea", "Daniel Jurafsky"], "venue": "Computational Linguistics,", "citeRegEx": "Gildea et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Gildea et al\\.", "year": 2002}, {"title": "A primer on neural network models for natural language processing", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Generating sequences with recurrent neural networks. arXiv:1308.0850", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages", "author": ["Haji\u010d et al.2009] Jan Haji\u010d", "Massimiliano Ciaramita", "Richard Johansson", "Daisuke Kawahara", "Maria Ant\u00f2nia Mart\u0131", "Llu\u0131\u0301s M\u00e0rquez", "Adam Meyers", "Joakim Nivre", "Sebastian Pad\u00f3", "Jan \u0160t\u011bp\u00e1nek", "Pavel Stra\u0148\u00e1k", "Mihai Surdeanu", "Nianwen Xue", "Yi Zhang"], "venue": "In Proc. of CoNLL", "citeRegEx": "Haji\u010d et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Haji\u010d et al\\.", "year": 2009}, {"title": "Dynamic feature selection for dependency parsing", "author": ["He et al.2013] He He", "Hal Daum\u00e9 III", "Jason Eisner"], "venue": "In Proc. of EMNLP", "citeRegEx": "He et al\\.,? \\Q2013\\E", "shortCiteRegEx": "He et al\\.", "year": 2013}, {"title": "A latent variable model of synchronous parsing for syntactic and semantic dependencies", "author": ["Paola Merlo", "Gabriele Musillo", "Ivan Titov"], "venue": "In Proc. of CoNLL", "citeRegEx": "Henderson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2008}, {"title": "Multi-lingual joint parsing of syntactic and semantic dependencies with a latent variable model", "author": ["Paola Merlo", "Ivan Titov", "Gabriele Musillo"], "venue": null, "citeRegEx": "Henderson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Henderson et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Dependency-based syntactic-semantic analysis with PropBank and NomBank", "author": ["Johansson", "Nugues2008] Richard Johansson", "Pierre Nugues"], "venue": "In Proc. of CoNLL", "citeRegEx": "Johansson et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Johansson et al\\.", "year": 2008}, {"title": "Statistical bistratal dependency parsing", "author": ["Richard Johansson"], "venue": "In Proc. of EMNLP", "citeRegEx": "Johansson.,? \\Q2009\\E", "shortCiteRegEx": "Johansson.", "year": 2009}, {"title": "Joint syntactic and semantic parsing with combinatory categorial grammar", "author": ["Krishnamurthy", "Tom M. Mitchell"], "venue": "In Proc. of ACL", "citeRegEx": "Krishnamurthy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Krishnamurthy et al\\.", "year": 2014}, {"title": "High-order low-rank tensors for semantic role labeling", "author": ["Lei et al.2015] Tao Lei", "Yuan Zhang", "Llu\u0131\u0301s M\u00e0rquez i Villodre", "Alessandro Moschitti", "Regina Barzilay"], "venue": "In Proc. of NAACL", "citeRegEx": "Lei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lei et al\\.", "year": 2015}, {"title": "Joint A* CCG parsing and semantic role labelling", "author": ["Lewis et al.2015] Mike Lewis", "Luheng He", "Luke Zettlemoyer"], "venue": "In Proc. of EMNLP", "citeRegEx": "Lewis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2015}, {"title": "Joint syntactic and semantic parsing of Chinese", "author": ["Li et al.2010] Junhui Li", "Guodong Zhou", "Hwee Tou Ng"], "venue": "In Proc. of ACL", "citeRegEx": "Li et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Li et al\\.", "year": 2010}, {"title": "Two/too simple adaptations of word2vec for syntax problems", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan Black", "Isabel Trancoso"], "venue": "In Proc. of NAACL", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Joint arc-factored parsing of syntactic and semantic dependencies", "author": ["Llu\u0131\u0301s et al.2013] Xavier Llu\u0131\u0301s", "Xavier Carreras", "Llu\u0131\u0301s M\u00e0rquez"], "venue": "Transactions of the ACL,", "citeRegEx": "Llu\u0131\u0301s et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Llu\u0131\u0301s et al\\.", "year": 2013}, {"title": "Building a large annotated corpus of English: The Penn treebank", "author": ["Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": null, "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "The NomBank project: An interim report", "author": ["Meyers et al.2004] Adam Meyers", "Ruth Reeves", "Catherine Macleod", "Rachel Szekely", "Veronika Zielinska", "Brian Young", "Ralph Grishman"], "venue": "In Proc. of NAACL", "citeRegEx": "Meyers et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Meyers et al\\.", "year": 2004}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E. Hinton"], "venue": "In Proc. of ICML", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "MaltParser: A language-independent system for data-driven dependency parsing", "author": ["Nivre et al.2007] Joakim Nivre", "Johan Hall", "Jens Nilsson", "Atanas Chanev", "G\u00fclsen Eryigit", "Sandra K\u00fcbler", "Svetoslav Marinov", "Erwin Marsi"], "venue": "Natural Language Engineering,", "citeRegEx": "Nivre et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2007}, {"title": "Algorithms for deterministic incremental dependency parsing", "author": ["Joakim Nivre"], "venue": "Computational Linguistics,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "Non-projective dependency parsing in expected linear time", "author": ["Joakim Nivre"], "venue": "In Proc. of ACL", "citeRegEx": "Nivre.,? \\Q2009\\E", "shortCiteRegEx": "Nivre.", "year": 2009}, {"title": "The Proposition Bank: An annotated corpus of semantic roles", "author": ["Palmer et al.2005] Martha Palmer", "Daniel Gildea", "Paul Kingsbury"], "venue": null, "citeRegEx": "Palmer et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Palmer et al\\.", "year": 2005}, {"title": "Composition of word representations improves semantic role labelling", "author": ["Roth", "Woodsend2014] Michael Roth", "Kristian Woodsend"], "venue": "In Proc. of EMNLP", "citeRegEx": "Roth et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roth et al\\.", "year": 2014}, {"title": "The CoNLL-2008 shared task on joint parsing of syntactic and semantic dependencies", "author": ["Richard Johansson", "Adam Meyers", "Llu\u0131\u0301s M\u00e0rquez", "Joakim Nivre"], "venue": "In Proc. of CoNLL", "citeRegEx": "Surdeanu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2008}, {"title": "Joint parsing and semantic role labeling", "author": ["Sutton", "McCallum2005] Charles Sutton", "Andrew McCallum"], "venue": "In Proc. of CoNLL", "citeRegEx": "Sutton et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2005}, {"title": "Efficient inference and structured learning for semantic role labeling", "author": ["Kuzman Ganchev", "Dipanjan Das"], "venue": "Transactions of the ACL,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2015\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2015}, {"title": "Online graph planarisation for synchronous parsing of semantic and syntactic dependencies", "author": ["Titov et al.2009] Ivan Titov", "James Henderson", "Paola Merlo", "Gabriele Musillo"], "venue": "In Proc. of IJCAI", "citeRegEx": "Titov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2009}, {"title": "A global joint model for semantic role labeling", "author": ["Aria Haghighi", "Christopher D. Manning"], "venue": null, "citeRegEx": "Toutanova et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2008}, {"title": "Structured training for neural network transition-based parsing", "author": ["Weiss et al.2015] David Weiss", "Chris Alberti", "Michael Collins", "Slav Petrov"], "venue": "In Proc. of ACL", "citeRegEx": "Weiss et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weiss et al\\.", "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["Ilya Sutskever", "Oriol Vinyals"], "venue": null, "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}, {"title": "Parsing syntactic and semantic dependencies with two single-stage maximum entropy models", "author": ["Zhao", "Kit2008] Hai Zhao", "Chunyu Kit"], "venue": "In Proc. of CoNLL", "citeRegEx": "Zhao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2008}, {"title": "Multilingual dependency learning: Exploiting rich features for tagging syntactic and semantic dependencies", "author": ["Zhao et al.2009] Hai Zhao", "Wenliang Chen", "Jun\u2019ichi Kazama", "Kiyotaka Uchimoto", "Kentaro Torisawa"], "venue": "In Proc. of CoNLL", "citeRegEx": "Zhao et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "End-to-end learning of semantic role labeling using recurrent neural networks", "author": ["Zhou", "Xu2015] Jie Zhou", "Wei Xu"], "venue": "In Proc. of ACL", "citeRegEx": "Zhou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 12, "context": "(2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks.", "startOffset": 20, "endOffset": 87}, {"referenceID": 40, "context": "(2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks.", "startOffset": 20, "endOffset": 87}, {"referenceID": 19, "context": "(2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks.", "startOffset": 20, "endOffset": 87}, {"referenceID": 24, "context": "Such features are a mainstay of high-performance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al.", "startOffset": 86, "endOffset": 179}, {"referenceID": 10, "context": "Such features are a mainstay of high-performance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al.", "startOffset": 86, "endOffset": 179}, {"referenceID": 22, "context": ", 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013).", "startOffset": 68, "endOffset": 102}, {"referenceID": 17, "context": ", 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013).", "startOffset": 68, "endOffset": 102}, {"referenceID": 9, "context": "The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser\u2019s state is conventionally encoded.", "startOffset": 49, "endOffset": 68}, {"referenceID": 45, "context": "Our system\u2019s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Bj\u00f6rkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (T\u00e4ckstr\u00f6m et al.", "startOffset": 93, "endOffset": 180}, {"referenceID": 3, "context": "Our system\u2019s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Bj\u00f6rkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (T\u00e4ckstr\u00f6m et al.", "startOffset": 93, "endOffset": 180}, {"referenceID": 24, "context": "Our system\u2019s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Bj\u00f6rkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (T\u00e4ckstr\u00f6m et al.", "startOffset": 93, "endOffset": 180}, {"referenceID": 39, "context": ", 2015), systems which perform optimal decoding (T\u00e4ckstr\u00f6m et al., 2015), or of systems that exploit additional, differently-annotated datasets (FitzGerald et al.", "startOffset": 48, "endOffset": 72}, {"referenceID": 10, "context": ", 2015), or of systems that exploit additional, differently-annotated datasets (FitzGerald et al., 2015).", "startOffset": 79, "endOffset": 104}, {"referenceID": 12, "context": "Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences.", "startOffset": 97, "endOffset": 121}, {"referenceID": 12, "context": "Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser\u2019s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al.", "startOffset": 97, "endOffset": 510}, {"referenceID": 8, "context": "(2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser\u2019s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu\u0131\u0301s et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic parse tree, and arbitrarily-scoped syntactic features\u2014such as the \u201cpath\u201d between predicate and argument, proposed by Gildea and Jurafsky (2002)\u2014for semantic analysis.", "startOffset": 21, "endOffset": 787}, {"referenceID": 18, "context": "1 2 Joint Syntactic and Semantic Dependency Parsing We largely follow the transition-based, synchronized algorithm of Henderson et al. (2013) to predict joint parse structures.", "startOffset": 118, "endOffset": 142}, {"referenceID": 33, "context": "Note that in the original arc-eager algorithm (Nivre, 2008), SHIFT and RIGHT-ARC actions move the item on the buffer front to the stack, whereas we only copy it (to allow the semantic operations to have access to it).", "startOffset": 46, "endOffset": 59}, {"referenceID": 31, "context": "The syntactic transitions are from the \u201carc-eager\u201d algorithm of Nivre (2008). They include: \u2022 S-SHIFT, which copies3 an item from the front of B and pushes it on S.", "startOffset": 64, "endOffset": 77}, {"referenceID": 18, "context": "com/clab/joint-lstm-parser This works better for the arc-eager algorithm (Ballesteros and Nivre, 2013), in contrast to Henderson et al. (2013), who initialized with root at the buffer front.", "startOffset": 119, "endOffset": 143}, {"referenceID": 18, "context": "We use two other semantic transitions from Henderson et al. (2013) which have no syntactic analogues: \u2022 M-SWAP swaps the top two items on M , to allow for crossing semantic arcs.", "startOffset": 43, "endOffset": 67}, {"referenceID": 18, "context": "We use two other semantic transitions from Henderson et al. (2013) which have no syntactic analogues: \u2022 M-SWAP swaps the top two items on M , to allow for crossing semantic arcs. \u2022 M-PRED(p) marks the item at the front of B as a semantic predicate with the sense p, and replaces it with the disambiguated predicate. The CoNLL 2009 corpus introduces semantic self-dependencies where many nominal predicates (from NomBank) are marked as their own arguments; these account for 6.68% of all semantic arcs in the English corpus. An example involving an eventive noun is shown in Figure 2. We introduce a new semantic transition, not in Henderson et al. (2013), to handle such cases: \u2022 M-SELF(r) adds a dependency, with label r between the item at the front of B and itself.", "startOffset": 43, "endOffset": 655}, {"referenceID": 18, "context": "3 Constraints on Transitions To ensure that the parser never enters an invalid state, the sequence of transitions is constrained, following Henderson et al. (2013). Actions that copy or move items from the buffer (S-SHIFT, S-RIGHT and M-SHIFT) are forbidden when the buffer is empty.", "startOffset": 140, "endOffset": 164}, {"referenceID": 18, "context": "3 Constraints on Transitions To ensure that the parser never enters an invalid state, the sequence of transitions is constrained, following Henderson et al. (2013). Actions that copy or move items from the buffer (S-SHIFT, S-RIGHT and M-SHIFT) are forbidden when the buffer is empty. Actions that pop from a stack (S-REDUCE and M-REDUCE) are forbidden when that stack is empty. We disallow actions corresponding to the same dependency, or the same predicate to be repeated in the sequence. Repetitive M-SWAP transitions are disallowed to avoid infinite swapping. Finally, as noted above, we restrict the parser to syntactic actions until it needs to shift an item from B to S, after which it can only execute semantic actions until it executes an M-SHIFT. Asymptotic runtime complexity of this greedy algorithm is linear in the length of the input, following the analysis by Nivre (2009).5 3 Statistical Model The transitions in \u00a72 describe the execution paths our algorithm can take; like past work, we apply a statistical classifier to decide which transition to take at each timestep, given the current state.", "startOffset": 140, "endOffset": 888}, {"referenceID": 15, "context": "1 Stack Long Short-Term Memory (LSTM) LSTMs are recurrent neural networks equipped with specialized memory components in addition to a hidden state (Hochreiter and Schmidhuber, 1997; Graves, 2013) to model sequences.", "startOffset": 148, "endOffset": 196}, {"referenceID": 9, "context": "Stack LSTMs (Dyer et al., 2015) are LSTMs that allow for stack operations: query, push, and pop.", "startOffset": 12, "endOffset": 31}, {"referenceID": 9, "context": "Implementation details (Dyer et al., 2015; Goldberg, 2015) and code have been made publicly available.", "startOffset": 23, "endOffset": 58}, {"referenceID": 14, "context": "Implementation details (Dyer et al., 2015; Goldberg, 2015) and code have been made publicly available.", "startOffset": 23, "endOffset": 58}, {"referenceID": 34, "context": "The analysis in (Nivre, 2009) does not consider SWAP actions.", "startOffset": 16, "endOffset": 29}, {"referenceID": 9, "context": "Like Dyer et al. (2015), we use a fourth stack LSTM, A, for the history of actions\u2014A is never popped from, only pushed to.", "startOffset": 5, "endOffset": 24}, {"referenceID": 9, "context": "To obtain vector representations of parse fragments, we use neural networks which recursively compute representations of the complex structured output (Dyer et al., 2015).", "startOffset": 151, "endOffset": 170}, {"referenceID": 27, "context": "(2015), \u201cstructured skip-gram\u201d embeddings (Ling et al., 2015) were used, trained on the English (AFP section), German, Spanish and Chinese Gigaword corpora, with a window of size 5; training was stopped after 5 epochs.", "startOffset": 42, "endOffset": 61}, {"referenceID": 9, "context": "5 Pretrained Embeddings Following Dyer et al. (2015), \u201cstructured skip-gram\u201d embeddings (Ling et al.", "startOffset": 34, "endOffset": 53}, {"referenceID": 37, "context": "4 Experimental Setup Our model is evaluated on the CoNLL shared tasks on joint syntactic and semantic dependency parsing in 2008 (Surdeanu et al., 2008) and 2009 (Haji\u010d et al.", "startOffset": 129, "endOffset": 152}, {"referenceID": 16, "context": ", 2008) and 2009 (Haji\u010d et al., 2009).", "startOffset": 17, "endOffset": 37}, {"referenceID": 32, "context": "As a preprocessing step, pseudoprojectivization of the syntactic trees (Nivre et al., 2007) was used, which allowed an accurate conversion of even the non-projective syntactic trees into syntactic transitions.", "startOffset": 71, "endOffset": 91}, {"referenceID": 29, "context": "1 CoNLL 2008 The CoNLL 2008 dataset contains annotations from the Penn Treebank (Marcus et al., 1993), PropBank (Palmer et al.", "startOffset": 80, "endOffset": 101}, {"referenceID": 35, "context": ", 1993), PropBank (Palmer et al., 2005) and NomBank (Meyers et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 30, "context": ", 2005) and NomBank (Meyers et al., 2004).", "startOffset": 20, "endOffset": 41}, {"referenceID": 43, "context": "2 (Zaremba et al., 2014) was used on all layers at training time, tuned on the development data from the set of values {0.", "startOffset": 2, "endOffset": 24}, {"referenceID": 42, "context": "Other hyperparameters have been set intuitively; careful tuning is expected to yield improvements (Weiss et al., 2015).", "startOffset": 98, "endOffset": 118}, {"referenceID": 9, "context": "1 (Dyer et al., 2015).", "startOffset": 2, "endOffset": 21}, {"referenceID": 8, "context": "Effectively, this model does not use any syntactic features, similar to Collobert et al. (2011) and Zhou and Xu (2015).", "startOffset": 72, "endOffset": 96}, {"referenceID": 8, "context": "Effectively, this model does not use any syntactic features, similar to Collobert et al. (2011) and Zhou and Xu (2015). It provides a controlled test of the benefit of explicit syntax in a semantic parser.", "startOffset": 72, "endOffset": 119}, {"referenceID": 8, "context": "Effectively, this model does not use any syntactic features, similar to Collobert et al. (2011) and Zhou and Xu (2015). It provides a controlled test of the benefit of explicit syntax in a semantic parser. Syntax-only: all semantic transitions in M, the semantic stack M , and the semantic composition function gm are discarded. S-SHIFT and S-RIGHT now move the item from the front of the buffer to the syntactic stack, instead of copying. The set of constraints on the transitions is again a subset of the full set of constraints. This model is an arc-eager variant of Dyer et al. (2015), and serves to check whether semantic parsing degrades syntactic performance.", "startOffset": 72, "endOffset": 589}, {"referenceID": 8, "context": "Effectively, this model does not use any syntactic features, similar to Collobert et al. (2011) and Zhou and Xu (2015). It provides a controlled test of the benefit of explicit syntax in a semantic parser. Syntax-only: all semantic transitions in M, the semantic stack M , and the semantic composition function gm are discarded. S-SHIFT and S-RIGHT now move the item from the front of the buffer to the syntactic stack, instead of copying. The set of constraints on the transitions is again a subset of the full set of constraints. This model is an arc-eager variant of Dyer et al. (2015), and serves to check whether semantic parsing degrades syntactic performance. For 1.5% of English sentences in the CoNLL 2009 English dataset, the transition sequence incorrectly encodes the goldstandard joint parse; details in Henderson et al. (2013).", "startOffset": 72, "endOffset": 841}, {"referenceID": 15, "context": "1 Henderson et al. (2008) 87.", "startOffset": 2, "endOffset": 26}, {"referenceID": 15, "context": "1 Henderson et al. (2008) 87.6 73.1 80.5 Johansson (2009) 86.", "startOffset": 2, "endOffset": 58}, {"referenceID": 15, "context": "1 Henderson et al. (2008) 87.6 73.1 80.5 Johansson (2009) 86.6 77.1 81.8 Titov et al. (2009) 87.", "startOffset": 2, "endOffset": 93}, {"referenceID": 15, "context": "1 Henderson et al. (2008) 87.6 73.1 80.5 Johansson (2009) 86.6 77.1 81.8 Titov et al. (2009) 87.5 76.1 81.8 CoNLL 2008 best: #3: Zhao and Kit (2008) 87.", "startOffset": 2, "endOffset": 149}, {"referenceID": 5, "context": "2 #2: Che et al. (2008) 86.", "startOffset": 6, "endOffset": 24}, {"referenceID": 5, "context": "2 #2: Che et al. (2008) 86.7 78.5 82.7 #2: Ciaramita et al. (2008) 87.", "startOffset": 6, "endOffset": 67}, {"referenceID": 5, "context": "2 #2: Che et al. (2008) 86.7 78.5 82.7 #2: Ciaramita et al. (2008) 87.4 78.0 82.7 #1: J&N (2008) 89.", "startOffset": 6, "endOffset": 97}, {"referenceID": 22, "context": "Several other joint learning models have been proposed (Llu\u0131\u0301s and M\u00e0rquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models.", "startOffset": 55, "endOffset": 118}, {"referenceID": 40, "context": "Several other joint learning models have been proposed (Llu\u0131\u0301s and M\u00e0rquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models.", "startOffset": 55, "endOffset": 118}, {"referenceID": 45, "context": "The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al.", "startOffset": 109, "endOffset": 169}, {"referenceID": 6, "context": "The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al.", "startOffset": 109, "endOffset": 169}, {"referenceID": 12, "context": "The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al.", "startOffset": 109, "endOffset": 169}, {"referenceID": 2, "context": "Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Bj\u00f6rkelund et al., 2009; Bj\u00f6rkelund et al., 2010; Roth and Woodsend, 2014); we have not used these features or reranking.", "startOffset": 144, "endOffset": 219}, {"referenceID": 3, "context": "Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Bj\u00f6rkelund et al., 2009; Bj\u00f6rkelund et al., 2010; Roth and Woodsend, 2014); we have not used these features or reranking.", "startOffset": 144, "endOffset": 219}, {"referenceID": 11, "context": "5 Results and Discussion CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of transitions is derived, showing the benefit of learning a representation for the entire algorithmic state.", "startOffset": 107, "endOffset": 131}, {"referenceID": 11, "context": "5 Results and Discussion CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of transitions is derived, showing the benefit of learning a representation for the entire algorithmic state. Several other joint learning models have been proposed (Llu\u0131\u0301s and M\u00e0rquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models. The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al.", "startOffset": 107, "endOffset": 550}, {"referenceID": 3, "context": "The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al.", "startOffset": 89, "endOffset": 107}, {"referenceID": 3, "context": "The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system\u2019s semantic and overall performance is comparable to these.", "startOffset": 89, "endOffset": 132}, {"referenceID": 3, "context": "The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system\u2019s semantic and overall performance is comparable to these.", "startOffset": 89, "endOffset": 156}, {"referenceID": 3, "context": "The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system\u2019s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower.", "startOffset": 89, "endOffset": 319}, {"referenceID": 3, "context": "The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system\u2019s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower. CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al.", "startOffset": 89, "endOffset": 600}, {"referenceID": 3, "context": "The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system\u2019s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower. CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013), demonstrating the benefit of our entire-parser-state representation learner compared to the more locally scoped model.", "startOffset": 89, "endOffset": 628}, {"referenceID": 3, "context": "The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system\u2019s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower. CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013), demonstrating the benefit of our entire-parser-state representation learner compared to the more locally scoped model. Given that syntax has consistently proven useful in SRL, we expected our Semantics-only model to underperform Hybrid and Joint, and it did. In the training domain, syntax and semantics benefit each other (Joint outperforms Hybrid). Out-of-domain (the Brown test set), the Hybrid pulls ahead, a sign that Joint overfits to WSJ. As a syntactic parser, our Syntax-only model performs slightly better than Dyer et al. (2015), who achieve 89.", "startOffset": 89, "endOffset": 1169}, {"referenceID": 3, "context": "The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system\u2019s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower. CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013), demonstrating the benefit of our entire-parser-state representation learner compared to the more locally scoped model. Given that syntax has consistently proven useful in SRL, we expected our Semantics-only model to underperform Hybrid and Joint, and it did. In the training domain, syntax and semantics benefit each other (Joint outperforms Hybrid). Out-of-domain (the Brown test set), the Hybrid pulls ahead, a sign that Joint overfits to WSJ. As a syntactic parser, our Syntax-only model performs slightly better than Dyer et al. (2015), who achieve 89.56 LAS on this task. Joint parsing is very slightly better still. The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed language-specific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expensive system.", "startOffset": 89, "endOffset": 1461}, {"referenceID": 2, "context": "Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Bj\u00f6rkelund et al., 2009; Bj\u00f6rkelund et al., 2010; Roth and Woodsend, 2014); we have not used these features or reranking. Lei et al. (2015) use syntactic parses to obtain interaction features between predicates and their arguments and then compress feature representations using a low-rank tensor.", "startOffset": 145, "endOffset": 285}, {"referenceID": 2, "context": "Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Bj\u00f6rkelund et al., 2009; Bj\u00f6rkelund et al., 2010; Roth and Woodsend, 2014); we have not used these features or reranking. Lei et al. (2015) use syntactic parses to obtain interaction features between predicates and their arguments and then compress feature representations using a low-rank tensor. T\u00e4ckstr\u00f6m et al. (2015) present an exact inference algorithm for SRL based on dynamic programming and their local and", "startOffset": 145, "endOffset": 467}, {"referenceID": 1, "context": "An alternative might be to infer word-internal representations using character-based word embeddings, which was found beneficial for syntactic parsing (Ballesteros et al., 2015).", "startOffset": 151, "endOffset": 177}, {"referenceID": 8, "context": "Their algorithm is adopted by FitzGerald et al. (2015) for inference in a model that jointly learns representations from a combination of PropBank and FrameNet annotations; we have not experimented with extra annotations.", "startOffset": 30, "endOffset": 55}, {"referenceID": 8, "context": "Their algorithm is adopted by FitzGerald et al. (2015) for inference in a model that jointly learns representations from a combination of PropBank and FrameNet annotations; we have not experimented with extra annotations. Our system achieves an end-to-end runtime of 177.6\u00b118 seconds to parse the CoNLL 2009 English test set on a single core. This is almost 2.5 times faster than the pipeline model of Lei et al. (2015) (439.", "startOffset": 30, "endOffset": 420}, {"referenceID": 25, "context": "Llu\u0131\u0301s et al. (2013) propose a graph-based joint model using dual decomposition for agreement between syntax and semantics, but do not achieve competitive performance on the CoNLL 2009 task.", "startOffset": 0, "endOffset": 21}, {"referenceID": 24, "context": "Lewis et al. (2015) proposed an efficient joint model for CCG syntax and SRL, which performs better than a pipelined model.", "startOffset": 0, "endOffset": 20}, {"referenceID": 24, "context": "Lewis et al. (2015) proposed an efficient joint model for CCG syntax and SRL, which performs better than a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours.", "startOffset": 0, "endOffset": 435}, {"referenceID": 24, "context": "Lewis et al. (2015) proposed an efficient joint model for CCG syntax and SRL, which performs better than a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset.", "startOffset": 0, "endOffset": 580}, {"referenceID": 8, "context": "Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features.", "startOffset": 0, "endOffset": 24}, {"referenceID": 8, "context": "Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and M\u00e0rquez, 2005), their approach inspired Zhou and Xu (2015), who achieved state-of-the-art results using deep bidirectional LSTMs.", "startOffset": 0, "endOffset": 252}], "year": 2016, "abstractText": "We present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008\u20139 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics.", "creator": "LaTeX with hyperref package"}}}