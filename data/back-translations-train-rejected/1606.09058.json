{"id": "1606.09058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "A Distributional Semantics Approach to Implicit Language Learning", "abstract": "In the present paper we show that distributional information is particularly important when considering concept availability under implicit language learning conditions. Based on results from different behavioural experiments we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. In our simulations, we train a Vector-Space model on either an English or a Chinese corpus and then feed the resulting representations to a feed-forward neural network. The task of the neural network was to find a mapping between the word representations and the novel words. Using datasets from four behavioural experiments, which used different semantic manipulations, we were able to obtain learning patterns very similar to those obtained by humans.", "histories": [["v1", "Wed, 29 Jun 2016 12:08:51 GMT  (676kb,D)", "http://arxiv.org/abs/1606.09058v1", "5 pages, 7 figures, NetWords 2015"]], "COMMENTS": "5 pages, 7 figures, NetWords 2015", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["dimitrios alikaniotis", "john n williams"], "accepted": false, "id": "1606.09058"}, "pdf": {"name": "1606.09058.pdf", "metadata": {"source": "CRF", "title": "A Distributional Semantics Approach to Implicit Language Learning", "authors": ["Dimitrios Alikaniotis", "John N. Williams"], "emails": ["da352@cam.ac.uk", "jnw12@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "2 Simulation", "text": "In fact, most of us are able to play by the rules we have set ourselves in order to fulfil them."}, {"heading": "3 Results and Discussion", "text": "In fact, the probability that P (y = gi | wlion) is higher than P (y = ro | wlion), and the inclinations of the gradients clearly show that the model favors grammatical combinations as such. Figures 2-3 show the results of two experiments from P & W that focus on abstract / concrete distinction, and the inclinations of the gradients clearly show that the model favors grammatical combinations as such. Figures 2-3 show the results of two experiments from P & W that focus on abstract / concrete distinction."}, {"heading": "Acknowledgments", "text": "The first author is supported by the Onassis Foundation and we thank the three anonymous reviewers for their valuable feedback."}], "references": [{"title": "Gradient grammar: An effect of animacy on the syntax of give in New Zealand and American English", "author": ["J. Bresnan", "J. Hay"], "venue": "Lingua, 118(2):245\u2013259.", "citeRegEx": "Bresnan and Hay,? 2008", "shortCiteRegEx": "Bresnan and Hay", "year": 2008}, {"title": "Retrieval time from semantic memory", "author": ["A.M. Collins", "M.R. Quillian"], "venue": "Journal of Verbal Learning and Verbal Behavior, 8(2):240\u2013247.", "citeRegEx": "Collins and Quillian,? 1969", "shortCiteRegEx": "Collins and Quillian", "year": 1969}, {"title": "Distributional structure", "author": ["Z. Harris"], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Concreteness and Corpora: A Theoretical and Practical Analysis", "author": ["F. Hill", "D. Kiela", "A. Korhonen"], "venue": "Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 75\u201383.", "citeRegEx": "Hill et al\\.,? 2013a", "shortCiteRegEx": "Hill et al\\.", "year": 2013}, {"title": "A Quantitative Empirical Analysis of the Abstract/Concrete Distinction", "author": ["F. Hill", "A. Korhonen", "C. Bentz"], "venue": "Cognitive Science, 38(1):162\u2013177.", "citeRegEx": "Hill et al\\.,? 2013b", "shortCiteRegEx": "Hill et al\\.", "year": 2013}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["T.K. Landauer", "S.T. Dumais"], "venue": "Psychological Review, 104(2):211\u2013240.", "citeRegEx": "Landauer and Dumais,? 1997", "shortCiteRegEx": "Landauer and Dumais", "year": 1997}, {"title": "Crosslinguistic Differences in Implicit Language Learning", "author": ["J.H.C. Leung", "J.N. Williams"], "venue": "Studies in Second Language Acquisition, 36(4):733\u2013755.", "citeRegEx": "Leung and Williams,? 2014", "shortCiteRegEx": "Leung and Williams", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Semantic generalisation in implicit language learning", "author": ["A. Paciorek", "J. Williams"], "venue": "Journal of Experimental Psychology: Learning, Memory and Cognition.", "citeRegEx": "Paciorek and Williams,? 2015", "shortCiteRegEx": "Paciorek and Williams", "year": 2015}, {"title": "Semantic Cognition: A Parallel Distributed Processing Approach", "author": ["T.T. Rogers", "J.L. McClelland"], "venue": "MIT Press.", "citeRegEx": "Rogers and McClelland,? 2004", "shortCiteRegEx": "Rogers and McClelland", "year": 2004}, {"title": "An improved model of semantic similarity based on lexical co-occurrence", "author": ["D. Rohde", "L.M. Gonnerman", "D.C. Plaut"], "venue": "Communications of the ACM.", "citeRegEx": "Rohde et al\\.,? 2006", "shortCiteRegEx": "Rohde et al\\.", "year": 2006}, {"title": "Learning without awareness", "author": ["J.N. Williams"], "venue": "Studies in Second Language Acquisition, 27:269\u2013304.", "citeRegEx": "Williams,? 2005", "shortCiteRegEx": "Williams", "year": 2005}], "referenceMentions": [{"referenceID": 2, "context": "By exploiting the fact that similar words tend to appear in similar contexts (Harris, 1954), such models have been very successful in tasks of semantic relatedness (Landauer and Dumais, 1997; Rohde et al.", "startOffset": 77, "endOffset": 91}, {"referenceID": 5, "context": "By exploiting the fact that similar words tend to appear in similar contexts (Harris, 1954), such models have been very successful in tasks of semantic relatedness (Landauer and Dumais, 1997; Rohde et al., 2006).", "startOffset": 164, "endOffset": 211}, {"referenceID": 10, "context": "By exploiting the fact that similar words tend to appear in similar contexts (Harris, 1954), such models have been very successful in tasks of semantic relatedness (Landauer and Dumais, 1997; Rohde et al., 2006).", "startOffset": 164, "endOffset": 211}, {"referenceID": 1, "context": "A common criticism addressed towards such models is that those co-occurrence patterns do not explicitly encode specific semantic features unlike more traditional models of semantic memory (Collins and Quillian, 1969; Rogers and McClelland, 2004).", "startOffset": 188, "endOffset": 245}, {"referenceID": 9, "context": "A common criticism addressed towards such models is that those co-occurrence patterns do not explicitly encode specific semantic features unlike more traditional models of semantic memory (Collins and Quillian, 1969; Rogers and McClelland, 2004).", "startOffset": 188, "endOffset": 245}, {"referenceID": 0, "context": "Recently, however, corpus studies (Bresnan and Hay, 2008; Hill et al., 2013b) have shown that some \u2018core\u2019 conceptual distinctions such as animacy and concreteness are reflected in the distributional patterns of words and can be captured by such models (Hill et al.", "startOffset": 34, "endOffset": 77}, {"referenceID": 4, "context": "Recently, however, corpus studies (Bresnan and Hay, 2008; Hill et al., 2013b) have shown that some \u2018core\u2019 conceptual distinctions such as animacy and concreteness are reflected in the distributional patterns of words and can be captured by such models (Hill et al.", "startOffset": 34, "endOffset": 77}, {"referenceID": 3, "context": ", 2013b) have shown that some \u2018core\u2019 conceptual distinctions such as animacy and concreteness are reflected in the distributional patterns of words and can be captured by such models (Hill et al., 2013a).", "startOffset": 183, "endOffset": 203}, {"referenceID": 9, "context": "For example, in studies by Williams (2005) (W) and Leung and Williams (2014) (L&W) the participants were introduced to four novel determinerlike words: gi, ro, ul, and ne.", "startOffset": 27, "endOffset": 43}, {"referenceID": 6, "context": "For example, in studies by Williams (2005) (W) and Leung and Williams (2014) (L&W) the participants were introduced to four novel determinerlike words: gi, ro, ul, and ne.", "startOffset": 51, "endOffset": 77}, {"referenceID": 6, "context": "For example, in studies by Williams (2005) (W) and Leung and Williams (2014) (L&W) the participants were introduced to four novel determinerlike words: gi, ro, ul, and ne. They were explicitly told that they functioned like the article \u2018the\u2019 but that gi and ro were used with near objects and ro and ne with far objects. What they were not told was that gi and ul were used with living things and ro and ne with non-living things. Participants were exposed to grammatical determinernoun combinations in a training task and afterwards given novel determiner-noun combinations to test for generalisation of the hidden regularity. W and L&W report such a generalisation effect even in participants who remained unaware of the relevance of animacy to article usage \u2013 semantic implicit learning. Paciorek and Williams (2015) (P&W) report similar effects for a system in which novel verbs (rather than determiners) collocate with either abstract or concrete nouns.", "startOffset": 51, "endOffset": 820}, {"referenceID": 11, "context": "A ct iv at io n Williams (2005)", "startOffset": 16, "endOffset": 32}, {"referenceID": 11, "context": "Figure 1: Generalisation gradients obtained from the Williams (2005) dataset.", "startOffset": 53, "endOffset": 69}, {"referenceID": 7, "context": "We obtained semantic representations using the skip-gram architecture (Mikolov et al., 2013) provided by the word2vec package,1 trained with hierarchical softmax on the British National Corpus or on a Chinese Wikipedia dump file of comparable size.", "startOffset": 70, "endOffset": 92}, {"referenceID": 11, "context": "A ct iv at io n Paciorek & Williams (2015), exp.", "startOffset": 27, "endOffset": 43}, {"referenceID": 8, "context": "Figure 2: Results of our simulation along with the behavioural results of Paciorek and Williams (2015), exp.", "startOffset": 74, "endOffset": 103}, {"referenceID": 8, "context": "Figure 2: Results of our simulation along with the behavioural results of Paciorek and Williams (2015), exp. 1. The hyperparameters used were the same as in the simulation of Williams (2005).", "startOffset": 74, "endOffset": 191}, {"referenceID": 11, "context": "A ct iv at io n Paciorek & Williams (2015), exp.", "startOffset": 27, "endOffset": 43}, {"referenceID": 8, "context": "Figure 3: Results of our simulation along with the behavioural results of Paciorek and Williams (2015), exp.", "startOffset": 74, "endOffset": 103}, {"referenceID": 8, "context": "Figure 3: Results of our simulation along with the behavioural results of Paciorek and Williams (2015), exp. 4. The hyperparameters used were the same as in the simulation of Williams (2005).", "startOffset": 74, "endOffset": 191}, {"referenceID": 11, "context": "A ct iv at io n Leung & Williams (2014), exp.", "startOffset": 24, "endOffset": 40}, {"referenceID": 6, "context": "Figure 4: Results from Leung and Williams (2014), exp.", "startOffset": 23, "endOffset": 49}, {"referenceID": 6, "context": "Figure 4: Results from Leung and Williams (2014), exp. 3. See text for more info on the measures used. The gradients for the ungrammatical combinations are (1\u2212 grammatical). The value of the weight decay was set to \u03b3 = 0.05 while the rest of the hyperparameters used were the same as in the simulation of Williams (2005).", "startOffset": 23, "endOffset": 321}], "year": 2016, "abstractText": "In the present paper we show that distributional information is particularly important when considering concept availability under implicit language learning conditions. Based on results from different behavioural experiments we argue that the implicit learnability of semantic regularities depends on the degree to which the relevant concept is reflected in language use. In our simulations, we train a VectorSpace model on either an English or a Chinese corpus and then feed the resulting representations to a feed-forward neural network. The task of the neural network was to find a mapping between the word representations and the novel words. Using datasets from four behavioural experiments, which used different semantic manipulations, we were able to obtain learning patterns very similar to those obtained by humans.", "creator": "LaTeX with hyperref package"}}}