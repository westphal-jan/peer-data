{"id": "1606.09152", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Actor-critic versus direct policy search: a comparison based on sample complexity", "abstract": "Sample efficiency is a critical property when optimizing policy parameters for the controller of a robot. In this paper, we evaluate two state-of-the-art policy optimization algorithms. One is a recent deep reinforcement learning method based on an actor-critic algorithm, Deep Deterministic Policy Gradient (DDPG), that has been shown to perform well on various control benchmarks. The other one is a direct policy search method, Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a black-box optimization method that is widely used for robot learning. The algorithms are evaluated on a continuous version of the mountain car benchmark problem, so as to compare their sample complexity. From a preliminary analysis, we expect DDPG to be more sample efficient than CMA-ES, which is confirmed by our experimental results.", "histories": [["v1", "Wed, 29 Jun 2016 15:22:13 GMT  (601kb,D)", "https://arxiv.org/abs/1606.09152v1", null], ["v2", "Mon, 22 Aug 2016 11:07:23 GMT  (758kb,D)", "http://arxiv.org/abs/1606.09152v2", "Proceedings JFPDA (Journees Francaises Planification Decision Apprentissage)"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["arnaud de froissard de broissia", "olivier sigaud"], "accepted": false, "id": "1606.09152"}, "pdf": {"name": "1606.09152.pdf", "metadata": {"source": "CRF", "title": "Actor-critic versus direct policy search: a comparison based on sample complexity", "authors": ["Arnaud de Froissard", "Olivier Sigaud"], "emails": ["olivier.sigaud@isir.upmc.fr"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most people who are able to survive on their own are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own. (...) Most of them are not able to survive on their own."}, {"heading": "2 Algorithms", "text": "In this section, we briefly describe the training mechanisms in DDPG and CMA-ES to highlight their differences."}, {"heading": "2.1 Deep Deterministic Policy Gradient", "text": "It is an actor-critic algorithm that uses deep neural networks to represent both the value function and the politics through a continuous state action space. It combines ideas from DQN (Mnih et al., 2015), Deterministic Policy Gradient (DPG) (Silver et al., 2014) and batch normalization (Ioffe & Szegedy, 2015). In DDPG maps the actor network deterrently learns a state vector to an action vector, i.e. a deterrent policy that is easier to learn than a stochastic one, the search space that smaller.We note hereafter t the current time step, the state vector at t, at the action vector at t and rt the reward at t.When interacting with the environment, each (st, at, rt, st + 1) sample is stored in a replay."}, {"heading": "2.2 Covariance Matrix Adaptation Evolution Strategy", "text": "The evolutionary strategy of the covariance matrix (Hansen et al., 2003) is a gradient-free evolutionary method that uses random variations to improve a set of real parameters relative to an objective function.The general idea is to represent a distribution of sets of parameter values through a covariance matrix, to evaluate each set of parameters, and to update the covariance matrix toward better performance.In the context of robotic experiments, the objective function is the result of one or more episodes of the task under consideration, and the parameters are those of the controller executing the episodes. Thus, at each training step, i.e. after every whole series of episodes, a population of test controllers is sampled based on the covariance matrix around the current task and evaluated based on the task. It is immediately apparent that the performance improvement is based on the execution of many episodes that are not sufficiently efficient, while each covariance matrix provides sufficient actionality for each method such as DPG, the information may be updated by the reviewer."}, {"heading": "3 Experimental set-up", "text": "Our goal in this paper is to compare DDPG and CMA-ES in terms of sample efficiency. JFPDA 2016A task is characterized by a state space, a transitional function that, given the current state and plot, gives a probability distribution over the next state and a reward function that, given a transition, yields a scalar. Here, we limit our analysis to episodic tasks that have an initial state and potentially multiple terminal states. We evaluate the performance of both algorithms on a continuous version of the mountain-car benchmark. In this task, a car is placed between two hills and a target must be reached on top of a hill. The car does not have sufficient power to achieve the reward by driving directly up and down the tracks of both hills. A wall prevents the car from moving away from the unrewarded hill."}, {"heading": "4 Simulation results", "text": "All performance curves shown below are averaged over 10 runs on all numbers and are achieved in less than an hour on a small CPU cluster with 16Go RAM nodes caddied at 2.26Ghz. Figure 1 illustrates the final policy achieved with CMA-ES and DDPG on the mountain car issue. However, one can see that the policy with DDPG shows a better generalization outside of the illustrated course than that with CMA-ES.Figure 2 shows the evolution of learning performance in terms of the time it takes to achieve the goal and the reward with DDPG and CMA-ES.One can see that in terms of time per episode as well as the accumulated reward, DDPG converts faster in terms of the number of interactions, with less variance across different runs, and significantly more sample efficiency than CMA-ES.In fact, on Figure 2 (b), the best performance, the EMA-Evaluations were easily found in terms of EMA-ES evaluations."}, {"heading": "5 Discussion", "text": "It should be noted that the use of a continuous version of this problem \"makes the task easier for the CMA-ES and more difficult for the NAC\" (Heidrich-Meisner & Igel, 2008).The general finding is that the DDPG requires much less interaction with the environment than CMA-ES and less variation between runs. In itself, this superiority is not surprising, since it results from different facts."}, {"heading": "6 Conclusion and future work", "text": "In this paper, we neglected scalability to focus more on sample efficiency, and our results suggest that the DDPG mechanisms are significantly more sample efficient than those of CMA-ES. This sample efficiency is likely to lie in the use of a replay buffer, but also in the more efficient gradient descent algorithm. However, the above comparison is limited in several respects. First, the CMA-ES and DDPG training processes were compared using a neural network as political representation, but the use of an open loop controller representation based on DMPs, as is often done in the field of robot learning, could affect DPG capability."}, {"heading": "Acknowledgments", "text": "This work was supported by the Horizon 2020 research and innovation programme of the European Union as part of the DREAM project under grant agreement no. 640891."}], "references": [{"title": "Reinforcement Learning with High-Dimensional, Continuous Actions", "author": ["C. BAIRD L", "H. KLOPF A"], "venue": null, "citeRegEx": "L. and A.,? \\Q1993\\E", "shortCiteRegEx": "L. and A.", "year": 1993}, {"title": "Continuous deep q-learning with model-based", "author": ["S. GU", "T. LILLICRAP", "I. SUTSKEVER", "S. LEVINE"], "venue": null, "citeRegEx": "GU et al\\.,? \\Q2016\\E", "shortCiteRegEx": "GU et al\\.", "year": 2016}, {"title": "Reducing the time complexity of the deran", "author": ["N. HANSEN", "D. M\u00dcLLER S", "P. KOUMOUTSAKOS"], "venue": null, "citeRegEx": "HANSEN et al\\.,? \\Q2003\\E", "shortCiteRegEx": "HANSEN et al\\.", "year": 2003}, {"title": "Memory-based control with recurrent neural", "author": ["N. 1\u201318. HEESS", "J. HUNT J", "P. LILLICRAP T", "D. SILVER"], "venue": null, "citeRegEx": "HEESS et al\\.,? \\Q2015\\E", "shortCiteRegEx": "HEESS et al\\.", "year": 2015}, {"title": "Human-level control through deep", "author": ["M. RIEDMILLER", "K. FIDJELAND A", "G OSTROVSKI"], "venue": null, "citeRegEx": "RIEDMILLER et al\\.,? \\Q2015\\E", "shortCiteRegEx": "RIEDMILLER et al\\.", "year": 2015}, {"title": "Reinforcement learning of motor skills with policy gradients", "author": ["J. PETERS", "S. SCHAAL"], "venue": "Neural networks : the official journal of the International Neural Network Society,", "citeRegEx": "PETERS and SCHAAL,? \\Q2008\\E", "shortCiteRegEx": "PETERS and SCHAAL", "year": 2008}, {"title": "From Motor Learning to Interaction Learning in Robots, volume", "author": ["J. PETERS", "O. SIGAUD"], "venue": null, "citeRegEx": "PETERS and SIGAUD,? \\Q2010\\E", "shortCiteRegEx": "PETERS and SIGAUD", "year": 2010}, {"title": "Learning forward models for the operational space control of redundant robots", "author": ["C. SALAUN", "V. PADOIS", "O. SIGAUD"], "venue": "From Motor Learning to Interaction Learning in Robots,", "citeRegEx": "SALAUN et al\\.,? \\Q2010\\E", "shortCiteRegEx": "SALAUN et al\\.", "year": 2010}, {"title": "Deterministic policy gradient algorithms", "author": ["D. SILVER", "G. LEVER", "N. HEESS", "T. DEGRIS", "D. WIERSTRA", "M. RIEDMILLER"], "venue": "In Proceedings of the 30th International Conference in Machine Learning", "citeRegEx": "SILVER et al\\.,? \\Q2014\\E", "shortCiteRegEx": "SILVER et al\\.", "year": 2014}, {"title": "Path integral policy improvement with covariance matrix adaptation", "author": ["F. STULP", "O. SIGAUD"], "venue": "In Proceedings of the 29th International Conference on Machine Learning (ICML),", "citeRegEx": "STULP and SIGAUD,? \\Q2012\\E", "shortCiteRegEx": "STULP and SIGAUD", "year": 2012}, {"title": "Policy improvement methods: Between black-box optimization and episodic reinforcement learning", "author": ["F. STULP", "O. SIGAUD"], "venue": "Rapport interne,", "citeRegEx": "STULP and SIGAUD,? \\Q2012\\E", "shortCiteRegEx": "STULP and SIGAUD", "year": 2012}, {"title": "Robot skill learning: From reinforcement learning to evolution strategies", "author": ["F. STULP", "O. SIGAUD"], "venue": "Paladyn Journal of Behavioral Robotics,", "citeRegEx": "STULP and SIGAUD,? \\Q2013\\E", "shortCiteRegEx": "STULP and SIGAUD", "year": 2013}, {"title": "Reinforcement Learning: An Introduction", "author": ["S. SUTTON R", "G. BARTO A"], "venue": null, "citeRegEx": "R. and A.,? \\Q1998\\E", "shortCiteRegEx": "R. and A.", "year": 1998}, {"title": "Policy gradient methods for reinforcement learning with function approximation", "author": ["S. SUTTON R", "D. MCALLESTER", "S. SINGH", "Y. MANSOUR"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "R. et al\\.,? \\Q2000\\E", "shortCiteRegEx": "R. et al\\.", "year": 2000}], "referenceMentions": [], "year": 2016, "abstractText": "Sample efficiency is a critical property when optimizing policy parameters for the controller of a robot. In this paper, we evaluate two state-of-the-art policy optimization algorithms. One is a recent deep reinforcement learning method based on an actor-critic algorithm, Deep Deterministic Policy Gradient (DDPG), that has been shown to perform well on various control benchmarks. The other one is a direct policy search method, Covariance Matrix Adaptation Evolution Strategy (CMA-ES), a black-box optimization method that is widely used for robot learning. The algorithms are evaluated on a continuous version of the mountain car benchmark problem, so as to compare their sample complexity. From a preliminary analysis, we expect DDPG to be more sample efficient than CMA-ES, which is confirmed by our experimental results.", "creator": "LaTeX with hyperref package"}}}