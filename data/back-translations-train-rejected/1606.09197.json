{"id": "1606.09197", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Model-Free Trajectory Optimization with Monotonic Improvement", "abstract": "Many of the recent Trajectory Optimization algorithms alternate between local approximation of the dynamics and conservative policy update. However, linearly approximating the dynamics in order to derive the new policy can bias the update and prevent convergence to the optimal policy. In this article, we propose a new model-free algorithm that backpropagates a local quadratic time-dependent Q-Function, allowing the derivation of the policy update in closed form. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics demonstrating improved performance in comparison to related Trajectory Optimization algorithms linearizing the dynamics.", "histories": [["v1", "Wed, 29 Jun 2016 17:39:09 GMT  (936kb,D)", "http://arxiv.org/abs/1606.09197v1", "10 pages, 4 figures, one page supplementary"], ["v2", "Fri, 12 May 2017 08:35:57 GMT  (920kb,D)", "http://arxiv.org/abs/1606.09197v2", null], ["v3", "Thu, 29 Jun 2017 09:07:37 GMT  (921kb,D)", "http://arxiv.org/abs/1606.09197v3", null]], "COMMENTS": "10 pages, 4 figures, one page supplementary", "reviews": [], "SUBJECTS": "cs.LG cs.RO", "authors": ["riad akrour", "abbas abdolmaleki", "hany abdulsamad", "jan peters", "gerhard neumann"], "accepted": false, "id": "1606.09197"}, "pdf": {"name": "1606.09197.pdf", "metadata": {"source": "META", "title": "Model-Free Trajectory Optimization for Reinforcement Learning", "authors": ["Riad Akrour", "Abbas Abdolmaleki", "Hany Abdulsamad", "Gerhard Neumann"], "emails": ["AKROUR@IAS.TU-DARMSTADT.DE", "ABBAS.A@UA.PT", "ABDULSAMAD@IAS.TU-DARMSTADT.DE", "NEUMANN@IAS.TU-DARMSTADT.DE"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "2. Notations", "text": "Consider a non-discounted finite horizon Markov decision-making process (MDP) of horizon T with the state space S-Rds and the space of action A-Rda. \u00b7 The transitional function p (st + 1 | st, at), which determines the probability (density) of transition to the state st + 1 after the execution of the act im st, is assumed to be time-independent; while there are time-dependent reward functions in t: S \u00b7 A 7 \u2192 R. A policy \u03c0 is defined by a series of time-dependent densities, where the probability of execution of the act is in st (a | s), where the probability of execution of the act in state s is in the time step t. The goal is to find the optimal policy in p-step 1."}, {"heading": "3. Information-Theoretic Policy Update", "text": "MOTO alternates between policy assessment and policy update. At each iteration i, the policy assessment step generates a series of M-rollouts1 from policy \u03c0i to estimate the Q-function Q-i (paragraph 4.1) and the state distribution \u03c1-i (paragraph 4.3) from samples. From these quantities, an information theoretical update of policy is derived - 1A rollout is a Monte Carlo simulation of a trajectory according to \u03c11, \u03c0 and p or the execution of \u03c0 on a physical system.Step as a solution to a limited optimization problem for calculating the new policy \u03c0i + 1."}, {"heading": "3.1. Optimization Problem", "text": "The aim of the policy update is to return to a new policy \u03c0i + 1, which maximizes the Q function Q-Q-i in anticipation of the state distribution p-i of the previous policy \u03c0i. However, in order to limit political oscillation between iterations, the KL w.r.t. \u03c0i is limited. Furthermore, the use of the KL divergence to define the step size of the policy update has been successfully applied in previous work (Peters et al., 2010; Levine & Abbeel, 2014; Schulman et al., 2015). Furthermore, we limit the entropy of i + 1 to better control the reduction in exploration. Therefore, the optimization problem for obtaining the exploration is given by the following non-linear program: maximize the KL optimization will not."}, {"heading": "3.2. Closed Form Update", "text": "Using the method of Lagrange multipliers, however, the solution of the optimization problem in Section 3.1 is given by \u03c0 \u2032 t (a | s) \u03c0t (a | s) \u0440 * / (\u03b7 \u0445 + \u03c9) exp (Q-T (s, a) \u03b7 * + \u03c9 *), (4), where \u03b7 * and \u03c9 * are the optimal Lagrange multipliers in connection with the KL and entropy constraints. Assuming that Q-T (s, a) has quadratic form in a and sQ-T (s, a) = 12 aTQaaa + a TQass + a + q (s), (5) with q (s) grouping of all terms of Q-T (s, a) that do not depend on a dependence 2 and sQ-T (a | s), then \u03c0 \u2032 t (a | s) is again of linear-Gauss + a-form + q (a-s), (a-s), (s + F-T)."}, {"heading": "3.3. Dual Minimization", "text": "Lagrange's multipliers \u03b7 and \u03c9 are minimized by minimizing the convex dual functions (\u03b7, \u03c9) = \u03b7 \u2212 \u03c9\u03b2 + (\u03b7 + \u03c9) \u0445 \u03c1 (s) log (\u0445 \u03c0 (a | s) \u03b7 / (\u03b7 + \u03c9) exp (Q-t (s, a) / (\u03b7 + \u03c9)) da) ds.The dual function is simplified thanks to the square form of Q-t (s, a) and can be used additionally in place of Qt (s, a) to update policy, but not to update policy.As such, and although we refer only to Qt (s, a) in this essay, the advantage function At (s, a) can be used instead of Qt (s, a) to optimize policy.The state distribution \u0456-t (s) = N (s | \u00b5s) to the function gt (s, a) can be interchangeable instead of Qs (a) to be used by Qs, a) to use the definition."}, {"heading": "4. Sample Efficient Policy Evaluation", "text": "The LL constraint introduced in the policy update leads to a non-linear optimization problem, which can still be solved in a closed form for the linear Gaussian policy class if the learned function is square in s and a. The first subsection presents the most important supervised learning problem solved during policy evaluation for learning Q-it, while the remaining subsections discuss how to improve its sampling efficiency."}, {"heading": "4.1. The Supervised Learning Problem", "text": "In the second half of the year in which the ECB raises interest rates, interest rates in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the third half of the year in the third half of the year in the second half of the year in the third half of the year in the third half of the year in the third half of the year in the second half of the year in the second half of the second half of the year in the third half of the third half of the third half of the second half of the year in the second half of the second half of the year in the second half of the second half of the year in the second half of the second half of the year in the second half of the second half of the year in the third half of the third half of the third half of the second half of the year in the second half of the second half of the second half of the year in the second half of the second half of the second half of the year in the second half of the second half of the second half of the year in the second half of the second half of the second half of the second half of the year in the second half of the second half of the second half of the year in the second half of the second half of the second half of the second half of the year in the second half of the second half of the second half of the second half of the second half of the second half of the second half of the year in the second half of the second half of the second half of the second half of the year in the second half of the second half of the second half of the second half of the year in the second half of the second half of the second half of the second half of the second half of the second half of the year in the second half of the second half of the year in the second half of the second half of the second half of the second half of the second half of the second half of the second half of the year in the second half of the second half of the second half of the second half of the second half of the second half of the year in the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second half of the second"}, {"heading": "4.2. Sample Reuse", "text": "In order to improve the sampling efficiency of our approach, we will reuse samples from different time steps and iterations using value samples. Let the expected loss that Q'it minimizes under the assumption of an infinite number of samples (s, a, s, s, w), where the loss \"it is the inner term within the sum in Eq. (6); the estimate Q, (s, a, a, k) is taken as in Eq. (7) and the expectation is in relation to the current state s, the act one (s, b) and the next state s, s, p (s, a).Reuse samples from different time steps. To use transition samples from all time steps in learning Q's, we rely on the importance sampling, where the importance of weight (IW) is cit by the ratio between the current time steps."}, {"heading": "4.3. Estimating the State Distribution", "text": "In practice, however, despite the errors made in past samples, the estimate could lead to errors. Since M-rollouts are evaluated randomly for each measure, only M-state samples are available for estimating the state distribution. (The first solution studied for estimating the state distribution is to take advantage of the estimated maximum distribution applied over time.) Starting from the one that is identical for all measures, the meaning of the sample is used to learn from the samples. (The first solution studied for estimating the state distribution is to take advantage of the estimated maximum distribution probability. (The second distinction is). (The first distinction is used to learn from the samples.) And the calculation of this IW depends only on the previously estimated state distribution. (In practice, the estimate of the actual distribution probability is weighted.) Where each of the st + 1 is weighted. (st / z: i) and the state distribution is estimated beforehand. (st / z)"}, {"heading": "5. Related Work", "text": "In the Approximate Policy Iteration Scheme (Szepesvari, 2010), political updates can potentially reduce the expected reward that leads to political oscillations (Wagner, 2011), unless the updated policy is \"close\" enough to the previous one (Kakade & Langford, 2002). (Pirotta et al., 2013b) refines the lower limit proposed in (Kakade & Langford, 2002), and produces more aggressive updates, but both approaches consider only discrete scope for action. (Pirotta et al., 2013a) offers an expansion into continuous domains, but only for one-dimensional scope for action. If the scope for action is continuous, which is typical of robotic applications, a stochastic policy and updating of these under a KL compulsion of \"proximity\" of successive strategies has shown that several empirical successes (Daniel et al., 2012; Levine & Koltun, 2014; Schulman et al., 2015)."}, {"heading": "6. Experimental Validation", "text": "The experimental section aims to analyze the proposed algorithm from four different angles: i) the quality of the returned policy compared to the current trajectory optimization algorithm; ii) the effectiveness of the proposed programs for variance reduction and sample reuse; iii) the contribution of additional entropy constraint in policy updates to the determination of better local optima; and iv) the algorithm's ability to scale to higher dimensional problems."}, {"heading": "6.1. Multi-link Swing-up Tasks", "text": "In fact, it is as if most people are able to survive themselves, and that they are able to survive themselves. (...) In fact, it is as if they are able to survive themselves. (...) In fact, it is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is not as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "6.2. Robot Table Tennis Task", "text": "The robotic table tennis task under consideration consists of a simulated robotic arm mounted on a floating base, with a racket connected to the end effector, which allows a prior contact with incoming balls on the opposite side of the table (Fig. 4).The arm has 9 degrees of freedom, comprising the six joints of the arm and the three linear joints of the base, which allow for a (small) 3D movement.Together with the common speeds and the 3D position of the incoming ball, the resulting state space is of dimensions da = 9 and consists of direct rotational speed commands. We use the analytical player of (Mu \u00bc lling et al., 2011) to generate a single forward stroke, which is then used to learn from the initial policy. The analytical player consists of a waiting phase (by holding the arm still), a preparation phase, a preparation phase, a punching phase and a return phase that puts back the waiting period."}, {"heading": "7. Conclusion", "text": "In this paper, we have proposed a new path optimization algorithm that, unlike other state-of-the-art algorithms, is not based on linearization of dynamics. Nevertheless, we are able to derive an efficient update of the guidelines by adapting a Q function locally and outperforming state-of-the-art path optimization methods for the more difficult tasks. One of the most important additions that would facilitate the transition from simulation to physical systems concerns the safety of exploration. On the technical side, the use of a more complex function to estimate the V function could be investigated to enable a refined trade-off between bias and variance."}, {"heading": "Acknowledgments", "text": "The research that led to these results was partly funded by the DFG LearnRobotS project within the framework of SPP 1527 Autonomous Learning."}, {"heading": "8. Dual Function", "text": "Remember the square shape of the Q function Q function Q function Q function (s, a) in plot a and state sQ point (s, a) = 12 aTQaaa + a TQass + a T qa + q (s). (9) The new policy \u03c0 t (a | s) in the solution of the limited maximization problem (FL) is again the linear-Gaussian form and the given letter sequence t (a \u2212 s) = N (a | FLs + Ff, F (s)))), so that the winning matrix, bias and covariance matrix of the number matrix (n) is not the function of the matrices F and L and the vector f-point f whereF = (f)."}], "references": [{"title": "Dynamic Porgramming and optimal control", "author": ["Bertsekas", "Dimitri P"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and P.,? \\Q1995\\E", "shortCiteRegEx": "Bertsekas and P.", "year": 1995}, {"title": "Dropping convexity for faster semi-definite optimization", "author": ["Bhojanapalli", "Srinadh", "Kyrillidis", "Anastasios T", "Sanghavi", "Sujay"], "venue": "CoRR, abs/1509.03917,", "citeRegEx": "Bhojanapalli et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bhojanapalli et al\\.", "year": 2015}, {"title": "Hierarchical Relative Entropy Policy Search", "author": ["C. Daniel", "G. Neumann", "J. Peters"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Daniel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Daniel et al\\.", "year": 2012}, {"title": "PILCO: A ModelBased and Data-Efficient Approach to Policy Search", "author": ["M. Deisenroth", "C. Rasmussen"], "venue": "In 28th International Conference on Machine Learning (ICML),", "citeRegEx": "Deisenroth and Rasmussen,? \\Q2011\\E", "shortCiteRegEx": "Deisenroth and Rasmussen", "year": 2011}, {"title": "A Survey on Policy Search for Robotics", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "Deisenroth et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Deisenroth et al\\.", "year": 2013}, {"title": "Learning Attractor Landscapes for Learning Motor Primitives", "author": ["A. Ijspeert", "S. Schaal"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "Ijspeert and Schaal,? \\Q2003\\E", "shortCiteRegEx": "Ijspeert and Schaal", "year": 2003}, {"title": "Differential dynamic programming. Modern analytic and computational methods in science and mathematics", "author": ["Jacobson", "David H", "Mayne", "David Q"], "venue": null, "citeRegEx": "Jacobson et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Jacobson et al\\.", "year": 1970}, {"title": "Approximately optimal approximate reinforcement learning", "author": ["Kakade", "Sham", "Langford", "John"], "venue": "In Machine Learning, Proceedings of the Nineteenth International Conference (ICML 2002),", "citeRegEx": "Kakade et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kakade et al\\.", "year": 2002}, {"title": "Learning complex neural network policies with trajectory optimization", "author": ["Levine", "Sergey", "Koltun", "Vladlen"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Levine et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2014}, {"title": "A Biomimetic Approach to Robot Table Tennis", "author": ["K. M\u00fclling", "J. Kober", "J. Peters"], "venue": "Adaptive Behavior Journal,", "citeRegEx": "M\u00fclling et al\\.,? \\Q2011\\E", "shortCiteRegEx": "M\u00fclling et al\\.", "year": 2011}, {"title": "Probabilistic differential dynamic programming", "author": ["Pan", "Yunpeng", "Theodorou", "Evangelos"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Pan et al\\.,? \\Q1915\\E", "shortCiteRegEx": "Pan et al\\.", "year": 1915}, {"title": "Relative Entropy Policy Search", "author": ["J. Peters", "K. M\u00fclling", "Y. Altun"], "venue": "In Proceedings of the 24th National Conference on Artificial Intelligence (AAAI). AAAI Press,", "citeRegEx": "Peters et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2010}, {"title": "Trust region policy optimization", "author": ["Schulman", "John", "Levine", "Sergey", "Abbeel", "Pieter", "Jordan", "Michael I", "Moritz", "Philipp"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning, ICML 2015,", "citeRegEx": "Schulman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schulman et al\\.", "year": 2015}, {"title": "Algorithms for Reinforcement Learning", "author": ["Szepesvari", "Csaba"], "venue": null, "citeRegEx": "Szepesvari and Csaba.,? \\Q2010\\E", "shortCiteRegEx": "Szepesvari and Csaba.", "year": 2010}, {"title": "Stochastic Differential Dynamic Programming", "author": ["E. Theodorou", "Y. Tassa", "E. Todorov"], "venue": "In Proceedings of the 29th American Control Conference,", "citeRegEx": "Theodorou et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Theodorou et al\\.", "year": 2010}, {"title": "Path Integral Stochastic Optimal Control for Rigid Body Dynamics. In ieee international symposium on approximate dynamic programming and reinforcement learning", "author": ["Theodorou", "Evangelos A", "J. Buchli", "S. Schaal"], "venue": null, "citeRegEx": "Theodorou et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Theodorou et al\\.", "year": 2009}, {"title": "Optimal control theory", "author": ["Todorov", "Emanuel"], "venue": "Bayesian Brain,", "citeRegEx": "Todorov and Emanuel.,? \\Q2006\\E", "shortCiteRegEx": "Todorov and Emanuel.", "year": 2006}, {"title": "Iterative local dynamic programming", "author": ["Todorov", "Emanuel", "Tassa", "Yuval"], "venue": "In IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Todorov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Todorov et al\\.", "year": 2009}, {"title": "Robot Trajectory Optimization using Approximate Inference", "author": ["M. Toussaint"], "venue": "In Proceedings of the 26th International Conference on Machine Learning,", "citeRegEx": "Toussaint,? \\Q2009\\E", "shortCiteRegEx": "Toussaint", "year": 2009}], "referenceMentions": [{"referenceID": 15, "context": "Trajectory Optimization methods based on stochastic optimal control (Todorov, 2006; Theodorou et al., 2009; Todorov & Tassa, 2009) have been very successful in learning high dimensional controls in complex settings such as end-to-end control of physical systems (Levine & Abbeel, 2014).", "startOffset": 68, "endOffset": 130}, {"referenceID": 4, "context": "As is common in Policy Search (Deisenroth et al., 2013), MOTO operates on a restricted class of parametrized policies \u03c0\u03b8, \u03b8 \u2208 R\u03b8 and is an iterative algorithm comprising two main steps, policy evaluation and policy update.", "startOffset": 30, "endOffset": 55}, {"referenceID": 11, "context": "The use of the KL divergence to define the step-size of the policy update has already been successfully applied in prior work (Peters et al., 2010; Levine & Abbeel, 2014; Schulman et al., 2015).", "startOffset": 126, "endOffset": 193}, {"referenceID": 12, "context": "The use of the KL divergence to define the step-size of the policy update has already been successfully applied in prior work (Peters et al., 2010; Levine & Abbeel, 2014; Schulman et al., 2015).", "startOffset": 126, "endOffset": 193}, {"referenceID": 1, "context": "Efficient algorithms for learning model parameters with a specific semidefinite shape are available (Bhojanapalli et al., 2015).", "startOffset": 100, "endOffset": 127}, {"referenceID": 4, "context": "(4) is an arbitrarily complex model then it is common that \u03c0\u2032 t, of linear-Gaussian form, is fit by weighted maximumlikelihood (Deisenroth et al., 2013); it is clear though from Eq.", "startOffset": 127, "endOffset": 152}, {"referenceID": 2, "context": "robotic applications, using a stochastic policy and updating it under a KL constraint to ensure \u2019closeness\u2019 of successive policies has shown several empirical successes (Daniel et al., 2012; Levine & Koltun, 2014; Schulman et al., 2015).", "startOffset": 169, "endOffset": 236}, {"referenceID": 12, "context": "robotic applications, using a stochastic policy and updating it under a KL constraint to ensure \u2019closeness\u2019 of successive policies has shown several empirical successes (Daniel et al., 2012; Levine & Koltun, 2014; Schulman et al., 2015).", "startOffset": 169, "endOffset": 236}, {"referenceID": 11, "context": "However, only an empirical sample estimate of the objective function is generally optimized (Peters et al., 2010; Schulman et al., 2015), which typically requires a high number of samples and precludes it from a direct application to physical systems.", "startOffset": 92, "endOffset": 136}, {"referenceID": 12, "context": "However, only an empirical sample estimate of the objective function is generally optimized (Peters et al., 2010; Schulman et al., 2015), which typically requires a high number of samples and precludes it from a direct application to physical systems.", "startOffset": 92, "endOffset": 136}, {"referenceID": 4, "context": "Learning more sophisticated models using for example Gaussian Processes was experimented by (Deisenroth & Rasmussen, 2011) and (Pan & Theodorou, 2014) in the Policy Search and Trajectory Optimization context, but it is still considered to be a challenging task, see (Deisenroth et al., 2013), chapter 3.", "startOffset": 266, "endOffset": 291}, {"referenceID": 14, "context": "Instances of such algorithms are for example iLQG (Todorov, 2006), DDP (Jacobson & Mayne, 1970; Theodorou et al., 2010), AICO (Toussaint, 2009) and the trajectory optimization algorithm used in the GPS algorithm (Levine & Abbeel, 2014).", "startOffset": 71, "endOffset": 119}, {"referenceID": 18, "context": ", 2010), AICO (Toussaint, 2009) and the trajectory optimization algorithm used in the GPS algorithm (Levine & Abbeel, 2014).", "startOffset": 14, "endOffset": 31}, {"referenceID": 9, "context": "We use the analytical player of (M\u00fclling et al., 2011) to generate a single forehand stroke, which is subsequently used to learn from demonstration the initial policy \u03c0.", "startOffset": 32, "endOffset": 54}, {"referenceID": 11, "context": "We compare MOTO to the REPS policy search algorithm (Peters et al., 2010) and the stochastic search algorithm MORE (Abdolmaleki et al.", "startOffset": 52, "endOffset": 73}], "year": 2016, "abstractText": "Many of the recent Trajectory Optimization algorithms alternate between local approximation of the dynamics and conservative policy update. However, linearly approximating the dynamics in order to derive the new policy can bias the update and prevent convergence to the optimal policy. In this article, we propose a new model-free algorithm that backpropagates a local quadratic time-dependent Q-Function, allowing the derivation of the policy update in closed form. Our policy update ensures exact KL-constraint satisfaction without simplifying assumptions on the system dynamics demonstrating improved performance in comparison to related Trajectory Optimization algorithms linearizing the dynamics.", "creator": "LaTeX with hyperref package"}}}