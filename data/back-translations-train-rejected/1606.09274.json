{"id": "1606.09274", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Jun-2016", "title": "Compression of Neural Machine Translation Models via Pruning", "abstract": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.", "histories": [["v1", "Wed, 29 Jun 2016 20:36:23 GMT  (5605kb,D)", "http://arxiv.org/abs/1606.09274v1", "Accepted to CoNLL 2016. 9 pages plus references"]], "COMMENTS": "Accepted to CoNLL 2016. 9 pages plus references", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.NE", "authors": ["abigail see", "minh-thang luong", "christopher d manning"], "accepted": false, "id": "1606.09274"}, "pdf": {"name": "1606.09274.pdf", "metadata": {"source": "CRF", "title": "Compression of Neural Machine Translation Models via Pruning", "authors": ["Abigail See", "Minh-Thang Luong", "Christopher D. Manning"], "emails": ["abisee@stanford.edu", "lmthang@stanford.edu", "manning@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to play by the rules they have imposed on themselves."}, {"heading": "2 Related Work", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "3 Our Approach", "text": "We will first give a brief overview of Neural Machine Translation before describing the interesting model architecture, the multi-layered recurring model with LSTM. Then we will explain the different types of NMT weights along with our approaches to cutting and retraining."}, {"heading": "3.1 Neural Machine Translation", "text": "Neural machine translation aims to directly model the conditional probability p (y | x) of translating a source sentence x1,.., xn, into a target sentence y1,.. ym. It achieves this goal using an encoder decoder framework (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). The encoder calculates a representation s (yt | y < t, s) for each source sentence. (1) Most NMT works use RNNs, but differ in terms of: (a) architecture that can be unidirectional, bidirectional, or deeply multi-layered."}, {"heading": "3.2 Understanding NMT Weights", "text": "In fact, it is so that it is a matter of a pure structure, in which it is a matter of an structure, in which it is a matter of an structure, in which it is a matter of an structure, in which it is a matter of an structure, in which it is a matter of an structure, in which it is a matter of an structure, in which it is a matter of an structure, in which it is a matter of a structure, in which it is a matter of an assembly, in which it is a matter of an assembly, in which it is a matter of an assembly, in which it is a matter of an assembly, of an assembly, in which it is a matter of an assembly, of an assembly, of an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, an assembly, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a structure, a, a structure, a structure, a structure, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a, a"}, {"heading": "3.3 Pruning Schemes", "text": "We follow the general size approach of Han et al. (2015b), which consists of pruning weights with the lowest absolute value. However, we question the authors \"pruning scheme with respect to the different weight classes and experiment with three pruning programs. Suppose we want to prune x% of the total parameters of the model. How do we distribute the cut across the different weight classes (illustrated in Figure 2) of our model? We propose to examine three different pruning programs: 1. Class blind: Let us take all parameters, sort them by size and crop the x% with the smallest size, regardless of the weight class. (So some classes are pruned proportionally more heavily than others).2. Class uniform: Within each class, we sort the weights by size and crop the x% with the smallest size. (Thus, all classes have pruned exactly x% of their parameters).3 Class inequality: These weight schemes are all the same, with the smallest weights."}, {"heading": "3.4 Retraining", "text": "To aggressively trim NMT models without sacrificing performance, we train our trimmed networks. That is, we continue to train the remaining weights while maintaining the sparse structure introduced by trimming. In our implementation, trimmed weights are represented by zeros in the weight matrices, and we use binary \"mask matrices,\" which represent the sparse structure of a network, to ignore updates of weights in trimmed places. This implementation has the advantage of simplicity as it requires minimal changes to the training and deployment code, but we note that more complex implementation with sparse matrices and sparse matrix multiplication could potentially lead to speed improvements. However, such implementation is outside the scope of this work."}, {"heading": "4 Experiments", "text": "We evaluate the effectiveness of our intersection approaches using a state-of-the-art NMT model. Specifically, an attention-based English-German NMT system by Luong et al. (2015a) is being considered. Training data is taken from WMT '14, consisting of 4.5 M sentence pairs (116M English words, 110M German words).For further details on the training hyper parameters, refer to Section 4.1 of Luong et al. (2015a).All models will be tested on the new year 2014 (2737 sentences).The model achieves a perplexity of 6.1 and a BLEU score of 20.5 (after unknown word exchange).2When retraining truncated NMT systems, we use the following settings: (a) we start with a lower learning rate of 0.5 (the original model uses a learning rate of 1.0), (b) we train for less epochs, 4 instead of 12, using simple Sc systems, and a learning time of 0.2 hours."}, {"heading": "4.1 Comparing pruning schemes", "text": "Despite its simplicity, in Figure 3 we find that the class-blind cut outperforms the other two programs in terms of translation quality at all trim percentages. To understand this result, we have trimmed each class individually for each of the three trimming programs and recorded the impact on performance (measured in terms of perplexity). Figure 4 shows that in a class-uniform cut, the overall performance loss is disproportionately caused by some classes: Target Layer 4, Attention and Softmax weights. Looking at Figure 5, we see that the most harmful classes being trimmed are also those with larger weights - these classes have much larger weights than others in the same perception, so the cut is more harmful under the class-uniform trimming program. Similarly, the situation with class-uniform trimming is similar. In contrast, Figure 4 shows that the damage caused by the class-blind cut is higher than the damage caused by the weak cut, the max and the target layer are reduced."}, {"heading": "4.2 Pruning and retraining", "text": "This shows the blue line in Figure 6. However, we note that up to about 40% of pruning remains largely unaffected, indicating a large amount of redundancy and over-parameterization in NMT. We are now looking at the effects of pruning models. The orange line in Figure 6 shows that after pruning the pruned models, the basic performance (20.48 BLEU) is both recovered and proven, up to 80% pruning (20.91 BLEU), with only a small loss of performance at 90% pruning. This may seem surprising as we do not expect a sparse model to perform a model with five times as many parameters. There are several possible explanations, two of which are instructed."}, {"heading": "4.3 Starting with sparse models", "text": "The favorable performance of the trimmed and retrained models raises the question: Can we get a shortcut to that performance by starting with sparse models? That is, instead of training, pruning and retraining, what if we just trim and then train? To test this, we took the sparse structure of our 50-90% trimmed models and trained entirely new models with the same sparse structure. The purple line in Figure 6 shows that the \"sparse from the outset\" models are not as powerful as the trimmed and retrained models, but they come close to basic performance. This shows that the sparse structure alone contains useful information about redundancy and therefore can produce a competitive compressed model, but it is important to link pruning with training. Although our method includes only one trimming step, other trimming methods allow trimming to interact more closely with training by including multiple iterations (Collins and Kohli, 2014; Hal, 2015b) that we would expect further improvements to be made."}, {"heading": "4.4 Storage size", "text": "The original uncropped model (a MATLAB file) has a size of 782 MB. The 80% cropped and retrained model has 272 MB, which corresponds to a reduction of 65.2%. In this work we focus on compression in terms of the number of parameters and not on the memory size, as it is invariable in all implementations."}, {"heading": "4.5 Distribution of redundancy in NMT", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "5 Generalizability of our results", "text": "To test the generalisability of our results, we are also testing our editing approach on a smaller, non-state-of-the-art NMT model, developed on the basis of the Vietnamese-English WIT3 dataset (Cettolo et al., 2012) and consisting of 133,000 sentence pairs. This model is effectively a scaled down version of the state-of-the-art model in Luong et al. (2015a), with fewer layers, smaller vocabulary, smaller hidden layer size, no attention mechanism and about 11% as many parameters in total. It achieves a BLEU score of 9.61 on the validation set.Although this model and its training set are on a different scale from our main model and the language pair differ, we found very similar results. In this model, it is possible to prune 60% of the parameters without any immediate loss of performance, and with re-training it is possible to prune 90% and regain the original performance as the models are not successful as most of the 4.1 models are, especially since our 4.5 observations are not successful in most of the 4.1 models."}, {"heading": "6 Future Work", "text": "As mentioned in Section 4.3, the compression and performance of our cutting method would probably be improved by multiple iterations of cutting and retraining. If possible, it would be extremely valuable to use the small number of cropped models to accelerate training and runtime, perhaps through sparse matrix representations and multiplications (see Section 3.4). Although we found that the size-based cut works very well, it would be instructive to verify the original claim that other cutting methods (e.g. optimal brain damage and optimal brain surgery) are more principled and conduct a comparative study."}, {"heading": "7 Conclusion", "text": "We have shown that weight reduction with retraining is a highly effective method of compression and regulation on a state-of-the-art NMT system, where the model is compressed to 20% of its size without sacrificing performance. Although we are the first to apply compression techniques to NMT, we get a similar level of compression to other current work on compressing state-of-the-art deep neural networks, with an approach that is simpler than most others. We have found that the absolute size of the parameters is of primary importance in selecting the parameters to intersect, leading to an approach that is extremely easy to implement and can be applied to any neural network. Finally, we have gained insights into the distribution of redundancy in the NMT architecture."}, {"heading": "8 Acknowledgment", "text": "This work was supported in part by the NSF Award IIS-1514268 and in part by a gift from Bloomberg L.P. We thank the Defense Advanced Research Projects Agency (DARPA) Communicating with Computers (CwC) under ARO prime Contract No. W911NF-15-1-0462. Finally, we thank NVIDIA Corporation for donating Tesla K40 GPUs."}], "references": [{"title": "Pruning algorithms of neural networks - a comparative study", "author": ["M. Gethsiyal Augasta", "Thangairulappan Kathirvalavakumar."], "venue": "Central European Journal of Computer Science, 3(3):105\u2013115.", "citeRegEx": "Augasta and Kathirvalavakumar.,? 2013", "shortCiteRegEx": "Augasta and Kathirvalavakumar.", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Wit: Web inventory of transcribed and translated talks", "author": ["Mauro Cettolo", "Christian Girardi", "Marcello Federico."], "venue": "EAMT.", "citeRegEx": "Cettolo et al\\.,? 2012", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T Wilson", "Stephen Tyree", "Kilian Q Weinberger", "Yixin Chen."], "venue": "ICML.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Memory bounded deep convolutional networks", "author": ["Maxwell D. Collins", "Pushmeet Kohli."], "venue": "arXiv preprint arXiv:1412.1442.", "citeRegEx": "Collins and Kohli.,? 2014", "shortCiteRegEx": "Collins and Kohli.", "year": 2014}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David."], "venue": "ICLR workshop.", "citeRegEx": "Courbariaux et al\\.,? 2015", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Exploiting linear structure within convolutional networks for efficient evaluation", "author": ["Emily L. Denton", "Wojciech Zaremba", "Joan Bruna", "Yann LeCun", "Rob Fergus."], "venue": "NIPS.", "citeRegEx": "Denton et al\\.,? 2014", "shortCiteRegEx": "Denton et al\\.", "year": 2014}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan."], "venue": "ICML.", "citeRegEx": "Gupta et al\\.,? 2015", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally."], "venue": "ICLR.", "citeRegEx": "Han et al\\.,? 2015a", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally."], "venue": "NIPS.", "citeRegEx": "Han et al\\.,? 2015b", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Second order derivatives for network pruning: Optimal brain surgeon", "author": ["Babak Hassibi", "David G. Stork."], "venue": "Morgan Kaufmann.", "citeRegEx": "Hassibi and Stork.,? 1993", "shortCiteRegEx": "Hassibi and Stork.", "year": 1993}, {"title": "Distilling the knowledge in a neural network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."], "venue": "NIPS Deep Learning Workshop.", "citeRegEx": "Hinton et al\\.,? 2015", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Squeezenet: Alexnet-level accuracy with 50x fewer parameters and< 0.5MB model size. arXiv preprint arXiv:1602.07360", "author": ["Forrest N. Iandola", "Matthew W. Moskewicz", "Khalid Ashraf", "Song Han", "William J. Dally", "Kurt Keutzer"], "venue": null, "citeRegEx": "Iandola et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Iandola et al\\.", "year": 2016}, {"title": "Speeding up convolutional neural networks with low rank expansions", "author": ["Max Jaderberg", "Andrea Vedaldi", "Andrew Zisserman."], "venue": "NIPS.", "citeRegEx": "Jaderberg et al\\.,? 2014", "shortCiteRegEx": "Jaderberg et al\\.", "year": 2014}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "ACL.", "citeRegEx": "Jean et al\\.,? 2015a", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Montreal neural machine translation systems for WMT\u201915", "author": ["S\u00e9bastien Jean", "Orhan Firat", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio."], "venue": "WMT.", "citeRegEx": "Jean et al\\.,? 2015b", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "EMNLP.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V. Le", "Navdeep Jaitly", "Geoffrey E. Hinton."], "venue": "arXiv preprint arXiv:1504.00941.", "citeRegEx": "Le et al\\.,? 2015", "shortCiteRegEx": "Le et al\\.", "year": 2015}, {"title": "Optimal brain damage", "author": ["Yann Le Cun", "John S. Denker", "Sara A. Solla."], "venue": "NIPS.", "citeRegEx": "Cun et al\\.,? 1989", "shortCiteRegEx": "Cun et al\\.", "year": 1989}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio."], "venue": "ICLR.", "citeRegEx": "Lin et al\\.,? 2016", "shortCiteRegEx": "Lin et al\\.", "year": 2016}, {"title": "Learning compact recurrent neural networks", "author": ["Zhiyun Lu", "Vikas Sindhwani", "Tara N. Sainath."], "venue": "ICASSP.", "citeRegEx": "Lu et al\\.,? 2016", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Stanford neural machine translation systems for spoken language domain", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "IWSLT.", "citeRegEx": "Luong and Manning.,? 2015", "shortCiteRegEx": "Luong and Manning.", "year": 2015}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["Minh-Thang Luong", "Christopher D. Manning."], "venue": "ACL.", "citeRegEx": "Luong and Manning.,? 2016", "shortCiteRegEx": "Luong and Manning.", "year": 2016}, {"title": "Effective approaches to attentionbased neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "EMNLP.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Minh-Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "ACL.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Auto-sizing neural networks: With applications to n-gram language models", "author": ["Kenton Murray", "David Chiang."], "venue": "EMNLP.", "citeRegEx": "Murray and Chiang.,? 2015", "shortCiteRegEx": "Murray and Chiang.", "year": 2015}, {"title": "On the compression of recurrent neural networks with an application to LVCSR acoustic modeling for embedded speech recognition", "author": ["Rohit Prabhavalkar", "Ouais Alsharif", "Antoine Bruguier", "Ian McGraw."], "venue": "ICASSP.", "citeRegEx": "Prabhavalkar et al\\.,? 2016", "shortCiteRegEx": "Prabhavalkar et al\\.", "year": 2016}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "ACL.", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Datafree parameter pruning for deep neural networks", "author": ["Suraj Srinivas", "R. Venkatesh Babu."], "venue": "BMVC.", "citeRegEx": "Srinivas and Babu.,? 2015", "shortCiteRegEx": "Srinivas and Babu.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "NIPS.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 31, "context": "Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 115, "endOffset": 157}, {"referenceID": 4, "context": "Neural Machine Translation (NMT) is a simple new architecture for translating texts from one language into another (Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 115, "endOffset": 157}, {"referenceID": 26, "context": "Despite being relatively new, NMT has already achieved state-of-the-art translation results for several language pairs including EnglishFrench (Luong et al., 2015b), English-German (Jean et al.", "startOffset": 143, "endOffset": 164}, {"referenceID": 29, "context": ", 2016), EnglishTurkish (Sennrich et al., 2016), and English-Czech (Jean et al.", "startOffset": 24, "endOffset": 47}, {"referenceID": 17, "context": ", 2016), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016).", "startOffset": 27, "endOffset": 72}, {"referenceID": 24, "context": ", 2016), and English-Czech (Jean et al., 2015b; Luong and Manning, 2016).", "startOffset": 27, "endOffset": 72}, {"referenceID": 25, "context": "For example, a recent state-of-the-art NMT system requires over 200 million parameters, resulting in a storage size of hundreds of megabytes (Luong et al., 2015a).", "startOffset": 141, "endOffset": 162}, {"referenceID": 11, "context": ", 1989) and Optimal Brain Surgeon (OBS) (Hassibi and Stork, 1993) techniques, which involve computing the Hessian matrix of the loss function with respect to the parameters, in order to assess the saliency of each parameter.", "startOffset": 40, "endOffset": 65}, {"referenceID": 0, "context": "However, the high computational complexity of OBD and OBS compare unfavorably to the computational simplicity of the magnitude-based approach, especially for large networks (Augasta and Kathirvalavakumar, 2013).", "startOffset": 173, "endOffset": 210}, {"referenceID": 5, "context": "Collins and Kohli (2014) prune 75% of AlexNet parameters with small accuracy loss on the ImageNet task, while Han et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 5, "context": "Collins and Kohli (2014) prune 75% of AlexNet parameters with small accuracy loss on the ImageNet task, while Han et al. (2015b) prune 89% of AlexNet parameters with no accuracy loss on the ImageNet task.", "startOffset": 0, "endOffset": 129}, {"referenceID": 27, "context": "Other approaches focus on pruning neurons rather than parameters, via sparsity-inducing regularizers (Murray and Chiang, 2015) or \u2018wiring together\u2019 pairs of neurons with similar input weights (Srinivas and Babu, 2015).", "startOffset": 101, "endOffset": 126}, {"referenceID": 30, "context": "Other approaches focus on pruning neurons rather than parameters, via sparsity-inducing regularizers (Murray and Chiang, 2015) or \u2018wiring together\u2019 pairs of neurons with similar input weights (Srinivas and Babu, 2015).", "startOffset": 192, "endOffset": 217}, {"referenceID": 25, "context": "Other approaches focus on pruning neurons rather than parameters, via sparsity-inducing regularizers (Murray and Chiang, 2015) or \u2018wiring together\u2019 pairs of neurons with similar input weights (Srinivas and Babu, 2015). These approaches are much more constrained than weight-pruning schemes; they necessitate finding entire zero rows of weight matrices, or near-identical pairs of rows, in order to prune a single neuron. By contrast weight-pruning approaches allow weights to be pruned freely and independently of each other. The neuron-pruning approach of Srinivas and Babu (2015) was shown to perform poorly (it suffered performance loss after removing only 35% of AlexNet parameters) compared to the weightpruning approach of Han et al.", "startOffset": 102, "endOffset": 582}, {"referenceID": 9, "context": "The neuron-pruning approach of Srinivas and Babu (2015) was shown to perform poorly (it suffered performance loss after removing only 35% of AlexNet parameters) compared to the weightpruning approach of Han et al. (2015b). Though Murray and Chiang (2015) demonstrates neuronpruning for language modeling as part of a (nonneural) Machine Translation pipeline, their approach is more geared towards architecture selection than compression.", "startOffset": 203, "endOffset": 222}, {"referenceID": 9, "context": "The neuron-pruning approach of Srinivas and Babu (2015) was shown to perform poorly (it suffered performance loss after removing only 35% of AlexNet parameters) compared to the weightpruning approach of Han et al. (2015b). Though Murray and Chiang (2015) demonstrates neuronpruning for language modeling as part of a (nonneural) Machine Translation pipeline, their approach is more geared towards architecture selection than compression.", "startOffset": 203, "endOffset": 255}, {"referenceID": 15, "context": "There are many other compression techniques for neural networks, including approaches based on on low-rank approximations for weight matrices (Jaderberg et al., 2014; Denton et al., 2014), or weight sharing via hash functions (Chen et al.", "startOffset": 142, "endOffset": 187}, {"referenceID": 7, "context": "There are many other compression techniques for neural networks, including approaches based on on low-rank approximations for weight matrices (Jaderberg et al., 2014; Denton et al., 2014), or weight sharing via hash functions (Chen et al.", "startOffset": 142, "endOffset": 187}, {"referenceID": 3, "context": ", 2014), or weight sharing via hash functions (Chen et al., 2015).", "startOffset": 46, "endOffset": 65}, {"referenceID": 6, "context": "Several methods involve reducing the precision of the weights or activations (Courbariaux et al., 2015), sometimes in conjunction with specialized hardware (Gupta et al.", "startOffset": 77, "endOffset": 103}, {"referenceID": 8, "context": ", 2015), sometimes in conjunction with specialized hardware (Gupta et al., 2015), or even using binary weights (Lin et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 21, "context": ", 2015), or even using binary weights (Lin et al., 2016).", "startOffset": 38, "endOffset": 56}, {"referenceID": 9, "context": "Some approaches use a sophisticated pipeline of several techniques to achieve impressive feats of compression (Han et al., 2015a; Iandola et al., 2016).", "startOffset": 110, "endOffset": 151}, {"referenceID": 14, "context": "Some approaches use a sophisticated pipeline of several techniques to achieve impressive feats of compression (Han et al., 2015a; Iandola et al., 2016).", "startOffset": 110, "endOffset": 151}, {"referenceID": 3, "context": ", 2014), or weight sharing via hash functions (Chen et al., 2015). Several methods involve reducing the precision of the weights or activations (Courbariaux et al., 2015), sometimes in conjunction with specialized hardware (Gupta et al., 2015), or even using binary weights (Lin et al., 2016). The \u2018knowledge distillation\u2019 technique of Hinton et al. (2015) involves training a small \u2018student\u2019 network on the soft outputs of a large \u2018teacher\u2019 network.", "startOffset": 47, "endOffset": 357}, {"referenceID": 22, "context": "There has been some recent work on compression for RNNs (Lu et al., 2016; Prabhavalkar et al., 2016), but it focuses on other, non-pruning compression techniques.", "startOffset": 56, "endOffset": 100}, {"referenceID": 28, "context": "There has been some recent work on compression for RNNs (Lu et al., 2016; Prabhavalkar et al., 2016), but it focuses on other, non-pruning compression techniques.", "startOffset": 56, "endOffset": 100}, {"referenceID": 9, "context": "We extend the magnitude-based pruning approach of Han et al. (2015b) to recurrent neural networks (RNN), in particular LSTM architectures for NMT, and to our knowledge we are the first to do so.", "startOffset": 50, "endOffset": 69}, {"referenceID": 18, "context": "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 63, "endOffset": 137}, {"referenceID": 31, "context": "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 63, "endOffset": 137}, {"referenceID": 4, "context": "It accomplishes this goal through an encoder-decoder framework (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014).", "startOffset": 63, "endOffset": 137}, {"referenceID": 13, "context": "Most NMT work uses RNNs, but approaches differ in terms of: (a) architecture, which can be unidirectional, bidirectional, or deep multilayer RNN; and (b) RNN type, which can be Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or the Gated Recurrent Unit (Cho et al.", "startOffset": 207, "endOffset": 241}, {"referenceID": 4, "context": "Most NMT work uses RNNs, but approaches differ in terms of: (a) architecture, which can be unidirectional, bidirectional, or deep multilayer RNN; and (b) RNN type, which can be Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) or the Gated Recurrent Unit (Cho et al., 2014).", "startOffset": 270, "endOffset": 288}, {"referenceID": 1, "context": "The hidden state at the top layer of the decoder is fed through an attention layer, which guides the translation by \u2018paying attention\u2019 to relevant parts of the source sentence; for more information see Bahdanau et al. (2015) or Section 3 of Luong et al.", "startOffset": 202, "endOffset": 225}, {"referenceID": 1, "context": "The hidden state at the top layer of the decoder is fed through an attention layer, which guides the translation by \u2018paying attention\u2019 to relevant parts of the source sentence; for more information see Bahdanau et al. (2015) or Section 3 of Luong et al. (2015a). Finally, for each target word, the top layer hidden unit is transformed by the softmax weights into a score vector of length V .", "startOffset": 202, "endOffset": 262}, {"referenceID": 1, "context": "The hidden state at the top layer of the decoder is fed through an attention layer, which guides the translation by \u2018paying attention\u2019 to relevant parts of the source sentence; for more information see Bahdanau et al. (2015) or Section 3 of Luong et al. (2015a). Finally, for each target word, the top layer hidden unit is transformed by the softmax weights into a score vector of length V . The target word with the highest score is selected as the output translation. Weight Subgroups in LSTM \u2013 For the aforementioned RNN block, we choose to use LSTM as the hidden unit type. To facilitate our later discussion on the different subgroups of weights within LSTM, we first review the details of LSTM as formulated by Zaremba et al. (2014) as follows: \uf8ec\uf8ec\uf8ed i f", "startOffset": 202, "endOffset": 739}, {"referenceID": 9, "context": "We follow the general magnitude-based approach of Han et al. (2015b), which consists of pruning weights with smallest absolute value.", "startOffset": 50, "endOffset": 69}, {"referenceID": 9, "context": "This is used by Han et al. (2015b).", "startOffset": 16, "endOffset": 35}, {"referenceID": 25, "context": "1 Specifically, an attention-based English-German NMT system from Luong et al. (2015a) is considered.", "startOffset": 66, "endOffset": 87}, {"referenceID": 25, "context": "1 Specifically, an attention-based English-German NMT system from Luong et al. (2015a) is considered. Training data was obtained from WMT\u201914 consisting of 4.5M sentence pairs (116M English words, 110M German words). For more details on training hyperparameters, we refer readers to Section 4.1 of Luong et al. (2015a). All models are tested on newstest2014 (2737 sentences).", "startOffset": 66, "endOffset": 318}, {"referenceID": 25, "context": "We thank the authors of Luong et al. (2015a) for providing their trained models and assistance in using the codebase at https://github.", "startOffset": 24, "endOffset": 45}, {"referenceID": 25, "context": "We thank the authors of Luong et al. (2015a) for providing their trained models and assistance in using the codebase at https://github.com/lmthang/nmt.matlab. The performance of this model is reported under row global (dot) in Table 4 of Luong et al. (2015a). same, such as mini-batch size 128, maximum gradient norm 5, and dropout with probability 0.", "startOffset": 24, "endOffset": 259}, {"referenceID": 5, "context": "Though our method involves just one pruning stage, other pruning methods interleave pruning with training more closely by including several iterations (Collins and Kohli, 2014; Han et al., 2015b).", "startOffset": 151, "endOffset": 195}, {"referenceID": 10, "context": "Though our method involves just one pruning stage, other pruning methods interleave pruning with training more closely by including several iterations (Collins and Kohli, 2014; Han et al., 2015b).", "startOffset": 151, "endOffset": 195}, {"referenceID": 22, "context": "In all eight matrices, we observe \u2014 as does Lu et al. (2016) \u2014 that the weights connecting to the input \u0125 are most crucial, followed by the input gate i, then the output gate o, then the forget gate f .", "startOffset": 44, "endOffset": 61}, {"referenceID": 19, "context": "Lastly, on close inspection, we notice several white diagonals emerging within some subsquares of the matrices in Figure 8, indicating that even without initializing the weights to identity matrices (as is sometimes done (Le et al., 2015)), an identity-like weight matrix is learned.", "startOffset": 221, "endOffset": 238}, {"referenceID": 2, "context": "To test the generalizability of our results, we also test our pruning approach on a smaller, nonstate-of-the-art NMT model trained on the WIT3 Vietnamese-English dataset (Cettolo et al., 2012), which consists of 133,000 sentence pairs.", "startOffset": 170, "endOffset": 192}, {"referenceID": 2, "context": "To test the generalizability of our results, we also test our pruning approach on a smaller, nonstate-of-the-art NMT model trained on the WIT3 Vietnamese-English dataset (Cettolo et al., 2012), which consists of 133,000 sentence pairs. This model is effectively a scaled-down version of the state-of-the-art model in Luong et al. (2015a), with fewer layers, smaller vocabulary size, smaller hidden layer size, no attention mechanism, and about 11% as many parameters in total.", "startOffset": 171, "endOffset": 338}], "year": 2016, "abstractText": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT\u201914 EnglishGerman translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.", "creator": "TeX"}}}