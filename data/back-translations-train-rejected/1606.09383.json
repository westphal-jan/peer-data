{"id": "1606.09383", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "On Approximate Dynamic Programming with Multivariate Splines for Adaptive Control", "abstract": "We define a SDP framework based on the RLSTD algorithm and multivariate simplex B-splines. We introduce a local forget factor capable of preserving the continuity of the simplex splines. This local forget factor is integrated with the RLSTD algorithm, resulting in a modified RLSTD algorithm that is capable of tracking time-varying systems. We present the results of two numerical experiments, one validating SDP and comparing it with NDP and another to show the advantages of the modified RLSTD algorithm over the original. While SDP requires more computations per time-step, the experiment shows that for the same amount of function approximator parameters, there is an increase in performance in terms of stability and learning rate compared to NDP. The second experiment shows that SDP in combination with the modified RLSTD algorithm allows for faster recovery compared to the original RLSTD algorithm when system parameters are altered, paving the way for an adaptive high-performance non-linear control method.", "histories": [["v1", "Thu, 30 Jun 2016 08:12:21 GMT  (37kb,D)", "http://arxiv.org/abs/1606.09383v1", "23 pages"]], "COMMENTS": "23 pages", "reviews": [], "SUBJECTS": "cs.LG cs.SY", "authors": ["willem eerland", "coen de visser", "erik-jan van kampen"], "accepted": false, "id": "1606.09383"}, "pdf": {"name": "1606.09383.pdf", "metadata": {"source": "CRF", "title": "On Approximate Dynamic Programming with Multivariate Splines for Adaptive Control", "authors": ["W.J. Eerland", "C.C. de Visser", "E. van Kampen"], "emails": ["[w.j.eerland@soton.ac.uk]"], "sections": [{"heading": "1 Introduction", "text": "The DP principle boils down to describing each state with a value in a value function and moving the system to the state with the highest value. However, the value function is also called a cost-to-go function and stores the expected sum of future rewards for each state. Generally, this function cannot be found directly, so an iterative approach such as Temporal Difference (TD) -learning [Sutton and Barto, 1998, Sutton, 1988] can be used. DP has the ability to solve complex control problems in different environments, and it is possible to view the environment as a black box by modelling it using system identification techniques. The advantage of combining DP and system identification techniques is that it leads to an adaptive control scheme. These adaptive controllers have already been successfully trained for many purposes, starting with agile missile interceptors."}, {"heading": "2 Preliminaries on Dynamic Programming", "text": "In this section, we present the preparatory work on DP, the algorithm that is part of the SDP framework. For a more complete description, we refer to Sutton and Barto [1998], Si et al. [2004], Powell [2007], Busoniu et al. [2010], Bertsekas [2007]. We begin with a brief overview of the MDP, followed by policy assessment and end with policy improvement."}, {"heading": "2.1 Markov Decision Processes", "text": "The solution to an MDP is a sequential optimisation problem where the objective is to find a policy that maximises the sum of the expected discounted rewards with an infinite horizon. The reward function is rt + 1 (xt + 1, ut) and 0 \u2264 \u03b3 < 1 is the discount factor. The aim is to find a policy that obtains the maximum total reward. For each policy there is a value function V \u03c0 (), which indicates a measure of long-term performance in each state: V (xt) = \u043c \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441knknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknknkn"}, {"heading": "2.2 Policy evaluation", "text": "To evaluate the value function iteratively, Sutton [1988] uses TD-Learning. For TD-Learning, the following must apply: V \u03c0 (xt) = rt + 1 + \u2211 \u221e k = t + 1 \u03b3k \u2212 trk + 1 = rt + 1 + \u03b3V \u03c0 (xt + 1) (2) If this equality does not apply, the difference is called a TD error: et = rt + 1 + \u03b3V \u03c0 (xt + 1) \u2212 V \u03c0 (xt) (3) By minimizing this TD error, the value function can be constructed in an iterative approach. In order to construct a value function for a continuous problem, parameterization is required to describe a complete state space with a finite number of parameters. The value function is now rendered V \u03c0 (xt, ct), where ct are the parameters at a given time that represent the continuous value function in domain X.In Bradtke and Barto [1996] the RLS-TAlgorithm step D is explained; the efficiency and the efficiency x is explained here."}, {"heading": "2.3 Policy improvement", "text": "The calculation of the value function is called policy evaluation, which can be used to select a greedy action. If a policy is updated in this way, it is called (greedy) policy improvement: ut (xt) = max ut U [V \u03c0 (xt)] (5) The repetition of policy evaluation and improvement is called policy iteration and leads to optimal policy [Sutton and Barto, 1998]. According to Doya [1996], the optimal nonlinear regulatory law is a function of the value cost differential: ut (xt) = u max g (1c \u0445 V \u03c0 (xt) \u2202 f (xt, ut) alifornice ut) (6), where umax is the maximum tax input, g (x) = tanh (\u03c02x), c is the tax cost parameter, \u03c4 is the increment parameter, and f (xt, ut) is the system dynamics. This optimal regulatory law is applied to the pendulum task, the result being A."}, {"heading": "3 Preliminaries on Multivariate Simplex B-Splines", "text": "This section serves as a brief introduction to the mathematical theory of simple B-splines. For a more comprehensive and general introduction to multivariate spline theory, we refer to Lai and Schumaker [2007]. We begin by introducing the basic concept of a single base polynomial and B-form, then introduce triangulation followed by the vector notation of the B-form. Finally, the estimator of the recursive smallest squares (RLS) for simple splines is checked."}, {"heading": "3.1 Simplex and barycentric coordinates", "text": "The polynomial basis of a multivariate simplex B spline is defined on a simplex. A simplex is defined by the undegenerated vertices (\u03c50, \u03c51,..., \u03c5n) and thus generates a span in n-dimensional space. Any point x = (x1, x2,..., xn) can be transformed in relation to a simplex into a barycentric coordinate b (x) = (b0, b1,..., bn). The relationship between the cartesian coordinate x and the barycentric coordinate b (x) is: x = n \u2211 i = 0 bi\u043e\u0441i = 0 bi = 1 (7)"}, {"heading": "3.2 Triangulation", "text": "Any number of simplifications can be combined into a triangulation, where a triangulation T is a special division of a domain into a series of J-non-overlapping simplifications defined in Lai and Schumaker [2007] as follows: T \u2261 J i = 1 ti, ti-tj-i, t-j-i, i-ti, tj-T, i 6 = j (8) with t-a-simplex of dimension (< n). A popular triangulation method is the Delaunay triangulation [Lee and Schachter, 1980]. Recently, a new method for generating globally optimal triangulations called intersplines was introduced in de Visser et al. [2012]."}, {"heading": "3.3 Basis functions", "text": "The polynomial basis of the simplex spline is the amber base polynomial Bd\u0445 (b), where d is the degree of the spline and b is the previously discussed barycentric coordinate: Bd\u0445 (b) = d! \u0440 (9) is a multi-index with properties: D! = \u03ba0! \u03ba1! \u00b7 \u00b7 \u03ban!, | \u0445 | = \u03ba0 + \u03ba1 + \u00b7 + \u03ban and b\u0445 = b\u03ba00 b \u03ba1 1 1 1 \u00b7 \u00b7 \u00b7 b\u03bann. Valid permutations of \u0445 with the constraint | = d correspond to the total number of B coefficients and base polynomials per simplex and are equal: d = (d + n)! n! d! (10) Combined with a series of J not overlapping simplices, the total number of B coefficients and base polynomials per simplex: d = (d + n)! d! Total number of coefficients >! = 10 does not result in a coefficient of B (in a combination of colilation B), i.e., the coefficient of B is a coefficient of 0.11."}, {"heading": "3.4 Vector formulation of the B-form", "text": "To complete the vector formula, Bdtj (b) and R d '1 are introduced as vectors of the amber base polynomials (Eq. 9) of Simplex tj, which are sorted lexicographically as of Eq. 12. Adapted by de Visser and Verhaegen [2013], we define the B form as a line vector on Simplex tj: p (b) = {Bd (b) > \u00b7 ctj, x-tj0, x / tj (13), where ctj are the B coefficients on Simplex tj. The matrix operation to evaluate the simplex B-spline function of the degree d and the continuity order r, defined on a triangulation is TJ: srd (b)."}, {"heading": "3.5 Continuity", "text": "Since p (b) is a linear combination of continuous functions, srd (b) is inherently continuous for any simplicity. However, in order to ensure the continuity of Srd (T) between the simplicities, constraints are imposed on the relations between the coefficients of different simplicities. The continuity order r fixes the derivatives drp dbron on the edges of adjacent simplicities. The required continuity conditions can be calculated with Lai and Schumaker [2007]: cti (\u03ba0,..., \u03ban \u2212 1, m) = \u2211 | = m c tj (\u03ba0,..., \u03ban \u2212 1.0) + \u03b3 Bm\u03b3 (w) 0 \u2264 m \u2264 r (18) Here w is a vertex of simple simplicity tj which is not found at the edge divided with simplex ti. All the constraints required for continuity are collected in the smoothing matrix H, with each row containing a new constraint equal to the columns of 2011."}, {"heading": "3.6 Approximation power", "text": "To describe the approximation force, the following definition of Lai and Schumaker [2007], Chapter 10.1 is used: Definition (approximation force of Srd (T) Fix 0 \u2264 r < d and 0 < \u03b8 \u2264 \u03c0 / 3. Let m be the largest integer, so that for each polygonal domain and for each regular triangulation T of the area usage angle with smallest angular constant, for each f-Wmq constant a toothing s-Srd (T) with | f-s-s | | q. If this applies to m = d + 1, we say that Srd has the full approximation force in the q standard."}, {"heading": "3.7 Recursive least squares", "text": "To initialize the P matrix, we use Z, the orthogonal projector on the zero space of H Lawson and Hanson [1974]: P1 = \u03b21 Z (21), where Z = (I \u2212 H + H), in which I and H are the identity or smoothness matrix, respectively. The parameter \u03b21 > 0 indicates confidence in the initial estimated parameters, with a greater \u03b21 indicating a lower confidence level. to initialize c, it is important to select them in such a way that the continuity constraints are not violated, using a limited minimum square (appropriateness) when the appropriateness (appropriateness) of 22 cannot be achieved."}, {"heading": "4 Spline Dynamic Programming", "text": "This section has a dual purpose: it introduces the framework that combines both simplex splines and the RLS-TD algorithm, and defines the modified RLS-TD algorithm with the ability to track systems that vary in time."}, {"heading": "4.1 The SDP framework", "text": "In order to successfully represent the optimal value function with a simple spline, a spline space must have sufficient approximation force at each point of the value function domain. While it is theoretically possible to have an infinite refinement in terms of triangulation, the idea behind parameterizing the value function is that there is no need for an infinite set of states, but only a limited set of parameters to describe the entire state space X. A triangulation consisting of nodes positioned in a grid is a good first estimate, since it distributes the approximation force evenly across the domain. What remains is to construct a spline space rd (T) are the polynomial degree and continuity sequence, parameters with a global effect on the spline function. Finally, to start the process, only the initial coefficients and covariance matrix need to be constructed."}, {"heading": "4.2 Recursive weighted least squares", "text": "In fact, it is the case that most of us are in a position to go into a different world, in which they are able to live in, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "5 Performance Evaluation of SDP", "text": "The proposed SDP framework condition itself consists of 20 s, with time increments of 0.002 s and 0.002 s as seen in A. (The proposed SDP framework condition was implemented on an approximate basis.) This is an incomplete control problem as seen in A. Initially, the controller has no knowledge of the optimum value function, and he must learn from the online measurements. (The aim of the experiment is to move and hold the pendulum in an ascending position by obtaining a limited number of experiments in each state where the top position is most advantageous. System dynamics is simulated using an Euler integration scheme in combination with the motion equations as measured in A.The performance of each study is measured by the maximum amount of time tup the pendulum is held in an ascending position, where the top position is most advantageous. (The system dynamics is measured by an Euler integration scheme, as it is presented by the combination of the pendulum in the same position as the pendulum in the pendulum it is presented in the combination of the pendulum in the same motion as the pendulum in the pendulum it is presented in the combination of the pendulum in the equilibrium it is)."}, {"heading": "5.1 Experiment I - The Stochastic System", "text": "In the first experiment, the four methods have the task of mastering a stochastic system. A stochastic system has a system error of \u03c3w = 3 \u0445 / s2. This means that the standard deviation of system noise is 60% of the influence of the input, since the maximum influence is Tmax / ml2 = 5 \u0445 / s2. Since system noise is applied to the motion equation, the effect of noise is only directly related to the influence of system noise on each method. Furthermore, it is shown that the deterministic system (\u03c3w = 0 \u0445 / s2) acts as a baseline.The results of the four methods are visible for the deterministic system in numbers 4 and 6, and for the stochastic system in numbers 5 and 7. It shows that the learning parameters and initialized weights are identical for both systems, while the NDP indicators are identical in both systems."}, {"heading": "5.2 Experiment II - The Time-Varying System", "text": "The second experiment involves controlling a time-varying system. To simulate this, the control system was first allowed to approach the optimum value function by performing 1000 learning attempts; the mass of the pendulum is then changed from m = 1 kg to m = 1.5 kg, and then 100 attempts are simulated, similar to the first experiment; the number of 100 attempts after the change is visible in Figures 8 and 9 for the NDP and SDP methods, respectively. However, by increasing the mass of the pendulum by 50%, the old control system does not stop functioning, and in many cases the tip can still be reached, although not as close as before the change (i.e. the pendulum is kept stationary at a slight angle).The modified optimum value function results in a TD error that spreads across the network.For the forward-facing network, a slight adjustment of the parameters is sufficient to adjust the global shape of the estimated value function."}, {"heading": "6 Discussion", "text": "The main difference between the neural networks and the simple splines for a DP system is that the neural networks are not linear in the parameters, while the simple splines are linear in the parameters. However, using a linear spline in the parameters allows the use of the RLS algorithm, which exhibits fast and detectable convergence within a stochastic framework."}, {"heading": "7 Conclusion", "text": "In this paper, the SDP framework was introduced; a combination of the RLS-TD algorithm and the multivariate Simplex-B splines. It was demonstrated that it is capable of solving the nonlinear control problem of pendulum swing with nothing but a reward function as feedback, in significantly fewer trials than NDP systems supported by function approximation systems with a comparable number of parameters. In addition, SDP presented greater resistance to system noise than NDP and showed no decrease in performance in the presence of interference. In addition, a forgetfulness method is introduced that maintains the continuity constraints that are merged with the RLS-TD algorithm to create an adaptive control system. In summary, the high convergence rate of the RLS-TD algorithm in combination with the high approximation performance of the multivariate simplex splines of the optimum B matching system is a result."}, {"heading": "A Pendulum Swing-Up Task", "text": "The dynamics of the non-linear control problem are determined by: ml2\u03b8 \u00b7 = \u2212 \u00b5\u03b8 \u2012 + possible sin \u03b8 + T (30), where \u03b8 is the angle from the upright position, T is the limited input torque, \u00b5 = 0.01 the coefficient of friction, m = 1.0kg the point mass at the rod tip, l = 1.0 m the length of the pendulum and g = 9.8 m / s2 the gravity acceleration. A schematic overview is shown in Fig. 10. The state vector is defined as x = [Douglas] > and the action as u = T. The complete system description is now: x \u00b2 = f (x, u) = (equilibrium deviation g-l sin.) + (0 1 ml2) u + (0 1) w (31), where w represents white noise with a standard deviation of Higw. The reward function of the system is: r = f (x, u) = cx (cos = 0.1) w (1) and vice versa (2) (1)."}], "references": [{"title": "Robust Reinforcement Learning Control Using Integral Quadratic Constrains for Recurrent Neural Networks", "author": ["C.W. Anderson", "P.M. Young", "M.R. Buehner", "J.N. Knight", "K.A. Bush", "D.C. Hittle"], "venue": "IEEE Transaction on neural networks,", "citeRegEx": "Anderson et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Anderson et al\\.", "year": 2007}, {"title": "Parallel MARS algorithm based on B-splines", "author": ["S. Bakin", "M. Hegland", "M. Osborne"], "venue": "Computational Statistics,", "citeRegEx": "Bakin et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bakin et al\\.", "year": 2000}, {"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE transactions on systems, man and cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman.,? \\Q1957\\E", "shortCiteRegEx": "Bellman.", "year": 1957}, {"title": "Polynomial Approximation - A New Computional Technique in Dynamic Programming: Allocation Processes", "author": ["R. Bellman", "R. Kalaba", "B. Kotkin"], "venue": "Mathematics of Computation,", "citeRegEx": "Bellman et al\\.,? \\Q1963\\E", "shortCiteRegEx": "Bellman et al\\.", "year": 1963}, {"title": "Dynamic Programming and Optimal Control, volume 2", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas.,? \\Q2007\\E", "shortCiteRegEx": "Bertsekas.", "year": 2007}, {"title": "Neuro-dynamic programming", "author": ["D.P. Bertsekas", "J.N. Tsitsiklis"], "venue": "Athena Scientific,", "citeRegEx": "Bertsekas and Tsitsiklis.,? \\Q1996\\E", "shortCiteRegEx": "Bertsekas and Tsitsiklis.", "year": 1996}, {"title": "Convergence and exponential convergence of identification algorithms with directional forgetting factor", "author": ["S. Bittanti", "P. Bolzern", "M. Campi"], "venue": null, "citeRegEx": "Bittanti et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Bittanti et al\\.", "year": 1990}, {"title": "Linear Least-Squares Algorithms for Temporal Difference Learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Machine Learning,", "citeRegEx": "Bradtke and Barto.,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto.", "year": 1996}, {"title": "Reinforcement Learning and Dynamic Programming Using Function Approximators", "author": ["L. Busoniu", "R. Babuska", "B. De Schutter", "D. Ernst"], "venue": "CRC Press,", "citeRegEx": "Busoniu et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Busoniu et al\\.", "year": 2010}, {"title": "Neural network and regression spline value function approximations for stochastic dynamic programming", "author": ["C. Cervellera", "A. Wen", "V. Chen"], "venue": "Computers and Operations Research,", "citeRegEx": "Cervellera et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cervellera et al\\.", "year": 2007}, {"title": "Application of orthogonal arrays and mars to inventory forecasting stochastic dynamic programs", "author": ["V. Chen"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Chen.,? \\Q1999\\E", "shortCiteRegEx": "Chen.", "year": 1999}, {"title": "Applying experimental design and regression splines to high-dimensional continuous-state stochastic dynamic programming", "author": ["V. Chen", "D. Ruppert", "C. Shoemaker"], "venue": "Operations Research,", "citeRegEx": "Chen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Chen et al\\.", "year": 1999}, {"title": "Splines and Efficiency in Dynamic Programming", "author": ["J.W. Daniel"], "venue": "Mathematical Analysis and Applications,", "citeRegEx": "Daniel.,? \\Q1976\\E", "shortCiteRegEx": "Daniel.", "year": 1976}, {"title": "B (asic)-spline basics", "author": ["C. de Boor"], "venue": "Mathematics Research Center, University of Wisconsin-Madison,", "citeRegEx": "Boor.,? \\Q1986\\E", "shortCiteRegEx": "Boor.", "year": 1986}, {"title": "Wavefront reconstruction in adaptive optics systems using nonlinear multivariate splines", "author": ["C.C. de Visser", "M. Verhaegen"], "venue": "JOSA A,", "citeRegEx": "Visser and Verhaegen.,? \\Q2013\\E", "shortCiteRegEx": "Visser and Verhaegen.", "year": 2013}, {"title": "A new approach to linear regression with multivariate splines", "author": ["C.C. de Visser", "Q.P. Chu", "J.A. Mulder"], "venue": null, "citeRegEx": "Visser et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Visser et al\\.", "year": 2009}, {"title": "Differential constraints for bounded recursive identification with multivariate splines", "author": ["C.C. de Visser", "Q.P. Chu", "J.A. Mulder"], "venue": null, "citeRegEx": "Visser et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Visser et al\\.", "year": 2011}, {"title": "Intersplines: A New Approach to Globally Optimal Multivariate Splines Using Interval Analysis", "author": ["C.C. de Visser", "E. van Kampen", "Q.P. Chu", "J.A. Mulder"], "venue": "Reliable Computing,", "citeRegEx": "Visser et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Visser et al\\.", "year": 2012}, {"title": "Temporal Difference Learning in Continuous Time and Space", "author": ["K. Doya"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Doya.,? \\Q1996\\E", "shortCiteRegEx": "Doya.", "year": 1996}, {"title": "Reinforcement Learning in Continuous Time and Space", "author": ["K. Doya"], "venue": "Neural Computation,", "citeRegEx": "Doya.,? \\Q2000\\E", "shortCiteRegEx": "Doya.", "year": 2000}, {"title": "Online Adaptive Critic Flight Control", "author": ["S. Ferrari", "R.F. Stengel"], "venue": "Journal of Guidance, Control and Dynamics,", "citeRegEx": "Ferrari and Stengel.,? \\Q2004\\E", "shortCiteRegEx": "Ferrari and Stengel.", "year": 2004}, {"title": "Multivariate Adaptive Regression Splines", "author": ["J.H. Friedman"], "venue": "The Annals of Statistics,", "citeRegEx": "Friedman.,? \\Q1991\\E", "shortCiteRegEx": "Friedman.", "year": 1991}, {"title": "Adaptive Critic Based Neural Networks for Control-Constrained Agile Missile Control", "author": ["D. Han", "S.N. Balakrishnan"], "venue": "Proceedings of the American Control Conference,", "citeRegEx": "Han and Balakrishnan.,? \\Q1999\\E", "shortCiteRegEx": "Han and Balakrishnan.", "year": 1999}, {"title": "Spline functions on triangulations", "author": ["M.J. Lai", "L.L. Schumaker"], "venue": null, "citeRegEx": "Lai and Schumaker.,? \\Q2007\\E", "shortCiteRegEx": "Lai and Schumaker.", "year": 2007}, {"title": "Solving least squares problems", "author": ["C.L. Lawson", "R.J. Hanson"], "venue": null, "citeRegEx": "Lawson and Hanson.,? \\Q1974\\E", "shortCiteRegEx": "Lawson and Hanson.", "year": 1974}, {"title": "Two algorithms for constructing a delaunay triangulation", "author": ["D.T. Lee", "B.J. Schachter"], "venue": "International Journal of Computer and Information Sciences,", "citeRegEx": "Lee and Schachter.,? \\Q1980\\E", "shortCiteRegEx": "Lee and Schachter.", "year": 1980}, {"title": "Theory and practice of recursive identification", "author": ["L. Ljung", "T. S\u00f6derstr\u00f6m"], "venue": null, "citeRegEx": "Ljung and S\u00f6derstr\u00f6m.,? \\Q1983\\E", "shortCiteRegEx": "Ljung and S\u00f6derstr\u00f6m.", "year": 1983}, {"title": "A convergent recursive least squares approximate policy iteration algorithm for multi-dimensional markov decision process with continuous state and action spaces", "author": ["J. Ma", "W.B. Powell"], "venue": "IEEE symposium on adaptive dynamic programming and reinforcement learning,", "citeRegEx": "Ma and Powell.,? \\Q2009\\E", "shortCiteRegEx": "Ma and Powell.", "year": 2009}, {"title": "Robust Reinforcement Learning", "author": ["J. Morimoto", "K. Doya"], "venue": "Neural Computation,", "citeRegEx": "Morimoto and Doya.,? \\Q2005\\E", "shortCiteRegEx": "Morimoto and Doya.", "year": 2005}, {"title": "Approximate Dynamic Programming", "author": ["W.B. Powell"], "venue": null, "citeRegEx": "Powell.,? \\Q2007\\E", "shortCiteRegEx": "Powell.", "year": 2007}, {"title": "Adaptive Critic Based Neurocontroller for Autolanding of Aircraft", "author": ["G. Saini", "S.N. Balakrishnan"], "venue": "Proceedings of the American Control conference,", "citeRegEx": "Saini and Balakrishnan.,? \\Q1997\\E", "shortCiteRegEx": "Saini and Balakrishnan.", "year": 1997}, {"title": "Generalized Polynomial Approximations in Markovian Decision Processes", "author": ["P.J. Schweitzer"], "venue": "Journal of Mathematical Analysis and Applications,", "citeRegEx": "Schweitzer.,? \\Q1985\\E", "shortCiteRegEx": "Schweitzer.", "year": 1985}, {"title": "Handbook of Learning and Approximate Dynamic Programming", "author": ["J. Si", "A. Barto", "W. Powell", "D. Wunsch"], "venue": "IEEE Press,", "citeRegEx": "Si et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Si et al\\.", "year": 2004}, {"title": "Approximate dynamic programming via sum of squares programming", "author": ["T.H. Summers", "K. Kunz", "N. Kariotoglou", "M. Kamgarpour", "S. Summers", "J. Lygeros"], "venue": "In European Control Conference (ECC),", "citeRegEx": "Summers et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Summers et al\\.", "year": 2013}, {"title": "Learning to Predict by the Methods of Temporal Differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton.,? \\Q1988\\E", "shortCiteRegEx": "Sutton.", "year": 1988}, {"title": "Reinforcement learning: an introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "An Analysis of Temporal-Difference Learning with Function Approximation", "author": ["J.N. Tsitsiklis", "B. Van Roy"], "venue": "IEEE transactions on automatic control,", "citeRegEx": "Tsitsiklis and Roy.,? \\Q1997\\E", "shortCiteRegEx": "Tsitsiklis and Roy.", "year": 1997}, {"title": "Self-tuning Systems: Control and Signal Processing", "author": ["P.E. Wellstead", "M.B. Zarrop"], "venue": null, "citeRegEx": "Wellstead and Zarrop.,? \\Q1991\\E", "shortCiteRegEx": "Wellstead and Zarrop.", "year": 1991}], "referenceMentions": [{"referenceID": 23, "context": "These adaptive controllers have already been successfully trained off-line for many purposes, ranging from agile missile interception [Han and Balakrishnan, 1999] to aircraft auto-landing and control [Saini and Balakrishnan, 1997].", "startOffset": 134, "endOffset": 162}, {"referenceID": 31, "context": "These adaptive controllers have already been successfully trained off-line for many purposes, ranging from agile missile interception [Han and Balakrishnan, 1999] to aircraft auto-landing and control [Saini and Balakrishnan, 1997].", "startOffset": 200, "endOffset": 230}, {"referenceID": 21, "context": "an on-line adaptive critic flight control was implemented on a six-degree-offreedom business jet aircraft over its full operating envelope, improving its performance when unexpected conditions are encountered for the first time [Ferrari and Stengel, 2004].", "startOffset": 228, "endOffset": 255}, {"referenceID": 2, "context": "Barto, Sutton and Anderson used neural networks to parametrise the value function [Barto et al., 1983].", "startOffset": 82, "endOffset": 102}, {"referenceID": 6, "context": "This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996].", "startOffset": 102, "endOffset": 134}, {"referenceID": 8, "context": "With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function.", "startOffset": 121, "endOffset": 146}, {"referenceID": 28, "context": "More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function.", "startOffset": 88, "endOffset": 109}, {"referenceID": 4, "context": "Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963].", "startOffset": 99, "endOffset": 121}, {"referenceID": 24, "context": "Furthermore, the spatial location of the B-coefficients and modularity of the triangulation allow for local model modification and refinement [Lai and Schumaker, 2007].", "startOffset": 142, "endOffset": 167}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability.", "startOffset": 61, "endOffset": 84}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function.", "startOffset": 61, "endOffset": 528}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function.", "startOffset": 61, "endOffset": 975}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP.", "startOffset": 61, "endOffset": 1331}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation.", "startOffset": 61, "endOffset": 1578}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation. According to Summers et al. [2013], using the sum of squares allows the use of higher order polynomials, but will eventually still lead to numerical instability.", "startOffset": 61, "endOffset": 1851}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation. According to Summers et al. [2013], using the sum of squares allows the use of higher order polynomials, but will eventually still lead to numerical instability. A recommendation in Ma and Powell [2009] is to use local polynomial regression to treat Markov Decision Process (MDP) problems with value functions of unknown form.", "startOffset": 61, "endOffset": 2019}, {"referenceID": 0, "context": "More recently, an adaptive controller has been introduced in Anderson et al. [2007], and again the neural network weights are bounded to guarantee stability. A drawback is that for a time-varying system these bounds shift and stability can no longer be guaranteed. This combination of neural networks and DP is commonly referred to as Neuro Dynamic Programming (NDP) [Bertsekas and Tsitsiklis, 1996]. There is a proof of convergence when linear-in-the-parameters function approximators are used in Tsitsiklis and Van Roy [1997], however, this proof demands knowledge of the shape of the optimal value function. With the Recursive Least Squares Temporal Difference (RLS TD) algorithm convergence in a stochastic framework is assured [Bradtke and Barto, 1996], even when the linear regression basis cannot perfectly fit the value function. In the last decade, the DP theory in continuous time and space has been further developed in Doya [1996, 2000], Morimoto and Doya [2005]. More recently, the proof of convergence has been extended to include the optimal policy [Ma and Powell, 2009], however one of the problems that remains is the a-priori unknown shape of the value function. Using polynomials as a function approximator in a DP framework was investigated by Bellman in 1963 [Bellman et al., 1963]. In 1985, Schweitzer [1985] concentrates on the use of global polynomials in combination with DP. With the development of the RLS TD algorithm, it is possible to obtain a proven convergence by combining it with a polynomial approximation as discussed in Ma and Powell [2009]. However the limitation is that the approximation power of global polynomials can only be increased by increasing the order of the polynomials, which will also lead to numerical instabilities in the solution schemes of the approximation. According to Summers et al. [2013], using the sum of squares allows the use of higher order polynomials, but will eventually still lead to numerical instability. A recommendation in Ma and Powell [2009] is to use local polynomial regression to treat Markov Decision Process (MDP) problems with value functions of unknown form. This recommendation is supported by Daniel [1976], which states that it is highly desirable from an efficiency point of view to use local polynomial regression.", "startOffset": 61, "endOffset": 2193}, {"referenceID": 22, "context": "There are existing approaches which combines Multivariate Adaptive Regression Splines (MARS) [Friedman, 1991] and DP [Chen, 1999, Chen et al.", "startOffset": 93, "endOffset": 109}, {"referenceID": 1, "context": "However, multivariate simplex B-splines distinguishes itself from MARS in terms of computational efficiency by using B-splines [Bakin et al., 2000].", "startOffset": 127, "endOffset": 147}, {"referenceID": 24, "context": "And they are supported by a triangulation of simplices, allowing functionality in a non-square domain [Lai and Schumaker, 2007].", "startOffset": 102, "endOffset": 127}, {"referenceID": 31, "context": "For a more complete description we refer to Sutton and Barto [1998], Si et al.", "startOffset": 44, "endOffset": 68}, {"referenceID": 30, "context": "For a more complete description we refer to Sutton and Barto [1998], Si et al. [2004], Powell [2007], Busoniu et al.", "startOffset": 69, "endOffset": 86}, {"referenceID": 28, "context": "[2004], Powell [2007], Busoniu et al.", "startOffset": 8, "endOffset": 22}, {"referenceID": 8, "context": "[2004], Powell [2007], Busoniu et al. [2010], Bertsekas [2007].", "startOffset": 23, "endOffset": 45}, {"referenceID": 5, "context": "[2010], Bertsekas [2007]. We start with a brief overview of the MDP followed by the policy evaluation and concluded with the policy improvement.", "startOffset": 8, "endOffset": 25}, {"referenceID": 35, "context": "This policy \u03c0\u2217 is called the optimal policy and can be found by applying both policy evaluation and policy improvement Sutton and Barto [1998]. The policy evaluation determines the V (xt) of the current policy \u03c0, where the policy improvement uses this knowledge to adjust the policy \u03c0 such that it ends up in the most valuable states.", "startOffset": 119, "endOffset": 143}, {"referenceID": 35, "context": "In order to evaluate the value function in an iterative fashion, Sutton [1988] uses TD-learning.", "startOffset": 65, "endOffset": 79}, {"referenceID": 35, "context": "Although RLS TD requires more computations per time-step than TD(\u03bb) algorithms [Sutton, 1988], it is more efficient in the statistical sense as more information is extracted from training experience, allowing it to converge faster [Bradtke and Barto, 1996].", "startOffset": 79, "endOffset": 93}, {"referenceID": 8, "context": "Although RLS TD requires more computations per time-step than TD(\u03bb) algorithms [Sutton, 1988], it is more efficient in the statistical sense as more information is extracted from training experience, allowing it to converge faster [Bradtke and Barto, 1996].", "startOffset": 231, "endOffset": 256}, {"referenceID": 8, "context": "In Bradtke and Barto [1996] the RLS TD algorithm was introduced; this algorithm and its computational complexity is visible in Table 1.", "startOffset": 3, "endOffset": 28}, {"referenceID": 8, "context": "Table 1: RLS TD algorithm, from Bradtke and Barto [1996] Step Action Computational Complexity (1) et = rt+1 \u2212 (Bt \u2212 \u03b3Bt+1)ct O(d\u0302) (2) Pt+1 = Pt \u2212 PtBt(Bt\u2212\u03b3Bt+1) Pt 1+(Bt\u2212\u03b3Bt+1)>PtBt O(\u00e2 ) (3) ct+1 = ct + Pt 1+(Bt\u2212\u03b3Bt+1)>PtBt Btet O(\u00e2 )", "startOffset": 32, "endOffset": 57}, {"referenceID": 36, "context": "The repetition of the policy evaluation and policy improvement is called policy iteration and will result in an optimal policy [Sutton and Barto, 1998].", "startOffset": 127, "endOffset": 151}, {"referenceID": 19, "context": "According to Doya [1996], the optimal non-linear feedback control law is a function of value function\u2019s gradient:", "startOffset": 13, "endOffset": 25}, {"referenceID": 24, "context": "For a more extensive and general introduction to multivariate spline theory we refer to Lai and Schumaker [2007]. We start by introducing the basic concept of a single basis polynomial and B-form, then introduce the triangulation, followed by the vector notation of the B-form.", "startOffset": 88, "endOffset": 113}, {"referenceID": 24, "context": "Any number of simplices can be combined into a triangulation, where a triangulation T is a special partitioning of a domain into a set of J non-overlapping simplices and is defined in Lai and Schumaker [2007] as:", "startOffset": 184, "endOffset": 209}, {"referenceID": 26, "context": "A popular triangulation method is Delaunay triangulation [Lee and Schachter, 1980].", "startOffset": 57, "endOffset": 82}, {"referenceID": 16, "context": "Recently, a new method for creating globally optimal triangulations named Intersplines was introduced in de Visser et al. [2012].", "startOffset": 108, "endOffset": 129}, {"referenceID": 24, "context": "The multi-index \u03ba has a requirement on the ordering, called a lexicographical sorting order which is introduced in Lai and Schumaker [2007]. This means \u03ba\u03bd\u03bc\u03ba comes before \u03baijk provided that \u03bd > i, or if \u03bd = i, then \u03bc > j, or if \u03bd = i and \u03bc = j, then \u03ba > k.", "startOffset": 115, "endOffset": 140}, {"referenceID": 15, "context": "Adapted from de Visser and Verhaegen [2013], we define the B-form as a row vector on simplex tj :", "startOffset": 16, "endOffset": 44}, {"referenceID": 24, "context": "We use the definition of the spline space from Lai and Schumaker [2007]: S d(T ) \u2261 {sd \u2208 C(T ) : sd|t \u2208 Pd,\u2200t \u2208 T } (17)", "startOffset": 47, "endOffset": 72}, {"referenceID": 24, "context": "The required continuity conditions can be calculated using Lai and Schumaker [2007]: ci (\u03ba0,.", "startOffset": 59, "endOffset": 84}, {"referenceID": 24, "context": "To describe the approximation power, the following definition from Lai and Schumaker [2007], chapter 10.", "startOffset": 67, "endOffset": 92}, {"referenceID": 24, "context": "The theory behind this is extensive and available in Lai and Schumaker [2007], however for now it is important to realize that |T | is a function of the longest edge in triangulation T .", "startOffset": 53, "endOffset": 78}, {"referenceID": 16, "context": "RLS is a method which allows the estimated parameters c to be updated online with the use of the parameter covariance matrix P de Visser et al. [2011]. The algorithm and the computational complexity is found in Table 2.", "startOffset": 130, "endOffset": 151}, {"referenceID": 16, "context": "RLS is a method which allows the estimated parameters c to be updated online with the use of the parameter covariance matrix P de Visser et al. [2011]. The algorithm and the computational complexity is found in Table 2. Note that it is essential to keep the column relations of P intact to enforce the constraints Hc = 0 from Eq. 19. To initialize the P matrix, we use Z, the orthogonal projector on the null-space of H Lawson and Hanson [1974]:", "startOffset": 130, "endOffset": 445}, {"referenceID": 16, "context": "For this, a constrained Least Squares (LS) fit of the estimated shape can be used, using the approach from de Visser et al. [2009]. If no knowledge is available, initialization of all coefficients at zero will satisfy the constraints, resulting in: c1 = 0 (22)", "startOffset": 110, "endOffset": 131}, {"referenceID": 16, "context": "Table 2: RLS algorithm from de Visser et al. [2011] Step Action Computational Complexity (1) t = yt \u2212Bt ct O(d\u0302) (2) Pt+1 = Pt \u2212 PtBtB > t Pt 1+Bt PtBt O(\u00e2) (3) ct+1 = ct + Pt+1Bt t O(\u00e2)", "startOffset": 31, "endOffset": 52}, {"referenceID": 15, "context": "Note that there is an efficient method to derive the directional derivatives, which are used in the optimal policy, available in de Visser et al. [2011]. While the framework functions in an infinite-time setting, the simulation has a time limit after which a new trial is started.", "startOffset": 132, "endOffset": 153}, {"referenceID": 8, "context": "As a consequence, the convergence proof as given in Bradtke and Barto [1996] applies.", "startOffset": 52, "endOffset": 77}, {"referenceID": 38, "context": "According to Wellstead and Zarrop [1991] the forget factor can be applied to the covariance matrix as: Pt+1 = \u03b2 \u22121 Pt (25)", "startOffset": 13, "endOffset": 41}, {"referenceID": 38, "context": "This approach is called directional forgetting Wellstead and Zarrop [1991] and updates the covariance matrix as follows: Pt+1 = Pt + \u03b22 BtB > t (26) where \u03b22 represents a forget factor applied to the updated parameters.", "startOffset": 47, "endOffset": 75}, {"referenceID": 26, "context": "To immediately apply the forget factor at time t, step (3) employs the Pt+1 matrix, as done in Ljung and S\u00f6derstr\u00f6m [1983]. The RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al.", "startOffset": 95, "endOffset": 123}, {"referenceID": 7, "context": "The RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al. [1990]. It should be noted that under this assumption, LMS algorithms also have proven convergence Tsitsiklis and Van Roy [1997].", "startOffset": 133, "endOffset": 156}, {"referenceID": 7, "context": "The RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al. [1990]. It should be noted that under this assumption, LMS algorithms also have proven convergence Tsitsiklis and Van Roy [1997]. Additionally, the modified RLS TD algorithm is capable of filtering out the residual noise to end up near the optimal coefficients c\u2217.", "startOffset": 133, "endOffset": 278}, {"referenceID": 7, "context": "The RLS algorithm with directional forgetting is simply convergent for a system where the data generation mechanism is deterministic Bittanti et al. [1990]. It should be noted that under this assumption, LMS algorithms also have proven convergence Tsitsiklis and Van Roy [1997]. Additionally, the modified RLS TD algorithm is capable of filtering out the residual noise to end up near the optimal coefficients c\u2217. With \u03b22 = 0, the filter has an infinite window in time, while at \u03b22 > 0, the window is infinite no longer, which has the advantage of being able to track time-varying systems and disadvantage of being susceptible to noise. This trade-off between noise filtering and tracking is an often returning phenomenon in adaptive control Wellstead and Zarrop [1991]. In principle \u03b22 > 0 only has a beneficial effect on the control of a time-varying system.", "startOffset": 133, "endOffset": 770}, {"referenceID": 17, "context": "As each trial is initialized in a random angle \u03b8 and a zero angle rate \u03b8\u0307 (consistent with the experiment in Morimoto and Doya [2005]), some trials require more swings to reach the top.", "startOffset": 122, "endOffset": 134}, {"referenceID": 5, "context": "More information on how these networks are constructed and trained is available in Bertsekas and Tsitsiklis [1996] and Rojas [1996].", "startOffset": 83, "endOffset": 115}, {"referenceID": 5, "context": "More information on how these networks are constructed and trained is available in Bertsekas and Tsitsiklis [1996] and Rojas [1996]. For SDP, a 4 degree spline space with 1 order continuity, without (\u03b22 = 0) and with (\u03b22 = 0.", "startOffset": 83, "endOffset": 132}, {"referenceID": 5, "context": "This increased learning rate of a dynamic programming algorithm in a stochastic system is a well known phenomenon, and is a result of the extra exploration that occurs due to the system noise Bertsekas and Tsitsiklis [1996]. Nevertheless the learning rate of SDP remains the highest in both the deterministic and stochastic system.", "startOffset": 192, "endOffset": 224}, {"referenceID": 8, "context": "Using a linear-in-the-parameters function approximator allows for the use of the RLS TD algorithm, which has a fast and proven convergence in a stochastic framework Bradtke and Barto [1996]. As a result, the SDP framework without forget factor (\u03b22 = 0) has proven con-", "startOffset": 165, "endOffset": 190}, {"referenceID": 14, "context": "Another option is to treat all unknown parameters as an additional optimization, and solve the entire optimization problem using Intersplines de Visser et al. [2012]. Unfortunately, at the moment Intersplines are limited to two-dimensional inputs, and require too much calculation power to make it attractive for real-time applications.", "startOffset": 145, "endOffset": 166}, {"referenceID": 13, "context": "While it is possible to use the multivariate simplex B-splines in higher dimensions de Boor [1986], there are two problems that arise.", "startOffset": 87, "endOffset": 99}, {"referenceID": 3, "context": "Secondly, due to the \u201ccurse of dimensionality\u201d Bellman [1957], the computational costs of dynamic programming are very high when moving to higher dimensions.", "startOffset": 47, "endOffset": 62}], "year": 2016, "abstractText": "We define a SDP framework based on the RLS TD algorithm and multivariate simplex B-splines. We introduce a local forget factor capable of preserving the continuity of the simplex splines. This local forget factor is integrated with the RLS TD algorithm, resulting in a modified RLS TD algorithm that is capable of tracking time-varying systems. We present the results of two numerical experiments, one validating SDP and comparing it with NDP and another to show the advantages of the modified RLS TD algorithm over the original. While SDP requires more computations per time-step, the experiment shows that for the same amount of function approximator parameters, there is an increase in performance in terms of stability and learning rate compared to NDP. The second experiment shows that SDP in combination with the modified RLS TD algorithm allows for faster recovery compared to the original RLS TD algorithm when system parameters are altered, paving the way for an adaptive highperformance non-linear control method.", "creator": "LaTeX with hyperref package"}}}