{"id": "1606.09577", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Ordering as privileged information", "abstract": "We propose to accelerate the rate of convergence of the pattern recognition task by directly minimizing the variance diameters of certain hypothesis spaces, which are critical quantities in fast-convergence results.We show that the variance diameters can be controlled by dividing hypothesis spaces into metric balls based on a new order metric. This order metric can be minimized as an ordinal regression problem, leading to a LUPI (Learning Using Privileged Information) application where we take the privileged information as some desired ordering, and construct a faster-converging hypothesis space by empirically restricting some larger hypothesis space according to that ordering. We give a risk analysis of the approach. We discuss the difficulties with model selection and give an innovative technique for selecting multiple model parameters. Finally, we provide some data experiments.", "histories": [["v1", "Thu, 30 Jun 2016 17:06:30 GMT  (17kb)", "http://arxiv.org/abs/1606.09577v1", "10 pages, 1 table, 2 page appendix giving proofs"]], "COMMENTS": "10 pages, 1 table, 2 page appendix giving proofs", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["thomas vacek"], "accepted": false, "id": "1606.09577"}, "pdf": {"name": "1606.09577.pdf", "metadata": {"source": "CRF", "title": "Ordering as privileged information", "authors": ["Tom Vacek"], "emails": ["vacek@cs.umn.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 6.09 577v 1 [cs.A I] 3 0Ju n"}, {"heading": "1 Introduction", "text": "This information is privileged because the learner can choose a hypothesis, but the privileged information will not be available to make decisions based on the hypotheses. This approach applies to discrimination-based hypotheses, in which predictions are derived from the threshold of the discriminatory value, which should be fully orderable. We show that the discriminatory order defines equivalence classes for the elements of space, and these classes are directly related to the hypotheses we seek. If we restrict the hypotheses to obtain only the good equivalence classes, we will establish a convergence class."}, {"heading": "2 What is order?", "text": "We cannot hope to provide any kind of overview of the field of order statistics. > Web search made this a lucrative and popular field. Nevertheless, we believe that our starting point is novel: Definition 1. It is any property of a set of real numbers that are invariable under any invariable increasing transformation condition. Suppose that any distribution P generates characteristics X - Rd, and assume that h1 and h2 hypotheses in a space H: Rd \u2192 R. If an increasing function m exists, so that for all X models P (X) = h2 (X), 2 then h1 and h2 are in the same equivalence class. The thread that follows the pattern recognition task is the growth function that measures the number of possible labels of a set of points of size n. We assume that H0 / 1 hypotheses will be set."}, {"heading": "2.1 Ordering metric", "text": "We define a set of definitions to meet this definition, so that two hypotheses are found in the same equivalence class if their metric distance is 0. (1) We define a different way in which there may be a higher risk. (2) We are unable to verify the truth between h1 and h2 isD (h1, h2). (2) We are unable to define the truth between h1 and h2 isD (h2). (2) The uniqueness of m is indispensable here.While many definitions would satisfy the metric definition, we have this definition. (2) The metric axioms (up to equivalence class elements) we are not capable of. (2) We are unable to establish the uniqueness of m. While many definitions would satisfy the metric axioms, we have chosen these definitions for two reasons. (2) We are unable to distinguish between the truth of m and the truth of h.While many definitions are able to satisfy the metric axioms, we have chosen these definitions for two reasons. (2) We are unable to distinguish between the truth of m and the truth in D. (2) We are unable to distinguish the truth in D. (2)."}, {"heading": "3 Putting the pieces together", "text": "The ordinal regression is the essential element of the definition, but to enable the use of this method, we must force the class balance. Suppose the user provides some parameters to favor or prevent a loss in a class. (1) It is the VC dimension to real functions. (1) It is the UK dimension of {h-max). (1) It is the UK dimension of {h-max). (1) It is the UK dimension of {h-max. (1) y-max. (1) y-max (1) y-0, y-0. (1) The VC dimension of this loss class can be applied in relation to the VC dimension for the underlying hypotheses space, which gives a similar limit. (1) We are ready for the main theorem: Theorem 2."}, {"heading": "5 GO-SVM", "text": "The global order SVM (GO-SVM) is the name for the formulation we propose. It is simply the usual SVM hypothesis space (thick hyperplanes) and loss (Scharge), but it is simultaneously with the Sashua & Levin [5] ordinal regression that fulfills an ordinal condition) as defined in Theorem 2. Loss and capacity control are traded between the two targets by user selectable weight classes. Capacity control in both SVM and ordinal regression formulations is achieved by the relationship between the quared norm of the predictor w and the size of the margin."}, {"heading": "6 Evaluation", "text": "The aim of the evaluation is to prove that the sequence of the space hypothesis enables faster convergence than a learning formulation that only takes into account the labels. Since GO-SVM is an extension of the standard SVM, it is a logical baseline, and we are only comparing it with it. However, these experiments are similar to other common experiments in the SVS literature, and we will pass them on to the reader where appropriate. Furthermore, due to the construction of the GO-SVM hypothesis, spaces are used that SVM cannot surpass due to a richer hypotheses space. Faster convergence is the only alternative explanation. The evaluation is not intended as a statement about the fitness of the hypotheses for the learning task, but only about the ability of the learner to select the best elements. The experimental setup is to hold a test set and the remaining examples of 12 random realizations of training and validation sets."}, {"heading": "6.1 Model selection", "text": "The Go-SVM formulation considers a model space of 3 dimensions: a parameter to control the complexity of the classification problem, a parameter to control the complexity of the problem of orderly regression, and a parameter to compensate for the loss between these two problems. All parameters are selected from fixed lists that were detailed supra. The most basic form of model selection requires selecting the best node of the grid. We found that traditional hold-out model selection strategies are more difficult with GO-SVM. The difficulty seems to be that the assumptions of structural risk minimization [4] no longer hold. In traditional SRM, the nested hypotheses spaces ensure that the loss of empirical risk mizers (as a function of complexity) is coercive, thereby selecting a minimum of risk minimization that is more reliable than when it occurred randomly. In our framework, there is no total order of hypotheses-spaces we do not have defined."}, {"heading": "6.2 Conclusions", "text": "A table of results is given in the table (6.1). Reminder: The quantities for training and testing are given in parentheses with the name of the experiment. The std, not smooth and smoothed methods have set a validation that is the same size as the amount of training. We first notice that the gap between the extended selection of the validation model and the performance of the typical technique is greater than that of the standard SVM. This is a boon as we have the opportunity to find a better model, but also a curse in that the variance is higher. It seems that this strain of LUPI methods is bound by model selection problems. The Gaussian smoothing approaches seem to be effective on the Digits data sets, and certainly they have not significantly impeded performance where the unsmoothed model selection proved superior. The MacKey Glass data set is the only one that has strongly significant results."}, {"heading": "7 Previous work", "text": "Most research, with limited exceptions, focuses on the development and evaluation of formulations [12, 13, 14, 15, 15] rather than trying to develop a theory to understand when and why such a technique might be useful. [16] analyze the SVM + algorithm in terms of boundaries of variance. Although this work places much emphasis on boundaries of variance, this work takes the SVM + loss function for granted and deduces boundaries for it, while this work works the other way around by trying to derive a formulation based on boundary. Lapin et. al. [11] suggest weighting examples based on the conditional probability of the class and most closely resembling the ideas proposed here. Intuitively, the method encourages the learner to prioritize performance on the simple examples over the hard examples."}, {"heading": "8 Appendix", "text": "The first result is a bridge between the conditions of variance and the metric balls of the hypotheses that we can actually define."}], "references": [{"title": "2009 special issue: A new learning paradigm: Learning using privileged information", "author": ["Vladimir Vapnik", "Akshay Vashist"], "venue": "Neural Netw.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Smooth discrimination analysis", "author": ["Enno Mammen", "Alexandre B. Tsybakov"], "venue": "Ann. Statist., 27(6):1808\u20131829,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "Optimal aggregation of classifiers in statistical learning", "author": ["Alexander B. Tsybakov"], "venue": "Ann. Statist., 32(1):135\u2013166,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2004}, {"title": "Statistical learning theory. Adaptive and learning systems for signal processing, communications, and control", "author": ["V.N. Vapnik"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1998}, {"title": "Ranking with large margin principle: Two approaches", "author": ["Amnon Shashua", "Anat Levin"], "venue": "In Advances in Neural Information Processing Systems 15 [Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2002}, {"title": "New support vector algorithms", "author": ["Bernhard Sch\u00f6lkopf", "Alex J. Smola", "Robert C. Williamson", "Peter L. Bartlett"], "venue": "Neural Comput.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "A generalized representer theorem", "author": ["Bernhard Sch olkopf", "Ralf Herbrich", "AlexJ. Smola"], "venue": "Computational Learning Theory,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2001}, {"title": "Nonlinear prediction of chaotic time series using support vector machines", "author": ["S. Mukherjee", "E. Osuna", "F. Girosi"], "venue": "In Neural Networks for Signal Processing", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1997}, {"title": "Learning using privileged information (LUPI) for modeling survival data", "author": ["Han-Tai Shiao", "Vladimir Cherkassky"], "venue": "In 2014 International Joint Conference on Neural Networks,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Learning using privileged information: SVM+ and weighted SVM", "author": ["Maksim Lapin", "Matthias Hein", "Bernt Schiele"], "venue": "Neural Networks,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning using privileged information: SVM+ and weighted SVM", "author": ["Maksim Lapin", "Matthias Hein", "Bernt Schiele"], "venue": "Neural Networks,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Boosting with side information", "author": ["Jixu Chen", "Xiaoming Liu", "Siwei Lyu"], "venue": "In Computer Vision - ACCV 2012 - 11th Asian Conference on Computer Vision, Daejeon,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Learning with hidden information using a max-margin latent variable model", "author": ["Ziheng Wang", "Tian Gao", "Qiang Ji"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Learning using privileged information in prototype based models", "author": ["Shereen Fouad", "Peter Tino", "Somak Raychaudhury", "Petra Schneider"], "venue": "Artificial Neural Networks and Machine Learning \u2013 ICANN 2012,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Classifier learning with hidden information", "author": ["Ziheng Wang", "Qiang Ji"], "venue": "Computer Vision and Pattern Recognition, IEEE Conference on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "On the theory of learning with privileged information", "author": ["D. Pechyony", "V. Vapnik"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "Theory of classification : a survey of some recent advances", "author": ["St\u00e9phane Boucheron", "Olivier Bousquet", "G\u00e1bor Lugosi"], "venue": "ESAIM: Probability and Statistics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2005}, {"title": "Local rademacher complexities", "author": ["Peter L. Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "venue": "Ann. Statist., 33(4):1497\u20131537,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "[1]) seeks to bring in privileged information to assist the learner.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "This paper proposes a LUPI method that directly minimizes the variance diameters of the hypothesis spaces under consideration, an essential quantity in fast-convergence literature [2, 3].", "startOffset": 180, "endOffset": 186}, {"referenceID": 2, "context": "This paper proposes a LUPI method that directly minimizes the variance diameters of the hypothesis spaces under consideration, an essential quantity in fast-convergence literature [2, 3].", "startOffset": 180, "endOffset": 186}, {"referenceID": 4, "context": "The regression problem can be made more tractable by relaxing it to alternative ordinal regression formulations, such as the one proposed by Sashua & Levin [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 4, "context": "It is simply is the usual SVM hypothesis space (thick hyperplanes) and loss (hinge), but it is simultaneously optimized with the Sashua & Levin [5] ordinal regression relaxation, with the SVM discriminant w constrained to be the same as the ordering hypothesis w.", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "Noting that the \u03bd-SVM formulations [6] have the margin as an optimization variable, we extended the approach so that the usual tradeoff between loss and capacity is preserved.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "Like \u03bd-SVM [6], the optimization problem can be characterized in terms of \u03bdb and \u03bdo and training data.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "The It can be proved that the problem is primal and dual feasible for \u03bdo \u2208 [0, 1], \u03b1 \u2208 [0, 1], and \u03bdb \u2208 [0, 2min(# positive examples,# negative examples)/n; and primal unbounded/dual infeasible otherwise.", "startOffset": 75, "endOffset": 81}, {"referenceID": 0, "context": "The It can be proved that the problem is primal and dual feasible for \u03bdo \u2208 [0, 1], \u03b1 \u2208 [0, 1], and \u03bdb \u2208 [0, 2min(# positive examples,# negative examples)/n; and primal unbounded/dual infeasible otherwise.", "startOffset": 87, "endOffset": 93}, {"referenceID": 6, "context": "The Representer Theorem [7] holds for GO-SVM, so the solution can be expressed in terms of the dual variables and kernels can be used.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "The first evaluation is up/down prediction of the MacKey-Glass synthetic timeseries [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 0, "context": "It was used in the LUPI setting (SVM+) in [1], where the authors used a 4-dimensional embedding (xt\u22123, xt\u22122, xt\u22121, xt) in order to predict xt+5 > xt.", "startOffset": 42, "endOffset": 45}, {"referenceID": 0, "context": "The last evaluation is handwritten digit recognition, which was used by Vapnik and Vahist [1] for SVM+ and slightly adapted by Lapin et al.", "startOffset": 90, "endOffset": 93}, {"referenceID": 9, "context": "for their proposed LUPI method [10].", "startOffset": 31, "endOffset": 35}, {"referenceID": 3, "context": "The trouble appears to be because the assumptions of structural risk minimization [4] no longer hold.", "startOffset": 82, "endOffset": 85}, {"referenceID": 10, "context": "We point out that many LUPI research papers require validation sets that would not ordinarily be a reasonable split between training and testing data (for example [11, 1].", "startOffset": 163, "endOffset": 170}, {"referenceID": 0, "context": "We point out that many LUPI research papers require validation sets that would not ordinarily be a reasonable split between training and testing data (for example [11, 1].", "startOffset": 163, "endOffset": 170}, {"referenceID": 0, "context": "The MacKey-Glass experiment appeared in the original SVM+ paper [1].", "startOffset": 64, "endOffset": 67}, {"referenceID": 10, "context": "The Digits experiment is intended to replicate one in [11].", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "The original SVM+ paper [1] touched off a fair amount of research in the area.", "startOffset": 24, "endOffset": 27}, {"referenceID": 11, "context": "Most research, with limited exceptions, has focused on developing and evaluating formulations [12, 13, 14, 15, 15] rather than attempting to develop theory to understand when and why such a technique might be useful.", "startOffset": 94, "endOffset": 114}, {"referenceID": 12, "context": "Most research, with limited exceptions, has focused on developing and evaluating formulations [12, 13, 14, 15, 15] rather than attempting to develop theory to understand when and why such a technique might be useful.", "startOffset": 94, "endOffset": 114}, {"referenceID": 13, "context": "Most research, with limited exceptions, has focused on developing and evaluating formulations [12, 13, 14, 15, 15] rather than attempting to develop theory to understand when and why such a technique might be useful.", "startOffset": 94, "endOffset": 114}, {"referenceID": 14, "context": "Most research, with limited exceptions, has focused on developing and evaluating formulations [12, 13, 14, 15, 15] rather than attempting to develop theory to understand when and why such a technique might be useful.", "startOffset": 94, "endOffset": 114}, {"referenceID": 14, "context": "Most research, with limited exceptions, has focused on developing and evaluating formulations [12, 13, 14, 15, 15] rather than attempting to develop theory to understand when and why such a technique might be useful.", "startOffset": 94, "endOffset": 114}, {"referenceID": 15, "context": "[16] analyze the SVM+ algorithm in terms of variance bounds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] propose weighting examples based on class conditional probability, and is most directly similar to the ideas proposed here.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "Unfortunately, the theoretical motivation for departing from empirical risk minimization takes a tenuous path through SVM+ [1, 16], namely that SVM+ is reducible to weighted learning.", "startOffset": 123, "endOffset": 130}, {"referenceID": 15, "context": "Unfortunately, the theoretical motivation for departing from empirical risk minimization takes a tenuous path through SVM+ [1, 16], namely that SVM+ is reducible to weighted learning.", "startOffset": 123, "endOffset": 130}], "year": 2016, "abstractText": "We propose to accelerate the rate of convergence of the pattern recognition task by directly minimizing the variance diameters of certain hypothesis spaces, which are critical quantities in fast-convergence results. We show that the variance diameters can be controlled by dividing hypothesis spaces into metric balls based on a new order metric. This order metric can be minimized as an ordinal regression problem, leading to a LUPI application where we take the privileged information as some desired ordering, and construct a faster-converging hypothesis space by empirically restricting some larger hypothesis space according to that ordering. We give a risk analysis of the approach. We discuss the difficulties with model selection and give an innovative technique for selecting multiple model parameters. Finally, we provide some data experiments.", "creator": "LaTeX with hyperref package"}}}