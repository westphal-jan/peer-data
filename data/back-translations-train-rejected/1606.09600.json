{"id": "1606.09600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Exploring Prediction Uncertainty in Machine Translation Quality Estimation", "abstract": "Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows.", "histories": [["v1", "Thu, 30 Jun 2016 18:10:46 GMT  (177kb,D)", "http://arxiv.org/abs/1606.09600v1", "Proceedings of CoNLL 2016"]], "COMMENTS": "Proceedings of CoNLL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["daniel beck", "lucia specia", "trevor cohn"], "accepted": false, "id": "1606.09600"}, "pdf": {"name": "1606.09600.pdf", "metadata": {"source": "CRF", "title": "Exploring Prediction Uncertainty in Machine Translation Quality Estimation", "authors": ["Daniel Beck", "Lucia Specia", "Trevor Cohn"], "emails": ["debeck1@sheffield.ac.uk,", "l.specia@sheffield.ac.uk,", "t.cohn@unimelb.edu.au"], "sections": [{"heading": "1 Introduction", "text": "This year it has come to the point where we will be able to put ourselves at the top, \"he said in an interview with the German Press Agency.\" We have never made as many mistakes as this year, \"he said.\" We have never made as many mistakes as this year, \"he said.\" We have never made as many mistakes as this year. \""}, {"heading": "2 Probabilistic Models for QE", "text": "Traditionally, QE is treated as a regression task with handmade functions. Kernel methods are probably the state of the art in QE, as they can easily model nonlinearity in the data. Furthermore, the scalability problems encountered with kernel methods do not tend to affect QE in practice, as the data sets are typically small, in the order of thousands of instances. The most popular method for QE is Support Vector Regression (SVR), as in the numerous cases of WMT QE tasks (Callisonburch et al., 2012; Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015). While SVR models can generate competitive predictions for this task, they lack a probable interpretation, making it difficult to extract uncertainty estimates."}, {"heading": "2.1 Gaussian Process Regression", "text": "Here we closely follow the definition of GP given by Rasmussen and Williams (2006). Let X = {(x1, y1), (x2, y2),.., (xn, yn)} be our data, with each x-dimensional input and y being the corresponding response variable. A GP is defined as a stochastic model of the latent function f, from which the data X: f (x) \u0445 GP (m (x), k (x, x)))), where m (x) is the mean function, which is usually the 0 constant, and k (x, x) is the core or covariance function, which describes the covariance between the values of f at the different places of x and x. The previous one is combined with a probability of Bayes to obtain a probability of the Bayes rule to obtain a posterible function: p (f | X) = p (y | X, f) (postble form), we form a posterial form, or we form an integral in the P, where we generate a P."}, {"heading": "2.2 Mate\u0300rn Kernels", "text": "Choosing a suitable kernel is a crucial step in defining a GP model (and any other kernel method). A common choice is to use the exposed square (EQ) kernel 1: kEQ (x, x \u2032) = \u03c3v exp (\u2212 r22), where r2 = D \u2211 i = 1 (xi \u2212 x \u2032 i) 2 l2iis the scaled distance between the two inputs, \u03c3v is a scale hyperparameter and l is a vector of longitudinal scales. Most core methods bind all longitudinal scales to a single value, resulting in an isotropic kernel. However, since GPs can efficiently perform hyperparameter optimization, it is common to apply a longitudinal scale per function, a method called Automatic Relevance Definition (ARD).The EQ kernel allows modelling of nonlinearity between the inputs and the reaction variables, but it makes a strong assumption that there will be a generalized function."}, {"heading": "2.3 Warped Gaussian Processes", "text": "The Gaussian probability of standard GPs has support across the entire real number line. A common way to deal with this problem is to model the logarithm of the response variables, which means that this transformation maps only positive values on the real line. However, there is no reason to think that this is the best possible mapping: it would be a better idea to learn it from the data. Delayed GPs (Snelson et al., 2004) are an extension of GPs that allows learning arbitrary mapping by placing a monotonous distortion function over the observations and modeling the distorted values within a standard GP. Posterior distribution is achieved by applying a modification of the variables: p (y) = f \u00b2 = f \u00b2 distortions."}, {"heading": "3 Intrinsic Uncertainty Evaluation", "text": "Given a number of different probabilistic QE models, we are interested in evaluating the performance of these models while taking into account their uncertainty, especially to distinguish between models with seemingly equal or similar performance. A simple way to measure the performance of a probabilistic model is to check its negative (log) marginal probability, but this measurement does not capture whether a model matches the training data. Instead, we can have a better generalization size by calculating the probability using test data. This has been suggested in previous work and is called Negative Log Predictive Density (NLPD) (Quin, onero-Candela et al., 2006): NLPD (y, y) = \u2212 1 n, p (y, i = yi | xi).Where y is a set of test predictions, y is the set of true labels and n the test set size. This measurement has since been largely adopted by the ML when assessing GPs and other QP models in the same way."}, {"heading": "3.1 Experimental Settings", "text": "Our experiments consist of data sets containing three different language pairs in which the label predicts time after editing: English-Spanish (en-es) This data set was used in the WMT14 QE shared task (Bojar et al., 2014). It contains 858 sentences that have been translated by a professional translator and have been post-edited by a professional translator. These data sets are part of the WMT16 QE shared task. This data set contains 2,525 sentences that have been translated by an MT system. English-German (en-de) This data set is part of the WMT16 QE translated."}, {"heading": "3.2 Results and Discussion", "text": "Table 1 shows the results obtained for all datasets, and the first two columns show an interesting finding in terms of model learning: The use of a warping function drastically reduces both NLL and NLPD. The main reason for this is that standard GPs distribute the probability mass over negative values, while the distorted models do not. As this dataset is significantly smaller than the others, we believe that this is evidence of overmatch and shows that NLL is not a reliable measurement for small datasets. In terms of various warping functions, the use of the parametric tanh function with 3 terms is better than the protocol for the fr-en and en-de datasets. This is not the case for the en-data."}, {"heading": "3.3 Qualitative Analysis", "text": "We show the distributions for a standard GP and a distorted GP with a tanh3 function in Figure 1. In the first case, where both models provide accurate predictions, we see that the distorted GP distribution peaks around the predicted value as it should be. It also gives positive values more probability mass and shows that the model is able to learn that the label is not negative. In the second case, we analyze the distributions when both models make inaccurate predictions. We see that the distorted GP is able to provide a wider distribution in this case, while most of the mass remains outside the negative range. We also report on each graph in Figure 1 the NLPD for each prediction. If we compare only the distorted GP predictions, we can see that their values reflect the distributions when predictions are accurate and wider when predictions are not accurate."}, {"heading": "4 Asymmetric Risk Scenarios", "text": "However, this assumption is too simplistic for many potential applications of QE. Example: \u2022 In a post-editing scenario, a project manager may have translators with limited expertise in post-editing. \u2022 In this case, the translator should not be provided with automatic translations unless they are likely to be of very good quality. This can be forced by increasing the penalty weight for underestimation. We call this the pessimistic scenario. \u2022 In a greed scenario, a company wants to automatically translate its product reviews so that they can be published in a foreign language without human intervention. The company would prefer to publish only the reviews well enough, but publishing more reviews will increase the chances of selling products."}, {"heading": "4.1 Bayes Risk for Asymmetric Losses", "text": "The losses presented above can be incorporated directly into learning algorithms to obtain models for a given scenario. In the context of AL loss, this is called quantity regression (Koenker, 2005), because optimal estimates for this loss are posterior quantities. However, in a production environment, the loss may change over time. In the gisting scenario discussed above parameter w, for example, the loss could be modified based on feedback from revenue indicators or user experience. If the loss is appended to the underlying learning algorithms, a change in w requires complete model retraining, which can be costly. Instead of retraining the model every time another loss occurs. Instead, we can train a single probability model and derive Bayes risk estimates for the loss we are interested in. This allows estimates to be obtained without having to retrain models when the loss changes. In addition, this model allows different losses to be applied at the same time using the same scenario."}, {"heading": "4.2 Experimental Settings", "text": "Here we evaluate the models and datasets used in Section 3.1 in terms of their performance in the asymmetric environment. Following the explanation in the previous section, we do not retrain: we collect the predictions we obtained using the 10-fold cross-validation protocol and apply various Bayes estimates that correspond to the asymmetric losses. The evaluation is done with the same loss used in the estimator (for example, if we use the Linex estimator with w = 0.75, we report the results with the same w) and are averaged over the 10 folders. To simulate both pessimistic and optimistic scenarios, we use w = 3, 1 / 3} for the AL loss and w = 0.75} for the linex loss. The only exception is the en-de dataset in which we report results for w = 0.25, 0.75 for linex3."}, {"heading": "4.3 Results and Discussion", "text": "The protocol-based models also provide good results for AL, but for linex, the results are mixed except en-es. This is probably again related to the larger sizes of fr-en and en-de datasets, which allows the tanh-based models to learn richer representations.3The use of w = \u2212 0.75 in this case resulted in loss values in the order of 107. In fact, as discussed in the next section, the results for the linex loss were inconclusive in the pessimistic scenario. However, we report on results that use a higher w in this case for completeness and to clarify the inconclusive trends we found. 4We have also tried w = 1 / 9, 1 / 7, 1 / 5, 5, 7, 9} for the AL loss and w = 0.5, \u2212 0.25, 0.5} for the forecast losses. This pessimistic scenario shows that the ex-GP trends are more consistent compared to the AL trends when the trends are not consistent, while the AL trends are not."}, {"heading": "5 Related Work", "text": "Quality Estimation is generally described as a text regression task, similar to many other applications such as film revenue forecasts based on reviews (Joshi et al., 2010; Bitvai and Cohn, 2015) and emotion recognition in news headlines (Strapparava and Mihalcea, 2008; Beck et al., 2014a) and song lyrics (Mihalcea and Strapparava, 2012). Generally, these applications are evaluated in terms of their score predictions, probably because not all of them use probability models.The NLPD is a common and established metric used in the GP literature to evaluate new approaches. Examples are the original work on Warped GPs (Snelson et al., 2004), but also others such as La'zaro-Gredilla (2012) and Chalupka et al. (2013). It has also been used to evaluate recent work on uncertainty propagation methods neural networks (Herna-Adams) and Lobna (2015)."}, {"heading": "6 Conclusions", "text": "By evaluating models that use NLPD, we can make more informed decisions about which model should be used for different settings. In addition, we have shown how information in the predictive distribution can be used in asymmetric loss scenarios and how the proposed models can be useful in these settings. Uncertainty estimates can also be useful in many other settings that go beyond those examined in this paper. Active learning can benefit from variance information in their query methods and they have proven useful for QE (Beck et al., 2013). Explorative analysis is another way of future work where error bars can provide further insight into the task, as shown in recent work (Nguyen and O dging, 2015)."}, {"heading": "Acknowledgements", "text": "Daniel Beck was supported by CNPq / Brazil (No. 237999 / 2012-9), Lucia Specia was supported by the QT21 project (H2020 No. 645452), Trevor Cohn is a recipient of an Australian Research Council Future Fellowship (Project No. FT130101105), and the authors thank James Hensman for his advice on Warped GPs and the three anonymous critics for their comments."}], "references": [{"title": "Query learning strategies", "author": ["Abe", "Mamitsuka1998] Naoki Abe", "Hiroshi Mamitsuka"], "venue": null, "citeRegEx": "Abe et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Abe et al\\.", "year": 1998}, {"title": "Reducing Annotation Effort for Quality Estimation via Active Learning", "author": ["Beck et al.2013] Daniel Beck", "Lucia Specia", "Trevor Cohn"], "venue": "In Proceedings of ACL", "citeRegEx": "Beck et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2013}, {"title": "Joint Emotion Analysis via Multi-task Gaussian Processes", "author": ["Beck et al.2014a] Daniel Beck", "Trevor Cohn", "Lucia Specia"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Beck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2014}, {"title": "SHEF-Lite 2.0 : Sparse Multitask Gaussian Processes for Translation Quality Estimation", "author": ["Beck et al.2014b] Daniel Beck", "Kashif Shah", "Lucia Specia"], "venue": "In Proceedings of WMT14,", "citeRegEx": "Beck et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Beck et al\\.", "year": 2014}, {"title": "Non-Linear Text Regression with a Deep Convolutional Neural Network", "author": ["Bitvai", "Cohn2015] Zsolt Bitvai", "Trevor Cohn"], "venue": "In Proceedings of ACL", "citeRegEx": "Bitvai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bitvai et al\\.", "year": 2015}, {"title": "Confidence estimation for machine translation", "author": ["Blatz et al.2004] John Blatz", "Erin Fitzgerald", "George Foster"], "venue": "In Proceedings of the 20th Conference on Computational Linguistics,", "citeRegEx": "Blatz et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Blatz et al\\.", "year": 2004}, {"title": "Findings of the 2015 Workshop on Statistical Machine Translation", "author": ["Turchi."], "venue": "Proceedings of WMT15, pages 22\u201364.", "citeRegEx": "Turchi.,? 2015", "shortCiteRegEx": "Turchi.", "year": 2015}, {"title": "Real Estate Price Prediction under Asymmetric Loss", "author": ["Cain", "Janssen1995] Michael Cain", "Christian Janssen"], "venue": "Annals of the Institute of Statististical Mathematics,", "citeRegEx": "Cain et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Cain et al\\.", "year": 1995}, {"title": "A Framework for Evaluating Approximation Methods for Gaussian Process Regression", "author": ["Christopher K.I. Williams", "Iain Murray"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Chalupka et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chalupka et al\\.", "year": 2013}, {"title": "Optimal Prediction Under Asymmetric Loss", "author": ["Christoffersen", "Francis X. Diebold"], "venue": null, "citeRegEx": "Christoffersen et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Christoffersen et al\\.", "year": 1997}, {"title": "Modelling Annotator Bias with Multi-task Gaussian Processes: An Application to Machine Translation Quality Estimation", "author": ["Cohn", "Specia2013] Trevor Cohn", "Lucia Specia"], "venue": "In Proceedings of ACL,", "citeRegEx": "Cohn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2013}, {"title": "Improving Evaluation of Machine Translation Quality Estimation", "author": ["Yvette Graham"], "venue": "In Proceedings of ACL", "citeRegEx": "Graham.,? \\Q2015\\E", "shortCiteRegEx": "Graham.", "year": 2015}, {"title": "Gaussian Processes for Big Data", "author": ["Nicol\u00f2 Fusi", "Neil D. Lawrence"], "venue": "In Proceedings of UAI,", "citeRegEx": "Hensman et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hensman et al\\.", "year": 2013}, {"title": "Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks", "author": ["Hern\u00e1ndez-Lobato", "Ryan P. Adams"], "venue": "In Proceedings of ICML", "citeRegEx": "Hern\u00e1ndez.Lobato et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hern\u00e1ndez.Lobato et al\\.", "year": 2015}, {"title": "Movie Reviews and Revenues: An Experiment in Text Regression", "author": ["Joshi et al.2010] Mahesh Joshi", "Dipanjan Das", "Kevin Gimpel", "Noah A. Smith"], "venue": "In Proceedings of NAACL", "citeRegEx": "Joshi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Joshi et al\\.", "year": 2010}, {"title": "Quantile Regression", "author": ["Roger Koenker"], "venue": null, "citeRegEx": "Koenker.,? \\Q2005\\E", "shortCiteRegEx": "Koenker.", "year": 2005}, {"title": "Postediting time as a measure of cognitive effort", "author": ["Wilker Aziz", "Luciana Ramos", "Lucia Specia"], "venue": "In Proceedings of WPTP", "citeRegEx": "Koponen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Koponen et al\\.", "year": 2012}, {"title": "Bayesian Warped Gaussian Processes", "author": ["Miguel L\u00e1zaro-Gredilla"], "venue": "In Proceedings of NIPS,", "citeRegEx": "L\u00e1zaro.Gredilla.,? \\Q2012\\E", "shortCiteRegEx": "L\u00e1zaro.Gredilla.", "year": 2012}, {"title": "Lyrics, Music, and Emotions", "author": ["Mihalcea", "Strapparava2012] Rada Mihalcea", "Carlo Strapparava"], "venue": "In Proceedings of the Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Mihalcea et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2012}, {"title": "Asymmetric Least Squares Estimation and Testing", "author": ["Newey", "Powell1987] Whitney K. Newey", "James L. Powell"], "venue": null, "citeRegEx": "Newey et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Newey et al\\.", "year": 1987}, {"title": "Posterior Calibration and Exploratory Analysis for Natural Language Processing Models", "author": ["Nguyen", "O\u2019Connor2015] Khanh Nguyen", "Brendan O\u2019Connor"], "venue": "In Proceedings of EMNLP, number September,", "citeRegEx": "Nguyen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Evaluating Predictive Uncertainty Challenge", "author": ["Carl Edward Rasmussen", "Fabian Sinz", "Olivier Bousquet", "Bernhard Sch\u00f6lkopf"], "venue": "MLCW", "citeRegEx": "Qui\u00f1oneroCandela et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Qui\u00f1oneroCandela et al\\.", "year": 2006}, {"title": "Gaussian processes for machine learning, volume 1", "author": ["Rasmussen", "Christopher K.I. Williams"], "venue": null, "citeRegEx": "Rasmussen et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Rasmussen et al\\.", "year": 2006}, {"title": "An Investigation on the Effectiveness of Features for Translation Quality Estimation", "author": ["Shah et al.2013] Kashif Shah", "Trevor Cohn", "Lucia Specia"], "venue": "In Proceedings of MT Summit XIV", "citeRegEx": "Shah et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Shah et al\\.", "year": 2013}, {"title": "A study of translation edit rate with targeted human annotation", "author": ["Bonnie Dorr", "Richard Schwartz", "Linnea Micciulla", "John Makhoul"], "venue": "Proceedings of AMTA", "citeRegEx": "Snover et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Estimating the sentence-level quality of machine translation systems", "author": ["Specia et al.2009] Lucia Specia", "Nicola Cancedda", "Marc Dymetman", "Marco Turchi", "Nello Cristianini"], "venue": "In Proceedings of EAMT,", "citeRegEx": "Specia et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2009}, {"title": "Multi-level Translation Quality Prediction with QUEST++", "author": ["Specia et al.2015] Lucia Specia", "Gustavo Henrique Paetzold", "Carolina Scarton"], "venue": "In Proceedings of ACL Demo Session,", "citeRegEx": "Specia et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Specia et al\\.", "year": 2015}, {"title": "Exploiting Objective Annotations for Measuring Translation Postediting Effort", "author": ["Lucia Specia"], "venue": "In Proceedings of EAMT,", "citeRegEx": "Specia.,? \\Q2011\\E", "shortCiteRegEx": "Specia.", "year": 2011}, {"title": "Learning to identify emotions in text", "author": ["Strapparava", "Mihalcea2008] Carlo Strapparava", "Rada Mihalcea"], "venue": "In Proceedings of the 2008 ACM Symposium on Applied Computing,", "citeRegEx": "Strapparava et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Strapparava et al\\.", "year": 2008}, {"title": "Bayesian Estimation and Prediction Using Asymmetric Loss Functions", "author": ["Arnold Zellner"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Zellner.,? \\Q1986\\E", "shortCiteRegEx": "Zellner.", "year": 1986}], "referenceMentions": [{"referenceID": 5, "context": "Quality Estimation (QE) (Blatz et al., 2004; Specia et al., 2009) models aim at predicting the quality of automatically translated text segments.", "startOffset": 24, "endOffset": 65}, {"referenceID": 25, "context": "Quality Estimation (QE) (Blatz et al., 2004; Specia et al., 2009) models aim at predicting the quality of automatically translated text segments.", "startOffset": 24, "endOffset": 65}, {"referenceID": 12, "context": "cesses (GPs) since they considered the state-ofthe-art for regression (Cohn and Specia, 2013; Hensman et al., 2013), especially in the low-data regimes typical for this task.", "startOffset": 70, "endOffset": 115}, {"referenceID": 12, "context": "cesses (GPs) since they considered the state-ofthe-art for regression (Cohn and Specia, 2013; Hensman et al., 2013), especially in the low-data regimes typical for this task. We focus our analysis in this paper on GPs since other common models used in QE can only provide point estimates as predictions. Another reason why we focus on probabilistic models is because this lets us employ the ideas proposed by Qui\u00f1onero-Candela et al. (2006), which defined new evaluation metrics that take into account probability distributions over predictions.", "startOffset": 94, "endOffset": 441}, {"referenceID": 23, "context": "Gaussian Processes (GPs) (Rasmussen and Williams, 2006) is an alternative kernel-based framework that gives competitive results for point estimates (Cohn and Specia, 2013; Shah et al., 2013; Beck et al., 2014b).", "startOffset": 148, "endOffset": 210}, {"referenceID": 11, "context": "We also report two point estimate metrics on test data: Mean Absolute Error (MAE), the most commonly used evaluation metric in QE, and Pearson\u2019s r, which has recently proposed by Graham (2015) as a more robust alternative.", "startOffset": 179, "endOffset": 193}, {"referenceID": 27, "context": "French-English (fr-en) Described in (Specia, 2011), this dataset contains 2, 525 sentences translated by one MT system and post-edited by a professional translator.", "startOffset": 36, "endOffset": 50}, {"referenceID": 24, "context": "Technically our approach could be used with any other numeric quality labels from the literature, including the commonly used Human Translation Error Rate (HTER) (Snover et al., 2006).", "startOffset": 162, "endOffset": 183}, {"referenceID": 16, "context": "Our decision to focus on post-editing time was based on the fact that time is a more complete measure of post-editing effort, capturing not only technical effort like HTER, but also cognitive effort (Koponen et al., 2012).", "startOffset": 199, "endOffset": 221}, {"referenceID": 26, "context": "For model building, we use a standard set of 17 features from the QuEst framework (Specia et al., 2015).", "startOffset": 82, "endOffset": 103}, {"referenceID": 29, "context": "Another asymmetric loss is the linear exponential or linex loss (Zellner, 1986):", "startOffset": 64, "endOffset": 79}, {"referenceID": 15, "context": "In the context of the AL loss this is called quantile regression (Koenker, 2005), since optimal estimators for this loss are posterior quantiles.", "startOffset": 65, "endOffset": 80}, {"referenceID": 14, "context": "views (Joshi et al., 2010; Bitvai and Cohn, 2015) and detection of emotion strength in news headlines (Strapparava and Mihalcea, 2008; Beck et al.", "startOffset": 6, "endOffset": 49}, {"referenceID": 17, "context": ", 2004), but also others like L\u00e1zaro-Gredilla (2012) and Chalupka et", "startOffset": 30, "endOffset": 53}, {"referenceID": 28, "context": "Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others.", "startOffset": 88, "endOffset": 103}, {"referenceID": 15, "context": "Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others.", "startOffset": 107, "endOffset": 122}, {"referenceID": 15, "context": "Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others. Besides the AL and the linex, another well studied loss is the asymmetric quadratic, which in turn relates to the concept of expectiles (Newey and Powell, 1987). This loss generalises the commonly used squared error loss. In terms of applications, Cain and Janssen (1995) gives an example in real estate assessment, where the consequences of under- and over-assessment are usually different depending on the specific scenario.", "startOffset": 107, "endOffset": 409}, {"referenceID": 15, "context": "Asymmetric loss functions are common in the econometrics literature and were studied by Zellner (1986) and Koenker (2005), among others. Besides the AL and the linex, another well studied loss is the asymmetric quadratic, which in turn relates to the concept of expectiles (Newey and Powell, 1987). This loss generalises the commonly used squared error loss. In terms of applications, Cain and Janssen (1995) gives an example in real estate assessment, where the consequences of under- and over-assessment are usually different depending on the specific scenario. An engineering example is given by Zellner (1986) in the context of dam construction, where an underestimate of peak water level is much more serious than an overestimate.", "startOffset": 107, "endOffset": 614}, {"referenceID": 1, "context": "Active Learning can benefit from variance information in their query methods and it has shown to be useful for QE (Beck et al., 2013).", "startOffset": 114, "endOffset": 133}], "year": 2016, "abstractText": "Machine Translation Quality Estimation is a notoriously difficult task, which lessens its usefulness in real-world translation environments. Such scenarios can be improved if quality predictions are accompanied by a measure of uncertainty. However, models in this task are traditionally evaluated only in terms of point estimate metrics, which do not take prediction uncertainty into account. We investigate probabilistic methods for Quality Estimation that can provide well-calibrated uncertainty estimates and evaluate them in terms of their full posterior predictive distributions. We also show how this posterior information can be useful in an asymmetric risk scenario, which aims to capture typical situations in translation workflows.", "creator": "LaTeX with hyperref package"}}}