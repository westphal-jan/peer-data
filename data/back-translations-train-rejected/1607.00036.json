{"id": "1607.00036", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Jun-2016", "title": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes", "abstract": "In this paper, we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both soft, differentiable and hard, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of the Facebook bAbI tasks and shown to outperform NTM and LSTM baselines.", "histories": [["v1", "Thu, 30 Jun 2016 20:45:12 GMT  (173kb,D)", "http://arxiv.org/abs/1607.00036v1", "13 pages, 2 figures"], ["v2", "Fri, 17 Mar 2017 05:56:48 GMT  (406kb,D)", "http://arxiv.org/abs/1607.00036v2", "13 pages, 3 figures"]], "COMMENTS": "13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["caglar gulcehre", "sarath chandar", "kyunghyun cho", "yoshua bengio"], "accepted": false, "id": "1607.00036"}, "pdf": {"name": "1607.00036.pdf", "metadata": {"source": "CRF", "title": "Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes", "authors": ["Caglar Gulcehre", "Sarath Chandar", "Kyunghyun Cho", "Yoshua Bengio"], "emails": ["name.lastname@umontreal.ca", "name.lastname@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2 Dynamic Neural Turing Machine", "text": "The proposed Turing Dynamic Neural Machine (D-NTM) extends the Turing Modular Neural Machine (NTM, [7]), which consists of two main modules, a controller and a memory. The controller, which is often implemented as a recursive neural network, issues a command to the memory to read, write and erase a subset of memory cells. Although the memory was originally designed as an integrated module, it is not necessary, and the memory can be an external black box [10]."}, {"heading": "2.1 Controller", "text": "For each step t, the controller (1) receives an input value xt, (2) addresses and reads mt from a portion of memory, (3) clears / writes a portion of memory, (4) updates its own hidden state ht, and (5) returns a value yt (if necessary). In this essay, we use both a gated recurrent unit (GRU, [11]) and a feedback controller to implement the controller such as thatht = GRU (xt, ht \u2212 1, mt) (1) or for a feedforward-controller ht = \u03c3 (xt, mt). (2)"}, {"heading": "2.2 Memory", "text": "In contrast to the original NTM, we divide each memory cell into two parts: M = [A; C].The first part A is a learnable address matrix and the second part C is a content matrix. In other words, each memory cell mi is nowmi = [ai; ci].The address part ai is a model parameter that is updated during the training. During the inference, the address part ci is not overwritten by the controller and remains constant. On the other hand, the content part ci is read and written by the controller both during the training and during the inference. At the beginning of each episode, the content part C is refreshed to an all-zero matrix. This introduction to the learnable address part for each memory cell allows the model to learn sophisticated location addressing strategies."}, {"heading": "2.3 Memory Addressing", "text": "The memory addressing in the D-NTM corresponds to the calculation of a N-dimensional address vector. The D-NTM calculates three such vectors for the respective read, delete et and write ut. Specifically for writing, the controller calculates a new content vector ct based on its current hidden state ht.Read With the read vector wt, the memory content is retrieved by mt = wtMt \u2212 1, (3), where wt is a line vector.Delete and Write In view of the delete, write and content vectors (etj, utj and ctj, respectively), the memory matrix is updated by bymtj = (1 \u2212 etjutj) mt \u2212 1j + u t jc t, (4) where the subscript j in mtj denotes the j element of the memory matrix M t and it is a vector."}, {"heading": "2.4 Learning", "text": "Once the proposed D-NTM is executed, it delivers the output distribution p (y | x1,..., xT). Consequently, we define a cost function as the negative log probability: C (\u03b8) = 1N N \u2211 n = 1 \u2212 log p (yn | xn1,..., xnT), (5) where the proposed D-NTM is a set of all parameters. Since the proposed D-NTM, just like the original NTM, is fully differentiable end-to-end, we can calculate the gradient of this cost function by backpropagation and learn the parameters of the model using a gradient-based optimization algorithm to train it end-to-end.No operation (NOP) As found in [12], an additional NOP action might be advantageous for the controller not to access the memory from time to time. We model this situation by ignoring a read from this cell or the NOP."}, {"heading": "3 Addressing Mechanism", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Address Vectors", "text": "First, the controller calculates a key vector: kt = W > k h t + btk, where Wk and bk are the parameters for this specific head (either read, written or deleted), and the sharpening factor \u03b2t is calculated: \u03b2t = soft plus (u > \u03b2 h t + b\u03b2).u\u03b2 and b\u03b2 are similar header parameters, the address vector is then defined by zti = \u03b2 tK (kt, mti) (6) wti = exp (zti) \u0445 j exp (z t j), (7) where the similarity function asK (x, y) = x \u00b7 y (| | | | | y | +) is defined."}, {"heading": "3.2 Multi-step Addressing", "text": "The original NTM addresses this by implementing multiple sets of read, erase, and write heads. In this paper, we explore a way to allow each head to work more than once per step, similar to the end-to-end storage network multi-hop mechanism [13]."}, {"heading": "3.3 Dynamic Least Recently Used Addressing", "text": "To learn LRU-based addressing, we first calculate the exponentially shiftable average values of the logits (zt) as vt, vt \u2190 0,1vt \u2212 1 + 0,9zt. We calculate the accumulated vt with \u03b3t so that the controller adjusts the influence on how much the previously written locations should affect the attention weights of a certain time step. Next, we subtract vt from zt to reduce the weights of the previously read or written locations. \u03b3t is a flat MLP with a scalar output and it depends on the hidden state of the controller. This behavior is parameterized with the parameters \u03b3 and b\u03b3, nitmoid = sigmoid (increased + bindexed), and it is conditioned by the hidden state of the controller."}, {"heading": "4 Regularizing Dynamic Neural Turing Machines", "text": "If the D-NTM controller is a powerful recursive neural network, it is important to regulate the formation of the D-NTM to avoid sub-optimal solutions where the D-NTM ignores memory and operates as a simple recursive neural network. One such sub-optimal solution, which we observed in our preliminary experiments with the proposed D-NTM, is that the D-NTM simply uses the address part A of the memory matrix as an additional weight matrix and not as a means of accessing the contents part C. We found that this pathological case can be effectively avoided by encouraging the read head to point to a memory cell that was also shown by the write head. This can be implemented as the following regulation term: Rrw (w, u) = \u03bb T-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-t-"}, {"heading": "5 Generating Discrete Address Vectors", "text": "FORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFORFOR"}, {"heading": "6 Related Work", "text": "A recursive neural network (RNN) used as controller in the proposed D-NTM has implicit memory in the form of recurring hidden states. Even with this implicit memory, it is known that there are difficulties in storing information over long periods of time [19]. Long-term memory (LSTM, [20]) and gated recurrent units (GRU, [11]) have been found to solve this problem. However, all of these models, which are based exclusively on RNNNs, have been found to be limited when used to solve, for example, algorithmic tasks and episodic questions. In addition to the finite access memory of the neural turing machine on which the D-NTM was designed, other data structures have been suggested as external storage for neural networks. [21, 22, 12] a continuous differentiable stack is proposed. [5, 10] Grid and tape storage are used."}, {"heading": "7 Episodic Question-Answering: bAbI Tasks", "text": "We evaluate the proposed D-NTM against the recently proposed episodic question and answer task called Facebook bAbI [27]. We use the data set of 10k training examples per subtask from Facebook.1For each episode, the D-NTM reads a sequence of fact sets, followed by a question, all of which are given as sentences in natural language. The D-NTM is expected to store and retrieve relevant information in memory to answer the question based on the facts presented. The computational complexity of the D-NTM in each episode is linear in terms of the number of facts, as the size of memory is constant, which differs from the storage network, which takes square time, as it scans all facts.1 https: / / research.facebook.com / 1543934539189348"}, {"heading": "7.1 Model and Training Details", "text": "We use the same hyperparameters for all tasks of a given model. Fact presentation We use a recursive neural network of GRU units to encode a variable-length fact into a fixed-size vector representation. This allows the D-NTM to use the word order in each fact, as opposed to facts encoded as word bag vectors.Controller We experiment with both a recursive and a feedback-forward neural network as the controller that generates the read and write weights. The controller has 180 units. We train our feed-forward controller using the noisy-tanh activation function [28], as we have had training difficulties with sigmoid and tanh activation functions. We use both single-stage and three-stage addressing with our GRU controller. The memory contains 120 memory cells. Each memory cell consists of a 16-dimensional address part and 28-dimensional content pieces Adam and 28-dimensional training examples. We use both single-level and three-level addressing with our GRU controller. Each memory cell contains 120 memory cells. Each memory cell consists of a 16-dimensional address part Adam and 28-dimensional content examples."}, {"heading": "7.2 Goals", "text": "The goal of this experiment is threefold: First, we present for the first time the performance of a memory-based network that can both read and write. To understand whether a model that needs to learn to write an incoming fact into memory, rather than store it as it is, can function well, we compare both the original NTM and the proposed D-NTM with an LSTM RNN. Second, we study the effects of writing. The fact that the NTM needs to learn to write is likely to have a detrimental effect on overall performance, compared, say, to end-to-end storage networks (MemN2N, [13]) and dynamic storage networks (DMN +, [23]), both of which simply store the incoming facts as they are. We quantify this effect in this experiment. Finally, we show the effect of the proposed learnable addressing scheme. We will further investigate the effect of using a feedback controller of the internal memory controller instead of the expandable GRU controller."}, {"heading": "7.3 Results and Analysis", "text": "In the table, we first find that the NTMs are actually able to solve these kinds of issues better than the LSTM-RNN. Although the availability of explicit memory is already evident in the LSTM-RNN, it is clear that the proposed new-fangled machines are able to trump the original NTs with the GRU controller (LSTM-RNN)."}, {"heading": "8 Conclusion and Future Work", "text": "In this paper, we expand the neural Turing machines (NTM) by introducing a learnable addressing scheme that enables the NTM to perform highly non-linear location-based addressing. This extension, which we refer to using dynamic NTM (D-NTM), is extensively tested with different configurations, including different addressing mechanisms (soft vs. discrete) and different number of addressing steps on the Facebook bAbI tasks. This is the first time that an NTM-like model has been tested on this task, and we observe that the NTM, in particular the proposed D-NTM, performs better than Vanilla LSTM-RNN. Furthermore, the experiments have shown that the discrete, hard addressing works better than the soft addressing with the GRU controller, and our analysis shows that this is the case when the task requires a precise recovery of memory contents, as our experiments have shown that the NTM are not based on the facts, but rather on the weakened NTM models that are manipulated in the NTM."}, {"heading": "Acknowledgements", "text": "The authors would like to thank the following research funding and computer support agencies: NSERC, Calcul Qu\u00e9bec, Compute Canada, Samsung, the Canada Research Chairs and CIFAR. We would also like to thank the developers of Theano 2 for developing such a powerful tool for scientific computing [32]. KC would like to thank Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)."}, {"heading": "9 Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Visualization of Discrete Attention", "text": "We visualize the attention of D-NTM with GRU controllers with hard attention in Figure 9.1. From this example we can see that D-NTM has learned to find the correct supporting fact in the visualization even without supervision of the respective story."}, {"heading": "9.2 Learning Curves for the Recurrent Controller", "text": "In Figure 9.2, we compare the learning curves of the soft and discrete attention model D-NTM with the recurring controller in task 1. Surprisingly, the discrete attention model D-NTM converges faster than the soft attention model. The main difficulty of learning soft attention is due to the fact that learning to write with soft attention can be challenging."}, {"heading": "9.3 Training with Soft-attention and Testing with Hard-attention", "text": "In Table 3, we provide results that examine the impact of using hard-care models at the test date for a model trained with a feed-forward controller and soft attention. Discrete \u0445 D-NTM model outlines discrete attention with soft attention, using the curriculum method we introduced in Section 5. Discrete \u2020 D-NTM model is the soft-care model that uses hard care at the test date. We observe that the discrete \u2020 D-NTM model trained with soft attention exceeds the discrete D-NTM model."}, {"heading": "9.4 D-NTM with BoW Fact Representation", "text": "In Table 4 we provide the results for D-NTM using BoW with Position Coding (PE) [13] as representation of the input facts. The factual representations are provided as input to the GRU controller. In accordance with our results with the GRU factual representation, we note improvements in BoW factual representation of multi-level addressing compared to single-level and discrete addressing compared to soft addressing."}], "references": [{"title": "Deep learning. Book in preparation for MIT Press, 2016", "author": ["Ian Goodfellow", "Yoshua Bengio", "Aaron Courville"], "venue": "URL http://www.deeplearningbook.org", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Teaching machines to read and comprehend", "author": ["Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "arXiv preprint arXiv:1506.03340,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations", "author": ["Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1511.02301,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning simple algorithms from examples", "author": ["Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": "arXiv preprint arXiv:1511.07275,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Describing videos by exploiting temporal structure", "author": ["Li Yao", "Atousa Torabi", "Kyunghyun Cho", "Nicolas Ballas", "Christopher Pal", "Hugo Larochelle", "Aaron Courville"], "venue": "In Computer Vision (ICCV),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Neural turing machines", "author": ["Alex Graves", "Greg Wayne", "Ivo Danihelka"], "venue": "arXiv preprint arXiv:1410.5401,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Large-scale simple question answering with memory networks", "author": ["Antoine Bordes", "Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1506.02075,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Evaluating prerequisite qualities for learning end-to-end dialog systems", "author": ["Jesse Dodge", "Andreea Gane", "Xiang Zhang", "Antoine Bordes", "Sumit Chopra", "Alexander Miller", "Arthur Szlam", "Jason Weston"], "venue": "CoRR, abs/1511.06931,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Reinforcement learning neural turing machines", "author": ["Wojciech Zaremba", "Ilya Sutskever"], "venue": "CoRR, abs/1505.00521,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "arXiv preprint arXiv:1503.08895,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "One-shot learning with memory-augmented neural networks", "author": ["Adam Santoro", "Sergey Bartunov", "Matthew Botvinick", "Daan Wierstra", "Timothy Lillicrap"], "venue": "arXiv preprint arXiv:1605.06065,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J. Williams"], "venue": "Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1992}, {"title": "Neural variational inference and learning in belief networks", "author": ["Andriy Mnih", "Karol Gregor"], "venue": "arXiv preprint arXiv:1402.0030,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Robust estimation of a location parameter", "author": ["Peter J. Huber"], "venue": "Ann. Math. Statist., 35(1):73\u2013101,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1964}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Aaron Courville", "Ruslan Salakhutdinov", "Richard Zemel", "Yoshua Bengio"], "venue": "In Proceedings Of The International Conference on Representation Learning (ICLR 2015),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1994}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1997}, {"title": "The neural network pushdown automaton: Architecture, dynamics and training", "author": ["Guo-Zheng Sun", "C. Lee Giles", "Hsing-Hen Chen"], "venue": "In Adaptive Processing of Sequences and Data Structures, International Summer School on Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings Of The International Conference on Representation Learning (ICLR 2015),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning"], "venue": "In Proceedings Of The Conference on Empirical Methods for Natural Language Processing", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Attention-based models for speech recognition", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1506.07503,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Towards ai-complete question answering: a set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Noisy activation functions", "author": ["Caglar Gulcehre", "Marcin Moczulski", "Misha Denil", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1603.00391,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M. Rush", "Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "VQA: visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In 2015 IEEE International Conference on Computer Vision,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": ", [1],) there are still a set of complex tasks that are not well addressed by conventional neural networks.", "startOffset": 2, "endOffset": 5}, {"referenceID": 1, "context": "They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].", "startOffset": 66, "endOffset": 75}, {"referenceID": 2, "context": "They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].", "startOffset": 66, "endOffset": 75}, {"referenceID": 3, "context": "They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 4, "context": "They include, but are not limited to, episodic question-answering [2, 3, 4], compact algorithms [5] and video caption generation [6].", "startOffset": 129, "endOffset": 132}, {"referenceID": 5, "context": "On the other hand, neural Turing machines (NTM, [7]) read each fact in an episode and decides whether to read, write the fact or do both to the external, differentiable memory.", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "In practice, this leads to easier learning in the memory network, which in turn resulted in it being used more in real tasks [8, 9].", "startOffset": 125, "endOffset": 131}, {"referenceID": 7, "context": "In practice, this leads to easier learning in the memory network, which in turn resulted in it being used more in real tasks [8, 9].", "startOffset": 125, "endOffset": 131}, {"referenceID": 8, "context": "We evaluate the proposed D-NTM on the full set of Facebook bAbI task [2] using either soft, differentiable attention or hard, non-differentiable attention [10] as an addressing strategy.", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "The proposed dynamic neural Turing machine (D-NTM) extends the neural Turing machine (NTM, [7]) which has a modular design.", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "Although the memory was originally envisioned as an integrated module, it is not necessary, and the memory may be an external, black box [10].", "startOffset": 137, "endOffset": 141}, {"referenceID": 9, "context": ") In this paper, we both use a gated recurrent unit (GRU, [11]) and a feedforward-controller to implement the controller such that", "startOffset": 58, "endOffset": 62}, {"referenceID": 10, "context": "No Operation (NOP) As found in [12], an additional NOP action might be beneficial for the controller not to access the memory once in a while.", "startOffset": 31, "endOffset": 35}, {"referenceID": 11, "context": "In this paper, we explore an option of allowing each head to operate more than once at each time step, similar to the multi-hop mechanism from the end-to-end memory network [13].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "As observed in [14], we find it more easier to learn pure content-based addressing by using a LRU addressing.", "startOffset": 15, "endOffset": 19}, {"referenceID": 13, "context": "Thus, we use REINFORCE [15] together with the three variance reduction techniques\u2013global baseline, input-dependent baseline and variance normalization\u2013 suggested in [16].", "startOffset": 23, "endOffset": 27}, {"referenceID": 14, "context": "Thus, we use REINFORCE [15] together with the three variance reduction techniques\u2013global baseline, input-dependent baseline and variance normalization\u2013 suggested in [16].", "startOffset": 165, "endOffset": 169}, {"referenceID": 15, "context": "The baseline network is trained to minimize the Huber loss [17] between the true reward R\u0303(x)\u2217 and the predicted reward b(x).", "startOffset": 59, "endOffset": 63}, {"referenceID": 16, "context": "As a further measure to reduce the variance, we regularize the negative entropy of all those category distributions to facilitate a better exploration during training [18].", "startOffset": 167, "endOffset": 171}, {"referenceID": 17, "context": "Even with this implicit memory, a vanilla RNN is however known to have difficulties in storing information for long time-spans [19].", "startOffset": 127, "endOffset": 131}, {"referenceID": 18, "context": "Long short-term memory (LSTM, [20]) and gated recurrent units (GRU, [11]) have been found to address this issue.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Long short-term memory (LSTM, [20]) and gated recurrent units (GRU, [11]) have been found to address this issue.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "In [21, 22, 12], a continuous, differentiable stack was proposed.", "startOffset": 3, "endOffset": 15}, {"referenceID": 20, "context": "In [21, 22, 12], a continuous, differentiable stack was proposed.", "startOffset": 3, "endOffset": 15}, {"referenceID": 10, "context": "In [21, 22, 12], a continuous, differentiable stack was proposed.", "startOffset": 3, "endOffset": 15}, {"referenceID": 3, "context": "In [5, 10], grid and tape storages are used.", "startOffset": 3, "endOffset": 10}, {"referenceID": 8, "context": "In [5, 10], grid and tape storages are used.", "startOffset": 3, "endOffset": 10}, {"referenceID": 11, "context": "Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].", "startOffset": 83, "endOffset": 97}, {"referenceID": 6, "context": "Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].", "startOffset": 83, "endOffset": 97}, {"referenceID": 7, "context": "Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].", "startOffset": 83, "endOffset": 97}, {"referenceID": 21, "context": "Memory networks and their variants have been applied to various tasks successfully [13, 8, 9, 23].", "startOffset": 83, "endOffset": 97}, {"referenceID": 22, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 152, "endOffset": 160}, {"referenceID": 23, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 152, "endOffset": 160}, {"referenceID": 24, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 181, "endOffset": 185}, {"referenceID": 1, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 217, "endOffset": 220}, {"referenceID": 16, "context": "Neural networks with soft or hard attention over an input have shown promising results on a variety of challenging tasks, including machine translation [24, 25], speech recognition [26], machine reading comprehension [3] and image caption generation [18].", "startOffset": 250, "endOffset": 254}, {"referenceID": 25, "context": "We evaluate the proposed D-NTM on the recently proposed episodic question-answering task called Facebook bAbI [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 26, "context": "We train our feed-forward controller using noisy-tanh activation function [28] since we were experiencing training difficulties with sigmoid and tanh activation functions.", "startOffset": 74, "endOffset": 78}, {"referenceID": 27, "context": "We train one D-NTM for each sub-task, using Adam [29] with its learning rate set to 0.", "startOffset": 49, "endOffset": 53}, {"referenceID": 11, "context": "The fact that the NTM needs to learn to write likely has adverse effect on the overall performance, when compared to, for instance, end-to-end memory networks (MemN2N, [13]) and dynamic memory network (DMN+, [23]) both of which simply store the incoming facts as they are.", "startOffset": 168, "endOffset": 172}, {"referenceID": 21, "context": "The fact that the NTM needs to learn to write likely has adverse effect on the overall performance, when compared to, for instance, end-to-end memory networks (MemN2N, [13]) and dynamic memory network (DMN+, [23]) both of which simply store the incoming facts as they are.", "startOffset": 208, "endOffset": 212}, {"referenceID": 16, "context": "Furthermore, this is in line with an earlier observation in [18], where discrete addressing was found to generalize better in the task of image caption generation.", "startOffset": 60, "endOffset": 64}, {"referenceID": 28, "context": "Second, the proposed D-NTM needs to be evaluated on a diverse set of applications, such as text summarization [30], visual question-answering [31] and machine translation, in order to make an even more concrete conclusion.", "startOffset": 110, "endOffset": 114}, {"referenceID": 29, "context": "Second, the proposed D-NTM needs to be evaluated on a diverse set of applications, such as text summarization [30], visual question-answering [31] and machine translation, in order to make an even more concrete conclusion.", "startOffset": 142, "endOffset": 146}], "year": 2016, "abstractText": "In this paper we extend neural Turing machine (NTM) into a dynamic neural Turing machine (D-NTM) by introducing a trainable memory addressing scheme. This scheme maintains for each memory cell two separate vectors, content and address vectors. This allows the D-NTM to learn a wide variety of location-based addressing strategies including both linear and nonlinear ones. We implement the D-NTM with both soft, differentiable and hard, non-differentiable read/write mechanisms. We investigate the mechanisms and effects for learning to read and write to a memory through experiments on Facebook bAbI tasks using both a feedforward and GRU-controller. The D-NTM is evaluated on a set of the Facebook bAbI tasks and shown to outperform NTM and LSTM baselines.", "creator": "LaTeX with hyperref package"}}}