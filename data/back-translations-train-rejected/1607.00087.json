{"id": "1607.00087", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Fractal Dimension Pattern Based Multiresolution Analysis for Rough Estimator of Person-Dependent Audio Emotion Recognition", "abstract": "As a general means of expression, audio analysis and recognition has attracted much attentions for its wide applications in real-life world. Audio emotion recognition (AER) attempts to understand emotional states of human with the given utterance signals, and has been studied abroad for its further development on friendly human-machine interfaces. Distinguish from other existing works, the person-dependent patterns of audio emotions are conducted, and fractal dimension features are calculated for acoustic feature extraction. Furthermore, it is able to efficiently learn intrinsic characteristics of auditory emotions, while the utterance features are learned from fractal dimensions of each sub-bands. Experimental results show the proposed method is able to provide comparative performance for audio emotion recognition.", "histories": [["v1", "Fri, 1 Jul 2016 00:54:10 GMT  (2253kb)", "https://arxiv.org/abs/1607.00087v1", null], ["v2", "Fri, 2 Dec 2016 13:12:35 GMT  (2251kb)", "http://arxiv.org/abs/1607.00087v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.SD", "authors": ["miao cheng", "ah chung tsoi"], "accepted": false, "id": "1607.00087"}, "pdf": {"name": "1607.00087.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["mew.cheng@gmail.com", "actsoi@must.edu.mo"], "sections": [{"heading": null, "text": "ar Xiv: 160 7,00 087v 2 [cs.A I] 2 Dec 2Index Terms - Audio emotion recognition (AER), multiresolution analysis, fractal dimension."}, {"heading": "1. INTRODUCTION", "text": "Audio-emotion analysis aims to understand the intrinsic explanation of people's emotional states. And as common sense, audio-emotion recognition (AER) identifies the emotional information from speech signals and learns audio features for classifying different emotions. In addition, audio analysis and recognition has become increasingly important and attractive with progress on broad applications of mobile devices, and brought much perspective to the applicable reality of life demand. And it has been found useful for many applications, such as human-machine conversation [1], emotion understanding [3], disease diagnosis [3], and so on. In the literature, AER has been studied under various aspects. One of these outstanding solutions is the use of differential features from audio data, and find the discriminatory information to identify different emotions. Without loss of generality, there were six emotions, such as anger, disgust, fear, happy, sad, and surprise, which are present in most emotion-recognition works."}, {"heading": "2. WAVELET TRANSFORM AND FRACTAL DIMENSION", "text": "-, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -, -., -, -, -, -, -, -, -,"}, {"heading": "3. FRACTAL DIMENSION BASED AUDIO EMOTION RECOGNITION", "text": "For an AER system, it is necessary to exploit the discrimination patterns in relation to selected audio features, and an acceptable result usually depends on suitable features that best represent the characteristics of certain data, which is common sense. In this work, the SAVEE dataset 1 is used for audio-emotion analysis, and the resulting audio features are described as obvious. It was considered difficult to exploit the audio patterns between audio data of different people."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "In this section, the experimental performance of the proposed method is evaluated on AER, and discriminatory performance is revealed in relation to different individuals. The Audio-Visual Expressed Emotion (SAVEE) dataset consists of recordings of 4 male actors in 7 different emotions, totaling 480 expressions in British English. And it is known for its difficulty accurately recognizing emotions in relation to each data modality. In our experiments, the audio data of six standard emotions is used while the neutral emotion is ignored, and then there are 90 expressions for each individual. To learn the reduced characteristics, the maximum margin criterion (MMC) [19] associated with the KNN classifier is applied to discriminatory classification. First, the audio data of an actor is used for learning, while the remaining expressions of other individuals are all used for testing AER."}, {"heading": "5. CONCLUSION", "text": "In this paper, AER is considered a multi-scale analysis problem, and FD characteristics are calculated to represent the best distinguishable patterns of different audio emotions. It is striking that both approximate and detail coefficients of wave substitution were used to perform the hard emotion data. Furthermore, no floating window is used at the stage of signal decomposition, while emotion proportions are also prescribed for further decreased learning. Experimental results show that the proposed method is able to provide a comparative performance for AER, even if the audio emotion differs significantly from other individuals for detection."}, {"heading": "6. REFERENCES", "text": "[1] T. Watanabe, \"The Adaptation of Machine Speed to Speed of Speed in Human Machine Communication 1085,\" IEEE Trans. Man Cybers., vol. 20, no. 2, pp. 502-507, 1990. [2] C. Maaoui, A. Pruski, and F. Abdat, \"Emotion recognition for human-machine communication,\" in Proc. Conf. Intell. Robots and Sys., 2008. [3] Syed, D. Curtis, J. Guttag, F. Nesta, and R. A. Levine, Audio-visual tools for computer-assisted diagnosis of cardiac disorders, in Proc. Int. Int. T. Syed, D. Curtis. 207-212. [4] S. Hoch, F. Althoff, G. McGlaun, and G. Rigoll, \"Bimodal fusion of emotinal data in an automotive environment.\""}], "references": [{"title": "The adaptation of machine conversational speed to speaker utterance speed in humanmachine communication", "author": ["T. Watanabe"], "venue": "IEEE Trans. Sys. Man Cybern., vol. 20, no. 2, pp. 502\u2013507, 1990.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1990}, {"title": "Emotion recognition for human-machine communication", "author": ["C. Maaoui", "A. Pruski", "F. Abdat"], "venue": "Proc. Int. Conf. Intell. Robots and Sys., 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Audio-visual tools for computer-assisted diagnosis of cardiac disorders", "author": ["Z. Syed", "D. Leeds", "D. Curtis", "J. Guttag", "F. Nesta", "R.A. Levine"], "venue": "Proc. Int. Symp. Comp. Med. Syst., 2006, pp. 207\u2013212.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Bimodal fusion of emotinal data in an automotive environment", "author": ["S. Hoch", "F. Althoff", "G. McGlaun", "G. Rigoll"], "venue": "Proc. IEEE Int. Conf. on Acoustic, Speech, and Signal Processing, 2005, pp. 1085\u20131088.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Classifying percussive sounds: a matter of zero-crossing rate", "author": ["F. Gouyon", "F. Pachet", "O. Delete"], "venue": "Proc. COST G-6 Conf. Digital Audio Effects, 2001, pp. 53\u201358.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Bimodal hci-related affect recognition", "author": ["Z. Zeng"], "venue": "Proc. Int. Conf. Multimodal Interfaces, 2004.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2004}, {"title": "Emotion classification of mandarin speech based on teo nonlinear features", "author": ["H. Gao", "S. Chen", "G. Su"], "venue": "ACIS Int. Conf. Soft. Eng., Artif. Intell. Networking, and Parallel/Distributed Computing, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Emotion recognition by speech signals", "author": ["O. Kwon", "K. Chan", "J. Hao", "T. Lee"], "venue": "Proc. Euro. Conf. on Speech Comm. and Tech., 2003.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2003}, {"title": "A First Course in Wavelets with Fourier Analysis, Wiley", "author": ["A. Boggess", "F.J. Narcowich"], "venue": "2nd edition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Time-frequency feature representation using multi-resolution texture analysis and acoustic activity detector for real-life speech emotion recognition", "author": ["K. Wang"], "venue": "Sensors, vol. 15, no. 1, pp. 1458\u20131478, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A fractal dimension and wavelet transform based method for protein sequence similarity analysis", "author": ["L. Yang", "Y.Y. Tang", "Y. Lu", "H. Luo"], "venue": "IEEE/ACM Trans. Compu. Bio. and Bioinfo., vol. 12, no. 2, pp. 348\u2013359, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Dimension und \u00e4u\u00dferes ma\u00df", "author": ["H. Felix"], "venue": "Mathematische Annalen, vol. 79, no. 1-2, pp. 157\u2013179, 1918.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1918}, {"title": "Fractal Geometry: Mathematical Foundations and Applications, Wiley, third edition", "author": ["Kenneth Falconer"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Fractals and the analysis of waveforms", "author": ["M.J. Katz"], "venue": "Computers in Biology and Medicine, vol. 18, no. 3, pp. 145\u2013156, 1988.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1988}, {"title": "Approach to an irregular time series on the basis of the fractal theory", "author": ["T. Higuchi"], "venue": "Physica D: Nonlinear Phenomena, vol. 31, no. 2, pp. 277\u2013283, 1988.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1988}, {"title": "A new approach of audio emotion recognition", "author": ["C.S. Ooi", "K.P. Seng", "L.M. Ang", "L.W. Chew"], "venue": "Expert Syst. Appl., vol. 41, pp. 5858\u20135869, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Dictionaries for sparse representation modeling", "author": ["R. Rubinstein", "A.M. Bruckstein", "M. Elad"], "venue": "Proc. IEEE, vol. 98, no. 6, pp. 1045\u20131057, 2010.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Nonparametric feature extraction via direct maximum margin alignment", "author": ["M. Cheng", "Y.Y. Tang", "C.M. Pun"], "venue": "Proceedings of Int. Conf. Mach. Learn. Appl., 2011.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "And it has been found useful for many applications, such as human-machine conversation [1], emotion understanding [2], sickness diagnosis [3], and so on.", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "And it has been found useful for many applications, such as human-machine conversation [1], emotion understanding [2], sickness diagnosis [3], and so on.", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "And it has been found useful for many applications, such as human-machine conversation [1], emotion understanding [2], sickness diagnosis [3], and so on.", "startOffset": 138, "endOffset": 141}, {"referenceID": 3, "context": "With the most significant characteristics of affect in speech, the pitch is usually estimated based on the Fourier analysis of the logarithmic amplitude spectrum of the signal [4], which is divided into a set of frames by windowing.", "startOffset": 176, "endOffset": 179}, {"referenceID": 4, "context": "The zero crossing rate [5] calculates average result of the number of times that the audio signal crosses zero within a particular time window, which is to determine the audio patterns of fevered and agitated emotions as a pattern feature.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "The log-energy features [6] are able to find the distinctive patterns of certain emotions among different audio signals, and decide the emotional states with the amplitude of a segment of speech.", "startOffset": 24, "endOffset": 27}, {"referenceID": 6, "context": "Distinguishingly, the teager energy operator (TEO) [7] uses the nonlinear operator to measure the changing energies of nonlinear emotional components.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "The well-known Mel-Frequency Cepstral Coefficients (MFCCs) [8] is based on the short-term power spectrum of speech signals, and a linear cosine transform of a log power spectrum on a nonlinear Mel scale of frequency.", "startOffset": 59, "endOffset": 62}, {"referenceID": 8, "context": "As a development of traditional Fourier transform, wavelet transform (WT) has been widely applied to signal processing and pattern analysis [9].", "startOffset": 140, "endOffset": 143}, {"referenceID": 9, "context": "In addition, wavelet packet based tree-structure [11] has also been widely applied to AER for its universally applicable property.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "On the other hand, Fractal dimension (FD) is a kind of nonlinear approximate methods of complicated measure, and has been widely applied to pattern recognition and bioinformatics [12].", "startOffset": 179, "endOffset": 183}, {"referenceID": 11, "context": "In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].", "startOffset": 128, "endOffset": 132}, {"referenceID": 12, "context": "In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].", "startOffset": 157, "endOffset": 161}, {"referenceID": 13, "context": "In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "In fact, several FD methods have been designed to conduct different kinds of calculational demands, such as Hausdorff dimension [13], box counting dimension [14], Katz\u2019s dimension [15], and Higuchi\u2019s dimension [16].", "startOffset": 210, "endOffset": 214}, {"referenceID": 11, "context": "The Hausdorff dimension [13] measures the local size of a space taking into account the distance between points, the metric.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "In fractal geometry, box-counting dimension [14] is a way of determining the fractal dimension of a set S in a Euclidean space Rn, which is also known as Minkowski dimension.", "startOffset": 44, "endOffset": 48}, {"referenceID": 13, "context": "More specifically, Katz\u2019s FD [15] is calculated as:", "startOffset": 29, "endOffset": 33}, {"referenceID": 14, "context": "Given an one-dimensional signal xi, (i = 1, \u00b7 \u00b7 \u00b7 ,N), Higuchi\u2019s FD [16] firstly calculates sub-sample sets from the signal data as", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "Different from some existing works that refer to a screening steps for AER only [17], it still fails to reach good results of complicated audio patterns, e.", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": ", dictionary learning [18], involved in discriminative matching, which has been a popular solution widely adopted in many related works.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "To learn the reduced features, the maximum margin criterion (MMC) [19] associated with KNN classifier is employed for discriminative classification.", "startOffset": 66, "endOffset": 70}], "year": 2016, "abstractText": "As a general means of expression, audio analysis and recognition has attracted much attentions for its wide applications in real-life world. Audio emotion recognition (AER) attempts to understand emotional states of human with the given utterance signals, and has been studied abroad for its further development on friendly human-machine interfaces. In this work, the discriminant patterns of audio emotions are conducted as multiresolution analysis of utterance signals, and fractal dimension features are calculated for acoustic feature extraction. Furthermore, it is able to efficiently learn intrinsic characteristics of auditory emotions, while the utterance features are learned from fractal dimensions of each sub-bands. Experimental results show the proposed method is able to provide comparative performance for audio emotion recognition.", "creator": "LaTeX with hyperref package"}}}