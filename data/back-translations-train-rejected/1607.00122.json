{"id": "1607.00122", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Less-forgetting Learning in Deep Neural Networks", "abstract": "A catastrophic forgetting problem makes deep neural networks forget the previously learned information, when learning data collected in new environments, such as by different sensors or in different light conditions. This paper presents a new method for alleviating the catastrophic forgetting problem. Unlike previous research, our method does not use any information from the source domain. Surprisingly, our method is very effective to forget less of the information in the source domain, and we show the effectiveness of our method using several experiments. Furthermore, we observed that the forgetting problem occurs between mini-batches when performing general training processes using stochastic gradient descent methods, and this problem is one of the factors that degrades generalization performance of the network. We also try to solve this problem using the proposed method. Finally, we show our less-forgetting learning method is also helpful to improve the performance of deep neural networks in terms of recognition rates.", "histories": [["v1", "Fri, 1 Jul 2016 06:50:17 GMT  (204kb,D)", "http://arxiv.org/abs/1607.00122v1", "5 pages, 4 figures"]], "COMMENTS": "5 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["heechul jung", "jeongwoo ju", "minju jung", "junmo kim"], "accepted": false, "id": "1607.00122"}, "pdf": {"name": "1607.00122.pdf", "metadata": {"source": "CRF", "title": "Less-forgetting Learning in Deep Neural Networks", "authors": ["Heechul Jung", "Minju Jung", "Junmo Kim"], "emails": ["junmo.kim}@kaist.ac.kr.", "veryju@kaist.ac.kr"], "sections": [{"heading": null, "text": "This year, it has come to the point where it is only a matter of time before a solution is found, in which a solution is found."}, {"heading": "A. Less-forgetting Problem", "text": "To theoretically show the less forgettable problem, we assume that we have the weight parameters of the pre-trained model for the source domain (v = D) and the training data for the target domain (target data) are given. Consequently, the data for the source domain (source data) is not accessible. To provide more clarity, we explain the problem using mathematical expressions as follows: D (t) = (t) i, y (t) i = 1, where ns and nt are the number of records for the source domain and target domain are i = 1, and the data set for the target domain is D (t) = (t) i = {t) i, y (t) i = 1, where ns and nt are the number of records for the source domain and target domain. Furthermore, x () i is the training data and y (\u00b7) i are the corresponding records. These two sets are mutually exclusive, and each record has both the - v as well as the training data (D = D)."}, {"heading": "II. LESS-FORGETTING LEARNING", "text": "In DNNs, the bottom layer is considered a trait extractor, and the top layer is considered a linear classifier, meaning that the weights of the Softmax function represent a decision boundary for the classification of the characteristics. Due to the linear classifier on the top layer, the properties extracted from the top hidden layer are normally linearly separable. With this knowledge, we propose a new learning scheme that fulfills the following two properties to reduce the problem of forgetting the information learned from the source domain: property 1. The decision boundaries should be immutable. property 2. The properties extracted from the source data through the target network should be in a position close to the properties extracted from the source network. We build a less forgotten learning algorithm based on two properties. The first property is easily implemented by setting the learning rates of the limit to zero, but the second property is not the satisfaction of the source data."}, {"heading": "III. LESS-FORGETTING FOR GENERAL LEARNING CASES", "text": "It is well known that the forgetfulness problem occurs when learning a new task [5]. In addition, we show that the forgetfulness problem also occurs when performing general learning process algorithms (1). (2). (2). (2). (2). (3).). (3).). (3). (3).). (3).). (3).). (3). (3).. (3). (3).. (3).. (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4). (4).). (4). (4). (4). (4).). (4). (4).). (4). (4).). (4). (4).). (4). (4).). (4).). (4).). (4).). (4). (4).). (4). (4).). (4). (4).). (4). (4). (4).). (5. (5. (5). (5. (5). (5.). (5.). (5. (5.). (5.). (5.). (5. (5.).). (5.). (5.). 5. (5.). (5.). 5. (5.). (5. 5.). (5. (5.).). (5.).). 5."}, {"heading": "IV. EXPERIMENTS", "text": "We establish two different detection experiments: unforging and generalization tests for algorithm 1 and 2, respectively. In the unforgeing experiment, we manually set up a source domain and a target domain using an object detection data set (CIFAR-10) and consider two different digit detection data sets (MNIST, SVHN) as source domain and target domain respectively. In the generalization experiment, we use CIFAR-10 dataset. In addition, we use a suspension method [10] for maxout and LWTA experiments because we want to increase their maximum performance."}, {"heading": "A. Unforgetting Test", "text": "To investigate how much less forgetting prevents a DNN from losing its source domain information while simultaneously working on the target domain, we manipulate CIFAR-10 while MNIST and SVHN remain intact. First, we split a set of CIFAR-10 images, Dt, into two sets so that they belong to the source and target domains, D (s) t, D (t) t; they contain 40,000 and 10,000 images, respectively. Second, we convert the pixel values of the images in D (t) t to new ones based on the R + G + B3 equation, where R, G, B are thepixel values in each channel, and the same conversion is performed on a test set where the converted set becomes D (t) v."}, {"heading": "B. Generalization Test", "text": "As explained in Section III, we apply algorithm 2 to object recognition and make the discovery that less forgetting leads to better performance, i.e. more generalization. Table II shows the accuracy of the original, batch normalization, batch normalization plus less forgetting and less forgetting of learning as the number of iterations increases. It is obvious that all cases with less forgetting have an improvement over those without forgetting."}, {"heading": "V. CONCLUSION", "text": "We proposed a less forgettable learning method to alleviate a catastrophic forgetfulness problem in DNNs. We also observed that the phenomenon of forgetfulness occurs when using a general learning method such as a stochastic gradient descend method. Our method is also effective in mitigating this problem. Finally, we demonstrated that our method is useful in improving the generalization capability of DNNs."}], "references": [{"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Y. Taigman", "M. Yang", "M. Ranzato", "L. Wolf"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on. IEEE, 2014, pp. 1701\u20131708.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "arXiv preprint arXiv:1409.4842, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "An empirical investigation of catastrophic forgeting in gradient-based neural networks", "author": ["I.J. Goodfellow", "M. Mirza", "D. Xiao", "A. Courville", "Y. Bengio"], "venue": "arXiv preprint arXiv:1312.6211, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Visual domain adaptation: A survey of recent advances", "author": ["V.M. Patel", "R. Gopalan", "R. Li", "R. Chellappa"], "venue": "Signal Processing Magazine, IEEE, vol. 32, no. 3, pp. 53\u201369, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, vol. 22, no. 10, pp. 1345\u2013 1359, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Compete to compute", "author": ["R.K. Srivastava", "J. Masci", "S. Kazerounian", "F. Gomez", "J. Schmidhuber"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 2310\u20132318.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["G.E. Hinton", "N. Srivastava", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "2012, technical Report arXiv:1207.0580.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1929}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A. Courville", "Y. Bengio"], "venue": "International Conference on Machine Learning (ICML), 2013.  JOURNAL OF  LATEX CLASS FILES, VOL. XX, NO. X, XXXX XXXX  5", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised neuron selection for mitigating catastrophic forgetting in neural networks", "author": ["B. Goodrich", "I. Arel"], "venue": "Circuits and Systems (MWSCAS), 2014 IEEE 57th International Midwest Symposium on. IEEE, 2014, pp. 997\u20131000.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Mitigating catastrophic forgetting in temporal difference learning with function approximation", "author": ["Goodrich", "I. Arel"], "venue": "2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Neuron clustering for mitigating catastrophic forgetting in feedforward neural networks", "author": ["B. Goodrich", "I. Arel"], "venue": "Computational Intelligence in Dynamic and Uncertain Environments (CIDUE), 2014 IEEE Symposium on. IEEE, 2014, pp. 62\u201368.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequential covariance-matrix estimation with application to mitigating catastrophic forgetting", "author": ["T. Lancewicki", "B. Goodrich", "I. Arel"], "venue": "Machine Learning and Applications and Workshops (ICMLA), 2015 14th International Conference on. IEEE, 2015, pp. 618\u2013629.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing data using t-sne", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. 2579-2605, p. 85, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "DEEP neural networks (DNNs) have grown to nearly human levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "DEEP neural networks (DNNs) have grown to nearly human levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4].", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "DEEP neural networks (DNNs) have grown to nearly human levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "DEEP neural networks (DNNs) have grown to nearly human levels of recognition in identifying objects, faces, and speeches [1], [2], [3], [4].", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "Despite this advancement of deep learning, remaining issues still exist; a catastrophic forgetting problem is the one of these remaining issues [5].", "startOffset": 144, "endOffset": 147}, {"referenceID": 5, "context": "Usually, the network pre-trained using the original data (source domain) is used as initial weights for adapting to the new data (target domain) [6], [7].", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "Usually, the network pre-trained using the original data (source domain) is used as initial weights for adapting to the new data (target domain) [6], [7].", "startOffset": 150, "endOffset": 153}, {"referenceID": 15, "context": "Visualization of the feature space for ten classes using t-SNE [16].", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "catastrophic forgetting [8].", "startOffset": 24, "endOffset": 27}, {"referenceID": 4, "context": "Recently, several experiments of a catastrophic forgetting problem in DNNs were empirically performed in [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "The paper shows a dropout method [9], [10] with a Maxout [11] activation function is helpful for forgetting less of the learned information.", "startOffset": 33, "endOffset": 36}, {"referenceID": 9, "context": "The paper shows a dropout method [9], [10] with a Maxout [11] activation function is helpful for forgetting less of the learned information.", "startOffset": 38, "endOffset": 42}, {"referenceID": 10, "context": "The paper shows a dropout method [9], [10] with a Maxout [11] activation function is helpful for forgetting less of the learned information.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "An unsupervised approach was proposed in [12].", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "al extended this method to a recurrent neural network [13].", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "[14] and [15] have similar issues mentioned above.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] and [15] have similar issues mentioned above.", "startOffset": 9, "endOffset": 13}, {"referenceID": 4, "context": "It is well known the forgetting problem occurs when learning a new task [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "In addition, we used a dropout method [10] for Maxout and LWTA experiments because we want to raise their maximum performance.", "startOffset": 38, "endOffset": 42}, {"referenceID": 4, "context": "Source network (Maxout) [5] 99.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Source network (LWTA) [8] 99.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "SVHN Transfer (Maxout) [5] 64.", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Transfer (LWTA) [8] 58.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "Source network (Maxout) [5] 78.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "Source network (LWTA) [8] 76.", "startOffset": 22, "endOffset": 25}, {"referenceID": 4, "context": "\u2193 Transfer (Maxout) [5] 71.", "startOffset": 20, "endOffset": 23}, {"referenceID": 7, "context": "CIFAR10 Transfer (LWTA) [8] 68.", "startOffset": 24, "endOffset": 27}], "year": 2016, "abstractText": "A catastrophic forgetting problem makes deep neural networks forget the previously learned information, when learning data collected in new environments, such as by different sensors or in different light conditions. This paper presents a new method for alleviating the catastrophic forgetting problem. Unlike previous research, our method does not use any information from the source domain. Surprisingly, our method is very effective to forget less of the information in the source domain, and we show the effectiveness of our method using several experiments. Furthermore, we observed that the forgetting problem occurs between mini-batches when performing general training processes using stochastic gradient descent methods, and this problem is one of the factors that degrades generalization performance of the network. We also try to solve this problem using the proposed method. Finally, we show our less-forgetting learning method is also helpful to improve the performance of deep neural networks in terms of recognition rates.", "creator": "LaTeX with hyperref package"}}}