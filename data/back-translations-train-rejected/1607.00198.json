{"id": "1607.00198", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Sharing Network Parameters for Crosslingual Named Entity Recognition", "abstract": "Most state of the art approaches for Named Entity Recognition rely on hand crafted features and annotated corpora. Recently Neural network based models have been proposed which do not require handcrafted features but still require annotated corpora. However, such annotated corpora may not be available for many languages. In this paper, we propose a neural network based model which allows sharing the decoder as well as word and character level parameters between two languages thereby allowing a resource fortunate language to aid a resource deprived language. Specifically, we focus on the case when limited annotated corpora is available in one language ($L_1$) and abundant annotated corpora is available in another language ($L_2$). Sharing the network architecture and parameters between $L_1$ and $L_2$ leads to improved performance in $L_1$. Further, our approach does not require any hand crafted features but instead directly learns meaningful feature representations from the training data itself. We experiment with 4 language pairs and show that indeed in a resource constrained setup (lesser annotated corpora), a model jointly trained with data from another language performs better than a model trained only on the limited corpora in one language.", "histories": [["v1", "Fri, 1 Jul 2016 10:35:59 GMT  (133kb,D)", "http://arxiv.org/abs/1607.00198v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["rudra murthy v", "mitesh khapra", "pushpak bhattacharyya"], "accepted": false, "id": "1607.00198"}, "pdf": {"name": "1607.00198.pdf", "metadata": {"source": "CRF", "title": "Sharing Network Parameters for Crosslingual Named Entity Recognition", "authors": ["Rudra Murthy", "Mitesh Khapra"], "emails": ["rudra@cse.iitb.ac.in", "mikhapra@in.ibm.com", "pb@cse.iitb.ac.in"], "sections": [{"heading": "1 Introduction", "text": "This demand for resources in the form of training data, gazetteers, tools, feature engineering, etc., makes it difficult to apply these approaches to resource-poor languages. Recently, several neural network-based approaches for new networks have been proposed (Collobert et al., 2011; Turian et al., 2010). Furthermore, they rely on language-specific handmade features (such as capitalizing the first character in English), some of which rely on knowledge resources in the form of gazetteers (Florian et al., 2003) and other NLP tools such as POS taggers, which in turn require their own training data. This resource requirement in the form of training data, gazetteers, tools, feature engineering, etc. makes it difficult to apply these approaches to resource-poor. Recently, several neural network-based approaches for nouresource have been proposed. Collobert et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al"}, {"heading": "2 Related Work", "text": "In this section, we present a quick overview of (i) neural network-based approaches for NERs that now report on the state of the art, results, and (ii) approaches tailored to multilingual NER.Neural networks were first investigated in the context of Hammerton's designated entity recognition (2003), but, Collobert et al. (2011) were the first to successfully use neural networks for multiple NLP tasks, including NLP tasks. Unlike existing monitored systems, they used minimal handcrafted functions and instead relied on automatic word representations from large, uncommented companies. The starting layer was a CRF layer that modeled on the entire sequence probability. They also used the idea of sharing network parameters across different tasks (but not between different languages), an idea developed by (Santos and Zadrozny, 2014; dos Santos et al., 2015) to include character-level information."}, {"heading": "3 Model", "text": "In this section, we describe our model, which encodes both character level and word level, for named entity recognition. As shown in Figure 3, our model consists of three components: (i) a revolutionary level for character level extraction, (ii) a bidirectional LSTM for encoding input word sequences, and (iii) an output level for predicting tags."}, {"heading": "3.1 Character level Convolutional Layer", "text": "The input to our model is a sequence of words X = {x1,.., xn}. We consider each word wi to be further composed of a sequence of characters, i.e. xi = {ci1, ci2,..., cik}, where k is the number of characters in the word. Each character ci1 is represented as a single hot vector, where | C | is the number of characters in the language. These one-hot representations of all characters in the word are stacked to form a matrix M-Rk \u00d7 | C |. We then apply several filters of a one-dimensional folding to this matrix. The width of these filters varies from 1 to n, i.e., these filters consider 1 to n-gram strings. The intuition is that a filter of length 1 could consider the filters of length 1 to plasma and hopefully learn to distinguish between upper and lower case letters. Similarly, a filter of length 4 could consider a sequence of characters."}, {"heading": "3.2 Bi-directional LSTM", "text": "Input into the bi-directional LSTM is a word sequence in which each word is represented by the following concatenated vector.h (xi) = [hemb (xi), hcnn (xi)] (1) hemb (xi).The second part, i.e. hcnn (xi) encodes information at the character level as described in the previous subsection (e.g. word2vec (Mikolov et al., 2013a; Mikolov et al., 2013b)) and then refines it during the training of our model. Forward-directed LSTM reads this word sequence from left to right, while backward-directed LSTM does the same from right to left. This results in a hidden representation for each word containing two pieces.gi = [fi (x1,.., xi), bi (xn,., xi)."}, {"heading": "3.3 Decoder", "text": "Given a training set D = (X, Y) where X = (x1,.., xn) is a sequence of words and Y = (y1,.., yn) is a corresponding sequence of entity tags, our goal is to maximize the log probability of the training data as in Equation 3.logP (X, Y) and D logP (Y,...., xn, yi \u2212 1) are the parameters of the network. (4) We model logP (yi | X) as in Equation 4, logP (Y | X) as in Equation 4, logP (Y | X) as in i = 1 logP (yi | x1,..., xn, yi \u2212 1) (4) We model logP (yi | x1,)."}, {"heading": "3.4 Sharing parameters across languages", "text": "As shown in Figure 3, our model includes the following parameters: (i) convolutional filters (ii) word embedding (iii) LSTM parameters and (iv) decoder parameters. The convolutional filters work on strings and can therefore be shared between languages that share a common character set. This is true for many European languages, and we are considering some of these languages for our experiments (English, Spanish, Dutch and German). Recently, there has been great interest in learning bilingual word representations together. The goal is to project words across languages into a common space, so that similar words across languages are very close together in that space. In this essay, we are experimenting with bilingual word embedding of Bilbowa, which allows us to divide the space of word embedding across languages. Similarly, we are also sharing the output layer across languages, as all languages have the same entity parameters."}, {"heading": "4 Experimental Setup", "text": "In this section we describe: (i) the data sets used for our experiments (ii) publicly available word embedding used for different languages and (iii) the hyperparameters used for all of our experiments."}, {"heading": "4.1 Dataset", "text": "For English, Spanish and Dutch, we use the data sets published in the context of CoNLL Shared Tasks on NER. For Spanish and Dutch, we use the data published in the context of the CoNLL 2002 Shared Task (Tjong Kim Sang and De Meulder, 2003). For Spanish and Dutch, we use the data published in the context of the CoNLL 2002 Shared Task (Tjong Kim Sang, 2002). Besides these three languages, we also evaluate our models in German. However, we did not have access to the German data of CoNLL (as it requires a special license). Instead, we used the publicly available German NER data as training, development and test files. In addition to these three languages, we also evaluate our models in German."}, {"heading": "4.2 Word Embeddings", "text": "We used pre-trained Spectral Word Embedings (Dhillon et al., 2015) for English, Spanish, German and Dutch. All Word Embedings have 200 dimensions. We update these pre-trained Word Embedings during the training. We convert all words into lowercase letters before we receive the corresponding word embedding. However, note that we receive the case information when we send the character sequence through the CNN layer (because the case information is important for the character filters) Word embedding for different languages is in different feature spaces (unless we use bilingual word embedding that is trained to be in the same feature space). These word embedding cannot be given directly as input into our model (since unrelated words from the 2 languages may have similar word embedding, i.e., similar features). We use a language-dependent linear level to assign the words from the Common 2 languages to the Common Task in a specific feature named it."}, {"heading": "4.3 Resource constrained setup", "text": "In all our resource-limited experiments, the LSTM parameters are always shared between the source and target languages. In addition, we share one or more of the following: (i) Convolutionary filters (ii) Space for word embedding and (iii) Decoder parameters. By sharing the space for word embedding, we mean that instead of individually trained monolingual Spectral embedding for the source and target languages, we use jointly trained word embedding that projects the words in a common space. We use the standard Bilbowa algorithm (Gouws et al., 2015) with default settings to train these bilingual word embedding for the source and target languages. Bilbowa takes both monolingual and bilingual corpora as input. For bilingual corpora, we receive the relevant source part of Euros embedding."}, {"heading": "4.4 Hyper-parameters", "text": "Our model includes the following hyperparameters: (i) LSTM size, (ii) maximum width of CNN filters, (iii) number of filters per width (i.e. number of filters for the same width n) and (iv) learning rate. All hyperparameters were matched by a grid search and error evaluation on the development set. For LSTM size, we looked at values from 100 to 300 in steps of 50, for the maximum width of CNN filters, we looked at values from k = 4 to 9 (i.e. we used all filters from width 1 to k). We varied the number of filters per width from 10 to 30 in steps of 5 and the learning rate from 0.05 to 0.50 in steps of 0.05."}, {"heading": "5 Results", "text": "In this section we report on our experimental results."}, {"heading": "5.1 Monolingual NER", "text": "The focus of this work is to find out whether a resource-limited language can benefit from a resource-rich language. However, before reporting on the results in this setup, we would like to check how well our model performs for monolingual NER (i.e. training and tests in the same language). Table 2 compares our results with some recently published state-of-the-art systems. We observe that our model provides current results for Dutch and English and comparable results in Spanish, showing that a fully neural network-based approach can also work on an equal footing with approaches that use a combination of neural networks and CRFs (Yang et al., 2016; Lample et al., 2016)."}, {"heading": "5.2 A naturally resource constrained scenario", "text": "In our primary experiments, we treat German as the target language and English, Spanish and Dutch as the source language. The reason for choosing German as the target language is that the data available for German is actually very small compared to the English, Spanish and Dutch datasets (which naturally creates a pair of resource-rich (English, Dutch, Spanish) and resource-poor (German) languages. We train our model together with the entire source (English or Dutch or Spanish) and target (German) data. We report separate results in case (i) the convolutionary filters are shared (ii) the decoder is shared and (iii) both become. We compare these results with the case when we train a model using the target language (German). The results are summarized in Table 3a: German, NL: Dutch, ES: Spanish."}, {"heading": "5.3 A simulated resource constrained scenario", "text": "To help us further analyze our model, we are conducting another experiment in which we use English as the source and Spanish as the target language. As sufficiently commented corpora are available in Spanish, we are simulating a resource-constrained structure by varying the amount of training data in Spanish from 10% to 90% in increments of 10%. These results are summarized in Figure 4a. We see an improvement from about 0.73% to 1.87% if the amount of Spanish data is between 30% and 80%. Naturally, the benefit of adding English data would decrease as more and more Spanish data become available. We hoped that the English data would be more useful if a smaller amount of Spanish data (< 30%) is available, but that is not the case. We believe that this is happening because with smaller Spanish data sizes, English data dominate the training process, possibly preventing the model from learning certain Spanish-specific characteristics. Figure 4b finally summarizes the overall results that we achieve by using a more common word (i.e., using more common words)."}, {"heading": "6 Analysis", "text": "We performed some error analyses to understand the effects of sharing different network parameters. Although our primary experiments were focused on English-German, Spanish-German, and Dutch-German, we limited our error analysis to English-Spanish because we could understand these two languages."}, {"heading": "6.1 Shared Decoder", "text": "Intuitively, sharing the decoder should allow one language to benefit from the tag sequence patterns learned in another language, which is not the case in the two languages with very different word sequences (e.g. English-Hindi). In fact, we observed that the Spanish model could benefit from certain tag sequences that were not often seen in the Spanish training data, but were seen in the English training data. For example, the tag sequence pattern (O w LOC is often confused and marked as (O w ORG) by the Spanish monolingual model. Here, the symbol \"(\" is marked as Other and w is a placeholder for a specific word. However, this tag pattern was often observed in the English training data. For example, such patterns were observed in English sports news: \"Ronaldo (O Brazil LOC) scored two goals in the game.\" The common model was able to benefit from this information, thus reducing some of the errors of the Spanish model."}, {"heading": "6.2 Shared Character Filters", "text": "We observed that sharing character filters also helps generalize by extracting entity characteristics named independently of language. For example, many place names start with a capital letter and end with the suffix ia, as in Australia, Austria, Colombia, India, Indonesia, Malaysia, etc. In the English corpus, there were many such entities with place names compared to the Spanish training corpus. We observed that Spanish benefited from this in the joint training setup and made fewer mistakes with such names (which were otherwise confused with the organization corpus in the monolingual environment)."}, {"heading": "7 Conclusion", "text": "In this paper, we focused on the problem of improving NERs in a resource-poor language by using additional commented corpora from another language. To this end, we proposed an architecture of neural networks that enables the sharing of various parameters between the two languages. Specifically, we share the decoder, the filters used to extract character-level characteristics, and a common space consisting of bilingual word embeddings. As the parameters are shared, the model can be trained jointly using commented corpora available in both languages. Our experiments with 4 language pairs suggest that such joint training actually improves performance in a resource-poor language. There are some interesting research directions that we would like to explore in the future. First, we observed that we achieve much greater gains when the space of word embeddings is shared. However, due to the poorer quality of bilingual embeddings, the overall results are no better than when compared to the case of using single-language embeddings."}], "references": [{"title": "A framework for learning predictive structures from multiple tasks and unlabeled data", "author": ["Ando", "Zhang2005] Rie Kubota Ando", "Tong Zhang"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Ando et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Ando et al\\.", "year": 2005}, {"title": "Improving machine translation quality with automatic named entity recognition", "author": ["Babych", "Hartley2003] Bogdan Babych", "Anthony Hartley"], "venue": "In Proceedings of the 7th International EAMT Workshop on MT and Other Language Technology", "citeRegEx": "Babych et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Babych et al\\.", "year": 2003}, {"title": "Named entity recognition with bilingual constraints", "author": ["Che et al.2013] Wanxiang Che", "Mengqiu Wang", "Christopher D. Manning", "Ting Liu"], "venue": null, "citeRegEx": "Che et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Che et al\\.", "year": 2013}, {"title": "On jointly recognizing and aligning bilingual named entities", "author": ["Chen et al.2010] Yufeng Chen", "Chengqing Zong", "Keh-Yih Su"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Chen et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2010}, {"title": "Named entity recognition with bidirectional lstm-cnns", "author": ["Chiu", "Nichols2015] Jason P.C. Chiu", "Eric Nichols"], "venue": null, "citeRegEx": "Chiu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chiu et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Eigenwords: Spectral word embeddings", "author": ["Dean P. Foster", "Lyle H. Ungar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Dhillon et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dhillon et al\\.", "year": 2015}, {"title": "Boosting named entity recognition with neural character embeddings", "author": ["Victor Guimaraes", "RJ Niter\u00f3i", "Rio de Janeiro"], "venue": "Proceedings of NEWS 2015 The Fifth Named Entities Workshop,", "citeRegEx": "Santos et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2015}, {"title": "Training and evaluating a german named entity recognizer with semantic generalization", "author": ["Faruqui", "Pad\u00f32010] Manaal Faruqui", "Sebastian Pad\u00f3"], "venue": "In Proceedings of KONVENS", "citeRegEx": "Faruqui et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2010}, {"title": "Named entity recognition through classifier combination", "author": ["Florian et al.2003] Radu Florian", "Abe Ittycheriah", "Hongyan Jing", "Tong Zhang"], "venue": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "citeRegEx": "Florian et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Florian et al\\.", "year": 2003}, {"title": "Multilingual language processing from bytes. CoRR, abs/1512.00103", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": "Proceedings of the 32nd International Conference on Machine Learn-", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Named entity recognition with long short-term memory", "author": ["James Hammerton"], "venue": "In Proceedings of the Seventh Conference on Natural Language Learning at HLT-NAACL", "citeRegEx": "Hammerton.,? \\Q2003\\E", "shortCiteRegEx": "Hammerton.", "year": 2003}, {"title": "Bidirectional LSTM-CRF models for sequence tagging. CoRR, abs/1508.01991", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": null, "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": "In Conference Proceedings: the tenth Machine Translation Summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Neural architectures for named entity recognition", "author": ["Miguel Ballesteros", "Kazuya Kawakami", "Sandeep Subramanian", "Chris Dyer"], "venue": "In In proceedings of NAACL-HLT (NAACL 2016).,", "citeRegEx": "Lample et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "DBpedia - a large-scale, multilingual knowledge base", "author": ["Lehmann et al.2014] Jens Lehmann", "Robert Isele", "Max Jakob", "Anja Jentzsch", "Dimitris Kontokostas", "Pablo Mendes", "Sebastian Hellmann", "Mohamed Morsey", "Patrick van Kleef", "S\u00f6ren Auer", "Chris Bizer"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 2014}, {"title": "Joint bilingual name tagging for parallel corpora", "author": ["Li et al.2012] Qi Li", "Haibo Li", "Heng Ji", "Wen Wang", "Jing Zheng", "Fei Huang"], "venue": "In Proceedings of the 21st ACM International Conference on Information and Knowledge Management,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Wen-tau Yih", "Geoffrey Zweig"], "venue": "In HLTNAACL,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Design challenges and misconceptions in named entity recognition", "author": ["Ratinov", "Roth2009] Lev Ratinov", "Dan Roth"], "venue": "In Proceedings of the Thirteenth Conference on Computational Natural Language Learning,", "citeRegEx": "Ratinov et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ratinov et al\\.", "year": 2009}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D. Santos", "Bianca Zadrozny"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14),", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] M. Schuster", "Kuldip K. Paliwal"], "venue": "Signal Processing, IEEE Transactions on,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Billions of parallel words for free: Building and using the eu bookshop corpus", "author": ["J\u00f6rg Tiedemann", "Roberts Rozis", "Daiga Deksne"], "venue": "In Proceedings of the 9th International Conference on Language Resources and Eval-", "citeRegEx": "Skadi\u0146\u0161 et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Skadi\u0146\u0161 et al\\.", "year": 2014}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computa-", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Introduction to the conll-2003 shared task: Language-independent named entity recognition", "author": ["Tjong Kim Sang", "Fien De Meulder"], "venue": "In Proceedings of the Seventh Conference on Natural Language Learning", "citeRegEx": "Sang et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2003}, {"title": "Introduction to the conll-2002 shared task: Languageindependent named entity recognition", "author": [], "venue": "In Proceedings of the 6th Conference on Natural Language Learning Volume 20,", "citeRegEx": "Sang.,? \\Q2002\\E", "shortCiteRegEx": "Sang.", "year": 2002}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Turian et al.2010] Joseph Turian", "Lev Ratinov", "Yoshua Bengio"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Cross-lingual projected expectation regularization for weakly supervised learning", "author": ["Wang", "Manning2014] Mengqiu Wang", "Christopher D. Manning"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Effective bilingual constraints for semi-supervised learning of named entity recognizers", "author": ["Wang et al.2013a] Mengqiu Wang", "Wanxiang Che", "Christopher D. Manning"], "venue": "Proceedings of the Twenty-Seventh", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Joint word alignment and bilingual named entity recognition using dual decomposition", "author": ["Wang et al.2013b] Mengqiu Wang", "Wanxiang Che", "Christopher D. Manning"], "venue": "In Proceedings of the 51st Annual Meeting of the Association", "citeRegEx": "Wang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2013}, {"title": "Multi-task cross-lingual sequence tagging from scratch", "author": ["Yang et al.2016] Zhilin Yang", "Ruslan Salakhutdinov", "William Cohen"], "venue": "arXiv preprint arXiv:1603.06270", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 9, "context": "Some of these features rely on knowledge resources in the form of gazetteers (Florian et al., 2003) and other", "startOffset": 77, "endOffset": 99}, {"referenceID": 5, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 13, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 15, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 31, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 10, "context": "proaches for NER have been proposed (Collobert et al., 2011; Huang et al., 2015; Lample et al., 2016; Yang et al., 2016; Gillick et al., 2015) which circumvent the need for hand-crafted features and thereby the need for gazetteers, part-of-speech taggers, etc.", "startOffset": 36, "endOffset": 142}, {"referenceID": 10, "context": "Very recently Gillick et al. (2015) proposed an encoder decoder based model for sequence labeling which takes a sequence of bytes (characters) as input instead of words and outputs spans as well as labels for these spans.", "startOffset": 14, "endOffset": 36}, {"referenceID": 10, "context": "Similar to Gillick et al. (2015), our character dependent parameters are shared across languages (which use the same character set).", "startOffset": 11, "endOffset": 33}, {"referenceID": 10, "context": "Similar to Gillick et al. (2015), our character dependent parameters are shared across languages (which use the same character set). However, unlike Gillick et al. (2015) we do not use an encoder decoder ar-", "startOffset": 11, "endOffset": 171}, {"referenceID": 11, "context": "Further, our model also employs word level features which can be shared across languages by using jointly learned bilingual word embeddings from parallel corpora (Gouws et al., 2015).", "startOffset": 162, "endOffset": 182}, {"referenceID": 26, "context": ", English-Spanish, English-German, SpanishGerman and Dutch-German using standard NER datasets released as part of the CoNLL shared task (Tjong Kim Sang and De Meulder, 2003; Tjong Kim Sang, 2002) and German NER data by Faruqui and Pad\u00f3 (2010). We artificially constrain the amount of training data available in one language and show that the network can still benefit from abundant annotated corpora in another language by jointly learning the shared parameters.", "startOffset": 147, "endOffset": 243}, {"referenceID": 11, "context": "Neural networks were first explored in the context of named entity recognition by Hammerton (2003) but, Collobert et al.", "startOffset": 82, "endOffset": 99}, {"referenceID": 5, "context": "Neural networks were first explored in the context of named entity recognition by Hammerton (2003) but, Collobert et al. (2011) were the first to successfully use neural networks for several NLP tasks including NER.", "startOffset": 104, "endOffset": 128}, {"referenceID": 5, "context": "The combined character features and word embeddings were fed to a time delay neural network as in Collobert et al. (2011) and used for Spanish and Portuguese NER.", "startOffset": 98, "endOffset": 122}, {"referenceID": 13, "context": "For examples Huang et al. (2015) use LSTMs for encoding word", "startOffset": 13, "endOffset": 33}, {"referenceID": 15, "context": "Very recently Lample et al. (2016) proposed Hierarchical Bi-LSTMs as an alternative to CNN-Bi-LSTMs wherein they first use a character level Bi-LSTMs followed by a word level Bi-LSTMs, thus forming a hierarchy of LSTMs.", "startOffset": 14, "endOffset": 35}, {"referenceID": 10, "context": "Very recently Gillick et al. (2015) proposed a", "startOffset": 14, "endOffset": 36}, {"referenceID": 30, "context": "Even more recently, Yang et al. (2016) extended Lample et al.", "startOffset": 20, "endOffset": 39}, {"referenceID": 15, "context": "(2016) extended Lample et al. (2016) and focused on both multi-task and multilingual setting.", "startOffset": 16, "endOffset": 37}, {"referenceID": 3, "context": "the aligned words in parallel tagged corpora (Chen et al., 2010; Li et al., 2012) or untagged parallel corpus (Wang et al.", "startOffset": 45, "endOffset": 81}, {"referenceID": 17, "context": "the aligned words in parallel tagged corpora (Chen et al., 2010; Li et al., 2012) or untagged parallel corpus (Wang et al.", "startOffset": 45, "endOffset": 81}, {"referenceID": 10, "context": "We use the standard definitions of the LSTM functions fi and bi as described in Gillick et al. (2015).", "startOffset": 80, "endOffset": 102}, {"referenceID": 6, "context": "We used pre-trained Spectral word embeddings (Dhillon et al., 2015) for English, Spanish, German", "startOffset": 45, "endOffset": 67}, {"referenceID": 11, "context": "We use off-the-shelf Bilbowa algorithm (Gouws et al., 2015) with default settings to train these bilingual word embeddings.", "startOffset": 39, "endOffset": 59}, {"referenceID": 14, "context": "For bilingual corpora, we use the relevant sourcetarget portion of Europarl corpus (Koehn, 2005) and Opus (Skadi\u0146\u0161 et al.", "startOffset": 83, "endOffset": 96}, {"referenceID": 23, "context": "For bilingual corpora, we use the relevant sourcetarget portion of Europarl corpus (Koehn, 2005) and Opus (Skadi\u0146\u0161 et al., 2014).", "startOffset": 106, "endOffset": 128}, {"referenceID": 16, "context": "For monolingiual corpora, we obtain short abstracts for each of the 4 languages from Dbpedia (Lehmann et al., 2014).", "startOffset": 93, "endOffset": 115}, {"referenceID": 31, "context": "This shows that a completely neural network based approach can also perform at par with approaches which use a combination of Neural Networks and CRFs (Yang et al., 2016; Lample et al., 2016).", "startOffset": 151, "endOffset": 191}, {"referenceID": 15, "context": "This shows that a completely neural network based approach can also perform at par with approaches which use a combination of Neural Networks and CRFs (Yang et al., 2016; Lample et al., 2016).", "startOffset": 151, "endOffset": 191}, {"referenceID": 10, "context": "English Gillick et al. (2015) 86.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "English Gillick et al. (2015) 86.50 Yang et al. (2016) 90.", "startOffset": 8, "endOffset": 55}, {"referenceID": 10, "context": "English Gillick et al. (2015) 86.50 Yang et al. (2016) 90.94 Lample et al. (2016) 90.", "startOffset": 8, "endOffset": 82}, {"referenceID": 10, "context": "Spanish Gillick et al. (2015) 82.", "startOffset": 8, "endOffset": 30}, {"referenceID": 10, "context": "Spanish Gillick et al. (2015) 82.95 Yang et al. (2016) 84.", "startOffset": 8, "endOffset": 55}, {"referenceID": 10, "context": "Spanish Gillick et al. (2015) 82.95 Yang et al. (2016) 84.69 Lample et al. (2016) 85.", "startOffset": 8, "endOffset": 82}, {"referenceID": 10, "context": "Dutch Gillick et al. (2015) 82.", "startOffset": 6, "endOffset": 28}, {"referenceID": 10, "context": "Dutch Gillick et al. (2015) 82.84 Yang et al. (2016) 85.", "startOffset": 6, "endOffset": 53}, {"referenceID": 10, "context": "Dutch Gillick et al. (2015) 82.84 Yang et al. (2016) 85.00 Lample et al. (2016) 81.", "startOffset": 6, "endOffset": 80}], "year": 2016, "abstractText": "Most state of the art approaches for Named Entity Recognition rely on hand crafted features and annotated corpora. Recently Neural network based models have been proposed which do not require handcrafted features but still require annotated corpora. However, such annotated corpora may not be available for many languages. In this paper, we propose a neural network based model which allows sharing the decoder as well as word and character level parameters between two languages thereby allowing a resource fortunate language to aid a resource deprived language. Specifically, we focus on the case when limited annotated corpora is available in one language (L1) and abundant annotated corpora is available in another language (L2). Sharing the network architecture and parameters between L1 and L2 leads to improved performance in L1. Further, our approach does not require any hand crafted features but instead directly learns meaningful feature representations from the training data itself. We experiment with 4 language pairs and show that indeed in a resource constrained setup (lesser annotated corpora), a model jointly trained with data from another language performs better than a model trained only on the limited corpora in one language.", "creator": "LaTeX with hyperref package"}}}