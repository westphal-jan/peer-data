{"id": "1607.00225", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource", "abstract": "Word embeddings have recently seen a strong increase in interest as a result of strong performance gains on a variety of tasks. However, most of this research also underlined the importance of benchmark datasets, and the difficulty of constructing these for a variety of language-specific tasks. Still, many of the datasets used in these tasks could prove to be fruitful linguistic resources, allowing for unique observations into language use and variability. In this paper we demonstrate the performance of multiple types of embeddings, created with both count and prediction-based architectures on a variety of corpora, in two language-specific tasks: relation evaluation, and dialect identification. For the latter, we compare unsupervised methods with a traditional, hand-crafted dictionary. With this research, we provide the embeddings themselves, the relation evaluation task benchmark for use in further research, and demonstrate how the benchmarked embeddings prove a useful unsupervised linguistic resource, effectively used in a downstream task.", "histories": [["v1", "Fri, 1 Jul 2016 12:48:35 GMT  (167kb,D)", "http://arxiv.org/abs/1607.00225v1", "in LREC 2016"]], "COMMENTS": "in LREC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["st\\'ephan tulkens", "chris emmery", "walter daelemans"], "accepted": false, "id": "1607.00225"}, "pdf": {"name": "1607.00225.pdf", "metadata": {"source": "CRF", "title": "Evaluating Unsupervised Dutch Word Embeddings as a Linguistic Resource", "authors": ["St\u00e9phan Tulkens"], "emails": ["first.lastname@uantwerpen.be"], "sections": [{"heading": "1 Introduction", "text": "The strong variability of language use within and via textual media (Collins et al., 1977; Linell et al., 1982) has been identified on many occasions as a major challenge for research in the field of computational linguistics (Resnik, 1999; Rosenfeld, 2000), particularly in social media applications (Gouws et al., 2011). Formal and informal varieties, as well as a plethora of deviations from grammar and spelling conventions in the latter, make the computational interpretation of meaning and relationships between words drastically more difficult. This task of understanding lies at the heart of natural language processing (NLP). Neural net-based language models such as the models in word2vec have recently gained a strong interest in NLP because they have improved the performance of words in relation to a variety of tasks in the field."}, {"heading": "2 Related Work", "text": "In fact, the majority of them are in a position to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3 Data", "text": "In our research, we used four large corporas and a combination of three of these corporas to train both SPPMI and word2vec. In addition, we found a dataset of regionally tagged Dutch social media posts as well as handmade dialect identification dictionaries (see 4.3)."}, {"heading": "3.1 Corpora", "text": "This year, the number of women in employment in Germany has tripled, so that the number of women in employment has risen by only 0.2 percent this year, while the number of men in employment has increased by 0.2 percent."}, {"heading": "3.2 Preprocessing", "text": "In addition, sentences shorter than five characters have been removed because they do not contain enough contextual words to yield meaningful results. An additional pre-processing has been carried out on the COW corpus: as a side effect of adapting the already tokenised version of the corpus, the Dutch section contains some mistokenised plurals, such as regio's, which are tokenised as regi + o + '+ s. In view of this, we decided to remove all tokens that consisted of only one character, except the token u, a Dutch pronoun indicating courtesy."}, {"heading": "3.3 Dictionaries", "text": "To compare our embedding with a handmade linguistic resource, we collected a dictionary of dialect words and sentences, as well as one for standard Dutch. The dialect dictionary was retrieved from MWB (Mijn WoordenBoek) 5, which offers custom dialect words, sentences and 5www.mijnwoordenboek.nl / dialecten, retrieved on 05 / 10 / 2015. Proverbs and their translations. Only the dialect section was retained and divided into individual words, which were then stored according to the region to which it was assigned by MWB and the province to which that region belongs. Intersecting words across dialects were removed. As a reference dialect for standard Dutch, the OpenTaal Word Index 6 was used additionally to remove universally valid words from dialect books, i.e. if this word could not be used in the reference dialect, both in Dutch and frequently in the reference dialect."}, {"heading": "4 Experiments", "text": "For the evaluation of our Dutch word embeddings, we have developed both a new benchmark task and a downstream task with which we can evaluate the performance of new embeddings in Dutch."}, {"heading": "4.1 Parameter Estimation", "text": "In order to determine optimal settings for the hyperparameters, several models were trained in parallel with different parameter values and evaluated in the relation evaluation task (see below). For word2vec, we created embeddings for the 50,000 most common words, experimented with window sizes of 5 and 10 and pushed constants of 1, 5 and 10. For all models, a shift constant of 1 and a window size of 11 led to the best results in all companies. For the SPPMI models, we created embeddings for the 50,000 most common words, experimented with window sizes of 5 and 10 and pushed constants of 1 and a window size of 5, with the exception being the model based on the Roularta corpus, which was best truncated with a shift constant of 5 and a window size of 5."}, {"heading": "4.2 Relation Identification", "text": "This task is based on the known relation identification data set included with the original word2vec toolkit8, which includes approximately 20,000 relation identification questions, each form: \"If A has a relation to B, which word has the same relation to D?\" As such, it uses the fact that vectors are compositional. For example, there is a man, a woman and a king, the answer to the question should be Queen, with the relationship here being \"gender.\" In the original sentence, these questions were divided into several categories, some based on semantic relationships, e.g. \"opposites\" or \"country-capitals,\" and some based on syntactic relationships, e.g. \"past-tense.\" We created a similar rating set for Dutch. In view of the categories used, we aimed to replicate the original rating as narrowly as possible."}, {"heading": "4.3 Dialect Identification", "text": "To counteract this, we have created a task in which we attempt to identify dialectical variations in social media posts, with the goal of measuring whether a resource that corresponds to a handmade resource can be created without any supervision. This identification of text that contains dialect was of interest to researchers in various languages such as Spanish (Gonc, Alves and Sa, 2014), German (Scheffler et al., 2014) and Arabic (Lin et al., 2014). The task is then to correctly assign text with dialect-specific language to the region of origin. To test whether embedding provides richer dialect information than handmade dictionaries, the performance of both approaches needs to be compared."}, {"heading": "5 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Relation Identification", "text": "The results of the experiment to determine the relationship are presented in Table 3, which shows that all models perform better in the syntactic categories when compared with the semantic categories, with the exception of the \"gender\" category, where all models performed comparatively well. In addition, the performance in terms of \"currency\" and \"opposites\" was consistently low, the former being explained by the low presence of currencies in our data. All models exceed the base embeddings, which is all the more problematic because the vocabulary of the baseline model was relatively small; only 6000 of the 10,000 predicates were in the vocabulary of the model. While it is not possible to estimate how the model would have performed on OOV (Out of Vocabulary) words, this shows that it works well even in the face of a wide variety of words."}, {"heading": "5.2 Dialect Identification", "text": "Since the models based on the COW corpus achieved the best results in relation to the previous task, we used them in the dialect identification task. To determine the validity of the use of these models on our test data, we report coverage with a large portion of the dialect words as expected. The dialect part of our hand-picked dialect words had a coverage of 68.3%, while the SPPMI model has a coverage of 24.4% compared to the dialect words."}, {"heading": "6 Conclusion", "text": "In this paper, we provided state-of-the-art word embeddings for Dutch, derived from four corpora, and compared two different algorithms. Since we had a high dimensionality and were derived from large corpora, we suspected that they could serve as a helpful resource for downstream tasks. To compare the efficiency of the embeddings and the algorithms used to derive them, we performed two-pronged tasks: first, a task to identify relationships, which is very similar to the task to the task to identify relationships presented with the original word2vec toolkit, but adapted to specific phenomena in the Dutch language. Here, we showed that we perform better than the basic model, comparable to the task of the English word2vec. Second, a downstream task to identify dialects, in which we showed that both methods we use to derive word embeddings meet simple, handheld dialects."}, {"heading": "7 Acknowledgements", "text": "We would like to thank TextGain for providing us with the corpora of social media posts, and A'kos Ka'da'r for helpful comments on our work. Part of this research was carried out as part of the Accumulate IWT SBO project, which is funded by the State Agency for Innovation by Science and Technology (IWT)."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["R. Al-Rfou", "B. Perozzi", "S. Skiena"], "venue": "Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 183\u2013192, Sofia, Bul-", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Don\u2019t count, predict! a systematic comparison of context-counting vs", "author": ["M. Baroni", "G. Dinu", "G. Kruszewski"], "venue": "context-predicting semantic vectors. In ACL (1), pages 238\u2013247.", "citeRegEx": "Baroni et al\\.,? 2014", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Neural probabilistic language models", "author": ["Y. Bengio", "H. Schwenk", "Sen\u00e9cal", "J.-S.", "F. Morin", "Gauvain", "J.-L."], "venue": "Innovations in Machine Learning, pages 137\u2013186. Springer.", "citeRegEx": "Bengio et al\\.,? 2006", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "An efficient memorybased morphosyntactic tagger and parser for dutch", "author": ["Bosch", "A. v. d.", "B. Busser", "S. Canisius", "W. Daelemans"], "venue": "LOT Occasional Series, 7:191\u2013206.", "citeRegEx": "Bosch et al\\.,? 2007", "shortCiteRegEx": "Bosch et al\\.", "year": 2007}, {"title": "Extracting semantic representations from word cooccurrence statistics: A computational study", "author": ["J.A. Bullinaria", "J.P. Levy"], "venue": "Behavior research methods, 39(3):510\u2013526.", "citeRegEx": "Bullinaria and Levy,? 2007", "shortCiteRegEx": "Bullinaria and Levy", "year": 2007}, {"title": "Inference in text understanding", "author": ["A. Collins", "J.S. Brown", "K.M. Larkin"], "venue": "BBN report; no. 3684.", "citeRegEx": "Collins et al\\.,? 1977", "shortCiteRegEx": "Collins et al\\.", "year": 1977}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["R. Collobert", "J. Weston"], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013", "citeRegEx": "Collobert and Weston,? 2008", "shortCiteRegEx": "Collobert and Weston", "year": 2008}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Pattern for python", "author": ["T. De Smedt", "W. Daelemans"], "venue": "The Journal of Machine Learning Research, 13(1):2063\u20132067.", "citeRegEx": "Smedt and Daelemans,? 2012", "shortCiteRegEx": "Smedt and Daelemans", "year": 2012}, {"title": "Computer information retrieval using latent semantic structure", "author": ["S.C. Deerwester", "S.T. Dumais", "G.W. Furnas", "R.A. Harshman", "T.K. Landauer", "K.E. Lochbaum", "L.A. Streeter"], "venue": "US Patent 4,839,853.", "citeRegEx": "Deerwester et al\\.,? 1989", "shortCiteRegEx": "Deerwester et al\\.", "year": 1989}, {"title": "Indexing by latent semantic analysis", "author": ["S.C. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R.A. Harshman"], "venue": "Journal of the American society for information science, 41(6):391.", "citeRegEx": "Deerwester et al\\.,? 1990", "shortCiteRegEx": "Deerwester et al\\.", "year": 1990}, {"title": "Crowdsourcing dialect characterization through twitter", "author": ["B. Gon\u00e7alves", "D. S\u00e1nchez"], "venue": "PloS one, 9(11):e112074.", "citeRegEx": "Gon\u00e7alves and S\u00e1nchez,? 2014", "shortCiteRegEx": "Gon\u00e7alves and S\u00e1nchez", "year": 2014}, {"title": "Contextual bearing on linguistic variation in social media", "author": ["S. Gouws", "D. Metzler", "C. Cai", "E. Hovy"], "venue": "Proceedings of the Workshop on Languages in Social Media, pages 20\u201329. Association for Computational Linguis-", "citeRegEx": "Gouws et al\\.,? 2011", "shortCiteRegEx": "Gouws et al\\.", "year": 2011}, {"title": "Distributional structure", "author": ["Z.S. Harris"], "venue": "Word, 10(2-3):146\u2013162.", "citeRegEx": "Harris,? 1954", "shortCiteRegEx": "Harris", "year": 1954}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval, pages 50\u201357. ACM.", "citeRegEx": "Hofmann,? 1999", "shortCiteRegEx": "Hofmann", "year": 1999}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems, pages 2177\u20132185.", "citeRegEx": "Levy and Goldberg,? 2014", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["O. Levy", "Y. Goldberg", "I. Dagan"], "venue": "Transactions of the Association for Computational Linguistics, 3:211\u2013225.", "citeRegEx": "Levy et al\\.,? 2015", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["O. Levy", "Y. Goldberg", "I. Ramat-Gan"], "venue": "CoNLL-2014, page 171.", "citeRegEx": "Levy et al\\.,? 2014", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "The cmu submission for the shared task on language identification in code-switched data", "author": ["Lin", "C.-C.", "W. Ammar", "L. Levin", "C. Dyer"], "venue": "EMNLP 2014, page 80.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "The Written Language Bias in Linguistics", "author": ["P. Linell"], "venue": "Link\u00f6ping University Electronic Press.", "citeRegEx": "Linell,? 1982", "shortCiteRegEx": "Linell", "year": 1982}, {"title": "Producing highdimensional semantic spaces from lexical cooccurrence", "author": ["K. Lund", "C. Burgess"], "venue": "Behavior Research Methods, Instruments, & Computers, 28(2):203\u2013208.", "citeRegEx": "Lund and Burgess,? 1996", "shortCiteRegEx": "Lund and Burgess", "year": 1996}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Mnih", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1081\u20131088.", "citeRegEx": "Mnih and Hinton,? 2009", "shortCiteRegEx": "Mnih and Hinton", "year": 2009}, {"title": "The construction of a 500million-word reference corpus of contemporary written dutch", "author": ["N. Oostdijk", "M. Reynaert", "V. Hoste", "I. Schuurman"], "venue": "Essential speech and language technology for Dutch, pages 219\u2013247. Springer.", "citeRegEx": "Oostdijk et al\\.,? 2013", "shortCiteRegEx": "Oostdijk et al\\.", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "EMNLP, volume 14, pages 1532\u2013 1543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks, pages 45\u201350, Valletta,", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka,? 2010", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka", "year": 2010}, {"title": "Semantic similarity in a taxonomy: An information-based measure and its application to problems of ambiguity in natural language", "author": ["P. Resnik"], "venue": "J. Artif. Intell. Res.(JAIR), 11:95\u2013 130.", "citeRegEx": "Resnik,? 1999", "shortCiteRegEx": "Resnik", "year": 1999}, {"title": "An improved model of semantic similarity based on lexical co-occurrence", "author": ["D.L. Rohde", "L.M. Gonnerman", "D.C. Plaut"], "venue": "Communications of the ACM, 8:627\u2013633.", "citeRegEx": "Rohde et al\\.,? 2006", "shortCiteRegEx": "Rohde et al\\.", "year": 2006}, {"title": "Two decades of statistical language modeling: Where do we go from here", "author": ["R. Rosenfeld"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Rosenfeld,? \\Q2000\\E", "shortCiteRegEx": "Rosenfeld", "year": 2000}, {"title": "Building large corpora from the web using a new efficient tool chain", "author": ["R. Sch\u00e4fer", "F. Bildhauer"], "venue": "LREC, pages 486\u2013493.", "citeRegEx": "Sch\u00e4fer and Bildhauer,? 2012", "shortCiteRegEx": "Sch\u00e4fer and Bildhauer", "year": 2012}, {"title": "Mapping german tweets to geographic regions", "author": ["T. Scheffler", "J. Gontrum", "M. Wegel", "S. Wendler"], "venue": "Proceedings of the NLP4CMC Workshop at Konvens.", "citeRegEx": "Scheffler et al\\.,? 2014", "shortCiteRegEx": "Scheffler et al\\.", "year": 2014}, {"title": "Evaluation methods for unsupervised word embeddings", "author": ["T. Schnabel", "I. Labutov", "D. Mimno", "T. Joachims"], "venue": "Proc. of EMNLP.", "citeRegEx": "Schnabel et al\\.,? 2015", "shortCiteRegEx": "Schnabel et al\\.", "year": 2015}, {"title": "Projections for efficient document clustering", "author": ["H. Sch\u00fctze", "C. Silverstein"], "venue": "ACM SIGIR Forum, volume 31, pages 74\u201381. ACM.", "citeRegEx": "Sch\u00fctze and Silverstein,? 1997", "shortCiteRegEx": "Sch\u00fctze and Silverstein", "year": 1997}, {"title": "Word representations: a simple and general method for semi-supervised learning", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": "Proceedings of the 48th annual meeting of the association for computational linguistics, pages", "citeRegEx": "Turian et al\\.,? 2010", "shortCiteRegEx": "Turian et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 6, "context": "The strong variability of language use within, and across textual media (Collins et al., 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al.", "startOffset": 72, "endOffset": 108}, {"referenceID": 20, "context": "The strong variability of language use within, and across textual media (Collins et al., 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al.", "startOffset": 72, "endOffset": 108}, {"referenceID": 28, "context": ", 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al.", "startOffset": 136, "endOffset": 167}, {"referenceID": 30, "context": ", 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al.", "startOffset": 136, "endOffset": 167}, {"referenceID": 13, "context": ", 1977; Linell, 1982) has on many occasions been marked as an important challenge for research in the area of computational linguistics (Resnik, 1999; Rosenfeld, 2000), in particular in applications to social media (Gouws et al., 2011).", "startOffset": 215, "endOffset": 235}, {"referenceID": 0, "context": "Given these developments, we found it surprising that only one set of word embeddings has been publicly released for Dutch (Al-Rfou et al., 2013), which does not offer sufficiently large dimensionality for state-of-the-art performance.", "startOffset": 123, "endOffset": 145}, {"referenceID": 1, "context": "In the literature, there has been some debate on the effectiveness of prediction-based embeddings when compared to more classical count-based embedding models (Baroni et al., 2014).", "startOffset": 159, "endOffset": 180}, {"referenceID": 14, "context": "An idea mostly brought forward by the earlier distributional semantic models (DSMs), is that the context in which words occur (the distribution of the words surrounding them) can serve as a representation of their meaning, also known as the distributional hypothesis (Harris, 1954).", "startOffset": 267, "endOffset": 281}, {"referenceID": 15, "context": ", 1989, 1990), PLSA (Hofmann, 1999) and LDA (Blei et al.", "startOffset": 20, "endOffset": 35}, {"referenceID": 3, "context": ", 1989, 1990), PLSA (Hofmann, 1999) and LDA (Blei et al., 2003), which first create an explicit matrix of occurrence counts for a number of documents, and then factor this matrix into a lowdimensional, dense representation using Singular Value Decomposition (SVD) (Sch\u00fctze and Silverstein, 1997).", "startOffset": 44, "endOffset": 63}, {"referenceID": 34, "context": ", 2003), which first create an explicit matrix of occurrence counts for a number of documents, and then factor this matrix into a lowdimensional, dense representation using Singular Value Decomposition (SVD) (Sch\u00fctze and Silverstein, 1997).", "startOffset": 208, "endOffset": 239}, {"referenceID": 21, "context": "A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al.", "startOffset": 129, "endOffset": 153}, {"referenceID": 5, "context": "A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al.", "startOffset": 290, "endOffset": 342}, {"referenceID": 16, "context": "A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al.", "startOffset": 290, "endOffset": 342}, {"referenceID": 29, "context": "A more explicit way of implementing the distributional hypothesis is through the use of matrices containing co-occurrence counts (Lund and Burgess, 1996), which are then optionally transformed through the use of some informationtheoretic measure, such as PMI (Pointwise Mutual Information) (Bullinaria and Levy, 2007; Levy and Goldberg, 2014) or entropy (Rohde et al., 2006).", "startOffset": 354, "endOffset": 374}, {"referenceID": 2, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 7, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 24, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 23, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 26, "context": "Recently, several models which create prediction-based word embeddings (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2009; Mikolov et al., 2013b; Pennington et al., 2014) have proved successful (Turian et al.", "startOffset": 71, "endOffset": 191}, {"referenceID": 35, "context": ", 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP.", "startOffset": 31, "endOffset": 97}, {"referenceID": 8, "context": ", 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP.", "startOffset": 31, "endOffset": 97}, {"referenceID": 1, "context": ", 2014) have proved successful (Turian et al., 2010; Collobert et al., 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP.", "startOffset": 31, "endOffset": 97}, {"referenceID": 26, "context": "When compared to other methods, such as GloVe (Pennington et al., 2014), SPPMI has showed increased performance (Levy et al.", "startOffset": 46, "endOffset": 71}, {"referenceID": 17, "context": ", 2014), SPPMI has showed increased performance (Levy et al., 2015).", "startOffset": 48, "endOffset": 67}, {"referenceID": 1, "context": ", 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP. Following Levy et al. (2014), we call the embeddings represented by dense vectors implicit, as it is not immediately clear what each dimension represents.", "startOffset": 8, "endOffset": 135}, {"referenceID": 1, "context": ", 2011; Baroni et al., 2014) and consequently have quickly found their way into many applications of NLP. Following Levy et al. (2014), we call the embeddings represented by dense vectors implicit, as it is not immediately clear what each dimension represents. Matrix-based sparse embeddings are then called explicit as each dimension represents a separate context, which is more easily interpretable. One of the more successful and most popular methods for creating word embeddings is word2vec (Mikolov et al., 2013a,b). While word2vec often referred to as a single model, it is actually a collection of two different architectures, SkipGram (SG) and Continuous Bag of Words (CBoW), and two different training methods, hierarchical skipgram (HS) and negative sampling (NS). Levy et al. (2015) show that one of the architectures in the word2vec toolkit, SkipGram with Negative Sampling (SGNS) implicitly factorizes a co-occurrence matrix which has been shifted by a factor of log(k), where k is the number of negative samples.", "startOffset": 8, "endOffset": 794}, {"referenceID": 25, "context": "SoNaR The SoNaR corpus (Oostdijk et al., 2013) is compiled from a large number of disparate sources, including newsletters, press releases, books, magazines and newspapers.", "startOffset": 23, "endOffset": 46}, {"referenceID": 31, "context": "COW The COW corpus (Sch\u00e4fer and Bildhauer, 2012) is a 4 billion word corpus which was automatically retrieved from domains from the .", "startOffset": 19, "endOffset": 48}, {"referenceID": 31, "context": "nl top level domains in 2011 and 2014 (Sch\u00e4fer and Bildhauer, 2012).", "startOffset": 38, "endOffset": 67}, {"referenceID": 4, "context": "Tokenization and lemmatization of each post was performed using Frog (Bosch et al., 2007).", "startOffset": 69, "endOffset": 89}, {"referenceID": 27, "context": "For each corpus, we trained models using the word2vec implementation (\u0158eh\u016f\u0159ek and Sojka, 2010; Mikolov et al., 2013a) from gensim7.", "startOffset": 69, "endOffset": 117}, {"referenceID": 22, "context": "For each corpus, we trained models using the word2vec implementation (\u0158eh\u016f\u0159ek and Sojka, 2010; Mikolov et al., 2013a) from gensim7.", "startOffset": 69, "endOffset": 117}, {"referenceID": 22, "context": "For each corpus, we trained models using the word2vec implementation (\u0158eh\u016f\u0159ek and Sojka, 2010; Mikolov et al., 2013a) from gensim7. In order to determine optimal settings for the hyperparameters, several models were trained with different parameter values in parallel and were evaluated in the relation evaluation task (see below). For word2vec the SGNS architecture with a negative sampling of 15, a vector size of 320, and a window size of 11 maximized the quality across all corpora. For the SPPMI models, we created embeddings for the 50,000 most frequent words, experimenting with window sizes of 5 and 10, and shift constants of 1, 5 and 10. For all models, a shift constant of 1 and a window size of 5 produced the best results, the exception being the model based on the Roularta corpus, which performed best with a shift constant of 5 and a window size of 5. Relying on only one set of hyperparameters, as well as the performance of the relation task, could be seen as a point of contention. However, we argue in line with Schnabel et al. (2015) that \u2018true\u2019 performance across unre-", "startOffset": 95, "endOffset": 1055}, {"referenceID": 0, "context": "Finally, in addition to our own models, we use the Polyglot embeddings (Al-Rfou et al., 2013) as a baseline, as this is currently the only available set of embeddings for Dutch.", "startOffset": 71, "endOffset": 93}, {"referenceID": 12, "context": "This identification of text containing dialect has been of interest to researchers across different languages such as Spanish (Gon\u00e7alves and S\u00e1nchez, 2014), German (Scheffler et al.", "startOffset": 126, "endOffset": 155}, {"referenceID": 32, "context": "This identification of text containing dialect has been of interest to researchers across different languages such as Spanish (Gon\u00e7alves and S\u00e1nchez, 2014), German (Scheffler et al., 2014), and Arabic (Lin et al.", "startOffset": 164, "endOffset": 188}, {"referenceID": 19, "context": ", 2014), and Arabic (Lin et al., 2014).", "startOffset": 20, "endOffset": 38}, {"referenceID": 22, "context": "Finally, Mikolov et al. (2013a) report comparable performance (51.", "startOffset": 9, "endOffset": 32}], "year": 2016, "abstractText": "Word embeddings have recently seen a strong increase in interest as a result of strong performance gains on a variety of tasks. However, most of this research also underlined the importance of benchmark datasets, and the difficulty of constructing these for a variety of language-specific tasks. Still, many of the datasets used in these tasks could prove to be fruitful linguistic resources, allowing for unique observations into language use and variability. In this paper we demonstrate the performance of multiple types of embeddings, created with both count and prediction-based architectures on a variety of corpora, in two language-specific tasks: relation evaluation, and dialect identification. For the latter, we compare unsupervised methods with a traditional, handcrafted dictionary. With this research, we provide the embeddings themselves, the relation evaluation task benchmark for use in further research, and demonstrate how the benchmarked embeddings prove a useful unsupervised linguistic resource, effectively used in a downstream task.", "creator": "LaTeX with hyperref package"}}}