{"id": "1607.00325", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation", "abstract": "We propose a novel deep learning model, which supports permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from most of the prior arts that treat speech separation as a multi-class regression problem and the deep clustering technique that considers it a segmentation (or clustering) problem, our model optimizes for the separation regression error, ignoring the order of mixing sources. This strategy cleverly solves the long-lasting label permutation problem that has prevented progress on deep learning based techniques for speech separation. Experiments on the equal-energy mixing setup of a Danish corpus confirms the effectiveness of PIT. We believe improvements built upon PIT can eventually solve the cocktail-party problem and enable real-world adoption of, e.g., automatic meeting transcription and multi-party human-computer interaction, where overlapping speech is common.", "histories": [["v1", "Fri, 1 Jul 2016 17:34:16 GMT  (226kb,D)", "http://arxiv.org/abs/1607.00325v1", "9 pages"], ["v2", "Tue, 3 Jan 2017 19:57:37 GMT  (131kb,D)", "http://arxiv.org/abs/1607.00325v2", "5 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["dong yu", "morten kolb{\\ae}k", "zheng-hua tan", "jesper jensen"], "accepted": false, "id": "1607.00325"}, "pdf": {"name": "1607.00325.pdf", "metadata": {"source": "CRF", "title": "Permutation Invariant Training of Deep Models for Speaker-Independent Multi-talker Speech Separation", "authors": ["Dong Yu", "Morten Kolb\u00e6k", "Zheng-Hua Tan", "Jesper Jensen"], "emails": ["dongyu@microsoft.com", "mok@es.aau.dk}", "zt@es.aau.dk}", "jje@es.aau.dk}"], "sections": [{"heading": "1 Introduction", "text": "This year is more than ever before in the history of the European Union."}, {"heading": "2 Speech separation problem", "text": "Although our approach is applicable to multi-channel speech signals, in many application scenarios it is often sufficient to regenerate the three remaining energy sources, focusing on the problem of monaural speech separation, which requires more complex and comprehensive assessments. (The goal of monaural speech separation is to estimate the individual source signals in a linear mixed microphone signal in which the source signals generally overlap in the time-frequency domain.) Let us estimate the S source signal sequences in the time domain as xs (t) and Y (t, f) as a mixed signal sequence as y (t) = 1 xs (t) = 1 xs (t) s (t) s (t, f), the corresponding short time-Fourier transformation (STFT) of these signals are Xs (t, f) and Y (t).s Ss = 1 Xs (t, f), f), and f) for each time respectively."}, {"heading": "3 Permutation invariant training", "text": "In fact, it is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project, and which is a reactionary project, which is a reactionary project, which is a reactionary project, which is a reactionary project."}, {"heading": "4 Speech separation and tracing", "text": "During the conclusion, the only information available is the mixed language. Language separation can be performed directly for each input meta frame for which one output meta frame with M frames of the language is estimated for each stream. The input meta frame is then shifted by one (or more) frames. As the output meta frames overlap, we can improve language tracing accuracy by not only optimizing the reconstruction error as described in Section 3, but also the MSE between a subset (e.g. the center) of the overlapped frames under the Multi-Task Optimization Frame. Further, accuracy improvement can be achieved by embedding each estimated source in an embedding space and optimizing the correlation between the meta frames from the same source."}, {"heading": "5 Related work", "text": "There are only two previous arts that are closely related to our work. In Weng et al. \"s work [28], the label ambiguity problem is alleviated by using immediate energy as a keyword. This approach poses three problems. Firstly, in many cases it is difficult to extend the immediate energy-based approach to more than two sources, and therefore the system performs poorly in these cases. However, in our approach, the PIT algorithm automatically determines the best keywords for separation for each mixture. Secondly, it is difficult to extend the immediate energy-based approach to more than two sources. However, PIT does not have this limitation. Thirdly, because the immediate energy frequently changes and there are no additional keywords to determine the currents, a complicated non-scalable joint decoder is used for loudspeaker tracking. However, with PIT it is possible to incorporate tracking mechanisms directly into the model."}, {"heading": "6 Experimental results", "text": "We evaluated our approach using a Danish corpus [18], which consists of about 560 loudspeakers, each speaking 312 utterances, which are a mixture of normal sentences, commands and numerical sequences, with an average duration of about 5 seconds.The training requires both the mixed language and each mixed source. After other work (e.g. [11]) in this room, the training data was artificially mixed in our experiments.There are various ways of mixing data from different sources.Since the most difficult constellation for both humans and machines is that the mixed sources have the same energy (i.e. 0 dB) [5, 28] our evaluation focused on this condition. If the mixing expressions have different lengths, the shorter ones were fed with lower noise. We used a 257-dimensional STFT size spectrum as a target and input indicator for the network. Our model was implemented using the computer network tool (CNT8]."}, {"heading": "6.1 Training behavior", "text": "In Figure 3, we have illustrated the training progress measured by the MSE on the training and validation set with conventional training and PIT. It is clear from the figure that training with the conventional approach achieves nothing, no matter how we adjust hyperparameters due to the label permutation problem discussed in [11, 28]. In contrast, training quickly converts to very small MSE for bilingual and trilingual mixed languages when PIT is used."}, {"heading": "6.2 Signal-to-distortion ratio improvement", "text": "We have the potential to improve the signal (SDR) [24], a metaphor that is widely used to assess the performance of people, and which then 232, 40 and 40 expressions of people who are able to learn another language (CC)."}, {"heading": "7 Conclusion and discussion", "text": "In this paper, we have described a novel permutation invariant training technique for speaker-independent multitalker language separation. To the best of our knowledge, this is the first successful work in which the separation view of the task is used instead of the multi-class regression or segmentation techniques used in earlier arts, but with intrinsic limitations. This is a major step towards solving the important cocktail-party problem in a real environment where the number of speakers during training time is unknown. Our experiments on the energy-equal construction of dual talker language separation tasks suggest that PIT-trained models generalize well to invisible speakers and languages. The key insights and ideas in this work can be applied to many tasks where symmetry is a property. The purpose of this essay is to describe the key and novel technique that enables training for the separation of multitalker language. The overall system can be improved in many ways."}], "references": [{"title": "Convolutional neural networks for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Auditory scene analysis: The perceptual organization of sound", "author": ["Albert S. Bregman"], "venue": "MIT press,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1994}, {"title": "Some experiments on the recognition of speech, with one and with two ears", "author": ["E. Colin Cherry"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1953}, {"title": "Modelling auditory processing and organisation, volume 7", "author": ["Martin Cooke"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Monaural speech separation and recognition challenge", "author": ["Martin Cooke", "John R. Hershey", "Steven J. Rennie"], "venue": "Computer Speech and Language,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition", "author": ["George E. Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Prediction-driven computational auditory scene analysis", "author": ["Daniel P.W. Ellis"], "venue": "PhD thesis, Massachusetts Institute of Technology,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1996}, {"title": "An introduction to computational networks and the computational network toolkit", "author": ["Amit"], "venue": "Technical report, Microsoft Technical Report MSR-TR-2014-112,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "CSR-I (WSJ0) Complete LDC93S6A", "author": ["Garofolo", "John"], "venue": "Philadelphia: Linguistic Data Consortium,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1993}, {"title": "Factorial hidden markov models", "author": ["Zoubin Ghahramani", "Michael I. Jordan"], "venue": "Machine learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Deep clustering: Discriminative embeddings for segmentation and separation", "author": ["John R. Hershey", "Zhuo Chen", "Jonathan Le Roux", "Shinji Watanabe"], "venue": "arXiv preprint arXiv:1508.04306,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N. Sainath"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Deep embedding network for clustering", "author": ["Peihao Huang", "Yan Huang", "Wei Wang", "Liang Wang"], "venue": "In ICPR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Joint optimization of masks and deep recurrent neural networks for monaural source separation", "author": ["Po-Sen Huang", "Minje Kim", "Mark Hasegawa-Johnson", "Paris Smaragdis"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Super-human multi-talker speech recognition: the ibm 2006 speech separation challenge system", "author": ["Trausti T. Kristjansson", "John R. Hershey", "Peder A. Olsen", "Steven J. Rennie", "Ramesh A. Gopinath"], "venue": "In INTERSPEECH,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Sparse nmf\u2013half-baked or well done", "author": ["Jonathan Le Roux", "Felix Weninger", "J Hershey"], "venue": "Mitsubishi Electric Research Labs (MERL),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Yann LeCun", "Yoshua Bengio"], "venue": "The handbook of brain theory and neural networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1995}, {"title": "The ibm 2016 english conversational telephone speech recognition system", "author": ["George Saon", "Tom Sercu", "Steven Rennie", "Hong-Kwang J. Kuo"], "venue": "arXiv preprint arXiv:1604.08242,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Single-channel speech separation using sparse non-negative matrix factorization", "author": ["Mikkel N. Schmidt", "Rasmus Kongsgaard Olsson"], "venue": "In INTERSPEECH,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Frank Seide", "Gang Li", "Dong Yu"], "venue": "In INTERSPEECH,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Convolutive speech bases and their application to supervised speech separation", "author": ["Paris Smaragdis"], "venue": "IEEE Transactions on Audio, Speech and Language Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "Learning deep representations for graph clustering", "author": ["Fei Tian", "Bin Gao", "Qing Cui", "Enhong Chen", "Tie-Yan Liu"], "venue": "In AAAI,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Performance measurement in blind audio source separation", "author": ["E. Vincent", "R. Gribonval", "C. Fevotte"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Speech recognition using factorial hidden markov models for separation in the feature space", "author": ["Tuomas Virtanen"], "venue": "In INTERSPEECH. Citeseer,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2006}, {"title": "On training targets for supervised speech separation", "author": ["Yuxuan Wang", "Arun Narayanan", "DeLiang Wang"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Monaural speech separation using source-adapted models", "author": ["Ron J. Weiss", "Daniel P.W. Ellis"], "venue": "In Applications of Signal Processing to Audio and Acoustics,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2007}, {"title": "Deep neural networks for single-channel multi-talker speech recognition", "author": ["Chao Weng", "Dong Yu", "Michael L. Seltzer", "Jasha Droppo"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr", "author": ["Felix Weninger", "Hakan Erdogan", "Shinji Watanabe", "Emmanuel Vincent", "Jonathan Le Roux", "John R. Hershey", "Bj\u00f6rn Schuller"], "venue": "In Latent Variable Analysis and Signal Separation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "An experimental study on speech enhancement based on deep neural networks", "author": ["Yong Xu", "Jun Du", "Li-Rong Dai", "Chin-Hui Lee"], "venue": "IEEE Signal Processing Letters,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Roles of pre-training and fine-tuning in context-dependent dbn-hmms for real-world speech recognition", "author": ["Dong Yu", "Li Deng", "George E. Dahl"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32].", "startOffset": 185, "endOffset": 200}, {"referenceID": 11, "context": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32].", "startOffset": 185, "endOffset": 200}, {"referenceID": 19, "context": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32].", "startOffset": 185, "endOffset": 200}, {"referenceID": 29, "context": "In the last five years, the accuracy of automatic speech recognition (ASR) systems has significantly improved thanks to the deep learning techniques exploited in the recent ASR systems [6, 12, 21, 32].", "startOffset": 185, "endOffset": 200}, {"referenceID": 17, "context": "Just six years ago, the word error rate (WER) on the widely accepted Switchboard conversational transcription benchmark task was over 20% and today it has been reduced to below 7% [19].", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "Despite the significant progress made in dictating single-speaker speech, the progress made in multitalker mixed speech separation and recognition, often referred to as the cocktail-party problem [2, 3], has been less impressive.", "startOffset": 196, "endOffset": 202}, {"referenceID": 2, "context": "Despite the significant progress made in dictating single-speaker speech, the progress made in multitalker mixed speech separation and recognition, often referred to as the cocktail-party problem [2, 3], has been less impressive.", "startOffset": 196, "endOffset": 202}, {"referenceID": 4, "context": "Although human listeners can easily perceive separate sources in an acoustic mixture, the same task seems to be extremely difficult for automatic computing systems, especially when only a single microphone recording of the mixed-speech is available [5, 28].", "startOffset": 249, "endOffset": 256}, {"referenceID": 26, "context": "Although human listeners can easily perceive separate sources in an acoustic mixture, the same task seems to be extremely difficult for automatic computing systems, especially when only a single microphone recording of the mixed-speech is available [5, 28].", "startOffset": 249, "endOffset": 256}, {"referenceID": 3, "context": "Before the deep learning era, the most popular technique was computational auditory scene analysis (CASA) [4, 7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 6, "context": "Before the deep learning era, the most popular technique was computational auditory scene analysis (CASA) [4, 7].", "startOffset": 106, "endOffset": 112}, {"referenceID": 15, "context": "Non-negative matrix factorization (NMF) [16, 20, 22] is another popular technique which aims to learn a set of non-negative bases that can be used to estimate mixing factors during evaluation.", "startOffset": 40, "endOffset": 52}, {"referenceID": 18, "context": "Non-negative matrix factorization (NMF) [16, 20, 22] is another popular technique which aims to learn a set of non-negative bases that can be used to estimate mixing factors during evaluation.", "startOffset": 40, "endOffset": 52}, {"referenceID": 20, "context": "Non-negative matrix factorization (NMF) [16, 20, 22] is another popular technique which aims to learn a set of non-negative bases that can be used to estimate mixing factors during evaluation.", "startOffset": 40, "endOffset": 52}, {"referenceID": 4, "context": "Both CASA and NMF led to very limited success in separating different sources in multi-talker mixed speech [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 14, "context": "The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics.", "startOffset": 87, "endOffset": 99}, {"referenceID": 23, "context": "The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics.", "startOffset": 87, "endOffset": 99}, {"referenceID": 25, "context": "The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics.", "startOffset": 87, "endOffset": 99}, {"referenceID": 9, "context": "The most successful technique before the deep learning era is the model based approach [15, 25, 27], such as factorial GMM-HMM [10], that models the interaction between the target and competing speech signals and their temporal dynamics.", "startOffset": 127, "endOffset": 131}, {"referenceID": 5, "context": "Motivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years.", "startOffset": 74, "endOffset": 89}, {"referenceID": 11, "context": "Motivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years.", "startOffset": 74, "endOffset": 89}, {"referenceID": 19, "context": "Motivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years.", "startOffset": 74, "endOffset": 89}, {"referenceID": 29, "context": "Motivated by the success of deep learning techniques in single-talker ASR [6, 12, 21, 32], researchers have developed many deep learning techniques for speech separation in recent years.", "startOffset": 74, "endOffset": 89}, {"referenceID": 13, "context": "Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31].", "startOffset": 104, "endOffset": 120}, {"referenceID": 24, "context": "Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31].", "startOffset": 104, "endOffset": 120}, {"referenceID": 27, "context": "Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31].", "startOffset": 104, "endOffset": 120}, {"referenceID": 28, "context": "Typically, networks are trained based on parallel sets of mixtures and their constituent target sources [14, 26, 29, 31].", "startOffset": 104, "endOffset": 120}, {"referenceID": 13, "context": ", [14]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 10, "context": "To the best of our knowledge, only two deep leaning based works [11, 28] have tried to address and solve this harder problem.", "startOffset": 64, "endOffset": 72}, {"referenceID": 26, "context": "To the best of our knowledge, only two deep leaning based works [11, 28] have tried to address and solve this harder problem.", "startOffset": 64, "endOffset": 72}, {"referenceID": 26, "context": "[28], which achieved the best result on the dataset used in 2006 monaural speech separation and recognition challenge [5], the instantaneous energy was used to solve the label ambiguity problem and a two-speaker joint-decoder with speaker switching penalty was used to separate and trace speakers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[28], which achieved the best result on the dataset used in 2006 monaural speech separation and recognition challenge [5], the instantaneous energy was used to solve the label ambiguity problem and a two-speaker joint-decoder with speaker switching penalty was used to separate and trace speakers.", "startOffset": 118, "endOffset": 121}, {"referenceID": 10, "context": "[11] made significant progress in solving the cocktail-party problem.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "Although this is a valid assumption on tasks such as image segmentation [13, 23], it\u2019s only a very rough approximation on speech separation.", "startOffset": 72, "endOffset": 80}, {"referenceID": 21, "context": "Although this is a valid assumption on tasks such as image segmentation [13, 23], it\u2019s only a very rough approximation on speech separation.", "startOffset": 72, "endOffset": 80}, {"referenceID": 24, "context": ", [26]) that better results can be achieved if, instead of estimating |Xs| directly, we first estimate a set of masks Ms(t, f) using a deep learning model h (f(|Y|; \u03b8)) = M\u0303s(t, f) with the constraint that M\u0303s(t, f) \u2265 0 and \u2211S s=1 M\u0303s(t, f) = 1 for all time-frequency bins (t, f).", "startOffset": 2, "endOffset": 6}, {"referenceID": 24, "context": "To overcome these limitations, recent works[26] directly minimize the MSE", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "\u2019s work [11], all other recent speech separation works use the architecture depicted in Figure 1 in which a two-talker condition is illustrated.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "This problem is referred to as the label ambiguity problem in [28] and label permutation problem in [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 10, "context": "This problem is referred to as the label ambiguity problem in [28] and label permutation problem in [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 26, "context": "\u2019s work [28], the label ambiguity problem is alleviated by using instantaneous energy as the cue.", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "[11], which estimates the likelihood that two time-frequency bins belong to the same speaker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 23, "endOffset": 39}, {"referenceID": 24, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 23, "endOffset": 39}, {"referenceID": 27, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 23, "endOffset": 39}, {"referenceID": 28, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 23, "endOffset": 39}, {"referenceID": 10, "context": "While other prior arts [14, 26, 29, 31] treat the mixed-speech separation as a multi-class regression problem, the approach in [11] considers it a partition (or segmentation) problem.", "startOffset": 127, "endOffset": 131}, {"referenceID": 10, "context": "Our approach sits between the approach in [11] and", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "other prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem.", "startOffset": 17, "endOffset": 33}, {"referenceID": 24, "context": "other prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem.", "startOffset": 17, "endOffset": 33}, {"referenceID": 27, "context": "other prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem.", "startOffset": 17, "endOffset": 33}, {"referenceID": 28, "context": "other prior arts [14, 26, 29, 31] and treats the mixed-speech separation problem as a truly separation problem.", "startOffset": 17, "endOffset": 33}, {"referenceID": 10, "context": "While it is difficult to incorporate, for example, complex masks into the system in [11], it is straight-forward in our model.", "startOffset": 84, "endOffset": 88}, {"referenceID": 10, "context": "In addition, the approach in [11] requires a post-DNN clustering step which cannot be jointly trained easily.", "startOffset": 29, "endOffset": 33}, {"referenceID": 10, "context": ", [11]) in this space, the training data were mixed artificially in our experiments.", "startOffset": 2, "endOffset": 6}, {"referenceID": 4, "context": "e, 0 dB) [5, 28], our evaluation focused on this condition.", "startOffset": 9, "endOffset": 16}, {"referenceID": 26, "context": "e, 0 dB) [5, 28], our evaluation focused on this condition.", "startOffset": 9, "endOffset": 16}, {"referenceID": 7, "context": "Our model was implemented using the computational network toolkit (CNTK) [8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "In all our experiments we used simple feed-forward DNNs with three hidden layers each with 1024 units, instead of more powerful CNNs [1, 17] and LSTMs.", "startOffset": 133, "endOffset": 140}, {"referenceID": 16, "context": "In all our experiments we used simple feed-forward DNNs with three hidden layers each with 1024 units, instead of more powerful CNNs [1, 17] and LSTMs.", "startOffset": 133, "endOffset": 140}, {"referenceID": 10, "context": "From the figure we can see clearly that training goes nowhere with the conventional approach no matter how we adjust hyper-parameters due to the label permutation problem discussed in [11, 28].", "startOffset": 184, "endOffset": 192}, {"referenceID": 26, "context": "From the figure we can see clearly that training goes nowhere with the conventional approach no matter how we adjust hyper-parameters due to the label permutation problem discussed in [11, 28].", "startOffset": 184, "endOffset": 192}, {"referenceID": 22, "context": "We further evaluated PIT on its potential to improve the signal-to-distortion ratio (SDR) [24], a metric widely used to evaluate speech enhancement performance, on a two-talker mixed-speech dataset.", "startOffset": 90, "endOffset": 94}, {"referenceID": 8, "context": "Finally, an 1k mixture test set was constructed using 45 male and 45 female speakers, each with 140 utterances, from the si_tr_s part of the Wall Street Journal (WSJ0) [9] English corpus.", "startOffset": 168, "endOffset": 171}, {"referenceID": 10, "context": "Similar to [11] we have used the true mixing sources to determine the correct assignment during evaluation.", "startOffset": 11, "endOffset": 15}, {"referenceID": 10, "context": "This best improvement is better than that reported in [11] which was trained on English data, used more powerful BLSTMs, and evaluated on simpler setups with SNRs across 0-10 dB.", "startOffset": 54, "endOffset": 58}], "year": 2016, "abstractText": "We propose a novel deep learning model, which supports permutation invariant training (PIT), for speaker independent multi-talker speech separation, commonly known as the cocktail-party problem. Different from most of the prior arts that treat speech separation as a multi-class regression problem and the deep clustering technique that considers it a segmentation (or clustering) problem, our model optimizes for the separation regression error, ignoring the order of mixing sources. This strategy cleverly solves the long-lasting label permutation problem that has prevented progress on deep learning based techniques for speech separation. Experiments on the equal-energy mixing setup of a Danish corpus confirms the effectiveness of PIT. We believe improvements built upon PIT can eventually solve the cocktail-party problem and enable real-world adoption of, e.g., automatic meeting transcription and multi-party human-computer interaction, where overlapping speech is common.", "creator": "LaTeX with hyperref package"}}}