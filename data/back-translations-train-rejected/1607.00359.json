{"id": "1607.00359", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Moving Toward High Precision Dynamical Modelling in Hidden Markov Models", "abstract": "Hidden Markov Model (HMM) is often regarded as the dynamical model of choice in many fields and applications. It is also at the heart of most state-of-the-art speech recognition systems since the 70's. However, from Gaussian mixture models HMMs (GMM-HMM) to deep neural network HMMs (DNN-HMM), the underlying Markovian chain of state-of-the-art models did not changed much. The \"left-to-right\" topology is mostly always employed because very few other alternatives exist. In this paper, we propose that finely-tuned HMM topologies are essential for precise temporal modelling and that this approach should be investigated in state-of-the-art HMM system. As such, we propose a proof-of-concept framework for learning efficient topologies by pruning down complex generic models. Speech recognition experiments that were conducted indicate that complex time dependencies can be better learned by this approach than with classical \"left-to-right\" models.", "histories": [["v1", "Fri, 1 Jul 2016 19:20:50 GMT  (87kb,D)", "http://arxiv.org/abs/1607.00359v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["s\\'ebastien gagnon", "jean rouat"], "accepted": false, "id": "1607.00359"}, "pdf": {"name": "1607.00359.pdf", "metadata": {"source": "CRF", "title": "Moving Toward High Precision Dynamical Modelling in Hidden Markov Models", "authors": ["S\u00e9bastien Gagnon"], "emails": ["sebastien.gagnon2@usherbrooke.ca,", "jean.rouat@usherbrooke.ca"], "sections": [{"heading": null, "text": "The Hidden Markov Model (HMM) is often seen as a dynamic model of choice in many areas and applications, and has been at the heart of most state-of-the-art speech recognition systems since the 1970s. However, from Gaussian mixing models HMM (GMMHMM) to deep neural network HMMs (DNN-HMM), the underlying Markovian chain of modern models has not changed significantly. \"Left-to-right\" topology is usually used because there are very few other alternatives. In this paper, we propose that finely tuned HMM topologies are indispensable for precise time modeling, and that this approach should be explored in the state-of-the-art HMM system. As such, we propose a proof-of-concept framework for learning efficient topologies by cutting down complex generic models. Speech recognition experiments that have been conducted suggest that complex time dependencies can be better learned through these \"right-by classical models.\""}, {"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "2 Transition and Emission", "text": "Probabilities ImbalanceAs discussed in [13], transition probabilities may not play a significant role in path decoding (using Viterbi); recognition could be completely independent of them. Then, all transitions that leave the same state could be considered effectively equivalent during path decoding. To the knowledge of the authors, this phenomenon is not quantitatively documented for speech recognition. In the symbolic implementation of the Viterbi algorithm, the state s (t + 1) currently occupied by t + 1 is given by [14]: s (t + 1) = argmax k = 1,2, N [as (t) \u2192 kbk (Ot) \u2192 kbk capability that is executed is the total number of emitting states in the model. Let there be a distinction between zero and non-zero transition probabilities: (i, k) [1, N]; ai \u2192 k 6 = 0."}, {"heading": "3 Pruning", "text": "The encoding of acoustic dynamic properties in a generic HMM model consists in changing its topology, i.e. activating or deactivating transitions. A deactivated transition has a probability value of 0 and is therefore not involved in (4). Topology can be learned in three ways: either by \"growing\" from a simple prototype model (e.g. [16]), \"pruning\" from a complex generic model (e.g. [17]) or by a mixture of both (e.g. [10]). In \"waxing\" techniques, i.e. increasing the complexity of the model, an almost unfounded guess must be made as to how the expansion will take place. This is very much subject to human error. On the other hand, \"circumcision processes\" are much more reliable because they only remove the paths that are not frequently visited. Mak and Chan [17], for example, have successfully used circumcisions on a \"left-oriented\" topology with long transitions."}, {"heading": "4 Proposed System", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Integration of the Pruning Module and Threshold Optimization", "text": "To increase temporal modeling accuracy, a modification of the standard HMM training method is proposed. Fig. 1 illustrates the entire proposed system. Intersection (Step # 5) is performed by comparing each individual transition probability with a threshold value: If ai \u2192 j > then hold, otherwise ai \u2192 j = 0 (5) Since the value is unknown, optimization steps are required during training to find it (using loop step # 6 in Fig. 1). The very simple optimization process is designed to exploit the continuous relationship between intersection threshold and performance, thus avoiding unpredictable local minimums."}, {"heading": "4.2 Initialization by Model Flattening", "text": "In order to maximize the beneficial effects of the transition parameter section, we are working on a complex prototype model (high number of states and transitions), but this can be difficult because the transition parameter space is larger than with simple \"left-to-right\" topologies. If it is inappropriate, alternative pathways tend to die during training (i.e. a very low occupancy probability), preferring only one way through the topology. Thus, the model effectively returns to a long \"left-to-right\" chain of mediocre performance. Therefore, the initialization of a complex HMM model is an important aspect of this work, using the flattening technique presented in [12]. Since a multi-Gauss mixing model can be considered as a single Gaussian state HMM, the flattening consists in replacing the states of a \"left-to-right\" model with the respective HMM shape. \"This already refers to the final class of the model."}, {"heading": "4.3 Feedback of Emission Models", "text": "In order to further enhance the detection performance of the proposed system, the emission models of the truncated HMMs are traced back to the initialization step (link between steps # 8 and # 3 in Fig.1), which are each the emission distributions of state i during initialization and after training. Since complex models require more care to train appropriately due to their enormous transition parameter space, this step is added to allow slower convergence. Experimentally, we observed that with about 10 iterations of this \"feedback\" process (step # 8 in Fig. 1), the detection performance on the training set appears to be saturated. While this step is not required to exceed basic accuracies in pure word detection, it increases the performance even further on average (Table 1). However, it also significantly increases the computational effort required for training."}, {"heading": "5 Experimental Framework", "text": "In fact, we will find ourselves in a position to be in a position to be in a position to be in a position to be in a position to be in a position to be angry."}, {"heading": "6 Results and Discussion", "text": "Table 1 shows significant WHO reductions with the proposed approach to pure word recognition. If this approach is trained and tested in reverb-sensitive environments without additive noise, performance also improves significantly on average. As the reverb adds time dependencies to signals, increasing their temporal complexity, this shows that temporal precision has indeed been increased. However, the proposed system is also less robust against additive noise, which can be explained by a loss of generalization power caused by the improved precision [5]. To our knowledge, the poor performance with the TIMIT database (shown in Table 2) is best explained by the fact that monophonic HMMs have little to no temporal structure, as the name suggests. Therefore, a model specifically designed to encode complex temporal behaviors is unsuitable for this recognition. We therefore see that monophonic speech recognition may not benefit from higher-time classification with higher precision."}], "references": [{"title": "Continuous speech recognition by statistical methods", "author": ["F. Jelinek"], "venue": "Proceedings of the IEEE, vol. 64, no. 4, pp. 532 \u2013 56, 1976/04.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1976}, {"title": "Speaking in shorthand-a syllablecentric perspective for understanding pronunciation variation", "author": ["S. Greenberg"], "venue": "Speech Communication, vol. 29, no. 2-4, pp. 159 \u2013 76, 1999/11.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Acoustic modeling using deep belief networks", "author": ["A. Mohamed", "G.E. Dahl", "G. Hinton"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 20, no. 1, pp. 14 \u2013 22, 2012/01.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "On over-fitting in model selection and subsequent selection bias in performance evaluation", "author": ["Gavin C. Cawley", "Nicola L.C. Talbot"], "venue": "Journal of Machine Learning Research, vol. 11, pp. 2079 \u2013 2107, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "A study on the generalization capability of acoustic models for robust speech recognition", "author": ["Xiong Xiao", "Jinyu Li", "Eng Siong Chng", "Haizhou Li", "Chin-Hui Lee"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 18, no. 6, pp. 1158 \u2013 69, 2010/08.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Noise adaptive training for robust automatic speech recognition", "author": ["Ozlem Kalinli", "Michael L. Seltzer", "Jasha Droppo", "Alex Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 18, no. 8, pp. 1889 \u2013 1901, 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1889}, {"title": "High-performance HMM adaptation with joint compensation of additive and convolutive distortions via vector taylor series", "author": ["Jinyu Li", "Li Deng", "Dong Yu", "Yifan Gong", "A. Acero"], "venue": "2007 IEEE Workshop on Automatic Speech Recognition and Understanding, Piscataway, NJ, USA, 2007//, pp. 65 \u2013 70.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving robustness of deep neural network acoustic models via speech separation and joint adaptive training", "author": ["A. Narayanan", "DeLiang Wang"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 23, no. 1, pp. 92 \u2013 101, 2015/01.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A left-to-right HDP-HMM with HDPM emissions", "author": ["A.H.H.N. Torbati", "J. Picone", "M. Sobel"], "venue": "2014 48th Annual Conference on Information Sciences and Systems (CISS), Piscataway, NJ, USA, 2014, p. 6 pp.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Stranded gaussian mixture hidden markov models for robust speech recognition", "author": ["Yong Zhao", "Biing-Hwang Juang"], "venue": "Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2012), Piscataway, NJ, USA, 2012, pp. 4301 \u2013 4.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Hidden markov models for speech recognition - strengths and limitations", "author": ["L.R. Rabiner", "B.H. Juang"], "venue": "Speech Recognition and Understanding. Recent Advances, Trends and Applica- 7  tions. Proceedings of the NATO Advanced Study Institute, pp. 3 \u2013 29, 1992.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Token passing: a simple conceptual model for connected speech recognition systems", "author": ["S.J. Young", "N.H. Russell", "J.H.S. Thornton"], "venue": "1989.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1989}, {"title": "A tutorial on hidden markov models and selected applications in speech recognition", "author": ["L.R. Rabiner"], "venue": "Proceedings of the IEEE, vol. 77, no. 2, pp. 257 \u2013 86, 1989/02/.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1989}, {"title": "Automatic generation of non-uniform hmm structures based on variational bayesian approach", "author": ["T. Jitsuhiro", "S. Nakamura"], "venue": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing, Piscataway, NJ, USA, 2004, vol. vol.1, pp. 805 \u2013 8.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Pruning hidden markov models with optimal brain surgeon", "author": ["B. Mak", "Kin-Wah Chan"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 13, no. 5, pp. 993 \u2013 1003, Sept. 2005.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Open acoustic impulse response (open air) library", "author": ["D. Murphy", "S. Shelley"], "venue": ". 8", "citeRegEx": "18", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "In the 70\u2019s a \u201cleft-to-right\u201d topology was first proposed for speech modelling, meaning that feature changes through time always flowed in a specific sequential order [1].", "startOffset": 167, "endOffset": 170}, {"referenceID": 1, "context": "It is however simplifying considering that spontaneous speech dynamics are known to be very variable [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "Up to these days, most state-ofthe-art ASR systems such as deep neural networksHMMs (DNN-HMMs, [3]) are still based on that architecture.", "startOffset": 95, "endOffset": 98}, {"referenceID": 3, "context": "Too much precision, however, can lead to an overfitted model without generalizing power [4], making it unable to recognize anything but the training signals.", "startOffset": 88, "endOffset": 91}, {"referenceID": 3, "context": "In model selection, as discussed in [4], the key is to balance precision and generalization for maximum performance.", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "Robustness, the ability of a system to tolerate recording environment changes, is also to be considered and seems to be strongly related to a model\u2019s generalizing power [5].", "startOffset": 169, "endOffset": 172}, {"referenceID": 5, "context": "Based on a more comprehensive approach, these methods deal with unseen noise by modifying model statistics at testing time [6, 7, 8].", "startOffset": 123, "endOffset": 132}, {"referenceID": 6, "context": "Based on a more comprehensive approach, these methods deal with unseen noise by modifying model statistics at testing time [6, 7, 8].", "startOffset": 123, "endOffset": 132}, {"referenceID": 7, "context": "Based on a more comprehensive approach, these methods deal with unseen noise by modifying model statistics at testing time [6, 7, 8].", "startOffset": 123, "endOffset": 132}, {"referenceID": 8, "context": "\u201cLeft-to-right\u201d HDP-HMM [10], a recently developed technique, is one example of such a framework; it is however unbound by Occam\u2019s razor principle or any strong underfitting/overfitting criterion.", "startOffset": 24, "endOffset": 28}, {"referenceID": 8, "context": "In [10], for example, some learned monophone topologies allow decoded paths to be as short as 2 time frames long whereas in standard monophone systems the shortest path is 3 time frames.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "In [12], such an approach is attempted on a complex generic topology with improved additive noise robustness.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "In fact, improved robustness can be linked to a greater generalizing power, as explored in [5].", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "Exposed by Rabiner and Huang in [13], this phenomenon is at the root of the popular thought that transition probabilities are almost useless.", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "Explained in [13] as a lack of pervasive discriminative power of the transition probabilities in path decoding, we conceptualize its effects as rendering equiprobable all transitions that leave the same state.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "Assuming that HMM spoken word models in conventional ASR systems are closer to underfitting than overfitting (a reasoning we based on [2]), we propose to use model flattening [12] in conjunction with transitions pruning to extract precise class topologies.", "startOffset": 134, "endOffset": 137}, {"referenceID": 9, "context": "Assuming that HMM spoken word models in conventional ASR systems are closer to underfitting than overfitting (a reasoning we based on [2]), we propose to use model flattening [12] in conjunction with transitions pruning to extract precise class topologies.", "startOffset": 175, "endOffset": 179}, {"referenceID": 10, "context": "As discussed in [13], transition probabilities may not play a significant role in path decoding (using Viterbi); recognition could be entirely independent of them.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "In the token passing implementation of the Viterbi algorithm, the state s(t+ 1) occupied at time t+ 1 is given by [14]:", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "The procedure is done in 2 steps for each training utterance: first, using the appropriate models (listed in the signal\u2019s label) an ideal path is computed with the forward-backward algorithm [15].", "startOffset": 191, "endOffset": 195}, {"referenceID": 10, "context": "In fact, considering how this problem is exposed in [13], we infer that only in topologies with states of near-infinite branching ratios may this imbalance vanishes.", "startOffset": 52, "endOffset": 56}, {"referenceID": 13, "context": "[16]), \u201cpruning\u201d from a complex generic model (ex.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[17]) or a mixture of both (ex.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[10]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Mak and Chan [17], for example, have successfully used pruning on a \u201cleft-toright\u201d topology with long range transitions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 9, "context": "Thus, the initialization of a complex HMM model is an important aspect of this work, for which the flattening technique presented in [12] is used.", "startOffset": 133, "endOffset": 137}, {"referenceID": 1, "context": "According to [2], this is most likely true for spoken word models.", "startOffset": 13, "endOffset": 16}, {"referenceID": 4, "context": "Furthermore, clean speech accuracies (the tested signal was recorded in the same conditions as the training signals) are the main focus since, as explored in [5], noisy recognition seems to deal more with generalizing power than precision.", "startOffset": 158, "endOffset": 161}, {"referenceID": 15, "context": "As a result, we generated 4 new versions of the Aurora-2 digits dataset, each of them convoluted with a different real world reverberation impulse response (IR) taken from the Openair IR database [18].", "startOffset": 196, "endOffset": 200}, {"referenceID": 4, "context": "This may be explained by a loss of generalizing power caused by the improved precision [5].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "As discussed in [2], this indicates that a high temporal variability exists at the syllable level.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Finally, the proposed framework can also be coupled with complementary state-of-the-art techniques that implement better emission models such as DNNHMMs [3] and online model adaptation [7].", "startOffset": 153, "endOffset": 156}, {"referenceID": 6, "context": "Finally, the proposed framework can also be coupled with complementary state-of-the-art techniques that implement better emission models such as DNNHMMs [3] and online model adaptation [7].", "startOffset": 185, "endOffset": 188}], "year": 2016, "abstractText": "Hidden Markov Model (HMM) is often regarded as the dynamical model of choice in many fields and applications. It is also at the heart of most state-of-theart speech recognition systems since the 70\u2019s. However, from Gaussian mixture models HMMs (GMMHMM) to deep neural network HMMs (DNN-HMM), the underlying Markovian chain of state-of-the-art models did not changed much. The \u201cleft-to-right\u201d topology is mostly always employed because very few other alternatives exist. In this paper, we propose that finely-tuned HMM topologies are essential for precise temporal modelling and that this approach should be investigated in state-of-the-art HMM system. As such, we propose a proof-of-concept framework for learning efficient topologies by pruning down complex generic models. Speech recognition experiments that were conducted indicate that complex time dependencies can be better learned by this approach than with classical \u201cleft-to-right\u201d models.", "creator": "LaTeX with hyperref package"}}}