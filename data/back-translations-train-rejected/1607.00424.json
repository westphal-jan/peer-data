{"id": "1607.00424", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Learning Relational Dependency Networks for Relation Extraction", "abstract": "We consider the task of KBP slot filling -- extracting relation information from newswire documents for knowledge base construction. We present our pipeline, which employs Relational Dependency Networks (RDNs) to learn linguistic patterns for relation extraction. Additionally, we demonstrate how several components such as weak supervision, word2vec features, joint learning and the use of human advice, can be incorporated in this relational framework. We evaluate the different components in the benchmark KBP 2015 task and show that RDNs effectively model a diverse set of features and perform competitively with current state-of-the-art relation extraction.", "histories": [["v1", "Fri, 1 Jul 2016 22:11:38 GMT  (231kb,D)", "http://arxiv.org/abs/1607.00424v1", "In Proceedings of Sixth International Workshop on Statistical Relational AI at the 25th International Joint Conference on Artificial Intelligence (IJCAI)"]], "COMMENTS": "In Proceedings of Sixth International Workshop on Statistical Relational AI at the 25th International Joint Conference on Artificial Intelligence (IJCAI)", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["dileep viswanathan", "ameet soni", "jude shavlik", "sriraam natarajan"], "accepted": false, "id": "1607.00424"}, "pdf": {"name": "1607.00424.pdf", "metadata": {"source": "CRF", "title": "Learning Relational Dependency Networks for Relation Extraction", "authors": ["Dileep Viswanathan", "Ameet Soni", "Jude Shavlik", "Sriraam Natarajan"], "emails": ["diviswan@indiana.edu", "soni@cs.swarthmore.edu", "shavlik@cs.wisc.edu", "natarasr@indiana.edu"], "sections": [{"heading": "Introduction", "text": "The problem of the Population Knowledge Base (KBP) - the construction of a Knowledge Base (KB) of facts derived from a large body of unstructured data - poses several challenges to the NLP community. Typically, this relation extraction task is divided into two subtasks - a linkage in which units are associated with already identified identities within the document or with units within the existing KB, and a slit fill that identifies certain attributes over a target object. We present our work-in-progress filling based on our probabilistic logistics formalisms and present the various components of the system. Specifically, we use Relational Dependency Networks (Neville and Jensen 2007), a formalism that is successfully used for common learning and inferences from stochastic, loud, relative data. We look at our RDN system against the current state of art for KBP to demonstrate the effectiveness of our probable relativities."}, {"heading": "Proposed Pipeline", "text": "We will present the various aspects of our pipeline that are illustrated in Figure 1. We will first describe our approach to generating features and training examples from the KBP corpus, before describing the core of our framework - the RDN boost algorithm."}, {"heading": "Feature Generation", "text": "Considering a training corpus of raw text documents, our learning algorithm first converts these documents into a set of facts (i.e., features) encoded in Logicar of the first order: 160 7.00 424v 1 [cs.A I] 1 July 201 6 (FOL). Raw text is processed using the Stanford CoreNLP Toolkit1 (Manning et al. 2014) to extract parts of words, word terminology, etc., and to generate trees, dependency diagrams, and name recognition information. A full set of extracted features is listed in Table 1. These are then converted into prolog (i.e. FOL) features and input into the system.In addition to the structured features from the output of the Stanford Toolkit, we also use deeper features based on word2vec (Mikolov et al. 2013) as input into our learning system."}, {"heading": "Weak Supervision", "text": "This year, it's so far that it will be able to retaliate, \"he said.\" It's not so far yet that we'll be able to do what we want to do, \"he said.\" But it's too early to do it, \"he said.\" But it's too early to do it. \""}, {"heading": "Learning Relational Dependency Networks", "text": "Previous research (Meza-Ruiz and Riedel 2009) has shown that common conclusions of relationships are more effective than looking at each relationship separately (Neville and Jensen 2007; Natarajan et al. 2010). RDNs extend dependency networks (DN) (Heckerman et al. 2001) to the relational environment. The key idea in a DN is to approximate the common distribution via a series of random variables as a product of their boundary distributions, i.e., P (y1, yn | X). It has been shown that the use of Gibbs sampling in the presence of a large amount of data makes this approach seem particularly effective."}, {"heading": "Incorporating Human Advice", "text": "While most relational learning methods confine people to the mere annotation of data, we go beyond that and ask people for advice. Intuition is that as human beings we read certain patterns and use them to derive the nature of the relationship between two entities present in the text. The aim of our work is to capture such human mental patterns as advice for the learning algorithm. We modified the work of Odom et al. (2015a; 2015b) to learn RDNs in the presence of advice. The basic idea is to explicitly present advice in the calculation of gradients, which allows the system to switch between data and advice throughout the learning phase, instead of considering advice only in initial iterations. In particular, advice becomes influential in the presence of loud or less amout of data. Some example rules in English (these are converted into a first-order logical format and are presented as input into our algorithm) in Table 3."}, {"heading": "Experiments and Results", "text": "We now present our experimental assessment. We have looked at 14 specific relationships from two categories, person and organization from the TAC KBP competition. The relationships we have looked at are in the left column of Table 4. We are using documents from the KBP 2014 for training, while using documents from the 2015 corpus for testing. All presented results were obtained from 5 different traction and test sets to provide more robust estimates of accuracy. We are looking at three standard ratios - range below the ROC curve, F-1 value and callback with some precision. We chose the precision of 0.66 because the fraction of the positive examples for negatives is 1: 2 (we highlighted the negative examples for the different training sets). Negative examples are scanned again for each training run. It should be noted that not all relationships had the same number of commented (gold standard) examples as the 781 documents we commented had different instances for these relationships."}, {"heading": "Weak Supervision", "text": "To answer Q1, we created positive training examples using the weak supervision techniques described above. Specifically, we evaluated 10 relationships as shown in Table 5. Based on experiments from (Soni et al. 2016), we used our knowledge-based weak supervision approach to provide positive examples in all but two of our relationships. 4 to 8 rules are derived for each relationship. Examples of the organizational relationships CountryHQ and FoundedBy were generated using standard remote monitoring techniques - freebase databases were mapped to FoundedBy, while Wikipedia info boxes provide entity pairs to CountryHQ. Finally, only 150 weakly monitored examples were used in our experiments (all gold standard examples were used). Carrying out larger editions is part of the work in progress. Results are presented in Table 5. We compared our standard pipeline (individually learned relationships with standard features only), which were used using gold standard examples, to compare the weak gold model with our superior model only."}, {"heading": "Joint learning", "text": "To answer our next question, we examined our pipeline when learning relationships independently (i.e. separately). Shared learning seems to help in about half of the relationships (8 / 14). Especially in the category of people, gold-standard joint learning outperforms their individual learning partners. This is due to the fact that some relationships such as parents, spouses, siblings, etc. are related to each other and joint learning actually improves them. Therefore, Q2 can be answered positively for half of the partners. word2vec Table 7 shows the results of experiments comparing the RDN framework with and without word2vec traits. For the most part, word2vec appears to have no impact, increasing the results in only 4 relationships. We suspect that this is due to a limitation in the depth of the learned trees."}, {"heading": "Advice", "text": "Table 8 shows the results of experiments that test the use of advice within the shared learning environment; the use of advice improves or equals the performance of shared learning only; the key effect of advice is mainly in improving memory in multiple relationships; this clearly shows that the use of human counselling patterns enables us to extract more relationships to effectively balance out loud or less number of training examples, in line with previously published machine learning literature (Towell and Shavlik 1994; Fung, Mangasarian and Shavlik 2002; Kunapuli et al. 2013; Odom et al. 2015b), in which humans can be more than mere laborers by providing useful advice on learning algorithms that can improve their performance; therefore, Q4 can be answered in the affirmative."}, {"heading": "RDN Boost vs Relation Factory", "text": "Relation Factory (RDN) (Roth et al. 2014) is an efficient open source system for the execution of relation extractions based on remote monitored classifiers. It was the top system in the TAC KBP 2013 competition (Surdeanu 2013) and therefore serves as a suitable starting point for our method. RF is very conservative in its responses, which makes it very difficult to adjust the precision levels. To be the most generous with RF, we present callback for all returned results (i.e. result > 0). The AUC ROC, callback and F1 values of our system against RF are shown in Table 9. Our system performs comparably and often better than the state-of-the-art Relation Factory system. In particular, our method performs better in all relationships than Relation Factory in AUC ROC. Recall presents a more mixed picture, with both approaches showing some improvements - RDN performs better in 6 relationships, while Relation Factory performs better in 8."}, {"heading": "Conclusion", "text": "We have shown that RDN is able to effectively learn the task of Relational Extraction, delivering comparable (and often better) performance than the state-of-the-art Relation Factory system. Furthermore, we have shown that RDN can integrate various concepts into a relational framework, including word2vec, human counseling, joint learning, and weak supervision. Some surprising results are that weak supervision and word2vec have not significantly improved performance. However, advice is extremely useful to validate the long-standing results within the Artificial Intelligence community for the task of relationship extraction as well. Possible future directions include considering a greater number of relationships, deeper characteristics, and finally comparisons with more systems. We believe that further work on developing word2vec characteristics and using weak supervision examples can provide further insights on how such characteristics can be used effectively in RDNs."}], "references": [{"title": "Top-down induction of first-order logical decision trees. Artificial intelligence 101(1):285\u2013297", "author": ["Blockeel", "H. Raedt 1998] Blockeel", "L.D. Raedt"], "venue": null, "citeRegEx": "Blockeel et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Blockeel et al\\.", "year": 1998}, {"title": "Toward an architecture for never-ending language learning", "author": ["Carlson"], "venue": null, "citeRegEx": "Carlson,? \\Q2010\\E", "shortCiteRegEx": "Carlson", "year": 2010}, {"title": "Markov Logic: An Interface Layer for AI", "author": ["Domingos", "P. Lowd 2009] Domingos", "D. Lowd"], "venue": null, "citeRegEx": "Domingos et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Domingos et al\\.", "year": 2009}, {"title": "Knowledge-Based support vector machine classifiers", "author": ["Mangasarian Fung", "G. Shavlik 2002] Fung", "O. Mangasarian", "J. Shavlik"], "venue": null, "citeRegEx": "Fung et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Fung et al\\.", "year": 2002}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["Heckerman"], "venue": "Journal of Machine Learning Research", "citeRegEx": "Heckerman,? \\Q2001\\E", "shortCiteRegEx": "Heckerman", "year": 2001}, {"title": "Guiding an autonomous agent to better behaviors through human advice", "author": ["Kunapuli"], "venue": "In ICDM", "citeRegEx": "Kunapuli,? \\Q2013\\E", "shortCiteRegEx": "Kunapuli", "year": 2013}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Manning"], "venue": "In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,", "citeRegEx": "Manning,? \\Q2014\\E", "shortCiteRegEx": "Manning", "year": 2014}, {"title": "Jointly identifying predicates, arguments and senses using markov logic", "author": ["Meza-Ruiz", "I. Riedel 2009] Meza-Ruiz", "S. Riedel"], "venue": "In Proceedings of NAACL HLT", "citeRegEx": "Meza.Ruiz et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Meza.Ruiz et al\\.", "year": 2009}, {"title": "Efficient estimation of word representations in vector space", "author": ["Mikolov"], "venue": "Proceedings of Workshop at ICLR", "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Yih Mikolov", "T. Zweig 2013] Mikolov", "W. Yih", "G. Zweig"], "venue": "Proceedings of NAACL HLT", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Boosting relational dependency networks", "author": ["Natarajan"], "venue": "In Proceedings of the International Conference on Inductive Logic Programming (ILP)", "citeRegEx": "Natarajan,? \\Q2010\\E", "shortCiteRegEx": "Natarajan", "year": 2010}, {"title": "Effectively creating weakly labeled training examples via approximate domain knowledge", "author": ["K. Kersting", "C. Re", "J. Shavlik"], "venue": "International Conference on Inductive Logic Programming.", "citeRegEx": "Kersting et al\\.,? 2014", "shortCiteRegEx": "Kersting et al\\.", "year": 2014}, {"title": "Relational dependency networks. In Introduction to Statistical Relational Learning", "author": ["Neville", "J. Jensen 2007] Neville", "D. Jensen"], "venue": null, "citeRegEx": "Neville et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Neville et al\\.", "year": 2007}, {"title": "Learning relational probability trees", "author": ["Neville"], "venue": "Proceedings of the ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD),", "citeRegEx": "Neville,? \\Q2003\\E", "shortCiteRegEx": "Neville", "year": 2003}, {"title": "Tuffy: Scaling up statistical inference in Markov logic networks using an RDBMS", "author": ["Niu"], "venue": "Proceedings of Very Large Data Bases (PVLDB)", "citeRegEx": "Niu,? \\Q2011\\E", "shortCiteRegEx": "Niu", "year": 2011}, {"title": "Extracting adverse drug events from text using human advice", "author": ["Odom"], "venue": "In Artificial Intelligence in Medicine (AIME)", "citeRegEx": "Odom,? \\Q2015\\E", "shortCiteRegEx": "Odom", "year": 2015}, {"title": "2015b. Knowledge-based probabilistic logic learning", "author": ["Odom"], "venue": "In Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI)", "citeRegEx": "Odom,? \\Q2015\\E", "shortCiteRegEx": "Odom", "year": 2015}, {"title": "Modeling relations and their mentions without labeled text. In Proceedings of the 2010 European conference on Machine learning and knowledge discovery in databases (ECML KDD)", "author": ["Yao Riedel", "S. McCallum 2010] Riedel", "L. Yao", "A. McCallum"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "Relationfactory: A fast, modular and effective system for knowledge base population", "author": ["Roth"], "venue": "In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,", "citeRegEx": "Roth,? \\Q2014\\E", "shortCiteRegEx": "Roth", "year": 2014}, {"title": "A comparison of weak supervision methods for knowledge base construction", "author": ["Soni"], "venue": "In 5th Workshop on Automated Knowledge Base Construction (AKBC) at NAACL", "citeRegEx": "Soni,? \\Q2016\\E", "shortCiteRegEx": "Soni", "year": 2016}, {"title": "Knowledge-based artificial neural networks. Artif", "author": ["Towell", "G. Shavlik 1994] Towell", "J. Shavlik"], "venue": null, "citeRegEx": "Towell et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Towell et al\\.", "year": 1994}, {"title": "Big data versus the crowd: Looking for relationships in all the right places", "author": ["Zhang"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers - Volume 1,", "citeRegEx": "Zhang,? \\Q2012\\E", "shortCiteRegEx": "Zhang", "year": 2012}], "referenceMentions": [{"referenceID": 9, "context": "In addition to the structured features from the output of Stanford toolkit, we also use deeper features based on word2vec (Mikolov et al. 2013) as input to our learning system.", "startOffset": 122, "endOffset": 143}, {"referenceID": 9, "context": "word2vec provide a continuous-space vector embedding of words that, in practice, capture many of these relationships (Mikolov et al. 2013; Mikolov, Yih, and Zweig 2013).", "startOffset": 117, "endOffset": 168}, {"referenceID": 19, "context": "Following our work in Soni et al. (2016), we employ two approaches for generating weakly supervised examples \u2013 distant supervision and knowledge-based weak supervision.", "startOffset": 22, "endOffset": 41}, {"referenceID": 15, "context": "Odom et al. (2015b) weigh the effect of the rules against the data and hence allow for partially correct rules.", "startOffset": 0, "endOffset": 20}], "year": 2016, "abstractText": "We consider the task of KBP slot filling \u2013 extracting relation information from newswire documents for knowledge base construction. We present our pipeline, which employs Relational Dependency Networks (RDNs) to learn linguistic patterns for relation extraction. Additionally, we demonstrate how several components such as weak supervision, word2vec features, joint learning and the use of human advice, can be incorporated in this relational framework. We evaluate the different components in the benchmark KBP 2015 task and show that RDNs effectively model a diverse set of features and perform competitively with current state-of-the-art relation extraction.", "creator": "LaTeX with hyperref package"}}}