{"id": "1607.00428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Jul-2016", "title": "Situated Structure Learning of a Bayesian Logic Network for Commonsense Reasoning", "abstract": "This paper details the implementation of an algorithm for automatically generating a high-level knowledge network to perform commonsense reasoning, specifically with the application of robotic task repair. The network is represented using a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz 2009), which combines a set of directed relations between abstract concepts, including IsA, AtLocation, HasProperty, and UsedFor, with a corresponding probability distribution that models the uncertainty inherent in these relations. Inference over this network enables reasoning over the abstract concepts in order to perform appropriate object substitution or to locate missing objects in the robot's environment. The structure of the network is generated by combining information from two existing knowledge sources: ConceptNet (Speer and Havasi 2012), and WordNet (Miller 1995). This is done in a \"situated\" manner by only including information relevant a given context. Results show that the generated network is able to accurately predict object categories, locations, properties, and affordances in three different household scenarios.", "histories": [["v1", "Fri, 1 Jul 2016 22:52:57 GMT  (315kb,D)", "http://arxiv.org/abs/1607.00428v1", "International Joint Conference on Artificial Intelligence (IJCAI), StarAI workshop"]], "COMMENTS": "International Joint Conference on Artificial Intelligence (IJCAI), StarAI workshop", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["haley garrison", "sonia chernova"], "accepted": false, "id": "1607.00428"}, "pdf": {"name": "1607.00428.pdf", "metadata": {"source": "CRF", "title": "Situated Structure Learning of a Bayesian Logic Network for Commonsense Reasoning", "authors": ["Haley Garrison", "Sonia Chernova"], "emails": ["hgarrison3@gatech.edu,", "chernova@cc.gatech.edu"], "sections": [{"heading": "Introduction", "text": "Imagine a world in which autonomous robots are available to ordinary people: you could go to the store, pick up a robot and place it at home. You could ask the robot to make dinner, do your laundry, or clean the house. However, for a robot to perform such high-level tasks in new or unsafe environments, it needs to be able to adapt the tasks it has learned to its local environment and fix missing information from the tasks. For example, a robot cooks a familiar recipe in a new kitchen. The cookware and other objects it originally used no longer exist. Instead, it needs to think about high-level concepts (e.g. pots and pans) that are suitable for the task (e.g. an object that can be used as a container), and find those objects in the new kitchen based on knowledge of their likely locations (e.g. pans that can be found in cabinets). For this kind of abstract reasoning, the robot needs to consult a general location."}, {"heading": "Related Work", "text": "Two existing common sense knowledge networks that are used for a variety of applications are WordNet (Miller 1995) and ConceptNet (Speer and Havasi 2012). WordNet consists of a collection of synsets that connect concepts hierarchically through the IsA relationship. WordNet also distinguishes between different senses of the same word and provides glosses or definitions for any sense. While WordNet is clean and hand-coded, it also lacks diversity in the types of relationships it contains. ConceptNet, on the other hand, contains a variety of different relationships, but it does not differentiate between word senses and it is not hand-coded, resulting in a large amount of noise.The next known work is the KnowRob Project (Tenorth and Beetz 2013). In this work, the authors created a knowledge network from a variety of encyclopedic sources and represented the network of prolog rules and web ontology."}, {"heading": "Bayesian Logic Networks", "text": "The knowledge generated by this work is represented by a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz 2009). BLN's are a type of controlled statistical relationship that serves as a template for a Bayesian Network, conceiving each node as a function / predicate with arguments rather than as a single random variable. Furthermore, BLN's logical constraints are allowed, which are presented as the first logical rules to be imposed on the network. A BLN is formally defined as a tuple, B = (D, F, L), so that: D = (T, E, T) is the declaration where T consists of the declared types, S is a set of function signatures, and E is a set of abstract entities where t: E \u2192 2T\\ {{{{M} {M} s is an illustration of each entity representing its respective activities."}, {"heading": "Network Representation", "text": "For the proposed knowledge network, the predicates, f, were selected to be boolean with the following signatures and associated parameter types in T: \u2022 IsA (object, concept) \u2022 HasProperty (object, property) \u2022 AtLocation (object, location) \u2022 UsedFor (object, affordability) The relationships between IsA, HasProperty and UsedFor were selected because they can be used to perform object substitution for plan repair by finding objects that are similar to the original object, or objects that can perform the same function as the missing object. The AtLocation relationship allows the robot to think about possible locations of objects to find missing objects. For each predicate, the \"object\" parameter will be a meta variable representing an earthed instance of an object that the robot can argue about, and the \"concept,\" \"\" property, \"\" location \"and\" affordability \"parameters will represent a probability that they represent an abstract the likelihood that they represent an element that they represent a probability, or the probability that some of the elements that they represent an abstract the probability that they are associated with the proposed knowledge network."}, {"heading": "Network Generation", "text": "For an overview of how to generate a network, see Figure 2. The dashed line shows the components that have been implemented as part of the network generation algorithm."}, {"heading": "Getting Seed Words", "text": "Before the network can be generated, a set of seed words must first be extracted. These seed words should be associated with the domain in which the robot is working, and could come from the robot's vision system (objects it sees in its environment) or from the task description. For testing purposes, a set of objects was extracted from three different household tasks and used as input into the network generation algorithm."}, {"heading": "Seed Word Disambiguation", "text": "After the seed words have been collected, they must be clearly determined in order to determine the contextually correct meaning of the words. For example, the word \"pan\" has the following four senses in WordNet: 1. pan, cooking pan - cooking utensil, consisting of wide metal vessels. Pan, goat god - (Greek Mythology) God of fields and woods and shepherds and flocks3. pan - shallow container made of metal4. Pan, genus Pan - chimpanzees; more closely related to Australopithecus than to other pongidsGiven a {senses) God of the fields and shepherds and shepherds. pan - shallow container made of metal4. Pan, genus Pan - chimpanzees; more closely related to Australopithecus than to other pongidsGiven a particular environment, not all of the above senses will be relevant contextually. In order to keep the size of the network small and contextually relevant, the networked senses can be accurately excluded from the contextually relevant meaning of the seeds."}, {"heading": "IsA Relation and Compression", "text": "Once the seed terms are clearly defined, the IsA relationship to the network is added by traversing the hypernym hierarchy from each of the unique seed terms to the root node, and each node along this path is added to the network. Although WordNet is manually encoded, it contains a large amount of redundant and high-level concepts that convey little information, as in Figure 3. If these nodes are not removed from the network, this can lead to a rapid expansion of the network size when other innovations are added. To reduce the size of the network to a manageable level and remove the high-level and redundant nodes, the compression strategy implemented in Stoica and Hearst 2004 is used. Compression uses the following three rules: 1. Eliminate selected (very general) categories such as abstraction, entity.2. Starting from the leaves, eliminate a parent who has fewer than n, except the parent."}, {"heading": "Adding ConceptNet Relations", "text": "In fact, it is in such a way that it is a way in which people are able to live in it, to live, to live, to live, to live, to live, to live, to live, to live, to work, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live,"}, {"heading": "Weight Learning", "text": "The final component of network generation is to learn the CPF for each fragment in the network by creating a set of training proofs with a probability corresponding to a linear combination of the weights assigned to each relationship in the ConceptNet and the ESA relationship measurement between the two concepts in the relationship. In future work, these proofs will be supplemented by evidence collected by the robot, but the creation of simulated proofs provides an initial estimate of the probabilities in the real world and enables the results to be ranked according to their relative probabilities. Once the evidence is collected, the CPFs can be learned with maximum probability by matching the frequency of each child node to each configuration of the parent nodes."}, {"heading": "Evaluation", "text": "To evaluate the network generation algorithm, three sets of seed outputs were considered either true or false. Each of the three scenarios involved cooking a recipe, washing laundry, and cleaning the house with 19, 15, and 11 seed words. An example of some of the relationships generated in each case is in Table 1. For each seed word, an inference was drawn across the network in which the evidence variable was IsA (object _ i, si) with a value set to true. Queries were then made for the variables IsA (object _ i, x), AtLocation (object _ i, x), HasProperty (object _ i, x), AtLocation (object _ i, x), AtLocation _ i (object _ i, and UsedFor (object _ i) for each seed word, and the output of the inference process was then compared to a gold standard."}, {"heading": "Future Work", "text": "One of the main objectives of future work is to carry out more comprehensive evaluations of the network generation algorithm, which could include the use of crowdsourcing to develop a gold standard that more accurately reflects the uncertainty of the relationships. In addition, the algorithm could be tested in several different areas to determine whether it goes beyond the budget scenarios presented in this document. Another future goal is to perform grounding to allow a robot to link the real objects it encounters in its environment to the abstract concepts beyond which it can execute inferences. Grounding the network will allow the robot to perform grounding on Source IsA AtLocation HasProperty UsedFor Recipe 97.6 86.8 82.0 88.1 Laundry 98.3 77.3 89.5 Cleaning 98.6 74.7 79.2 Table 2: Total inference accuracy (as a percentage) over the set of seeds from each source when compared with the gold standard."}], "references": [{"title": "and Liu", "author": ["J. Chen"], "venue": "J.", "citeRegEx": "Chen and Liu 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "and Chang", "author": ["R. Fung"], "venue": "K.-C.", "citeRegEx": "Fung and Chang 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Markovitch", "author": ["E. Gabrilovich"], "venue": "S.", "citeRegEx": "Gabrilovich and Markovitch 2007", "shortCiteRegEx": null, "year": 2007}, {"title": "and Geman", "author": ["S. Geman"], "venue": "D.", "citeRegEx": "Geman and Geman 1984", "shortCiteRegEx": null, "year": 1984}, {"title": "G", "author": ["Miller"], "venue": "A.", "citeRegEx": "Miller 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "and Domingos", "author": ["M. Richardson"], "venue": "P.", "citeRegEx": "Richardson and Domingos 2006", "shortCiteRegEx": null, "year": 2006}, {"title": "and Havasi", "author": ["R. Speer"], "venue": "C.", "citeRegEx": "Speer and Havasi 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "M", "author": ["E. Stoica", "Hearst"], "venue": "A.", "citeRegEx": "Stoica and Hearst 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "and Beetz", "author": ["M. Tenorth"], "venue": "M.", "citeRegEx": "Tenorth and Beetz 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Word Sense Disambiguation with Semantic Networks", "author": ["Varlamis Tsatsaronis", "I. Varlamis", "M. Vazirgiannis"], "venue": "Text, Speech, and Dialogue,", "citeRegEx": "Tsatsaronis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Tsatsaronis et al\\.", "year": 2008}, {"title": "and Palmer", "author": ["Z. Wu"], "venue": "M.", "citeRegEx": "Wu and Palmer 1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Reasoning About Object Affordances in a Knowledge Base Representation", "author": ["Fathi Zhu", "Y. Fei-Fei 2014] Zhu", "A. Fathi", "L. FeiFei"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Zhu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [], "year": 2016, "abstractText": "This paper details the implementation of an algorithm for automatically generating a high-level knowledge network to perform commonsense reasoning, specifically with the application of robotic task repair. The network is represented using a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz 2009), which combines a set of directed relations between abstract concepts, including IsA, AtLocation, HasProperty, and UsedFor, with a corresponding probability distribution that models the uncertainty inherent in these relations. Inference over this network enables reasoning over the abstract concepts in order to perform appropriate objectconcepts in order to perform appropriate object substitution or to locate missing objects in the robot\u2019s environment. The structure of the network is generated by combining information from two existing knowledge sources: ConceptNet (Speer and Havasi 2012), and WordNet (Miller 1995). This is done in a \"situated\" manner by only including information relevant a given context. Results show that the generated network is able to accurately predict object categories, locations, properties, and affordances in three different household", "creator": "LaTeX with hyperref package"}}}