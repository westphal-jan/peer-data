{"id": "1607.00534", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jul-2016", "title": "Text comparison using word vector representations and dimensionality reduction", "abstract": "This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (t-SNE) and how it can be implemented using Python. The technique provides a bird's-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like \"capital city of\". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn.", "histories": [["v1", "Sat, 2 Jul 2016 17:17:22 GMT  (1059kb,D)", "http://arxiv.org/abs/1607.00534v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hendrik heuer"], "accepted": false, "id": "1607.00534"}, "pdf": {"name": "1607.00534.pdf", "metadata": {"source": "CRF", "title": "Text comparison using word vector representations and dimensionality reduction", "authors": ["Hendrik Heuer"], "emails": ["hendrikh@kth.se"], "sections": [{"heading": null, "text": "Index Terms - Text Comparison, Topic Comparison, word2vec, t-SNE"}, {"heading": "1 INTRODUCTION", "text": "When summarizing a large text, only a subset of the available topics and stories can be considered. Deciding which topics to cover is largely editorial in nature. This paper introduces a tool that supports this editorial process with word vector representation and dimensionality reduction. It allows a user to visually identify matches and inconsistencies between two text sources. There are a variety of ways to approach the problem of visualizing the topics present in a text. The simplest approach is to look at unique words and their occurrences and visualize the words in a list. Topics could also be visualized with word clouds, where the font size of a word is determined by the frequency of the word. Word clouds have a variety of shortcomings: they can visualize only small subsets, they focus on the most common words and they do not take synonymous and semantically similar words in Account.This paper describes a human-computer interaction-inspired approach to compare two textual sources."}, {"heading": "2 DISTRIBUTIONAL SEMANTIC MODELS", "text": "The Harris distribution hypothesis states that words with similar meanings occur in similar contexts [Sah05], which implies that the meaning of a word can be derived from its distribution in different contexts. [Bru14] The goal of distribution semantics is to find a representation, e.g. a vector that approximates the meaning of a word [Bru14]. The traditional approach to statistical modeling of language is based on counting the frequencies of occurrence of short word sequences from length to N and using non-distributed representations [Cun15]. Distribution semantics takes word contexts into account in context windows. The general idea behind word space models is to use distributed vector spaces to generate high-dimensional vector spaces in which a word is represented by a context vector encoding semantic similarities [Sah05]. Representations are called distributed representations because the characteristics are not mutually exclusive and the data are seen in the contexts they are observed."}, {"heading": "2.1 word2vec", "text": "word2vec is a tool developed by Mikolov, Sutskever, Chen, Corrado and Dean at Google. Both model architectures of the C tool were provided under an open source license [Mik13]. Gensim offers a Python reimplementation of word2vec [R-eh10]. Word vectors encode semantic meaning and capture many different degrees of similarity [Lev14]. word2vec word vectors capture linguistic properties such as gender, tension, plurality and even semantic concepts such as \"is the capital of.\" word2vec captures domain similarity - while other more dependent approaches capture functional similarity. In the word2vec vector space, linear algebra can be used to exploit the encoded dimensions of similarity - word2vec captures domain similarity - while other more dependent approaches capture functional similarity. \"In the word2vec vector space, linear algebra can be used to exploit the encoded dimensions of similarity."}, {"heading": "3 DIMENSIONALITY REDUCTION WITH T-SNE", "text": "t-SNE is a method for reducing dimensionality that maintains the local structure of the data and helps to represent large real-world datasets with limited computing requirements [Maa08]. Vectors similar in high-dimensional vector space are represented by two- or three-dimensional vectors that are close together in two- or three-dimensional vector space. t-SNE is well suited for high-dimensional data that lie on several different but interconnected low-dimensional manifolds [Maa08]. t-SNE achieves this by minimizing the Kullback-Leibler divergence between the common probabilities of high-dimensional precipitation and low-dimensional representation. Kullbor-Glybor distribution. t-SNE measures the probability of divergence by minimizing the probability of stocking."}, {"heading": "4 IMPLEMENTATION", "text": "The text comparison tool implements a workflow that consists of a Python tool for the backend and a Javascript tool for the frontend. Python converts text into a collection of two-dimensional word vectors that are visualized using the Javascript frontend. Javascript frontend allows the user to explore and zoom in and out of the Word map to examine both the local and global structure of the text sources. Javascript frontend can be published online. The workflow of the tool comprises the following four steps:"}, {"heading": "4.1 Pre-processing", "text": "Tokenization is done with the Penn Treebank tokenizer, which is implemented in the Natural Language Processing Toolkit (NLTK) for Python [Bir09]. Alternatively, this can also be achieved with a regular expression. By means of a hash map, all words are counted. In order to reduce the size, only unique words, i.e. the keys of the hash map, are taken into account. The 3000 most common English words according to a frequency list from Wikipedia are ignored in order to reduce the amount of data."}, {"heading": "4.2 Word representations", "text": "Each word is represented by a N-dimensional vector (N = 300, informed by the best accuracy in [Mik13] and according to the default in [Che13]). From gensim.models import Word2Vecmodel = Word2Vec.load _ word2vec _ format (word _ vectors _ filename, binary = True) for word in word: if word in model: print model [word]"}, {"heading": "4.3 Dimensionality Reduction", "text": "The resulting N-dimensional word2vec vectors are projected down to 2D using the t-SNE Python implementation in scikitlearn [Ped11]. In the dimensionality reduction step, the N-dimensional word vectors are projected onto a two-dimensional space so that they can easily be visualized in a 2D coordinate system (see Fig. 2). For the implementation, the t-SNE implementation in scikitlearn is used: from sklearn.manifold import TSNEtsne = TSNE (n _ components = 2) tsne.fit _ transform (word _ vectors)"}, {"heading": "4.4 Visualization", "text": "After the dimension reduction, the vectors are exported to a JSON file and visualized with the JavaScript data visualization library D3.js [Bos12]. With the help of D3.js, an interactive map has been developed on which the user can move and zoom in. Color coding helps to assess the ratio of different and similar words. On a global scale, the map can be used to assess how similar two text sources are to each other. Clusters of similar words can be explored on a local scale."}, {"heading": "5 RESULTS", "text": "As with many unguarded methods, evaluation can be difficult and the quality of the visualization is difficult to quantify, so the goal of this section is to introduce relevant use cases and illustrate how the technique can be used. The flow described in the previous section can be applied to various revisions of Wikipedia articles, using a comfort sample of the most popular articles in 2013 from Wikipedia. For each article, the last revision was collected on December 31, 2013 and the most recent revision on May 26, 2015. Assuming that popular articles will attract enough changes to make interesting comparisons, the list of most popular Wikipedia articles includes Facebook, Game of Thrones, the United States and World War II. The article on Game of Thrones was considered particularly vivid for the task of comparing the topics in a text, as the storyline of the TV show was developed between the two different moments."}, {"heading": "6 CONCLUSION", "text": "Word2vec word vector representations and t-SNE dimensionality reduction can be used to provide a bird's eye view of various text sources, including text summaries and their source material, allowing users to explore a text source such as a geographic map. The paper provides an overview of an ongoing investigation into the usefulness of word vector representations and dimensionality reduction in the text and topic comparison context. The main flaw of this paper is that the introduced approach of text visualization and text comparison is not empirically validated. As many researchers publish their source code under open source licenses, and as the Python community accepts and supports these publications, it has been possible to integrate the results from the literature review of my master thesis into a usable tool. Distributed representations are an active field of research. New insights on word, sentence or paragraph vectors can be easily integrated into the workflow of the UFW tool by integrating them into the DA file as well as the end of the application."}], "references": [{"title": "Natural Language Processing with Python, 1st ed", "author": ["S. Bird", "E. Klein", "E. Loper"], "venue": null, "citeRegEx": "Bird et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Bird et al\\.", "year": 2009}, {"title": "D3.js - Data-Driven Documents", "author": ["M. Bostock"], "venue": null, "citeRegEx": "Bostock,? \\Q2012\\E", "shortCiteRegEx": "Bostock", "year": 2012}, {"title": "Multimodal Distributional Semantics,", "author": ["E. Bruni", "N.K. Tran", "M. Baroni"], "venue": "J. Artif. Int. Res.,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Dependency-Based Word Embeddings,\u201d in Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)", "author": ["O. Levy", "Y. Goldberg"], "venue": null, "citeRegEx": "Levy and Goldberg,? \\Q2014\\E", "shortCiteRegEx": "Levy and Goldberg", "year": 2014}, {"title": "Visualizing data using tSNE,", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Maaten and Hinton,? \\Q2008\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2008}, {"title": "Efficient Estimation of Word Representations in Vector Space,", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR, vol. abs/1301.3781,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "GloVe: Global Vectors for Word Representation,", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "An introduction to random indexing,\u201d in Methods and applications of semantic indexing workshop at the 7th international conference on terminology and knowledge engineering, TKE", "author": ["M. Sahlgren"], "venue": null, "citeRegEx": "Sahlgren,? \\Q2005\\E", "shortCiteRegEx": "Sahlgren", "year": 2005}, {"title": "Software Framework for Topic Modelling with Large Corpora,", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "\u0158eh\u016f\u0159ek and Sojka,? \\Q2010\\E", "shortCiteRegEx": "\u0158eh\u016f\u0159ek and Sojka", "year": 2010}], "referenceMentions": [], "year": 2016, "abstractText": "This paper describes a technique to compare large text sources using word vector representations (word2vec) and dimensionality reduction (tSNE) and how it can be implemented using Python. The technique provides a bird\u2019s-eye view of text sources, e.g. text summaries and their source material, and enables users to explore text sources like a geographical map. Word vector representations capture many linguistic properties such as gender, tense, plurality and even semantic concepts like \"capital city of\". Using dimensionality reduction, a 2D map can be computed where semantically similar words are close to each other. The technique uses the word2vec model from the gensim Python library and t-SNE from scikit-learn.", "creator": "LaTeX with hyperref package"}}}