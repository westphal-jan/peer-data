{"id": "1607.00570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Jul-2016", "title": "Representation learning for very short texts using weighted word embedding aggregation", "abstract": "Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box.", "histories": [["v1", "Sat, 2 Jul 2016 23:10:09 GMT  (473kb,D)", "http://arxiv.org/abs/1607.00570v1", "8 pages, 3 figures, 2 tables, appears in Pattern Recognition Letters"]], "COMMENTS": "8 pages, 3 figures, 2 tables, appears in Pattern Recognition Letters", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["cedric de boom", "steven van canneyt", "thomas demeester", "bart dhoedt"], "accepted": false, "id": "1607.00570"}, "pdf": {"name": "1607.00570.pdf", "metadata": {"source": "META", "title": "Representation learning for very short texts using weighted word embedding aggregation", "authors": ["Cedric De Booma", "Steven Van Canneyta", "Thomas Demeestera", "Bart Dhoedta"], "emails": ["cedric.deboom@ugent.be"], "sections": [{"heading": null, "text": "Traditional text representations such as tf-idf have difficulty grasping the semantic meaning of such texts, which is important for applications such as event detection, opinion polls, news recommendations, etc. We constructed a method based on semantic word embedding and frequency information to arrive at low-dimensional representations for short texts that are designed to capture semantic similarities. To this end, we developed a weight-based model and learning process based on a novel median-based loss function. This paper discusses the details of our model and optimization methods, along with the experimental results on both Wikipedia and Twitter data. We note that our method exceeds the basic approaches in the experiments and that it generalizes well to different word embedding without retraining. Therefore, our method is able to retain most of the semantic information in the text and is usable outside of box.2016 reserved rights."}, {"heading": "1. Introduction", "text": "It is therefore important that we recognise the similarity between the two levels. Millions of such short texts are sent every day, especially on social media, and it is fast becoming a daunting task to find similar messages among them, which lies at the heart of applications such as event detection. (De Boom et al.), most of the semantic information in this fragment is used by us.) In this paper we address the problem of finding an effective vector representation for a very short text fragment. We believe that representation should capture most of the semantic information in this fragment. (That is, we use semantic word embeddings to represent individual words, and we learn each word in the text through the use of tf-idf-idf frequency) information."}, {"heading": "2. Related work", "text": "In this context, it should also be mentioned that the two are a very rare group of people, but not a very rare group, who are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are not able to identify themselves. (...) Most of them are not able to identify themselves. (...)"}, {"heading": "3. Methodology", "text": "The core principle of our methodology is to assign a weight to each word in a short text. These weights are determined on the basis of the idf value of the individual words in that text. The idea is that important words - i.e. words needed to determine most of the semantics of the text - generally have higher idf values than less important words such as articles and auxiliary words. Indeed, the latter appear more frequently in different texts, while words with a high idf value mostly occur in similar contexts. The end goal is to combine the weighted words into a semantically effective, single text representation.To achieve this goal, we are modelling the problem of finding a suitable text representation as a semantic similarity task between text pairs of short texts. To classify such text fragments into semantically related pairs or unrelated pairs, the vector representations of these two texts are directly compared. In this paper, we use a simple threshold function for text pairs that are not related to either pairs of these pairs."}, {"heading": "3.1. Basic architecture", "text": "In order to learn these weights, we develop a model, which is shown in Figure 1. In the learning scheme, we use related and unrelated text pairs as input data. First, the words in each text are multiplied by a weight that can be learned. Finally, the weighted vectors are averaged to arrive at a single text representation. In detail, we look at a data set D, which consists of pairs of short texts. An arbitrary pair is denoted by C, and the two texts of C by C\u03b1 and C\u03b2. We show the vector that represents the word j by the text C\u03b1. All word vectors have the same dimensionality."}, {"heading": "3.2. Loss functions", "text": "As explained at the beginning of this section, we will instead describe two different functions as a regular function. At first, the functionality of this function is problematic because it comes close to each other in the representation space and unrelated texts are far apart from each other. We can visually examine the distribution of distances between each pair in the dataset. In fact, we calculate two distributions, one for the related pairs and one for the unrelated pairs. However, there is considerable overlap between the two distributions, which means that binary decisions on similarity based on a well-chosen distance threshold will result in a big error. The goal is to reduce this error and thus minimize the overlap between the two distributions."}, {"heading": "3.3. Texts with variable length", "text": "The method described so far is applicable only to short texts of fixed length, which is a limitation. In this section, we will extend our technique to texts with variable but maximum length. To this end, we have developed several approaches from which we will elaborate the text that performed best in the experiments.Suppose that all texts have a fixed maximum length of nmax. In the learning process, we will first learn the total number of nmax weights using the techniques described above. To find the weights for a text with the length m \u2264 nmax, we will use subsampling and linear interpolation. That is, for any text C ', we will first find the sequence of the real indices I (C', j), j."}, {"heading": "4. Data collection", "text": "In order to train the weights for each embed and conduct experiments, we collect data from various sources. First, we collect pairs of text from Wikipedia, which we use as a base data set to refine our methodology, and also use this data set to test our hypotheses and conduct initial experiments; second, we will use Twitter message pairs to show that our method can be used in a practical streaming context."}, {"heading": "4.1. Wikipedia", "text": "The main advantage of using Wikipedia is that there is a lot of well-structured data. We convert all letters into lowercase letters, and each number is replaced by a single character \"0\" (zero). Next, we construct related pairs of text, both of which have the same fixed length n. To do this, we take a Wikipedia article and extract n consecutive words from a paragraph. Then we skip two words, after which we extract the next n consecutive words, as long as they remain in the same paragraph. To extract unrelated pairs of text, we follow the same procedure, but we make sure that the two texts are from different articles that we randomly select. This approach is closely related to the data collection that is used in (Hu)."}, {"heading": "4.2. Twitter", "text": "Twitter is a very different type of medium from Wikipedia. Wikipedia articles are written in a formal register and consist largely of linguistically correct sentences. Twitter messages, on the other hand, do not exceed 140 characters and are full of misspellings, abbreviations and slang. We suggest that two tweets are semantically related when they are generated by the same event. As in (De Boom et al. (2015b), we require that such an event be represented by one or more hashtags. Since Twitter posts and related events are very noisy by nature, we limit ourselves to tweets from 100 English news agencies. We have created this list manually by checking their Twitter accounts; the list is available from our GitHub page, see Section 6.We collected 341,949 tweets from all news agencies via the Twitter REST API at the end of August 2015. We convert words to lowercase Jheuard, replacing all numbers with a single \"0\" and we will consider these tweets # informative and # hashtags to be transformed into four."}, {"heading": "5. Experiments", "text": "In this section, we will discuss the results of several experiments on all aspects of our methodology in light of the data we have collected. First, we will discuss some results using the Wikipedia dataset, after which we will also take a look at the Twitter dataset. We will use two performance metrics in our evaluation. First, the optimal split error, i.e. we will classify all pairs according to Equation (2) - after determining the optimal split point \u03b8 - and we will determine the classification error rate. A second performance metric is the Jensen-Shannon (JS) divergence. This is a symmetrical distance measurement between two probability distributions, based on the - known but asymmetrical - KL divergence. We will use it to capture the difference between the related and unrelated pairs \"distributions,\" as shown in Figure 2. The greater the JS divergence, the greater the difference between the two distributions, the greater the greater the difference between the two."}, {"heading": "5.1. Baselines", "text": "The simplest and most widely used baseline is a tf-idf vector. The comparison of two tf-idf vectors is done by a standard cosine-like similarity. In a second baseline, we simply take the mean for each dimension across all embedding. (15) In the third baseline, we replace the mean with a maximum operation. Taking a mean or maximum is a very common approach to combining word embedding - or other intermediate representations in an NLP architecture - into a sentence representation. We can also replace the maximum in the equation (15) with a minimum. The fourth baseline is a concatenation of the maximum and minimum vectors, which results in a vector having twice the original dimension (\"idleness rate\") and the maximum in the equation (15) with a minimum. \""}, {"heading": "5.2. Details on the learning procedure", "text": "In the following results, we use the methodology from Section 3 to learn weights for individual words in a short text. All procedures are implemented with the Theano2 framework and executed with an Nvidia-Tesla-K40 GPU. We use a stochastic gradient descent in the minibatch as a learning procedure and an L2 regularization on the weights. Thus, the total loss for a training part results in: Ltotal = Lbatch + \u03bb nmax = 1 w2j. (16) Here, we empirically set the parameter \u03bb to 0.001, andLbatch is, depending on the experiment, equal to either the contrastive or the median loss. The batch size corresponds to 100 text pairs, of which 50 are related pairs and 50 are unrelated pairs. An initial learning rate of 0.01 is used, which we immediately lower to 0.001 as soon as the average epoch-related loss begins to deteriorate. Then, we stop training when the losses are calculated by a value between the weights of 0.5% and less than 0.05%."}, {"heading": "5.3. Results on Wikipedia", "text": "rE \"s tis,\" so \u00fcrebtl\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc \u00fc the r the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f the f"}, {"heading": "5.4. Results on Twitter", "text": "We do not train additional word embeddings for Twitter, but we continue to use the Wikipedia embeddings as we have limited ourselves to tweets from news publishers that mainly use clean language. We also maintain the same tweeting setting as in the Wikipedia experiments. The results for the Twitter experiments are in Table 2. As expected, the error rate is quite high given the noise in the data set. We also note that the split error is slightly higher than the human error rate of 28%. Tf-idf performs worst in this experiment. Compared to Wikipedia, tfidf vectors for tweets are much more sparse, resulting in a higher error rate. Tf-idf is clearly not suitable for displaying tweets efficiently, whereas the baselines have a much better, but overall comparable performance. Our method of media-based data loss is the best."}, {"heading": "6. Conclusion", "text": "We developed an effective method for deriving vector representations for very short text fragments. To this end, we learned how to weight word embedding based on its idf value, using both a contrastive loss function and a novel median-based loss function that can effectively mitigate the effects of outliers. Our method is applicable to fixed-length texts, but can easily be extended to variable-length texts by subsampling and linear interpolation of learned weights. Our method can be applied out-of-the-box, meaning there is no need to retrain the model using different types of word embeds.We showed that our method outperforms widely used baselines, naively combining word embedding into text representation using both toy Wikipedia and real Twitter data.The entire code for this work is available on our hub site githubric.com / Representationboom / Learning."}, {"heading": "Acknowledgments", "text": "Cedric De Boom is funded by a PhD scholarship from the Flanders Research Foundation (FWO) and Steven Van Canneyt is funded by a PhD scholarship from the Agency for Innovation by Science and Technology in Flanders (IWT). We thank Nvidia for its generous hardware support."}], "references": [{"title": "The Evaluation of Sentence Similarity Measures, DaWaK", "author": ["P Achananuparp"], "venue": null, "citeRegEx": "Achananuparp,? \\Q2008\\E", "shortCiteRegEx": "Achananuparp", "year": 2008}, {"title": "Latent dirichlet allocation", "author": ["Blei", "D.M"], "venue": null, "citeRegEx": "Blei and D.M,? \\Q2003\\E", "shortCiteRegEx": "Blei and D.M", "year": 2003}, {"title": "Learning Semantic Similarity for Very Short Texts", "author": ["C JMLR. De Boom"], "venue": null, "citeRegEx": "Boom,? \\Q2015\\E", "shortCiteRegEx": "Boom", "year": 2015}, {"title": "2015b. Semantics-driven Event Clustering in Twitter", "author": ["C in: ICDMW. De Boom"], "venue": null, "citeRegEx": "Boom,? \\Q2015\\E", "shortCiteRegEx": "Boom", "year": 2015}, {"title": "Indexing by Latent Semantic Analysis. JASIS", "author": ["Feeds", "#MSM. Deerwester", "S.C"], "venue": "Hadsell, R., et al.,", "citeRegEx": "Feeds. et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Feeds. et al\\.", "year": 1990}, {"title": "Convolutional Neural Network Architectures for Matching", "author": ["Mapping", "B CVPR. Hu"], "venue": null, "citeRegEx": "Mapping. and Hu,? \\Q2014\\E", "shortCiteRegEx": "Mapping. and Hu", "year": 2014}, {"title": "Personalized News Recommendation", "author": ["S. Gauch"], "venue": null, "citeRegEx": "Jonnalagedda and Gauch,? \\Q2013\\E", "shortCiteRegEx": "Jonnalagedda and Gauch", "year": 2013}, {"title": "A Convolutional Neural Network for Modelling", "author": ["Twitter", "N WI-IAT. Kalchbrenner"], "venue": null, "citeRegEx": "Twitter and Kalchbrenner,? \\Q2014\\E", "shortCiteRegEx": "Twitter and Kalchbrenner", "year": 2014}, {"title": "A Short Texts Matching Method using Shallow", "author": ["Sentences", "L. ACL. Kang", "B Hu"], "venue": null, "citeRegEx": "Sentences et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sentences et al\\.", "year": 2014}, {"title": "Skip-Thought Vectors. arXiv.org arXiv:1506.06726v1", "author": ["R CIKM. Kiros"], "venue": "Kusner, M.J., et al.,", "citeRegEx": "Kiros,? \\Q2015\\E", "shortCiteRegEx": "Kiros", "year": 2015}, {"title": "Distributed Representations of Sentences", "author": ["Q.V. ICML. Le", "T. Mikolov"], "venue": null, "citeRegEx": "Le and Mikolov,? \\Q2014\\E", "shortCiteRegEx": "Le and Mikolov", "year": 2014}, {"title": "N-gram-Based Low-Dimensional Representa", "author": ["R. Collobert"], "venue": null, "citeRegEx": "Lebret and Collobert,? \\Q2015\\E", "shortCiteRegEx": "Lebret and Collobert", "year": 2015}, {"title": "Efficient Estimation of Word Representations in Vec", "author": ["T Press. Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "TagSpace: Semantic embeddings from hashtags", "author": ["works", "J NIPS. Weston"], "venue": null, "citeRegEx": "works and Weston,? \\Q2014\\E", "shortCiteRegEx": "works and Weston", "year": 2014}, {"title": "Convolutional Neural Network for Paraphrase", "author": ["W. NAACL. Yin", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Yin and Sch\u00fctze,? \\Q2015\\E", "shortCiteRegEx": "Yin and Sch\u00fctze", "year": 2015}], "referenceMentions": [{"referenceID": 2, "context": "Especially on social media, millions of such short texts are sent every day, and it quickly becomes a daunting task to find similar messages among them, which is at the core of applications such as event detection (De Boom et al. (2015b)), news recommendation (Jonnalagedda and Gauch (2013)), etc.", "startOffset": 218, "endOffset": 238}, {"referenceID": 2, "context": "Especially on social media, millions of such short texts are sent every day, and it quickly becomes a daunting task to find similar messages among them, which is at the core of applications such as event detection (De Boom et al. (2015b)), news recommendation (Jonnalagedda and Gauch (2013)), etc.", "startOffset": 218, "endOffset": 291}, {"referenceID": 0, "context": "Such an embedding is a distributed vector representation of a single word in a fixed-dimensional semantic space, as opposed to term tfidf vectors, in which a word is represented by a one-hot vector (Achananuparp et al. (2008); Manning et al.", "startOffset": 199, "endOffset": 226}, {"referenceID": 0, "context": "Such an embedding is a distributed vector representation of a single word in a fixed-dimensional semantic space, as opposed to term tfidf vectors, in which a word is represented by a one-hot vector (Achananuparp et al. (2008); Manning et al. (2009)).", "startOffset": 199, "endOffset": 249}, {"referenceID": 10, "context": "Although LSI and LDA have been used with success in the past, Skip-gram models have been shown to outperform them in various tasks (Mikolov et al. (2013); Lebret and Collobert (2015)).", "startOffset": 132, "endOffset": 154}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)).", "startOffset": 8, "endOffset": 36}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)). In Skip-gram, part of Google\u2019s word2vec toolkit1, distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window. Once the word embeddings are obtained, we have to combine them into a useful sentence representation. One possibility is to use an multilayer perceptron (MLP) with the whole sentence as an input, or a 1D convolutional neural network (Collobert et al. (2011); Hu et al.", "startOffset": 8, "endOffset": 476}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)). In Skip-gram, part of Google\u2019s word2vec toolkit1, distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window. Once the word embeddings are obtained, we have to combine them into a useful sentence representation. One possibility is to use an multilayer perceptron (MLP) with the whole sentence as an input, or a 1D convolutional neural network (Collobert et al. (2011); Hu et al. (2014); Xu et al.", "startOffset": 8, "endOffset": 494}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)). In Skip-gram, part of Google\u2019s word2vec toolkit1, distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window. Once the word embeddings are obtained, we have to combine them into a useful sentence representation. One possibility is to use an multilayer perceptron (MLP) with the whole sentence as an input, or a 1D convolutional neural network (Collobert et al. (2011); Hu et al. (2014); Xu et al. (2015); Johnson and Zhang (2015)).", "startOffset": 8, "endOffset": 512}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)). In Skip-gram, part of Google\u2019s word2vec toolkit1, distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window. Once the word embeddings are obtained, we have to combine them into a useful sentence representation. One possibility is to use an multilayer perceptron (MLP) with the whole sentence as an input, or a 1D convolutional neural network (Collobert et al. (2011); Hu et al. (2014); Xu et al. (2015); Johnson and Zhang (2015)).", "startOffset": 8, "endOffset": 538}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)). In Skip-gram, part of Google\u2019s word2vec toolkit1, distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window. Once the word embeddings are obtained, we have to combine them into a useful sentence representation. One possibility is to use an multilayer perceptron (MLP) with the whole sentence as an input, or a 1D convolutional neural network (Collobert et al. (2011); Hu et al. (2014); Xu et al. (2015); Johnson and Zhang (2015)). Such an approach, however, requires either an input of fixed length or aggregation operations \u2013 such as dynamic k-max pooling (Kalchbrenner et al. (2014)) \u2013 to arrive at a sentence representation that has the same dimensionality for every input.", "startOffset": 8, "endOffset": 694}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)). In Skip-gram, part of Google\u2019s word2vec toolkit1, distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window. Once the word embeddings are obtained, we have to combine them into a useful sentence representation. One possibility is to use an multilayer perceptron (MLP) with the whole sentence as an input, or a 1D convolutional neural network (Collobert et al. (2011); Hu et al. (2014); Xu et al. (2015); Johnson and Zhang (2015)). Such an approach, however, requires either an input of fixed length or aggregation operations \u2013 such as dynamic k-max pooling (Kalchbrenner et al. (2014)) \u2013 to arrive at a sentence representation that has the same dimensionality for every input. Recurrent neural networks (RNNs) and variants can overcome the problem of fixed dimensionality or aggregation, since one can feed word after word in the system and in the end arrive at a text representation (Sutskever et al. (2014); Sordoni et al.", "startOffset": 8, "endOffset": 1018}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)). In Skip-gram, part of Google\u2019s word2vec toolkit1, distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window. Once the word embeddings are obtained, we have to combine them into a useful sentence representation. One possibility is to use an multilayer perceptron (MLP) with the whole sentence as an input, or a 1D convolutional neural network (Collobert et al. (2011); Hu et al. (2014); Xu et al. (2015); Johnson and Zhang (2015)). Such an approach, however, requires either an input of fixed length or aggregation operations \u2013 such as dynamic k-max pooling (Kalchbrenner et al. (2014)) \u2013 to arrive at a sentence representation that has the same dimensionality for every input. Recurrent neural networks (RNNs) and variants can overcome the problem of fixed dimensionality or aggregation, since one can feed word after word in the system and in the end arrive at a text representation (Sutskever et al. (2014); Sordoni et al. (2015); Sundermeyer et al.", "startOffset": 8, "endOffset": 1041}, {"referenceID": 10, "context": "(2013); Lebret and Collobert (2015)). In Skip-gram, part of Google\u2019s word2vec toolkit1, distributed word embeddings are learned through a neural network architecture to predict its surrounding words in a fixed window. Once the word embeddings are obtained, we have to combine them into a useful sentence representation. One possibility is to use an multilayer perceptron (MLP) with the whole sentence as an input, or a 1D convolutional neural network (Collobert et al. (2011); Hu et al. (2014); Xu et al. (2015); Johnson and Zhang (2015)). Such an approach, however, requires either an input of fixed length or aggregation operations \u2013 such as dynamic k-max pooling (Kalchbrenner et al. (2014)) \u2013 to arrive at a sentence representation that has the same dimensionality for every input. Recurrent neural networks (RNNs) and variants can overcome the problem of fixed dimensionality or aggregation, since one can feed word after word in the system and in the end arrive at a text representation (Sutskever et al. (2014); Sordoni et al. (2015); Sundermeyer et al. (2015)).", "startOffset": 8, "endOffset": 1068}, {"referenceID": 9, "context": "The recently introduced Skip-thought vectors, heavily inspired on Skip-gram, combine the learning of word embeddings with the learning of a useful sentence representation using an RNN encoder and decoder (Kiros et al. (2015)).", "startOffset": 205, "endOffset": 225}, {"referenceID": 10, "context": "Paragraph2vec is another method, inspired by the Skip-gram algorithm, to derive sentence vectors (Le and Mikolov (2014)).", "startOffset": 98, "endOffset": 120}, {"referenceID": 14, "context": "(2014); dos Santos and Gatti (2014); Yin and Sch\u00fctze (2015); Collobert et al.", "startOffset": 37, "endOffset": 60}, {"referenceID": 14, "context": "(2014); dos Santos and Gatti (2014); Yin and Sch\u00fctze (2015); Collobert et al. (2011)).", "startOffset": 37, "endOffset": 85}, {"referenceID": 2, "context": "As in (De Boom et al. (2015b)), we require that such an event is represented by one or more hashtags.", "startOffset": 10, "endOffset": 30}, {"referenceID": 2, "context": "we will also use the Euclidean distance throughout our experiments here (De Boom et al. (2015a)).", "startOffset": 76, "endOffset": 96}], "year": 2016, "abstractText": "Short text messages such as tweets are very noisy and sparse in their use of vocabulary. Traditional textual representations, such as tf-idf, have difficulty grasping the semantic meaning of such texts, which is important in applications such as event detection, opinion mining, news recommendation, etc. We constructed a method based on semantic word embeddings and frequency information to arrive at low-dimensional representations for short texts designed to capture semantic similarity. For this purpose we designed a weight-based model and a learning procedure based on a novel median-based loss function. This paper discusses the details of our model and the optimization methods, together with the experimental results on both Wikipedia and Twitter data. We find that our method outperforms the baseline approaches in the experiments, and that it generalizes well on different word embeddings without retraining. Our method is therefore capable of retaining most of the semantic information in the text, and is applicable out-of-the-box. c \u00a9 2016 Elsevier Ltd. All rights reserved.", "creator": "LaTeX with hyperref package"}}}