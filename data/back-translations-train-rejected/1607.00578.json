{"id": "1607.00578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2016", "title": "Context-Dependent Word Representation for Neural Machine Translation", "abstract": "We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.", "histories": [["v1", "Sun, 3 Jul 2016 02:18:16 GMT  (139kb,D)", "http://arxiv.org/abs/1607.00578v1", "13 pages, 2 figures"]], "COMMENTS": "13 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["heeyoul choi", "kyunghyun cho", "yoshua bengio"], "accepted": false, "id": "1607.00578"}, "pdf": {"name": "1607.00578.pdf", "metadata": {"source": "CRF", "title": "Context-Dependent Word Representation for Neural Machine Translation", "authors": ["Heeyoul Choi", "Kyunghyun Cho"], "emails": ["heeyoul@gmail.com", "kyunghyun.cho@nyu.edu", "yoshua.bengio@umontreal.ca"], "sections": [{"heading": null, "text": "First, we observe a potential weakness in the continuous vector representation of symbols in neural machine translation. That is, the continuous vector representation or a vector for word embedding of a symbol encodes several dimensions of similarity, corresponding to the encoding of more than one meaning of the word. It follows that the encoder and decoder in neural machine translation must devote a significant portion of its capacity to the unique representation of source and target words based on the context defined by a source sentence. Based on this observation, we propose to contextualize the word with vectors that use a nonlinear word-string representation of the source set. Furthermore, we propose to represent special symbols (such as numbers, idioms, and acronyms) with typed symbols to facilitate the translation of those words that are not well suited to be translated via continuous vectors."}, {"heading": "1 Introduction", "text": "In fact, it is in such a way that it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about a way in which it is about a way and in which it is about which it is about a way in which it is about a way and in which it is about a way it is about a way and in which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about a way and in which it is about a way and in which it is about which it is about which it is about which it is about which it is about a way and in which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it is about which it"}, {"heading": "2 Background: Neural Machine Translation", "text": "In this section we give a brief overview of neural machine translation. Specifically, we describe the attention-based neural machine translation system Bahdanau et al. (2015), which will later be used in the experiments. (2014) However, the attention-based neural machine translation system calculates a conditional distribution over translations that are generally applicable to any other type of neural machine translation system, such as the sequence model Sutskever et al. (wx1, w x 2,.)., w x x x 2,.,., w x 2,.,.,.), carried out by a neural network consisting of an encoder, a decoder, and the attention mechanism. The encoder is often implemented as a bidirectional recurrent neural network that reads the source sentence word by word."}, {"heading": "3 Contextualized Word Embedding Vectors", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Word Embedding Vectors", "text": "The uniform representation of a word in equivalence (2) is unique in the sense that each word in a vocabulary is equally distant from any other word. This means that the words lose all information compared to the other words when presented as a hot vector. In other words, the meaning of a word, relative to those of the other words in the vocabulary, is thus learned through the associated word embedding vector (equivalent (1) - (4))) during the training. In other words, the training brings similar words close to each other in the word embedding space and dissimilar words far apart from each other. This phenomenon of learning similarity by means of word embedding vectors has been observed in many different tasks for processing natural language that are performed using neural networks. As early as 1991, Miicculains and Dyer Miicculains and Dyer (1991) noticed that the formation of a neural network using unified vectors as input vectors can form the most embedding elements of the word. \""}, {"heading": "3.2 Multiple Dimensions of Similarity", "text": "An important feature of high-dimensional word embedding of vectors is that it encodes multiple dimensions of similarities, which is necessary for a neural network to cope with polysemy. We can qualitatively verify this phenomenon of multi-dimensional embedding of similarities by examining a local diagram of the multiplicity on which the word embedding vectors.For each word that is under inspection, we now find the N \u2212 1 next adjacent diagram, centered around x \u2032, and we use the main component analysis (PCA) to find the corresponding low-dimensional euclidean space. The N-word embedding of vectors {x \u2032, x1,., xN \u2212 1} now characterizes a local diagram centered around x \u2032, and we use the main component analysis (PCA) to find the corresponding low-dimensional euclidean space. In this clidean area, we can inspect the closest neighbors along each coordinate."}, {"heading": "3.3 Contextualized Word Embeddings", "text": "The fact that every word that questions the vector represents several dimensions of similarity implies that a subsequent part of a neural network really needs to be disabled in order to disamuse the word based on the context in which the word was used. In the case of neural machine translation, we should indeed consider source and target words that embed vectors separately; the encoder implemented as a bi-directional recurrent network can disambiguate it by using all other words in a source set; on the other hand, the encoder can exploit both the previous words in a target sentence and all source words. As early as 1949, Weaver pointed out that much of the ambiguity in the meaning of the word can be resolved by taking surrounding words into account; the consequence of this neural machine translation is that the recurrent network, either in the previous one, or the decoder, decides to remember all of the words in question."}, {"heading": "4 Symbolization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Proper Nouns, Digits and Rare Words", "text": "The use of continuous vectors as an intermediate representation of source and target sentences in neural machine translation greatly improves the generalization of machine translation by avoiding the problem of data sparseness (see e.g. Sec. 5.2.1 in Cho (2015). However, this also entails some unnecessary complications, one such complication being the handling of correct nouns, numbers and rare words. First, the rare words, which often occur up to a handful of times in an entire training corpus, are problematic because their word vectors cannot be well embedded during training. Therefore, it is common practice to trace all these rare words, whose frequency is below a predefined threshold, to a single symbol representing an unknown word. This approach has been applied without much problem in language modeling, where the goal is to evaluate a particular sentence."}, {"heading": "4.2 Previous Approaches", "text": "In most of these specific cases, a source word is copied directly to a target sentence, either as it is, or after a simple dictionary search transformation. This property suggests a simple algorithm that can be executed outside of neural machine translation. Assuming that there is an alignment between a rare source word and a word or placeholder in the target sentence, we can look up the rare word in a pre-built dictionary and replace the target-side placeholder with the requested word, which may be the source word itself or its appropriate translation or translation. Based on this observation, Jean et al. (2015a) previously proposed a number of heuristics for dealing with rare words by attention-based neural machine translation. In their approach, attention weights are weighted on a word, t of eq. (3) are used to determine the source word aligned to each of the unknown characters."}, {"heading": "4.3 Symbolization of Proper Nouns, Digits and Rare Words", "text": "In this paper, the approach of Luong et al. (2015b), which is based on the unfinished, unfinished tokens to include several unfinished tokens, is expanded. < S > n: Proper noun3. < C > n: Acronyms The unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished, unfinished. \""}, {"heading": "5 Experimental Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Tasks and Corpora", "text": "We evaluate the neural machine translation systems with and without the proposed methods based on two tasks of translation from English to French (En-Fr) and English to German (En-De). We use all parallel corporas made publicly available via WMT '14.4. The corporas of En-Fr are cleaned according to the procedure in Cho et al. (2014b) and after tokenization have approximately 250 million words (English side). The corporas of En-De are prepared according to the procedure in Jean et al. (2015a), resulting in approximately 100 million words (English side). We use newstest-2014 and newtest-2013 as development sets and news discussion testers 2015 as test sets for En-Fr and En-De respectively."}, {"heading": "5.2 Vocabulary Preparation", "text": "Both corporas go through a minimal series of pre-processing processes. First, we tokenize them using the script provided as part of Moses.5 In comparison, we evaluate a setting that uses byte pair encoding (BPE) as a substitute for the proposed symbolization to extract subwords symbols Sennrich et al. (2015b).This has proven to be an effective and efficient approach to dealing with the problem of a large target vocabulary Sennrich et al. (2015a); Firat et al. (2016); Chung et al. (2016) and is able to transcribe rare, proper nouns to a certain degree Sennrich et al. (2015b). In all cases, we use up to 30k most common symbols (either tokenized words or BPE-based subwords). The proposed symbolization is applied immediately after initial tokenization."}, {"heading": "5.3 Training", "text": "We use the same attention-based neural translation model from Bahdanau et al. (2015). The only change we make is the use of short-term memory units (LSTM) instead of gated recurrent units (GRU). Furthermore, instead of Adadelta Zeiler (2012), we use Adam Kingma and Ba (2014) to adjust the adaptive learning rate with gradient cutout (threshold at 1.) Since the proposed contextualization and symbolization do not change the internals of the neural machine translation model, we use this model and the training configuration for all experiments. We remove each sentence pair if more than 10% and 30% of all words fall out of the vocabulary after the symbolization for En-Fr and En-De respectively. Furthermore, we only use sentence pairs, both sentences of which are only up to 50 symbols long."}, {"heading": "5.4 Evaluation", "text": "A simple forward search is used to find approximately the most likely translation taken from a source sentence of a trained model. We have set the width of the bar according to Bahdanau et al. (2015) to 12. The translations generated are compared with the reference translations made by Moses's \"multi-bleu.perl\" script."}, {"heading": "6 Result and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Quantitative Analysis", "text": "In Table 4, we present the translation qualities of all trained models, as measured by BLEU on the test kits, on both En-Fr and En-De. The most striking observation is that both proposed methods - contextualization and symbolization - improve the translation quality compared to the base model, which is a vanilla-based neural translation system (+ context and + symbol). In addition, the proposed contextualizations and symbolizations complement each other, and the greatest improvement is achieved when used together (baseline + context + symbol.) As expected, the proposed symbolization has a similar effect to the BPE segmentation (baseline + symbol vs. baseline + BPE) on both language pairs. We note that the proposed contextualization, which proved to be complementary to symbolization, improves the translation quality, even when the BPE-based subsymbols are used. 6 ps: crihttps / scripts / scripts / githmosper.com / multi-depgents /"}, {"heading": "6.2 Effect of Contextualization", "text": "We suspected earlier in paragraph 3.3 that contextualizing the translation helps by selectively hiding some dimensions of word meaning based on context, which directly implies that the neural translation system is less confused with contextualization among many similar words. Leafing through the translations of the source sentences contained in the development and test sentences, we have observed many such cases, and the following is an example: The Scorsese-DiCaprio duo seems to have rediscovered the magic that brought them together in Shutter Island. The above sentence is translated by the neural translation system using BPE in"}, {"heading": "Le duo de Corsso-DiCaprio semble avoir retrouve\u0301 la magie qui les a re\u0301unis dans l\u2019\u0131\u0302le Shuttle .", "text": "The same sentence has since been translated into"}, {"heading": "Le duo Scorson-DiCaprio semble avoir rede\u0301couvert la magie qui les rassemblait dans l&apos; \u0131\u0302le de Shutter .", "text": "Without contextualization, the rediscovered word has been translated by the contextualized model closer to the original English word \"rede\" (\"rediscovered\" from the same source8), which is not an incorrect translation example for the source-side context, but the translation of that word into \"rede\" couvert \"(\" rediscovered \"from the same source8).Let's give another example in the case of En-De. The following source set includes 13,435 cumulative cases (12,158 AIDS and 1,317 HIV).Translated by the neural translation model with BPE intoThere are 13,335 accumulated cases (12,158 AIdg and 1,317 HIV).The same sentence is translated by the same model with the contextualization intoEs netoEs."}, {"heading": "6.3 Effect of Symbolization", "text": "As mentioned briefly in Section 5.2, Neural Machine Translation transliterates rare (eigen-) nouns with BPE-based subword symbols. However, this is not perfect, as the rules for transliteration have many exceptions and the statistical generalization often does not take into account rare eigenword symbols. Thus, for example, the following sentence Trachtenberg moderated many programs before Hali-Gali times."}, {"heading": "7 http://www.collinsdictionary.com/dictionary/french-english/retrouver", "text": "8 http: / / www.collinsdictionary.com / dictionary / french-english / red% C3% A9couvrirTrakttenberg was the moderator of many programs prior to Hali-Gi Male.It is easy to notice that the model failed to correctly transcribe \"Trakhtenberg.\" On the contrary, if the symbolization is used (along with the contextualization), the model translates the source set via intoTrakhtenberg was the moderator of many programs prior to Hali-Gali - Tenses. A similar behavior can be observed with \"Hali-Gali.\" This example clearly shows the effectiveness of the proposed symbolization, even if the BPE-based subword symbols are used. The advantage of the symbolization becomes clearer when words are used as basic symbols rather than as BPE-based subword symbols. The same source set is translated in UNK was the moderator of many programs prior to UNK."}, {"heading": "7 Conclusions", "text": "Starting from the observation that a word embedding a vector encodes several dimensions of similarities, we proposed to contextualize it by adaptive masking each dimension of the source and target embedding of vectors based on the context of a source sentence. In addition to contextualizing the word embedding vectors, we also propose to symbolize special tokens to facilitate the translation of those tokens that are not well suited for translation using continuous vectors. Experiments on En-Fr and En-De showed that both contextualization and symbolization techniques improve the translation quality of neural machine translation. Furthermore, we confirmed that these approaches are agnostic to represent linguistic symbols and propose objectives by using byte words at the level of source coding and goal coding at the level of PE-2016 (we propose the target coding at the level, which is particularly interesting)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of Theano Bastien et al. (2012) for their support of the following research funding and computer support agencies: NSERC, Calcul Que'bec, Compute Canada, the Canada Research Chairs, CIFAR and Samsung. KC would like to thank Facebook and Google for their support (Google Faculty Award 2016)."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In Proc. Int\u2019l Conf. on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A Neural Probabilistic Language Model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Natural language understanding with distributed representation. arXiv preprint arXiv:1511.07916", "author": ["K. Cho"], "venue": null, "citeRegEx": "Cho,? \\Q2015\\E", "shortCiteRegEx": "Cho", "year": 2015}, {"title": "2014a: On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "2014b: Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Conference on Empirical Methods in Natural Language Processing (EMNLP", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "2016: A character-level decoder without explicit segmentation for neural machine translation", "author": ["J. Chung", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.06147", "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "2016: Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["O. Firat", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1601.01073", "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Learning to understand phrases by embedding the dictionary", "author": ["F. Hill", "K. Cho", "A. Korhonen", "Y. Bengio"], "venue": "arXiv preprint arXiv:1504.00548", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "2015a: On Using Very Large Target Vocabulary for Neural Machine Translation", "author": ["S. Jean", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "In 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "2015b: Montreal neural machine translation systems for wmt15", "author": ["S. Jean", "O. Firat", "K. Cho", "R. Memisevic", "Y. Bengio"], "venue": "In Proceedings of the Tenth Workshop on Statistical Machine Translation,", "citeRegEx": "Jean et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2015}, {"title": "Recurrent continuous translation", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "models. EMNLP,", "citeRegEx": "Kalchbrenner and Blunsom,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner and Blunsom", "year": 2013}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["D.P. Kingma", "J.L. Ba"], "venue": null, "citeRegEx": "Kingma and Ba,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba", "year": 2014}, {"title": "Statistical Machine Translation", "author": ["P. Koehn"], "venue": "Statistical Machine Translation,", "citeRegEx": "Koehn,? \\Q2010\\E", "shortCiteRegEx": "Koehn", "year": 2010}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume", "citeRegEx": "Koehn et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Character-based neural machine translation", "author": ["W. Ling", "I. Trancoso", "C. Dyer", "A.W. Black"], "venue": "arXiv preprint arXiv:1511.04586", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "2015a: Effective approaches to attention-based neural machine translation", "author": ["Luong", "M.-T", "H. Pham", "C.D. Manning"], "venue": "arXiv preprint arXiv:1508.04025", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "2015b: Addressing the Rare Word Problem in Neural Machine Translation", "author": ["Luong", "M.-T", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "arXiv preprint arXiv:1410.8206", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Natural language processing with modular neural networks and distributed lexicon", "author": ["R. Miikkulainen", "M.G. Dyer"], "venue": "Cognitive Science,", "citeRegEx": "Miikkulainen and Dyer,? \\Q1991\\E", "shortCiteRegEx": "Miikkulainen and Dyer", "year": 1991}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "G. Corrado", "K. Chen", "J. Dean"], "venue": "In Proc. Int\u2019l Conf. on Learning Representations (ICLR)", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "2015a: Improving neural machine translation models with monolingual data", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Minimum Risk Training for Neural Machine Translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "arXiv preprint arXiv:1512.02433", "citeRegEx": "Shen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Word representations: a simple and general method for semi-supervised learning. In Proceedings of the 48th annual meeting of the association for computational linguistics, Association for Computational Linguistics, pp. 384\u2013394", "author": ["J. Turian", "L. Ratinov", "Y. Bengio"], "venue": null, "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Visualizing non-metric similarities in multiple maps", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Machine learning,", "citeRegEx": "Maaten and Hinton,? \\Q2012\\E", "shortCiteRegEx": "Maaten and Hinton", "year": 2012}, {"title": "Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701", "author": ["M.D. Zeiler"], "venue": null, "citeRegEx": "Zeiler,? \\Q2012\\E", "shortCiteRegEx": "Zeiler", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "Neural machine translation is a recently proposed paradigm in machine translation, which is often entirely built as a single neural network Kalchbrenner and Blunsom (2013); Sutskever et al.", "startOffset": 140, "endOffset": 172}, {"referenceID": 12, "context": "Neural machine translation is a recently proposed paradigm in machine translation, which is often entirely built as a single neural network Kalchbrenner and Blunsom (2013); Sutskever et al. (2014); Bahdanau et al.", "startOffset": 140, "endOffset": 197}, {"referenceID": 0, "context": "(2014); Bahdanau et al. (2015). The neural machine translation system, which often consists of an encoder and decoder, projects and manipulates a source sequence of discrete linguistic symbols (source sentence) in a continuous vector space, and decodes a target sequence of symbols (target sentence or translation.", "startOffset": 8, "endOffset": 31}, {"referenceID": 0, "context": "(2014); Bahdanau et al. (2015). The neural machine translation system, which often consists of an encoder and decoder, projects and manipulates a source sequence of discrete linguistic symbols (source sentence) in a continuous vector space, and decodes a target sequence of symbols (target sentence or translation.) This is contrary to the conventional machine translation systems, such as phrase-based statistical machine translation Koehn et al. (2003), which work directly at the discrete symbol level.", "startOffset": 8, "endOffset": 455}, {"referenceID": 19, "context": "The encoder network, which is often implemented as a recurrent neural network, encodes this source sentence either into a single context vector Sutskever et al. (2014); Cho et al.", "startOffset": 144, "endOffset": 168}, {"referenceID": 2, "context": "(2014); Cho et al. (2014b) or into a sequence of context vectors Kalchbrenner and Blunsom (2013); Bahdanau et al.", "startOffset": 8, "endOffset": 27}, {"referenceID": 2, "context": "(2014); Cho et al. (2014b) or into a sequence of context vectors Kalchbrenner and Blunsom (2013); Bahdanau et al.", "startOffset": 8, "endOffset": 97}, {"referenceID": 0, "context": "(2014b) or into a sequence of context vectors Kalchbrenner and Blunsom (2013); Bahdanau et al. (2015).", "startOffset": 79, "endOffset": 102}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard.", "startOffset": 85, "endOffset": 108}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al.", "startOffset": 85, "endOffset": 309}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al.", "startOffset": 85, "endOffset": 360}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al. (2015b), and En-Zh Shen et al.", "startOffset": 85, "endOffset": 387}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al. (2015b), and En-Zh Shen et al. (2015). Much of these recent improvements have been made by tackling, e.", "startOffset": 85, "endOffset": 417}, {"referenceID": 0, "context": "Among different variants of neural machine translation, the attention-based approach Bahdanau et al. (2015) has recently become de facto standard. It has been found to perform comparably to or better than the existing phrase-based statistical systems in many language pairs including EnFr Jean et al. (2015a), En-De Jean et al. (2015a,b); Luong et al. (2015a), En-Cs Jean et al. (2015b), and En-Zh Shen et al. (2015). Much of these recent improvements have been made by tackling, e.g., the attention mechanism (which is central to the attention-based neural translation system) and the computational issues arising from having a large target vocabulary. Unlike these recent works, we focus on source- and target-side word embedding vectors in this paper. More specifically, we first notice that the transformation from and to high-dimensional word embedding vectors is done for each word largely independent of each other. We conjecture that only a few axes in this high-dimensional space are relevant given a source sentence and that we can remove much of the ambiguity in the choice of words by restricting, or turning off, most of the irrelevant dimensions. We propose to achieve this automated way to turn off some dimensions of word embeddings by contextualizing a word embedding vector. In addition to the proposed contextualization of both source and target word embedding vectors, we propose to extend the unknown token replacement technique proposed in Luong et al. (2015b) to multiple token types.", "startOffset": 85, "endOffset": 1483}, {"referenceID": 0, "context": "More specifically, we describe the attention-based neural machine translation Bahdanau et al. (2015) which will be used in the experiments later.", "startOffset": 78, "endOffset": 101}, {"referenceID": 0, "context": "More specifically, we describe the attention-based neural machine translation Bahdanau et al. (2015) which will be used in the experiments later. However, we note that the proposed contextualization and symbolization techniques are generally applicable to any other type of neural machine translation systems such as the sequence-to-sequence model Sutskever et al. (2014).", "startOffset": 78, "endOffset": 372}, {"referenceID": 7, "context": "The recurrent activation functions \u2212 \u2192 \u03c6 and \u2190\u2212 \u03c6 are in most cases either long short-term memory units (LSTM, Hochreiter and Schmidhuber (1997)) or gated recurrent units (GRU, Cho et al.", "startOffset": 111, "endOffset": 145}, {"referenceID": 3, "context": "The recurrent activation functions \u2212 \u2192 \u03c6 and \u2190\u2212 \u03c6 are in most cases either long short-term memory units (LSTM, Hochreiter and Schmidhuber (1997)) or gated recurrent units (GRU, Cho et al. (2014a).) The decoder consists of two sub-components\u2013a recurrent network and the attention mechanism.", "startOffset": 177, "endOffset": 196}, {"referenceID": 18, "context": "1 For more variants of the attention mechanism, we refer the readers to Luong et al. (2015a).", "startOffset": 72, "endOffset": 93}, {"referenceID": 18, "context": "Already in 1991, Miikkulainen and Dyer Miikkulainen and Dyer (1991) noticed that training a neural network with one-hot vectors as its input learns the word embedding vectors that \u201ccode properties of the input elements that are most crucial to the task.", "startOffset": 17, "endOffset": 68}, {"referenceID": 1, "context": "\u201d Based on this observation Bengio et al. Bengio et al. (2003) proposed to build a neural network based language model and found that it generalizes better to unseen or rare n-grams: the word embedding vectors capture similarities between words and the neural network can learn a smooth mapping that automatically generalizes by producing similar outputs for semantically similar input sequences.", "startOffset": 28, "endOffset": 63}, {"referenceID": 1, "context": "\u201d Based on this observation Bengio et al. Bengio et al. (2003) proposed to build a neural network based language model and found that it generalizes better to unseen or rare n-grams: the word embedding vectors capture similarities between words and the neural network can learn a smooth mapping that automatically generalizes by producing similar outputs for semantically similar input sequences. The interest in word embedding vectors, or distributed representation of words, was fueled by the earlier observations that these unsupervised word embedding vectors can be used to improve supervised natural language tasks greatly Collobert et al. (2011); Turian et al.", "startOffset": 28, "endOffset": 652}, {"referenceID": 1, "context": "\u201d Based on this observation Bengio et al. Bengio et al. (2003) proposed to build a neural network based language model and found that it generalizes better to unseen or rare n-grams: the word embedding vectors capture similarities between words and the neural network can learn a smooth mapping that automatically generalizes by producing similar outputs for semantically similar input sequences. The interest in word embedding vectors, or distributed representation of words, was fueled by the earlier observations that these unsupervised word embedding vectors can be used to improve supervised natural language tasks greatly Collobert et al. (2011); Turian et al. (2010).", "startOffset": 28, "endOffset": 674}, {"referenceID": 21, "context": "2 In Table 1, we show two such examples using the word embedding vectors trained as a part of the continuous-bag-of-word (CBoW) network Mikolov et al. (2013).3 These examples clearly show that each word embedding vector encodes more than one notions of similarities.", "startOffset": 136, "endOffset": 158}, {"referenceID": 21, "context": "2 In Table 1, we show two such examples using the word embedding vectors trained as a part of the continuous-bag-of-word (CBoW) network Mikolov et al. (2013).3 These examples clearly show that each word embedding vector encodes more than one notions of similarities. A similar behaviour can only be observed with multi-map t-SNE Van der Maaten and Hinton (2012).", "startOffset": 136, "endOffset": 362}, {"referenceID": 9, "context": "3 We used the word embedding vectors provided as a part of Hill et al. (2015).", "startOffset": 59, "endOffset": 78}, {"referenceID": 3, "context": "1 in Cho (2015).) This, however, brings in some unnecessary complications as well.", "startOffset": 5, "endOffset": 16}, {"referenceID": 11, "context": "Based on this observation, Jean et al. Jean et al. (2015a) earlier proposed a number of heuristics for handling rare words with the attention-based neural machine translation.", "startOffset": 27, "endOffset": 59}, {"referenceID": 17, "context": "Simultaneously, Luong et al. Luong et al. (2015b) proposed another mechanism that does not require the attention mechanism.", "startOffset": 16, "endOffset": 50}, {"referenceID": 2, "context": "They used an external alignment mechanism, such as IBM Model 2 Brown et al. (1993), to find the alignment between the source and target words in the training corpus.", "startOffset": 63, "endOffset": 83}, {"referenceID": 11, "context": "For instance, Jean et al. Jean et al. (2015a) reported +3 and +2.", "startOffset": 14, "endOffset": 46}, {"referenceID": 11, "context": "For instance, Jean et al. Jean et al. (2015a) reported +3 and +2.5 BLEU improvement with the simple replacement technique based on the attention mechanism on En-Fr and En-De, respectively. Similarly, Luong et al. Luong et al. (2015b) reported +1.", "startOffset": 14, "endOffset": 234}, {"referenceID": 18, "context": "In this paper, we extend the approach by Luong et al. Luong et al. (2015b), which is based on the positional unknown tokens, to include multiple positional special tokens.", "startOffset": 41, "endOffset": 75}, {"referenceID": 15, "context": "4 of Koehn (2010). This approach however has not been adopted widely in neural machine translation yet.", "startOffset": 5, "endOffset": 18}, {"referenceID": 3, "context": "4 The En-Fr corpora are cleaned following the procedure in Cho et al. (2014b), and after tokenization, has approximately 250M words (English side.", "startOffset": 59, "endOffset": 78}, {"referenceID": 3, "context": "4 The En-Fr corpora are cleaned following the procedure in Cho et al. (2014b), and after tokenization, has approximately 250M words (English side.) The En-De corpora are prepared following the procedure in Jean et al. (2015a), resulting in approximately 100M words (English side.", "startOffset": 59, "endOffset": 226}, {"referenceID": 20, "context": "5 As a comparison, we evaluate a setting where byte pair encoding (BPE) is used, as a replacement of the proposed symbolization, to extract sub-word symbols Sennrich et al. (2015b). This has been found to be an effective and efficient approach to handling the issue of a large target vocabulary Sennrich et al.", "startOffset": 157, "endOffset": 181}, {"referenceID": 20, "context": "5 As a comparison, we evaluate a setting where byte pair encoding (BPE) is used, as a replacement of the proposed symbolization, to extract sub-word symbols Sennrich et al. (2015b). This has been found to be an effective and efficient approach to handling the issue of a large target vocabulary Sennrich et al. (2015a); Firat et al.", "startOffset": 157, "endOffset": 319}, {"referenceID": 7, "context": "(2015a); Firat et al. (2016); Chung et al.", "startOffset": 9, "endOffset": 29}, {"referenceID": 6, "context": "(2016); Chung et al. (2016) and is known to be able to transliterate rare, proper nouns up to a certain extent Sennrich et al.", "startOffset": 8, "endOffset": 28}, {"referenceID": 6, "context": "(2016); Chung et al. (2016) and is known to be able to transliterate rare, proper nouns up to a certain extent Sennrich et al. (2015b). In all the cases, we use up to top-30k most frequent symbols (either tokenized words or BPE-based sub-words.", "startOffset": 8, "endOffset": 135}, {"referenceID": 0, "context": "We use the very same attention-based neural translation model from Bahdanau et al. (2015). The only change we make is to use long short-term memory units (LSTM) instead of gated recurrent units (GRU).", "startOffset": 67, "endOffset": 90}, {"referenceID": 0, "context": "We use the very same attention-based neural translation model from Bahdanau et al. (2015). The only change we make is to use long short-term memory units (LSTM) instead of gated recurrent units (GRU). Further, instead of Adadelta Zeiler (2012), we use Adam Kingma and Ba (2014) for adaptive learning rate adjustment with gradient clipping (threshold at 1.", "startOffset": 67, "endOffset": 244}, {"referenceID": 0, "context": "We use the very same attention-based neural translation model from Bahdanau et al. (2015). The only change we make is to use long short-term memory units (LSTM) instead of gated recurrent units (GRU). Further, instead of Adadelta Zeiler (2012), we use Adam Kingma and Ba (2014) for adaptive learning rate adjustment with gradient clipping (threshold at 1.", "startOffset": 67, "endOffset": 278}, {"referenceID": 0, "context": "We set the width of the beam to 12 following Bahdanau et al. (2015). The generated translations are scored against the reference translations using \u2018multi-bleu.", "startOffset": 45, "endOffset": 68}, {"referenceID": 16, "context": "Especially, the proposed contextualization was found to be complementary to the use of BPE-based subword symbols, and we find it an interesting future work to test the contextualization with character-level neural machine translation Ling et al. (2015); Chung et al.", "startOffset": 234, "endOffset": 253}, {"referenceID": 6, "context": "(2015); Chung et al. (2016).", "startOffset": 8, "endOffset": 28}], "year": 2016, "abstractText": "We first observe a potential weakness of continuous vector representations of symbols in neural machine translation. That is, the continuous vector representation, or a word embedding vector, of a symbol encodes multiple dimensions of similarity, equivalent to encoding more than one meaning of the word. This has the consequence that the encoder and decoder recurrent networks in neural machine translation need to spend substantial amount of their capacity in disambiguating source and target words based on the context which is defined by a source sentence. Based on this observation, in this paper we propose to contextualize the word embedding vectors using a nonlinear bag-of-words representation of the source sentence. Additionally, we propose to represent special tokens (such as numbers, proper nouns and acronyms) with typed symbols to facilitate translating those words that are not well-suited to be translated via continuous vectors. The experiments on En-Fr and En-De reveal that the proposed approaches of contextualization and symbolization improves the translation quality of neural machine translation systems significantly.", "creator": "LaTeX with hyperref package"}}}