{"id": "1607.00623", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2016", "title": "Visualizing Natural Language Descriptions: A Survey", "abstract": "A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. One of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation. This survey discusses requirements and challenges of developing such systems and reports 26 graphical systems that exploit natural language interfaces and addresses both artificial intelligence and visualization aspects. This work serves as a frame of reference to researchers and to enable further advances in the field.", "histories": [["v1", "Sun, 3 Jul 2016 10:30:40 GMT  (284kb,D)", "http://arxiv.org/abs/1607.00623v1", "Due to copyright most of the figures only appear in the journal version"]], "COMMENTS": "Due to copyright most of the figures only appear in the journal version", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.CV cs.GR cs.HC", "authors": ["kaveh hassani", "won-sook lee"], "accepted": false, "id": "1607.00623"}, "pdf": {"name": "1607.00623.pdf", "metadata": {"source": "CRF", "title": "Visualizing Natural Language Descriptions: A Survey", "authors": ["Kaveh Hassani", "Won-Sook Lee"], "emails": ["kaveh.hassani@uottawa.ca;", "wslee@uottawa.ca"], "sections": [{"heading": "1 Introduction", "text": "In fact, most people who are able to survive themselves are also able to survive themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "1.1 Requirements and Challenges", "text": "A system for visualizing natural language descriptions requires a visualization engine to realize the final interpretation of the language. Fortunately, current computer graphics software and hardware technologies are sophisticated and can generate natural visualizations in real time. Therefore, this requirement does not pose any challenges; the second requirement relates to understanding natural language; a natural language interface must be able to decipher a description, discover the hidden semantics therein and transform it into a formal representation of knowledge; this requirement can be a fundamental challenge, even for a limited system; the third requirement is the development of an integrated architecture; the definition of a system capable of integrating a natural language interface and a GUI for visualization purposes requires addressing profound technical challenges at different conceptual and operational levels; and the integration of artificial intelligence (AI) techniques such as understanding the natural language (R), the rationale of time, the rationale of the rationale of the rationale, the rationale of the rationale of the rationale, the rationale of the rationale of the R), the rationale of the temporal representation and the rationale."}, {"heading": "1.1.1 Natural Language Understanding", "text": "NLU is the process of disambiguating a set of descriptions expressed in natural language, based on a hierarchy of some sub-processes, including but not limited to (1) morphological analyses such as stemming and lemmatization; (2) syntactic analyses such as part-of-speech (POS) tagging, syntactical parsing, recognition of nomenclatures and anaphora resolution; (3) semantic analyses such as word disambiguation, capture of predicate argument structures and role labeling; and (4) discourse analysis. A natural language interface with the purpose of visualization should decode the descriptions based on scene arrangements and capture semantics associated with scene layout, spatial-temporal constraints, parameterized actions, etc."}, {"heading": "1.1.2 Inferring Implicit Knowledge", "text": "When people communicate, they assume that the audience knows the context from the outset and therefore does not deepen it. They also ignore common sense and assume that the audience fills the gaps [14]. Deriving implicit knowledge from this is a major challenge for current computer software. Furthermore, it is a challenging task to derive meaningful interpretations of spatio-temporal relations from descriptions of the world model."}, {"heading": "1.1.3 Knowledge Representation", "text": "The representation should support the insertion, updating and querying of target knowledge operations, it should also represent concepts, entities, relationships, constraints, uncertainty, etc. A system for converting natural language descriptions into visual representation requires a KR component to represent the semantics discovered and determine the actions to be taken. Moreover, a thinking mechanism embedded in the KR component can help the system derive implicit knowledge from existing knowledge. Designing such a component is not a trivial task."}, {"heading": "1.1.4 Symbol Grounding", "text": "Semantics is presented as high-level concepts within the KR component that ultimately have to be grounded into low-threshold graphical objects, visual features, transformations and relationships. In this mapping process, high-level concepts are broken down into a series of low-threshold graphical instructions that run in series or parallel and parameterise these instructions. Automating this process is one of the research objectives of AI."}, {"heading": "1.1.5 Scalability", "text": "A scalable system should consistently combine high-level semantic processing with low-level degradation and use data-driven techniques to generalize to invisible scenarios. Gathering necessary tools and repositories, such as lexical resources and object databases, is also a challenging task, as is acquiring the knowledge itself (both implicit and explicit)."}, {"heading": "1.2 Classification", "text": "Given the interdisciplinary nature of visualization systems with natural language interfaces, it is possible to categorize the literature from several points of view. In terms of design methodology, the systems can be divided into rule-based, data-driven and multiagenic systems. Another possible classification may be based on the system behavior that divides the systems into reactive and advisory systems. It is also possible to classify the literature on the basis of the applied language understanding approach, syntactical analysis, knowledge base scheme, etc., but none of these classifications deals with the graphic aspects. Likewise, one can categorize the literature on the basis of graphic aspects that ignore the intelligent aspects of the systems. In order to have a uniform classification scheme that can take into account various aspects of the research, we categorize the literature based on the results produced. Using this scheme, we classify the literature into three categories: text-text-system-text-to-to-text-image-to-to-to-to-to-be interpreted or to-to-text-to-to-image-to-to-to-to-to-to-to-to-to-text-to-to-to-to-text-to-to-to-to-to-to-be-to-to-to-to-to-to-the-to-to-text-to-to-to-to-to-the-to-to-be-to-to-to-the-to-to-to-the-to-be-to-to-to-the-to-to-to-the-to-be-text-to-to-to-the-to-to-to-to-to-to-to-be-to-to-be-to-be-text-to-the-to-to-to-to-the-to-be-to-text-to-to-to-to-be-to-be-an-to-be-to-text-to-the-to-to-to-to-an-to-to-be-to-an-to-to-to-an-to-to-to-to-to-to-to-to-to-to-to-be-to-to-to"}, {"heading": "1.3 Contribution", "text": "In terms of authors \"knowledge, this article is the first comprehensive overview of systems and approaches related to the visualization of natural language descriptions. Surprisingly, despite their scientific and industrial value, not so many studies have been conducted in this direction, and among the existing work there are few that make solid contributions to this area. In this overview, the requirements and challenges for the development of such systems are discussed and 26 graphical systems that use natural language interfaces and deal with aspects of both artificial intelligence and visualization are reported, serving as a reference framework for researchers and enabling further progress in this field. For each presented system, the inputs and outputs, design methodology, architecture, implementation, language processes, graphical processes, intelligent processes and resources are performed, and the pros and cons discussed."}, {"heading": "1.4 Organization", "text": "The article is structured as follows: Section 2 provides a concise terminology of computer linguistics; Section 3 provides an overview of text-to-image conversion systems and examines two sample systems; Section 4 covers text-to-scene conversion systems and covers seven systems; Section 5 provides a comprehensive overview of 17 text-to-animation conversion systems; Section 6 discusses the general limitations of developed systems and provides possible solutions and directions for future studies. Section 7 concludes the article."}, {"heading": "2 Terminology of Computational Linguistics", "text": "Given the interdisciplinary nature of this article, it may attract audiences from various fields such as computational linguistics, human-computer interaction, artificial intelligence, and computer graphics. In order to provide readers with a coherent article, this section offers a concise terminology of computational linguistics as follows. \u2022 Stop words: words with syntactic functionality that carry insignificant semantic information (e.g., that and is). \u2022 Sack-of-Words model: a text presentation model that treats a given text as a set of words and frequencies, disregarding syntax and word order. \u2022 Lemmatization: the process of grouping the various bent forms of a word so that they can be analyzed as a single element. As an example, go is the Lemma of words [go, goes, going, went, gone, gone, gone, gone, gone]."}, {"heading": "3 Text-to-Picture", "text": "Text-to-image conversion is probably the easiest way to visualize natural language descriptions [3, 40, 41, 83]. It addresses the problem of mapping natural language descriptions to a visual representation as a data-driven retrieval and ranking problem, and attempts to solve it with the basics of commercial web-based image search engines. In this approach, descriptive terms or components representing the main concepts of the text are extracted using text-mining techniques such as bagging words, named units, and N-gram models. However, it is assumed that a annotated dataset of images is available. In the case of automatic annotation, it is customary to collect a repository of multimodal information containing both images and text, and then to use the co-occurring text around images to comment on them. In web-based image editing systems, annotations of initial text, this text appearing within the process, and performing the three-dimensional HTML, are performed."}, {"heading": "3.1 Story Picturing Engine", "text": "The Image Machine of History [40, 41] addresses the process of mapping a given textual story into a series of representative images by focusing on \"quantifying the meaning of the image in a pool of images.\" This system receives entry stories such as \"Vermont is largely a rural state, the landscape has the comforting feel of a place, the.\" [41] and ranks the related and available images accordingly as a result. This system is a pipeline of three processes as follows: First, the descriptor's keywords are extracted from the story. To this end, the stop words are eliminated using a manually created dictionary, and then a subset of the remaining words is selected based on a combination of bag-of-word model and named entity recognition. WordNet uses the Bag-of-Word model used to determine the number of senses 242 of a particular word of the words of the words are counted."}, {"heading": "3.2 Text-to-Picture Synthesis System", "text": "The goal of this system is to expand the communication between humans and computers by adding a visual modality to the natural language channel. [83] Unlike the image recognition engine, this system associates a different image to each extracted keyphrase and presents the story as a sequence of related images. It treats the problem of textto-image conversion as an optimization process that attempts to optimize the probability of the extracted keyphrases, images, and placement against the input description. To extract the keyphrases, the system first eliminates the stopwords and then uses a POS tagger to extract the nouns, self-concepts, and adjectives, which are then fed into a logistic regression model to determine the probability of their reproduction based on Google Web hit stories and image narratives."}, {"heading": "4 Text-to-Scene", "text": "One way to improve visualization is to create the scene directly, rather than showing representative images. This approach, known as the text-to-scene conversion paradigm, allows the system to deal with background, layout, lighting, objects, poses, relative sizes, spatial relationships, and other features that cannot be addressed by text-to-scene conversion systems. [24] In a text-to-scene conversion system, words with specific POS tags carry more visual information than others. Noun and proper-name POS tags are usually associated with objects, agents, and places, and the words with these tags can be used to retrieve three-dimensional (3D) models from model repositories. An adjective POS tag is usually associated with a set of object features, and the words with this tag are used to change object features such as color, relative size, and locations, and the words can be used with those models to retrieve positional (3D) tags."}, {"heading": "4.1 NALIG", "text": "Natural Language Driven Image Generation (NALIG) is one of the early projects to create static 2D scenes from descriptions of natural language. It uses a very limited form of input language, which is basically a simple regular expression. NALIG focuses on studying the relationship between spatial information and prepositions in Italian phrases. The accepted form of phrases in this system is: [Subject] [Preposition] [Object] Using this regular expression, NALIG can understand inputs such as the book on the table. It can also handle ambiguities within phrases and simple implicit spatial arrangements using taxonomic rules such as object X support object Y, which defines the relationships between existing objects. These rules are defined on the basis of state conditions, containment constraints, structural constraints and supporting rules."}, {"heading": "4.2 PUT", "text": "It generates static scenes by directly manipulating the spatial arrangements of rigid objects with a limited subset of natural language. This system consists of a simple parser implemented in C + + and a rendering engine that visualizes the static 3D environment. The syntax of the limited language can be seen in the form of a regular expression. [P] + V indicates the limited input language and the way it is used. [P] + V indicates the placement of verbs that manifest the type of positioning of the object."}, {"heading": "4.3 WordsEye", "text": "In fact, the two of them are purely a mental game, with the aim of putting the world at the centre."}, {"heading": "4.4 AVDT", "text": "In fact, most of the people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "4.5 System Developed at Stanford University", "text": "Unlike previous systems, the system developed at Stanford University derives the implicit relationships and partially supports interactive scene manipulation and active learning. In this system, a scene template is generated from an input text and converted into a geometric graph, which is then used to render a static scene. The scene template, which is constructed from the input text using the Stanford CoreNLP language processing tool, is a graph with objects as their vertices and relationships as their edges. Objects are recognized by recognizing nouns that are considered visualizable according to WordNet. Words with an adjective tag within the noun phrases are extracted to identify the attributes of the objects. Spatial relationships are extracted from a set of predefined patterns. Natural language descriptions typically do not contain sound facts about the spatial system. To alleviate this challenge, the system uses conditional probability to model the structures and hierarchies associated with them."}, {"heading": "4.6 Systems That Learn Visual Clues", "text": "The aforementioned systems use textual cues either as a set of predefined rules or as a set of models derived from corporations. A new trend in text conversion systems is learning the associations between textual and visual cues. This research follows the approach of text-to-image conversion systems by focusing on learning the visual features from the existing image database and making associations between visual and textual cues in order to automate the visualization processes.Unlike text-to-image conversion systems, they use associations to position objects within a static scene rather than selecting representative images. AttribIt [17] is designed to help users create visual attributes such as dangerous aircraft."}, {"heading": "5 Text-to-Animation", "text": "The text-to-animation paradigm adds dynamism to static scenes and realizes temporal relations as an additional layer to the naturalness of the visualization generated. In this paradigm, in addition to the linguistic analyses performed by the text-to-scene conversion systems, the visual verbs within the text are captured, parameterized, and then based on a series of virtual actions and manipulations within the digital world. Action parameterization is a major challenge in the case of general domain systems and requires conclusions about trajectories, goals, intermediate actions, etc. Furthermore, in a text-to-animation conversion system, the conversion network is expanded to capture the spatio-temporal constraints rather than just static spatial constraints. In other words, the objects can enter or leave the scene and the spatial relationships between users as a pulsating conversion in the simulation time."}, {"heading": "5.1 SHRLDU", "text": "SHRLDU, developed by Winograd [77] at the Massachusetts Institute of Technology, was one of the pioneering systems that integrated AI into computer graphics. It was also one of the early systems that used deep semantic analysis. SHRLDU consists of a simulated robot manipulator equipped with an intelligent controller that operates within a virtual toy world. The world contains a few blocks of different shapes (e.g. cubes, pyramids, etc.), sizes, and colors. The robot arm can perform three actions on these blocks, including (1) moving a block to a place, (2) capturing a block, and (3) untangling the sequence. The robot manipulates the environment according to a limited set of natural speech commands. SHRLDU is implemented in the Microplaner and Lisp programming languages. Its architecture consists of four modules, including a voice analyzer, a dialogue manager, and a graphical construction."}, {"heading": "5.2 PAR", "text": "Parameterized Action Representation (PAR) developed at the University of Pennsylvania, is a framework for controlling virtual people with natural voice commands in a context-sensitive manner [5, 6, 11]. The main focus of the PAR is the development of a comprehensive knowledge representation to reflect the input commands on the behavior of the agents. The structure of the PAR contains applicability state, results, participants, semantics, path, termination, duration, manner, sub-actions, parent action, previous action, simultaneous action [5].In this representation is the applicability state indicating the feasibility of an action for a particular agent. The start and result point to the states and timestamps of a particular action by the agent and the termination of that action. Participants refer to the agent performing the current PAR and the passive objects."}, {"heading": "5.3 Carsim", "text": "Carsim [4, 27, 39] is a domain-specific system designed to generate simple animations of car accidents based on a series of Swedish accident reports collected from news articles, victim accounts and official transcriptions of officers. It consists of two main modules, including information extraction module and visualization module. The information extraction module analyzes the input text and converts it into a triple representation \u00a1S, R, C, \"in which S denotes scenic objects such as weather, R represents a series of road objects such as cars, and C is a series of collisions that occurred in the accident. This module uses the Granska POS tagger [13] for meeting the input text and uses a small lexicon and a few regular expressions to extract the named entities, such as street names. It also uses a local dictionary extracted from WordNet to discover the plot lines."}, {"heading": "5.4 ScriptViz", "text": "ScriptViz [46] aims to replace manual storyboard drawing with automatic dynamic scene creation in the production process of movies. It consists of three interacting modules, including a language comprehension module, a high-level planner and a scene generator. The language comprehension module uses the Apple pie parser [65] to derive the syntactic structure of the input text. It separates the clauses based on the conjunctions, extracts the actions from the verbs and recognizes the objects from the correct nouns. The verb and the correct nouns are matched with the actions and the objects use a naive binary matching mechanism that does not recognize the interaction between the objects. The high-level planning module generates action plans based on the information obtained from the language comprehension module. The planning process is completed within four consecutive phases, a plan is the first result of the virtual surrounding objects being collected."}, {"heading": "5.5 CONFUCIS", "text": "It is basically a narrative system designed to animate virtual people. It supports lip synchronization and the parallel animation of the upper and lower bodies of human models. [48] CONFUCIS consists of a knowledge base, a speech processor that animates the virtual human. It supports lip synchronization and the simultaneous animation of the upper and lower bodies of human models. [48] CONFUCIS consists of a knowledge base, a media allocator, an animation machine, a text man and a synchronizer."}, {"heading": "5.6 Scene Maker", "text": "SceneMaker [34,35] is a collaborative and multimodal system for pre-visualizing the scenes of predefined scripts to facilitate the film production process. This system is a successor of the CONFUCIS system and uses the underlying language processing and multimodal animation tools. SceneMaker expands CONFUCIS with healthy knowledge of genre specifications, emotional expressions and the capture of emotions from the scripts. Users can edit the generated animation online via mobile devices.SceneMaker consists of two layers, including a user interface that can be executed on a PC or mobile device, and a scene production layer that runs on a server. The user interface receives a script from the user and provides it with a 3D animation of the script and a scene editor to edit the generated animation. The scene production layer contains three components that work in a serial manner, including a modularity of understanding and a visualization module."}, {"heading": "5.7 System Developed at Kyushu Institute of Technology", "text": "This system is designed to generate movements for virtual agents stored within a motion database. To accomplish this task, it captures predefined verbs of action, including intransitive (not a target object), transitive (a target object) and ditransitive (two targets) verbs using a local dictionary. It uses motion frames to change the motion sequences. It is assumed that the characters, objects and motion sequences are predefined manually by the users, an instrument, a target, a contact position, an initial posture and a series of adverbs to change the movement."}, {"heading": "5.8 IVELL", "text": "It also implements a few scenarios, such as an airport and a shopping mall, where learners talk to domain-specific agents while manipulating the virtual world with a haptic robot.Agents can change the degree of difficulty of the conversation by automatically evaluating the linguistic competence of the user. Each agent consists of an abstract layer and an embodied layer.The abstract layer consists of a speech interpreter, a user evaluator, a haptic interviewer, a haptic interpreter, a speech generator and an actor."}, {"heading": "5.9 Other Systems", "text": "This year, the time has come to find a new home in a country where most people live in poverty."}, {"heading": "6 Discussion", "text": "In fact, it is such that most people will be able to move to another world, in which they are able to move to another world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they are able to change the world, in which they live, in which they live, in which they live, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, she lives, in which she lives, in which she lives, in which she lives, in which she lives, in which she lives, she lives, in which she lives, in which she lives, in which she lives, in which she lives,"}, {"heading": "7 Conclusion", "text": "In this article we discussed the requirements and challenges for the development of systems capable of displaying descriptions in a natural language. We reported on 26 such systems and discussed the methodology, implementation, aspects of processing natural language, including morphological, syntactic and semantic analysis, knowledge bases, lexicographies, AI components, computer graphics aspects, such as rendering and model memory, and the advantages and disadvantages of these systems. We conclude that most of the systems introduced in the literature do not appreciate the available resources (e.g. lexicographs and semantic networks) nor the available techniques (active learning, flat semantic analysis, etc.) in addition to the current technical challenges in the field of natural language comprehension, the provision of common sense, the conclusion of implicit knowledge, action learning, etc., and therefore do not meet the expected requirements for a natural language visualizer. We predict that the current challenges of developing these systems will be met by the use of algorithmic end-to-mitigate in the foreseeable future."}], "references": [{"title": "Natural language driven image generation", "author": ["Giovanni Adorni", "Mauro Di Manzo", "Fausto Giunchiglia"], "venue": "In Proceedings of the 10th International Conference on Computational Linguistics and 22nd annual meeting on Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1984}, {"title": "Natural language input for scene generation", "author": ["Givoanni Adorni", "Mauro Di Manzo", "Giacomo Ferrari"], "venue": "In Proceedings of the first conference on European chapter of the Association for Computational Linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1983}, {"title": "Enriching textbooks with images", "author": ["Rakesh Agrawal", "Sreenivas Gollapudi", "Anitha Kannan", "Krishnaram Kenthapadi"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "Carsim: an automatic 3d text-to-scene conversion system applied to road accident reports", "author": ["Ola \u00c5kerberg", "Hans Svensson", "Bastian Schulz", "Pierre Nugues"], "venue": "In Proceedings of the tenth conference on European chapter of the Association for Computational Linguistics-Volume", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Embodied conversational agents", "author": ["Norman I. Badler", "Rama Bindiganavale", "Jan Allbeck", "William Schuler", "Liwei Zhao", "Martha Palmer"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2000}, {"title": "Animation control for real-time virtual humans", "author": ["Norman I Badler", "Martha S Palmer", "Rama Bindiganavale"], "venue": "Communications of the ACM,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1999}, {"title": "Simulating humans: computer graphics animation and control", "author": ["Norman I Badler", "Cary B Phillips", "Bonnie Lynn Webber"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1993}, {"title": "The berkeley framenet project", "author": ["Collin F Baker", "Charles J Fillmore", "John B Lowe"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics- Volume", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1998}, {"title": "Mica: a probabilistic dependency parser based on tree insertion grammars application note", "author": ["Srinivas Bangalore", "Pierre Boulllier", "Alexis Nasr", "Owen Rambow", "Ben\u00f4\u0131t Sagot"], "venue": "In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Learning deep architectures for ai", "author": ["Yoshua Bengio"], "venue": "Foundations and trends R  \u00a9 in Machine Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Dynamically altering agent behaviors using natural language instructions", "author": ["Rama Bindiganavale", "William Schuler", "Jan M Allbeck", "Norman I Badler", "Aravind K Joshi", "Martha Palmer"], "venue": "In Proceedings of the fourth international conference on Autonomous agents,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Using multi-agent systems to visualize text descriptions", "author": ["Edgar Bola\u00f1o-Rod\u0155\u0131guez", "Juan C Gonz\u00e1lez-Moreno", "David Ramos-Valcarcel", "Luiz V\u00e1zquez-L\u00f3pez"], "venue": "In Advances on Practical Applications of Agents and Multiagent Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Implementing an efficient part-of-speech tagger", "author": ["Johan Carlberger", "Viggo Kann"], "venue": "Software-Practice and Experience,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Interactive learning of spatial knowledge for text to 3d scene generation", "author": ["Angel X Chang", "Manolis Savva", "Christopher D Manning"], "venue": "Sponsor: Idibon,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Learning spatial knowledge for text to 3d scene generation", "author": ["Angel X Chang", "Manolis Savva", "Christopher D Manning"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Semantic parsing for text to 3d scene generation", "author": ["Angel X Chang", "Manolis Savva", "Christopher D Manning"], "venue": "ACL 2014,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Attribit: content creation with semantic attributes", "author": ["Siddhartha Chaudhuri", "Evangelos Kalogerakis", "Stephen Giguere", "Thomas Funkhouser"], "venue": "In Proceedings of the 26th annual ACM symposium on User interface software and technology,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "An annotation rule extraction algorithm for image retrieval", "author": ["Zeng Chen", "Jin Hou", "Dengsheng Zhang", "Xue Qin"], "venue": "Pattern Recognition Letters,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Interactive tool for image annotation using a semi-supervised and hierarchical approach", "author": ["Cheng-Chieh Chiang"], "venue": "Computer Standards & Interfaces,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Put: Language-based interactive manipulation of objects", "author": ["Sharon Rose Clay", "Jane Wilhelms"], "venue": "IEEE Computer Graphics and Applications,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1996}, {"title": "Vignet: Grounding language in graphics using frame semantics", "author": ["Bob Coyne", "Daniel Bauer", "Owen Rambow"], "venue": "In Proceedings of the ACL 2011 Workshop on Relational Models of Semantics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Annotation tools and knowledge representation for a text-to-scene system", "author": ["Bob Coyne", "Alex Klapheke", "Masoud Rouhizadeh", "Richard Sproat", "Daniel Bauer"], "venue": "In COLING,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Frame semantics in text-to-scene generation", "author": ["Bob Coyne", "Owen Rambow", "Julia Hirschberg", "Richard Sproat"], "venue": "In International Conference on Knowledge-Based and Intelligent Information and Engineering Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "Wordseye: an automatic text-to-scene conversion system", "author": ["Bob Coyne", "Richard Sproat"], "venue": "In Proceedings of the 28th annual conference on Computer graphics and interactive techniques,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2001}, {"title": "Spatial relations in text-to-scene conversion", "author": ["Bob Coyne", "Richard Sproat", "Julia Hirschberg"], "venue": "In Computational Models of Spatial Language Interpretation, Workshop at Spatial Cognition,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "A framework and graphical development environment for robust nlp tools and applications", "author": ["Hamish Cunningham", "Diana Maynard", "Kalina Bontcheva", "Valentin Tablan"], "venue": "In ACL,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2002}, {"title": "Generating a 3d simulation of a car accident from a written description in natural language: The carsim system", "author": ["Sylvain Dupuy", "Arjan Egges", "Vincent Legendre", "Pierre Nugues"], "venue": "In Proceedings of the workshop on Temporal and spatial information processing-Volume 13,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "The case for case", "author": ["Charles J Fillmore"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1967}, {"title": "Amazon mechanical turk: Gold mine or coal mine", "author": ["Kar\u00ebn Fort", "Gilles Adda", "K Bretonnel Cohen"], "venue": "Computational Linguistics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2011}, {"title": "Automating the conversion of natural language fiction to multi-modal 3d animated virtual environments", "author": ["Kevin Glass"], "venue": "South African Computer Journal,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Automating the creation of 3d animation from annotated fiction text", "author": ["Kevin Glass", "Shaun Bangay"], "venue": "In Proceedings of the IADIS International Conference on Computer Graphics and Visualization,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Web image indexing by using associated texts", "author": ["Zhiguo Gong", "Chan Wa Cheang"], "venue": "Knowledge and information systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2006}, {"title": "Scenemaker: automatic visualisation of screenplays", "author": ["Eva Hanser", "Paul Mc Kevitt", "Tom Lunney", "Joan Condell"], "venue": "In Annual Conference on Artificial Intelligence,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2009}, {"title": "Scenemaker: multimodal visualisation of natural language film scripts", "author": ["Eva Hanser", "Paul Mc Kevitt", "Tom Lunney", "Joan Condell", "Minhua Ma"], "venue": "In International Conference on Knowledge-Based and Intelligent Information and Engineering Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Adaptive animation generation using web content mining", "author": ["Kaveh Hassani", "Won-Sook Lee"], "venue": "In Evolving and Adaptive Intelligent Systems (EAIS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Architectural design and implementation of intelligent embodied conversational agents using fuzzy knowledge base", "author": ["Kaveh Hassani", "Ali Nahvi", "Ali Ahmadi"], "venue": "Journal of Intelligent & Fuzzy Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}, {"title": "Design and implementation of an intelligent virtual environment for improving speaking and listening skills", "author": ["Kaveh Hassani", "Ali Nahvi", "Ali Ahmadi"], "venue": "Interactive Learning Environments,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Carsim: a system to visualize written road accident reports as animated 3d scenes", "author": ["Richard Johansson", "David Williams", "Anders Berglund", "Pierre Nugues"], "venue": "In Proceedings of the 2nd Workshop on Text Meaning and Interpretation,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2004}, {"title": "The story picturing engine: finding elite images to illustrate a story using mutual reinforcement", "author": ["Dhiraj Joshi", "James Z Wang", "Jia Li"], "venue": "In Proceedings of the 6th ACM SIGMM international workshop on Multimedia information retrieval,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2004}, {"title": "The story picturing engine\u2014a system for automatic text illustration", "author": ["Dhiraj Joshi", "James Z Wang", "Jia Li"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM),", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Using storytelling to motivate programming", "author": ["Caitlin Kelleher", "Randy Pausch"], "venue": "Communications of the ACM,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2007}, {"title": "An expansion and reranking approach for annotation-based image retrieval from web", "author": ["Deniz K\u0131l\u0131n\u00e7", "Adil Alpkocak"], "venue": "Expert Systems with Applications,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2011}, {"title": "The potential of a text-based interface as a design medium: An experiment in a computer animation environment", "author": ["Sangwon Lee", "Jin Yan"], "venue": "Interacting with Computers, page iwu036,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Conceptneta practical commonsense reasoning tool-kit", "author": ["Hugo Liu", "Push Singh"], "venue": "BT technology journal,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2004}, {"title": "Script visualization (scriptviz): a smart system that makes writing fun", "author": ["Zhi-Qiang Liu", "Ka-Ming Leung"], "venue": "Soft Computing,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2006}, {"title": "Automatic conversion of natural language to 3D animation", "author": ["Minhua Ma"], "venue": "PhD thesis, University of Ulster,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2006}, {"title": "Visual semantics and ontology of eventive verbs", "author": ["Minhua Ma", "Paul Mc Kevitt"], "venue": "In International Conference on Natural Language Processing,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2004}, {"title": "Virtual human animation in natural language visualisation", "author": ["Minhua Ma", "Paul Mc Kevitt"], "venue": "Artificial Intelligence Review,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2006}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Christopher D Manning", "Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky"], "venue": "In ACL (System Demonstrations),", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2014}, {"title": "Textrank: Bringing order into texts", "author": ["Rada Mihalcea", "Paul Tarau"], "venue": "Association for Computational Linguistics,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Using techniques based on natural language in the development process of multiagent systems", "author": ["Juan Carlos Gonz\u00e1lez Moreno", "Luis V\u00e1zquez L\u00f3pez"], "venue": "In International Symposium on Distributed Computing and Artificial Intelligence", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2008}, {"title": "Applying deductive techniques to the creation of realistic historical 3d spatiotemporal visualisations from natural language narratives", "author": ["Amanda Oddie", "Hope Park", "Paul Hazlewood", "Brian Farrimond", "Steve Presland"], "venue": "In Proceedings of the 2011 international conference on Electronic Visualisation and the Arts,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2011}, {"title": "Generating animation from natural language texts and framework of motion database", "author": ["Masaki Oshita"], "venue": "In CyberWorlds,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Generating animation from natural language texts and semantic analysis for motion search and scheduling", "author": ["Masaki Oshita"], "venue": "The Visual Computer,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2010}, {"title": "Xtag: a graphical workbench for developing tree-adjoining grammars", "author": ["Patrick Paroubek", "Yves Schabes", "Aravind K Joshi"], "venue": "In Proceedings of the third conference on Applied natural language processing,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1992}, {"title": "Agent oriented software engineering with ingenias", "author": ["Juan Pav\u00f3n", "Jorge G\u00f3mez-Sanz"], "venue": "In International Central and Eastern European Conference on Multi- Agent Systems,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2003}, {"title": "Creating complex interactive 3d visualisations of naval battles from natural language narratives", "author": ["Steve Presland", "Brian Farrimond", "Paul Hazlewood", "Amanda Oddie"], "venue": "In Developments in E-systems Engineering (DESE),", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2010}, {"title": "Msr splat, a language analysis toolkit", "author": ["Chris Quirk", "Pallavi Choudhury", "Jianfeng Gao", "Hisami Suzuki", "Kristina Toutanova", "Michael Gamon", "Wen-tau Yih", "Lucy Vanderwende", "Colin Cherry"], "venue": "In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Demonstration Session,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2012}, {"title": "A maximum entropy model for part-of-speech tagging", "author": ["Adwait Ratnaparkhi"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 1996}, {"title": "Collecting spatial information for locations in a text-to-scene conversion system", "author": ["Masoud Rouhizadeh", "Daniel Bauer", "Bob Coyne", "Owen Rambow", "Richard Sproat"], "venue": "Computational Models of Spatial Language Interpretation and Generation", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2011}, {"title": "Data collection and normalization for building the scenario-based lexical knowledge resource of a text-to-scene conversion system", "author": ["Masoud Rouhizadeh", "Margit Bowler", "Richard Sproat", "Bob Coyne"], "venue": "In Semantic Media Adaptation and Personalization (SMAP),", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2010}, {"title": "Collecting semantic information for locations in the scenario-based lexical knowledge resource of a text-to-scene conversion system", "author": ["Masoud Rouhizadeh", "Bob Coyne", "Richard Sproat"], "venue": "In International Conference on Knowledge-Based and Intelligent Information and Engineering Systems,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2011}, {"title": "Corpus-based parsing and sublanguage studies", "author": ["Satoshi Sekine"], "venue": "PhD thesis,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 1998}, {"title": "Web2animation-automatic generation of 3d animation from the web text", "author": ["Hyunju Shim", "Bogyeong Kang", "Kyungsoo Kwag"], "venue": "In Proceedings of the 2009 IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technology-Volume", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2009}, {"title": "Textonboost for image understanding: Multi-class object recognition and segmentation by jointly modeling texture, layout, and context", "author": ["Jamie Shotton", "John Winn", "Carsten Rother", "Antonio Criminisi"], "venue": "International Journal of Computer Vision,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2009}, {"title": "Avdt-automatic visualization of descriptive texts", "author": ["Christian Spika", "Katharina Schwarz", "Holger Dammertz", "Hendrik PA Lensch"], "venue": "In VMV,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2011}, {"title": "Web image annotation using an effective term weighting", "author": ["Vundavalli Srinivasarao", "Vasudeva Varma"], "venue": "In International Conference on Intelligent Text Processing and Computational Linguistics,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2012}, {"title": "Wordnet affect: an affective extension of wordnet", "author": ["Carlo Strapparava", "Alessandro Valitutti"], "venue": "In LREC,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2004}, {"title": "Animated storytelling system via text", "author": ["Kaoru Sumi", "Mizue Nagata"], "venue": "In Proceedings of the 2006 ACM SIGCHI international conference on Advances in computer entertainment technology,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2006}, {"title": "Automatic conversion from e-content into virtual storytelling", "author": ["Kaoru Sumi", "Katsumi Tanaka"], "venue": "In International Conference on Virtual Storytelling,", "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2005}, {"title": "Story driven animation", "author": ["Yosuke Takashima", "Hideo Shimazu", "Masahiro Tomono"], "venue": "In ACM SIGCHI Bulletin,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1987}, {"title": "A non-projective dependency parser", "author": ["Pasi Tapanainen", "Timo J\u00e4rvinen"], "venue": "In Proceedings of the fifth conference on Applied natural language processing,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1997}, {"title": "Simplicity: Semantics-sensitive integrated matching for picture libraries", "author": ["James Ze Wang", "Jia Li", "Gio Wiederhold"], "venue": "IEEE Transactions on pattern analysis and machine intelligence,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2001}, {"title": "Understanding spontaneous speech", "author": ["Wayne Ward"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 1989}, {"title": "Procedures as a representation for data in a computer program for understanding natural language", "author": ["Terry Winograd"], "venue": "Technical report, DTIC Document,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1971}, {"title": "Towards automatic animated storyboarding", "author": ["Patrick Ye", "Timothy Baldwin"], "venue": "In AAAI,", "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2008}, {"title": "3d scene creation using story-based descriptions", "author": ["Xin Zeng", "Qasim Mehdi", "Norman Gough"], "venue": null, "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2005}, {"title": "From visual semantic parameterization to graphic visualization", "author": ["Xin Zeng", "Quasim H Mehdi", "Norman E Gough"], "venue": "In Ninth International Conference on Information Visualisation", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2005}, {"title": "The development of a language interface for 3d scene generation", "author": ["Xin Zeng", "Manling Tan"], "venue": "In Proceedings of the Second IASTED International Conference on Human Computer Interaction,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2007}, {"title": "Understanding bag-of-words model: a statistical framework", "author": ["Yin Zhang", "Rong Jin", "Zhi-Hua Zhou"], "venue": "International Journal of Machine Learning and Cybernetics,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 2010}, {"title": "A text-to-picture synthesis system for augmenting communication", "author": ["Xiaojin Zhu", "Andrew B Goldberg", "Mohamed Eldawy", "Charles R Dyer", "Bradley Strock"], "venue": "In AAAI,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2007}, {"title": "Learning the visual interpretation of sentences", "author": ["C Lawrence Zitnick", "Devi Parikh", "Lucy Vanderwende"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2013}], "referenceMentions": [{"referenceID": 42, "context": "In recent research [44], comprehensive user studies are performed to compare the performance of natural language interfaces against conventional GUI for animation design tasks in terms of control, creativity, and learning measures.", "startOffset": 19, "endOffset": 23}, {"referenceID": 13, "context": "They also omit the common-sense facts and assume the audience fill in the gaps [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "\u2022 FrameNet: an English lexical database containing manually annotated sentences for semantic role labeling [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 43, "context": "\u2022 ConceptNet: a semantic network of common-sense knowledge [45].", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "Text-to-picture conversion is probably the simplest method for visualizing natural language descriptions [3, 40, 41, 83].", "startOffset": 105, "endOffset": 120}, {"referenceID": 38, "context": "Text-to-picture conversion is probably the simplest method for visualizing natural language descriptions [3, 40, 41, 83].", "startOffset": 105, "endOffset": 120}, {"referenceID": 39, "context": "Text-to-picture conversion is probably the simplest method for visualizing natural language descriptions [3, 40, 41, 83].", "startOffset": 105, "endOffset": 120}, {"referenceID": 81, "context": "Text-to-picture conversion is probably the simplest method for visualizing natural language descriptions [3, 40, 41, 83].", "startOffset": 105, "endOffset": 120}, {"referenceID": 17, "context": "The extracted text is then tokenized and a subset of terms is selected and scored to determine the weighted annotations of the corresponding images [18, 19, 33, 43, 69].", "startOffset": 148, "endOffset": 168}, {"referenceID": 18, "context": "The extracted text is then tokenized and a subset of terms is selected and scored to determine the weighted annotations of the corresponding images [18, 19, 33, 43, 69].", "startOffset": 148, "endOffset": 168}, {"referenceID": 31, "context": "The extracted text is then tokenized and a subset of terms is selected and scored to determine the weighted annotations of the corresponding images [18, 19, 33, 43, 69].", "startOffset": 148, "endOffset": 168}, {"referenceID": 41, "context": "The extracted text is then tokenized and a subset of terms is selected and scored to determine the weighted annotations of the corresponding images [18, 19, 33, 43, 69].", "startOffset": 148, "endOffset": 168}, {"referenceID": 67, "context": "The extracted text is then tokenized and a subset of terms is selected and scored to determine the weighted annotations of the corresponding images [18, 19, 33, 43, 69].", "startOffset": 148, "endOffset": 168}, {"referenceID": 80, "context": "Also, because of exploiting statistical information retrieval rather than natural language understanding, the text-to-picture conversion approach is computationally efficient [82].", "startOffset": 175, "endOffset": 179}, {"referenceID": 38, "context": "The story picturing engine [40, 41] addresses the mapping process of a given textual story to a set of representative pictures by focusing on \u201c quantifying image importance in a pool of images.", "startOffset": 27, "endOffset": 35}, {"referenceID": 39, "context": "The story picturing engine [40, 41] addresses the mapping process of a given textual story to a set of representative pictures by focusing on \u201c quantifying image importance in a pool of images.", "startOffset": 27, "endOffset": 35}, {"referenceID": 39, "context": "\u201d [41] and ranks the related and available images accordingly as the output.", "startOffset": 2, "endOffset": 6}, {"referenceID": 73, "context": "The next step in the pipeline is to estimate the similarity between pairs of images based on their visual and lexical features, which is calculated based on a linear combination of Integrated Region Matching (IRM) distance [75] and WordNet hierarchy.", "startOffset": 223, "endOffset": 227}, {"referenceID": 81, "context": "The goal of this system is to augment the human-human and human-computer communications by adding a visual modality to the natural language channel [83].", "startOffset": 148, "endOffset": 152}, {"referenceID": 49, "context": "Then, the TextRank algorithm [51] is applied to the computed probabilities and the top 20 keywords are selected and used to form the key phrases.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "This approach, known as text-to-scene conversion paradigm, lets the system elaborate on background, layout, lighting, objects, poses, relative sizes, spatial relations, and other features that cannot be addressed using text topicture conversion systems [24].", "startOffset": 253, "endOffset": 257}, {"referenceID": 0, "context": "Natural Language Driven Image Generation (NALIG) is one of the early projects on generating static 2D scenes from natural language descriptions [1, 2].", "startOffset": 144, "endOffset": 150}, {"referenceID": 1, "context": "Natural Language Driven Image Generation (NALIG) is one of the early projects on generating static 2D scenes from natural language descriptions [1, 2].", "startOffset": 144, "endOffset": 150}, {"referenceID": 19, "context": "The PUT language-based placement system [20] is a rule-based spatial-manipulation system inspired by cognitive linguistics.", "startOffset": 40, "endOffset": 44}, {"referenceID": 23, "context": "WordsEye [24] is designed to generate 3D scenes containing environment, objects, characters, attributes, poses, kinematics, and spatial relations.", "startOffset": 9, "endOffset": 13}, {"referenceID": 8, "context": "In the early version, an analyzer was implemented in common Lisp and, later, MICA parser [9] was exploited as well.", "startOffset": 89, "endOffset": 92}, {"referenceID": 22, "context": "This structure is then utilized to construct a semantic representation in which objects, actions, and relations are represented in terms of semantic frames [23].", "startOffset": 156, "endOffset": 160}, {"referenceID": 24, "context": "In a recent development, lexical knowledge extracted from WordNet and FrameNet are semi-manually refined to construct a Scenario-Based Lexical Knowledge Resource (SBLR), which is essentially a lexical knowledgebase tailored to represent the lexical and common-sense knowledge for text-to-scene conversion purposes [25].", "startOffset": 314, "endOffset": 318}, {"referenceID": 20, "context": "The knowledge in SBLR is represented by VigNet [21,22] which is an extension of FrameNet and consists of a set of intermediate frames called Vignettes that bridge the semantic gap between the semantic frames of FrameNet and the low-level graphical frames.", "startOffset": 47, "endOffset": 54}, {"referenceID": 21, "context": "The knowledge in SBLR is represented by VigNet [21,22] which is an extension of FrameNet and consists of a set of intermediate frames called Vignettes that bridge the semantic gap between the semantic frames of FrameNet and the low-level graphical frames.", "startOffset": 47, "endOffset": 54}, {"referenceID": 28, "context": "This knowledge is a remedy for missing common-sense facts within the natural language descriptions and is acquired through manual descriptions of the pictures gathered from Amazon Mechanical Turk (AMT) [30]-an online crowd-sourcing framework for data collection using Human Intelligence Task (HITs).", "startOffset": 202, "endOffset": 206}, {"referenceID": 60, "context": "text processing techniques to populate the VigNet with extracted Vignettes [62\u201364].", "startOffset": 75, "endOffset": 82}, {"referenceID": 61, "context": "text processing techniques to populate the VigNet with extracted Vignettes [62\u201364].", "startOffset": 75, "endOffset": 82}, {"referenceID": 62, "context": "text processing techniques to populate the VigNet with extracted Vignettes [62\u201364].", "startOffset": 75, "endOffset": 82}, {"referenceID": 22, "context": "Although it has achieved a good degree of success, the allowed input language for describing the scenes is stilted [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 66, "context": "Automatic Visualization of Descriptive Texts (AVDT) [68] generates static 3D scenes from descriptive text by emphasizing the spatial relations and the naturalness of the generated scene.", "startOffset": 52, "endOffset": 56}, {"referenceID": 25, "context": "It utilizes GATE [26]-an open-source text processing tool as a pre-processor for tasks such as lemmatizing the nouns, POS tagging, and generating dependency structures.", "startOffset": 17, "endOffset": 21}, {"referenceID": 13, "context": "Contrary to previous systems, the system developed at Stanford University [14\u201316] infers the implicit relations and partially supports interactive scene manipulation and active learning.", "startOffset": 74, "endOffset": 81}, {"referenceID": 14, "context": "Contrary to previous systems, the system developed at Stanford University [14\u201316] infers the implicit relations and partially supports interactive scene manipulation and active learning.", "startOffset": 74, "endOffset": 81}, {"referenceID": 15, "context": "Contrary to previous systems, the system developed at Stanford University [14\u201316] infers the implicit relations and partially supports interactive scene manipulation and active learning.", "startOffset": 74, "endOffset": 81}, {"referenceID": 48, "context": "The scene template constructed from the input text using Stanford CoreNLP language processing tool [50] is a graph with objects as its vertices and relations as its edges.", "startOffset": 99, "endOffset": 103}, {"referenceID": 16, "context": "AttribIt [17] is designed to help the users create visual content using subjective attributes such as dangerous airplane.", "startOffset": 9, "endOffset": 13}, {"referenceID": 82, "context": "A promising data-driven system developed at Microsoft Research Center is introduced in [84].", "startOffset": 87, "endOffset": 91}, {"referenceID": 65, "context": "It exploits Conditional Random Field (CRF) [67] to extract objects and their occurrences, attributes, and positions.", "startOffset": 43, "endOffset": 47}, {"referenceID": 58, "context": "After extracting visual features, the system extracts semantics in the form of predicate tuples using semantic role analysis [60].", "startOffset": 125, "endOffset": 129}, {"referenceID": 75, "context": "SHRLDU, developed by Winograd [77] at Massachusetts Institute of Technology, was one of the pioneer systems that integrated AI into computer graphics.", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "Parameterized Action Representation (PAR) developed at the University of Pennsylvania, is a framework for controlling virtual humans using natural language commands in a context-sensitive fashion [5, 6, 11].", "startOffset": 196, "endOffset": 206}, {"referenceID": 5, "context": "Parameterized Action Representation (PAR) developed at the University of Pennsylvania, is a framework for controlling virtual humans using natural language commands in a context-sensitive fashion [5, 6, 11].", "startOffset": 196, "endOffset": 206}, {"referenceID": 10, "context": "Parameterized Action Representation (PAR) developed at the University of Pennsylvania, is a framework for controlling virtual humans using natural language commands in a context-sensitive fashion [5, 6, 11].", "startOffset": 196, "endOffset": 206}, {"referenceID": 4, "context": "The structure of PAR contains applicability condition, start, results, participants, semantics, path, purpose, termination, duration, manner, sub-actions, parent action, previous action, concurrent action, and next action [5].", "startOffset": 222, "endOffset": 225}, {"referenceID": 55, "context": "XTAG parser [57] and uses a naive string matching algorithm to find the corresponding", "startOffset": 12, "endOffset": 16}, {"referenceID": 6, "context": ", PatNet data structure) to be executed [7].", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "Carsim [4, 27, 39] is a domain-specific system developed for generating simple animations of car accidents based on a set of Swedish accident reports collected from news articles, narratives from victims, and official transcriptions from officers.", "startOffset": 7, "endOffset": 18}, {"referenceID": 26, "context": "Carsim [4, 27, 39] is a domain-specific system developed for generating simple animations of car accidents based on a set of Swedish accident reports collected from news articles, narratives from victims, and official transcriptions from officers.", "startOffset": 7, "endOffset": 18}, {"referenceID": 37, "context": "Carsim [4, 27, 39] is a domain-specific system developed for generating simple animations of car accidents based on a set of Swedish accident reports collected from news articles, narratives from victims, and official transcriptions from officers.", "startOffset": 7, "endOffset": 18}, {"referenceID": 12, "context": "This module utilizes the Granska POS tagger [13] for tagging the input text and uses a small lexicon and a few regular expressions to extract the named entities, such as street names.", "startOffset": 44, "endOffset": 48}, {"referenceID": 44, "context": "ScriptViz [46] aims to replace the manual storyboard drawing with automatic dynamic", "startOffset": 10, "endOffset": 14}, {"referenceID": 63, "context": "The language understanding module uses Apple Pie parser [65] to derive the syntactical structure of the input text.", "startOffset": 56, "endOffset": 60}, {"referenceID": 4, "context": "In case of a feasible action, parameters of the offline plan are set and the result is represented using PAR structure [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 45, "context": "CONFUCIS [47] is a multi-modal text-to-animation conversion system that can generate animation from a single input sentence containing an action verb and synchronize it with speech.", "startOffset": 9, "endOffset": 13}, {"referenceID": 46, "context": "It supports lip synchronization, facial expressions, and parallel animation of the upper and the lower body of human models [48].", "startOffset": 124, "endOffset": 128}, {"referenceID": 72, "context": "The language processor uses a Connexor functional-dependency Grammar parser [74], WordNet, and a lexical conceptual structure database [49] to parse the input sentence and capture the semantics it carries.", "startOffset": 76, "endOffset": 80}, {"referenceID": 47, "context": "The language processor uses a Connexor functional-dependency Grammar parser [74], WordNet, and a lexical conceptual structure database [49] to parse the input sentence and capture the semantics it carries.", "startOffset": 135, "endOffset": 139}, {"referenceID": 32, "context": "SceneMaker [34,35] is a collaborative and multi-modal system designed for pre-visualizing the scenes of given scripts to facilitate the movie production process.", "startOffset": 11, "endOffset": 18}, {"referenceID": 33, "context": "SceneMaker [34,35] is a collaborative and multi-modal system designed for pre-visualizing the scenes of given scripts to facilitate the movie production process.", "startOffset": 11, "endOffset": 18}, {"referenceID": 68, "context": "The reasoning module uses WordNetAffect [70]-an extension to WordNet and ConceptNet to interpret the context, manage emotions, and plan the actions.", "startOffset": 40, "endOffset": 44}, {"referenceID": 53, "context": "This system is designed to generate motion for virtual agents using a set of motion animations stored within a motion database [55, 56].", "startOffset": 127, "endOffset": 135}, {"referenceID": 54, "context": "This system is designed to generate motion for virtual agents using a set of motion animations stored within a motion database [55, 56].", "startOffset": 127, "endOffset": 135}, {"referenceID": 27, "context": "semantic valence [29]-as its knowledge representation scheme, which consists of an agent, a motion, an instrument, a target, a contact position, a direction, an initial posture, and a set of adverbs to modify the motion.", "startOffset": 17, "endOffset": 21}, {"referenceID": 35, "context": "Intelligent Virtual Environment for Language Learning (IVELL) [37, 38] is a domainspecific multi-modal virtual reality system that consists of a few Embodied Conversational Agents (ECAs).", "startOffset": 62, "endOffset": 70}, {"referenceID": 36, "context": "Intelligent Virtual Environment for Language Learning (IVELL) [37, 38] is a domainspecific multi-modal virtual reality system that consists of a few Embodied Conversational Agents (ECAs).", "startOffset": 62, "endOffset": 70}, {"referenceID": 71, "context": "One of the early text-to-animation synthesis systems is the Story Driven Animation System (SDAS) introduced in [73].", "startOffset": 111, "endOffset": 115}, {"referenceID": 77, "context": "3DSV [79\u201381] attempts to create an interactive interface for animating 3D stages of simple stories described in restricted sentences.", "startOffset": 5, "endOffset": 12}, {"referenceID": 78, "context": "3DSV [79\u201381] attempts to create an interactive interface for animating 3D stages of simple stories described in restricted sentences.", "startOffset": 5, "endOffset": 12}, {"referenceID": 79, "context": "3DSV [79\u201381] attempts to create an interactive interface for animating 3D stages of simple stories described in restricted sentences.", "startOffset": 5, "endOffset": 12}, {"referenceID": 69, "context": "Interactive e-Hon [71,72] is a Japanese multi-modal storytelling system designed for facilitating the interactions between parents and children by animating and explaining the difficult concepts in a simpler form using Web content.", "startOffset": 18, "endOffset": 25}, {"referenceID": 70, "context": "Interactive e-Hon [71,72] is a Japanese multi-modal storytelling system designed for facilitating the interactions between parents and children by animating and explaining the difficult concepts in a simpler form using Web content.", "startOffset": 18, "endOffset": 25}, {"referenceID": 29, "context": "A semi-automatic system developed at Rhodes University [31, 32] generates animations from given annotated fiction texts.", "startOffset": 55, "endOffset": 63}, {"referenceID": 30, "context": "A semi-automatic system developed at Rhodes University [31, 32] generates animations from given annotated fiction texts.", "startOffset": 55, "endOffset": 63}, {"referenceID": 76, "context": "A data-driven system developed at the University of Melbourne [78]attempts to train a classifier to ground high-level verbs into a set of low-level graphical tasks.", "startOffset": 62, "endOffset": 66}, {"referenceID": 59, "context": "These linguistic and visual features are then used to train a maximum entropy classifier [61] to decide the next graphical action.", "startOffset": 89, "endOffset": 93}, {"referenceID": 64, "context": "Web2Animation [66] is a multi-modal pedagogical system that uses Web content related to recipes to create an online animation to teach the users how to cook.", "startOffset": 14, "endOffset": 18}, {"referenceID": 74, "context": "The relevant recipe information is located by traversing the HTML tags and analyzed using the Phoenix parser [76].", "startOffset": 109, "endOffset": 113}, {"referenceID": 52, "context": "Vist3D [54, 59] is a domain-specific system for creating 3D animation of historical naval battles from narratives provided by the users.", "startOffset": 7, "endOffset": 15}, {"referenceID": 57, "context": "Vist3D [54, 59] is a domain-specific system for creating 3D animation of historical naval battles from narratives provided by the users.", "startOffset": 7, "endOffset": 15}, {"referenceID": 11, "context": "A different approach that relies on service-oriented and multi-agent design methodology is proposed in [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 51, "context": "It models the agents using NLP4INGENIAS [53], which is a multi-agent system based on the INGENIAS framework [58].", "startOffset": 40, "endOffset": 44}, {"referenceID": 56, "context": "It models the agents using NLP4INGENIAS [53], which is a multi-agent system based on the INGENIAS framework [58].", "startOffset": 108, "endOffset": 112}, {"referenceID": 40, "context": "The acquired agent models are fed to Alice [42]-a", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": "An adaptive animation generation system is introduced in Hassani and Lee [36].", "startOffset": 73, "endOffset": 77}, {"referenceID": 50, "context": "learning, has shown potentially promising results that can be applied to mitigate the mentioned challenges [52].", "startOffset": 107, "endOffset": 111}, {"referenceID": 42, "context": "As suggested by the study reported in [44], it is better to use a hybrid interface consisting of natu-", "startOffset": 38, "endOffset": 42}, {"referenceID": 9, "context": "Deep learning has shown promising results in machine vision, object recognition, speech recognition, and language modeling [10], and deep reinforcement learning has shown promising results in action learning [52].", "startOffset": 123, "endOffset": 127}, {"referenceID": 50, "context": "Deep learning has shown promising results in machine vision, object recognition, speech recognition, and language modeling [10], and deep reinforcement learning has shown promising results in action learning [52].", "startOffset": 208, "endOffset": 212}], "year": 2016, "abstractText": "A natural language interface exploits the conceptual simplicity and naturalness of the language to create a high-level user-friendly communication channel between humans and machines. One of the promising applications of such interfaces is generating visual interpretations of semantic content of a given natural language that can be then visualized either as a static scene or a dynamic animation. This survey discusses requirements and challenges of developing such systems and reports 26 graphical systems that exploit natural language interfaces and addresses both artificial intelligence and visualization aspects. This work serves as a frame of reference to researchers and to enable further advances in the field.", "creator": "LaTeX with hyperref package"}}}