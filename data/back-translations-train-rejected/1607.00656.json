{"id": "1607.00656", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2016", "title": "A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching", "abstract": "This article presents an agent architecture for controlling an autonomous agent in stochastic environments. The architecture combines the partially observable Markov decision process (POMDP) model with the belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent architecture takes the best features from the two approaches, that is, the online generation of reward-maximizing courses of action from POMDP theory, and sophisticated multiple goal management from BDI theory. We introduce the advances made since the introduction of the basic architecture, including (i) the ability to pursue multiple goals simultaneously and (ii) a plan library for storing pre-written plans and for storing recently generated plans for future reuse. A version of the architecture without the plan library is implemented and is evaluated using simulations. The results of the simulation experiments indicate that the approach is feasible.", "histories": [["v1", "Sun, 3 Jul 2016 17:11:52 GMT  (43kb,D)", "http://arxiv.org/abs/1607.00656v1", "26 pages, 3 figures, unpublished version"]], "COMMENTS": "26 pages, 3 figures, unpublished version", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["gavin rens", "deshendran moodley"], "accepted": false, "id": "1607.00656"}, "pdf": {"name": "1607.00656.pdf", "metadata": {"source": "CRF", "title": "A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning and Plan Caching", "authors": ["Gavin Rens", "Deshendran Moodley"], "emails": [], "sections": [{"heading": null, "text": "Keywords: autonomous agents, POMDP, BDI, satisfaction, plans, planning, memory"}, {"heading": "1 Introduction", "text": "This year, it is more than ever in the history of the city, in which it is so far that it is a place where it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "2 Preliminaries", "text": "The basic components of a BDI architecture (Wooldridge, 1999, 2002) are \u2022 a set or knowledge base B of beliefs; \u2022 an option generation function desire that generates the goals that the agent would ideally like to pursue (his wishes); \u2022 a set of wishes D (goals to be achieved); \u2022 a \"focus\" function that selects intentions from the set of wishes; \u2022 a structure of intentions that I have returned from the most desirable options / desires; \u2022 a library of plans and subplans that decides whether to invoke the focus function; \u2022 an execution procedure that influences the world according to the plan I have associated with conservation; \u2022 a perception procedure that gathers information about the state of the environment; and \u2022 a faith actualization function that updates the beliefs of the agent according to his latest observations and actions. Exactive in how these components are implemented in a particular BDI architecture."}, {"heading": "3 The Basic HPB Architecture", "text": "In BDI theory, one of the great challenges is knowing when the agent should change his current destination and what his new destination should be. (Schut et al., 2004) To meet this challenge, we propose that an agent should maintain intensity levels for each destination. This intensity of desire could be interpreted as a kind of emotion. To meet this challenge, the goals that are most intensely sought are the goals that the agent is aiming for. We also define the notion of how much an intention is fulfilled in the agent's current state of belief. Suppose that the agent is currently watching a movie and eating a snack. Then, these two goals become the agent's intentions. Food is not allowed inside the movie theater, and if the agent buys a snack, he would miss the beginning of the movie."}, {"heading": "3.1 Declarative Semantics", "text": "The state of an HPB agent is determined by the tuple < B, I >, where B is the current belief (i.e., a probability distribution over the states S that are defined below), D is the current desire of the agent, and I am the agent who has the attribute pairs (in short, the attribute is set).An HPB agent could be defined by the tuple < Atrb, G, Z, T, P, Util >, where the attribute pairs (in short, the attribute is set).For each (atrb: variety).Atrb"}, {"heading": "3.2 The Desire Function", "text": "The wish function D is a total function of targets in G into positive real numbers R +. The real number represents the intensity or degree of desire of the target. For example ({(BatryLevel: 13), (WeekDay: Tue), 2,2) could be in D, which means that the target to have the battery level at 13 and the weekday Tuesday with a level of 2.2 is desired. ({(BatryLevel: 33), 56) and ({(weekday: Wed), 444) are also examples of desires in D. I is the current intention of the agent; an element of G; the goal with the highest desired level. This goal is actively pursued by the agent and relegates the meaning of the other goals to the background. The fact that only one intention is maintained makes the HPB agent architecture very different from the default BDIAs.We propose updating the following wish update rule."}, {"heading": "3.3 Focusing and Satisfaction Levels", "text": "In the original version of the architecture, the chosen target is the one with the highest desired level. After each execution of an action in the real world, Refocus is called to decide whether to call focus to select a new intention. Refocus is a meta-reasoning function analogous to the verification function mentioned in Section 2. It is important to keep the agent focused on a target long enough to give it a reasonable chance to achieve it. It is the task of Refocus to detect when the current intention seems impossible or too expensive. Let Satf level be the sequence of satisfaction levels of the current intention since it became active, and let MRY be a number specified by the designer representing the length of a sub-sequence of satf levels - the final satisfaction level of MRY. One possible definition of Refocus is Refocus (c, 2001) def = \"no,\" if Satf-focus is not the next level of a good one, < Y if it is an average of a good one, then MRY is the next level of a good one; yes."}, {"heading": "3.4 Planning for the Next Action", "text": "A basic HPB agent controls his behavior according to the strategies he generates. Plan is a procedure that follows a POMDP policy of depth h. Essentially, we want to consider all sequences of action of length h and the states of belief in which the agent would find himself if he followed the sequences. Then, we want to select the sequence (or at least his first act) that yields the least cost and that ends in the state of belief that is most satisfactory in terms of intention. Plan S Satf (I, s) B (s) B (s) where Satf (I, s) is defined above and B (s) the probability that an agent receives satisfaction for an intention in his current state of belief is defined as Satf \u03b2 (I, B): = Plan S Satf (I, s) B (s) B (s) where Satf (I, s) is defined above and B (s) the probability that an agent in his current state of belief is defined as having an intention for MPB (the same as the intention for MPB)."}, {"heading": "4 The Extended HPB Architecture", "text": "The operational semantics of the extended architecture are essentially the same as with the first version, except that a plan library is now involved. The broker starts with a first set of intentions, a subset of its goals. For the current set of intentions, he must either select a plan from the plan library or create a plan to track all of his intentions. Each time the broker's control loop is repeated, an action is taken, an observation is made, the belief state is updated, and a decision is made whether to change the set of intentions. But only when the current policy (the conditional plan) is \"exhausted,\" the broker seeks a new policy by consulting his plan library, and if no appropriate policy is found, it is generated. In the next subsection, we introduce some new notations and changes to the architecture. Section 4.2 describes how the focus process needs to change to take account of the changes. Section 4.3 explains how guidelines for the simultaneous pursuit of multiple goals are generated."}, {"heading": "4.1 Prologue", "text": "The HPB agent gets three new components - a target-weight function W, a compatibility function Cpbl = Q, and the plan library Lbry. So it can be defined by the tuple < Atrb, G, W, Cpbl, A, Z, P, Util, Lbry. (In the previous version, satisfaction and preference were characterized by the \"trade-off factor,\" which was not explicitly mentioned in the agent model.) In fact, the trade-off factor should be part of the model because it must be provided by the agent designer, and it directly affects the agent's behavior. (In the new version, each target g-G is weighted to capture the agent's meaning. (G) The target weights are limited so that W (g) > 0 is available to all g-agent designers."}, {"heading": "4.2 A New Approach to Focusing", "text": "Given that I have a number of intentions, some careful consideration is required to ensure that the \"right\" objectives are intentions at the \"right\" time to ensure that the agent behaves as desired. It is still important to keep the agent focused on one intention long enough to give him a reasonable chance of pursuing intentions that he is difficult to achieve. HPB architecture has no focus function that returns a subset of G's intentions. Rather, we have a set of procedures that decide at each iteration which intention I should remove (if any) and which goal I should add (if any). Incompatibility also needs to be addressed."}, {"heading": "4.2.1 Over-optimistic Strategy", "text": "This strategy ignores compatibility problems between objectives. In this sense, the agent is (over) optimistic that he can successfully pursue objectives that are not compatible with each other at the same time. Add MI I I only when MI is added to I. When MI is added to I, delete the MI satisfaction values, that is, let Satf levels (MI) be the empty order."}, {"heading": "4.2.2 Compatibility Strategy", "text": "Add MI to I only if MI is 6 \"I\" and there is no g \"I,\" so that g \"6\" Cpbl \"(MI) is easy to change. If MI is added to I, delete the satisfaction values of MI,\" that is, let Satf values (MI) be the empty order. Next: For each g \"I,\" if | I | > 1 and Remove (g \"I\") there is a case that still needs to be dealt with in the compatibility strategy: let's say, for some g \"G,\" g \"6\" Cpbl (g). Furthermore, let's assume that I = \"g\" (i.e. I \"= 1) and g\" is and remains the most intensely desired target. Now g must not be added to I because it is incompatible with g, no other target is attempted to be added to I \"and g\" must not be removed as long as it is the only intention, even if it is average (I)."}, {"heading": "4.2.3 A New Desire Function", "text": "The old rule (in a new spelling) is still available: D (g) \u2190 D (g) + W (g) (1 \u2212 \u03c3g\u03b2 (B)). (3) We have established through experiments that when the desired levels of a desired goal are updated, non-intentional goals may not get the opportunity to become intentions. In other words, it may happen that when new intentional goals are considered intentions, they are always \"dominated\" by goals with higher desired levels that are already intentions. By prohibiting intention levels to increase, non-intentions get the opportunity to \"catch up\" their desired levels. A new form of the desired update rule is therefore proposed for this version of the architecture: D (g) \u2190 D (g) + (1 \u2212 i (I, g) W (g) (g)) (1 \u2212 g\u03b2 (B)) (4)."}, {"heading": "4.3 Planning by Policy Generation", "text": "In this section we will see how the planner can be expanded to calculate a policy that pursues several objectives at the same time (\u03b2 \u03b2) Q (\u03b2) Q (\u03b2). Target weights are also included in the value function of the State of Action. However, the satisfaction that an actor receives for an intention g in his current state of belief is defined as \u03c3g\u03b2 (B): The definition of the State of Action has the same form as the reward function in relation to the states of belief in the POMDP theory: \u03ba\u03b2 (a, B), with the faith (s) defined above and B (s) the probability that it is in the depth of the State. The main function used in the planning process is the reward function over the states of belief in the POMDP theory: \u03ba\u03b2 (a, B)."}, {"heading": "4.4 Introducing a Plan Library", "text": "s either be sentences. < (atrb: v), i.e., an attribute-value pair, (atrb: v), \u2022 \u03c6: a sentence is either fulfilled or true in a state s, let us write s:. The semantics of L is defined by \u2022 s true always, \u2022 s (atrb: v), (atrb: v), (atrb: v: s, \u2022 s: a sentence is fulfilled or true in a state s:. \"The semantics of L is defined by \u2022 s true always, \u2022 s: s (atrb: v), (atrb: v), (atrb: s)."}, {"heading": "5 Simulations", "text": "We have performed some tests on an HPB agent in two areas (1, 6), bl (6) (6) (1), Cpbl (6) (1) (3) and a three-battery system (3). In the experiments that follow, the threshold is set to 0.05, MRY is set to 5 and h = 3. Desire levels are initially set to zero for all targets. 10 attempts have been made for each experiment. The plan library is not used. In the Grid World, the agent's task is to visit each of the four corners and collect twelve items that are randomly scattered. Goals are {(1, 1), (1), (6, 1), (6, 6), collect, collect}, and (1, 6), collect, collect}, and (1), collect. The agent's task is to visit each of the four corners and (6, 6) are mutually incompatible. That is to collect, Cbl, collect, collect, collect (1), collect."}, {"heading": "5.1 Evaluation of HPB Agent Performance", "text": "Table 1 shows the results. It is clearly evident that the active ingredient can be steered into specific corners to collect items with a devotion proportional to the weights chosen by the active ingredient designer for the respective goals. Table 2: BatryPack system performance for different combinations of target weights, for mutually compatible (common) and incompatible (incompatible) objectives. Common Maintenance Weight 0.2 0.5 0.8Charge Weight 0.8 0.5 0.2% in the range 50 65.8 56.6% one / two intentions 19 / 81 18 / 82 58 / 42Disjoint Maintainweight 0.2 0.5 0.8Charge Weight 0.8 0.5 0.2% in the range 52 53.6 48.8% one / two intentions 100 / 0Table 2 shows the results. It is evident that the system performs better when the objectives are pursued jointly, especially when the packaging tension is maintained and attempts to charge the batteries. In the table, \"one / two intentions\" means that multiple intentions can be achieved for each of the four percent of the actual time, whereby 1% of the objectives can actually be achieved."}, {"heading": "6 Related Work", "text": "In fact, it is in such a way that one sees oneself in a position to live in a country in which most people are able to live in a country in which they are able to move, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live, to live"}, {"heading": "7 Conclusion", "text": "The idea behind it is that the people who are able to understand and understand the world, what they are doing in the world, to change and to change the world, to change the world, to change the world, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change, to change."}], "references": [{"title": "Using emotions to enhance decision-making", "author": ["D. Antos", "A. Pfeffer"], "venue": "Walsh, T., editor, Proceedings of the Twenty-second Intl. Joint Conf. on Artif. Intell. (IJCAI-11), pages 24\u201330, Menlo Park, CA. AAAI Press.", "citeRegEx": "Antos and Pfeffer,? 2011", "shortCiteRegEx": "Antos and Pfeffer", "year": 2011}, {"title": "Probabilistic planning in agentspeak using the pomdp framework", "author": ["K. Bauters", "K. McAreavey", "J. Hong", "Y. Chen", "W. Liu", "L. Godo", "C. Sierra"], "venue": "Hatzilygeroudis, I., Palade, V., and Prentzas, J., editors, Combinations of Intelligent Methods and Applications: Proceedings of the Fourth Intl. Workshop, CIMA 2014, volume 46 of Smart Innovation, Systems and Technologies. Springer.", "citeRegEx": "Bauters et al\\.,? 2015", "shortCiteRegEx": "Bauters et al\\.", "year": 2015}, {"title": "Decision-theoretic, high-level agent programming in the situation calculus", "author": ["C. Boutilier", "R. Reiter", "M. Soutchanski", "S. Thrun"], "venue": "Proceedings of the Seventeenth Natl. Conf. on Artif. Intell. (AAAI-00) and of the Twelfth Conf. on Innovative Applications of Artif. Intell. (IAAI-00), pages 355\u2013362. AAAI Press, Menlo Park, CA.", "citeRegEx": "Boutilier et al\\.,? 2000", "shortCiteRegEx": "Boutilier et al\\.", "year": 2000}, {"title": "Intention, Plans, and Practical Reason", "author": ["M. Bratman"], "venue": "Harvard University Press, Massachusetts/England.", "citeRegEx": "Bratman,? 1987", "shortCiteRegEx": "Bratman", "year": 1987}, {"title": "Learning to explore and exploit in pomdps", "author": ["C. Cai", "X. Liao", "L. Carin"], "venue": "NIPS, pages 198\u2013206.", "citeRegEx": "Cai et al\\.,? 2009", "shortCiteRegEx": "Cai et al\\.", "year": 2009}, {"title": "Incorporating PGMs into a BDI architecture", "author": ["Y. Chen", "J. Hong", "W. Liu", "L. Godo", "C. Sierra", "M. Loughlin"], "venue": "Boella, G., Elkind, E., Savarimuthu, B., Dignum, F., and Purvis, M., editors, PRIMA 2013: Principles and Practice of Multi-Agent Systems, volume 8291 of Lecture Notes in Computer Science, pages 54\u201369. Springer, Berlin/Heidelberg.", "citeRegEx": "Chen et al\\.,? 2013", "shortCiteRegEx": "Chen et al\\.", "year": 2013}, {"title": "I-divergence geometry of probability distributions and minimization problems", "author": ["I. Csisz\u00e1r"], "venue": "Annals of Probability, 3:146\u2013158.", "citeRegEx": "Csisz\u00e1r,? 1975", "shortCiteRegEx": "Csisz\u00e1r", "year": 1975}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["L. Kaelbling", "M. Littman", "A. Cassandra"], "venue": "Artif. Intell., 101(1\u20132):99\u2013134.", "citeRegEx": "Kaelbling et al\\.,? 1998", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Commitment and effectiveness of situated agents", "author": ["D. Kinny", "M. Georgeff"], "venue": "Proceedings of the 12th Intl. Joint Conf. on Artif. Intell. (IJCAI-91), pages 82\u201388.", "citeRegEx": "Kinny and Georgeff,? 1991", "shortCiteRegEx": "Kinny and Georgeff", "year": 1991}, {"title": "Experiments in optimal sensing for situated agents", "author": ["D. Kinny", "M. Georgeff"], "venue": "Proceedings of the the Second Pacific Rim Intl. Conf. on Artif. Intell. (PRICAI-92).", "citeRegEx": "Kinny and Georgeff,? 1992", "shortCiteRegEx": "Kinny and Georgeff", "year": 1992}, {"title": "Agent-centered search", "author": ["S. Koenig"], "venue": "Artif. Intell. Magazine, 22:109\u2013131.", "citeRegEx": "Koenig,? 2001", "shortCiteRegEx": "Koenig", "year": 2001}, {"title": "Information theory and statistics, volume 1", "author": ["S. Kullback"], "venue": "Dover, New York, 2nd edition.", "citeRegEx": "Kullback,? 1968", "shortCiteRegEx": "Kullback", "year": 1968}, {"title": "Towards solving large-scale POMDP problems via spatio-temporal belief state clustering", "author": ["X. Li", "W. Cheung", "J. Liu"], "venue": "Proceedings of IJCAI-05 Workshop on Reasoning with Uncertainty in Robotics (RUR-05).", "citeRegEx": "Li et al\\.,? 2005", "shortCiteRegEx": "Li et al\\.", "year": 2005}, {"title": "Improving adaptiveness in autonomous characters", "author": ["M. Lim", "J. Dias", "R. Aylett", "A. Paiva"], "venue": "Prendinger, H., Lester, J., and Ishizuka, M., editors, Intelligent Virtual Agents, volume 5208 of Lecture Notes in Computer Science, pages 348\u2013355. Springer, Berlin/Heidelberg.", "citeRegEx": "Lim et al\\.,? 2008", "shortCiteRegEx": "Lim et al\\.", "year": 2008}, {"title": "A survey of algorithmic methods for partially observed Markov decision processes", "author": ["W. Lovejoy"], "venue": "Annals of Operations Research, 28:47\u201366.", "citeRegEx": "Lovejoy,? 1991", "shortCiteRegEx": "Lovejoy", "year": 1991}, {"title": "Incorporating planning into BDI systems", "author": ["F. Meneguzzi", "A. Zorzo", "M. M\u00f3ra", "M.", "L."], "venue": "Scalable Computing: Practice and Experience, 8(1):15\u201328.", "citeRegEx": "Meneguzzi et al\\.,? 2007", "shortCiteRegEx": "Meneguzzi et al\\.", "year": 2007}, {"title": "A survey of partially observable Markov decision processes: Theory, models, and algorithms", "author": ["G. Monahan"], "venue": "Management Science, 28(1):1\u201316.", "citeRegEx": "Monahan,? 1982", "shortCiteRegEx": "Monahan", "year": 1982}, {"title": "Introduction to AI Robotics", "author": ["R. Murphy"], "venue": "MIT Press, Massachusetts/England.", "citeRegEx": "Murphy,? 2000", "shortCiteRegEx": "Murphy", "year": 2000}, {"title": "Hybrid bdi-pomdp framework for multiagent teaming", "author": ["R. Nair", "M. Tambe"], "venue": "J. Artif. Intell. Res.(JAIR), 23:367\u2013420.", "citeRegEx": "Nair and Tambe,? 2005", "shortCiteRegEx": "Nair and Tambe", "year": 2005}, {"title": "Real-time decision making for large POMDPs", "author": ["S. Paquet", "L. Tobin", "B. Chaib-draa"], "venue": "Advances in Artif. Intell.: Proceedings of the Eighteenth Conf. of the Canadian Society for Computational Studies of Intelligence, volume 3501 of Lecture Notes in Computer Science, pages 450\u2013455. Springer Verlag.", "citeRegEx": "Paquet et al\\.,? 2005", "shortCiteRegEx": "Paquet et al\\.", "year": 2005}, {"title": "Constructing bdi plans from optimal pomdp policies, with an application to agentspeak programming", "author": ["D. Pereira", "L. Gon\u00e7alves", "G. Dimuro", "A. Costa"], "venue": "G. Henning, M. G. and Goneet, S., editors, XXXIV Confer\u00eancia Latinoamericano de Inform\u00e1tica, Santa Fe. Anales CLEI 2008, pages 240\u2013249.", "citeRegEx": "Pereira et al\\.,? 2008", "shortCiteRegEx": "Pereira et al\\.", "year": 2008}, {"title": "Introducing the Tileworld: Experimentally evaluating agent architectures", "author": ["M. Pollack", "M. Ringuette"], "venue": "Proceedings of the Eighth Conf. on Artif. Intell., pages 183\u2013189. AAAI Press.", "citeRegEx": "Pollack and Ringuette,? 1990", "shortCiteRegEx": "Pollack and Ringuette", "year": 1990}, {"title": "AgentSpeak(L): BDI agents speak out in a logical computable language", "author": ["A. Rao"], "venue": "Proceedings of the 7th European Workshop on Modelling Autonomous Agents in a MultiAgent World (MAAMAW-96), pages 42\u201355, Berlin/Heidelberg. Springer Verlaag.", "citeRegEx": "Rao,? 1996", "shortCiteRegEx": "Rao", "year": 1996}, {"title": "BDI agents: From theory to practice", "author": ["A. Rao", "M. Georgeff"], "venue": "Proceedings of the ICMAS-95, pages 312\u2013319. AAAI Press.", "citeRegEx": "Rao and Georgeff,? 1995", "shortCiteRegEx": "Rao and Georgeff", "year": 1995}, {"title": "A BDI agent architecture for a POMDP planner", "author": ["G. Rens", "A. Ferrein", "E. Van der Poel"], "venue": "Lakemeyer, G., Morgenstern, L., and Williams, M.-A., editors, Proceedings of the Ninth Intl. Symposium on Logical Formalizations of Commonsense Reasoning (Commonsense 2009), pages 109\u2013114, University of Technology, Sydney. UTSe Press.", "citeRegEx": "Rens et al\\.,? 2009", "shortCiteRegEx": "Rens et al\\.", "year": 2009}, {"title": "Hybrid POMDP-BDI: An agent architecture with online stochastic planning and desires with changing intensity levels", "author": ["G. Rens", "T. Meyer"], "venue": "Duval, B., Van den Herik, J., Loiseau, S., and Filipe, J., editors, Proceedings of the Seventh Intl. Conf. on Agents and Artif. Intell. (ICAART), Revised Selected Papers, LNAI, pages 79\u201399. Springer Verlaag.", "citeRegEx": "Rens and Meyer,? 2015", "shortCiteRegEx": "Rens and Meyer", "year": 2015}, {"title": "Online planning algorithms for POMDPs", "author": ["S. Ross", "J. Pineau", "S. Paquet", "B. Chaib-draa"], "venue": "Journal of Artif. Intell. Research (JAIR), 32:663\u2013704.", "citeRegEx": "Ross et al\\.,? 2008", "shortCiteRegEx": "Ross et al\\.", "year": 2008}, {"title": "Finding approximate POMDP solutions through belief compressions", "author": ["N. Roy", "G. Gordon", "S. Thrun"], "venue": "Journal of Artif. Intell. Research (JAIR), 23:1\u201340.", "citeRegEx": "Roy et al\\.,? 2005", "shortCiteRegEx": "Roy et al\\.", "year": 2005}, {"title": "Intention reconsideration in complex environments", "author": ["M. Schut", "M. Wooldridge"], "venue": "Proceedings of the the Fourth Intl. Conf. on Autonomous Agents (AGENTS-00), pages 209\u2013216, New York, NY, USA. ACM. 25", "citeRegEx": "Schut and Wooldridge,? 2000", "shortCiteRegEx": "Schut and Wooldridge", "year": 2000}, {"title": "The control of reasoning in resource-bounded agents", "author": ["M. Schut", "M. Wooldridge"], "venue": "The Knowledge Engineering Review, 16(3):215\u2013240.", "citeRegEx": "Schut and Wooldridge,? 2001", "shortCiteRegEx": "Schut and Wooldridge", "year": 2001}, {"title": "The theory and practice of intention reconsideration", "author": ["M. Schut", "M. Wooldridge", "S. Parsons"], "venue": "Experimental and Theoretical Artif. Intell., 16(4):261\u2013293.", "citeRegEx": "Schut et al\\.,? 2004", "shortCiteRegEx": "Schut et al\\.", "year": 2004}, {"title": "Forward search value iteration for POMDPs", "author": ["G. Shani", "R. Brafman", "S. Shimony"], "venue": "de Mantaras, R. L., editor, Proceedings of the Twentieth Intl. Joint Conf. on Artif. Intell. (IJCAI-07), pages 2619\u20132624, Menlo Park, CA. AAAI Press.", "citeRegEx": "Shani et al\\.,? 2007", "shortCiteRegEx": "Shani et al\\.", "year": 2007}, {"title": "A survey of point-based pomdp solvers", "author": ["G. Shani", "J. Pineau", "R. Kaplow"], "venue": "Autonomous Agents and Multi-Agent Systems, 27(1):1\u201351.", "citeRegEx": "Shani et al\\.,? 2013", "shortCiteRegEx": "Shani et al\\.", "year": 2013}, {"title": "On the relationship between mdps and the bdi architecture", "author": ["G. Simari", "S. Parsons"], "venue": "Proceedings of the Fifth Intl. Joint Conf. on Autonomous Agents and Multiagent Systems, AAMAS \u201906, pages 1041\u20131048, New York, NY, USA. ACM.", "citeRegEx": "Simari and Parsons,? 2006", "shortCiteRegEx": "Simari and Parsons", "year": 2006}, {"title": "Markov Decision Processes and the Belief-Desire-Intention Model", "author": ["G. Simari", "S. Parsons"], "venue": "Springer Briefs in Computer Science. Springer, New York, Dordrecht, Heidelberg, London.", "citeRegEx": "Simari and Parsons,? 2011", "shortCiteRegEx": "Simari and Parsons", "year": 2011}, {"title": "Integrating learning into a BDI agent for environments with changing dynamics", "author": ["D. Singh", "S. Sardina", "L. Padgham", "G. James"], "venue": "Walsh, T., editor, Proceedings of the Twenty-Second Intl. Joint Conf. on Artif. Intell. (IJCAI-11), pages 2525\u20132530, Menlo Park, CA. AAAI Press.", "citeRegEx": "Singh et al\\.,? 2011", "shortCiteRegEx": "Singh et al\\.", "year": 2011}, {"title": "Augmenting BDI agents with deliberative planning techniques", "author": ["A. Walczak", "L. Braubach", "A. Pokahr", "W. Lamersdorf"], "venue": "Bordini, R., Dastani, M., Dix, J., and Seghrouchni, A., editors, Proceedings of the Fourth Intl. Workshop of Programming MultiAgent Systems (ProMAS-06), pages 113\u2013127, Heidelberg/Berlin. Springer Verlag.", "citeRegEx": "Walczak et al\\.,? 2007", "shortCiteRegEx": "Walczak et al\\.", "year": 2007}, {"title": "Intelligent agents", "author": ["M. Wooldridge"], "venue": "Weiss, G., editor, Multiagent Systems: A Modern Approach to Distributed Artif. Intell., chapter 1. MIT Press, Massachusetts/England.", "citeRegEx": "Wooldridge,? 1999", "shortCiteRegEx": "Wooldridge", "year": 1999}, {"title": "Reasoning about Rational Agents", "author": ["M. Wooldridge"], "venue": "MIT Press, Massachusetts/England.", "citeRegEx": "Wooldridge,? 2000", "shortCiteRegEx": "Wooldridge", "year": 2000}, {"title": "An introduction to multiagent systems", "author": ["M. Wooldridge"], "venue": "John Wiley & Sons, Chichester, England. 26", "citeRegEx": "Wooldridge,? 2002", "shortCiteRegEx": "Wooldridge", "year": 2002}], "referenceMentions": [{"referenceID": 3, "context": "The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991).", "startOffset": 63, "endOffset": 102}, {"referenceID": 23, "context": "The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991).", "startOffset": 63, "endOffset": 102}, {"referenceID": 16, "context": "The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991).", "startOffset": 163, "endOffset": 193}, {"referenceID": 14, "context": "The architecture combines belief-desire-intention (BDI) theory (Bratman, 1987; Rao and Georgeff, 1995) and partially observable Markov decision processes (POMDPs) (Monahan, 1982; Lovejoy, 1991).", "startOffset": 163, "endOffset": 193}, {"referenceID": 10, "context": "One solution to the intractability of POMDP policy generation is to employ a continuous planning strategy, or agent-centred search (Koenig, 2001).", "startOffset": 131, "endOffset": 145}, {"referenceID": 26, "context": "Aligned with agent-centred search is the forward-search approach or online planning approach in POMDPs (Ross et al., 2008).", "startOffset": 103, "endOffset": 122}, {"referenceID": 25, "context": "The Hybrid POMDP-BDI agent architecture (or HPB architecture, for short) has recently been introduced (Rens and Meyer, 2015).", "startOffset": 102, "endOffset": 124}, {"referenceID": 25, "context": "This article serves to introduce two significant extensions to the first iteration (Rens and Meyer, 2015) of the HPB architecture.", "startOffset": 83, "endOffset": 105}, {"referenceID": 3, "context": "See, for instance, Bratman (1987); Pollack and Ringuette (1990); Kinny and Georgeff (1991, 1992); Schut and Wooldridge (2000, 2001); Schut et al.", "startOffset": 19, "endOffset": 34}, {"referenceID": 3, "context": "See, for instance, Bratman (1987); Pollack and Ringuette (1990); Kinny and Georgeff (1991, 1992); Schut and Wooldridge (2000, 2001); Schut et al.", "startOffset": 19, "endOffset": 64}, {"referenceID": 3, "context": "See, for instance, Bratman (1987); Pollack and Ringuette (1990); Kinny and Georgeff (1991, 1992); Schut and Wooldridge (2000, 2001); Schut et al. (2004).", "startOffset": 19, "endOffset": 153}, {"referenceID": 7, "context": "Formally (Kaelbling et al., 1998), a POMDP is a tuple \u3008S,A, T,R, Z, P, b\u3009 with", "startOffset": 9, "endOffset": 33}, {"referenceID": 30, "context": "In BDI theory, one of the big challenges is to know when the agent should switch its current goal and what its new goal should be (Schut et al., 2004).", "startOffset": 130, "endOffset": 150}, {"referenceID": 25, "context": "A Hybrid POMDP-BDI (HPB) agent (Rens and Meyer, 2015) maintains (i) a belief state which is periodically updated, (ii) a mapping from goals to numbers representing the level of desire to achieve the goals, and (iii) the current set of intentions, the goals with the highest desire levels (roughly speaking).", "startOffset": 31, "endOffset": 53}, {"referenceID": 11, "context": "The \u2018directed divergence\u2019 (Kullback, 1968; Csisz\u00e1r, 1975) of belief state C from belief state B is defined as", "startOffset": 26, "endOffset": 57}, {"referenceID": 6, "context": "The \u2018directed divergence\u2019 (Kullback, 1968; Csisz\u00e1r, 1975) of belief state C from belief state B is defined as", "startOffset": 26, "endOffset": 57}, {"referenceID": 1, "context": "AgentSpeak (Bauters et al., 2015) extends the BDI language AgentSpeak (Rao, 1996) with on-demand probabilistic planning in uncertain environments.", "startOffset": 11, "endOffset": 33}, {"referenceID": 22, "context": ", 2015) extends the BDI language AgentSpeak (Rao, 1996) with on-demand probabilistic planning in uncertain environments.", "startOffset": 44, "endOffset": 55}, {"referenceID": 22, "context": "Please refer to (Rao, 1996) for details.", "startOffset": 16, "endOffset": 27}, {"referenceID": 20, "context": "It is not exactly clear from their paper (Pereira et al., 2008) how or when intentions are chosen.", "startOffset": 41, "endOffset": 63}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al.", "startOffset": 0, "endOffset": 1485}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations.", "startOffset": 0, "endOffset": 1513}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents.", "startOffset": 0, "endOffset": 1670}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents. However, their framework is very different to our architecture. They use POMDP theory to determine good role assignments of team members, not for generating policies online. Lim et al. (2008) provide a rather sophisticated architecture for controlling the behavior of an emotional agent.", "startOffset": 0, "endOffset": 1910}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents. However, their framework is very different to our architecture. They use POMDP theory to determine good role assignments of team members, not for generating policies online. Lim et al. (2008) provide a rather sophisticated architecture for controlling the behavior of an emotional agent. Their agents reason with several classes of emotion and their agents are supposed to portray emotional behavior, not simply to solve problems, but to look believable to humans. Their architecture has a \u201ccontinuous planner [...] that is capable of partial order planning and includes emotion-focused coping [...]\u201d Their work has a different application to ours, however, we could take inspiration from them to improve the HPB architecture. Pereira et al. (2008) take a different approach to use POMDPs to improve BDI agents.", "startOffset": 0, "endOffset": 2467}, {"referenceID": 1, "context": "Bauters et al. (2015) make use of only the first action of any POMDP policy. Online POMDP planners do forward-search to a given depth h (number of future actions). The deeper the look-ahead depth, the more optimal the actions in the policy. It might actually be a waste of computational resources to discard the whole policy of depth h once it is available. An agent could use its whole policy-tree and only generate a new policy after it has finished using the current policy to execute h actions. However, the actions closer to the end of the policy tree will tend to be farther from optimal than those closer to the tree\u2019s root. In future work, we would like to find ways to balance out the myopic take-first-action approach and the over-optimistic take-all-actions approach. AgentSpeak does not have a mechanism for storing and reusing generated policies. An advantage of AgentSpeak is that their written plans can be more expressive than HPB plans: elements of their plans are written in a language based on a fragment of first-order logic, including n-ary predicates and variable terms. Nonetheless, even though an HPB a-plan is propositional in nature (not relational), a policy has a reasonably expressive tree structure with branching conditional on observations of context sentences. A desirable feature that AgentSpeak plans have that HPB plans lack is the ability to call plans from within plans. Some slightly less related work will now be reviewed. Walczak et al. (2007) and Meneguzzi et al. (2007) have incorporated online plan generation into BDI systems, however the planners deal only with deterministic actions and observations. Nair and Tambe (2005) use POMDP theory to coordinate teams of agents. However, their framework is very different to our architecture. They use POMDP theory to determine good role assignments of team members, not for generating policies online. Lim et al. (2008) provide a rather sophisticated architecture for controlling the behavior of an emotional agent. Their agents reason with several classes of emotion and their agents are supposed to portray emotional behavior, not simply to solve problems, but to look believable to humans. Their architecture has a \u201ccontinuous planner [...] that is capable of partial order planning and includes emotion-focused coping [...]\u201d Their work has a different application to ours, however, we could take inspiration from them to improve the HPB architecture. Pereira et al. (2008) take a different approach to use POMDPs to improve BDI agents. By leveraging the relationship between POMDP and BDI models, as discussed by Simari and Parsons (2006), they devised an algorithm to extract BDI plans from optimal POMDP policies.", "startOffset": 0, "endOffset": 2633}, {"referenceID": 2, "context": "Although their basic approaches to combine the POMDP and BDI frameworks is the same as ours, there are at least three major differences: Firstly, they define their architecture in terms of the GOLOG agent language (Boutilier et al., 2000).", "startOffset": 214, "endOffset": 238}, {"referenceID": 22, "context": "Rens et al. (2009) also introduced a hybrid POMDP-BDI architecture, but without a notion of desire levels or satisfaction levels.", "startOffset": 0, "endOffset": 19}, {"referenceID": 2, "context": "Although their basic approaches to combine the POMDP and BDI frameworks is the same as ours, there are at least three major differences: Firstly, they define their architecture in terms of the GOLOG agent language (Boutilier et al., 2000). Secondly, their approach uses a computationally intensive method for deciding whether to refocus; performing short policy look-aheads to ascertain the most valuable goal to pursue. Our approach seems much more efficient. Thirdly, in their approach, the agent cannot pursue several goals concurrently. Chen et al. (2013) incorporate probabilistic graphical models into the BDI framework for plan selection in stochastic environments.", "startOffset": 215, "endOffset": 560}, {"referenceID": 25, "context": "In previous work (Rens and Meyer, 2015), we argued that maintenance goals like avoiding moist areas (or collecting soil samples) should rather be viewed as a preference and modeled as a POMDP reward function.", "startOffset": 17, "endOffset": 39}, {"referenceID": 17, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 27, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 19, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 12, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 31, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 26, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 4, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 32, "context": "A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated.", "startOffset": 99, "endOffset": 246}, {"referenceID": 13, "context": "Although Nair and Tambe (2005) and Chen et al.", "startOffset": 9, "endOffset": 31}, {"referenceID": 3, "context": "Although Nair and Tambe (2005) and Chen et al. (2013) call their approaches hybrid, our architecture can arguably more confidently be called hybrid because of its more intimate integration of POMDP and BDI concepts.", "startOffset": 35, "endOffset": 54}, {"referenceID": 0, "context": "We could take some advice from Antos and Pfeffer (2011). They provide a systematic methodology to incorporate emotion into a decision-theoretic framework, and also provide \u201ca principled, domain-independent methodology for generating heuristics in novel situations\u201d.", "startOffset": 31, "endOffset": 56}, {"referenceID": 0, "context": "We could take some advice from Antos and Pfeffer (2011). They provide a systematic methodology to incorporate emotion into a decision-theoretic framework, and also provide \u201ca principled, domain-independent methodology for generating heuristics in novel situations\u201d. Policies returned by Plan as defined in this paper are optimal. A major benefit of a POMDPbased architecture is that the literature on POMDP planning optimization (Murphy, 2000; Roy et al., 2005; Paquet et al., 2005; Li et al., 2005; Shani et al., 2007; Ross et al., 2008; Cai et al., 2009; Shani et al., 2013) (for instance) can be drawn upon to improve the speed with which policies can be generated. Evaluating the proposed architecture in richer domains would highlight problems in the architecture and indicate new directions for research and development in the area of hybrid POMDP-BDI architectures. The expressivity of the language we use for describing goals and for writing conditions in a-plans is relatively low. AgentSpeak, for instance, has a richer language. The language\u2019s expressivity is mostly independent of the architecture. We thus chose to use a simple language to better focus on the components we want to discuss. The design of the HPB agent architecture is a medium-to-long-term programme. We would like to keep improving its capabilities to deal with unforeseen, complex events in a changing, noisy environment. The next step is to rigorously test the architecture using an HPB agent in a complex simulated world. In particular, HPB agents with a plan library, including (pre-written) a-plans and (generated) b-plans, must still be assessed. There is also scope for improving the focussing procedure. And analyzing under what conditions the two forms of desire update rule produce better performance must be investigated. There may be better methods for learning than policy reuse. Policy reuse has its place when reasoning time or power is limited, but given the time and power, more sophisticated techniques could perhaps generate and store shorter, more effective plans. For instance, when an agent encounters a landmark with relatively high certainty, the landmark\u2019s location can be stored. The agent could then augment its sensor readings with the stored location data to reach the landmark more easily in future. Some objects in the environment might not be stable, and their location data should \u2018degrade\u2019 over time in proportion to the environment\u2019s dynamism. Singh et al. (2011) provide a method for learning which (pre-written) plans in a BDI system should be executed in which contexts (given a selection of context-applicable plans).", "startOffset": 31, "endOffset": 2480}], "year": 2016, "abstractText": "This article presents an agent architecture for controlling an autonomous agent in stochastic environments. The architecture combines the partially observable Markov decision process (POMDP) model with the belief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent architecture takes the best features from the two approaches, that is, the online generation of reward-maximizing courses of action from POMDP theory, and sophisticated multiple goal management from BDI theory. We introduce the advances made since the introduction of the basic architecture, including (i) the ability to pursue multiple goals simultaneously and (ii) a plan library for storing pre-written plans and for storing recently generated plans for future reuse. A version of the architecture without the plan library is implemented and is evaluated using simulations. The results of the simulation experiments indicate that the approach is feasible.", "creator": "LaTeX with hyperref package"}}}