{"id": "1607.07804", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Jul-2016", "title": "Error-Resilient Machine Learning in Near Threshold Voltage via Classifier Ensemble", "abstract": "In this paper, we present the design of error-resilient machine learning architectures by employing a distributed machine learning framework referred to as classifier ensemble (CE). CE combines several simple classifiers to obtain a strong one. In contrast, centralized machine learning employs a single complex block. We compare the random forest (RF) and the support vector machine (SVM), which are representative techniques from the CE and centralized frameworks, respectively. Employing the dataset from UCI machine learning repository and architectural-level error models in a commercial 45 nm CMOS process, it is demonstrated that RF-based architectures are significantly more robust than SVM architectures in presence of timing errors due to process variations in near-threshold voltage (NTV) regions (0.3 V - 0.7 V). In particular, the RF architecture exhibits a detection accuracy (P_{det}) that varies by 3.2% while maintaining a median P_{det} &gt; 0.9 at a gate level delay variation of 28.9% . In comparison, SVM exhibits a P_{det} that varies by 16.8%. Additionally, we propose an error weighted voting technique that incorporates the timing error statistics of the NTV circuit fabric to further enhance robustness. Simulation results confirm that the error weighted voting achieves a P_{det} that varies by only 1.4%, which is 12X lower compared to SVM.", "histories": [["v1", "Sun, 3 Jul 2016 16:34:24 GMT  (1479kb,D)", "http://arxiv.org/abs/1607.07804v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sai zhang", "naresh shanbhag"], "accepted": false, "id": "1607.07804"}, "pdf": {"name": "1607.07804.pdf", "metadata": {"source": "CRF", "title": "Error-Resilient Machine Learning in Near Threshold Voltage via Classifier Ensemble", "authors": ["Sai Zhang", "Naresh Shanbhag"], "emails": ["szhang12@illinois.edu", "shanbhag@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "In practice, however, machine learning algorithms play an important role in enabling in-situ data analytics that include energy-related embedding platforms, and this stringent energy constraint excludes the use of universal hardware platforms, resulting in significant energy costs, leading to great interest in the dedicated implementation of integrated circuits for machine learning [4, 11]. These implementations have shown that they achieve a reduction in energy for Conventional Neural Networking Workshop visions [4] and a 5.2-fold throughput enhancement enhancement for a nearest machine (KNN) engine [11] compared to implementations on universal platforms."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Classifier Ensemble (CE)", "text": "A variety of CE methods exist. Bootstrap aggregation (bagging) [2] generates multiple training sets from the original training set by randomly scanning with a replacement to train multiple classifiers. Adaboost [9] is another popular method for ensemble generation. Training samples are re-weighted after each iteration, giving higher weights to the incorrectly classified samples. Other methods such as random injection, random subspace [10] and output coding [6] also exist. RF is a CE method that combines random subspace and bagging, while using an ensemble of decision trees (DTs) as weak classifiers. It is a popular technique for classification, prediction, variable selection, and has superior results compared to other linear and non-linear prediction techniques [3]."}, {"heading": "2.2 Support Vector Machine", "text": "Support vector machine (SVM) [5] is a popular supervised learning method for classification and regression. SVM works first with a model (the training phase), followed by test / classification (the testing phase).During the training phase, labeled feature vectors are used to train a model.During the testing phase, SVM produces a predictive label when it is provided with a new (test) feature vector. SVM training can be formulated as a solution to the following optimization problem: min 12, w, 2 + C, i, i, s.t. ci (wTxi \u2212 b) \u2265 1 \u2212 \u0448i, where C is the cost factor, \u0448i is the soft edge, xi the feature vector, ci the label corresponding to the feature vector xi, w the weight vector and b the bias. It can be shown that the optimal weights are represented as a linear combination of feature vectors that are related to the support vectors, i.e."}, {"heading": "2.3 Computational Error Model", "text": "The probability of such an error event increases dramatically if circuits are operated in NTV [8], i.e., supply voltages 0.3 V \u2264 Vdd \u2264 0.7 V. The resulting time errors are a complex function of the circuit state, inputs, architecture and process technology. Therefore, a statistical model of such errors is indispensable to understand the effects of errors on system performance.In this paper, the calculated output and time errors are treated as random variables (RVs).We use upper and lower case letters to name an RV Y and its realization y. We use the following additive error model [19]: ya = yo \u00b2 s (1), where ya = [yba, 0,.... y b, B \u2212 1] T is the observed B \u2212 bit (erroneous) output of a pipeline stage, which is also a realization of the RV vector."}, {"heading": "3 System Architecture", "text": "In this section, we present system architectures for RF and SVM classifiers. RF classifiers are selected as an implemented CE architecture based on their comparison-based architecture, resulting in a simple basic-learner architecture. Polynomial SVM is chosen as the implemented central architecture because it offers a good trade-off between decision flexibility and hardware complexity [12]."}, {"heading": "3.1 The RF Architecture", "text": "Stage 1 of the lth-DT consists of a comparator array that calculates sgn (xl, i \u2212 Tl, i). (l = 1, 2,.) The L-dimensional test data vector x (M Ml). Stage 1 of the lth-DT consists of a comparator array that calculates sgn (xl, i \u2212 Tl, i). (l = 1, 2,.) The L-dimensional test data vector x (M Ml), where the thresholds are reached via training. Stage 2 consists of a table (LUT) that converts the decisions of the individual root-to-Lef paths into a Bit-1 output."}, {"heading": "3.2 The SVM Architecture", "text": "The centralized machine learning algorithm used in this thesis is a second-order polynomial, where x = [x1, x2,..., xM] T is the M-dimensional test data vector, si = [s1, s2,..., sM] T is the ith support vector, \u03b1i is the weight associated with si, b the bias, \u03b2 and \u03b3 are the parameters of the polynomic core, andN the total number of support vectors (typically M). Direct compression of (14) requiresO (NM) multiplies-accumulated (MAC) operations. The following reformulation [12] reduces the number of MAC operations to O (M2): ya = x x TW + b (15) W = N = i-rite (MAC) operations where the weight (1 x) and the compression (1 x) are between 1 and 3 dW."}, {"heading": "3.3 System Analysis", "text": "The potential improvement in robustness achieved by RF can be analysed by examining the generalised error E [(C \u2212 1L \u2211 L = 1 Y \u0441a, l) 2], where C is the designation and 1L \u2211 L = 1 Y \u0441a, l is the generalised error, where equilibrium is assumed by the selector for the sake of simplicity. Here, the expectation about the distribution of the marking C, the training set S and the timing error N1,..., NL is adopted. We start by deriving the generalised error for a single DT, which is defined as E [(C \u2212 Y \u0441a) 2], where Y \u0441a is the DT output. It can be shown that (see Appendix A): E [(C \u2212 Y \u0432 a) 2] = \u03c32C + b2 + \u03c32 Y + \u0441\u0442a (16), where \u04412C = E [(C \u2212 E] 2] is the irreducible error (noise)."}, {"heading": "4 Simulation Results", "text": "This section begins with the validation of the timing error model of Section 2.3 in a commercial 45 nm CMOS process. These timing error models are derived for the RF and SVM classifiers of Picture 2 (a) and Picture 3, respectively. Next, in Section 4.2, the detection accuracy of the SVM and RF architectures is compared using the validated error models. We use the Breast Cancer Wisconsin dataset from the machine learning repository [1], which consists of labeled feature vectors (benign vs. malicious) constructed from digitized images of fine needle aspirates (FNA) from patient tissue. The SVM architecture used in this study consists of two types of MACs: Stage 1 employs 8 b-input, 8 b-coefficient, and Stage 2 employs 10 b-input, where the RACs input, 8 b-efficiency, and the RACs input input-ACs input-ACs-4 input-input-ACs-input-input-input-input-input-input-input-input-ACs-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-ACs-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input-input"}, {"heading": "4.1 Model Generation Methodology and Validation", "text": "Error model generation and validation methodology is illustrated in Figure 4 (a) and described below: 1. Characterise the gate delay distribution vs. operating voltage Vdd of the basic gates using HSPICE in the NTV range 0.3 V-0.7 V.2. Implement the SVM and RF architectures shown in Figure 3 and Figure 2 (a), and use the structural Verilog HDL simulations characterized in step 1.3 \u2212 \u2212 d \u2212. Emulate process variations on the NTV by generating multiple (30) architectural instances of each type (SVM and RF) and mapping random gate delays obtained by sampling the gate delay distributions obtained in step 1 \u2212 d \u2212 statistical distributions."}, {"heading": "4.2 Comparison of SVM and RF", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.2.1 Comparison of timing error rates", "text": "First, we compare the timing error rates of SVM and RF, which were determined using HDL simulations that decrease the voltage in the NTV. Figure 5 (a) shows that the mean timing error rate p-ig increases by 500 \u00b7 from 2.1 \u00b7 10 \u2212 3 to 0.99 and from 1.1 \u00b7 10 \u2212 3 to 0.61 for SVM and RF, respectively, as the voltage Vdd decreases by 0.7 V to 0.3 V compared to the MAC units in the SVM, suggesting that the RF architecture has a timing error rate up to 4.5 times lower than SVM. The error rate of the RF architecture is lower because it has comparison blocks that have a much simpler data path compared to the MAC units in the SVM. Figure 5 (a) also shows that the gate level delay variation (\u03c3 / \u00b5) d increases by 12 \u00d7 from 2.8% to 33% as the voltage decreases from V7 dd V to 0.3 V."}, {"heading": "4.2.2 Comparison of p\u0304det", "text": "Figure 5 (b) shows that RF has a higher p-det than SVM if the ensemble size L is sufficiently large. Specifically, RF-M is able to maintain p-det \u2265 0.9 for (\u03c3 / \u00b5) d \u2264 28.9% with L = 10, while SVM can maintain the same performance only for (\u03c3 / \u00b5) d \u2264 11.7%. Furthermore, RF-EW achieves up to 3% higher p-det compared to RF-W and RF-EW and is able to maintain p-det \u2265 0.9 for (\u03c3 / \u00b5) d \u2264 29.6%. Finally, Figure 5 (b) further shows that RF with L = 10 is able to maintain p-det \u2265 0.9 even at (\u03c3 / \u00b5) d of 28.9%, suggesting that RF architectures exhibit higher robustness against time errors than SVM, although their complexity is lower at L = 10%."}, {"heading": "4.2.3 Comparison of \u03c3pdet", "text": "Figure 5 (c) shows that \u03c3pdet is significantly reduced when L increases. RF-M reaches \u03c3pdet \u2264 3.5 \u00d7 10 \u2212 4 when L = 10, which is 5x lower than SVM or RF-M with L = 1. This also shows that distributed architectures are inherently more robust against timing errors than centralized ones. Figure 5 (c) also shows that RF-EW increases robustness at (\u03c3 / \u00b5) d \u2264 29.6%, which is 12x3.5 x lower than SVM or RF-W. This shows that the inclusion of timing error statistics in the decision process increases robustness. If (\u03c3 / \u00b5) d \u2265 30%, \u03c3pdet of RF-EW is higher than that of RF-M and RF-W, because all instances of RF-M and RF-W reach a low Pdet accuracy, while some cases of RF-EW can reach a higher Pdet diversity than that of RF-16, resulting in a reduction."}, {"heading": "5 Conclusion", "text": "This paper compares the inherent robustness of CE and centralized machine learning architectures in the event of time violations, demonstrates that distributed architectures in which CE is used are inherently more robust than centralized ones, demonstrates that the algorithm itself can be customized to further increase robustness, achieves such an improvement through error-weighted coordination during decision combination and the use of precision diversity in the architectural data path, and demonstrates that the CE framework can integrate information at the architectural level at the system level to achieve increased robustness. In the future, techniques can be used to improve the robustness of CE at the architectural and algorithmic levels, and assess the robustness of CE in the event of errors (\"stuck-at-faults\")."}, {"heading": "Acknowledgment", "text": "This work was partially supported by Systems on Nanoscale Information fabriCs (SONIC), one of six SRC STARnet centers sponsored by MARCO and DARPA."}], "references": [{"title": "Bagging predictors", "author": ["L. Breiman"], "venue": "Mach. Learn., 24(2):123\u2013140, Aug.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1996}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Mach. Learn., 45(1):5\u201332, Oct.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks", "author": ["Y.H. Chen", "T. Krishna", "J. Emer", "V. Sze"], "venue": "ISSCC", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Support-vector networks", "author": ["C. Cortes", "V. Vapnik"], "venue": "Mach. Learn., 20(3):273\u2013297, Sept.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Ensemble methods in machine learning", "author": ["T.G. Dietterich"], "venue": "Proceedings of the First International Workshop on Multiple Classifier Systems, MCS \u201900, pages 1\u201315, London, UK, UK,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2000}, {"title": "A unified bias-variance decomposition and its applications", "author": ["P. Domingos"], "venue": "In proc. 17th International Conf. on Machine Learning, pages 231\u2013238. Morgan Kaufmann,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2000}, {"title": "Near-threshold computing: Reclaiming moore\u2019s law through energy efficient integrated circuits", "author": ["R. Dreslinski", "M. Wieckowski", "D. Blaauw", "D. Sylvester", "T. Mudge"], "venue": "Proceedings of the IEEE, 98(2):253\u2013266, Feb", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Experiments with a new boosting algorithm", "author": ["Y. Freund", "R.E. Schapire"], "venue": "L. Saitta, editor, Proceedings of the Thirteenth International Conference on Machine Learning (ICML 1996), pages 148\u2013156. Morgan Kaufmann,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "The random subspace method for constructing decision forests", "author": ["T.K. Ho"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., 20(8):832\u2013844, Aug.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "A 21.5M-query-vectors/s 3.37nJ/vector reconfigurable k-nearestneighbor accelerator with adaptive precision in 14nm tri-gate CMOS", "author": ["H. Kaul", "M.A. Anders", "S.K. Mathew", "G. Chen", "S.K. Satpathy", "S.K. Hsu", "A. Agarwal", "R.K. Krishnamurthy"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Low-energy formulations of support vector machine kernel functions for biomedical sensor applications", "author": ["K. Lee", "S. Kung", "N. Verma"], "venue": "Signal Processing Systems, 69(3):339\u2013349,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Generating spike trains with specified correlation coefficients", "author": ["J.H. Macke", "P. Berens", "A.S. Ecker", "A.S. Tolias", "M. Bethge"], "venue": "Neural Computation, 21(2):397\u2013423, Feb", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Programs for Machine Learning", "author": ["J.R. Quinlan"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Beyond charge-based computation: Boolean and non-boolean computing with spin torque devices", "author": ["K. Roy", "M. Sharad", "D. Fan", "K. Yogendra"], "venue": "Low Power Electronics and Design (ISLPED), 2013 IEEE International Symposium on, Sept", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Approximate computing and the quest for computing efficiency", "author": ["S. Venkataramani", "S.T. Chakradhar", "K. Roy", "A. Raghunathan"], "venue": "2015 52nd ACM/EDAC/IEEE Design Automation Conference (DAC), pages 1\u20136, June", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Enabling system-level platform resilience through embedded data-driven inference capabilities in electronic devices", "author": ["N. Verma", "K.H. Lee", "K.J. Jang", "A. Shoeb"], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5285\u20135288, March", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Carbon nanotube circuits: Opportunities and challenges", "author": ["H. Wei", "M. Shulaker"], "venue": "DATE", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Probabilistic error models for machine learning kernels implemented on stochastic nanoscale fabrics", "author": ["S. Zhang", "N. Shanbhag"], "venue": "Design, Automation Test in Europe Conference Exhibition (DATE),", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "This stringent energy constraint precludes the use of general-purpose hardware platforms, resulting in much interest in dedicated integrated circuit implementation of machine learning kernels [4, 11].", "startOffset": 192, "endOffset": 199}, {"referenceID": 9, "context": "This stringent energy constraint precludes the use of general-purpose hardware platforms, resulting in much interest in dedicated integrated circuit implementation of machine learning kernels [4, 11].", "startOffset": 192, "endOffset": 199}, {"referenceID": 2, "context": "These implementations have shown to achieve a 252\u00d7 energy reduction for convolutional neural networkbased vision system [4] and a 5.", "startOffset": 120, "endOffset": 123}, {"referenceID": 9, "context": "2\u00d7 throughput enhancement for a k-nearest-neighbor (KNN) engine [11] as compared to implementations on general-purpose platforms.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "Conventionally, energy consumption of machine learning implementations is minimized by reducing the computational complexity, precision, and data movement [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 6, "context": "Examples of such circuit fabrics include near threshold voltage (NTV) CMOS [8], and emerging devices such as CNFET [18] and spin [15].", "startOffset": 75, "endOffset": 78}, {"referenceID": 16, "context": "Examples of such circuit fabrics include near threshold voltage (NTV) CMOS [8], and emerging devices such as CNFET [18] and spin [15].", "startOffset": 115, "endOffset": 119}, {"referenceID": 13, "context": "Examples of such circuit fabrics include near threshold voltage (NTV) CMOS [8], and emerging devices such as CNFET [18] and spin [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 6, "context": "For example, it has been shown that NTV operation achieves up to 10\u00d7 energy savings, but leads to increased delay variations as large as 14\u00d7 [8].", "startOffset": 141, "endOffset": 144}, {"referenceID": 13, "context": "Hardware errors are expected to be common place in scaled or beyond CMOS processes [15, 18], and there is great interest in understanding ar X iv :1 60 7.", "startOffset": 83, "endOffset": 91}, {"referenceID": 16, "context": "Hardware errors are expected to be common place in scaled or beyond CMOS processes [15, 18], and there is great interest in understanding ar X iv :1 60 7.", "startOffset": 83, "endOffset": 91}, {"referenceID": 15, "context": "However, the computational complexity of centralized architecture increases dramatically as a function of the non-linearity of the decision boundary [17].", "startOffset": 149, "endOffset": 153}, {"referenceID": 17, "context": "Specifically, we compare a CE method random forest (RF) - with SVM using architectural-level error models [19] in 45 nm CMOS.", "startOffset": 106, "endOffset": 110}, {"referenceID": 4, "context": "Classifier ensemble (also referred to as Multiple Classifier System) has been employed to enhance the performance of single classifier system [6].", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "In bootstrap aggregating (bagging) [2], multiple training sets are generated from the original training set via random sampling with replacement, in order to train multiple classifiers.", "startOffset": 35, "endOffset": 38}, {"referenceID": 7, "context": "Adaboost [9] is another popular method for ensemble generation.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Other methods such as randomness injection, random subspace [10] and output coding [6] also exist.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "Other methods such as randomness injection, random subspace [10] and output coding [6] also exist.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "It is a popular technique for classification, prediction, variable selection, and has shown superior results compared to other linear and non-linear predictive modeling techniques [3].", "startOffset": 180, "endOffset": 183}, {"referenceID": 1, "context": "Classification and regression tree (CART) [3] employs the Gini index as a measure of the impurity of nodes.", "startOffset": 42, "endOffset": 45}, {"referenceID": 12, "context": "ID3 [14] employs information gain as the criterion.", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "5 [14] improves ID3 by using the information gain ratio.", "startOffset": 2, "endOffset": 6}, {"referenceID": 3, "context": "Support vector machine (SVM) [5] is a popular supervised learning method for classification and regression.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Kernel trick [5] can be employed to realize non-linear decision boundaries.", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "The probability of such an error event increases dramatically when circuits are operating in NTV [8], i.", "startOffset": 97, "endOffset": 100}, {"referenceID": 17, "context": "We employ the following additive error model [19]: ya = yo \u2295 \u03b7 (1) where ya = [y a,0, .", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": ", uB\u22121] of RV U , 2) employ the dichotomized Gaussian (DG) approximation [13] to generate \u03b7 i from ui as follows:", "startOffset": 73, "endOffset": 77}, {"referenceID": 10, "context": "The polynomial SVM is chosen as the implemented centralized architecture as it offers a good trade-off between decision boundary flexibility and hardware complexity [12].", "startOffset": 165, "endOffset": 169}, {"referenceID": 1, "context": "Each DT is trained using the Gini index [3] as the training criterion.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "When error rate p\u03b7l = 0, the error weighted voting scheme reduces to the conventional weighted voter [6] where pl = P (Rl|\u03b7l = 0).", "startOffset": 101, "endOffset": 104}, {"referenceID": 10, "context": "The following reformulation [12] reduces the number of MAC operations to O(M): ya = x\u0303 W\u0303x\u0303 + b (15)", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "3 [19].", "startOffset": 2, "endOffset": 6}, {"referenceID": 1, "context": "Four architectures are compared: 1) SVM, 2) RF with majority voter [3] (RF-M), 3) RF with weighted majority voter [6] (RF-W), and 4) RF with the proposed error weighted voter (RF-EW).", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "Four architectures are compared: 1) SVM, 2) RF with majority voter [3] (RF-M), 3) RF with weighted majority voter [6] (RF-W), and 4) RF with the proposed error weighted voter (RF-EW).", "startOffset": 114, "endOffset": 117}], "year": 2016, "abstractText": "In this paper, we present the design of error-resilient machine learning architectures by employing a distributed machine learning framework referred to as classifier ensemble (CE). CE combines several simple classifiers to obtain a strong one. In contrast, centralized machine learning employs a single complex block. We compare the random forest (RF) and the support vector machine (SVM), which are representative techniques from the CE and centralized frameworks, respectively. Employing the dataset from UCI machine learning repository and architecturallevel error models in a commercial 45 nm CMOS process, it is demonstrated that RF-based architectures are significantly more robust than SVM architectures in presence of timing errors due to process variations in near-threshold voltage (NTV) regions (0.3 V 0.7 V). In particular, the RF architecture exhibits a detection accuracy (Pdet) that varies by 3.2% while maintaining a median Pdet \u2265 0.9 at a gate level delay variation of 28.9% . In comparison, SVM exhibits a Pdet that varies by 16.8%. Additionally, we propose an error weighted voting technique that incorporates the timing error statistics of the NTV circuit fabric to further enhance robustness. Simulation results confirm that the error weighted voting achieves a Pdet that varies by only 1.4%, which is 12\u00d7 lower compared to SVM.", "creator": "LaTeX with hyperref package"}}}