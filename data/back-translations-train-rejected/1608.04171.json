{"id": "1608.04171", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Power Data Classification: A Hybrid of a Novel Local Time Warping and LSTM", "abstract": "As many applications organize data into temporal sequences, the problem of time series data classification has been widely studied. Recent studies show that the 1-nearest neighbor with dynamic time warping (1NN-DTW) and the long short term memory (LSTM) neural network can achieve a better performance than other machine learning algorithms. In this paper, we build a novel time series classification algorithm hybridizing 1NN-DTW and LSTM, and apply it to a practical data center power monitoring problem. Firstly, we define a new distance measurement for the 1NN-DTW classifier, termed as Advancing Dynamic Time Warping (ADTW), which is non-commutative and non-dynamic programming. Secondly, we hybridize the 1NN-ADTW and LSTM together. In particular, a series of auxiliary test samples generated by the linear combination of the original test sample and its nearest neighbor with ADTW are utilized to detect which classifier to trust in the hybrid algorithm. Finally, using the power consumption data from a real data center, we show that the proposed ADTW can improve the classification accuracy from about 84\\% to 89\\%. Furthermore, with the hybrid algorithm, the accuracy can be further improved and we achieve an accuracy up to about 92\\%. Our research can inspire more studies on non-commutative distance measurement and the hybrid of the deep learning models with other traditional models.", "histories": [["v1", "Mon, 15 Aug 2016 02:49:17 GMT  (3451kb,D)", "http://arxiv.org/abs/1608.04171v1", null], ["v2", "Wed, 17 Aug 2016 12:51:46 GMT  (3451kb,D)", "http://arxiv.org/abs/1608.04171v2", null], ["v3", "Sat, 8 Oct 2016 04:33:02 GMT  (3459kb,D)", "http://arxiv.org/abs/1608.04171v3", null], ["v4", "Wed, 7 Jun 2017 04:34:06 GMT  (3457kb,D)", "http://arxiv.org/abs/1608.04171v4", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["yuanlong li", "han hu", "yonggang wen", "jun zhang"], "accepted": false, "id": "1608.04171"}, "pdf": {"name": "1608.04171.pdf", "metadata": {"source": "CRF", "title": "Power Series Classification: A Hybrid of LSTM and a Novel Advancing Dynamic Time Warping", "authors": ["Yuanlong Li", "Yonggang Wen"], "emails": ["ygwen}@ntu.edu.sg.", "zhang@ieee.org."], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. RELATED WORKS", "text": "The problem is that each country is a country where it is a country where most people are unable to feed themselves, where they live."}, {"heading": "B. LSTM", "text": "LSTM is first proposed by Hochreiter and Gers et al. as an upgrade of the recursive neural network (RNN) [13]. RNN is used to process sequential data with a special calculation process after the time step, while the traditional neural network only treats the sequence as a pure vector. Due to this nature, RNN is suitable for modeling sequential data. However, it suffers from a problem called decreasing gradient caused by the iterative process on the timeline, making the gradient used in the training process extremely small and causing training errors. To solve the problem, the LSTM is proposed and uses a memory core to avoid the decreasing gradient. Details of the neural network LSTM are presented in Section IV.LSTM. It has demonstrated great modeling power for sequential data and has been successfully used in various machine learning fields such as natural language use (NSTM), video analysis, and STP can also be used for [ST15] discrimination."}, {"heading": "III. POWER SERIES DATA COLLECTION AND PRELIMINARY ANALYSIS", "text": "In this section, we present the power series data we collect, followed by a preliminary analysis of the data. We will explain in detail the rules of the simulation design for data collection and data samples collected with a pre-treatment. The proposed preliminary analysis includes visualization of the data using various dimension reduction methods, classification results with some canonical classifiers, and a feature study."}, {"heading": "A. Power Series Data Collection", "text": "In this sense, our guideline for the collection of performance data is \"different\" and \"similar.\" The consequence is that the performance sequence has to be generated by different programs. By using \"similar,\" the different programs can also look in such a way that the classification algorithms really have to be discriminated against."}, {"heading": "B. Preliminary Analysis", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "IV. THE PROPOSED POWER SERIES CLASSIFICATION ALGORITHM", "text": "In this section, we present the proposed new power series classification algorithm, which hybridizes a next neighbor classifier with a novel distance measurement and an LSTM classifier. In the following, we present the two components and the hybrid algorithm respectively."}, {"heading": "A. Nearest Neighbour with the Advancing Dynamic Time Warping (ADTW)", "text": "We propose a new classifier that uses a new type of distance measurement to calculate the distance between two sequences, which we call Advancing Dynamic Time Warping (ADTW), because its calculation can be done with the time step that proceeds from beginning to end (for several rounds), which differs from the dynamic programming process in 5DTW. ADTW is designed to replace the DTW distance measurement in the 1NN-DTW classification, and the ADTW distance measurement is calculated in the following way. Suppose we have two sequences x and y, both of length n. We define the order k ADTW distance between x and y as: ADTWk distance i = 1 n \u2212 i \u2212 1 expressed as j = 0 min (| xj \u2212 yj the distance to the next x)."}, {"heading": "B. Long Short Term Memory Neural Network", "text": "We use the LSTM classifier according to [19] for our potential series classification problem. The neural LSTM network consists of an input layer, an LSTM layer, and a logistic regression layer, as shown in Fig. 3. The three layers then function in the following way: \u2022 Input layer: The input data collection, which is a length vector x, is first discredited into the range [0, S]. Such an operation is a smoothing operation of the original potential series, which can influence the performance of the LSTM. Then, each time step xt, which is a length vector x, is n \u2212 1 enriched intoa m-dimensional vector xt, which can facilitate the following calculation, i.e. xt = xt \u00b7 e, where e is an m-dimensional vector with all entries equal to 1. After the above process, the new sequence x0, x1,... xn-dimensional vector is used as input class M-ST1."}, {"heading": "C. Hybridization of LSTM and 1NN-ADTW", "text": "In this subsection, we propose to combine a method that does not require an increase in time. Underlying logic is that both types can achieve a high classification accuracy for our problem, but in very different manners: the 1NN-ADTW is a close neighbor that has a data-based classification method; while LSTM is a classification-based model in which the training data is first used to build a model and then the model is used to classify the test data. In our experiments, both classifiers can perform well, but our numerical simulation shows that the accurately classified samples from the two classifiers show significant differences. In this sense, we propose to combine the two algorithms to construct an even stronger classification."}, {"heading": "V. NUMERICAL EVALUATION AND ANALYSIS", "text": "In this section we present the experimental results of the above proposed algorithms and the analysis. We compare the classification accuracy of the proposed 1NN-ADTW, LSTM and their hybrid algorithm ASTM / ADTW with the 7Baseline algorithm 1NN-DTW. For the baseline algorithm 1NN-DTW, the maximum distortion r is set manually and set to 0.15 * n, where n is the length of the power series sample. For the 1NN-ADTW, the order of the ADTW distance is set to 8, and we will show analyses of the effects of the order. For the neural network LSTM, we set the maximum number of epochs to 100. For some key parameters that may affect the performance of the LSTM, we give a detailed explanation in the following parameter setting study. Test data and codes ADTW tests are available at: https / bropbbboxbox7 / www.bSTL7 / bSTL7."}, {"heading": "A. The Classification Accuracy Rate Comparison", "text": "The fivefold classification accuracy results for different algorithms are shown in Table IV. For convenience, we simply use Test Fold i to denote the test with test samples in Fi and F (i + 1)%. From Table IV, we can note the following: \u2022 The proposed 1NN-ADTW method performs better than 1NN-DTW in all five tests, proving that the proposed ADTW test is better suited to our problem of power series classification. \u2022 The proposed LSTM classifier has a similar accuracy to 1NN-ADTW and it also outperforms 1NNDTW. \u2022 The hybrid algorithm LSTM / ADTW can achieve a higher accuracy compared to 1NN-ADTW and LSTM by 1% -2%, proving that the hybrid algorithm can actually improve classification accuracy."}, {"heading": "B. Analysis on the Accurately Classified Samples", "text": "In this section, we will analyze the accurately classified samples of the Power series and study the difference between the different classifiers. We will be able to see why and how the hybrid algorithm works. Fig. 4 shows the accurately classified samples for each class and for each algorithm. From Fig. 4 and 5, we can observe that the proposed 1NN-ADTW method works similarly to the 1NN-DTW, although the 1NN-ADTW can accurately predict how the two classifiers are."}, {"heading": "C. Discussion on the Parameter Settings", "text": "In this subsection we discuss the parameter settings in the above algorithms. First, we examine the parameters used in the ADTW measurement, the order k. The test results with different k settings are given in Table VI. It can be seen that a correct k setting is required, since a value that is too small or too large can both deteriorate the performance. In our experiments, we find that a value in the range [8,12] is appropriate. Note that increasing the order can cause higher computing costs. Second, we discuss the parameter settings for the LSTM classifier. The adjustment of the hyperparameters of the LSTM network is crucial. In our experiments, we find that an inappropriate performance with an accuracy of less than 50% can result in higher computing costs. We find the following key settings in the LSTM classifier that we have tested, and find the correct setting of the LSTM parameter is not helpful in our case 100, although detailed results are omitted here)."}, {"heading": "VI. CONCLUSION AND FUTURE WORKS", "text": "In this research, we propose a hybrid algorithm of the 1NN-ADTW and LSTM neural networks. First, we define a progressive dynamic time warping (ADTW) for distance measurement that performs better than the DTW in classifying the power series. Second, we apply the continuous data modeling of neural networks LSTM to classify the power series. Our study shows that 1NN-ADTW and LSTM can both outperform the 1NN-DTW with similar accuracy; however, these two algorithms have their unique different nature and the accurately classified samples of these two algorithms have a significant difference. In this sense, we propose a hybrid algorithm of the two classifiers called LSTM / ADTW that further improves accuracy. The proposed hybrid algorithm can achieve a classification accuracy of 92% in our experiments."}], "references": [{"title": "The case for energy-proportional computing", "author": ["L.A. Barroso", "U. Holzle"], "venue": "Computer, vol. 40, no. 12, pp. 33\u201337, 2007.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Data center workload monitoring, analysis, and emulation", "author": ["J. Moore", "J. Chase", "K. Farkas", "P. Ranganathan"], "venue": "Eighth Workshop on Computer Architecture Evaluation using Commercial Workloads, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "A new approach to signal classification using spectral correlation and neural networks", "author": ["A. Fehske", "J. Gaeddert", "J.H. Reed"], "venue": "New Frontiers in Dynamic Spectrum Access Networks, 2005. DySPAN 2005. 2005 First IEEE International Symposium on. IEEE, 2005, pp. 144\u2013 150.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Signal classification using statistical moments", "author": ["S.S. Soliman", "S.-Z. Hsue"], "venue": "Communications, IEEE Transactions on, vol. 40, no. 5, pp. 908\u2013916, 1992.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1992}, {"title": "Techniques of emg signal analysis: detection, processing, classification and applications", "author": ["M. Reaz", "M. Hussain", "F. Mohd-Yasin"], "venue": "Biological procedures online, vol. 8, no. 1, pp. 11\u201335, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Fast time series classification using numerosity reduction", "author": ["X. Xi", "E. Keogh", "C. Shelton", "L. Wei", "C.A. Ratanamahatana"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 1033\u20131040.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural computation, vol. 12, no. 10, pp. 2451\u20132471, 2000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Everything you know about dynamic time warping is wrong.", "author": ["C.A. Ratanamahatana", "E. Keogh"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "Toward accurate dynamic time warping in linear time and space", "author": ["S. Salvador", "P. Chan"], "venue": "Intelligent Data Analysis, vol. 11, no. 5, pp. 561\u2013580, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in INTERSPEECH,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 2625\u20132634.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Beyond short snippets: Deep networks for video classification", "author": ["J. Yue-Hei Ng", "M. Hausknecht", "S. Vijayanarasimhan", "O. Vinyals", "R. Monga", "G. Toderici"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 4694\u20134702.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young"], "venue": "arXiv preprint arXiv:1508.01745, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Each data center is typically equipped with hundreds of thousands servers and requires many mega-watts electricity to power its hosted servers and the auxiliary facilities [2].", "startOffset": 172, "endOffset": 175}, {"referenceID": 1, "context": "Monitoring technologies [3] can be divided into two categories: intrusive and non-intrusive.", "startOffset": 24, "endOffset": 27}, {"referenceID": 2, "context": "Currently there are a few works studies on classify signals like in [4] [5] [6].", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "Currently there are a few works studies on classify signals like in [4] [5] [6].", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "Currently there are a few works studies on classify signals like in [4] [5] [6].", "startOffset": 76, "endOffset": 79}, {"referenceID": 5, "context": "For the general time series classification problem, in [7], dynamic time warping (DTW) distance metric based method 1NNDTW is proven to be the most suitable algorithm.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "Second we apply the stateof-art sequential data modeling neural network long time short time memory (LSTM) [8] [9] to classify the power series.", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "Second we apply the stateof-art sequential data modeling neural network long time short time memory (LSTM) [8] [9] to classify the power series.", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "For this problem, on one hand, common classifiers like support vector machine (SVM), k-nearest neighbor (KNN) have been proved to be noncompetitive to the DTW distance metric based method like 1NN-DTW [7].", "startOffset": 201, "endOffset": 204}, {"referenceID": 8, "context": "On the other hand, recently with the fast development of deep learning [10], LSTM neural network has also been proved to hold high modeling ability for sequential data.", "startOffset": 71, "endOffset": 75}, {"referenceID": 9, "context": "In practice, usually a threshold r is used to restrict the index offset in the alignment, which can be critical to the classification results [11].", "startOffset": 142, "endOffset": 146}, {"referenceID": 10, "context": "Also there are many study [12] working on accelerating the computing speed of DTW, which results in the fast DTW that can be computed in linear time of the length of the sequences.", "startOffset": 26, "endOffset": 30}, {"referenceID": 11, "context": "as an upgrade of the recurrent neural network (RNN) [13].", "startOffset": 52, "endOffset": 56}, {"referenceID": 12, "context": "LSTM has shown great modeling power for sequential data and has been successfully applied in various machine learning fields like natural language process (NLP) [14], video analysis [15] and etc.", "startOffset": 161, "endOffset": 165}, {"referenceID": 13, "context": "LSTM has shown great modeling power for sequential data and has been successfully applied in various machine learning fields like natural language process (NLP) [14], video analysis [15] and etc.", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "By discriminative, LSTM can be used for classification tasks while by generative, LSTM can be used to generate similar sequences like the training samples [16].", "startOffset": 155, "endOffset": 159}, {"referenceID": 5, "context": "The left number of sequences for each class is: [77, 31, 30, 35, 28, 7, 40, 14, 5, 100, 58, 36, 40].", "startOffset": 48, "endOffset": 99}, {"referenceID": 12, "context": "The left number of sequences for each class is: [77, 31, 30, 35, 28, 7, 40, 14, 5, 100, 58, 36, 40].", "startOffset": 48, "endOffset": 99}, {"referenceID": 3, "context": "The left number of sequences for each class is: [77, 31, 30, 35, 28, 7, 40, 14, 5, 100, 58, 36, 40].", "startOffset": 48, "endOffset": 99}, {"referenceID": 15, "context": "We utilize eight different dimension reduction methods with scikit-learn [17] and project the original fixed length power sequences into a 2-dimensional space.", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "The canonical classifiers tested here are listed as follows: Nearest Neighbors, Linear SVM, RBF SVM, Decision Tree, Random Forest, AdaBoost, Naive Bayes, LDA and QDA [17] .", "startOffset": 166, "endOffset": 170}, {"referenceID": 5, "context": "accuracy is not promising (when compared to the 1NN-DTW shown below), which actually proves that our power series labeling problem is a typical time series classification problem, as stated in [7], for such problem, canonical Euclidean distance metric based classifiers cannot achieve good results usually.", "startOffset": 193, "endOffset": 196}, {"referenceID": 6, "context": "In our experiments we find that a value in range [8,12] is suitable.", "startOffset": 49, "endOffset": 55}, {"referenceID": 10, "context": "In our experiments we find that a value in range [8,12] is suitable.", "startOffset": 49, "endOffset": 55}], "year": 2017, "abstractText": "As many applications organize data into temporal sequences, the problem of time series data classification has been widely studied. Recent studies show that the 1-nearest neighbor with dynamic time warping (1NN-DTW) and the long short term memory (LSTM) neural network can achieve a better performance than other machine learning algorithms. In this paper, we build a novel time series classification algorithm hybridizing 1NN-DTW and LSTM, and apply it to a practical data center power monitoring problem. Firstly, we define a new distance measurement for the 1NN-DTW classifier, termed as Advancing Dynamic Time Warping (ADTW), which is non-commutative and non-dynamic programming. Secondly, we hybridize the 1NNADTW and LSTM together. In particular, a series of auxiliary test samples generated by the linear combination of the original test sample and its nearest neighbor with ADTW are utilized to detect which classifier to trust in the hybrid algorithm. Finally, using the power consumption data from a real data center, we show that the proposed ADTW can improve the classification accuracy from about 84% to 89%. Furthermore, with the hybrid algorithm, the accuracy can be further improved and we achieve an accuracy up to about 92%. Our research can inspire more studies on non-commutative distance measurement and the hybrid of the deep learning models with other traditional models.", "creator": "LaTeX with hyperref package"}}}