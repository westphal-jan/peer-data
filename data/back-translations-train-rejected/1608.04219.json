{"id": "1608.04219", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Using Machine Learning to Decide When to Precondition Cylindrical Algebraic Decomposition With Groebner Bases", "abstract": "Cylindrical Algebraic Decomposition (CAD) is a key tool in computational algebraic geometry, particularly for quantifier elimination over real-closed fields. However, it can be expensive, with worst case complexity doubly exponential in the size of the input. Hence it is important to formulate the problem in the best manner for the CAD algorithm. One possibility is to precondition the input polynomials using Groebner Basis (GB) theory. Previous experiments have shown that while this can often be very beneficial to the CAD algorithm, for some problems it can significantly worsen the CAD performance.", "histories": [["v1", "Mon, 15 Aug 2016 09:44:29 GMT  (168kb,D)", "http://arxiv.org/abs/1608.04219v1", null]], "reviews": [], "SUBJECTS": "cs.SC cs.LG", "authors": ["zongyan huang", "matthew england", "james h davenport", "lawrence c paulson"], "accepted": false, "id": "1608.04219"}, "pdf": {"name": "1608.04219.pdf", "metadata": {"source": "CRF", "title": "Using Machine Learning to Decide When to Precondition Cylindrical Algebraic Decomposition With Groebner Bases", "authors": ["Zongyan Huang", "Matthew England", "James H. Davenport", "Lawrence C. Paulson"], "emails": ["rubyhuang87@gmail.com;", "lp15@cam.ac.uk", "Matthew.England@coventry.ac.uk", "J.H.Davenport@bath.ac.uk"], "sections": [{"heading": null, "text": "In the present paper, we investigate whether machine learning, in particular a support vector machine (SVM), can be used to identify those CAD problems that benefit from GB pre-conditioning. We conduct experiments with over 1,000 problems (many times larger than previous studies) and find that machine-learned choice performs better than human-made heuristics.I. INTRODUCTION"}, {"heading": "A. Cylindrical Algebraic Decomposition", "text": "Cylindrical algebraic decomposition (CAD) is a decomposition of the ordered Rn space into cells. These are arranged cylindrically, meaning that the projections of any pair are either equal or inconsistent in relation to the given order. In this definition, algebraic is actually short for semi-algebraic schemes, since any CAD cell can be described with a finite sequence of polynomial constraints. CAD is produced to be invariant for the input of polynomials or truth-invariant for input formulas. CADs and the first algorithm to calculate these constraints were introduced by Collins in 1975 [19]. CAD usually has two stages: projection, in which an operator is applied recursively to the input to deduce corresponding problems in lower dimensions; and cancellation, where CADs are gradually built according to dimensions, according to the polynomial functions identified in the projection [1] polynomial input, while some QAD [33] are required."}, {"heading": "B. Preconditioning with Groebner Bases", "text": "One definition is that the ideal generated by the leading terms of I was generated by the leading terms of G. Groebner Bases (GB) to derive the properties of the ideal introduced by Buchberger in his 1965 doctoral thesis [14]. Like CAD, there has been much research to improve and optimize GB calculation, with the F5 algorithm [31] perhaps the most commonly used approach. But also like CAD, the calculation of GB is necessarily twice exponential at worst [46] (using a lexicographic monomial calculation)."}, {"heading": "C. Contribution and plan", "text": "In this paper, we consider whether machine learning can be applied to deciding whether pre-conditioning CAD input with GB is advantageous for a particular problem. We work on the reasonable assumption that GB calculation is cheap for the problems where CAD is traceable (as shown in [29], the CAD calculates results that overestimate the GB).Therefore, we use algebraic features of both the input problem and the GB itself to decide whether we will use the GB. In Section II, we describe the data set and computer algebra calculations used for the experiment, and in Section III, we describe the features that were identified to train the machine learning algorithm: a Support Vector Machine (SVM). Then, in Section IV, we describe the initial machine learning experiment and its results before conducting functional selection experiments in Section V. Finally, we compare the machine learning decision of the second TI with the decision of the second TAD, which is to discuss the choice of the VIAD."}, {"heading": "II. DATASET AND COMPUTER ALGEBRA", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Computer Algebra", "text": "All computer algebra calculations were performed in MAPLE-17. The CAD algorithm used was an implementation of [18], which is part of RegularChains Library1 [16], [17] whose CAD procedure differs from the traditional Collins projection and lifting frame, instead of disassembling Cn cylindrically and then refining it to a CAD of Rn. Previous experiments [57] showed that this implementation has the same problems of GB preconditioning as the traditional approach. The standard implementation of MAPLE GB was used: a meta algorithm that calls several GB implementations. GBs were calculated using a purely lexicographic sequence of monomials based on the same variable sequence as the CAD. All calculations were performed on a 2.4GHz Intel processor, but this is not relevant, as we correlate the CAD performance with this CAD model based on the final cell count, i.e. we narrowly evaluated the CAD performance based on the number of cells in the final cell count."}, {"heading": "B. Dataset", "text": "A key difficulty in applying machine learning techniques to computer algebra is the lack of suitable datasets. CAD problem sets such as [56] do not have nearly a sufficient number of problems to perform the experiment. In our previous study on the selection of variable order for CAD [41], we used the nlsat datasets [58], which were designed for non-linear arithmetic SAT solvers, which contained many suitable problems. For the current experiment, we need problems expressed with a conjunction of at least two qualities to build a non-trivial GB. Of the nlsat dataset 493 three-variable problems and 403 four-variable problems fit these criteria, which should be a sufficient number. GB preconditioning was applied to each problem and the cell numbers from the calculation of the CAD with the original polynomials and their replacement with GB were not calculated and compared."}, {"heading": "III. PROBLEM FEATURES", "text": "The abbreviations tds and stds represent the maximum total degree and sum of total trades: tds (F) = max f tds (f), stds (F) = f tds (f).We also make use of the metric TNoI (1).Finally, we have the base 2 logarithm of differences between some of the key departments."}, {"heading": "IV. MACHINE LEARNED CHOICES", "text": "A. IntroductionMachine learning deals with the design of programs that learn rules from data. It is an attractive alternative to manual design when the underlying functional relationship is complex, as seems to be the case here. In the last decade, the use of machine learning has spread rapidly after the invention of the Support Vector Machine (SVM) (see for example [51]). This provides a powerful and robust method for both: classification, the mapping of input examples into a specific group of classes; and regression, a supervised pattern analysis where the results have real value. The standard SVM classifier takes a set of input data and predicts one of two possible classes from the input. In the face of a series of examples, each marked as belonging to one of two classes, an SVM training algorithm forms a model that assigns new examples to one of the classes. The examples used to fit the model are referred to as training examples."}, {"heading": "B. Cross-validation and grid-search", "text": "The 1062 problems were divided into 80% training (849 problems) and 20% test (213 problems) to obtain relative proportions of positive and negative examples, using the radial basic function (RBF) kernel, which was selected after previous experiments in which machine learning was applied to an automated theory tester that found the RBF kernel to perform well with similar simple algebraic properties [9]. The RBF function is defined as: K (x, x \u2032) = exp (\u2212 \u03b3 | x \u2032 | 2) (2), where x and x \u2032 are feature vectors. The process depends on the kernel parameter \u03b3 and another parameter C, which regulates the trade between margin and training error, and finding the optimal values of these values is not trivial. The correlation coefficient (MCC) [45], [2] is commonly used to evaluate choices. This takes into account true ttb, \u2212 tp, and perfect n (n)."}, {"heading": "C. Results for the three feature sets", "text": "The 213-problem test set contained 159 positive samples and 54 negative samples (i.e. 75% of the test problems benefited from GB preconditioning), and the results of the machine-learned decisions are summarized in Table II. Firstly, we note that 75% of the problems were predicted in the selection based on the previous feature training 2http: / / svmlight.joachims.org gaccurately. I.e. a decision based on these features does not lead to a more correct decision than blindly making the GB precondition each time. However, the other two sets of features led to superior decisions. Although only a small improvement over preconditioning is blind, we remember that the wrong choice can cause major changes in the size of the CAD or even change the traceability of the problem [57]. Results suggest that the GB characteristics themselves are needed to decide whether to use preconditioning."}, {"heading": "V. FEATURE SELECTION", "text": "There is strong evidence that not all features contribute to the machine learning process. Furthermore, a reduced feature set may be beneficial to a better understanding of the underlying relationships. Therefore, we applied some feature selection methods. Both filter and wrapper methods were used, as discussed in the following sections. Feature selection experiments were conducted using WEKA (Waikato Environment for Knowledge Analysis) [35], a Java machine learning library that supports tasks such as data pre-processing, clustering, classification, regression and feature selection. Moreover, each data point is represented as a fixed number of features."}, {"heading": "A. The filter method", "text": "Unlike other filtering methods [36], these measure the rank of feature subsets rather than individual features. A feature subset that contains features that are strongly correlated with the class but are not correlated with each other is preferred. The metric below is used to measure the quality of a feature subset, and takes into account the feature-class correlation as well as the feature-feature correlation. The numerator of the equation (4) indicates how much relevance there is between the class and a set of features, rci denotes the average feature-class correlation of feature i and the feature-feature correlation of feature i, and rii \u2032 the average feature-feature correlation between feature i and i \u2032. The numerator of the equation (4) indicates how much relevance there is between the class and a set of features, while the denominator measures the redundancy between the features. The higher the Gs, the better we must apply the hay set to the subset."}, {"heading": "B. The wrapper method", "text": "The SVM algorithm is executed on the data set, using the same data partitions as described in Section IV-B. Also, a five-fold cross-validation was performed; the feature subset with the highest average accuracy was selected as the final set on which the SVM algorithm is executed; in each training / validation fold, starting with an empty set of features: each feature was added; a model was put on top of the training data set; the classifier was then tested on the validation set \u2212 This was performed on all features, resulting in a score reflecting the accuracy of the classifier; the final score for each feature was reduced to the average of the five features."}, {"heading": "C. Results with reduced feature sets", "text": "After obtaining the reduced characteristics, we performed the experiment again to evaluate the new options. The data set was again divided into 80% training and 20% test set to maintain the relative class proportions in both the training and test partitions.Again, a fivefold cross-validation and a finer grid search optimization procedure was performed across the range of (C-, \u03b3) pairs as described above. To better estimate the generalization performance of classifiers with maximum averaged characteristics, the data was permutated and re-divided into 80% training and 20% test, and the entire process was repeated 50 times. For each run, the classification accuracy was used to measure the performance of the classifier. To better estimate the generalization performance of classifiers with reduced characteristics, the data was again divided into 80% training and 20% test, and the entire process was repeated 50 times."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Comparison with human developed heuristic", "text": "We can compare the machine-learned choice with the human-developed TNoI heuristics [57], whose performance in relation to the 213 test problems is shown in Table V. It correctly predicted whether GB preconditioning was beneficial for 118 examples, only 55%. So, on average, it would have been better for this data set to assume blindly than to make a decision with TNoI alone. TNoI heuristics performed better in the experiments of Wilson et al. [57] These experiments included only 22 problems (compared to 213 in the test set here), but they were human designed to have certain geometric properties while those were random. We also find that TNoI heuristics actually performed very differently in positive and negative examples of our data set, as the separate data in Table V. It was able to identify most of the cases where GB prediction was harmful without identifying many cases in which it was partial."}, {"heading": "B. Summary", "text": "We investigated the application of machine learning to the problem of predicting when GB preconditioning would be advantageous for CAD. We had to create a new set of random polynomials for the experiment. We acknowledge that it would be preferable to perform such a test on an established dataset, but this was not available. Of course, such a random dataset could be expanded to increase diversity almost indefinitely, but we had to keep the experiment within mathematically feasible limits. We emphasize the interesting initial finding in Section II-B that supposedly different established datasets can have a hidden uniformity; and emphasize that our generated dataset agreed with previously published results [15], [57] for the study topic with most, but not all, who benefited from the GB precondition."}, {"heading": "C. Future Work", "text": "There are many possible extensions to this project: \u2022 To see how the choice learned is made on a non-random dataset; \u2022 There is a large set of derivatives from the entrance exams for mathematics at universities [44] that are not yet publicly available but could be in the future; \u2022 There are other CAD optimizations for several equations under development [27], [29], [24] that could affect the role of GB preconditioning from CAD; \u2022 In the present paper, the variable sequence for CAD and the monomic sequence for GB have been determined; in reality, such decisions may also have to be made in parallel, and it is an open problem how best to do this; the variable sequence may influence the choice of whether GB preconditioning should be used and vice versa; and finally, we point out that there are other algorithm optimization decisions for CAD, and even elsewhere in computer algebra."}, {"heading": "Acknowledgements", "text": "Thanks to David Wilson and James Bridge, our collaborators at [41], for useful conversations on machine learning to optimize computer algebra. This work was supported by the EPSRC grant EP / J003247 / 1 and the EU project H2020FETOPEN-2016-2017-CSA SC2 (712689)."}], "references": [{"title": "Cylindrical algebraic decomposition I: The basic algorithm", "author": ["D. Arnon", "G.E. Collins", "S. McCallum"], "venue": "SIAM J. Computing,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1984}, {"title": "Assessing the accuracy of prediction algorithms for classification: An overview", "author": ["P. Baldi", "S. Brunak", "Y. Chauvin", "C.A.F. Andersen", "H. Nielsen"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "Algorithms in Real Algebraic Geometry", "author": ["S. Basu", "R. Pollack", "M.F. Roy"], "venue": "Vol. 10 of Algorithms & Comp. in Math. Springer,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Gr\u00f6bner bases using SAC2", "author": ["W. B\u00f6ge", "R. Gebauer", "H. Kredel"], "venue": "In EUROCAL \u201985,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1985}, {"title": "Truth table invariant cylindrical algebraic decomposition by regular chains", "author": ["R. Bradford", "C. Chen", "J.H. Davenport", "M. England", "M. Moreno Maza", "D. Wilson"], "venue": "In Computer Algebra in Scientific Computing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Cylindrical algebraic decompositions for boolean combinations", "author": ["R. Bradford", "J.H. Davenport", "M. England", "S. McCallum", "D. Wilson"], "venue": "In Proc. ISSAC", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Truth table invariant cylindrical algebraic decomposition", "author": ["R. Bradford", "J.H. Davenport", "M. England", "S. McCallum", "D. Wilson"], "venue": "J. Symbolic Computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Optimising problem formulations for cylindrical algebraic decomposition", "author": ["R. Bradford", "J.H. Davenport", "M. England", "D. Wilson"], "venue": "In Intelligent Computer Mathematics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Machine learning for firstorder theorem proving", "author": ["J.P. Bridge", "S.B. Holden", "L.C. Paulson"], "venue": "J. Automated Reasoning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Improved projection for cylindrical algebraic decomposition", "author": ["C.W. Brown"], "venue": "J. Symbolic Computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2001}, {"title": "Constructing a single open cell in a cylindrical algebraic decomposition", "author": ["C.W. Brown"], "venue": "In Proc. ISSAC", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "The complexity of quantifier elimination and cylindrical algebraic decomposition", "author": ["C.W. Brown", "J.H. Davenport"], "venue": "In Proc. ISSAC", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Algorithmic methods for investigating equilibria in epidemic modelling", "author": ["C.W. Brown", "M. El Kahoui", "D. Novotni", "A. Weber"], "venue": "J. Symbolic Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "An algorithm for finding the basis elements of the residue class ring of a zero dimensional polynomial ideal", "author": ["B. Buchberger"], "venue": "J. Symbolic Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1965}, {"title": "Speeding up quantifier elimination by Gr\u00f6bner bases", "author": ["B. Buchberger", "H. Hong"], "venue": "Tech. Report,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1991}, {"title": "Real quantifier elimination in the RegularChains library", "author": ["C. Chen", "M. Moreno Maza"], "venue": "In Mathematical Software \u2013 ICMS 2014,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Quantifier elimination by cylindrical algebraic decomposition based on regular chains", "author": ["C. Chen", "M. Moreno Maza"], "venue": "J. Symbolic Computation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Computing cylindrical algebraic decomposition via triangular decomposition", "author": ["C. Chen", "M. Moreno Maza", "B. Xia", "L. Yang"], "venue": "In Proc. ISSAC", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Quantifier elimination for real closed fields by cylindrical algebraic decomposition", "author": ["G.E. Collins"], "venue": "In Proc. 2nd GI Conference on Automata Theory and Formal Languages,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1975}, {"title": "The SAC-2 computer algebra system", "author": ["G.E. Collins"], "venue": "In EUROCAL \u201985,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1985}, {"title": "Partial cylindrical algebraic decomposition for quantifier elimination", "author": ["G.E. Collins", "H. Hong"], "venue": "J. Symbolic Computation,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "Program verification in the presence of complex numbers, functions with branch cuts etc", "author": ["J.H. Davenport", "R. Bradford", "M. England", "D. Wilson"], "venue": "In Proc. SYNASC", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Real quantifier elimination is doubly exponential", "author": ["J.H. Davenport", "J. Heintz"], "venue": "J. Symbolic Computation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1988}, {"title": "Need polynomial systems be doubly exponential", "author": ["J.H. Davenport", "M. England"], "venue": "In Mathematical Software \u2013 ICMS 2016,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Efficient projection orders for CAD", "author": ["A. Dolzmann", "A. Seidl", "T. Sturm"], "venue": "In Proc. ISSAC", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Problem formulation for truth-table invariant cylindrical algebraic decomposition by incremental triangular decomposition", "author": ["M. England", "R. Bradford", "C. Chen", "J.H. Davenport", "M. Moreno Maza", "D. Wilson"], "venue": "In Intelligent Computer Mathematics,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Improving the use of equational constraints in cylindrical algebraic decomposition", "author": ["M. England", "R. Bradford", "J.H. Davenport"], "venue": "In Proc. ISSAC", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Choosing a variable ordering for truth-table invariant cylindrical algebraic decomposition by incremental triangular decomposition", "author": ["M. England", "R. Bradford", "J.H. Davenport", "D. Wilson"], "venue": "ICMS", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "The complexity of cylindrical algebraic decomposition with respect to polynomial degree", "author": ["M. England", "J.H. Davenport"], "venue": "To appear In: Computer Algebra in Scientific Computing,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2016}, {"title": "Synthesis of optimal numerical algorithms using real quantifier elimination (Case Study: Square root computation)", "author": ["M. Erascu", "H. Hong"], "venue": "In Proc. ISSAC", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "A new efficient algorithm for computing groebner bases without reduction to zero (F5)", "author": ["J.C. Faug\u00e8re"], "venue": "In Proc. ISSAC", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2002}, {"title": "Multi-interval discretization of continuousvalued attributes for classification learning", "author": ["U.M. Fayyad", "K.B. Irani"], "venue": "In Proc. of the International Joint Conference on Uncertainty in AI, pages 1022\u20131027,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1993}, {"title": "Nonlinear parametric optimization using cylindrical algebraic decomposition", "author": ["I.A. Fotiou", "P.A. Parrilo", "M. Morari"], "venue": "In Proc. CDC-ECC", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2005}, {"title": "An introduction to variable and feature selection", "author": ["I. Guyon", "A. Elisseeff"], "venue": "J. Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "The WEKA data mining software: An update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "SIGKDD Explorations Newsletter,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Correlation-based feature selection for discrete and numeric class machine learning", "author": ["M.A. Hall"], "venue": "In Proc. of the Seventeenth International Conference on Machine Learning,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2000}, {"title": "Benchmarking attribute selection techniques for discrete class data mining", "author": ["M.A. Hall", "G. Holmes"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2003}, {"title": "Constructing fewer open cells by gcd computation in CAD projection", "author": ["J. Han", "L. Dai", "B. Xia"], "venue": "In Proc. ISSAC", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "An improvement of the projection operator in cylindrical algebraic decomposition", "author": ["H. Hong"], "venue": "In Proc. ISSAC", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 1990}, {"title": "A practical guide to support vector classification", "author": ["C. Hsu", "C. Chang", "C. Lin"], "venue": "Tech. Report, Department of Computer Science, National Taiwan Uni.,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2003}, {"title": "Applying machine learning to the problem of choosing a heuristic to select the variable ordering for cylindrical algebraic decomposition", "author": ["Z. Huang", "M. England", "D. Wilson", "J.H. Davenport", "L. Paulson", "J. Bridge"], "venue": "In Intelligent Computer Mathematics,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2014}, {"title": "An effective implementation of a symbolic-numeric cylindrical algebraic decomposition for quantifier elimination", "author": ["H. Iwane", "H. Yanami", "H. Anai", "K. Yokoyama"], "venue": "In Proc. SNC", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2009}, {"title": "Making large-scale support vector machine learning practical", "author": ["T. Joachims"], "venue": "Advances in Kernel Methods,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1999}, {"title": "Efficient subformula orders for real quantifier elimination of non-prenex formulas", "author": ["M. Kobayashi", "H. Iwane", "T. Matsuzaki", "H. Anai"], "venue": "In Mathematical Aspects of Computer and Information Sciences (MACIS \u201915),", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Comparison of the predicted and observed secondary structure of T4 phage lysozyme", "author": ["B.W. Matthews"], "venue": "Biochimica et Biophysica Acta (BBA)- Protein Structure,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1975}, {"title": "The complexity of the word problems for commutative semigroups and polynomial ideals", "author": ["E.W. Mayr", "A.R. Meyer"], "venue": "Advances in Mathematics,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1982}, {"title": "An improved projection operation for cylindrical algebraic decomposition", "author": ["S. McCallum"], "venue": "Symbolic Computation,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 1998}, {"title": "Metitarski: Past and future", "author": ["L.C. Paulson"], "venue": "In Interactive Theorem Proving,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Numerical Recipes in C (2nd Ed.): The Art of Scientific Computing", "author": ["W.H. Press", "S.A. Teukolsky", "W.T. Vetterling", "B.P. Flannery"], "venue": null, "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1992}, {"title": "Induction of decision trees", "author": ["J.R. Quinlan"], "venue": "Machine Learning,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1986}, {"title": "Kernel methods in computational biology", "author": ["B. Sch\u00f6lkopf", "K. Tsuda", "J.-P. Vert"], "venue": null, "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2004}, {"title": "A mathematical theory of communication", "author": ["Claude E. Shannon"], "venue": "Mobile Computing and Communications Review,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2001}, {"title": "Kernel methods for pattern analysis", "author": ["J. Shawe-Taylor", "N. Cristianini"], "venue": null, "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2004}, {"title": "Cylindrical algebraic decomposition using validated numerics", "author": ["A. Strzebo\u0144ski"], "venue": "J. Symbolic Computation,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2006}, {"title": "Cylindrical algebraic decomposition using local projections", "author": ["A. Strzebo\u0144ski"], "venue": "In Proc. ISSAC", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2014}, {"title": "A repository for CAD examples", "author": ["D.J. Wilson", "R.J. Bradford", "J.H. Davenport"], "venue": "ACM Comm. Computer Algebra,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2012}, {"title": "Speeding up cylindrical algebraic decomposition by Gr\u00f6bner bases", "author": ["D.J. Wilson", "R.J. Bradford", "J.H. Davenport"], "venue": "In Intelligent Computer Mathematics (LNCS", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2012}], "referenceMentions": [{"referenceID": 18, "context": "CADs and the first algorithm to compute them were introduced by Collins in 1975 [19].", "startOffset": 80, "endOffset": 84}, {"referenceID": 0, "context": "CAD usually has two stages: projection where an operator is applied recursively on the input to derive corresponding problems in lower dimensions; and lifting where CADs are built incrementally by dimension according to the polynomials identified in projection [1].", "startOffset": 261, "endOffset": 264}, {"referenceID": 32, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 162, "endOffset": 166}, {"referenceID": 47, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 184, "endOffset": 188}, {"referenceID": 21, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 228, "endOffset": 232}, {"referenceID": 29, "context": "The original motivation was quantifier elimination (QE) in real closed fields, while other applications include: parametric optimisation [33], epidemic modelling [13], theorem proving [48], reasoning with multi-valued functions [22] derivation of optimal numerical schemes [30], and much more.", "startOffset": 273, "endOffset": 277}, {"referenceID": 22, "context": "CAD has worst case complexity doubly exponential in the number of variables [23] applicable whatever the data structure [12].", "startOffset": 76, "endOffset": 80}, {"referenceID": 11, "context": "CAD has worst case complexity doubly exponential in the number of variables [23] applicable whatever the data structure [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "For some applications there exist algorithms with better complexity [3], but CAD implementations still remain the best general purpose approach for many.", "startOffset": 68, "endOffset": 71}, {"referenceID": 38, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 161, "endOffset": 165}, {"referenceID": 46, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 167, "endOffset": 171}, {"referenceID": 9, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 173, "endOffset": 177}, {"referenceID": 37, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 179, "endOffset": 183}, {"referenceID": 20, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 231, "endOffset": 235}, {"referenceID": 53, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 274, "endOffset": 278}, {"referenceID": 41, "context": "This may be due to the numerous approaches used to improve the efficiency of CAD since Collins\u2019 original work including: improvements to the projection operator [39], [47], [10], [38]: partial CAD (lift only when necessary for QE) [21]; and symbolic-numeric lifting schemes [54], [42].", "startOffset": 280, "endOffset": 284}, {"referenceID": 5, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 83, "endOffset": 86}, {"referenceID": 26, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 88, "endOffset": 92}, {"referenceID": 10, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 122, "endOffset": 126}, {"referenceID": 54, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 128, "endOffset": 132}, {"referenceID": 17, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 171, "endOffset": 175}, {"referenceID": 4, "context": "Some recent advances include making use of any Boolean structure in the input [6], [7], [27]; local projection approaches [11], [55]; and decompositions via complex space [18], [5].", "startOffset": 177, "endOffset": 180}, {"referenceID": 6, "context": "[7].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "Their properties and an algorithm to derive a GB for any ideal was introduced by Buchberger in his PhD thesis of 1965 [14].", "startOffset": 118, "endOffset": 122}, {"referenceID": 30, "context": "Like CAD, there has been much research to improve and optimise GB calculation, with the F5 algorithm [31] perhaps the most used approach currently.", "startOffset": 101, "endOffset": 105}, {"referenceID": 45, "context": "However, also like CAD the calculation of GB is necessarily doubly exponential in the worst case [46] (when using a lexicographic monomial ordering).", "startOffset": 97, "endOffset": 101}, {"referenceID": 14, "context": "The first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]).", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "The first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]).", "startOffset": 122, "endOffset": 125}, {"referenceID": 20, "context": "The first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 19, "context": "The first attempt to answer this question was given by Buchberger and Hong in 1991 [15] who used the implementation of GB [4] to precondition an implementation of CAD [21] (both in C on top of the SAC-2 system [20]).", "startOffset": 210, "endOffset": 214}, {"referenceID": 56, "context": "[57].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The authors recreated the experiments of Buchberger and Hong [15] using QEPCAD-B for the CAD and MAPLE 16 for the GB.", "startOffset": 61, "endOffset": 65}, {"referenceID": 14, "context": "As we may expect, there had been a big decrease in the computation timings, especially the GB: the two test problems previously intractable [15] could now have the GB calculated quickly.", "startOffset": 140, "endOffset": 144}, {"referenceID": 17, "context": "The experiments were then extended to: a wider example set (an additional 12 problems); the alternative CAD implementation in MAPLE-16 [18]; and the case where we further precondition by reducing inequalities of the system (the set F above) with respect to the GB.", "startOffset": 135, "endOffset": 139}, {"referenceID": 28, "context": "We work on the reasonable assumption that GB computation is cheap for the problems on which CAD is tractable (in fact as shown in [29] the CAD will compute resultants which overestimate the GB).", "startOffset": 130, "endOffset": 134}, {"referenceID": 40, "context": "We previously studied the choice of variable ordering for CAD in [41].", "startOffset": 65, "endOffset": 69}, {"referenceID": 43, "context": "[44] who applied a SVM to decide the order of sub-formulae solving for QE.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "The CAD algorithm used was an implementation of [18].", "startOffset": 48, "endOffset": 52}, {"referenceID": 15, "context": "This is part of the RegularChains Library1 [16], [17] whose CAD procedures differ from the traditional projection and lifting framework of Collins, instead first decomposing C cylindrically and then refining to a CAD of R.", "startOffset": 43, "endOffset": 47}, {"referenceID": 16, "context": "This is part of the RegularChains Library1 [16], [17] whose CAD procedures differ from the traditional projection and lifting framework of Collins, instead first decomposing C cylindrically and then refining to a CAD of R.", "startOffset": 49, "endOffset": 53}, {"referenceID": 56, "context": "Previous experiments [57] showed this implementation has the same issues of GB preconditioning as the traditional approach.", "startOffset": 21, "endOffset": 25}, {"referenceID": 55, "context": "CAD problem sets such as [56] do not have anywhere near a sufficient number of problems to perform the experiment.", "startOffset": 25, "endOffset": 29}, {"referenceID": 40, "context": "In our previous study on choosing the variable ordering for CAD [41] we used the nlsat-dataset [58], which although developed for non-linear arithmetic SAT-solvers, contained many suitable problems.", "startOffset": 64, "endOffset": 68}, {"referenceID": 14, "context": "For each one of these problems the GB preconditioning was beneficial or made no difference; surprising as the experiments on much smaller datasets [15], [57] had shown much greater volatility.", "startOffset": 147, "endOffset": 151}, {"referenceID": 56, "context": "For each one of these problems the GB preconditioning was beneficial or made no difference; surprising as the experiments on much smaller datasets [15], [57] had shown much greater volatility.", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "So our randomly generated dataset matched the previously found results of [15], [57] with most problems benefiting but not all and so is suitable for the purpose of this experiment.", "startOffset": 74, "endOffset": 78}, {"referenceID": 56, "context": "So our randomly generated dataset matched the previously found results of [15], [57] with most problems benefiting but not all and so is suitable for the purpose of this experiment.", "startOffset": 80, "endOffset": 84}, {"referenceID": 56, "context": "We also make use of the metric TNoI (see equation (1)) [57].", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "We note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28].", "startOffset": 76, "endOffset": 80}, {"referenceID": 7, "context": "We note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28].", "startOffset": 117, "endOffset": 120}, {"referenceID": 25, "context": "We note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 27, "context": "We note that the stds measure differs from the sotd heuristic introduced in [25] and used for multiple CAD decisions [8], [26], [28].", "startOffset": 128, "endOffset": 132}, {"referenceID": 50, "context": "In the last decade, the use of machine learning has spread rapidly following the invention of the Support Vector Machine (SVM) (see for example [51]).", "startOffset": 144, "endOffset": 148}, {"referenceID": 52, "context": "An important concept in the SVM theory is the use of a kernel function to map data into a high dimensional feature space and then separate samples in the transformed space [53].", "startOffset": 172, "endOffset": 176}, {"referenceID": 42, "context": "For our experiment we used SVM-LIGHT2 [43]; an implementation of SVMs in C.", "startOffset": 38, "endOffset": 42}, {"referenceID": 8, "context": "This was chosen after earlier experiments applying machine learning to an automated theorem prover found the RBF kernel to perform well with similar simple algebraic features [9].", "startOffset": 175, "endOffset": 178}, {"referenceID": 44, "context": "Matthews\u2019 Correlation Coefficient (MCC) [45], [2] is often used to evaluate choices.", "startOffset": 40, "endOffset": 44}, {"referenceID": 1, "context": "Matthews\u2019 Correlation Coefficient (MCC) [45], [2] is often used to evaluate choices.", "startOffset": 46, "endOffset": 49}, {"referenceID": 39, "context": "We tested a commonly used range of values in our grid search process [40]: \u03b3 varied between {2\u221215, 2\u221214, 2\u221213, .", "startOffset": 69, "endOffset": 73}, {"referenceID": 56, "context": "Although only a small improvement on preconditioning blindly, we recall that the wrong choice can give large changes to the size of the CAD or even change the tractability of the problem [57].", "startOffset": 187, "endOffset": 191}, {"referenceID": 33, "context": "However, we cannot conclude this directly: earlier research shows that a variable completely useless by itself can provide a significant performance improvement when taken in conjunction with others [34].", "startOffset": 199, "endOffset": 203}, {"referenceID": 34, "context": "The feature selection experiments were conducted with WEKA (Waikato Environment for Knowledge Analysis) [35], a Java machine learning library which supports tasks such as data preprocessing, clustering, classification, regression and feature selection.", "startOffset": 104, "endOffset": 108}, {"referenceID": 36, "context": "A correlation based feature selection method, was applied as described in [37].", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "Unlike other filter methods [36], these measure the rank of feature subsets instead of individual features.", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "With the exception of the class attribute all 28 features are continuous, so in order to have a common measure for computing the correlations we first discretize using the method of Fayyad and Irani [32].", "startOffset": 199, "endOffset": 203}, {"referenceID": 51, "context": "We define the entropy of a variable X [52] as", "startOffset": 38, "endOffset": 42}, {"referenceID": 49, "context": "The information gain (IG) [50] measures the amount by which the entropy of X decreases by additional information about X provided by Y , and it is given by", "startOffset": 26, "endOffset": 30}, {"referenceID": 48, "context": "The symmetrical uncertainty (SU) (a modified information gain measure) is then used to measure the correlation between two discrete variables (X and Y) [49]:", "startOffset": 152, "endOffset": 156}, {"referenceID": 56, "context": "We may compare the machine learned choice with the human developed TNoI heuristic [57], whose performance on the 213 test problems is shown in Table V.", "startOffset": 82, "endOffset": 86}, {"referenceID": 56, "context": "[57].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "We emphasise the interesting initial finding in Section II-B that supposedly varied established sets can have hidden uniformity; and highlight that our generated dataset matched previously reported results [15], [57] for the topic of study with most, but not all, benefiting from GB preconditioning.", "startOffset": 206, "endOffset": 210}, {"referenceID": 56, "context": "We emphasise the interesting initial finding in Section II-B that supposedly varied established sets can have hidden uniformity; and highlight that our generated dataset matched previously reported results [15], [57] for the topic of study with most, but not all, benefiting from GB preconditioning.", "startOffset": 212, "endOffset": 216}, {"referenceID": 56, "context": "A machine learned choice on whether to precondition was found to yield better results than either always preconditioning blindly, or using the previously human developed TNoI heuristic [57].", "startOffset": 185, "endOffset": 189}, {"referenceID": 56, "context": "THE PERFORMANCE OF THE TNOI-BASED HEURISTIC [57]", "startOffset": 44, "endOffset": 48}, {"referenceID": 43, "context": "There is a large set derived from university mathematics entrance exams [44], which is not yet publicly available but may be in the future.", "startOffset": 72, "endOffset": 76}, {"referenceID": 26, "context": "\u2022 There are further CAD optimisations for multiple equalities under development [27], [29], [24] which may affect the role of GB preconditioning from CAD.", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "\u2022 There are further CAD optimisations for multiple equalities under development [27], [29], [24] which may affect the role of GB preconditioning from CAD.", "startOffset": 86, "endOffset": 90}, {"referenceID": 23, "context": "\u2022 There are further CAD optimisations for multiple equalities under development [27], [29], [24] which may affect the role of GB preconditioning from CAD.", "startOffset": 92, "endOffset": 96}, {"referenceID": 40, "context": "Thanks to David Wilson and James Bridge, our collaborators on [41], for useful conversations on the topic of machine learning to optimise computer algebra.", "startOffset": 62, "endOffset": 66}], "year": 2016, "abstractText": "Cylindrical Algebraic Decomposition (CAD) is a key tool in computational algebraic geometry, particularly for quantifier elimination over real-closed fields. However, it can be expensive, with worst case complexity doubly exponential in the size of the input. Hence it is important to formulate the problem in the best manner for the CAD algorithm. One possibility is to precondition the input polynomials using Groebner Basis (GB) theory. Previous experiments have shown that while this can often be very beneficial to the CAD algorithm, for some problems it can significantly worsen the CAD performance. In the present paper we investigate whether machine learning, specifically a support vector machine (SVM), may be used to identify those CAD problems which benefit from GB preconditioning. We run experiments with over 1000 problems (many times larger than previous studies) and find that the machine learned choice does better than the human-made heuristic.", "creator": "LaTeX with hyperref package"}}}