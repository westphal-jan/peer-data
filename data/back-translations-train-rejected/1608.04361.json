{"id": "1608.04361", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Multi-way Monte Carlo Method for Linear Systems", "abstract": "We study the Monte Carlo method for solving a linear system of the form $x = H x + b$. A sufficient condition for the method to work is $\\| H \\| &lt; 1$, which greatly limits the usability of this method. We improve this condition by proposing a new multi-way Markov random walk, which is a generalization of the standard Markov random walk. Under our new framework we prove that the necessary and sufficient condition for our method to work is the spectral radius $\\rho(H^{+}) &lt; 1$, which is a weaker requirement than $\\| H \\| &lt; 1$. In addition to solving more problems, our new method can work faster than the standard algorithm. In numerical experiments on both synthetic and real world matrices, we demonstrate the effectiveness of our new method.", "histories": [["v1", "Mon, 15 Aug 2016 18:45:08 GMT  (230kb,D)", "http://arxiv.org/abs/1608.04361v1", null]], "reviews": [], "SUBJECTS": "cs.NA cs.AI", "authors": ["tao wu", "david f gleich"], "accepted": false, "id": "1608.04361"}, "pdf": {"name": "1608.04361.pdf", "metadata": {"source": "CRF", "title": "Multi-way Monte Carlo Method for Linear Systems", "authors": ["Tao Wu", "David F. Gleich"], "emails": ["wu577@purdue.edu", "dgleich@purdue.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.04 361v 1 [cs.N A] 15 Aug 201 6"}, {"heading": "1 Introduction", "text": "The Monte Carlo Method [18] for solving a linear system uses a random approach to the solution. Due to its unique properties, this method has several advantages over traditional deterministic algorithms (e.g. Gaussian elimination and iterative methods). Firstly, the Monte Carlo Method can be very effective when only modest accuracy is required, as is common with many data problems such as PageRank calculations [2]. Secondly, it is known that Monte Carlo algorithms are highly parallelisable [13, 5], making them ideal for modern parallel computers or clusters. Thirdly, Monte Carlo methods can only be identified on a single component or a linear form of the solution, which is often all that is needed in many applications [17]. Last but not least, Monte Carlo methods have advantages when dealing with large linear systems [11, 7] because they do not always require a complete solution vector."}, {"heading": "1.1 The standard Monte Carlo method", "text": "Consider the following linear system: x = Hx + bwhere H'Rn \u00b7 n and x, b'Rn and where our goal is to evaluate the functionality < h, x > = \u2211 ni = 1 hixi. We could then use these primitives to calculate the solution by evaluating the functionality for each standard base vector to obtain each individual component of the solution. It is known that if the spectral radius \u03c1 (H) < 1, then the Neumann series \u2211 '= 0H'b will converge with the solution vector x. Monte Carlo method uses this observation to generate a Markov random path Xt in the state space S = {1, 2, \u00b7 \u00b7 \u00b7, n} with the initial probability Pr (X0 = i) = pi and transition probability Pr (X' + 1 = j | X = lt.K)."}, {"heading": "1.2 Our Contributions", "text": "To apply the Monte Carlo Method, the existing work [8, 16, 17] assumes that the method \"H\" < 1 (for the infinity standard \"H\" = \"maxi\" = \"Hi,\" \"Hi\" |) is sufficient to indicate Var [X] < \u221e, but is a stronger condition than \"R\" (H) < 1. Although it is possible to construct Var [X] < \u221e if \"H\" \u2265 1 is present, there is no easy way to verify this. To solve this problem, we propose a multiple Markov random path using multiple transition matrices. At each step of the random path, the transition matrix is constructed in a similar way to the Monte Carlo Fast Optimal Frame (MAO) [8, 12]. We prove that in this type of random path, the new method converges whenever (H +) < 1, if H + the non-negative matrix is implemented faster than the local one, then the \"Hij\" + iH = tendency."}, {"heading": "1.3 Related Work", "text": "Research into the Monte Carlo Methods for Linear Systems can be divided into two classes: direct methods and hybrid methods. Direct methods examine the various techniques of using the Monte Carlo solvers themselves, such as non-diagonal splitting [16] and relaxation parameters [7]. Hybrid methods [11, 10, 1] use Monte Carlo as a black box combined with iterative techniques. Examples of this work are the Monte Carlo sequential method [11] and the synthetic acceleration method [10]. In addition, there are a number of parallel implementation studies [6, 14, 1], real-world application [17, 2], convergence analysis [12] and spectral analysis [15]. In this paper, we focus on the direct Monte Carlo method. Our ideas can also be incorporated into the hybrid frames to improve performance."}, {"heading": "2 Multi-way Markov Random Walk", "text": "In this section, we generalize the idea of the random walk to estimate functionality based on a hypermatrix of transitions to calculate the estimate. Then, we analyze the convergence of simulations based on the variance of the relevant random variables. We use upper case letters like A to denote matrices, and lower case letters like x to denote vectors. Hypermatrices like P are bold, underlined, and uppercase. We use letters with drawings of indices to denote elements xi of a vector, and Ai, j of a matrix. In a mode 3 hypermatrix P, their elements are denoted by P (') i, j."}, {"heading": "2.1 Hypermatrix Transitions", "text": "Instead of using a fixed transition matrix P as in the classical Monte Carlo method in section 1, we allow the random path to vary transition matrices at each step. A m \u2212 path Markov random path (p, P) can be interpreted as walking over m different transition matrices, which run periodically in a round robin manner. Formally, we define a m \u2212 path Markov random path (p, P) as Zt: k0 \u2192 k1 \u2192 k2 \u2192 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7, with the initial probability following p and the transition probability following a hypermatrix P: Pr (k0 = i) = piPr (k '+ 1 = j | k' = i) = P (mod (', m) + 1) i, j. (1) Here, mod (', m) stands for the rest after the division 'by m. For simplicity of notation, we use P (') i, j for P (mod (', \u2212 1) + 2.), j = \u00b7 i"}, {"heading": "2.2 The Multi-way Monte Carlo Method", "text": "Our goal is to understand the mode of operation < h, x > where x is the solution of the linear system = > b = > b = = > b = = b =. Through the paper we have the basic assumption \u03c1 (H) < 1. We also exclude the corner cases in which h is a zero vector, or H has zero rows / columns. If we construct the initial probability so that hi 6 = 0 \u21d2 pi 6 = 0 and the transition hypermatrix P so that Hi, j 6 = 0 \u21d2 P (') i, j 6 = 0, then we can similarly define the related weights W' and the variable Z with section 1, and formally: W 'kkk0Hk0 \u2212 k0, k1Hk1, k2 \u00b7 Hk' \u2212 1, k'pk0P (1) k0, k1, k1, k1, k1 (k1) k1, k1 (k1), k1 (k1), k1 (k1), k1 (k1)."}, {"heading": "2.3 Convergence Analysis", "text": "To be able to make a statistical estimate, we must make sure that Var [Z] k = K = K = K = K = K (K = K = K = K = K (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K ="}, {"heading": "3 Multi-way Monte Carlo Method", "text": "In this section, we will discuss the two aspects of applying the Monte Carlo Method based on the multiple Markov random path introduced in Section 2. First, we will explain the construction of the transition hypermatrix P. Second, we will provide the error analysis with respect to the random path section and the probable error."}, {"heading": "3.1 Transition Hypermatrix", "text": "In section 2 Corollary 2.3 and 2.4 we point out that the spectral radius of the matrix H (1) is decisive for the variance Var [Z]. The matrix H (1) is determined by the transition hypermatrix P. Since it is usually inefficient to calculate the spectral radius of a matrix directly, the common practice is to find an upper limit of the matrix (H). The spectral radius of a matrix is limited by some sub-multiplicative matrix norm. As before, we use the infinity norm. \u00b7 In this paper, we first consider the case for the standard Markov Random Walk (i.e., m = 1) in which H (1), j = H2i, j / P (1) i, j / P (1) i (1), j / P (12) provides insight into what the probability is in relation to the minimization of the norm. Lemma 3.1."}, {"heading": "3.2 Random Walk Error Analysis", "text": "To practically estimate the value < h, x > of the simulation of the value of Z, we have to shorten the Markov multiple random path so that it ends after a large number of steps N. The practical solution [7, 3] to determining N is by the criterion: | WN | \u2264 | W0 | where > 0 denotes a small number. In case the initial probability is pi = | hi | / \u2211 n = 1 | hj |, we have W0 = h. We note that WN is a random variable, and follow the similar analysis with it in Theorem 2.1, it is easy to see its expected value as < h, (H +) Ne >. So it is a necessary condition to determine the truncation number N. Here we can see that our m \u2212 path Markov random path has the minimum requirements for H because it (H +) < 1 is required for all Monte Carlo random work to fulfill this condition."}, {"heading": "4 Numerical Experiments", "text": "In this section, we perform numerical experiments to demonstrate the two most important improvements of our new method 1. In section 4.1, we show that our new method can solve more problems than the standard method. In section 4.2, we show that our new method is available with considerable 1codes for this work at https: / / github.com / wutao27 / multi-way-MCspeed-up.The test methods are the standard 1-way method and our multi-way methods with m = 2, 3, 4, 5. In both experiments, synthetic matrices and real matrices are used. For synthetic data, we generate matrix H over the command line (1000, 1000, 0.2), which yields a 1 000 by 1 000 matrix, with 0.2 of its entries being non-zeros and each non-zero matrix matrix being a random number following a uniform distribution between (0, 1).The synthetic matrices are shortened to achieve certain elements while we need the experiments."}, {"heading": "4.1 The Number of Solvable Problems", "text": "The ratio of solvable problems is the percentage of random problems that can be solved. Figure 1 shows the results because we vary the spectral radius. As we see, our multipath methods can solve more problems than the standard method, which cannot guarantee convergence if \u03c1 (H +) \u2265 0.85. And if m increases, even more problems can be solved. We also find several real matrices where the standard method cannot guarantee convergence, but ours. These are the matrices fs _ 760 _ 1, jpwh _ 991, nos7 from the Harwell-Boeing Collection and add32 from the Hamm matrix group 2."}, {"heading": "4.2 Algorithm Efficiency", "text": "We apply the conclusion in Theorem 3 to calculate Var [Z] for all test methods, then compare Var [Z] for different methods. Formally, we define acceleration times as Var [X] / Var [Z], where X and Z denote the variable of the standard 1-way method or our method. The2http: / / www.cise.ufl.edu / research / sparse / matrices / Hamm / index.htmlspeed-up-times is an indicator of how much faster our reusable methods can be compared with the standard 1-way method. Table 1 shows that we have considerable acceleration in the application of our reusable methods. Note that we only have the problems with (H) < 1 with respect to Var [Z] <."}, {"heading": "5 Conclusion", "text": "In this paper, we investigated a generalization of the Monte Carlo methods for linear systems. Generalization allows the Markov random passage to the transition by means of a series of matrices. We derive the variance of the resulting estimator and construct the matrices in a manner that attempts to generate a finite variance. The advantages of this new random walk method are twofold: firstly, it can solve more problems than the standard method cannot solve; secondly, our new method has a tendency to reduce the variance, thereby reducing the calculations required for estimating the solution. Numerical experiments on both synthetic and real matrices confirm the superiority of our method in the above two aspects compared to the standard Monte Carlo method. One obvious problem that suggests our work is to obtain a purely local method that avoids global work in constructing the order of adjacent matrices."}], "references": [{"title": "Parallel hybrid Monte Carlo algorithms for matrix computations", "author": ["V. Alexandrov", "E. Atanassov", "I. Dimov", "S. Branford", "A. Thandavan", "C. Weihrauch"], "venue": "In International Conference on Computational Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2005}, {"title": "Monte Carlo methods in PageRank computation: When one iteration is sufficient", "author": ["K. Avrachenkov", "N. Litvak", "D. Nemirovsky", "N. Osipova"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Analysis of Monte Carlo accelerated iterative methods for sparse linear systems", "author": ["M. Benzi", "T. Evans", "S. Hamilton", "M.L. Pasini", "S. Slattery"], "venue": "Technical Report Math/CS Technical Report TR-2016-002,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "The university of florida sparse matrix collection", "author": ["T.A. Davis", "Y. Hu"], "venue": "ACM Transactions on Mathematical Software (TOMS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Scalar and parallel optimized implementation of the direct simulation Monte Carlo method", "author": ["S. Dietrich", "I.D. Boyd"], "venue": "Journal of Computational Physics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Parallel resolvent Monte Carlo algorithms for linear algebra problems", "author": ["I. Dimov", "V. Alexandrov", "A. Karaivanova"], "venue": "Mathematics and Computers in Simulation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2001}, {"title": "A new iterative Monte Carlo approach for inverse matrix problem", "author": ["I. Dimov", "T. Dimov", "T. Gurov"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1998}, {"title": "A new walk on equations Monte Carlo method for solving systems of linear algebraic equations", "author": ["I. Dimov", "S. Maire", "J.M. Sellier"], "venue": "Applied Mathematical Modelling,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Users\u2019 guide for the harwell-boeing sparse matrix collection", "author": ["I.S. Duff", "R.G. Grimes", "J.G. Lewis"], "venue": "(release i),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1992}, {"title": "A Monte Carlo syntheticacceleration method for solving the thermal radiation diffusion equation", "author": ["T.M. Evans", "S.W. Mosher", "S.R. Slattery", "S.P. Hamilton"], "venue": "Journal of Computational Physics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Sequential Monte Carlo techniques for the solution of linear systems", "author": ["J.H. Halton"], "venue": "Journal of Scientific Computing,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1994}, {"title": "Convergence analysis of Markov chain Monte Carlo linear solvers using Ulam-von Neumann algorithm", "author": ["H. Ji", "M. Mascagni", "Y. Li"], "venue": "SIAM Journal on Numerical Analysis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A parallel implementation of the direct simulation Monte Carlo method", "author": ["G. LeBeau"], "venue": "Computer Methods in Applied Mechanics and Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Parallel Monte Carlo Synthetic Acceleration methods for discrete transport problems", "author": ["S.R. Slattery"], "venue": "PhD thesis, University of Wisconsin Madison,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "A spectral analysis of the domain decomposed Monte Carlo method for linear systems", "author": ["S.R. Slattery", "T.M. Evans", "P.P. Wilson"], "venue": "Nuclear Engineering and Design,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Monte Carlo linear solvers with non-diagonal splitting", "author": ["A. Srinivasan"], "venue": "Mathematics and Computers in Simulation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2010}, {"title": "A Monte Carlo method for solving unsteady adjoint equations", "author": ["Q. Wang", "D. Gleich", "A. Saberi", "N. Etemadi", "P. Moin"], "venue": "Journal of Computational Physics,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A note on the inversion of matrices by random walks", "author": ["W. Wasow"], "venue": "Mathematical Tables and Other Aids to Computation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1952}], "referenceMentions": [{"referenceID": 17, "context": "The Monte Carlo method [18] for solving a linear system uses a random walk to approximate the solution.", "startOffset": 23, "endOffset": 27}, {"referenceID": 1, "context": "First, the Monte Carlo method can be highly effective when only modest accuracy is required, as is common for many problems on data such as PageRank computations [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 12, "context": "Second, it is well-known that Monte Carlo algorithms are highly parallelizable [13, 5], thus they are ideal for modern paralleled computers or clusters.", "startOffset": 79, "endOffset": 86}, {"referenceID": 4, "context": "Second, it is well-known that Monte Carlo algorithms are highly parallelizable [13, 5], thus they are ideal for modern paralleled computers or clusters.", "startOffset": 79, "endOffset": 86}, {"referenceID": 16, "context": "Third, Monte Carlo methods can identify only on a single component or a linear form of the solution, which is often all that is required in many applications [17].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": "Last but not least, Monte Carlo methods have advantages when dealing with large linear systems [11, 7] because they do not always require a full solution vector.", "startOffset": 95, "endOffset": 102}, {"referenceID": 6, "context": "Last but not least, Monte Carlo methods have advantages when dealing with large linear systems [11, 7] because they do not always require a full solution vector.", "startOffset": 95, "endOffset": 102}, {"referenceID": 6, "context": "Then it can be shown (for instance [7]) that E[X] = \u3008h,x\u3009, and more specifically E[W`fk` ] = \u3008h,H f\u3009.", "startOffset": 35, "endOffset": 38}, {"referenceID": 11, "context": "However the random walk model does not guarantee convergence [12, 18].", "startOffset": 61, "endOffset": 69}, {"referenceID": 17, "context": "However the random walk model does not guarantee convergence [12, 18].", "startOffset": 61, "endOffset": 69}, {"referenceID": 11, "context": "Empirical studies [12, 3] show that it is easy to have Var[X] =\u221e even when the Neumann Series converges (i.", "startOffset": 18, "endOffset": 25}, {"referenceID": 2, "context": "Empirical studies [12, 3] show that it is easy to have Var[X] =\u221e even when the Neumann Series converges (i.", "startOffset": 18, "endOffset": 25}, {"referenceID": 7, "context": "2 Our Contributions In order to apply the Monte Carlo method, existing work [8, 16, 17] assumes \u2016H\u2016 < 1 (for the infinity norm \u2016H\u2016 = maxi \u2211 j |Hi,j |), which suffices to show Var[X] <\u221e, but which is a stronger condition than \u03c1(H) < 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 15, "context": "2 Our Contributions In order to apply the Monte Carlo method, existing work [8, 16, 17] assumes \u2016H\u2016 < 1 (for the infinity norm \u2016H\u2016 = maxi \u2211 j |Hi,j |), which suffices to show Var[X] <\u221e, but which is a stronger condition than \u03c1(H) < 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 16, "context": "2 Our Contributions In order to apply the Monte Carlo method, existing work [8, 16, 17] assumes \u2016H\u2016 < 1 (for the infinity norm \u2016H\u2016 = maxi \u2211 j |Hi,j |), which suffices to show Var[X] <\u221e, but which is a stronger condition than \u03c1(H) < 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 7, "context": "At each step of the random walk, the transition matrix is constructed in a way akin to the Monte Carlo Almost Optimal (MAO) framework [8, 12].", "startOffset": 134, "endOffset": 141}, {"referenceID": 11, "context": "At each step of the random walk, the transition matrix is constructed in a way akin to the Monte Carlo Almost Optimal (MAO) framework [8, 12].", "startOffset": 134, "endOffset": 141}, {"referenceID": 15, "context": "Direct methods study the various techniques of using the Monte Carlo solvers themselves, for instances non-diagonal splitting [16] and relaxation parameters [7].", "startOffset": 126, "endOffset": 130}, {"referenceID": 6, "context": "Direct methods study the various techniques of using the Monte Carlo solvers themselves, for instances non-diagonal splitting [16] and relaxation parameters [7].", "startOffset": 157, "endOffset": 160}, {"referenceID": 10, "context": "Hybrid methods [11, 10, 1] use Monte Carlo as a black box combined with iterative techniques.", "startOffset": 15, "endOffset": 26}, {"referenceID": 9, "context": "Hybrid methods [11, 10, 1] use Monte Carlo as a black box combined with iterative techniques.", "startOffset": 15, "endOffset": 26}, {"referenceID": 0, "context": "Hybrid methods [11, 10, 1] use Monte Carlo as a black box combined with iterative techniques.", "startOffset": 15, "endOffset": 26}, {"referenceID": 10, "context": "Examples of these works are Sequential Monte Carlo method [11] and synthetic-acceleration method [10].", "startOffset": 58, "endOffset": 62}, {"referenceID": 9, "context": "Examples of these works are Sequential Monte Carlo method [11] and synthetic-acceleration method [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 5, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 67, "endOffset": 77}, {"referenceID": 13, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 67, "endOffset": 77}, {"referenceID": 0, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 67, "endOffset": 77}, {"referenceID": 16, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 1, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 102, "endOffset": 109}, {"referenceID": 11, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "Also there are a variety of studies of the parallel implementation [6, 14, 1], real world application [17, 2], convergence analysis [12], and spectral analysis [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 11, "context": "The following lemma [12] provides insight on how to assign the probability in terms of minimizing the norm.", "startOffset": 20, "endOffset": 24}, {"referenceID": 2, "context": "It is common to have \u03c1(H) be very close to 1 even with the help of preconditioners [3].", "startOffset": 83, "endOffset": 86}, {"referenceID": 2, "context": "Since the infinity norm is generally a loose upper-bound for the spectral radius, \u2016H\u2016 > 1 is likely [3].", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "The practical solution [7, 3] to determine N is through the criterion: |WN | \u2264 |W0| where > 0 denotes some small number.", "startOffset": 23, "endOffset": 29}, {"referenceID": 2, "context": "The practical solution [7, 3] to determine N is through the criterion: |WN | \u2264 |W0| where > 0 denotes some small number.", "startOffset": 23, "endOffset": 29}, {"referenceID": 8, "context": "For real world matrices, we focus on the Harwell-Boeing sparse matrix collection [9, 4].", "startOffset": 81, "endOffset": 87}, {"referenceID": 3, "context": "For real world matrices, we focus on the Harwell-Boeing sparse matrix collection [9, 4].", "startOffset": 81, "endOffset": 87}], "year": 2016, "abstractText": "We study the Monte Carlo method for solving a linear system of the form x = Hx + b. A sufficient condition for the method to work is \u2016H\u2016 < 1, which greatly limits the usability of this method. We improve this condition by proposing a new multi-way Markov random walk, which is a generalization of the standard Markov random walk. Under our new framework we prove that the necessary and sufficient condition for our method to work is the spectral radius \u03c1(H) < 1, which is a weaker requirement than \u2016H\u2016 < 1. In addition to solving more problems, our new method can work faster than the standard algorithm. In numerical experiments on both synthetic and real world matrices, we demonstrate the effectiveness of our new method. ar X iv :1 60 8. 04 36 1v 1 [ cs .N A ] 1 5 A ug 2 01 6", "creator": "LaTeX with hyperref package"}}}