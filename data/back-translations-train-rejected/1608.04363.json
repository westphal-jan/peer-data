{"id": "1608.04363", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification", "abstract": "The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a \"shallow\" dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model's classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.", "histories": [["v1", "Mon, 15 Aug 2016 18:57:10 GMT  (106kb,D)", "http://arxiv.org/abs/1608.04363v1", null], ["v2", "Mon, 28 Nov 2016 17:48:04 GMT  (107kb,D)", "http://arxiv.org/abs/1608.04363v2", "Accepted November 2016, IEEE Signal Processing Letters. Copyright IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material, creating new collective works, for resale or redistribution, or reuse of any copyrighted component of this work in other works"]], "reviews": [], "SUBJECTS": "cs.SD cs.CV cs.LG cs.NE", "authors": ["justin salamon", "juan pablo bello"], "accepted": false, "id": "1608.04363"}, "pdf": {"name": "1608.04363.pdf", "metadata": {"source": "CRF", "title": "Deep Convolutional Neural Networks and Data Augmentation for Environmental Sound Classification", "authors": ["Justin Salamon", "Juan Pablo Bello"], "emails": ["(justin.salamon@nyu.edu)", "(jpbello@nyu.edu)"], "sections": [{"heading": null, "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "II. METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Deep Convolutional Neural Network", "text": "The deep revolutionary architecture proposed in this study is surrounded by 3 revolutionary layers associated with 2 pooling operations, followed by 2 fully connected (dense) layers. Similar to previously proposed feature approaches applied to environmental noise classification (e.g., [7]), input to the network consists of time frequency patches (TF patches) covering the audible frequency range (0-22050 Hz), using a window size of 23 ms (1024 ms) and a hop size of the same duration. Since the excerpts in our evaluation dataset (described) are of varying duration (up to 4 s), we adjust the size of the input of TF-Patch-X to 12seconds (12x)."}, {"heading": "B. Data Augmentation", "text": "We experiment with 4 different audio data augmentations (deformations), resulting in 5 augmentation sets as described below. Each deformation is applied directly to the audio signal before it is converted into the input representation used to train the network (log-mel spectrogram). Note that for each augmentation it is important to select the deformation parameters to maintain the semantic validity of the label. \u2022 The deformations and resulting augmentation sets are described below: \u2022 Time Stretching (while the audio sample remains unchanged). Each sample was stretched by four factors: {0.81, 0.93, 1.23}. Pitch Shifting (PS1): Increase or decrease the audio sample (while the audio sample remains unchanged)."}, {"heading": "C. Evaluation", "text": "To evaluate the proposed CNN architecture and the impact of the different augmentation sets, we use the UrbanSound8K dataset [17]. The dataset consists of 8732 sound clips of up to 4 s duration from field recordings. The clips include 10 environmental noise classes: air conditioning, car horn, children playing, dog barking, drilling, idle, cannon shot, jackhammer, siren and street music. Using this dataset, we can compare the results of this study with previously published approaches that have been evaluated on the same data, including the dictionary learning approach proposed in [7] and the CNN approach proposed in [11], which has a different architecture from ours and does not use augmentation during training.The proposed approach and the approaches used in this study are evaluated for classification accuracy, the dataset is evaluated in 10 layered torture, and all models were evaluated on the basis of 10 cross-validated results we obtained for the cross-validation of the 1."}, {"heading": "III. RESULTS", "text": "The reality is that most people who work for people's rights are not prepared to stand up for people's rights, but to stand up for people's rights, for people's rights, for people's rights, for people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, people's rights, the rights, people's rights, people's rights, people's rights, people's rights, the rights, people's rights, people's rights, the rights, people's rights, the rights, the rights,"}, {"heading": "IV. CONCLUSION", "text": "In this article, we proposed a deep Convolutionary Neural Network Architecture that, when combined with a range of audio data enhancements, delivers state-of-the-art results for classifying environmental noise. We demonstrated that the improved performance was due to the combination of a deep, powerful model and an advanced training set: this combination exceeded both the proposed CNN without augmentation and a \"shallow\" dictionary learning model with augmentation. Finally, we investigated the impact of each augmentation on the classification accuracy of the model. We observed that the performance of the model for each sound class is affected differently by each augmentation set, suggesting that the performance of the model could be further improved by applying class-based data augmentation."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors thank Brian McFee and Eric Humphrey for their valuable feedback and Karol Piczak for explaining the results reported in [11]."}], "references": [{"title": "Environmental sound recognition with time-frequency audio features", "author": ["S. Chu", "S. Narayanan", "C.-C. Kuo"], "venue": "IEEE Trans. on Audio, Speech, and Language Processing, vol. 17, no. 6, pp. 1142\u20131158, Aug. 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio analysis for surveillance applications", "author": ["R. Radhakrishnan", "A. Divakaran", "P. Smaragdis"], "venue": "IEEE Worksh. on Apps. of Signal Processing to Audio and Acoustics (WASPAA\u201905), New Paltz, NY, USA, Oct. 2005, pp. 158\u2013161.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "The implementation of lowcost urban acoustic monitoring devices", "author": ["C. Mydlarz", "J. Salamon", "J.P. Bello"], "venue": "Applied Acoustics, vol. In Press, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Sound event detection in real life recordings using coupled matrix factorization of spectral representations and class activity annotations", "author": ["A. Mesaros", "T. Heittola", "O. Dikmen", "T. Virtanen"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015, pp. 151\u2013155.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Detection of overlapping acoustic events using a temporally-constrained probabilistic model", "author": ["E. Benetos", "G. Lafay", "M. Lagrange", "M.D. Plumbley"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, Mar. 2016, pp. 6450\u2013 6454.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Acoustic scene classification with matrix factorization for unsupervised feature learning", "author": ["V. Bisot", "R. Serizel", "S. Essid", "G. Richard"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, Mar. 2016, pp. 6445\u20136449.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Unsupervised feature learning for urban sound classification", "author": ["J. Salamon", "J.P. Bello"], "venue": "IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), Brisbane, Australia, Apr. 2015, pp. 171\u2013175.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Feature learning with deep scattering for urban sound analysis", "author": ["\u2014\u2014"], "venue": "2015 European Signal Processing Conference, Nice, France, Aug. 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving event detection for audio surveillance using gabor filterbank features", "author": ["J.T. Geiger", "K. Helwani"], "venue": "23rd European Signal Processing Conference (EUSIPCO), Nice, France, Aug. 2015, pp. 714\u2013 718.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Polyphonic sound event detection using multi label deep neural networks", "author": ["E. Cakir", "T. Heittola", "H. Huttunen", "T. Virtanen"], "venue": "2015 International Joint Conference on Neural Networks (IJCNN), July 2015, pp. 1\u20137.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["K.J. Piczak"], "venue": "25th International Workshop on Machine Learning for Signal Processing (MLSP), Boston, MA, USA, Sep. 2015, pp. 1\u20136.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Detection and classification of acoustic scenes and events: An IEEE AASP challenge", "author": ["D. Giannoulis", "E. Benetos", "D. Stowell", "M. Rossignol", "M. Lagrange", "M.D. Plumbley"], "venue": "IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), New Paltz, NY, USA, Oct. 2013, pp. 1\u20134.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Detection and classification of acoustic scenes and events", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M.D. Plumbley"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 10, pp. 1733\u20131746, Oct. 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic environmental sound recognition: Performance versus computational cost", "author": ["S. Sigtia", "A. Stark", "S. Krstulovic", "M. Plumbley"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. PP, no. 99, pp. 1\u20131, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov. 1998.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "Spectral vs. spectro-temporal features for acoustic event detection", "author": ["C.V. Cotton", "D.P.W. Ellis"], "venue": "IEEE Worksh. on Apps. of Signal Processing to Audio and Acoustics (WASPAA), New Paltz, NY, USA, Oct. 2011, pp. 69\u201372.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "A dataset and taxonomy for urban sound research", "author": ["J. Salamon", "C. Jacoby", "J.P. Bello"], "venue": "22nd ACM International Conference on Multimedia (ACM-MM\u201914), Orlando, FL, USA, Nov. 2014, pp. 1041\u20131044.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "ESC: Dataset for environmental sound classification", "author": ["K.J. Piczak"], "venue": "23rd ACM International Conference on Multimedia, Brisbane, Australia, Oct. 2015, pp. 1015\u20131018.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "TUT sound events 2016, development dataset", "author": ["A. Mesaros", "E. Fagerlund", "A. Hiltunen", "T. Heittola", "T. Virtanen"], "venue": "Available online: http://dx.doi.org/10.5281/zenodo.45759 (accessed 10 August 2016), 2016. [Online]. Available: http://dx.doi.org/10.5281/zenodo.45759", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "Advances in neural information processing systems (NIPS), 2012, pp. 1097\u20131105.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Best practices for convolutional neural networks applied to visual document analysis.", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "in International Conference on Document Analysis and Recognition,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2003}, {"title": "A software framework for musical data augmentation", "author": ["B. McFee", "E. Humphrey", "J. Bello"], "venue": "16th Int. Soc. for Music Info. Retrieval Conf., Malaga, Spain, Oct. 2015, pp. 248\u2013254.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural networks for polyphonic sound event detection in real life recordings", "author": ["G. Parascandolo", "H. Huttunen", "T. Virtanen"], "venue": "International Conference on Acoustics, Speech and Signal Processing (ICASSP), Shanghai, China, Mar. 2016, pp. 6440\u20136444.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "ESSENTIA: an audio analysis library for music information retrieval", "author": ["D. Bogdanov", "N. Wack", "E. G\u00f3mez", "S. Gulati", "P. Herrera", "O. Mayor", "G. Roma", "J. Salamon", "J. Zapata", "X. Serra"], "venue": "14th Int. Soc. for Music Info. Retrieval Conf., Curitiba, Brazil, Nov. 2013, pp. 493\u2013498.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "19th International Conference on Computational Statistics (COMPSTAT), Paris, France, Aug. 2010, pp. 177\u2013186. [Online]. Available: http://dx.doi.org/10.1007/978-3-7908-2604-3 16", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1929}, {"title": "Lasagne: First release", "author": ["S. Dieleman", "J. Schl\u00fcter", "C. Raffel", "E. Olson", "S. S\u00f8nderby", "D. Nouri", "D. Maturana", "M. Thoma", "E. Battenberg", "J. Kelly"], "venue": "https://github.com/Lasagne/Lasagne, 2015. [Online]. Available: http://dx.doi.org/10.5281/zenodo.27878", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "pescador: 0.1.0", "author": ["B. McFee", "E.J. Humphrey"], "venue": "https://github.com/ bmcfee/pescador, 2015. [Online]. Available: http://dx.doi.org/10.5281/ zenodo.32468", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Standards and practices for authoring Dolby Digital and Dolby E bitstreams", "author": ["Dolby Labortories", "Inc."], "venue": "2002.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "JAMS: A JSON annotated music specification for reproducible MIR research", "author": ["E.J. Humphrey", "J. Salamon", "O. Nieto", "J. Forsyth", "R. Bittner", "J.P. Bello"], "venue": "15th Int. Soc. for Music Info. Retrieval Conf., Taipei, Taiwan, Oct. 2014, pp. 591\u2013596.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Pump up the JAMS: V0.2 and beyond", "author": ["B. McFee", "E.J. Humphrey", "O. Nieto", "J. Salamon", "R. Bittner", "J. Forsyth", "J.P. Bello"], "venue": "Music and Audio Research Laboratory, New York University, Tech. Rep., Oct. 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Its applications range from context aware computing [1] and surveillance [2] to noise mitigation enabled by smart acoustic sensor networks [3].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "Its applications range from context aware computing [1] and surveillance [2] to noise mitigation enabled by smart acoustic sensor networks [3].", "startOffset": 73, "endOffset": 76}, {"referenceID": 2, "context": "Its applications range from context aware computing [1] and surveillance [2] to noise mitigation enabled by smart acoustic sensor networks [3].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "To date, a variety of signal processing and machine learning techniques have been applied to the problem, including matrix factorization [4]\u2013[6], dictionary learning [7], [8], wavelet filterbanks [8], [9] and most recently deep neural networks [10], [11].", "startOffset": 137, "endOffset": 140}, {"referenceID": 5, "context": "To date, a variety of signal processing and machine learning techniques have been applied to the problem, including matrix factorization [4]\u2013[6], dictionary learning [7], [8], wavelet filterbanks [8], [9] and most recently deep neural networks [10], [11].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "To date, a variety of signal processing and machine learning techniques have been applied to the problem, including matrix factorization [4]\u2013[6], dictionary learning [7], [8], wavelet filterbanks [8], [9] and most recently deep neural networks [10], [11].", "startOffset": 166, "endOffset": 169}, {"referenceID": 7, "context": "To date, a variety of signal processing and machine learning techniques have been applied to the problem, including matrix factorization [4]\u2013[6], dictionary learning [7], [8], wavelet filterbanks [8], [9] and most recently deep neural networks [10], [11].", "startOffset": 171, "endOffset": 174}, {"referenceID": 7, "context": "To date, a variety of signal processing and machine learning techniques have been applied to the problem, including matrix factorization [4]\u2013[6], dictionary learning [7], [8], wavelet filterbanks [8], [9] and most recently deep neural networks [10], [11].", "startOffset": 196, "endOffset": 199}, {"referenceID": 8, "context": "To date, a variety of signal processing and machine learning techniques have been applied to the problem, including matrix factorization [4]\u2013[6], dictionary learning [7], [8], wavelet filterbanks [8], [9] and most recently deep neural networks [10], [11].", "startOffset": 201, "endOffset": 204}, {"referenceID": 9, "context": "To date, a variety of signal processing and machine learning techniques have been applied to the problem, including matrix factorization [4]\u2013[6], dictionary learning [7], [8], wavelet filterbanks [8], [9] and most recently deep neural networks [10], [11].", "startOffset": 244, "endOffset": 248}, {"referenceID": 10, "context": "To date, a variety of signal processing and machine learning techniques have been applied to the problem, including matrix factorization [4]\u2013[6], dictionary learning [7], [8], wavelet filterbanks [8], [9] and most recently deep neural networks [10], [11].", "startOffset": 250, "endOffset": 254}, {"referenceID": 11, "context": "See [12]\u2013[14] for further reviews of existing approaches.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "See [12]\u2013[14] for further reviews of existing approaches.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "In particular, deep convolutional neural networks (CNN) [15] are, in principle, very well suited to the problem of environmental sound classification: first, they are capable of capturing energy modulation patterns across time and frequency when applied to spectrogram-like inputs, which has been shown to be an important trait for distinguishing between different, often noise-like, sounds such as engines and jackhammers [8].", "startOffset": 56, "endOffset": 60}, {"referenceID": 7, "context": "In particular, deep convolutional neural networks (CNN) [15] are, in principle, very well suited to the problem of environmental sound classification: first, they are capable of capturing energy modulation patterns across time and frequency when applied to spectrogram-like inputs, which has been shown to be an important trait for distinguishing between different, often noise-like, sounds such as engines and jackhammers [8].", "startOffset": 423, "endOffset": 426}, {"referenceID": 15, "context": "receptive field, the network should, in principle, be able to successfully learn and later identify spectro-temporal patterns that are representative of different sound classes even if part of the sound is masked (in time/frequency) by other sources (noise), which is where traditional audio features such as MelFrequency Cepstral Coefficients (MFCC) fail [16].", "startOffset": 356, "endOffset": 360}, {"referenceID": 10, "context": "For instance, the CNN proposed in [11] obtained comparable results to those yielded by a dictionary learning approach [7] (which can be considered an instance of \u201cshallow\u201d feature learning), but did not improve upon it.", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "For instance, the CNN proposed in [11] obtained comparable results to those yielded by a dictionary learning approach [7] (which can be considered an instance of \u201cshallow\u201d feature learning), but did not improve upon it.", "startOffset": 118, "endOffset": 121}, {"referenceID": 16, "context": ", [17]\u2013[19]), they are still considerably smaller than the datasets available for research on, for example, image classification [20].", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [17]\u2013[19]), they are still considerably smaller than the datasets available for research on, for example, image classification [20].", "startOffset": 7, "endOffset": 11}, {"referenceID": 19, "context": ", [17]\u2013[19]), they are still considerably smaller than the datasets available for research on, for example, image classification [20].", "startOffset": 129, "endOffset": 133}, {"referenceID": 19, "context": "An elegant solution to this problem is data augmentation, that is, the application of one or more deformations to a collection of annotated training samples which result in new, additional training data [20]\u2013[22].", "startOffset": 203, "endOffset": 207}, {"referenceID": 21, "context": "An elegant solution to this problem is data augmentation, that is, the application of one or more deformations to a collection of annotated training samples which result in new, additional training data [20]\u2013[22].", "startOffset": 208, "endOffset": 212}, {"referenceID": 21, "context": "Semantics-preserving deformations have also been proposed for the audio domain, and have been shown to increase model accuracy for music classification tasks [22].", "startOffset": 158, "endOffset": 162}, {"referenceID": 10, "context": ", [11], [23]),", "startOffset": 2, "endOffset": 6}, {"referenceID": 22, "context": ", [11], [23]),", "startOffset": 8, "endOffset": 12}, {"referenceID": 10, "context": "with the author of [11] reporting that \u201csimple augmentation techniques proved to be unsatisfactory for the UrbanSound8K dataset given the considerable increase in training time they generated and negligible impact on model accuracy\u201d.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": ", [7]), the input to the network consists of timefrequency patches (TF-patches) taken from the log-scaled melspectrogram representation of the audio signal.", "startOffset": 2, "endOffset": 5}, {"referenceID": 23, "context": "Specifically, we use Essentia [24] to extract log-scaled mel-spectrograms with 128 components (bands) covering the audible frequency range (0-22050 Hz), using a window size of 23 ms (1024 samples at 44.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "For training, the model optimizes cross-entropy loss via mini-batch stochastic gradient descent [25].", "startOffset": 96, "endOffset": 100}, {"referenceID": 25, "context": "Dropout [26] is applied to the input of the last two layers, ` \u2208 {4, 5}, with probability 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "The CNN is implemented in Python with Lasagne [27], and we used Pescador [28] to manage and multiplex data streams during training.", "startOffset": 46, "endOffset": 50}, {"referenceID": 27, "context": "The CNN is implemented in Python with Lasagne [27], and we used Pescador [28] to manage and multiplex data streams during training.", "startOffset": 73, "endOffset": 77}, {"referenceID": 28, "context": "\u2022 Dynamic Range Compression (DRC): compress the dynamic range of the sample using 4 parameterizations, 3 taken from the Dolby E standard [29] and 1 (radio) from the icecast online radio streaming server [30]: {music standard, film standard, speech, radio}.", "startOffset": 137, "endOffset": 141}, {"referenceID": 21, "context": "The augmentations were applied using the MUDA library [22], to which the reader is referred for further details about the implementation of each deformation.", "startOffset": 54, "endOffset": 58}, {"referenceID": 29, "context": "MUDA takes an audio file and corresponding annotation file in JAMS format [31], [32], and outputs the deformed audio together with an enhanced JAMS file containing all the parameters used for the deformation.", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "MUDA takes an audio file and corresponding annotation file in JAMS format [31], [32], and outputs the deformed audio together with an enhanced JAMS file containing all the parameters used for the deformation.", "startOffset": 80, "endOffset": 84}, {"referenceID": 16, "context": "To evaluate the proposed CNN architecture and the influence of the different augmentation sets we use the UrbanSound8K dataset [17].", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "By using this dataset we can compare the results of this study to previously published approaches that were evaluated on the same data, including the dictionary learning approach proposed in [7] and the CNN proposed in [11] which has a different architecture to ours and did not employ augmentation during training.", "startOffset": 191, "endOffset": 194}, {"referenceID": 10, "context": "By using this dataset we can compare the results of this study to previously published approaches that were evaluated on the same data, including the dictionary learning approach proposed in [7] and the CNN proposed in [11] which has a different architecture to ours and did not employ augmentation during training.", "startOffset": 219, "endOffset": 223}, {"referenceID": 6, "context": "Left of the dashed line: classification accuracy without augmentation \u2013 dictionary learning (SKM [7]), Piczak\u2019s CNN (PiczakCNN [11]) and the proposed model (SB-CNN).", "startOffset": 97, "endOffset": 100}, {"referenceID": 10, "context": "Left of the dashed line: classification accuracy without augmentation \u2013 dictionary learning (SKM [7]), Piczak\u2019s CNN (PiczakCNN [11]) and the proposed model (SB-CNN).", "startOffset": 127, "endOffset": 131}, {"referenceID": 6, "context": "For comparison, we also provide the accuracy obtained on the same dataset by the dictionary learning approach proposed in [7] (SKM, using the best parameterization identified by the authors in that study) and the CNN proposed by Piczak [11] (PiczakCNN, using the best performing model variant (LP) proposed by the author).", "startOffset": 122, "endOffset": 125}, {"referenceID": 10, "context": "For comparison, we also provide the accuracy obtained on the same dataset by the dictionary learning approach proposed in [7] (SKM, using the best parameterization identified by the authors in that study) and the CNN proposed by Piczak [11] (PiczakCNN, using the best performing model variant (LP) proposed by the author).", "startOffset": 236, "endOffset": 240}, {"referenceID": 10, "context": "The authors would like to thank Brian McFee and Eric Humphrey for their valuable feedback, and Karol Piczak for providing details on the results reported in [11].", "startOffset": 157, "endOffset": 161}], "year": 2017, "abstractText": "The ability of deep convolutional neural networks (CNN) to learn discriminative spectro-temporal patterns makes them well suited to environmental sound classification. However, the relative scarcity of labeled data has impeded the exploitation of this family of high-capacity models. This study has two primary contributions: first, we propose a deep convolutional neural network architecture for environmental sound classification. Second, we propose the use of audio data augmentation for overcoming the problem of data scarcity and explore the influence of different augmentations on the performance of the proposed CNN architecture. Combined with data augmentation, the proposed model produces state-of-the-art results for environmental sound classification. We show that the improved performance stems from the combination of a deep, high-capacity model and an augmented training set: this combination outperforms both the proposed CNN without augmentation and a \u201cshallow\u201d dictionary learning model with augmentation. Finally, we examine the influence of each augmentation on the model\u2019s classification accuracy for each class, and observe that the accuracy for each class is influenced differently by each augmentation, suggesting that the performance of the model could be improved further by applying class-conditional data augmentation.", "creator": "LaTeX with hyperref package"}}}