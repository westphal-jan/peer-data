{"id": "1608.04428", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "TerpreT: A Probabilistic Programming Language for Program Induction", "abstract": "We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs. Our aims are to develop new machine learning approaches based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programming languages community.", "histories": [["v1", "Mon, 15 Aug 2016 22:34:50 GMT  (5474kb,D)", "http://arxiv.org/abs/1608.04428v1", "50 pages, 20 figures, 4 tables"]], "COMMENTS": "50 pages, 20 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["alexander l gaunt", "marc brockschmidt", "rishabh singh", "nate kushman", "pushmeet kohli", "jonathan taylor", "daniel tarlow"], "accepted": false, "id": "1608.04428"}, "pdf": {"name": "1608.04428.pdf", "metadata": {"source": "CRF", "title": "TerpreT: A Probabilistic Programming Language for Program Induction", "authors": ["Alexander L. Gaunt", "Marc Brockschmidt", "Rishabh Singh", "Nate Kushman", "Pushmeet Kohli", "Jonathan Taylor", "Daniel Tarlow"], "emails": ["t-algaun@microsoft.com", "mabrocks@microsoft.com", "risin@microsoft.com", "nkushman@microsoft.com", "pkohli@microsoft.com", "jtaylor@perceptiveio.com", "dtarlow@microsoft.com"], "sections": [{"heading": null, "text": "Our central contribution is the proposal of TerpreT, a domain-specific language for the formulation of program synthesis problems. TerpreT resembles a probabilistic programming language: A model consists of the specification of a program representation (explanations of random variables) and an interpreter who describes how programs map input to results (a model that combines the unknown with observations). The inference task consists of observing a series of input-output examples and deriving from them the underlying program. TerpreT has two main advantages: First, it enables the rapid investigation of a series of domains, program representations and interpreter models. Second, it separates the model specification from the input-output algorithm SYYYYYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSYSY"}, {"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far, until it is so far."}, {"heading": "2 Motivating Example: Differentiable Control Flow Graphs", "text": "As an introductory example, we describe a new execution model that we would like to use for IPS. In this section, we describe the high-level model. In later sections, we describe how to express the model in TerpreT and how to perform conclusions. Control flow diagrams (CFGs) (Allen, 1970) are a representation of programs commonly used for static analysis and compiler optimization. They consist of a series of basic blocks that contain sequences of non-jump statements (i.e., straight line code) followed by a jump or conditional jump statement to transfer control to another block. CFGs are expressive enough to represent all the constructs used in modern programming languages such as C + +. In fact, the intermediate representation of LLVM is based on basic blocks. Our first model is inspired by CFGs, but is limited to use a limited set of statements and does not support the model we are fixated on."}, {"heading": "3 Front-end: Describing an IPS problem", "text": "One of our central goals is to untangle the description of an execution model from the inference task so that we can make comparable comparisons between different inference approaches for the same IPS task. As a reference, the key components for solving an IPS problem are in Fig. 2. In forward mode, the system is analogous to a conventional interpreter, but in reverse mode, the system follows a representation of the source code, which only provides observed results from a series of inputs. Even before we develop an inference method, we need both a means for parameterizing the source code of the program and a precise description of the forward transformation of the interpreter. This section describes how these modeling tasks are accomplished in TerpreT.3.1. Full grammar for syntactically correct TerpreT programs is presented in Fig. 3, and we describe the most important semantic features of the language in the following sections: The Terpretatics-Complete Grammatics for T."}, {"heading": "3.1.1 Declarations and Assignments", "text": "We allow declarations to give \"magic\" constants names, as in line 1 of fig. 4. Additionally, we allow declarations of parameters and variables that range over a finite domain 0, 1,.. N \u2212 1 with param (N) and var (N), where n must be a compilation time constant (i.e. a natural number or an expression over constants). Parameters are used to model the source code to be inferred, while variables are used to model the calculation (i.e., intermediate values). For convenience, (multidimensional) variable arrays can be declared with the syntax foo = var (N) [dim1, dim2,...] and called as foo [idx1, idx2,...]. Similar syntax is available for params. These arrays can be unrolled during compilation so that distinct symbols representing each element are passed to an algorithm."}, {"heading": "3.1.2 Control flow", "text": "TerpreT supports standard control flow structures such as if-else (where elif is the usual abbreviation for else if) and for. In addition, TerpreT uses a unique element with structure. The need for the latter is caused by our requirement to use only compilation time constants to access arrays. So, to set the second element of the tape in our toy example (i.e., the first step of the calculation), we need code as follows to access the values of the first two values on the tape: 1 if band [1] = 0: 2 if band [0] = 0: 3 band [2].set to (ruleTable [0,0]) 4 elif tape [0] = 1: 5 band [1].set to (rule.set]."}, {"heading": "3.1.3 Operations", "text": "TerpreT supports user-defined functions to facilitate the modeling of interpreters that support non-trivial command sets. For example, the toolbar (arg1,.., argM) can be applied to the arguments arg1,.., argM. The toolbar: ZM \u2192 Z can be defined as a default Python function with the additional @ CompileMe decoration (in domains, out domains), specifying the domains of the input and output variables. To illustrate this function, Fig. 5 shows a variant of the running example in which the automaton updates the tape according to a rule table that only depends on the sum of the previous two entries. This is simplified by the add function in lines 3-6. Note that we define this function with default Python and let the compiler render the function according to the inference algorithm."}, {"heading": "3.1.4 Modelling Inputs and Outputs", "text": "Using statements from the preceding sections, an execution model can be fully specified, and we now combine this model with input / output observations to drive program induction, using the statements set to constant (or observation value) to model program input (or output), so that a single input / output observation for the running example could be written in TerpreT as follows. To separate the execution model from the observations, we store the observation snippets in a separate file and use pre-processor directive # IMPORT OBSERVED * to collect the corresponding output examples before compilation (see lines 13 and 18 of Fig. 4). We also allow all constant literals to be stored separately from the TerpreT execution model, and we implement these values using this processor directive # Vrays-AM to execute the additional program directive for Vlex = Vlex."}, {"heading": "3.2 Example Execution Models", "text": "To illustrate the versatility of TerpreT, we use it to describe four exemplary execution models. Broadly speaking, the examples of more abstract execution models evolve into models very similar to the assembly languages for RISC machines. In any case, we present the basic model and complete three representative synthesis tasks to be examined in Table 2. In addition, we provide the indicators for the \"difficulty\" of each task, which is calculated from the minimal computational resources required for a solution. Since the difficulty of a synthesis problem generally depends on the chosen inference algorithm, these indicators are primarily intended to give a sense of the extent of the problem. The first difficulty metric D is the number of structurally different (but not necessarily functionally different) programs that would have to be listed in the worst case of a brute force search, and the second metric T is the unrolled length of all steps in the synthesized program."}, {"heading": "3.2.1 Automaton: Turing Machine", "text": "A Turing machine consists of an infinite band of memory cells, each containing one of the S symbols, and a head that moves across the tape in one of the H + 1 states (a state is the special hold case). At each execution step, while in an unpaused state, the head reads the st symbol at its current position, xt, on the tape, then writes the newValue [st, ht] symbol to position xt, moves in the direction [st, ht] (a cell to the left or right or no movement), and assumes a new state newState [st, ht]. The source code for the Turing machine are the entries in the newValue, direction, and newState control tables, which can be in any configuration of D = [3S (H + 1)] SH."}, {"heading": "3.2.2 Straight-line programs: Boolean Circuits", "text": "As a more complex model, we now consider a simple machine capable of performing a sequence of logical operations (AND, OR, XOR, NOT, COPY) on a series of registers with Boolean values. Each operation takes two registers as input (the second register is ignored in NOT and COPY operation) and outputs them to a register reminiscent of standard languages for mounting three-address codes. To embed this example in a real application, analogies can be drawn that link the instruction set to electronic logic gates and link the registers to electronic wires. This analogy underscores an advantage of interpretability in our model: The synthesized program describes a digital circuit that could easily be translated to real hardware (see Fig. 20). The TerpreT implementation of this embodiment model is presented in Appendix B."}, {"heading": "3.2.3 Loopy programs 1: Basic block model", "text": "To build loopy execution models, we take inspiration from compiler intermediate languages (e.g. LLVM Intermediate Representation), model complete programs as graphs of \"Basic Blocks.\" Such programs operate on a fixed number of registers and a byte-addressable heap memory accessible by special statements, READ and WRITE. Each block has an instruction from the regout = instin1 regin2 form, followed by a branch decision if regcond > 0 goto blockthen goto blockelse (see Fig. 1, and the TerpreT model in Appendix B.3). This representation can be easily converted back and forth into higher-level program sources (by standard compilation / decompilation techniques) and into executable machine code.We use an instruction set containing H = 9 statements: ZERO, INC, DEC, ADD, SUB, LSTAN, HNOOP."}, {"heading": "3.2.4 Loopy programs 2: Assembly model", "text": "In the basic block model, each expression is followed by a conditional branch, giving the model great freedom to display rich control flow graphs. However, useful programs often execute a sequence of multiple expressions between each branch. Therefore, it may be advantageous to distort the model to create chains of sequentially ordered basic blocks with only occasional branching where necessary, by replacing the basic blocks with objects that are similar to lines in the assembly code.The command set is shown in Fig. 6 (and in the TerpreT code in Appendix B.4) with the jump-if-zero (JZ (Regin1): branchAddr) and jump-if-not-zero (JNZ (Regin1): branchAddr) jump instructions. Each line of code behaves like a conditional branch, acting like a conditional branch only when the assigned instance (JZ, other JZ) continues to execute the next branch and branch expression."}, {"heading": "4 Back-ends: Solving the IPS problem", "text": "TerpreT is a factor graph that represents the factorization of a complex function or probability distribution into a composition of relationships or distribution outcomes. This section outlines the compilation steps for each of the ones in Table 1. For each of these rear ends, we present the compiler transformation of the TerpreT primitives listed in Fig. 7. For some rear ends, we find it useful to represent these transformations using an intermediate graphical representation that resembles a factor graph, or more specifically, a gated factor graph (Minka and Winn, 2009) that visualizes the TerpreT program. We describe the gated factor graphs and provide the mapping of TerpreT syntax to primitives in these models. Then, in Section 4.2 - 4.5, we show how to compile TerpreT for each rear end of the care. 4.1 TerpreT for Gated Factor Graph Description A factor graph is a means that represents factorization of a function of a complex distribution or a composition of probability."}, {"heading": "4.2 Forward Marginals Gradient Descent (FMGD) Back-end", "text": "The factor graphs mentioned above are easily converted into calculation graphs that represent the execution of an interpreter by the following operations. \u2022 The function transforms the incoming variables X into the outgoing variable, Y = fi (X).1Strictly speaking, this notation does not deal with the case where there are multiple gates with identical path conditions; for clarity of notation, we assume that all path conditions are unique. The implementation handles repeated path conditions (by identifying local margins according to a unique gate ID).In the FMGD procedure, we initialize the source nodes of this directed graph spectrum, assuming that all path conditions are unique."}, {"heading": "4.2.1 Forward Marginals...", "text": "In this limited configuration index, we will then enumerate the possible outputs Y from all possible output conditions by the possible output configurations xk of the form [xk] i for i,..., M}. We will then marginalize ourselves via the configuration index, k) by producing the weights \u00b5i (xk) as follows: \u00b5Y (y) = f (xk)} w (xk) w (xk), where 1) is an indicator function and the weighting function w: w (xk)."}, {"heading": "4.2.2 ... Gradient Descent", "text": "Given a random initialization of the marginals for the Xp-P param variable, we use the above techniques to propagate marginals forward through the TerpreT model to achieve all the variables, Xo-O, associated with an observation value (x-o) statement. Then, we use a cross-entropy loss, L to compare the calculated marginal value with the observed value. L = \u2212 \u2211 Xo-O protocol [\u00b5o (x-o)]. (6) L reaches its lower limit L = 0 when each of the marginal losses \u00b5p (x) representing the params sets the unit weight to a single value \u00b5p (x-p) = 1, so that the assignments {Xp = x-p} describe a valid program that explains the observations. Therefore, the synthesis task is an optimization problem to minimize L, which we try to achieve by means of regression and gradient solution to replace a zero loss."}, {"heading": "4.2.3 Optimization Heuristics", "text": "Since many different programs can be aligned with the observations, there can be many global Optima (L = 0) points in the FMGD loss landscape. Therefore, we are trying several random approaches that can also lead to local optimizations (x), and note the fault line of the initializations that lead to a global optimum. Specifically, we are trying two approaches to learning using this model: \u2022 Vanilla FMGD. Run the algorithm as presented above, with the RMSProp (Tieleman and Hinton, 2012)."}, {"heading": "4.3 (Integer) Linear Program Back-end", "text": "We now turn to the first alternative back-end that can be compared to the FMGD. If we look at the TerpreT program as a factor diagram, we can build on the current practice of constructing LP relaxations to solve maximum a posteriori (MAP) problems in discrete graphical models (Schlesinger, 1976; Wainwright and Jordan, 2008). In the following sections, we describe how these techniques can be applied to the TerpreT models, in particular how the methods can be extended to the handling of gates."}, {"heading": "4.3.1 LP Relaxation", "text": "The derivative problem can be formulated as a task, the highest scoring configuration of a set of discrete variables X0,.., XD \u2212 1. The score is defined as the sum of local factor values, \u03b8j, where \u03b8j: X j \u2192 R, and X j = \u00d7 i \u0418SjXi are the common configuration space of variables x with indexes Sj = (i0,.., iMj), which spans the range of factor j. In the simplest case (if we are looking for a validation solution), the factor score on a node representing a function fj is simply to measure the consistency of input (x\\ 0) and output (x0) at this factor."}, {"heading": "4.3.2 Linear Constraints in Gated Models", "text": "We are expanding the LP relaxation to a sum of 1, as a factor between the local boundaries. In each gate, we will update the local marginalities for each active variable and local marginal size as follows: The main difference in the Gate LP from the standard LP is how the normalization constraints are handled. The key idea is that each local marginal size is normalized in Gate A. To look at the sum of these marginal sizes as follows: The local marginal size for Xi in the Gate LP is how the respective marginal sizes are normalized in Gate A. (11) For the local marginal sizes in the gate condition, the marginal sizes in the gate condition (1), Y = y."}, {"heading": "4.4 SMT Back-end", "text": "In essence, an IPS problem in TerpreT leads to a simple linear integer constraint system. To take advantage of mature constraint solving systems such as Z3 (de Moura and Bj\u00f8rner, 2008), we have implemented a satisfaction modulo theories (SMT) backend. To this end, we have defined a syntax-guided transformation function J \u00b7 KESMT that translates TerpreT expressions into SMT LIB expressions (Barrett et al., 2015), according to which any standard SMT solver can be called. To this end, we have defined a syntax-guided transformation function J \u00b7 KESMT that translates TerpreT expressions into SMT LIB expressions using integral variables, shown in Fig. 11. We use the previously discussed unrolling techniques to eliminate arrays, for loops, and with statements. When we encounter a function call that we only use as a part of an expression, we replace the inline with the small arguments expressed in the T."}, {"heading": "4.5 Sketch Back-end", "text": "rE \"s tis rf\u00fc eid rf\u00fc ide rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green.\" rE \"s tis rf\u00fc eid rf\u00fc ide rf\u00fc ide rf\u00fc the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green"}, {"heading": "5 Analysis", "text": "One motivation for this work was to compare the performance of the gradient-based FMGD technique for IPS with other backends. Below, we present a task that all other backends can easily solve, but FMGD fails due to the prevalence of local optima."}, {"heading": "5.1 Failure of FMGD", "text": "Kurach et al. (2015) and Neelakantan et al. (2016b) mention that many random restarts and a careful hyperparameter search are necessary to arrive at a correct deterministic solution. Here, we develop an understanding of the loss surface that arises using FMGD in a simpler setting, which we believe sheds some light on the local optimal structure that arises when we use FMGD more generally. Let's say x0,.,. xK \u2212 1 are binary variables with x0 = 0 and all others not observed. For each k = 0,.,., K \u2212 1, let yk = (xk + x) mod 2 the parity of adjacent x variables connected in a ring form. Suppose that all yk are equal and the goal is to derive the values of each xk. The TerpreT program is as follows, to which we refer as parity in the model (2): const = x x x = 1 K."}, {"heading": "5.2 Parity Chain Experiments", "text": "Here we offer an empirical counterpart to the theoretical analysis in the previous section. In particular, we showed that there are exponentially many local Optimas for FMGD that fall into the parity chain model, but this does not necessarily mean that these local Optimas are encountered in practice. It is conceivable that there is a large pool of attractiveness around the global optimum, and smaller, negligible basins of attractiveness around the suboptimal local optimist. To answer this question, we perform Vanilla FMGD (no Optimization Heuristics) with random initialization parameters that are selected so that initial parameters are drawn uniformly from the single basin."}, {"heading": "6 Experiments", "text": "We will now turn to experimental results. Our primary goal is to better understand the capabilities of the different backends on a number of issues and to identify some trends in the performance of the backends as the problem characteristics are different."}, {"heading": "6.1 Benchmarks Results", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "6.2 Zooming in on FMGD Boolean Circuits", "text": "There is a stark contrast between the performance of the FMGD and the alternatives on the Boolean circuit problems. In the Controlled Shift and Full Adder benchmarks, each run of the FMGD took 35 \u2212 80 \u00d7 as long as the SMT backend. In addition, during the random search, we performed 120 \u00d7 20 = 2400 runs. However, there were no successful runs."}, {"heading": "6.2.1 Slow convergence", "text": "While most races approached a local optimum during their assigned 2000 epochs, in some cases this was not the case. Therefore, we decided to allocate the algorithm 5 times as many epochs (10,000) and to repeat the random search, which resulted in some, if very few, successes. On the issue of controlled displacement, 1 out of 2400 runs converged, and on Full Adder, 3 out of 2400 runs converged, so it looks as if the results could be slightly improved if FMGD were allowed to run longer. However, given the long running times of the FMGD in relation to the SMT and sketch backends, this would not change the qualitative conclusions from the previous section."}, {"heading": "6.2.2 Varying the problem dimension", "text": "We take inspiration from the new-fangled network literature, which approaches the problem of stagnation in local minimums by increasing the dimensions of the problem. It has been argued that local minima are becoming increasingly rare because dynamic learning processes such as RMSProp are very effective in handling saddle points and panels. To assess the impact of the dimensions of FMSD, we accept a minimum of examples."}, {"heading": "6.3 Challenge Benchmark", "text": "Before leaving this section, we note that Sketch has solved all benchmark tasks so far. To provide a target for future work, we present a final benchmark that none of the backends is currently able to do (a) (b) AND NOToutabAND NOToutNOTab0.5 0.5outsolve from 5 input output examples even after 40 hours: ASSEMBLY M R B log10 D T DescriptionMerge 17 6 22 103 69 Merging two contiguous sorted lists into a contiguous sorted list. The first entries in the initial heap are Heap0 [0] = p1, Heap0 [1] = p2, Heap0 [2] = pout, where p1 (p2) is a pointer to the first (second) sorted sublist (ending with 0) and pout is a pointer to the header of the desired output list. All elements of the sorted sublists are larger than 0 and all unused cells in the header are initialized to 0."}, {"heading": "7 Related Work", "text": "There are many probable programming systems that specialize in different use cases. A dominant axis of variability is the expressivity of language. Some probable programming languages, which were essentially dictated by the church (Goodman et al., 2008), allow great freedom in the way language is expressed, including constructions such as recursion and higher order. The cost of the way language is expressed is that the inferencing techniques cannot be so specialized, and therefore tend to use these methods for inferencing. At the other end of the spectrum are specialized systems such as inferencing and higher order."}, {"heading": "8 Discussion & Future Work", "text": "In fact, it is that we are able to assert ourselves, that we are able to be able to be ourselves, to be able to be ourselves, to be able to be ourselves, to be able to be ourselves."}, {"heading": "Acknowledgements", "text": "We thank several people for discussions that have helped improve this report: Tom Minka for discussions related to the Gates LP relaxation; John Winn for several discussions related to probabilistic programming and Gates; Ryota Tomioka for discussions related to the FMGD lossspace; Andy Gordon for pushing us towards probabilistic programming of TerpreT; Abdel-rahman Mohamed for discussions related to neural networks and program synthesis; Jack Feser for being the first non-author of TerpreT; Aditya Nori for helpful discussions on program synthesis and Matej Balog for a critical reading of this manuscript."}, {"heading": "A Proof of Lemma 1", "text": "The plan stipulates that for each of the island structures described above, the partial derivative approach (partial) (partial) (partial) (partial) (partial) (partial) (logical) (partial) (1,., K), a) {0, 1} are the free parameters, where si (a) is the unnormalised protocol probability that xi is equal to a. The probability over xi is then given by a softmax; i.e., p (xi = a) = \u2202 (s) = exp si (a) exp si (0) exp si (1) exp si (1). Let the probability over xi (a) (i) (softmax; i.e.,.,., K), a), a), a), 0, 0, 0, 0, 1}. Let the target o (s) be the protocol probability of the observations, i.e. o (s) (s) = logical (s) = a), o (s) = logical (1) (a)."}, {"heading": "B Benchmark models", "text": "# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}], "references": [{"title": "Control flow analysis", "author": ["Frances E Allen"], "venue": "In ACM Sigplan Notices,", "citeRegEx": "Allen.,? \\Q1970\\E", "shortCiteRegEx": "Allen.", "year": 1970}, {"title": "Syntax-guided synthesis", "author": ["Rajeev Alur", "Rastislav Bod\u0301\u0131k", "Eric Dallal", "Dana Fisman", "Pranav Garg", "Garvit Juniwal", "Hadas KressGazit", "P. Madhusudan", "Milo M.K. Martin", "Mukund Raghothaman", "Shamwaditya Saha", "Sanjit A. Seshia", "Rishabh Singh", "Armando Solar-Lezama", "Emina Torlak", "Abhishek Udupa"], "venue": "In Dependable Software Systems Engineering,", "citeRegEx": "Alur et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Alur et al\\.", "year": 2015}, {"title": "The SMT-LIB standard: Version 2.5", "author": ["Clark Barrett", "Pascal Fontaine", "Cesare Tinelli"], "venue": "Technical report, The University of Iowa,", "citeRegEx": "Barrett et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2015}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "IEEE transactions on neural networks,", "citeRegEx": "Bengio et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "The inference of regular lisp programs from examples", "author": ["Alan W Biermann"], "venue": "IEEE transactions on Systems, Man, and Cybernetics,", "citeRegEx": "Biermann.,? \\Q1978\\E", "shortCiteRegEx": "Biermann.", "year": 1978}, {"title": "Adaptive neural compilation", "author": ["Rudy Bunel", "Alban Desmaison", "Pushmeet Kohli", "Philip H.S. Torr", "M. Pawan Kumar"], "venue": "CoRR, abs/1605.07969,", "citeRegEx": "Bunel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bunel et al\\.", "year": 2016}, {"title": "Stan: A probabilistic programming language", "author": ["Bob Carpenter"], "venue": "Journal of Statistical Software,", "citeRegEx": "Carpenter.,? \\Q2015\\E", "shortCiteRegEx": "Carpenter.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["KyungHyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "CoRR, abs/1409.1259,", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Bounded model checking using satisfiability solving", "author": ["Edmund Clarke", "Armin Biere", "Richard Raimi", "Yunshan Zhu"], "venue": "Formal Methods in System Design,", "citeRegEx": "Clarke et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Clarke et al\\.", "year": 2001}, {"title": "Using prior knowledge in a {NNPDA} to learn contextfree languages", "author": ["Sreerupa Das", "C. Lee Giles", "Guo-Zheng Sun"], "venue": "In Proceedings of the 5th Conference on Advances in Neural Information Processing Systems,", "citeRegEx": "Das et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Das et al\\.", "year": 1992}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann N Dauphin", "Razvan Pascanu", "Caglar Gulcehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Z3: an efficient SMT solver", "author": ["Leonardo Mendon\u00e7a de Moura", "Nikolaj Bj\u00f8rner"], "venue": "Internal Conference on Tools and Algorithms for the Construction and Analysis of Systems,", "citeRegEx": "Moura and Bj\u00f8rner.,? \\Q2008\\E", "shortCiteRegEx": "Moura and Bj\u00f8rner.", "year": 2008}, {"title": "Unsupervised learning by program synthesis", "author": ["Kevin Ellis", "Armando Solar-Lezama", "Joshua B. Tenenbaum"], "venue": "In Advances in Neural Information Processing Systems NIPS,", "citeRegEx": "Ellis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ellis et al\\.", "year": 2015}, {"title": "Higher order recurrent networks and grammatical inference", "author": ["C. Lee Giles", "Guo-Zheng Sun", "Hsing-Hen Chen", "Yee-Chun Lee", "Dong Chen"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "Giles et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Giles et al\\.", "year": 1989}, {"title": "Church: a language for generative models", "author": ["Noah D. Goodman", "Vikash K. Mansinghka", "Daniel M. Roy", "Keith Bonawitz", "Joshua B. Tenenbaum"], "venue": "In Proc. of Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Goodman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goodman et al\\.", "year": 2008}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves"], "venue": "CoRR, abs/1308.0850,", "citeRegEx": "Graves.,? \\Q2013\\E", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Learning to transduce with unbounded memory", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Grefenstette et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2015}, {"title": "Automating string processing in spreadsheets using input-output examples", "author": ["Sumit Gulwani"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Gulwani.,? \\Q2011\\E", "shortCiteRegEx": "Gulwani.", "year": 2011}, {"title": "Program verification as probabilistic inference", "author": ["Sumit Gulwani", "Nebojsa Jojic"], "venue": "In ACM SIGPLAN Notices,", "citeRegEx": "Gulwani and Jojic.,? \\Q2007\\E", "shortCiteRegEx": "Gulwani and Jojic.", "year": 2007}, {"title": "Spreadsheet data manipulation using examples", "author": ["Sumit Gulwani", "William Harris", "Rishabh Singh"], "venue": "Communications of the ACM,", "citeRegEx": "Gulwani et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Gulwani et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Inferring algorithmic patterns with stack-augmented recurrent nets", "author": ["Armand Joulin", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems 2, [NIPS Conference,", "citeRegEx": "Joulin and Mikolov.,? \\Q1989\\E", "shortCiteRegEx": "Joulin and Mikolov.", "year": 1989}, {"title": "Neural gpus learn algorithms", "author": ["Lukasz Kaiser", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations.,", "citeRegEx": "Kaiser and Sutskever.,? \\Q2016\\E", "shortCiteRegEx": "Kaiser and Sutskever.", "year": 2016}, {"title": "A clockwork RNN", "author": ["Jan Kout\u0144\u0131k", "Klaus Greff", "Faustino J. Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Kout\u0144\u0131k et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kout\u0144\u0131k et al\\.", "year": 2014}, {"title": "Complete functional synthesis", "author": ["Viktor Kuncak", "Mika\u00ebl Mayer", "Ruzica Piskac", "Philippe Suter"], "venue": "In PLDI,", "citeRegEx": "Kuncak et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kuncak et al\\.", "year": 2010}, {"title": "Neural random-access machines", "author": ["Karol Kurach", "Marcin Andrychowicz", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations 2016,", "citeRegEx": "Kurach et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kurach et al\\.", "year": 2015}, {"title": "Human-level concept learning through probabilistic program induction", "author": ["Brenden M Lake", "Ruslan Salakhutdinov", "Joshua B Tenenbaum"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2015}, {"title": "Llvm: A compilation framework for lifelong program analysis & transformation", "author": ["Chris Lattner", "Vikram Adve"], "venue": "In Code Generation and Optimization,", "citeRegEx": "Lattner and Adve.,? \\Q2004\\E", "shortCiteRegEx": "Lattner and Adve.", "year": 2004}, {"title": "Learning longer memory in recurrent neural networks", "author": ["Tomas Mikolov", "Armand Joulin", "Sumit Chopra", "Micha\u00ebl Mathieu", "Marc\u2019Aurelio Ranzato"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations", "citeRegEx": "Mikolov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2015}, {"title": "A connectionist symbol manipulator that discovers the structure of context-free languages", "author": ["Michael Mozer", "Sreerupa Das"], "venue": "In Advances in Neural Information Processing Systems 5, [NIPS Conference,", "citeRegEx": "Mozer and Das.,? \\Q1992\\E", "shortCiteRegEx": "Mozer and Das.", "year": 1992}, {"title": "Neural programmer: Inducing latent programs with gradient descent", "author": ["Arvind Neelakantan", "Quoc V. Le", "Ilya Sutskever"], "venue": "In Proceedings of the 4th International Conference on Learning Representations 2016,", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Adding gradient noise improves learning for very deep networks", "author": ["Arvind Neelakantan", "Luke Vilnis", "Quoc V Le", "Ilya Sutskever", "Lukasz Kaiser", "Karol Kurach", "James Martens"], "venue": "In Proceedings of the International Conference on Learning Representations 2015,", "citeRegEx": "Neelakantan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2016}, {"title": "Automatic sampler discovery via probabilistic programming and approximate bayesian computation", "author": ["Yura Perov", "Frank Wood"], "venue": "In International Conference on Artificial General Intelligence,", "citeRegEx": "Perov and Wood.,? \\Q2016\\E", "shortCiteRegEx": "Perov and Wood.", "year": 2016}, {"title": "Bod\u0301\u0131k. Chlorophyll: synthesis-aided compiler for low-power spatial architectures", "author": ["Phitchaya Mangpo Phothilimthana", "Tikhon Jelvis", "Rohin Shah", "Nishant Totla", "Sarah Chasins", "Rastislav"], "venue": "In PLDI,", "citeRegEx": "Phothilimthana et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Phothilimthana et al\\.", "year": 2014}, {"title": "Flashmeta: a framework for inductive program synthesis", "author": ["Oleksandr Polozov", "Sumit Gulwani"], "venue": "In OOPSLA,", "citeRegEx": "Polozov and Gulwani.,? \\Q2015\\E", "shortCiteRegEx": "Polozov and Gulwani.", "year": 2015}, {"title": "Learning programs from noisy data", "author": ["Veselin Raychev", "Pavol Bielik", "Martin T. Vechev", "Andreas Krause"], "venue": "In POPL,", "citeRegEx": "Raychev et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raychev et al\\.", "year": 2016}, {"title": "Counterexampleguided quantifier instantiation for synthesis in SMT", "author": ["Andrew Reynolds", "Morgan Deters", "Viktor Kuncak", "Cesare Tinelli", "Clark W. Barrett"], "venue": "In CAV,", "citeRegEx": "Reynolds et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Reynolds et al\\.", "year": 2015}, {"title": "Programming with a differentiable forth interpreter", "author": ["Sebastian Riedel", "Matko Bosnjak", "Tim Rockt\u00e4schel"], "venue": "CoRR, abs/1605.06640,", "citeRegEx": "Riedel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2016}, {"title": "Stochastic superoptimization", "author": ["Eric Schkufza", "Rahul Sharma", "Alex Aiken"], "venue": "In ASPLOS, pages 305\u2013316,", "citeRegEx": "Schkufza et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Schkufza et al\\.", "year": 2013}, {"title": "Syntactic analysis of two-dimensional visual signals in the presence of noise", "author": ["MI Schlesinger"], "venue": "Cybernetics and systems analysis,", "citeRegEx": "Schlesinger.,? \\Q1976\\E", "shortCiteRegEx": "Schlesinger.", "year": 1976}, {"title": "Blinkfill: Semi-supervised programming by example for syntactic string", "author": ["Rishabh Singh"], "venue": "transformations. PVLDB,", "citeRegEx": "Singh.,? \\Q2016\\E", "shortCiteRegEx": "Singh.", "year": 2016}, {"title": "Automated feedback generation for introductory programming assignments", "author": ["Rishabh Singh", "Sumit Gulwani", "Armando Solar-Lezama"], "venue": "In PLDI,", "citeRegEx": "Singh et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Singh et al\\.", "year": 2013}, {"title": "Program Synthesis By Sketching", "author": ["Armando Solar-Lezama"], "venue": "PhD thesis, EECS Dept., UC Berkeley,", "citeRegEx": "Solar.Lezama.,? \\Q2008\\E", "shortCiteRegEx": "Solar.Lezama.", "year": 2008}, {"title": "Programming by sketching for bit-streaming programs", "author": ["Armando Solar-Lezama", "Rodric Rabbah", "Rastislav Bodik", "Kemal Ebcioglu"], "venue": "In PLDI,", "citeRegEx": "Solar.Lezama et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Solar.Lezama et al\\.", "year": 2005}, {"title": "Combinatorial sketching for finite programs", "author": ["Armando Solar-Lezama", "Liviu Tancau", "Rastislav Bod\u0301\u0131k", "Sanjit A. Seshia", "Vijay A. Saraswat"], "venue": "In ASPLOS,", "citeRegEx": "Solar.Lezama et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Solar.Lezama et al\\.", "year": 2006}, {"title": "Tightening lp relaxations for map using message passing", "author": ["David Sontag", "Talya Meltzer", "Amir Globerson", "Tommi S Jaakkola", "Yair Weiss"], "venue": "In Uncertainty in Artificial Intelligence (UAI),", "citeRegEx": "Sontag et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sontag et al\\.", "year": 2008}, {"title": "End-to-end memory networks. In Advances in Neural Information Processing Systems", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "Annual Conference on Neural Information Processing Systems", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "A methodology for lisp program construction from examples", "author": ["Phillip D Summers"], "venue": "Journal of the ACM (JACM),", "citeRegEx": "Summers.,? \\Q1977\\E", "shortCiteRegEx": "Summers.", "year": 1977}, {"title": "Lecture 6.5\u2014RmsProp: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "Tieleman and Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Tieleman and Hinton.", "year": 2012}, {"title": "TRANSIT: specifying protocols with concolic snippets", "author": ["Abhishek Udupa", "Arun Raghavan", "Jyotirmoy V. Deshmukh", "Sela Mador-Haim", "Milo M.K. Martin", "Rajeev Alur"], "venue": "In PLDI,", "citeRegEx": "Udupa et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Udupa et al\\.", "year": 2013}, {"title": "Graphical models, exponential families, and variational inference", "author": ["Martin J Wainwright", "Michael I Jordan"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Wainwright and Jordan.,? \\Q2008\\E", "shortCiteRegEx": "Wainwright and Jordan.", "year": 2008}, {"title": "A linear programming approach to max-sum problem: A review", "author": ["Tomas Werner"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Werner.,? \\Q2007\\E", "shortCiteRegEx": "Werner.", "year": 2007}, {"title": "Learning simple algorithms from examples", "author": ["Wojciech Zaremba", "Tomas Mikolov", "Armand Joulin", "Rob Fergus"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "citeRegEx": "Zaremba et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 17, "context": "The field has produced many successes, with perhaps the most visible example being the FlashFill system in Microsoft Excel (Gulwani, 2011; Gulwani et al., 2012).", "startOffset": 123, "endOffset": 160}, {"referenceID": 19, "context": "The field has produced many successes, with perhaps the most visible example being the FlashFill system in Microsoft Excel (Gulwani, 2011; Gulwani et al., 2012).", "startOffset": 123, "endOffset": 160}, {"referenceID": 44, "context": "1 Introduction Learning computer programs from input-output examples, or Inductive Program Synthesis (IPS), is a fundamental problem in computer science, dating back at least to Summers (1977) and Biermann (1978).", "startOffset": 178, "endOffset": 193}, {"referenceID": 4, "context": "1 Introduction Learning computer programs from input-output examples, or Inductive Program Synthesis (IPS), is a fundamental problem in computer science, dating back at least to Summers (1977) and Biermann (1978). The field has produced many successes, with perhaps the most visible example being the FlashFill system in Microsoft Excel (Gulwani, 2011; Gulwani et al.", "startOffset": 197, "endOffset": 213}, {"referenceID": 13, "context": "These include Recurrent Neural Networks (RNNs) augmented with a stack or queue memory (Giles et al., 1989; Joulin and Mikolov, 2015; Grefenstette et al., 2015), Neural Turing Machines (Graves et al.", "startOffset": 86, "endOffset": 159}, {"referenceID": 16, "context": "These include Recurrent Neural Networks (RNNs) augmented with a stack or queue memory (Giles et al., 1989; Joulin and Mikolov, 2015; Grefenstette et al., 2015), Neural Turing Machines (Graves et al.", "startOffset": 86, "endOffset": 159}, {"referenceID": 22, "context": ", 2014), Neural GPUs (Kaiser and Sutskever, 2016), Neural Programmer-Interpreters (Reed and de Freitas, 2016), and Neural Random Access Machines (Kurach et al.", "startOffset": 21, "endOffset": 49}, {"referenceID": 25, "context": ", 2014), Neural GPUs (Kaiser and Sutskever, 2016), Neural Programmer-Interpreters (Reed and de Freitas, 2016), and Neural Random Access Machines (Kurach et al., 2015).", "startOffset": 145, "endOffset": 166}, {"referenceID": 27, "context": "To address the first question we develop models inspired by intermediate representations used in compilers like LLVM (Lattner and Adve, 2004) that can be trained by gradient descent.", "startOffset": 117, "endOffset": 141}, {"referenceID": 5, "context": "We note two concurrent works, Adaptive Neural Compilation (Bunel et al., 2016) and Differentiable Forth (Riedel et al.", "startOffset": 58, "endOffset": 78}, {"referenceID": 37, "context": ", 2016) and Differentiable Forth (Riedel et al., 2016), which implement similar ideas.", "startOffset": 33, "endOffset": 54}, {"referenceID": 25, "context": "Technique name Family Optimizer/Solver Description FMGD (Forward marginals, gradient descent) Machine learning TensorFlow A gradient descent based approach which generalizes the approach used by Kurach et al. (2015). (I)LP ((Integer) linear programming) Machine learning Gurobi A novel linear program relaxation approach based on adapting standard linear program relaxations to support Gates (Minka and Winn, 2009).", "startOffset": 195, "endOffset": 216}, {"referenceID": 42, "context": "TerpreT currently has four back-end inference algorithms, which are listed in Table 1: gradientdescent (thus any TerpreT model can be viewed as a differentiable interpreter), (integer) linear program (LP) relaxations, SMT, and the Sketch program synthesis system (Solar-Lezama, 2008).", "startOffset": 263, "endOffset": 283}, {"referenceID": 39, "context": "3, we show how to adapt the ideas of gates (Minka and Winn, 2009) to the linear program relaxations commonly used in graphical model inference (Schlesinger, 1976; Werner, 2007; Wainwright and Jordan, 2008).", "startOffset": 143, "endOffset": 205}, {"referenceID": 51, "context": "3, we show how to adapt the ideas of gates (Minka and Winn, 2009) to the linear program relaxations commonly used in graphical model inference (Schlesinger, 1976; Werner, 2007; Wainwright and Jordan, 2008).", "startOffset": 143, "endOffset": 205}, {"referenceID": 50, "context": "3, we show how to adapt the ideas of gates (Minka and Winn, 2009) to the linear program relaxations commonly used in graphical model inference (Schlesinger, 1976; Werner, 2007; Wainwright and Jordan, 2008).", "startOffset": 143, "endOffset": 205}, {"referenceID": 25, "context": "For the gradient descent case, we generalize the approach taken by Kurach et al. (2015), lifting discrete operations to operate on discrete distributions, which then leads to a differentiable system.", "startOffset": 67, "endOffset": 88}, {"referenceID": 25, "context": "For the gradient descent case, we generalize the approach taken by Kurach et al. (2015), lifting discrete operations to operate on discrete distributions, which then leads to a differentiable system. For the linear program case, we need to extend the standard LP relaxation for discrete graphical models to support if statements. In Section 4.3, we show how to adapt the ideas of gates (Minka and Winn, 2009) to the linear program relaxations commonly used in graphical model inference (Schlesinger, 1976; Werner, 2007; Wainwright and Jordan, 2008). This could serve as a starting point for further work on LP-based message passing approaches to IPS (e.g., following Sontag et al. (2008)).", "startOffset": 67, "endOffset": 688}, {"referenceID": 25, "context": "\u2022 A novel linear program relaxation to handle the if statement structure that is common in execution models, and a generalization of the smoothing technique from Kurach et al. (2015) to work on any execution model expressible in TerpreT.", "startOffset": 162, "endOffset": 183}, {"referenceID": 0, "context": "Control flow graphs (CFGs) (Allen, 1970) are a representation of programs commonly used for static analysis and compiler optimizations.", "startOffset": 27, "endOffset": 40}, {"referenceID": 25, "context": "While this model focuses on interpretability, it also builds on an observation from the results of Kurach et al. (2015). In NRAMs, a RNN-based controller chooses a short sequence of instructions to execute next based on observations of the current program state.", "startOffset": 99, "endOffset": 120}, {"referenceID": 48, "context": "Run the algorithm as presented above, with \u03b1i = 1 and using the RMSProp (Tieleman and Hinton, 2012) gradient descent optimization algorithm.", "startOffset": 72, "endOffset": 99}, {"referenceID": 25, "context": "Add the heuristics below, which are inspired by Kurach et al. (2015) and designed to avoid getting stuck in local minima, and optimize the hyperparameters for these heuristics by random search.", "startOffset": 48, "endOffset": 69}, {"referenceID": 3, "context": "We mitigate the \u201cexploding gradient\u201d problem (Bengio et al., 1994) by globally rescaling the whole gradient vector so that its L2 norm is not bigger than some hyperparameter value C.", "startOffset": 45, "endOffset": 66}, {"referenceID": 30, "context": "Following Neelakantan et al. (2016b), we decay the variance of this noise during the training according to the following schedule: \u03c32 t = \u03b7 (1 + t)\u03b3 (8) where the values of \u03b7 and \u03b3 are hyperparameters and t is the epoch counter.", "startOffset": 10, "endOffset": 37}, {"referenceID": 25, "context": "Kurach et al. (2015) considered two additional tricks which we did not implement generally.", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "Kurach et al. (2015) solve this by adding rescaling operations to ensure normalization.", "startOffset": 0, "endOffset": 21}, {"referenceID": 25, "context": "Kurach et al. (2015) used a curriculum learning scheme which involved first training on small instances of a given problem, and only moving to train on larger instances once the error rate had reduced below a certain value.", "startOffset": 0, "endOffset": 21}, {"referenceID": 39, "context": "Casting the TerpreT program as a factor graph allows us to build upon standard practice in constructing LP relaxations for solving maximum a posteriori (MAP) inference problems in discrete graphical models (Schlesinger, 1976; Wainwright and Jordan, 2008).", "startOffset": 206, "endOffset": 254}, {"referenceID": 50, "context": "Casting the TerpreT program as a factor graph allows us to build upon standard practice in constructing LP relaxations for solving maximum a posteriori (MAP) inference problems in discrete graphical models (Schlesinger, 1976; Wainwright and Jordan, 2008).", "startOffset": 206, "endOffset": 254}, {"referenceID": 2, "context": "For this, a TerpreT instance is translated into a set of constraints in the SMT-LIB standard (Barrett et al., 2015), after which any standard SMT solver can be called.", "startOffset": 93, "endOffset": 115}, {"referenceID": 8, "context": "(Clarke et al., 2001)) which for a given program, search for an input that shows some behavior.", "startOffset": 0, "endOffset": 21}, {"referenceID": 42, "context": "5 Sketch Back-end The final back-end which we consider is based on the Sketch (Solar-Lezama, 2008) program synthesis system, which allows programmers to write partial programs called sketches while leaving fragments unspecified as holes.", "startOffset": 78, "endOffset": 98}, {"referenceID": 42, "context": "More details about the CEGIS algorithm in Sketch can be found in Solar-Lezama (2008).", "startOffset": 65, "endOffset": 85}, {"referenceID": 25, "context": "1 Failure of FMGD Kurach et al. (2015) and Neelakantan et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 25, "context": "1 Failure of FMGD Kurach et al. (2015) and Neelakantan et al. (2016b) mention that many random restarts and a careful hyperparameter search are needed in order to converge to a correct deterministic solution.", "startOffset": 18, "endOffset": 70}, {"referenceID": 5, "context": "minimal length (Bunel et al., 2016) or resource usage).", "startOffset": 15, "endOffset": 35}, {"referenceID": 25, "context": "Secondly, FMGD makes the synthesis task fully differentiable, allowing its incorporation into a larger end-to-end differentiable system (Kurach et al., 2015).", "startOffset": 136, "endOffset": 157}, {"referenceID": 10, "context": "It has been argued that local optima become increasingly rare in neural network loss surfaces as the dimension of the hidden layers increase, and instead saddle points become increasingly common (Dauphin et al., 2014).", "startOffset": 195, "endOffset": 217}, {"referenceID": 10, "context": "It has been argued that local optima become increasingly rare in neural network loss surfaces as the dimension of the hidden layers increase, and instead saddle points become increasingly common (Dauphin et al., 2014). Exchanging local minima for saddle points is beneficial because dynamic learning rate schedules such as RMSProp are very effective at handling saddle points and plateaus in the loss function. To assess how dimensionality affects FMGD, we first take a minimal example in the boolean circuit domain: the task of synthesizing a NAND gate. The minimum solution for this task is shown in Fig. 18(a), along with an example configuration which resides at one local minimum of the FMGD loss surface. For a synthesis task involving two gates and two wires, there are a total of 14 independent degrees of freedom to be optimized, and there is only one global optimum. Increasing the available resources to three gates and three wires, gives an optimization problem over 30 dimensions and several global minima. The contrast between the learning trajectories in these two cases is shown in Fig. 18. We attempt to infer the presence of saddle points by exploring the loss surface with vanilla gradient descent and a small learning rate. Temporary stagnation of the learning is an indication of a saddle-like feature. Such features are clearly more frequently encountered in the higher dimensional case where we also observe a greater overall success rate (36% of 100 random initializations converge on a global optimum in the low dimensional case vs. 60% in the high dimensional case). These observations are consistent with the intuition from Dauphin et al. (2014), suggesting that we will have more success in the benchmark tasks if we provide more resources (i.", "startOffset": 196, "endOffset": 1673}, {"referenceID": 14, "context": "Some probabilistic programming languages, exemplified by Church (Goodman et al., 2008), allow great freedom in the expressibility of the language, including constructs like recursion and higher order functions.", "startOffset": 64, "endOffset": 86}, {"referenceID": 6, "context": ", 2014) and Stan (Carpenter, 2015; Stan Development Team, 2015).", "startOffset": 17, "endOffset": 63}, {"referenceID": 6, "context": ", 2014) and Stan (Carpenter, 2015; Stan Development Team, 2015). These systems restrict models to be constructed of predefined building blocks and do not support arbitrary program constructs like recursion and higher order functions. They do generally support basic loops and branching structure, however. In Infer.NET, for example, loops are unrolled, and if statements are handled via special constructs known as Gates (Minka and Winn, 2009). The result is that the program can be viewed as a finite gated factor graph, on which message passing inference can be performed. In these terms, TerpreT is most similar to Infer.NET, and its handling of loops and if statements are inspired by Infer.NET. Compared to Infer.NET, TerpreT is far more extreme in the restrictions that it places upon modelling constructs. The benefit is that the restricted language allows us to support a broader range of back-ends. Looking forward, Infer.NET provides inspiration for how TerpreT might be extended to handle richer data types like real numbers and strings. Another related line of work is in casting program synthesis as a problem of inference in probabilistic models. Gulwani and Jojic (2007) phrase program synthesis as inference in a graphical model and use", "startOffset": 18, "endOffset": 1186}, {"referenceID": 32, "context": "The problem of inducing samplers for probability distributions has also been cast as a problem of inference in a probabilistic program (Perov and Wood, 2016).", "startOffset": 135, "endOffset": 157}, {"referenceID": 26, "context": "Lake et al. (2015) induce probabilistic programs by performing inference in a probabilistic model describing how primitives are composed to form types and instances of types.", "startOffset": 0, "endOffset": 19}, {"referenceID": 20, "context": "This is problematic when the task at hand requires to relate inputs that are far apart from each other, which more recent models try to mitigate using tools such as Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves, 2013) and Gated Recurrent Units (Cho et al.", "startOffset": 188, "endOffset": 236}, {"referenceID": 15, "context": "This is problematic when the task at hand requires to relate inputs that are far apart from each other, which more recent models try to mitigate using tools such as Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves, 2013) and Gated Recurrent Units (Cho et al.", "startOffset": 188, "endOffset": 236}, {"referenceID": 7, "context": "This is problematic when the task at hand requires to relate inputs that are far apart from each other, which more recent models try to mitigate using tools such as Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Graves, 2013) and Gated Recurrent Units (Cho et al., 2014).", "startOffset": 263, "endOffset": 281}, {"referenceID": 28, "context": ", (Mikolov et al., 2015; Kout\u0144\u0131k et al., 2014)).", "startOffset": 2, "endOffset": 46}, {"referenceID": 23, "context": ", (Mikolov et al., 2015; Kout\u0144\u0131k et al., 2014)).", "startOffset": 2, "endOffset": 46}, {"referenceID": 13, "context": "Initial extensions provided a stack (Giles et al., 1989) or a scratch pad simplified to a stack (Mozer and Das, 1992), and the controlling network learned when to push data to and pop (load) data from that stack.", "startOffset": 36, "endOffset": 56}, {"referenceID": 29, "context": ", 1989) or a scratch pad simplified to a stack (Mozer and Das, 1992), and the controlling network learned when to push data to and pop (load) data from that stack.", "startOffset": 47, "endOffset": 68}, {"referenceID": 16, "context": "3 Recently, similar ideas have been picked up again, leading to stack and queue-augmented recurrent nets (Joulin and Mikolov, 2015; Grefenstette et al., 2015), memory networks with freely addressable storage (Weston et al.", "startOffset": 105, "endOffset": 158}, {"referenceID": 46, "context": ", 2015), memory networks with freely addressable storage (Weston et al., 2014; Sukhbaatar et al., 2015), and extensions that additionally use registers for intermediate results (Kurach et al.", "startOffset": 57, "endOffset": 103}, {"referenceID": 25, "context": ", 2015), and extensions that additionally use registers for intermediate results (Kurach et al., 2015).", "startOffset": 81, "endOffset": 102}, {"referenceID": 52, "context": "A number of recent advances build on this observation to learn algorithms from input-output data (Graves et al., 2014; Joulin and Mikolov, 2015; Neelakantan et al., 2016a; Reed and de Freitas, 2016; Zaremba et al., 2016).", "startOffset": 97, "endOffset": 220}, {"referenceID": 5, "context": "To support adaptive neural compilation (Bunel et al., 2016), a machine model similar to our assembly model (cf.", "startOffset": 39, "endOffset": 59}, {"referenceID": 37, "context": "Differentiable Forth (Riedel et al., 2016) is a similar step in this direction, where the learning task is to fill in holes in a partial Forth program.", "startOffset": 21, "endOffset": 42}, {"referenceID": 1, "context": "Program Synthesis The area of program synthesis has recently seen a renewed interest in the programming language community (Alur et al., 2015).", "startOffset": 123, "endOffset": 142}, {"referenceID": 19, "context": "There have been many synthesis techniques developed for a wide range of problems including data wrangling (Gulwani et al., 2012; Polozov and Gulwani, 2015), inference of efficient synchronization in concurrent programs, synthesizing efficient low-level code from partial programs (Solar-Lezama et al.", "startOffset": 106, "endOffset": 155}, {"referenceID": 34, "context": "There have been many synthesis techniques developed for a wide range of problems including data wrangling (Gulwani et al., 2012; Polozov and Gulwani, 2015), inference of efficient synchronization in concurrent programs, synthesizing efficient low-level code from partial programs (Solar-Lezama et al.", "startOffset": 106, "endOffset": 155}, {"referenceID": 43, "context": ", 2012; Polozov and Gulwani, 2015), inference of efficient synchronization in concurrent programs, synthesizing efficient low-level code from partial programs (Solar-Lezama et al., 2005), compilers for low-power spatial architectures (Phothilimthana et al.", "startOffset": 159, "endOffset": 186}, {"referenceID": 33, "context": ", 2005), compilers for low-power spatial architectures (Phothilimthana et al., 2014), efficient compilation of declarative specifications (Kuncak et al.", "startOffset": 55, "endOffset": 84}, {"referenceID": 24, "context": ", 2014), efficient compilation of declarative specifications (Kuncak et al., 2010), statistical code completion (Raychev et al.", "startOffset": 61, "endOffset": 82}, {"referenceID": 35, "context": ", 2010), statistical code completion (Raychev et al., 2016), and automated feedback generation for programming assignments (Singh et al.", "startOffset": 37, "endOffset": 59}, {"referenceID": 41, "context": ", 2016), and automated feedback generation for programming assignments (Singh et al., 2013).", "startOffset": 71, "endOffset": 91}, {"referenceID": 9, "context": "The hypothesis space of possible programs is typically defined using a domain-specific language, which is designed to be expressive enough to encode majority of desired tasks but at the same time concise enough for efficient 3Interestingly enough, extracting an interpretable deterministic pushdown automaton from a trained stack-using recurrent network was already proposed in (Das et al., 1992).", "startOffset": 378, "endOffset": 396}, {"referenceID": 42, "context": "Finally, some of the common search algorithms include constraint-based symbolic synthesis algorithms (Solar-Lezama, 2008; Reynolds et al., 2015), smart enumerative algorithms with pruning (Udupa et al.", "startOffset": 101, "endOffset": 144}, {"referenceID": 36, "context": "Finally, some of the common search algorithms include constraint-based symbolic synthesis algorithms (Solar-Lezama, 2008; Reynolds et al., 2015), smart enumerative algorithms with pruning (Udupa et al.", "startOffset": 101, "endOffset": 144}, {"referenceID": 49, "context": ", 2015), smart enumerative algorithms with pruning (Udupa et al., 2013), version-space algebra based search algorithms (Gulwani et al.", "startOffset": 51, "endOffset": 71}, {"referenceID": 19, "context": ", 2013), version-space algebra based search algorithms (Gulwani et al., 2012; Gulwani, 2011), and stochastic search (Schkufza et al.", "startOffset": 55, "endOffset": 92}, {"referenceID": 17, "context": ", 2013), version-space algebra based search algorithms (Gulwani et al., 2012; Gulwani, 2011), and stochastic search (Schkufza et al.", "startOffset": 55, "endOffset": 92}, {"referenceID": 38, "context": ", 2012; Gulwani, 2011), and stochastic search (Schkufza et al., 2013).", "startOffset": 46, "endOffset": 69}, {"referenceID": 40, "context": "There has also been some recent work on learning from inputs in addition to the input-output examples to guide the synthesis algorithm (Singh, 2016), and synthesizing programs without any examples by performing a joint inference over the program and the inputs to recover compressed encodings of the observed data (Ellis et al.", "startOffset": 135, "endOffset": 148}, {"referenceID": 12, "context": "There has also been some recent work on learning from inputs in addition to the input-output examples to guide the synthesis algorithm (Singh, 2016), and synthesizing programs without any examples by performing a joint inference over the program and the inputs to recover compressed encodings of the observed data (Ellis et al., 2015).", "startOffset": 314, "endOffset": 334}, {"referenceID": 1, "context": "We note that another recent effort SyGuS (Alur et al., 2015) aims to unify different program synthesis approaches using a common intermediate format based on context-free grammars so that different inferences techniques can be compared, but the TerpreT language allows for encoding richer programming models than SyGuS, and also allows for compilation to gradient-descent based inference algorithms.", "startOffset": 41, "endOffset": 60}, {"referenceID": 25, "context": "Our results also raise an interesting question when taken in comparison to (Kurach et al., 2015).", "startOffset": 75, "endOffset": 96}, {"referenceID": 45, "context": "Natural next steps are back-ends based on local search or Markov Chain Monte Carlo, and on message passing inference in graphical models, perhaps taking inspiration from Sontag et al. (2008). Third, we would like to build higher level languages on top of TerpreT, to support more compact specification of common TerpreT programming patterns.", "startOffset": 170, "endOffset": 191}], "year": 2016, "abstractText": "We study machine learning formulations of inductive program synthesis; that is, given input-output examples, we would like to synthesize source code that maps inputs to corresponding outputs. Our aims in this work are to develop new machine learning approaches to the problem based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programming languages community. Our key contribution is the proposal of TerpreT, a domain-specific language for expressing program synthesis problems. TerpreT is similar to a probabilistic programming language: a model is composed of a specification of a program representation (declarations of random variables) and an interpreter that describes how programs map inputs to outputs (a model connecting unknowns to observations). The inference task is to observe a set of input-output examples and infer the underlying program. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing proper like-to-like comparisons between different approaches to inference. From a single TerpreT specification we can automatically perform inference using four different back-ends that include machine learning and program synthesis approaches. These are based on gradient descent (thus each specification can be seen as a differentiable interpreter), linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. We illustrate the value of TerpreT by developing several interpreter models and performing an extensive empirical comparison between alternative inference algorithms on a variety of program models. Our key, and perhaps surprising, empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations. We conclude with some suggestions on how the machine learning community can make progress on program synthesis.", "creator": "LaTeX with hyperref package"}}}