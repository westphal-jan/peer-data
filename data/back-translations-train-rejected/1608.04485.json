{"id": "1608.04485", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Authorship clustering using multi-headed recurrent neural networks", "abstract": "A recurrent neural network that has been trained to separately model the language of several documents by unknown authors is used to measure similarity between the documents. It is able to find clues of common authorship even when the documents are very short and about disparate topics. While it is easy to make statistically significant predictions regarding authorship, it is difficult to group documents into definite clusters with high accuracy.", "histories": [["v1", "Tue, 16 Aug 2016 05:04:46 GMT  (554kb)", "http://arxiv.org/abs/1608.04485v1", "14 pages, 5 figures; notebook for PAN@CLEF 2016"]], "COMMENTS": "14 pages, 5 figures; notebook for PAN@CLEF 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["douglas bagnall"], "accepted": false, "id": "1608.04485"}, "pdf": {"name": "1608.04485.pdf", "metadata": {"source": "CRF", "title": "Authorship clustering using multi-headed recurrent neural networks Notebook for PAN at CLEF 2016", "authors": ["Douglas Bagnall"], "emails": ["douglas@halo.gen.nz"], "sections": [{"heading": "1 Introduction", "text": "The relative success of each authoring model when presented with an anonymous text was treated as an indication of true authorship. [2] The novelty of [2] was the use of a single recursive state shared by multiple language models, reducing both overadaptation and computational costs; the system produces results in the form of relative entropies that fit well with the mapping problem because it avoids the problems of the high-dimensional attribute space by tailoring it directly to pairwise similarity scales. While this is suitable for the PAN2015 author identification task [7] and the combination of language modeling and information theory is clearly useful when it comes to finding an approach to determining clues, it cannot be used in providing clues for clues to the clues. [2] The most successful entry in the PAN-2015 author identification task [7] [2] uses a form of multiple authoring networks at the same time."}, {"heading": "1.1 The PAN 2016 author clustering task", "text": "A full description of the competition can be found in the review paper [7]. The following concise definition comes from the PAN website: 1 1 http: / / pan.webis.de / clef16 / pan16-web / author-identification.htmlFor a collection of (up to 100) documents, you identify references to authorship and groups of documents by the same author. All documents are written in the same language and belong to the same genre. However, the subject or length of the documents may vary. No indication is given of the number of different authors whose documents are included in the collection. The task comprises three alphabetical languages (English, Greek and Dutch) with six problems in each language. As described in the passage quoted, each problem consists of up to 100 documents. For each problem, two answers are required: a set of clusters indicating texts adopted by a single author; and a set of weighted links between text pairs, each of which has a higher weighting of the two, with a higher probability of each of the two."}, {"heading": "2 The multi-headed recurrent neural network language model", "text": "This description is simplified for brevity; for more details, see [2] or [6] for an overview of RNN-based language modeling.A standard character-level language model becomes, given an unfinished sequence of text x1, x2, x3,.., xi \u2212 1, primarily includes an overview of the next character x. that is, there is a probability distribution over the possible characters p (xi | xi \u2212 1, xi \u2212 2,., x1).Such a model is usually formed on a corpus of text and the probabilities it sends are based on that text (and, of course, its structure and meta parameters).Whyxi \u2212 h The hypothesis that an author's writing style is necessarily reflected in decisions at the character level (even if this relationship is very weak) follows that a language model trained solely on the work of an author probably predicts a different text better than a model trained on the work of another author."}, {"heading": "3 Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Text preprocessing", "text": "In order to simplify the arithmetic and eliminate the distorting effect of extremely rare characters, all texts have been assigned to a smaller character set using the method described in [2]. The following description is a shortened version of a section of this paper. Furthermore, in two of the five passes for each language, rare words have been replaced by special characters (Section 3.2). The text is converted to the NFKD Unicode normal form, the accented letters into the letters, followed by the combined accent. Capital letters are further separated into uppercase letters, followed by the corresponding lowercase letter. Various rare letters that appear to be largely equivalent are mapped together; for example, the En hyphen (\"-\") and the Em hyphen (\"-\") are rare and seem to be used interchangeably in practice, so that these letters are composed."}, {"heading": "3.2 Eliminating low document frequency words", "text": "It is always about the way in which it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what it is about, to what it is about the question, to what extent it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question, whether it is about the question is about the question, whether it is about the question, whether it is about the question is about the question, whether it is about the question, whether it is about the question is about the question, whether it is about the question is about the question, whether it is about the question is about the question, whether it is about the question is about the question is about the question, whether it is about the question is about the question, whether it is about the question is about the question is about the question, whether it is about the question is about the question is about the question, whether it is about the question is about the question is about the question, whether it is about the question is about the question is about the question, whether it is about the question is about the question is about the question is about the question, whether it is about the question is about the question is about the question, whether it is about the question is about the question is about the question is about the question, whether it is about the question is about the question is about the question, whether it is about the question is about the question is about the question is about the question, whether it is about the question is about the question is about the question is about the question, whether it is about the question is about the question is about the question is about the question is"}, {"heading": "3.3 MHRNN set up", "text": "Although there are six independent problems for each language in both the training and the test sets, there is overlap between the sentences (at least in the training set), with some texts having multiple problems. As a result, although the training problem sizes are 390, 471 and 330 texts for English, Dutch and Greek, there are only 201, 278 and 189 individual texts, respectively. Figures for the test sets are not known. All documents from all the problems in each language have been combined into a single model, along with 80 randomly selected \"control texts\" from 2013, 2014 and 2015 PAN Training Corpus. This means that the training set has softmax groups for the Greek model 269 (i.e. 189 problem texts + 80 controls); for the Dutch model 358 and for the English model 281. Calculating all problems at once is beneficial in several ways. The more text the model sees overall, the better it can model the target language."}, {"heading": "3.4 Interpreting the results", "text": "For each problem, the relevant subset of the matrix (i.e. problem texts evaluated by problem models) is collected. As some texts are inherently more difficult to model than others, the problem matrix is adjusted by subtracting the mean value given to each text by the control models, giving the problem matrix an average of about zero. Control models were used for this (and not the models of the problem itself or other problems) because the problems the authors share in many texts between them. Using these models for normalization could distort the matrix if a few authors dominate. On the other hand, some of the control texts are known to have specific styles, while others may well be from the same authors as the problem texts (provided that they come from previous competitions and PAN \u00b2 corpora draws from limited pools)."}, {"heading": "3.5 Optimising F(BCubed): the cowardly approach", "text": "By definition, the F (BCubed) measurement value must be above 0.5 if all documents are placed in their own individual clusters of size one (since this makes the accuracy of each document 1, while retrieval is at worst 1 / N if all documents belong to the same cluster). On the other hand, placing all documents in a single large cluster typically results in a Fscore of less than 0.5. 3 This reflects that the completely separate solution only indicates the a-priori truth that each document is in a cluster with itself, and F (BCubed) rewards the restraint of this assertion. Therefore, given other information, the optimal strategy is to predict completely separate clusters of size 1. It only makes sense to deviate from this strategy if the underlying detection is strong. In this essay, the completely separate solution is called a cowardly strategy, and the rest of this section is devoted to identifying ways it could be improved. Figure 3 shows the problem."}, {"heading": "3.6 Optimising F(BCubed): the cluster-aware approach", "text": "3 Typical not only in the sense of a randomized partition, but above all empirically for all problems with the training sets. A problem with the simple agglomerative approach is that a link between two documents can bundle a large number of other documents that might otherwise be unrelated, corresponding to a single linking in the case of thematic clustering. Figure 4 tries to illustrate the problem. As clusters grow in size, the likelihood of a single link leading to cataclysmic superclusters grows, causing the cliff in Figure 3.A. A modified agglomerative approach has been developed in which the value of each link is adjusted to the mean value of all links in the cluster it forms. This approach, which seemed to provide better results, is not described here because it was mistakenly not used in the PAN assessment."}, {"heading": "3.7 Optimising F(BCubed): the accidental small-cluster steep cliff approach", "text": "Due to a stupid programming error, the cluster-conscious strategy was replaced by an algorithm that effectively punished any linkage linking more than two documents together, i.e., the documents were all created before any of them could consider larger clusters. Although this error appears drastic, the algorithm proves to have some nice properties. Figure 5 shows the modified F (BCubed) curves for the same examples as Figure 3. The cliff is steeper and the hill where there is one is wider and flatter (though lower), making targeting more than cowardly with the simple heuristics described in the next section. Results on the training set using the same heuristics and the intended algorithm (as in Section 3.6) are collectively very similar to those obtained using this random method, although the deviation is greater."}, {"heading": "3.8 Optimising F(BCubed): the clusteriness heuristic", "text": "The goal of these cluster explorations is to find a method that beats the cowardly strategy, which seems to be achievable by applying the simple heuristics of finding anchor points in the F (BCubed) landscape, as shown in Figure 5, and selecting a point between them by a fixed ratio. The exact ratio was chosen for each language and genre based on a terrible mixture of greed and fear. In the end, a slightly risky coefficient was chosen, as the underlying recognition appears quite solid (reflected in much better than random MAP values on the training set), and if sticking to the cowardly baseline is the best strategy, it will almost certainly lead to a draw."}, {"heading": "3.9 Optimising F(BCubed): example training set results", "text": "Table 1 shows some results obtained on the training set. It shows that for most English problems there is no chance of achieving a better result than a cowardly result (with these techniques), but for the other languages this is not only possible, but is also achieved with the preset values. These preset values have, of course, been adjusted with full knowledge of the training set."}, {"heading": "3.10 Ensembles", "text": "For each language, five nets were trained, and the raw transverse entropy matrices were summarized prior to subsequent processing. All models were trained in a similar way, using some form of adagrad optimization with somewhat random variations in a few meta parameters. A summary of the meta parameters is presented in Table 2.4 to see if this variation in meta parameters (or indeed the use of ensembles at all) actually helps, but no thorough research has been done. As the number of documents is much greater than is usually found in the PAN2015 challenge, the hidden layers may be larger than in [2], resulting in hopefully more accurate and nuanced modeling at the expense of training time. Conventional language modeling is about achieving maximum accuracy, and for this purpose a validation corpus is often used to prevent the model from being adapted to the training set. A similar approach is used for this task, although the accuracy of the language model itself is not found to be ambiguous compared to a model."}, {"heading": "4 Results", "text": "The results are shown in Table 3. At the time of writing, it is not known how these results compare to other approaches. If the P-BCubed column in Table 3 is 1, it is likely (but not certain) that the software is committed to the cowardly strategy (Section 3.5). Where it is not 1, the software has deviated from this strategy, whether successful or not. There is reason for optimism if the MAP values are greater than 0.1 and the BCubed precision is not far below 1."}, {"heading": "5 Discussion", "text": "The technique described seems to work reasonably well on an inherently difficult problem, but it is unfortunately difficult to be sure how it behaves in comparison to other methods. Most entries in the PAN contest seem to have problems with their understanding of the scoring system, which may conceal some success in recognizing connections between texts."}, {"heading": "5.1 A zero-effort baseline", "text": "The PAN Committee published a baseline result on an early bird test set with an F (BCubed) score of 0.6922 and a MAP of 0.00459. As is well known, the totally disconnected, cowardly strategy provides a hard-to-beat, easy-to-reach baseline for F (BCubed). For MAP, an unbeatable but useful baseline could be a fully linked graph with random link strengths. Even if a method does not contain an order of unwanted links, it is better to assign low random weights to them than to ignore them collectively. As explained in Section 3.5, a system that adheres to the a priori truth that each document in a cluster with itself has an F (BCubed) double value is better than if it were better to ignore them altogether."}, {"heading": "5.2 Variations and run-time", "text": "As described in the paper, it took much longer than any other technique. An obvious way to speed up the process would be to reduce the ensemble to one (for a fivefold improvement); reducing the number of hidden neurons would further improve the speed, but slightly decrease the effectiveness; conversely, increasing the amount and quality of the control text would improve the overall understanding of the system and allow an increase in the number of hidden nodes, which should lead to slightly better results without making fundamental changes."}], "references": [{"title": "A comparison of extrinsic clustering evaluation metrics based on formal constraints", "author": ["E. Amig\u00f3", "J. Gonzalo", "J. Artiles", "F. Verdejo"], "venue": "Information retrieval 12(4), 461\u2013486", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Author identification using multi-headed recurrent neural networks", "author": ["D. Bagnall"], "venue": "arXiv preprint arXiv:1506.04891", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research 12, 2121\u20132159", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive science 14(2), 179\u2013211", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Introduction to information retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": "Cambridge University Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Statistical Language Models Based on Neural Networks", "author": ["T. Mikolov"], "venue": "Ph.D. thesis, Ph. D. thesis, Brno University of Technology", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Clustering by Authorship Within and Across Documents", "author": ["E. Stamatatos", "M. Tschuggnall", "B. Verhoeven", "W. Daelemans", "G. Specht", "B. Stein", "M. Potthast"], "venue": "Working Notes Papers of the CLEF 2016 Evaluation Labs. CEUR Workshop Proceedings, CLEF and CEUR-WS.org", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "The most successful entry in the PAN 2015 author identification task [7][2] used a form of recurrent neural network (RNN) to simultaneously model the language of several authors.", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "The most successful entry in the PAN 2015 author identification task [7][2] used a form of recurrent neural network (RNN) to simultaneously model the language of several authors.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "The use of recurrent neural networks for language models is not new, and was most recently revived by Mikolov [6].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "The novelty of [2] was the use of a single recurrent state that was shared by multiple language models, reducing both overfitting and computational cost.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "While this suited the PAN2015 author identification task[7], and the combination of language modelling and information theory is clearly useful in uncovering authorship, the approach may have drawbacks when used for clustering.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "This paper briefly describes the multi-headed recurrent neural network introduced in [2], then looks at a method of turning its output into clustering decisions.", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "For a full description of the competition, see the overview paper [7].", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "The clustering are evaluated using the F(BCubed)[1] measure which averages the precision and recall of each document.", "startOffset": 48, "endOffset": 51}, {"referenceID": 4, "context": "The weighted links are scored using mean average precision[5] (or MAP), which punishes for every false link that is scored higher than a true link.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "This description is simplified for brevity; for more detail see [2] or [6] for an overview of RNN based language modelling.", "startOffset": 64, "endOffset": 67}, {"referenceID": 5, "context": "This description is simplified for brevity; for more detail see [2] or [6] for an overview of RNN based language modelling.", "startOffset": 71, "endOffset": 74}, {"referenceID": 3, "context": "In most regards the network follows the basic structure described by Elman [4], which is to say it resembles a multi-layer perceptron with a single hidden layer modified so the hidden state depends in part on the previous iteration\u2019s hidden state.", "startOffset": 75, "endOffset": 78}, {"referenceID": 1, "context": "For this work, as in [2], the \u201cReSQRT\u201d function is used for fh:", "startOffset": 21, "endOffset": 24}, {"referenceID": 2, "context": "The network is trained using a variant of adagrad[3] and back-propagation through time (BPTT).", "startOffset": 49, "endOffset": 52}, {"referenceID": 1, "context": "In order to simplify the computational task and remove the distorting effect of extremely rare characters, all the texts were mapped to a smaller character set following the method described in [2].", "startOffset": 194, "endOffset": 197}, {"referenceID": 1, "context": "As the number of documents is much greater than typically found in the PAN2015 challenge, the hidden layers can be larger than seen in [2], resulting in hopefully more accurate and nuanced modelling at the cost of training time.", "startOffset": 135, "endOffset": 138}, {"referenceID": 0, "context": "This suggests a weakness in F(BCubed)[1] for evaluating very difficult clustering problems.", "startOffset": 37, "endOffset": 40}], "year": 2016, "abstractText": "A recurrent neural network that has been trained to separately model the language of several documents by unknown authors is used to measure similarity between the documents. It is able to find clues of common authorship even when the documents are very short and about disparate topics. While it is easy to make statistically significant predictions regarding authorship, it is difficult to group documents into definite clusters with high accuracy.", "creator": " XeTeX output 2016.07.02:0046"}}}