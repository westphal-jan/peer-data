{"id": "1608.04622", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Training Echo State Networks with Regularization through Dimensionality Reduction", "abstract": "In this paper we introduce a new framework to train an Echo State Network to predict real valued time-series. The method consists in projecting the output of the internal layer of the network on a space with lower dimensionality, before training the output layer to learn the target task. Notably, we enforce a regularization constraint that leads to better generalization capabilities. We evaluate the performances of our approach on several benchmark tests, using different techniques to train the readout of the network, achieving superior predictive performance when using the proposed framework. Finally, we provide an insight on the effectiveness of the implemented mechanics through a visualization of the trajectory in the phase space and relying on the methodologies of nonlinear time-series analysis. By applying our method on well known chaotic systems, we provide evidence that the lower dimensional embedding retains the dynamical properties of the underlying system better than the full-dimensional internal states of the network.", "histories": [["v1", "Tue, 16 Aug 2016 14:41:12 GMT  (1538kb,D)", "http://arxiv.org/abs/1608.04622v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["sigurd l{\\o}kse", "filippo maria bianchi", "robert jenssen"], "accepted": false, "id": "1608.04622"}, "pdf": {"name": "1608.04622.pdf", "metadata": {"source": "CRF", "title": "Training Echo State Networks with Regularization through Dimensionality Reduction", "authors": ["Sigurd L\u00f8kse", "Filippo Maria Bianchi", "Robert Jenssen"], "emails": ["sigurd.lokse@uit.no", "filippo.m.bianchi@uit.no", "robert.jenssen@uit.no"], "sections": [{"heading": null, "text": "Keywords - echo state network, nonlinear time series analysis, dimensionality reduction, time series prediction"}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move to another world, to move to another world, to find their way to another world, to find their way in another world."}, {"heading": "2 Background material", "text": "In the following, we give a brief overview of the methods used in our framework. First, we describe the classical ESN architecture and two effective approaches for its training. We successively summarize two well-known methods that are used to reduce the dimensionality of the data and to map it in a smaller subspace."}, {"heading": "2.1 Echo state Network", "text": "It is not as if this is a real problem. (1) The equations describing the ESN updating and output are each defined as follows: (2) Where there is a small i.d.d.d.d.d.d. The reservoir consists of neurons that are characterized by a transmission / activation. (1) Typically implemented as a hyperbolic tangent function. In time, the network is generated by the input signal x [k].RNi and generates the output y [k].Ni and the dimensionality of the input function. (2) The network is generated by the input signal x [k].RNi"}, {"heading": "2.2 Dimensionality reduction methods", "text": "In the following, we describe the dimensionality reduction techniques that we have implemented in our framework. First, we emphasize that several approaches to reducing the dimensionality of the data can be pursued and the underlying diversity in a subspace of the data space [33, 34, 53]. In this work, we limit our analysis to the well-known and effective, yet simple approach, namely the Principal Component Analysis (PCA) [26] and the Principal Component Analysis (kPCA) [48].PCA is a statistically motivated method that projects the data on an orthonormal basis that maintains the greatest variance in the input signal while ensuring that the individual components are uncorrelated."}, {"heading": "3 Proposed architecture", "text": "This year it has come to the point that it will only be a matter of time before we will be able to do it again, until we will be able to find a solution that will enable us to do it."}, {"heading": "3.1 Hyperparameter optimization", "text": "The set of hyperparameters used to control the architecture of the ESN, the regression during reading and the procedure for dimensionality reduction are optimized by minimizing a loss function L (\u00b7), defined as L (\u03b8i) = (1 \u2212 \u03b1) Err (Yvs) + \u03b1\u03b8 (d) i, (10) where \u03b8 (d) = dNr is the hyperparameter that defines the number of dimensions, d, of the new subspace. To reduce the complexity of the model, L (\u00b7) collectively punishes prediction errors on the validation set and the number of dimensions maintained after the dimensionality reduction. The loss function is minimized using a standard genetic algorithm with Gaussian mutation, random crossover, elitism and tournament selection [51]. While the hyperparameter optimization is performed on the validation set, the best found network is used during the training phase to consume the individual."}, {"heading": "4 Experiments", "text": "The component of the loss function (Eq.10) in relation to the error in the given task is implemented by the Normalized Root Mean Squared Error (NRMSE): NRMSE = \u221a < and [k]. The GA uses a population size of 50 individuals and evaluates 20 generations. Individuals are mutated and bred for each generation with a mutation probability of Pmut = 0.2 and a crossing probability of Pcx = 0.5. Next generation individuals are selected by a tournament strategy with a tournament size of 4 individuals. Limits for all parameters are shown in Tab. 4 and bred for each generation with a mutation probability of Pmut = 0.2 and a crossover probability of Pcx = 0.5. Weight parameters in the loss function (Eq.10) are determined by a tournament strategy with a tournament size of 4 individuals. Limits for all parameters are shown in Tab. 4."}, {"heading": "4.1 Datasets description", "text": "To test our system, we look at 3 benchmark tasks commonly used in predicting time series, namely predicting Mackey-Glass time series, multiple superimposed oscillators, and the NARMA signal. The prediction problems we are looking at have a different difficulty level given by the nature of the signal and the complexity of the prediction task. In accordance with a commonly used approach [32], we set the prediction step \u03c4f in each prediction task by calculating a statistic that measures the independence of \u03c4f - separated points in the time series. Normally, we want the smallest \u03c4f, which guarantees the measurement that the measurements will be correlated. Therefore, we look at the first zero of the autocorrelation function of the time series, which results in the smallest prediction that maximizes the linear independence between samples. Alternatively, it is possible to select the precedence Glass, by selecting minimally forms, such as the local one, or generating the first Mackey-18 correlation."}, {"heading": "4.2 Results", "text": "This reflects the convergence rate during the optimization of hyperparameters for each method, expressed as NRMSE error on the validation theorem, is presented in the figure. 4.Predicting MG is a fairly simple task and each model manages to achieve a high prediction accuracy. However, by applying a dimensionality reduction to the states of the reservoir, it is possible to reduce the error by one or more orders of magnitude. Also, the standard deviation of the prediction error decreases, especially in the models that use kPCA. Best results are achieved by using the SVP + PCA and the SVP + kPCA models, while using the SVP + kPCR without reducing the dimensions of the reservoir is shown to be less effective. This means that non-linearities are advantages of formation, but without enforcing the complexity of the model."}, {"heading": "5 Discussion", "text": "In order to understand the mechanisms and effectiveness of the proposed architecture, we analyze the results through the theory of nonlinear time series analysis, which offers powerful methods for retrieving dynamic information from temporally ordered data. [10] The goal of time series analysis is to reconstruct the complete dynamics of a complex nonlinear dynamic system, starting from a measurement of only one of the item variables. In fact, in many cases it is possible to observe only a subset of the components necessary to determine the dynamic system. The main idea that inspires this analysis is that a dynamic system is fully described by time-dependent trajectory theory in its phase space. Hence, a recurrent neural network that is able to reconstruct the dynamic attractor with a high degree of accuracy, can calculate future states adopted by the system at a given moment."}, {"heading": "5.1 ESN phase space reconstruction", "text": "In the following, we will analyze two chaotic time series generated by Lorenz and the Moore Mirror System. We will evaluate the accuracy of the phase space reconstruction performed with our ESN by comparing the topological properties of the true attractor of dynamics with what we obtain by applying a dimensionality reduction to the network reservoir. We will calculate the equivalence of the attractor geometries by measuring the dynamic invariants estimated by the correlation sum and the divergent motion of the reconstructed spaces. In the following, we will refer to the true attractor as the trajectory in phase space generated directly by the differential equations of the dynamic system. With delayed embedding of the attractor, we will refer to the trajectory generated by the delay coordinate method. Finally, the ESN attractor is the component of the multivariate vector."}, {"heading": "6 Conclusions and future directions", "text": "In this thesis, we have presented a new framework for the formation of an Echo State Network, which improves its generalization capabilities through the regulatory constraints introduced by the smoothing effect of a dimensionality reduction process. Through a series of tests based on benchmark datasets, we have shown how the proposed architecture can achieve better prediction capabilities in various contexts. Successively, we have provided a theoretically sound explanation for the functioning of the proposed architecture, based on the theory of nonlinear time series analysis. By studying the dynamic properties of the network from this novel perspective, we have shown that it is possible to reconstruct the phase space of the dynamic system through an ESN; this provides a solid but simple alternative to the time-delayed embedding of the process. We believe that this work could not only be useful to improve the prediction capabilities of an ESN, but also provide a new tool for analyzing dynamic systems matrix, 38 as a follow-up to the assessment of the criticality generated by the ESsix."}], "references": [{"title": "Benchmarking reservoir computing on time-independent classification tasks", "author": ["L.A. Alexandre", "M.J. Embrechts", "J. Linton"], "venue": "In Neural Networks,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "Synchronizing moore and spiegel", "author": ["N. Balmforth", "R. Craster"], "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Out-of-sample extensions for lle, isomap, mds, eigenmaps, and spectral clustering", "author": ["Y. Bengio", "J.-F. Paiement", "P. Vincent", "O. Delalleau", "N. Le Roux", "M. Ouimet"], "venue": "Advances in neural information processing systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Short-term electric load forecasting using echo state networks and PCA decomposition", "author": ["F.M. Bianchi", "E. De Santis", "A. Rizzi", "A. Sadeghian"], "venue": "IEEE Access,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Prediction of telephone calls load using Echo State Network with exogenous variables", "author": ["F.M. Bianchi", "S. Scardapane", "A. Uncini", "A. Rizzi", "A. Sadeghian"], "venue": "Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Investigating echo state networks dynamics by means of recurrence analysis", "author": ["F.M. Bianchi", "L. Livi", "C. Alippi"], "venue": "arXiv preprint arXiv:1601.07381,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Information processing in echo state networks at the edge of chaos", "author": ["J. Boedecker", "O. Obst", "J.T. Lizier", "N.M. Mayer", "M. Asada"], "venue": "Theory in Biosciences,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Nonlinear time-series analysis revisited", "author": ["E. Bradley", "H. Kantz"], "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A tutorial on support vector machines for pattern recognition", "author": ["C.J. Burges"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Practical method for determining the minimum embedding dimension of a scalar time series", "author": ["L. Cao"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "The smashed filter for compressive classification and target recognition", "author": ["M.A. Davenport", "M.F. Duarte", "M.B. Wakin", "J.N. Laska", "D. Takhar", "K.F. Kelly", "R.G. Baraniuk"], "venue": "In Electronic Imaging 2007, pages 64980H\u201364980H. International Society for Optics and Photonics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Application of echo state networks in short-term electric load", "author": ["A. Deihimi", "H. Showkati"], "venue": "forecasting. Energy,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Short-term electric load and temperature forecasting using wavelet echo state networks with neural reconstruction", "author": ["A. Deihimi", "O. Orang", "H. Showkati"], "venue": "Energy, 57:382\u2013401,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Pruning and regularization in reservoir computing", "author": ["X. Dutoit", "B. Schrauwen", "J.V. Campenhout", "D. Stroobandt", "H.V. Brussel", "M. Nuttin"], "venue": "doi: http://dx.doi.org/10.1016/ j.neucom.2008.12.020. Advances in Machine Learning and Computational Intelligence16th European Symposium on Artificial Neural Networks 200816th European Symposium on Artificial Neural Networks", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "A survey of dimension reduction techniques", "author": ["I.K. Fodor"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Independent coordinates for strange attractors from mutual information", "author": ["A.M. Fraser", "H.L. Swinney"], "venue": "Physical review A,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1986}, {"title": "On bias, variance, 0/1\u2014loss, and the curse-of-dimensionality", "author": ["J.H. Friedman"], "venue": "Data mining and knowledge discovery,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1997}, {"title": "Multiscale analysis of complex time series: integration of chaos and random fractal theory, and beyond", "author": ["J. Gao", "Y. Cao", "W.-w. Tung", "J. Hu"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Measuring the strangeness of strange attractors", "author": ["P. Grassberger", "I. Procaccia"], "venue": "In The Theory of Chaotic Attractors,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2004}, {"title": "A multiple objective optimization based echo state network tree and application to intrusion detection", "author": ["D. Hai-yan", "P. Wen-jiang", "H. Zhen-ya"], "venue": "In VLSI Design and Video Technology,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Fuzzy echo state neural networks and funnel dynamic surface control for prescribed performance of a nonlinear dynamic system", "author": ["S. Han", "J. Lee"], "venue": "Industrial Electronics, IEEE Transactions on,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Fuzzy echo state neural networks and funnel dynamic surface control for prescribed performance of a nonlinear dynamic system", "author": ["S.I. Han", "J.M. Lee"], "venue": "Industrial Electronics, IEEE Transactions on,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Nonparametric estimation of Fisher information from real data", "author": ["O. Har-Shemesh", "R. Quax", "B. Mi\u00f1ano", "A.G. Hoekstra", "P.M.A. Sloot"], "venue": "Physical Review E,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Analysis of a complex of statistical variables into principal components", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1933}, {"title": "A particle swarm optimization to identifying the armax model for short-term load forecasting", "author": ["C.-M. Huang", "C.-J. Huang", "M.-L. Wang"], "venue": "Power Systems, IEEE Transactions on,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "author": ["P. Indyk", "R. Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1998}, {"title": "The \u201cecho state\u201d approach to analysing and training recurrent neural networks-with an erratum note", "author": ["H. Jaeger"], "venue": "Bonn, Germany: German National Research Center for Information Technology GMD Technical Report,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2001}, {"title": "Adaptive nonlinear system identification with echo state networks", "author": ["H. Jaeger"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2002}, {"title": "Harnessing nonlinearity: Predicting chaotic systems and saving energy in wireless communication", "author": ["H. Jaeger", "H. Haas"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2004}, {"title": "Did the ecmwf seasonal forecast model outperform statistical enso forecast models over the last 15 years", "author": ["G. Jan van Oldenborgh", "M.A. Balmaseda", "L. Ferranti", "T.N. Stockdale", "D.L. Anderson"], "venue": "Journal of climate,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2005}, {"title": "Kernel entropy component analysis", "author": ["R. Jenssen"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2010}, {"title": "Entropy-relevant dimensions in the kernel feature space: Cluster-capturing dimensionality reduction", "author": ["R. Jenssen"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Nonlinear time series analysis, volume 7", "author": ["H. Kantz", "T. Schreiber"], "venue": "Cambridge university press,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2004}, {"title": "Chaotic time series prediction based on a novel robust echo state network", "author": ["D. Li", "M. Han", "J. Wang"], "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2012}, {"title": "Proper choice of the time delay for the analysis of chaotic time series", "author": ["W. Liebert", "H. Schuster"], "venue": "Physics Letters A,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1989}, {"title": "Determination of the edge of criticality in echo state networks through fisher information maximization", "author": ["L. Livi", "F.M. Bianchi", "C. Alippi"], "venue": "arXiv preprint arXiv:1603.03685,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Reservoir computing approaches to recurrent neural network training", "author": ["M. Luko\u0161evi\u010dius", "H. Jaeger"], "venue": "Computer Science Review,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Recurrence plots for the analysis of complex systems", "author": ["N. Marwan", "M.C. Romano", "M. Thiel", "J. Kurths"], "venue": "Physics reports,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "Utilization of echo state networks for differentiating source and nonlinear load harmonics in the utility network", "author": ["J. Mazumdar", "R. Harley"], "venue": "Power Electronics, IEEE Transactions on,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "Geometry from a time series", "author": ["N.H. Packard", "J.P. Crutchfield", "J.D. Farmer", "R.S. Shaw"], "venue": "Physical Review Letters,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1980}, {"title": "Nonlinear time-series analysis", "author": ["U. Parlitz"], "venue": null, "citeRegEx": "43", "shortCiteRegEx": "43", "year": 1998}, {"title": "A novel hybridization of echo state networks and multiplicative seasonal ARIMA model for mobile communication traffic series forecasting", "author": ["Y. Peng", "M. Lei", "J.-B. Li", "X.-Y. Peng"], "venue": "Neural Computing and Applications,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "On the dimension and entropy of probability distributions", "author": ["A. R\u00e9nyi"], "venue": "Acta Mathematica Academiae Scientiarum Hungarica,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 1959}, {"title": "The false nearest neighbors algorithm: An overview", "author": ["C. Rhodes", "M. Morari"], "venue": "Computers & Chemical Engineering,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1997}, {"title": "Significance-Based Pruning for Reservoir\u2019s Neurons in Echo State Networks, pages 31\u201338", "author": ["S. Scardapane", "D. Comminiello", "M. Scarpiniti", "A. Uncini"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "Kernel principal component analysis", "author": ["B. Sch\u00f6lkopf", "A. Smola", "K.-R. M\u00fcller"], "venue": "In International Conference on Artificial Neural Networks,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 1997}, {"title": "New support vector algorithms", "author": ["B. Sch\u00f6lkopf", "A.J. Smola", "R.C. Williamson", "P.L. Bartlett"], "venue": "Neural computation,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2000}, {"title": "Automatic speech recognition using a predictive echo state network classifier", "author": ["M.D. Skowronski", "J.G. Harris"], "venue": "Neural networks,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2007}, {"title": "Genetic algorithms: a survey", "author": ["M. Srinivas", "L.M. Patnaik"], "venue": "ISSN 0018-9162", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 1994}, {"title": "Detecting strange attractors in turbulence", "author": ["F. Takens"], "venue": null, "citeRegEx": "52", "shortCiteRegEx": "52", "year": 1981}, {"title": "Dimensionality reduction: a comparative", "author": ["L. Van Der Maaten", "E. Postma", "J. Van den Herik"], "venue": "J Mach Learn Res,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2009}, {"title": "Half Hourly Electricity Load Prediction using Echo State Network", "author": ["S. Varshney", "T. Verma"], "venue": "International Journal of Science and Research,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2014}, {"title": "On the quantification of dynamics in reservoir computing", "author": ["D. Verstraeten", "B. Schrauwen"], "venue": "Artificial Neural Networks \u2013 ICANN 2009,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2009}, {"title": "Modeling systems with internal state using evolino", "author": ["D. Wierstra", "F.J. Gomez", "J. Schmidhuber"], "venue": "In Proceedings of the 7th annual conference on Genetic and evolutionary computation,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2005}, {"title": "Determining lyapunov exponents from a time series", "author": ["A. Wolf", "J.B. Swift", "H.L. Swinney", "J.A. Vastano"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1985}, {"title": "Compressed and privacy-sensitive sparse regression", "author": ["S. Zhou", "J. Lafferty", "L. Wasserman"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2009}], "referenceMentions": [{"referenceID": 37, "context": "Echo State Networks (ESN) belong to the class of computational dynamical systems, implemented according to the so-called reservoir computing approach [39].", "startOffset": 150, "endOffset": 154}, {"referenceID": 25, "context": "Contrary to most hard computing approaches, which demand long training procedures to learn model parameters through an optimization algorithm [27], ESN is characterized by a very fast learning procedure that usually consists in solving a convex optimization problem.", "startOffset": 142, "endOffset": 146}, {"referenceID": 0, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 88, "endOffset": 91}, {"referenceID": 48, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 21, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 161, "endOffset": 165}, {"referenceID": 39, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 199, "endOffset": 203}, {"referenceID": 22, "context": "ESN have been adopted in a variety of different contexts, such as static classification [1], speech recognition [50], intrusion detection [22], adaptive control [23] harmonic distortion measurements [41] and, in general, for modeling of various kinds of non-linear dynamical systems [24].", "startOffset": 283, "endOffset": 287}, {"referenceID": 4, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 12, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 13, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 13, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 42, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 52, "context": "The application field where ESN has been used the most, is the problem of predicting real valued time-series relative, for example, to telephonic or electric load, where the forecast is usually performed 1-hour and a 24-hours ahead [6, 14, 15, 15, 44, 54].", "startOffset": 232, "endOffset": 255}, {"referenceID": 29, "context": "Outstanding results have also been achieved by ESN in prediction of chaotic time-series [31, 36], which highlighted the capability of these neural networks to learn amazingly accurate models to forecast a chaotic process from almost noise-free training data.", "startOffset": 88, "endOffset": 96}, {"referenceID": 34, "context": "Outstanding results have also been achieved by ESN in prediction of chaotic time-series [31, 36], which highlighted the capability of these neural networks to learn amazingly accurate models to forecast a chaotic process from almost noise-free training data.", "startOffset": 88, "endOffset": 96}, {"referenceID": 5, "context": "Additionally, several regression methods adopted to train the readout layer could be affected by the curse of dimensionality in case of high dimensional data, which could also cause increments in both the computational requirements in software and the resource needed in hardware [7].", "startOffset": 280, "endOffset": 283}, {"referenceID": 56, "context": "Several tasks in signal processing and machine learning applications have been tackled by evaluating regression functions [58], performing classification [13] or finding neighbors [28] in a reduced dimensional space.", "startOffset": 122, "endOffset": 126}, {"referenceID": 11, "context": "Several tasks in signal processing and machine learning applications have been tackled by evaluating regression functions [58], performing classification [13] or finding neighbors [28] in a reduced dimensional space.", "startOffset": 154, "endOffset": 158}, {"referenceID": 26, "context": "Several tasks in signal processing and machine learning applications have been tackled by evaluating regression functions [58], performing classification [13] or finding neighbors [28] in a reduced dimensional space.", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "For example, in [16], the authors propose a form of regularization by shrinking the weights of the connections from the reservoir to the readout layer.", "startOffset": 16, "endOffset": 20}, {"referenceID": 45, "context": "In [47] by pruning some connections from the reservoir to the readout layer, better generalization capabilities are achieved along with some insight on which neurons are actually useful for the output, providing clues on how to create a good reservoir.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Even if additional operations are introduced to compute the reduced dimensionality embedding, training the readout layer becomes less demanding, especially in regression methods whose computational complexity depends on input dimension [17].", "startOffset": 236, "endOffset": 240}, {"referenceID": 37, "context": "According to the ESN theory, the reservoir W r must satisfies the so-called \u201cecho state property\u201d (ESP) [39].", "startOffset": 104, "endOffset": 108}, {"referenceID": 6, "context": "A widely used rule-of-thumb suggests to rescale the matrix W r to have \u03c1(W r) < 1, where \u03c1(\u00b7) denotes the spectral radius, but several theoretically-founded approaches have been proposed in the literature to properly tune \u03c1 in an ESN driven by a specific input [8, 9, 55].", "startOffset": 261, "endOffset": 271}, {"referenceID": 7, "context": "A widely used rule-of-thumb suggests to rescale the matrix W r to have \u03c1(W r) < 1, where \u03c1(\u00b7) denotes the spectral radius, but several theoretically-founded approaches have been proposed in the literature to properly tune \u03c1 in an ESN driven by a specific input [8, 9, 55].", "startOffset": 261, "endOffset": 271}, {"referenceID": 53, "context": "A widely used rule-of-thumb suggests to rescale the matrix W r to have \u03c1(W r) < 1, where \u03c1(\u00b7) denotes the spectral radius, but several theoretically-founded approaches have been proposed in the literature to properly tune \u03c1 in an ESN driven by a specific input [8, 9, 55].", "startOffset": 261, "endOffset": 271}, {"referenceID": 0, "context": "To determine them, let us consider the training sequence sequence of Ttr desired input-outputs pairs given by: (x[1], y\u2217[1]) .", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "To determine them, let us consider the training sequence sequence of Ttr desired input-outputs pairs given by: (x[1], y\u2217[1]) .", "startOffset": 120, "endOffset": 123}, {"referenceID": 0, "context": "1, producing a sequence of internal states h[1], .", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "S = \uf8ef\uf8f0 x T [1], h [1] .", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "S = \uf8ef\uf8f0 x T [1], h [1] .", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "x [Ttr], h T [Ttr] \uf8fa\uf8fb , y\u2217 = \uf8ef\uf8f0 y \u2217[1] .", "startOffset": 35, "endOffset": 38}, {"referenceID": 27, "context": "The standard procedure to train the readout, originally proposed in [29], consists in a regularized least-square regression, which can be easily computed through the Moore-Penrose pseudo-inverse.", "startOffset": 68, "endOffset": 72}, {"referenceID": 9, "context": "However, to learn the optimal readout we also consider the Support Vector Regression (SVR), a supervised learning model that can efficiently perform a non-linear separation of data using a kernel function to map the inputs into high-dimensional feature spaces, where they are linearly separable [11].", "startOffset": 295, "endOffset": 299}, {"referenceID": 47, "context": "Support Vector Regression: we adopt a \u03bd-SVR [49] with a Gaussian kernel, initially proposed in [7] as method for readout training.", "startOffset": 44, "endOffset": 48}, {"referenceID": 5, "context": "Support Vector Regression: we adopt a \u03bd-SVR [49] with a Gaussian kernel, initially proposed in [7] as method for readout training.", "startOffset": 95, "endOffset": 98}, {"referenceID": 31, "context": "First of all, we underline that several approaches can be followed for reducing the dimensionality of the data and to learn underlying manifold on a subspace of the data space [33, 34, 53].", "startOffset": 176, "endOffset": 188}, {"referenceID": 32, "context": "First of all, we underline that several approaches can be followed for reducing the dimensionality of the data and to learn underlying manifold on a subspace of the data space [33, 34, 53].", "startOffset": 176, "endOffset": 188}, {"referenceID": 51, "context": "First of all, we underline that several approaches can be followed for reducing the dimensionality of the data and to learn underlying manifold on a subspace of the data space [33, 34, 53].", "startOffset": 176, "endOffset": 188}, {"referenceID": 24, "context": "In this work, we limit our analysis to the well know and effective, yet simple procedures, namely Principal Component Analysis (PCA) [26] and kernel Principal Component Analysis (kPCA) [48].", "startOffset": 133, "endOffset": 137}, {"referenceID": 46, "context": "In this work, we limit our analysis to the well know and effective, yet simple procedures, namely Principal Component Analysis (PCA) [26] and kernel Principal Component Analysis (kPCA) [48].", "startOffset": 185, "endOffset": 189}, {"referenceID": 2, "context": "However, as the size of the reservoir increases, also the complexity of the model grows, with a consequent risk of overfitting caused by a reduced generalization capability [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 17, "context": "Dimensionality reduction and manifold learning are techniques that allows to diminish the variance in the data and to introduce a bias, which can reduce the expected value on the prediction error [19].", "startOffset": 196, "endOffset": 200}, {"referenceID": 6, "context": "Additionally, several methods used to identify, in an unsupervised way, the configurations of hyperparameters which maximize the computational capabilities of the network, require computational demanding procedures of analysis [8, 9, 38].", "startOffset": 227, "endOffset": 237}, {"referenceID": 7, "context": "Additionally, several methods used to identify, in an unsupervised way, the configurations of hyperparameters which maximize the computational capabilities of the network, require computational demanding procedures of analysis [8, 9, 38].", "startOffset": 227, "endOffset": 237}, {"referenceID": 36, "context": "Additionally, several methods used to identify, in an unsupervised way, the configurations of hyperparameters which maximize the computational capabilities of the network, require computational demanding procedures of analysis [8, 9, 38].", "startOffset": 227, "endOffset": 237}, {"referenceID": 49, "context": "The loss function is minimized using a standard genetic algorithm with Gaussian mutation, random crossover, elitism and tournament selection [51].", "startOffset": 141, "endOffset": 145}, {"referenceID": 30, "context": "Accordingly to a commonly used approach [32], in each prediction task we set the forecast step \u03c4f by computing a statistic that measures the independence of \u03c4f separated points in the time series.", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "Alternatively, it is possible to choose the forecast step by considering more general forms of independence, such as the first local minimum on the average mutual information [18] or on the correlation sum [37].", "startOffset": 175, "endOffset": 179}, {"referenceID": 35, "context": "Alternatively, it is possible to choose the forecast step by considering more general forms of independence, such as the first local minimum on the average mutual information [18] or on the correlation sum [37].", "startOffset": 206, "endOffset": 210}, {"referenceID": 28, "context": "NARMA signal: the Non-Linear Auto-Regressive Moving Average (NARMA) task, originally proposed in [30], consists in modeling the output of the following r-order system:", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "The input to the system x(t) is a uniform random noise in [0, 1], and the model is trained to reproduce y(t+1).", "startOffset": 58, "endOffset": 64}, {"referenceID": 29, "context": "The signal we consider is the multiple superimposed oscillator (MSO), studied in [31] and defined as:", "startOffset": 81, "endOffset": 85}, {"referenceID": 54, "context": "ESN struggles to solve this task, since neurons in the reservoir tends to couple, while the task requires the simultaneous existence of multiple decoupled internal states [56].", "startOffset": 171, "endOffset": 175}, {"referenceID": 8, "context": "To understand the mechanics and the effectiveness of the proposed architecture, we analyze the results through the theory of nonlinear time-series analysis, which offer powerful methods to retrieve dynamical information from time-ordered data [10].", "startOffset": 243, "endOffset": 247}, {"referenceID": 18, "context": "We refer the interested reader to [20, 35] for a comprehensive overview of these methods and many other aspects of time-series analysis.", "startOffset": 34, "endOffset": 42}, {"referenceID": 33, "context": "We refer the interested reader to [20, 35] for a comprehensive overview of these methods and many other aspects of time-series analysis.", "startOffset": 34, "endOffset": 42}, {"referenceID": 40, "context": "The delay-coordinate embedding method allows to reconstruct such state vectors from a time-discrete measurement of only one generic smooth function of the state space [42].", "startOffset": 167, "endOffset": 171}, {"referenceID": 50, "context": "With a proper choice of embedding parameters m and \u03c4e, Taken theorem guarantees the existence of a diffeomorhpism between the real and reconstructed dynamic [52].", "startOffset": 157, "endOffset": 161}, {"referenceID": 44, "context": "The value of m is usually computed with the false nearest-neighbors algorithm [46], which provides an estimation of the smallest sufficient embedding dimension.", "startOffset": 78, "endOffset": 82}, {"referenceID": 10, "context": "On the other hand, a suitable time-delay \u03c4e can be estimated looking at the first zero of the autocorrelation function of x or by relying on nonlinear time dependencies, such as the mutual information [12].", "startOffset": 201, "endOffset": 205}, {"referenceID": 43, "context": "Attractors of dissipative chaotic systems often exhibit complicated geometries (hence the name strange) which are contained in a fractal dimension Dq, called R\u00e9nyi dimension [45].", "startOffset": 174, "endOffset": 178}, {"referenceID": 19, "context": "An efficient estimator of fractal dimensions is Grassberger-Procaccia algorithm [21], which computes the correlation dimension D2 through the correlation sum C2:", "startOffset": 80, "endOffset": 84}, {"referenceID": 41, "context": "divergent motion of the reconstructed space, without fitting a model to the data [43, 57].", "startOffset": 81, "endOffset": 89}, {"referenceID": 55, "context": "divergent motion of the reconstructed space, without fitting a model to the data [43, 57].", "startOffset": 81, "endOffset": 89}, {"referenceID": 1, "context": "Moore\u2013Spiegel: this dynamical systems manifests interesting synchronization properties, generated by complicated patterns of period-doubling, saddle-node and homoclinic bifurcations [3].", "startOffset": 182, "endOffset": 185}, {"referenceID": 38, "context": "The reconstructed attractors have a lower correlation dimension, which usually denotes a poor embedding [40].", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "In fact, this provides a valid alternative to the standard approach based on the time-delay embedding for reconstructing the phase of the system, which presents several caveats and pitfalls [10].", "startOffset": 190, "endOffset": 194}, {"referenceID": 33, "context": "This a fundamental tool for a wide set of applications, where an accurate estimation of the phase space of the system is required [35].", "startOffset": 130, "endOffset": 134}, {"referenceID": 36, "context": "As a follow-up of a recent work focused on identifying the edge of criticality of an ESN by evaluating the Fisher information on the state matrix [38], we plan to study the criticality using more reliable Fisher Information Matrix estimators, which are capable of working only on space with few dimensions (e.", "startOffset": 146, "endOffset": 150}, {"referenceID": 23, "context": ", [25]).", "startOffset": 2, "endOffset": 6}, {"referenceID": 2, "context": "We also plan on investigating other dimensionality reduction methods, manifold learning and semi-supervised learning approaches to shrink and regularize the output of the network recurrent layer [4, 5].", "startOffset": 195, "endOffset": 201}, {"referenceID": 3, "context": "We also plan on investigating other dimensionality reduction methods, manifold learning and semi-supervised learning approaches to shrink and regularize the output of the network recurrent layer [4, 5].", "startOffset": 195, "endOffset": 201}], "year": 2016, "abstractText": "In this paper we introduce a new framework to train an Echo State Network to predict real valued time-series. The method consists in projecting the output of the internal layer of the network on a space with lower dimensionality, before training the output layer to learn the target task. Notably, we enforce a regularization constraint that leads to better generalization capabilities. We evaluate the performances of our approach on several benchmark tests, using different techniques to train the readout of the network, achieving superior predictive performance when using the proposed framework. Finally, we provide an insight on the effectiveness of the implemented mechanics through a visualization of the trajectory in the phase space and relying on the methodologies of nonlinear time-series analysis. By applying our method on well known chaotic systems, we provide evidence that the lower dimensional embedding retains the dynamical properties of the underlying system better than the full-dimensional internal states of the network. Keywords\u2014 Echo state network, nonlinear time-series analysis, dimensionality reduction, timeseries prediction", "creator": "LaTeX with hyperref package"}}}