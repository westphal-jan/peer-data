{"id": "1608.04670", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Aug-2016", "title": "Attribute Extraction from Product Titles in eCommerce", "abstract": "This paper presents a named entity extraction system for detecting attributes in product titles of eCommerce retailers like Walmart. The absence of syntactic structure in such short pieces of text makes extracting attribute values a challenging problem. We find that combining sequence labeling algorithms such as Conditional Random Fields and Structured Perceptron with a curated normalization scheme produces an effective system for the task of extracting product attribute values from titles. To keep the discussion concrete, we will illustrate the mechanics of the system from the point of view of a particular attribute - brand. We also discuss the importance of an attribute extraction system in the context of retail websites with large product catalogs, compare our approach to other potential approaches to this problem and end the paper with a discussion of the performance of our system for extracting attributes.", "histories": [["v1", "Mon, 15 Aug 2016 03:34:13 GMT  (319kb)", "http://arxiv.org/abs/1608.04670v1", "Accepted at the Workshop on Enterprise Intelligence, KDD 2016"]], "COMMENTS": "Accepted at the Workshop on Enterprise Intelligence, KDD 2016", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["ajinkya more"], "accepted": false, "id": "1608.04670"}, "pdf": {"name": "1608.04670.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["@WalmartLabs", "amore@walmartlabs.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.04 670v 1 [cs.C L] 15 Aug 201 6"}, {"heading": "1. INTRODUCTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Vocabulary", "text": "Before we start discussing the problem, we will first define some terms that will be used in the rest of the paper. A product is any commodity that can be sold by a retailer. An attribute is a feature that describes a certain property of a product or product list. Some examples of attributes are brand, color, gender, material, title, description, etc. An attribute value is a certain value that is assumed by the attribute. For example, for the product title Apple iPad Mini 3 16GB Wi-Fi Refurbished, Goldthe brand attribute value is \"Apple\" and the value of the color attribute is \"Gold.\" A product can alternatively be defined as a collection of such attribute value pairs where a value can be potentially empty. Formally, let p represent a product with the attributes \u03b11, \u03b12,..., \u03b1m and values v1, v2,..., vm or the attribute vm. \"Then we present p = {1: vetane, 1}, \u03b12, and \u03b1p."}, {"heading": "1.2 Problem set up", "text": "We formalize attribute extraction according to the following definition.Definition 1. Let x be a product title and let (x1, x2,.., xn) a certain tokenization xt of x. Given an attribute \u03b1, attribute extraction is the process of discovering two functions E (raw extraction) and N (normalization) such that \u2022 E (xt) = E (x1, x2,..., xn)) = (xi, xi + 1,..., xk) for 1 \u2264 i \u2264 k \u2264 n, where av = (xi, xi + 1,..., xk) a tokenization of a certain value of \u03b1, \u2022 N ((xi, xi + 1,..., xk)) = like the standardized representation of av.Example 1. Consider the product titlex = Hex = 1, xL03A # B1H Officejet Pro Eaioso that whitespace tokenization ydsydsxt = 1, x2, xlett = x1, xlett = (x1)."}, {"heading": "2. USE CASES", "text": "In this section, we explain the importance of attribute extraction for retail websites."}, {"heading": "2.1 Discoverability", "text": "To ensure a good multi-faceted navigation experience, it is important to assign value metadata to the products for the attributes that appear in the facets. For example, let S be a search query entered by a user, and R be the amount of products returned as a result. Suppose the product p, R, and p has an attribute that also happens to be a facet. Let's also assume that the value from p to p is applicable. If the user clicks on the facet value v, the filtered result is R. Then p, if and only if p (\u03b1) = v. is more specifically assumed that a user enters the search query \"T-Shirt\" and the result set contains the product titled Hanes Mens NANO-T Dri T-Shirt S Deep Red, the product will have a value if the attribute \"Color\" is missing for the product."}, {"heading": "2.2 Ad campaigns", "text": "Certain attribute values are also required by ad publishers such as Google and Bing to launch product ad campaigns. If the required attributes are missing, the ad campaigns are rejected."}, {"heading": "2.3 Compliance", "text": "Some attribute values are necessary to meet government compliance requirements, such as the unit price of food or items sold in large quantities."}, {"heading": "2.4 Knowledge discovery", "text": "The list of valid values for a particular attribute cannot be specified, for example, attributes such as \"brand,\" \"character,\" \"model number,\" etc. It is desirable to develop a solution to find attribute values that are not currently part of the knowledge base."}, {"heading": "3. CHALLENGES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Extracting attributes from product titles", "text": "In the next few sections, we will describe the attribute extraction system based on the specific example of branded goods. In certain product categories, the brand is an important attribute that presents a number of interesting challenges, some of which are outlined below. In later sections, we will discuss how these problems can be solved. Unlike other product types that do not adhere to a syntactic structure, they may involve a concatenation of several nouns and adjectives. Verbs tend to be missing, and there are no standardized products that adhere to a syntactic structure."}, {"heading": "3.2 Comparison with other approaches", "text": "In fact, it is the case that most people who are able are able to determine for themselves what they want and what they do not want."}, {"heading": "4. PRIOR WORK", "text": "Tagging of products with attributes falls under the umbrella of knowledge extraction from text. Such problems have been explored in a variety of application areas. We present a brief overview of similar undertakings in other areas. Bikel et.al. [2] discuss the problem of named entity extraction to identify location names, person names, named organizations, and a few other entities. They present a hidden-markov model and show that for named entity extraction tasks it is based on standard data sets such as MUC-6 and MET-1. Ritter et. al. [19] built a system for mining named entities from tweets - which is another example of text significantly different from fluent prospectuses. Part of language tagging is a quintessence example of an entity detection task and a number of approaches have been studied in the literature. Eric Brill [3] introduced a simple rule of taggers learning the language of sharing a decision, which has a 90% error rate if they have a 7.9% error rate."}, {"heading": "5. SEQUENCE LABELING APPROACHES", "text": "Considering an input sequence x = x1, x2,..., xm, a sequence marking algorithm aims to find a sequence marking sequence y = y1, y2,..., ym in such a way that the element xj is labeled with yj. Labels yj usually come from a finite set. A typical example of such an approach is language marking in natural language processing. In this paper, we evaluated performance using two sequence marking algorithms - Structured Perception and Conditional Random Fields. In Section 9, we compare the performance of these models against another sequence marking algorithm - the Hidden Markov Model."}, {"heading": "5.1 Feature Functions", "text": "Let X be the set of all input sequences and let Y be the set of all caption sequences of length m. Let I = {1, 2,..., m}. A feature function for a annotation algorithm of the sequence is a function f: X \u00b7 Y \u00b7 I \u2192 R. Consider, for example, the problem of the part of language tagging, in which each word in a text unit (e.g. a sentence) is assigned a tag / label corresponding to its part of the language. Tags appearing in Penn Treebank [14] include DT (determinator), JJ (adjective), NN (noun), VB (verb) and IN (preposition). Now leave x = (Die, fast, brown, fox, jumps, the, the, the, the, lazy, dog), y = (DT, JJ, JJ, NN, VB, IN, verb) and the sequence d (preposition)."}, {"heading": "5.2 Structured Perceptron", "text": "Michael Collins introduced the learning algorithm Structured Perceptron in [6]. We use the refinement of the algorithm referred to as averaged parameters in this essay. Averaging the parameters reduces the variance, which has a regulating effect and improves performance ([6], [10]).Structured Perceptron is a supervised learning algorithm. The learning set for the algorithm consists of labeled sequences {(xi, yi)} in which i = 1, 2,..., n. Each input xi is a sequence of form (x1, x2,..., xm) i with a corresponding sequence of labels (y1, y2,..., ym) i, so that the input element xj has a corresponding label yj. The labels belong to a finite set YL. Let YS label the set of all labels of length m so that each entry in the sequence belongs to Yj."}, {"heading": "5.3 Linear Chain Conditional Random Fields", "text": "Conditional Random Fields (CRF) is a probabilistic structured label algorithm introduced by Lafferty, McCallum and Pereira in [13]. In our system, we used a linear chain CRF (LCCRF). Let x, y specify input and label sequences and let YS represent the set of all label sequences. Let F (x, y) be the d-dimensional label vector corresponding to the pair x, y as defined in Section 5.1. Then, for a given weight vector w-R, according to the LCCRF model we have, Pr (y = y-x-w) = exp (wTF (x, y-x-p)). Given a weight vector w-w, we try to find a label sequence that maximizes the above conditional probability."}, {"heading": "6. ANNOTATING PRODUCT TITLES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Labeling scheme", "text": "To input both models, we used a product title that was split into a sequence of tokens with organic encoding. In the first step of the labeling process, the title is split into a sequence of tokens separated by spaces and / or specific special characters. The BIO encoding scheme assigns one of the following three labels to each of the tokens obtained in this way. 1. B Trademark: This label indicates that the tokens are the initial character of a brand name. It is not used if the title does not contain a brand name. 2. I Trademark: This label indicates that the tokens are an intermediate character (i.e. other than the first) of a brand name. It is not used if the title does not contain a brand name or if the brand name does not contain a brand name in the title."}, {"heading": "6.2 Distantly supervised training data", "text": "To grasp the variations in which an attribute name may appear in product titles, we required a substantial training set. However, creating an appropriate training record by manual labeling would require significant and time-consuming effort. Alternatively, we could use the set of products that already had the corresponding attribute value. However, this approach is vulnerable to any noise in the existing tags that turned out to be significant. We used a remote monitoring approach to build our first training record. For each attribute, we built regex-based rules to make product titles programmable according to the labeling scheme in Section 6.1. For example, in the case of the brand, we only mentioned product titles where the brand name appears exactly as a substring, except possibly for a change in the case and / or the presence or absence of certain special characters. Note that not all brand values are accurately included in product titles that represent a value normalization that we discuss in Section 8.1."}, {"heading": "6.3 Interpreting output labels", "text": "The output of the learning algorithm on a product title x is a sequence of labels - one label per token in the tokenization text of x, according to the BIO encoding scheme specified in Section 6.1. This labeling is converted into a candidate brand name by applying the following steps: 1. If the label of all the tokens of the product title is \"O,\" we assume that a brand name is not in the title and the output value for the product title is \"unbranded.\" In the notation of Section 1.2, E is (xt) = \"and N (E (xt) =\" unbranded. \"2 Otherwise, there must be a token labeled with the\" Bbrand. \"Consider the contiguous sub-sequence of the tokens xs satisfactory: (a) The label of the first token of the first token is\" B mark. \"(b) If the length of the sub-sequence is greater than 1, then the label of each token."}, {"heading": "7. FEATURES", "text": "Both SP and LCCRF models were trained on similar features, and feature selection was based on a variety of literature on the subject, including [21] and [18]. We curated and adapted feature functions to those that improved the performance of the feature extraction system, and feature selection was done using ablative analysis. In this methodology, we begin with a set of feature functions and measure the effects of iterative disabling of each feature on relevant metrics (e.g. formula 1 measurement). The final set of feature functions used has the property that removing any feature function adversely affects those of interest. Alternatively, we can start with a feature function and add features that improve performance over and over again."}, {"heading": "7.1 Feature functions", "text": "Some examples of attributes used in the system to extract attributes are listed below. Unlike many other named entity recognition systems, we do not use attributes based on attribute-specific lexicographs.7.1.1 Characteristics These attributes are based solely on information about a token, such as the identity of the token, the composition of the letter, the length of the token in relation to the number of characters. 7.1.2 Local attributes These attributes depend on the position of the token in the order in which the title is decomposed, for example, the number of tokens in the title before the given token. 7.1.3 Contextual attributes In order for the sequence marking algorithms to work well, we must gather information about tokens that are in the vicinity of a given token. This can be achieved by attributes such as the identity of the preceding / following token, whether the preceding / following token is capitalized, the token that consists of the predecessor, the bigram, and so on."}, {"heading": "7.2 Features used in brand extraction", "text": "For the specific case of brand extraction, we show below the set of characteristics that gave the best performance over a sample of product titles in selected departments. We added characteristics step by step until there was an improvement in the average F1 measure 1 achieved by a 10-fold cross-validation. We use the following notation. To determine feature functions corresponding to a position i in a title, we mark w0 the token under consideration. If applicable, we mark w \u2212 1, w \u2212 2,... the preceding tokens and leave w1, w2,... the consecutive tokens. For a token w [j], we mark the jth character. The table below shows the decline in the average F1 measure \u2212 F1 (in percent) recorded by turning off a specific feature at a specific time. All measurements are based on a 10-fold cross-validation."}, {"heading": "8. POST PROCESSING", "text": "An artifact of using a sequence labeling algorithm for the task of extracting trademarks is that the extracted brand equity for each product title is a substring (possibly empty) of the title. As a result, a false brand prediction is rarely itself a brand name. Consider, for example, the product title Sea Gull Lighting Parkfield 3 Light Bath Vanity Light, where the brand name is \"Sea Gull Lighting.\" Suppose that we are introducing Sea Gull Lighting products for the first time and therefore the brand does not appear in the list of known brands. Furthermore, each subline of that product title consisting of consecutive tokens is not a brand name. This motivated the subsequent post-processing scheme that significantly increased the accuracy of accepted brand predictions."}, {"heading": "8.1 Normalization", "text": "As mentioned in Section 3, a given brand name can occur in a variety of spellings (for example, missing spaces, presence / absence of special characters, use of acronyms, etc.) in product titles. To represent a facet, we formally define the relevant ratios associated with the extraction of attributes in Section 9.1. Variations of a single brand name must be normalized to a unique value. We maintain a collection of key value pairs where the key is a brand variant and the value is the normalized brand name. Normalized values are curated by internal analysts. We will call this collection the \"Normalization Dictionary.\" Consider, for example, the following titles that correspond to the brand \"Chenille Kraft.\" We highlight the relevant brand names in bold. \u2022 Chenille Kraft Wonderfoam Magnetic Alphabet Letters, Assorted Colors, 105pk \u2022 THE CHENILLE KRAFTER {Chillifoam Chillers, Chillifoam Strength it, Stem Strength it."}, {"heading": "8.2 Blacklist", "text": "In addition to the normalization dictionary, we also maintain a list of terms that are not known as brand names. Typically, the list consists of terms from product titles that do not contain brand names. This list is expanded whenever the process of brand extraction yields an unacceptable value."}, {"heading": "8.3 Manual feedback", "text": "In this subsection, we will consider the case where brand extraction is performed in batch mode. In a single pass, any product with a missing brand value within a category to which a brand facet is applicable will be considered a candidate for extraction. We will follow the procedure below to obtain the final result. Suppose the brand extraction algorithm runs on n positions that are considered indexed 1, 2,..., n. Let the prediction for article i count as brand (i) and designate the normalization dictionary 2 and the black list as N or B. For each predicted value that is not contained in N, we will maintain a dictionary F that tracks the number of a given value that has been encountered so far. We will ignore predicted values that are either not in the normalization dictionary or that do not meet a given threshold frequency f in a given batch. For i = 1 to n, if the brand (i), if the brand (vandale) function is used (we vandale)."}, {"heading": "9. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "9.1 Metrics", "text": "Now we define the metrics used to measure the performance of the brand extraction algorithm. Let n be the number of products on which we run the algorithm. Let nTB be the number of products that have a true brand equity, other than \"unbranded.\" Let nPB be the number of products that have a predicted brand equity, other than \"unbranded.\" Let c be the number of predictions that are correct and have a brand equity other than \"unbranded.\" Then we define precision = P = cnPBandRecall = R = cnTB. The motivation to calculate precision and recall only products with a brand equity other than \"unbranded\" stems from the fact that \"unbranded\" value is not presented in facets and therefore these metrics more accurately represent the quality of the brand facet. Finally, we define F1 = 2PRP + R For diagnostic purposes that we use the label, we define x."}, {"heading": "9.2 Model performance", "text": "In fact, it is the case that most of us are able to establish ourselves in the United States, both in the United States and in the United States. (...) In the United States, it is the case that most of us are able to establish ourselves in the United States. (...) In the United States, it is the case that most of us have emigrated to the United States. (...) In the United States, it is the case that most of us have emigrated to the United States. (...) In the United States, it is the case that most of us have emigrated to the United States. (...) In the United States, it is the case that they have emigrated to the United States. (...) In the United States, it is the case that they have emigrated to the United States. (...) In the United States, it is the case that they have emigrated to the United States. (...)"}, {"heading": "10. OTHER ATTRIBUTES", "text": "Some examples are manufacturer-specific model numbers and names, fictional characters that inspire children's toys, sports team and league names for sportswear and memorabilia, product lines, etc. The above approach is readily applicable to extracting such attributes. Most of the methodology is similar to that used for brands. We were able to use the same algorithms, similar features, remote monitoring to create annotated product titles for training and standardization schemes to standardize variations of attribute values to create models with high precision and remember such attributes."}, {"heading": "11. REAL WORLD PERFORMANCE", "text": "We used the approach outlined above to extract brands for a subset of products in selected departments. Early results of algorithmic extraction were manually validated by analysts; the normalization dictionary was regularly updated based on analyst feedback; for the manually validated batches, accuracy varied from 92% -95% and showed an improvement of 1% -3% over algorithmically extracted values; sample validations of products categorized as unbranded showed that over 98% of them were in fact true negatives; the algorithm could show over a thousand brand values that were not part of the brand database at this point; and we estimated the impact of brand value creation on the R package CausalImpact."}, {"heading": "12. CONCLUSION", "text": "In this paper, we presented the problem of extracting attribute values from product titles to expand product metadata for e-commerce catalogs. We discussed the importance of attribute extraction for retail websites. We explored the challenges associated with this task, especially when the product catalog is large. Extracting attributes from product titles was modeled as a sequence labeling problem. We also illustrated a method to use existing products marked with attribute values to create the first training dataset without manual labeling. Experimental results show that SP or LCCRF models, combined with a curated normalization scheme, provide an effective mechanism to mark products with specific attribute values with high precision and memory."}, {"heading": "13. REFERENCES", "text": "[1] Harith Alani, Sanghee Kim, David E. Millard, Mark J. Weal, Wendy Hall, Paul H. Lewis, and Nigel R. Shadbolt. Automatic ontology-based knowledge extraction from web documents. IEEE Intelligent Systems, 18 (1): 14-21, 2003. [2] Daniel M. Bikel, Richard Schwartz, and Ralph M. Weischedel. Proceedings of the Third Conference on Applied Natural Language Processing, pages 152-155, 1992. [3] Eric Brill. A simple rule-based part of speech tagger, and Ralph M. Weischedel. Proceedings of the Third Conference on Applied Natural Language Processing, pages 152-155, 1992. Kay H Brodersen, Fabian Gallusser, Jim Koehler, Nicolas Remy, Steven L Scott, et al. Inferring causal."}], "references": [{"title": "Automatic ontology-based knowledge extraction from web documents", "author": ["Harith Alani", "Sanghee Kim", "David E. Millard", "Mark J. Weal", "Wendy Hall", "Paul H. Lewis", "Nigel R. Shadbolt"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "An algorithm that learns what\u2019s in a name", "author": ["Daniel M. Bikel", "Richard Schwartz", "Ralph M. Weischedel"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1999}, {"title": "A simple rule-based part of speech tagger", "author": ["Eric Brill"], "venue": "Proceedings of the Third Conference on Applied Natural Language Processing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Inferring causal impact using bayesian structural time-series models", "author": ["Kay H Brodersen", "Fabian Gallusser", "Jim Koehler", "Nicolas Remy", "Steven L Scott"], "venue": "The Annals of Applied Statistics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Domain adaptation of rule-based annotators for namedentity recognition tasks", "author": ["Laura Chiticariu", "Rajasekar Krishnamurthy", "Yunyao Li", "Frederick Reiss", "Shivakumar Vaithyanathan"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2002}, {"title": "Unsupervised named-entity extraction from the web: An experimental study", "author": ["Oren Etzioni", "Michael Cafarella", "Doug Downey", "Ana- Maria Popescu", "Tal Shaked", "Stephen Soderland", "Daniel S. Weld", "Alexander Yates"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2005}, {"title": "Rule-based named entity recognition for greek financial texts", "author": ["Dimitra Farmakiotou", "Vangelis Karkaletsis", "John Koutsias", "George Sigletos", "Constantine D Spyropoulos", "Panagiotis Stamatopoulos"], "venue": "In Proceedings of the Workshop on Computational lexicography and Multimedia Dictionaries (COMLEX", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2000}, {"title": "Text mining for product attribute extraction", "author": ["R. Ghani", "K. Probst", "Y. Liu", "M. Krema", "A. Fano"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Learning sparser perceptron models", "author": ["Yoav Goldberg", "Michael Elhadad"], "venue": "Technical report, Ben Gurion University of the Negev,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Improving product classification using images", "author": ["Anitha Kannan", "Patha Talukdar", "Nikhil Rasiwasia", "Qifa Ke"], "venue": "Data Mining (ICDM),", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Exploiting wikipedia as external knowledge for named entity recognition", "author": ["Jun\u2019ichi Kazama", "Kentaro Torisawa"], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Conditional random fields: probabilistic models for segmenting and labeling sequence data", "author": ["J. Lafferty", "A. McCallum", "F. Pereira"], "venue": "International Conference on Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz"], "venue": "Computational Linguistics,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Named entity recognition without gazetteers", "author": ["Andrei Mikheev", "Marc Moens", "Clair Grover"], "venue": "In Proceedings of the ninth conference on European chapter of the Association for Computational Linguistics,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1999}, {"title": "A survey of named entity recognition and classification", "author": ["David Nadeau", "Satoshi Sekine"], "venue": "Lingvisticae Investigationes,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "Extracting product features and opinions from reviews", "author": ["Ana-Maria Popescu", "Oren Etzioni"], "venue": "Natural language processing and text mining,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "Bootstrapped named entity recognition for product attribute extraction", "author": ["Duangmanee Putthividhya", "Junling Hu"], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "Named entity recognition in tweets: An experimental study", "author": ["Alan Ritter", "Sam Clark", "Mausam", "Oren Etzioni"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["Helmut Schmid"], "venue": "Proceedings of the international conference on new methods in language processing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1994}, {"title": "Named entity recognition: Exploring features", "author": ["Maksim Tkachenko", "Andrey Simanovsky"], "venue": "In Proceedings of KONVENS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}], "referenceMentions": [{"referenceID": 15, "context": "Some gazetteer based approaches are discussed in [16] in the context of named entity recognition problems.", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "Rule based approaches have had success in certain named entity recognition tasks [5], [8], [15].", "startOffset": 81, "endOffset": 84}, {"referenceID": 7, "context": "Rule based approaches have had success in certain named entity recognition tasks [5], [8], [15].", "startOffset": 86, "endOffset": 89}, {"referenceID": 14, "context": "Rule based approaches have had success in certain named entity recognition tasks [5], [8], [15].", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "[2] discuss the named entity extraction problem to identify location names, person names, named organizations and a few other entities.", "startOffset": 0, "endOffset": 3}, {"referenceID": 18, "context": "[19] built a system for mining named entities from tweets - which is another example of text that significantly deviates from fluent prose.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Eric Brill [3] introduced a simple rule based tagger for learning part of speech which was shown to have an error rate of 7.", "startOffset": 11, "endOffset": 14}, {"referenceID": 19, "context": "Schmid [20] illustrates a probabilistic tagger in which transition probabilities are estimated using a decision tree and which achieves an accuracy of 96.", "startOffset": 7, "endOffset": 11}, {"referenceID": 0, "context": "[1] present a system to extract knowledge about artists from web pages leveraging a curated ontology.", "startOffset": 0, "endOffset": 3}, {"referenceID": 11, "context": "Kazama and Torisawa [12] use features relying on Wikipedia information to generate a Conditional Random Fields (CRF) based model for named entity recognition.", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "[7] describe unsupervised techniques to extract relationships between entities from web documents.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] discuss the representation of retail products as attribute-value pairs.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "Popescu and Etzioni [17] extend their system described in [7] to the problem of mining product features", "startOffset": 20, "endOffset": 24}, {"referenceID": 6, "context": "Popescu and Etzioni [17] extend their system described in [7] to the problem of mining product features", "startOffset": 58, "endOffset": 61}, {"referenceID": 17, "context": "Putthividhya and Hu [18] designed a system for extracting product attributes from short listing titles such as those found on eBay.", "startOffset": 20, "endOffset": 24}, {"referenceID": 13, "context": "The tags that appear in Penn Treebank [14] include DT (determiner), JJ (adjective), NN (noun), VB (verb) and IN (preposition).", "startOffset": 38, "endOffset": 42}, {"referenceID": 5, "context": "Michael Collins presented the Structured Perceptron learning algorithm in [6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "Parameter averaging reduces variance providing a regularization effect and improves performance ([6], [10]).", "startOffset": 97, "endOffset": 100}, {"referenceID": 9, "context": "Parameter averaging reduces variance providing a regularization effect and improves performance ([6], [10]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 12, "context": "Conditional Random Fields (CRF) is a probabilistic structured labeling algorithm introduced by Lafferty, McCallum and Pereira in [13].", "startOffset": 129, "endOffset": 133}, {"referenceID": 20, "context": "The choice of feature functions was motivated from a variety of literature on this subject including [21] and [18].", "startOffset": 101, "endOffset": 105}, {"referenceID": 17, "context": "The choice of feature functions was motivated from a variety of literature on this subject including [21] and [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "Additionally, for the LCCRF model, we define Precision and Recall as functions of the threshold \u03b8 of the conditional probability of the predicted label sequence [11].", "startOffset": 161, "endOffset": 165}, {"referenceID": 1, "context": "In estimating probabilities for unknown words, we mapped them to one of several morphological classes similar to [2].", "startOffset": 113, "endOffset": 116}, {"referenceID": 3, "context": "We estimated the impact of brand value extraction on product discoverability using the R package CausalImpact [4].", "startOffset": 110, "endOffset": 113}], "year": 2016, "abstractText": "This paper presents a named entity extraction system for detecting attributes in product titles of eCommerce retailers like Walmart. The absence of syntactic structure in such short pieces of text makes extracting attribute values a challenging problem. We find that combining sequence labeling algorithms such as Conditional Random Fields and Structured Perceptron with a curated normalization scheme produces an effective system for the task of extracting product attribute values from titles. To keep the discussion concrete, we will illustrate the mechanics of the system from the point of view of a particular attribute brand. We also discuss the importance of an attribute extraction system in the context of retail websites with large product catalogs, compare our approach to other potential approaches to this problem and end the paper with a discussion of the performance of our system for extracting attributes.", "creator": "LaTeX with hyperref package"}}}