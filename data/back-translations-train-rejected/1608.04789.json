{"id": "1608.04789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Modelling Student Behavior using Granular Large Scale Action Data from a MOOC", "abstract": "Digital learning environments generate a precise record of the actions learners take as they interact with learning materials and complete exercises towards comprehension. With this high quantity of sequential data comes the potential to apply time series models to learn about underlying behavioral patterns and trends that characterize successful learning based on the granular record of student actions. There exist several methods for looking at longitudinal, sequential data like those recorded from learning environments. In the field of language modelling, traditional n-gram techniques and modern recurrent neural network (RNN) approaches have been applied to algorithmically find structure in language and predict the next word given the previous words in the sentence or paragraph as input. In this paper, we draw an analogy to this work by treating student sequences of resource views and interactions in a MOOC as the inputs and predicting students' next interaction as outputs. In this study, we train only on students who received a certificate of completion. In doing so, the model could potentially be used for recommendation of sequences eventually leading to success, as opposed to perpetuating unproductive behavior. Given that the MOOC used in our study had over 3,500 unique resources, predicting the exact resource that a student will interact with next might appear to be a difficult classification problem. We find that simply following the syllabus (built-in structure of the course) gives on average 23% accuracy in making this prediction, followed by the n-gram method with 70.4%, and RNN based methods with 72.2%. This research lays the ground work for recommendation in a MOOC and other digital learning environments where high volumes of sequential data exist.", "histories": [["v1", "Tue, 16 Aug 2016 21:46:48 GMT  (82kb,D)", "http://arxiv.org/abs/1608.04789v1", "15 pages, 7 tables, 3 figures"]], "COMMENTS": "15 pages, 7 tables, 3 figures", "reviews": [], "SUBJECTS": "cs.CY cs.LG", "authors": ["steven tang", "joshua c peterson", "zachary a pardos"], "accepted": false, "id": "1608.04789"}, "pdf": {"name": "1608.04789.pdf", "metadata": {"source": "CRF", "title": "Modelling Student Behavior using Granular Large Scale Action Data from a MOOC", "authors": ["Steven Tang", "Joshua C. Peterson", "Zachary A. Pardos"], "emails": ["steventang@berkeley.edu", "peterson.c.joshua@gmail.com", "pardos@berkeley.edu"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.04 789v 1 [cs.C Y]"}, {"heading": "1 Introduction", "text": "In this context, it should be noted that the measures in question are measures taken in the past."}, {"heading": "2 Related Work", "text": "In this case, the fact is that most of them are able to move, to move and to move."}, {"heading": "3 Dataset", "text": "The data set used in this paper came from a BerkeleyX MOOC statistic from the spring of 2013. MOOC ran for five weeks with video lectures, homework assignments, discussion forums and two exams. The original data set includes 17 million actions by approximately 31,000 students, with each action representing access to a specific link in the course. Certification sometimes means that the student pays for a specific certification, but that is not the case for these assignments and achieves high scores on the exams considered \"certified\" by the course's teachers."}, {"heading": "4 Methodology", "text": "In this section, we describe the architecture of the recurrent neural network and the LSTM extension, the model that we assume will perform best in predicting the next action. Other \"flat\" models, such as the n-gram, are described below."}, {"heading": "4.1 Recurrent Neural Networks", "text": "Recursive neural networks (RNNs) are a family of networks that can connect neurons over time, meaning that sequences of any length can be fed into RNNs. Crucially, RNNs have a high-dimensional, continuous latent state that allows RNs to use information from the past to make predictions at a later time. In this paper, each input to the RNN will be a granular student action from a MOOC dataset. The RNN is trained to predict the next action of the student. Figure 1 shows a diagram of a simple recursive neural network where inputs would be student actions and outputs would be the next student action from the sequence.ht = tanh (W xxt + W hht \u2212 1 + bh) (1) yt = circuit (W yht + by) (2) The RN model is weighted by an input matrix, or \u2212 W."}, {"heading": "4.2 LSTM Models", "text": "This year it is so far that it is only a matter of time before it is so far, until it is so far."}, {"heading": "4.3 Shallow Models", "text": "N-gram ModelN-gram models are simple but powerful probabilistic models that aim to capture the structure of sequences through the statistics of n-large sub-sequences called grams and are equivalent to n-order Markov chains. Specifically, the model says each sequence state xi using the estimated conditional probability P (xi | xi \u2212 (n \u2212 1), which is the probability that xi follows the previous n-1 states in the training set. Ngram models are both quick and easy to calculate and have a simple interpretation. We expect n-grams to be an extremely competitive standard as they are relatively high parameter models that essentially assign one parameter per possible action in the action set. N-gram Model StructureFor the n-gram models, we have evaluated models where n ranged from 2 to 10, the largest of which corresponds to the size of our LSTM context window during training."}, {"heading": "5 Results", "text": "In this section we discuss the results of the aforementioned LSTM models, which have been trained at different learning rates, the number of hidden nodes per shift, and the number of LSTM layers. Success of the model is determined by 5-fold cross-validation and depends on how well the model predicts the next action. N-gram models and other course structure models are validated by 5-fold cross-validation."}, {"heading": "5.1 LSTM Models", "text": "Table 2 shows the CV accuracy for all 21 LSTM models calculated after 10 iterations of training. For models with a learning rate of 0.01, the accuracy of mountaineering sets peaked in iteration 10. For models with lower learning rates, it would be reasonable to expect that the maximum CV accuracy would improve with more training. We chose to simply report the results after 10 iterations rather than provide a snapshot of how well these models perform during the training process. We also expect that model performance will improve dramatically over time. We have a need to maximize the most promising explorations in order to run on limited GPU calculation resources. The best CV accuracy for each learning rate is bold for emphasis. One drawback of using LSTMs is that they require the use of a GPU and are relatively slow."}, {"heading": "5.2 Course Structure Models", "text": "The results indicate that many actions can be predicted using simple heuristics, such as the stationarity (as last) or the structure of the course content. The combination of both heuristics (\"curriculum + repetition\") delivers the best results, although none of the alternative models has performed well in terms of LSTM or n-gram results."}, {"heading": "5.3 N-gram Models", "text": "The best-performing models made predictions using either the previous 7 or 8 actions (8 grams or 9 grams). Larger gradients did not improve performance, suggesting that our n-gram spectrum was sufficiently large. Performance in general suggests that n-gram models competed with the LSTM models, even though the best n-gram model performed worse than the best LSTM models. Table 6 shows the proportion of n-gram models used for the most complex model (10 grams). More than 62% of the predictions were made using 10-gram observations. Furthermore, less than 1% of the cases fell back on unigrams or bigrams to make macro-credits, suggesting that there was no significant lack of observations for larger gram patterns. However, about 6% less data appears to be predicted by successive larger n-gram models."}, {"heading": "5.4 Validating on Uncertified Students", "text": "We used the highest-performing \"original\" LSTM model after 10 training periods (0.01 learning rate, 256 nodes, 2 shifts) to predict actions based on streams of students who were ultimately not certified. Many non-certified students had few logged-in actions, so we limited the analysis to students who had at least 30 logged-in actions. There were 10761 students who met these criteria, with a total of 2151662 actions. The LSTM model was able to correctly predict actions from the non-certified classroom with 6709 accuracy, compared with.7093 cross-validated accuracy for certified students. This difference shows that actions of certified students tend to be different from actions of non-certified students and may have a potential application by providing an automated suggestion framework to assist students in leadership."}, {"heading": "6 Contribution", "text": "In this paper, we approached the problem of modelling granular student action data by modelling all types of interactions within a MOOC. This differs from previous work, which focused primarily on modelling latent student knowledge based on evaluation outcomes. In predicting a student's next action, the most powerful LSTM model resulted in a cross-validation accuracy of.7223, which was an improvement over the best n-gram model accuracy of.7035, corresponding to 210,000 more accurate predictions out of a total of a possible 11 million. Table 7 shows how often the two models agreed or disagreed on a correct or incorrect prediction during cross-validation. Both the LSTM model and the n-gram model offer significant improvements over the structural model of predicting the next action through curriculum structure and continuous repeats, showing that there are patterns of student engagement that clearly differ from a course navigation entirely through linear."}, {"heading": "7 Future Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Model and Dataset Improvements", "text": "Both the LSTM and the n-gram models have room for improvement. In particular, our n-gram models could benefit from a combination of baking and smoothing techniques that allow for better handling of invisible grammars. Our LSTM could benefit from a broader search for hyperparameters, more training time, longer training context windows, and higher-dimensional action embedding. In addition, the signal-to-noise ratio in our data set could be increased by removing less informative or redundant student actions or adding additional tokens to represent the time between actions."}, {"heading": "7.2 Applications", "text": "The main reason for applying deep learning models to large amounts of student action data is to model student behavior in MOOC settings, leading to insights into how successful and unsuccessful students navigate the course. These patterns can be used to help create automated recommendation systems, where a struggling student can be given transition recommendations to view content based on their past behavior and performance. To evaluate the possibility of such an application, we plan to test a referral system from our network experimentally against an undirected control group."}, {"heading": "7.3 Incorporating Larger Datasets", "text": "In this paper, we examined student actions from a single MOOC course. Future work should evaluate the performance of similar models for a variety of courses and examine the extent to which cross-course patterns can be learned from a single model."}, {"heading": "Acknowledgement", "text": "This work was supported by a grant from the National Science Foundation (IIS: BIGDATA 1547055)."}], "references": [{"title": "Class-based n-gram models of natural language", "author": ["Peter F Brown", "Peter V Desouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai"], "venue": "Computational linguistics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1992}, {"title": "Recurrent neural network based language model", "author": ["Tomas Mikolov", "Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Latent skill embedding for personalized lesson sequence recommendation", "author": ["Siddharth Reddy", "Igor Labutov", "Thorsten Joachims"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Knowledge tracing: Modeling the acquisition of procedural knowledge", "author": ["Albert T Corbett", "John R Anderson"], "venue": "User modeling and user-adapted interaction,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1994}, {"title": "Deep knowledge tracing", "author": ["Chris Piech", "Jonathan Bassen", "Jonathan Huang", "Surya Ganguli", "Mehran Sahami", "Leonidas J Guibas", "Jascha Sohl-Dickstein"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "How deep is knowledge tracing", "author": ["Mohammad Khajah", "Robert V Lindsey", "Michael C Mozer"], "venue": "arXiv preprint arXiv:1604.02416,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1997}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141 ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Identifying latent study habits by mining learner behavior patterns in massive open online courses", "author": ["Miaomiao Wen", "Carolyn Penstein Ros\u00e9"], "venue": "In Proceedings of the 23rd ACM International Conference on Information and Knowledge Management,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Yoshua Bengio", "Patrice Simard", "Paolo Frasconi"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2000}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "word2vec explained: deriving mikolov et al.\u2019s negative-sampling word-embedding method", "author": ["Yoav Goldberg", "Omer Levy"], "venue": "CoRR, abs/1402.3722,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Generalization of backpropagation with application to a recurrent gas market model", "author": ["Paul J Werbos"], "venue": "Neural Networks,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1988}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "In Frontiers in Handwriting Recognition (ICFHR),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1409.2329,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "A simple but powerful model used in natural language processing (NLP) is the n-gram model [1], where a probability distribution is learned over every possible sequence of n terms from the training set.", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "More recently, recurrent neural networks (RNNs) have been used to perform next-word prediction [2], where previously seen words are subsumed into a high dimensional continuous latent state.", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "Other work has been done to map assignments, student ability, lesson gains and pre-requisites all onto the same dimensional embedding space, where dimensions represent latent skills [3].", "startOffset": 182, "endOffset": 185}, {"referenceID": 3, "context": "More broadly, much work has been done to assess the latent knowledge of students through models such as Bayesian Knowledge Tracing (BKT) [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 4, "context": "Deep Knowledge Tracing [5] uses recurrent neural networks to create a continuous latent representation of students based on previously seen assessment results as they navigate online learning environments.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "Such results, however, are hypothesized to be explained by already existing extensions of BKT [6].", "startOffset": 94, "endOffset": 97}, {"referenceID": 6, "context": "Specifically, in this paper we consider using both the n-gram approach as well as using a variant of the RNN known as the Long Short-Term Memory (LSTM) architecture [7].", "startOffset": 165, "endOffset": 168}, {"referenceID": 7, "context": "The use of LSTM architectures and similar variants have recently achieved impressive results in a variety fields that involve sequential data, including speech, image, and text analysis [8, 9, 10], in part due to its mutable memory that allows for the capture of longand short-range dependencies in sequences.", "startOffset": 186, "endOffset": 196}, {"referenceID": 8, "context": "The use of LSTM architectures and similar variants have recently achieved impressive results in a variety fields that involve sequential data, including speech, image, and text analysis [8, 9, 10], in part due to its mutable memory that allows for the capture of longand short-range dependencies in sequences.", "startOffset": 186, "endOffset": 196}, {"referenceID": 9, "context": "The use of LSTM architectures and similar variants have recently achieved impressive results in a variety fields that involve sequential data, including speech, image, and text analysis [8, 9, 10], in part due to its mutable memory that allows for the capture of longand short-range dependencies in sequences.", "startOffset": 186, "endOffset": 196}, {"referenceID": 10, "context": "previous work, modelling of student clicksteam data has shown promise with methods such as n-gram models [11].", "startOffset": 105, "endOffset": 109}, {"referenceID": 6, "context": "A popular variant of the RNN is the Long Short-Term Memory [7] architecture, which is thought to help RNNs train by the addition of \u201dgates\u201d that learn when to retain meaningful information in the latent state and when to clear or \u201dforget\u201d the latent state, allowing for meaningful long-term interactions to persist.", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "As a result of these gates, LSTMs have been found to train more effectively than simple RNNs [12, 13].", "startOffset": 93, "endOffset": 101}, {"referenceID": 12, "context": "As a result of these gates, LSTMs have been found to train more effectively than simple RNNs [12, 13].", "startOffset": 93, "endOffset": 101}, {"referenceID": 13, "context": "The generative LSTM models used in this paper were implemented using Keras [14], a Python library built on top of Theano [15, 16].", "startOffset": 121, "endOffset": 129}, {"referenceID": 14, "context": "The generative LSTM models used in this paper were implemented using Keras [14], a Python library built on top of Theano [15, 16].", "startOffset": 121, "endOffset": 129}, {"referenceID": 15, "context": "The use of an embedding layer is common in natural language processing and language modelling [17] as a way to map words to a multi dimensional semantic space.", "startOffset": 94, "endOffset": 98}, {"referenceID": 16, "context": "Back propagation through time [18] is used to train the LSTM parameters, using a softmax layer with the index of the next action as the ground truth.", "startOffset": 30, "endOffset": 34}, {"referenceID": 17, "context": "Drop out layers were added between LSTM layers as a method to curb overfitting [19].", "startOffset": 79, "endOffset": 83}, {"referenceID": 18, "context": "In future work, it may be worthwhile to evaluate other regularization techniques crafted specifically for LSTMs and RNNs [20].", "startOffset": 121, "endOffset": 125}, {"referenceID": 19, "context": "These hyperparameters were chosen for grid search based on previous work which prioretized different hyperparameters based on effect size [21].", "startOffset": 138, "endOffset": 142}], "year": 2016, "abstractText": "Digital learning environments generate a precise record of the actions learners take as they interact with learning materials and complete exercises towards comprehension. With this high quantity of sequential data comes the potential to apply time series models to learn about underlying behavioral patterns and trends that characterize successful learning based on the granular record of student actions. There exist several methods for looking at longitudinal, sequential data like those recorded from learning environments. In the field of language modelling, traditional n-gram techniques and modern recurrent neural network (RNN) approaches have been applied to algorithmically find structure in language and predict the next word given the previous words in the sentence or paragraph as input. In this paper, we draw an analogy to this work by treating student sequences of resource views and interactions in a MOOC as the inputs and predicting students\u2019 next interaction as outputs. In this study, we train only on students who received a certificate of completion. In doing so, the model could potentially be used for recommendation of sequences eventually leading to success, as opposed to perpetuating unproductive behavior. Given that the MOOC used in our study had over 3,500 unique resources, predicting the exact resource that a student will interact with next might appear to be a difficult classification problem. We find that simply following the syllabus (built-in structure of the course) gives on average 23% accuracy in making this prediction, followed by the n-gram method with 70.4%, and RNN based methods with 72.2%. This research lays the ground work for recommendation in a MOOC and other digital learning environments where high volumes of sequential data exist. 1 ar X iv :1 60 8. 04 78 9v 1 [ cs .C Y ] 1 6 A ug 2 01 6", "creator": "LaTeX with hyperref package"}}}