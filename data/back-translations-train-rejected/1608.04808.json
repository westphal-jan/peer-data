{"id": "1608.04808", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Aug-2016", "title": "Learning Latent Local Conversation Modes for Predicting Community Endorsement in Online Discussions", "abstract": "Many social media platforms offer a mechanism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the latent modes can be used to weight text features thereby improving prediction accuracy.", "histories": [["v1", "Tue, 16 Aug 2016 23:37:43 GMT  (1573kb,D)", "https://arxiv.org/abs/1608.04808v1", "10 pages, 7 figures"], ["v2", "Wed, 28 Sep 2016 09:46:24 GMT  (1574kb,D)", "http://arxiv.org/abs/1608.04808v2", "10 pages, 7 figures"]], "COMMENTS": "10 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.SI cs.CL", "authors": ["hao fang", "hao cheng", "mari ostendorf"], "accepted": false, "id": "1608.04808"}, "pdf": {"name": "1608.04808.pdf", "metadata": {"source": "CRF", "title": "Learning Latent Local Conversation Modes for Predicting Community Endorsement in Online Discussions", "authors": ["Hao Fang", "Hao Cheng", "Mari Ostendorf"], "emails": ["hfang@uw.edu", "chenghao@uw.edu", "ostendorf@uw.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to move, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to move, to fight, to fight, to fight, to move, to fight, to fight, to move, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2 Related Work", "text": "The cumulative response of readers to social media and online content was studied using a variety of metrics that predict the response to the proposed cases, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares in news articles (Bandari et al., 2012), the number of repetitions on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Zhao et al., 2015), and the difference in the number of repetitions on posts and comments on Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). One advantage of working with the Reddit data is that both positive and negative reactions are taken into account, so that the total (Karma in Reddit) provides an appropriate prediction for the community."}, {"heading": "3 Data and Task", "text": "Data: Reddit (https: / / www.reddit.com) is a discussion forum with thousands of sub-communities organized as sub-speakers. Users can initiate a tree-structured discussion thread by submitting a post in a sub-redit. Comments are sent either directly to the root post or to other comments within the thread, sometimes triggering sub-discussions. Any comment can receive up- and downvotes from registered users; the difference is shown as a karma score next to the comment. The graph structure of a Reddit discussion thread is shown inTask: Community support (i.e., Karma in Reddit) is widespread in many discussion forums, including those examined here, with most comments receiving minimal support and high support from comments that are of the utmost interest, and we don't want to treat them as a regression problem."}, {"heading": "4 Model Description", "text": "The idea behind this is that people in the world who are in a position to put themselves in the world, to put themselves in the world in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which, in which they, in which they, in which they, in which, in which they, in which, in which they, in which, in which they, in the world, in which they, in which, in which they, in the world, in the world, in which they, in the world, in which they, in the world, in which they, in which they, in which they, in which, in which they, in which, in which, in which, in which they, in which, in the world, in which they, in the world, in which, in which"}, {"heading": "5 Parameter Learning", "text": "To train the proposed model, each comment is treated as an independent sample, and the goal is the maximum log probability of these samples. We use mini-batch stochastic gradient lineage with a batch size of 32, calculating the gradients using the back-propagation algorithm (Rumelhart et al., 1986). Specifically, the Adam algorithm is used (Kingma and Ba, 2015).The initial learning rate is selected from the range of [0.0010, 0.0100], with a step size of 0.0005, corresponding to the log similarity of the validation data in the first epoch. The learning rate is halved in each epoch as soon as the log probability of the validation data decreases. The entire training procedure ends when the log probability decreases for the second time. Each comment is treated as a data sample and a partition number in {0, 1,."}, {"heading": "6 Experiments", "text": "In this section, we present the performance of the proposed model and conduct contrastive experiments to examine model variants in two dimensions. For the characteristics of the submission context, we compare representations obtained via feedback-forward neural networks with those obtained through a learned combination of latent base vectors. In terms of text-dependent characteristics, we compare a model that does not use text, context-dependent text features, and a context-dependent gating mechanism. Finally, we analyze the learned latent submission contexts as well as context-dependent gate values that reflect the amount of text information used by the entire model."}, {"heading": "6.1 Model Configuration", "text": "All parameters in the neural networks except bias terms are randomly initialized according to the Gaussian distribution N (0, 10 \u2212 2). We match the number of latent bases K and the number of hidden layer neurons C and D based on the macro F1 values to the validation data. For the complete model, the best configuration uses K = 8, C = 32, and D = 64 for all substances except Politics, where D = 32 is used."}, {"heading": "6.2 Main Results", "text": "Top-level comment detection is most reliable in Subreddit Politics, but still difficult. Table 2 compares models that use only the contextual characteristics of the transmission; the SubtreeSize baseline uses a multinominal logistical regression model to predict the level based solely on the subtree size characteristic, while the ConvStruct uses the same model but with all the contextual structural characteristics defined in Tabel 1. All baselines are stronger than predictions based on previous distributions, the F1 values in the range 11-17. The Feedfwd-n model is a forward-oriented neural network with n hidden layers; it uses the contextual characteristic c in Fig. 3 for prediction; the LatentBases model represents the context information of the transmission through a linear combination of latent bases; it uses c values for prediction."}, {"heading": "6.3 Analysis", "text": "In this subsection, we analyze the learned submission contexts and gate values that control the amount of text information used by the topic to predict comments. Submission context modes: To study the submission context modes, we assign each comment to a cluster according to which the base vector gets the highest weight: argmaxk = 1,... Kak. The label distribution for each cluster is shown in Fig. 5. It can be observed that some clusters are dominated by level 0 comments, and others are dominated by level 7 comments. In Fig. 6, we visualize the learned clusters by projecting the raw conversation structures x onto a two-dimensional space."}, {"heading": "7 Conclusion", "text": "In summary, this paper addresses the problem of predicting community consent to comments in a discussion forum using a new neural network architecture that integrates contextual features of submission (including relative timing and response structure), with features extracted from the text of a comment. It presents the context of submission in the sense of a linear combination of latent base vectors that characterize the dynamic conversation mode that leads to results that resemble a deep network but are easier to interpret. It also includes a dynamic gateway for text content, and the analysis shows that when the predictor has a response structure available, the content of a comment has the greatest benefit for comments that are not in active regions of the discussion. These results are based on the characterization of quantified karma layers with a set of binary classifiers. Quantized karma predictions could also be used as a regular regression task, which would require the submission task to be clearly modified."}, {"heading": "Acknowledgments", "text": "This paper is based on work supported by the DARPA DEFT Program. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the US government."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proc. Int. Conf. Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "The pulse of news in social media: forecasting popularity", "author": ["Bandari et al.2012] Roja Bandari", "Sitaram Asur", "Bernardo Huberman"], "venue": "In Proc. Int. AAAI Conf. Web and Social Media (ICWSM)", "citeRegEx": "Bandari et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bandari et al\\.", "year": 2012}, {"title": "Compositional morphology for word representations and language modelling", "author": ["Botha", "Blunsom2014] Jan A. Botha", "Phil Blunsom"], "venue": "In Proc. Int. Conf. Machine Learning (ICML)", "citeRegEx": "Botha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Botha et al\\.", "year": 2014}, {"title": "Can cascade be predicted", "author": ["Cheng et al.2014] Justin Cheng", "Lada Adamic", "P. Alex Dow", "Jon Michael Kleinberg", "Jure Leskovec"], "venue": "In Proc. Int. Conf. World Wide Web (WWW),", "citeRegEx": "Cheng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2014}, {"title": "Open-domain name error detection using a multi-task RNN", "author": ["Cheng et al.2015] Hao Cheng", "Hao Fang", "Mari Ostendorf"], "venue": "In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP)", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahadanau", "Fethhi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Exponential language modeling using morphological features and multi-task learning", "author": ["Fang et al.2015] Hao Fang", "Mari Ostendorf", "Peter Baumann", "Janet Pierrehumbert"], "venue": "IEEE Trans. Audio, Speech, and Language Process.,", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Predicting popular messages in Twitter", "author": ["Hong et al.2011] Liangjie Hong", "Ovidiu Dan", "Brian Davison"], "venue": "In Proc. Int. Conf. World Wide Web (WWW),", "citeRegEx": "Hong et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hong et al\\.", "year": 2011}, {"title": "Talking to the crowd: What do people react to in online discussions", "author": ["Jaech et al.2015] Aaron Jaech", "Vicky Zayats", "Hao Fang", "Mari Ostendorf", "Hannaneh Hajishirzi"], "venue": "In Proc. Conf. Empirical Methods Natural Language Process. (EMNLP),", "citeRegEx": "Jaech et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jaech et al\\.", "year": 2015}, {"title": "Head/tail break: A new classification scheme for data with a heavy-tailed distribution", "author": ["Bin Jiang"], "venue": "The Professional Geographer,", "citeRegEx": "Jiang.,? \\Q2013\\E", "shortCiteRegEx": "Jiang.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "In Proc. Int. Conf. Learning Representations (ICLR)", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "What\u2019s in a name? Understanding the interplay between titles, content, and communities in social media", "author": ["Julian McAuley", "Jure Leskovec"], "venue": "In Proc. Int. AAAI Conf. Web and Social Media (ICWSM)", "citeRegEx": "Lakkaraju et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lakkaraju et al\\.", "year": 2013}, {"title": "The Stanford CoreNLP natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven J. Bethard", "David McClosky"], "venue": "In Proc. Annu. Meeting Assoc. for Computational Linguistics:", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["Mass et al.2013] Andrew L. Mass", "Awni Y. Hannun", "Andrew Y. Ng"], "venue": "In Proc. Int. Conf. Machine Learning (ICML)", "citeRegEx": "Mass et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mass et al\\.", "year": 2013}, {"title": "Deep sentence embedding using long short-term memory networks: Analysis and application to information", "author": ["Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab Ward"], "venue": null, "citeRegEx": "Palangi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Palangi et al\\.", "year": 2016}, {"title": "Learning representations by back-propogating", "author": ["Geoffrey E. Hinton", "Ronald J. Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1986\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1986}, {"title": "Want to be retweeted? large scale analytics on factors impacting retweet in twitter network", "author": ["Suh et al.2010] B. Suh", "L. Hong", "P. Pirolli", "E.H. Chi"], "venue": "In Proc. IEEE Inter. Conf. on Social Computing (SocialCom),", "citeRegEx": "Suh et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Suh et al\\.", "year": 2010}, {"title": "End-toend memory networks", "author": ["Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS),", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "The effect of wording on message propagation: Topic- and author-controlled natural experiments on Twitter", "author": ["Tan et al.2014] Chenhao Tan", "Lillian Lee", "Bo Pang"], "venue": "In Proc. Annu. Meeting Assoc. for Computational Linguistics (ACL),", "citeRegEx": "Tan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Predicting the volume of comments on online news stories", "author": ["Wouter Weerkamp", "Maarten de Rijke"], "venue": "In Proc. CIKM,", "citeRegEx": "Tasgkias et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Tasgkias et al\\.", "year": 2009}, {"title": "Predicting the polularity of online articles based on user comments", "author": ["Jeremie Leguay", "Panayotis Antoniadis", "Arnaud Limbourg", "Marcelo Dias de Amorim", "Serge Fdida"], "venue": "In Proc. Inter. Conf. on Web", "citeRegEx": "Tatar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Tatar et al\\.", "year": 2011}, {"title": "Visualizing data using t-SNE", "author": ["van der Maaten", "Geoffrey Hinton"], "venue": "Machine Learning Research,", "citeRegEx": "Maaten et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Maaten et al\\.", "year": 2008}, {"title": "Grammar as a foreign language", "author": ["Lukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "In Proc. Annu. Conf. Neural Inform. Process. Syst. (NIPS),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "What\u2019s worthy of comment? content and comment volume in political blogs", "author": ["Yano", "Smith2010] Tae Yano", "Noah A. Smith"], "venue": "In Proc. Int. AAAI Conf. Weblogs and Social Media (ICWSM)", "citeRegEx": "Yano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Yano et al\\.", "year": 2010}, {"title": "SEISMIC: A self-exciting point process model for predicting Tweet popularity", "author": ["Zhao et al.2015] Qingyuan Zhao", "Murat A. Erdogdu", "Hera Y. He", "Anand Rajaraman", "Jure Leskovec"], "venue": "In Proc. ACM SIGKDD Conf. Knowledge Discovery and Data Min-", "citeRegEx": "Zhao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "The cumulative response of readers to social media and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al.", "startOffset": 221, "endOffset": 264}, {"referenceID": 22, "context": "The cumulative response of readers to social media and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al.", "startOffset": 221, "endOffset": 264}, {"referenceID": 1, "context": ", 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al.", "startOffset": 55, "endOffset": 77}, {"referenceID": 3, "context": ", 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al.", "startOffset": 44, "endOffset": 64}, {"referenceID": 17, "context": ", 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al.", "startOffset": 32, "endOffset": 106}, {"referenceID": 8, "context": ", 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al.", "startOffset": 32, "endOffset": 106}, {"referenceID": 20, "context": ", 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al.", "startOffset": 32, "endOffset": 106}, {"referenceID": 26, "context": ", 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al.", "startOffset": 32, "endOffset": 106}, {"referenceID": 12, "context": ", 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015).", "startOffset": 120, "endOffset": 164}, {"referenceID": 9, "context": ", 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015).", "startOffset": 120, "endOffset": 164}, {"referenceID": 1, "context": "Various prediction tasks have been proposed with this in mind, including regression on a log score (Bandari et al., 2012), classification into 3-4 groups (e.", "startOffset": 99, "endOffset": 121}, {"referenceID": 21, "context": "none, low, high) (Tasgkias et al., 2009; Hong et al., 2011; Yano and Smith, 2010), a binary decision as to whether the score will double given a current score (Lakkaraju et al.", "startOffset": 17, "endOffset": 81}, {"referenceID": 8, "context": "none, low, high) (Tasgkias et al., 2009; Hong et al., 2011; Yano and Smith, 2010), a binary decision as to whether the score will double given a current score (Lakkaraju et al.", "startOffset": 17, "endOffset": 81}, {"referenceID": 12, "context": ", 2011; Yano and Smith, 2010), a binary decision as to whether the score will double given a current score (Lakkaraju et al., 2013), and relative ranking of comments (Tan et al.", "startOffset": 107, "endOffset": 131}, {"referenceID": 20, "context": ", 2013), and relative ranking of comments (Tan et al., 2014; Jaech et al., 2015).", "startOffset": 42, "endOffset": 80}, {"referenceID": 9, "context": ", 2013), and relative ranking of comments (Tan et al., 2014; Jaech et al., 2015).", "startOffset": 42, "endOffset": 80}, {"referenceID": 11, "context": "To tease apart the factor of content quality, Lakkaraju et al. (2013) predict resharing of duplicated image submissions, investigating both the submission context (community, time of day, resubmission statistics) and language factors.", "startOffset": 46, "endOffset": 70}, {"referenceID": 11, "context": "To tease apart the factor of content quality, Lakkaraju et al. (2013) predict resharing of duplicated image submissions, investigating both the submission context (community, time of day, resubmission statistics) and language factors. Our work differs in that content is not controlled and the submission context includes the response structure and relative timing of the comment within the discussion. Tan et al. (2014) futher control the author and temporal factors in addition to the topic of the content, by ranking pairs of tweets with almost identical content made by the same author within a limited time window.", "startOffset": 46, "endOffset": 421}, {"referenceID": 9, "context": "Jaech et al. (2015) control the temporal factor for ranking Reddit comments made in a time-limited window and study different language factors.", "startOffset": 0, "endOffset": 20}, {"referenceID": 10, "context": "The quantization process is similar to the head-tail break rule described in (Jiang, 2013).", "startOffset": 77, "endOffset": 90}, {"referenceID": 14, "context": "Given the raw submission context feature vector x \u2208 RN , the model computes a vector c \u2208 RC as c = LReL(Px), where P \u2208 RC\u00d7N is a projection matrix, and LReL(\u00b7) is the leaky rectified linear function (Mass et al., 2013) with 0.", "startOffset": 199, "endOffset": 218}, {"referenceID": 0, "context": "The computation of basis coefficients is similar to the attention mechanism that has been used in the context of machine translation (Bahdanau et al., 2015), constituency parsing (Vinyals et al.", "startOffset": 133, "endOffset": 156}, {"referenceID": 24, "context": ", 2015), constituency parsing (Vinyals et al., 2015), question answering and language modeling (Weston et al.", "startOffset": 30, "endOffset": 52}, {"referenceID": 18, "context": ", 2015), question answering and language modeling (Weston et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 50, "endOffset": 96}, {"referenceID": 19, "context": "Text embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016).", "startOffset": 145, "endOffset": 211}, {"referenceID": 4, "context": "Text embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016).", "startOffset": 145, "endOffset": 211}, {"referenceID": 15, "context": "Text embeddings: Recurrent neural networks (RNNs) have been widely used to obtain sequence embeddings for different applications in recent years (Sutskever et al., 2014; Cheng et al., 2015; Palangi et al., 2016).", "startOffset": 145, "endOffset": 211}, {"referenceID": 13, "context": "To generate the token input vector to the RNN, we utilize the lemma and part-of-speech (POS) tag of each token (obtained with the Stanford CoreNLP toolkit (Manning et al., 2014)), in addition to its word form.", "startOffset": 155, "endOffset": 177}, {"referenceID": 5, "context": "GRU(\u00b7, \u00b7) denotes the gated recurrent unit (GRU), which is proposed by Cho et al. (2014) as a simpler alternative to the long short-term memory unit (Hochreiter and Schmidhuber, 1997) for addressing the vanishing gradient issue in RNNs.", "startOffset": 71, "endOffset": 89}, {"referenceID": 6, "context": "This type of additive token embedding has been used in (Botha and Blunsom, 2014; Fang et al., 2015) to integrate various types of information about the token.", "startOffset": 55, "endOffset": 99}, {"referenceID": 16, "context": "We use mini-batch stochastic gradient descent with a batch size of 32, where the gradients are computed with the back-propagation algorithm (Rumelhart et al., 1986).", "startOffset": 140, "endOffset": 164}, {"referenceID": 9, "context": "Prior work has shown that the relevance of a comment to the preceding discussion matters (Jaech et al., 2015), and clearly the sentiment expressed in responses should provide important cues.", "startOffset": 89, "endOffset": 109}], "year": 2016, "abstractText": "Many social media platforms offer a mechanism for readers to react to comments, both positively and negatively, which in aggregate can be thought of as community endorsement. This paper addresses the problem of predicting community endorsement in online discussions, leveraging both the participant response structure and the text of the comment. The different types of features are integrated in a neural network that uses a novel architecture to learn latent modes of discussion structure that perform as well as deep neural networks but are more interpretable. In addition, the latent modes can be used to weight text features thereby improving prediction accuracy.", "creator": "LaTeX with hyperref package"}}}