{"id": "1608.04868", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Towards Music Captioning: Generating Music Playlist Descriptions", "abstract": "Descriptions are often provided along with recommendations to help users' discovery. Recommending automatically generated music playlists (e.g. personalised playlists) introduces the problem of generating descriptions. In this paper, we propose a method for generating music playlist descriptions, which is called as music captioning. In the proposed method, audio content analysis and natural language processing are adopted to utilise the information of each track.", "histories": [["v1", "Wed, 17 Aug 2016 06:24:46 GMT  (62kb,D)", "http://arxiv.org/abs/1608.04868v1", "2 pages, ISMIR 2016 Late-breaking/session extended abstract"], ["v2", "Sun, 15 Jan 2017 05:23:51 GMT  (62kb,D)", "http://arxiv.org/abs/1608.04868v2", "2 pages, ISMIR 2016 Late-breaking/session extended abstract"]], "COMMENTS": "2 pages, ISMIR 2016 Late-breaking/session extended abstract", "reviews": [], "SUBJECTS": "cs.MM cs.AI cs.CL", "authors": ["keunwoo choi", "george fazekas", "brian mcfee", "kyunghyun cho", "mark sandler"], "accepted": false, "id": "1608.04868"}, "pdf": {"name": "1608.04868.pdf", "metadata": {"source": "CRF", "title": "TOWARDS MUSIC CAPTIONING: GENERATING MUSIC PLAYLIST DESCRIPTIONS", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Brian McFee", "Kyunghyun Cho"], "emails": ["keunwoo.choi@qmul.ac.uk", "first.last@nyu.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "A common method is to add descriptions of a piece of music or a playlist, such as \"Getting emotional with the undisputed King of Pop 1,\" \"Just the right mix of chilled out acoustic songs to work, relax, think, and dream to 2.\" These examples show that they are more than simple descriptions and even add value to the curated playlist as a product. Attempts have been made to automate the generation of these descriptions. In [8], Eck et al. suggested using social tags to describe each music product. Fields suggested developing a similar idea for the playlist with social tags and theme models. [9] With Latent Dirichlet Allocation, Bogdanov introduced music avatars whose outlook - hairstyle, clothing, and accessories - describes the recommended music."}, {"heading": "2. PROBLEM DEFINITION", "text": "The problem of subtitling music can be defined as generating a description for a set of songs based on their audio content and text data. If the set contains more than one element, it can also be called a subtitle for music playlists."}, {"heading": "3. THE PROPOSED METHOD", "text": "Both approaches use sequence-to-sequence model, as shown in Figure 2. In the sequence-to-sequence model, the encoder consists of two-layer RNN with GRU and encodes the trace characteristics into a vector, i.e., the encoded vector summarizes the information of the input. This vector is also called context vector because it delivers the context vector Xiv: 160 8.04 868v 1 [cs.M] 17 August 201 6information to the decoder. The decoder consists of two-layer RNN with GRU and decodes the context vector to a sequence of word or word embedding. The models are written in keras and uploaded online 3 [6]."}, {"heading": "3.1 Pre-training approach", "text": "This approach uses a pre-formed word embedding model 4 and a pre-formed auto-tagger 5. Therefore, the number of parameters to be learned is reduced while additional data is used to train word embedding and auto-tagger.Each data example consists of a sequence of N-track characteristics as input and an output word sequence length of M, which is an album characteristic. Input / Output 6: An n-th-track characteristic, tn-R350, represents a track and is concatenated by the audio characteristics, tna-R50, and the word function, tnw-R300, i.e. t = [ta; tw]. For the calculation of ta, a revolutionary neural network, trained to predict tags, is used to generate 50-dim vector for each track [5]. tw is calculated by k-wk / K / K, where wk kernel references the embedding of the word into the metadata."}, {"heading": "3.2 Fully-training approach", "text": "The model of this approach involves training a ConvNet for audio summary and an RNN for text summary of each track. ConvNet's structure can be similar to that of the pre-trained one. The RNN module is trained to summarize the text of each track and outputs a sentence vector. These networks can be provided with additional labels (noted as y in Figure 2) to help with the training, such as genres or tags. In this case, the goal of the entire structure consists of two different tasks and therefore the training can be more regulated and stable. As the audio and text summary modules are traceable, they can be more relevant to the subtitling task. However, this flexibility requires more training data."}, {"heading": "4. EXPERIMENTS AND CONCLUSIONS", "text": "The data set includes 374 albums and 17,354 tracks with descriptions of tracks, albums, audio signals and metadata. Learning rate is controlled by ADAM [11] with an objective function of 1 cosine proximity. The model was trained to predict the album descriptions. Currently, the model does not fit and does not generate correct sentences. An example of generated word sequences is dramatic and motivates the intense epic action adventure that shoots to glorious heights, Roger Deakin's cinematography Maryse Alberti. This is to be expected, as there are only 374 output sequences in the data set - if we use early stops, the model does not fit, otherwise it will over.In the future, we plan to solve the current problem - the lack of data. Sentence generation can be partially trained by (music) companies. A word2vec model that is trained with music companies can be used to reduce the embedding dimension [14] so that the model can also be modified."}, {"heading": "5. ACKNOWLEDGEMENTS", "text": "This work was funded by the FAST IMPACt EPSRC Grant EP / L019981 / 1 and the H2020 Research and Innovation Grant AudioCommons (688382) of the European Commission. Mark Sandler acknowledges the support of the Royal Society as recipient of a Wolfson Research Merit Award. Brian McFee is supported by the Moore Sloan Data Science Environment at NYU. Kyunghyun Cho thanks the support of Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016)."}, {"heading": "6. REFERENCES", "text": "[1] David M Blei, Andrew Y Ng, and Michael I Jordan. Latent dirichlet Allokation. Journal of machine learning research, 3 (Jan): 993-1022, 2003. [2] Dmitry Bogdanov, Mart\u0131n Haro, Ferdinand Fuhrmann, Anna Xambo, Emilia Go \u0301 mez, and Perfecto Herrera. Semantic audio content-based music recommendation and visualization based on user preference examples. Information Processing & Management, 49 (1): 13-33, 2013. [3] Kyunghyun Cho, Bart Van Merrie \ufffd nboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder distributions. arXiv preprint arXiv preprint arXiv, 1409.1259, 2014."}], "references": [{"title": "Latent dirichlet allocation", "author": ["David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Semantic audio content-based music recommendation and visualization based on user preference examples", "author": ["Dmitry Bogdanov", "Mart\u0131\u0301N Haro", "Ferdinand Fuhrmann", "Anna Xamb\u00f3", "Emilia G\u00f3mez", "Perfecto Herrera"], "venue": "Information Processing & Management,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["Keunwoo Choi", "George Fazekas", "Mark Sandler"], "venue": "In International Society of Music Information Retrieval Conference", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Keras. GitHub repository: https://github", "author": ["Fran\u00e7ois Chollet"], "venue": "com/fchollet/keras,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Document embedding with paragraph vectors", "author": ["Andrew M Dai", "Christopher Olah", "Quoc V Le"], "venue": "arXiv preprint arXiv:1507.07998,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Automatic generation of social tags for music recommendation", "author": ["Douglas Eck", "Paul Lamere", "Thierry Bertin-Mahieux", "Stephen Green"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Using song social tags and topic models to describe and compare playlists", "author": ["Ben Fields", "Christophe Rhodes", "Mark d\u2019Inverno"], "venue": "In 1st Workshop On Music Recommendation And Discovery (WOMRAD), ACM RecSys,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2010}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "J Dean"], "venue": "Advances in neural information processing systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Natural language processing for music information retrieval", "author": ["Sergio Oramas", "Luies Espinosa-Anke", "Shuo Zhang", "Horacio Saggion", "Xavier Serra"], "venue": "In 17th International Society for Music Information Retrieval Conference (ISMIR", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "In [8], Eck et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 8, "context": "Fields proposed a similar idea for playlist using social tag and topic model [9] using Latent Dirichlet Allocation [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 0, "context": "Fields proposed a similar idea for playlist using social tag and topic model [9] using Latent Dirichlet Allocation [1].", "startOffset": 115, "endOffset": 118}, {"referenceID": 1, "context": "Besides text, Bogdanov introduced music avatars, whose outlook - hair style, clothes, and accessories - describes the recommended music [2].", "startOffset": 136, "endOffset": 139}, {"referenceID": 9, "context": "Two types of RNN unit are widely used: Long Short-Term Memory (LSTM) unit [10] and Gated Recurrent Unit (GRU) [3].", "startOffset": 74, "endOffset": 78}, {"referenceID": 2, "context": "Two types of RNN unit are widely used: Long Short-Term Memory (LSTM) unit [10] and Gated Recurrent Unit (GRU) [3].", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "Seq2seq models can be used to machine translation, where a phrase in a language is summarised by an encoder RNN, which is followed by a decoder RNN to generate a phrase in another language [4].", "startOffset": 189, "endOffset": 192}, {"referenceID": 12, "context": "One successful example is word2vec algorithm, which is usually trained with large corpora in an unsupervised manner [13].", "startOffset": 116, "endOffset": 120}, {"referenceID": 11, "context": "\u2022ConvNets: Convolutional neural networks (ConvNets) have been extensively adopted in nearly every computer vision task and algorithm since the record-breaking performance of AlexNet [12].", "startOffset": 182, "endOffset": 186}, {"referenceID": 4, "context": "ConvNets also show state-ofthe-art results in many music information retrieval tasks including auto-tagging [5].", "startOffset": 108, "endOffset": 111}, {"referenceID": 5, "context": "The models are written in Keras and uploaded online 3 [6].", "startOffset": 54, "endOffset": 57}, {"referenceID": 4, "context": "For computing ta, a convolutional neural network that is trained to predict tags is used to output 50-dim vector for each track [5].", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "The word embedding were trained by word2vec algorithms and Google news dataset [13].", "startOffset": 79, "endOffset": 83}, {"referenceID": 4, "context": "com/keunwoochoi/music-auto_ tagging-keras, [5] 6 The dimensions can vary, we describe in details for better understanding.", "startOffset": 43, "endOffset": 46}, {"referenceID": 6, "context": "7 Because these word embeddings are distributed representations in a semantic vector space, average of the words can summarise a bag of words and was used as a baseline in sentence and paragraph representation [7].", "startOffset": 210, "endOffset": 213}, {"referenceID": 10, "context": "The learning rate is controlled by ADAM [11] with an objective function of 1-cosine proximity.", "startOffset": 40, "endOffset": 44}, {"referenceID": 13, "context": "A word2vec model that is trained with music corpora can be used to reduce the embedding dimension [14].", "startOffset": 98, "endOffset": 102}], "year": 2016, "abstractText": "Descriptions are often provided along with recommendations to help users\u2019 discovery. Recommending automatically generated music playlists (e.g. personalised playlists) introduces the problem of generating descriptions. In this paper, we propose a method for generating music playlist descriptions, which is called as music captioning. In the proposed method, audio content analysis and natural language processing are adopted to utilise the information of each track.", "creator": "LaTeX with hyperref package"}}}