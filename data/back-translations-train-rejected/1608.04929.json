{"id": "1608.04929", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes", "abstract": "A recent goal in the Reinforcement Learning (RL) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon. For several RL problems in operation research and optimal control, the optimal policy of the underlying Markov Decision Process (MDP) is characterized by a known structure. The current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret. In this work, we develop new RL algorithms that exploit the structure of the optimal policy to minimize regret. Numerical experiments on MDPs with structured optimal policies show that our algorithms have better performance, are easy to implement, have a smaller run-time and require less number of random number generations.", "histories": [["v1", "Wed, 17 Aug 2016 11:35:32 GMT  (55kb,D)", "http://arxiv.org/abs/1608.04929v1", "An extended abstract appears in AAMAS 2016"]], "COMMENTS": "An extended abstract appears in AAMAS 2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["k j prabuchandran", "tejas bodas", "theja tulabandhula"], "accepted": false, "id": "1608.04929"}, "pdf": {"name": "1608.04929.pdf", "metadata": {"source": "CRF", "title": "Reinforcement Learning algorithms for regret minimization in structured Markov Decision Processes", "authors": ["Prabuchandran K J", "Tejas Bodas"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "These problems are modeled using the framework of MDPs, in which one goal is to find a policy that maximizes the long-term average reward. However, in an important subclass of these problems, one can characterize the structure of the optimal policy that this long-term average reward aims at. [3, 4, 6, 7] and communication systems [8] and the threshold policy on machine replacement are problems in which the optimal policy is characterized by a special structure. For such a class of problems, one can develop efficient algorithms to obtain the optimal policy by limiting the search for such structured policies. In the MDP problems outlined above, the optimal policy is characterized by a special structure."}, {"heading": "2 Preliminaries and Motivation", "text": "A finite MDP consists of the state space S = {1, 2,.,.,., action space A = {1, 2,..,.,.,., a probability transition matrix P = [pi, j (a)]] where i, j, S and A are true. Here, pi, j (a) denotes the probability of a transition to state j if the action a is chosen. The reward is characterized by the function R (i, j), j, S, a reward for the selection of a measure in state i and the transition to j. We assume that the reward function is limited by 1. a stationary deterministic policy that specifies the action (i) that must be chosen in the state. Let us denote the series of all deterministic stationary measures. Associated with each policy, we define the long-term average reward."}, {"heading": "3 Algorithms", "text": "In our algorithms, we look at strategies that have the same structure as the optimal policy. Let K specify the number of such strategies, which are known in advance."}, {"heading": "3.1 Algorithm 1: pUCB", "text": "In the pUCB algorithm, the amount of K strategies is selected together with a start state sstart = \u03b2 \u03b2 \u03b2 function using the reward (\u03b2 (t)) and {\u03b2 (t)} Tt = 1 are provided as input. At the beginning of the algorithm, a random policy is decided, which is followed in the sequence. After an episode, we begin to track the course of the total reward (see line 12 in algorithm 1 and line 11 in algorithm 2) and the number of time steps elapsed before one of the termination conditions are as follows: we end the episode when either the time steps in the episode are equal or we have reached the start state. Note that a key difference to MAB algorithms is that we act in episodes instead of time increments.We keep an estimate of the long-term average reward achieved within each policy."}, {"heading": "3.2 Algorithm 2: pThompson", "text": "This update is critical to ensure that the average distribution of the reward is similar to the pUCB algorithms described above and we will focus on the differences here. In terms of input, pThompson cannot include the sequence {\u03b2 (t)} Tt = 1 as one of its inputs.The initialization for pThompson is similar to pUCB's except that pThompson maintains a different set of internal estimates. Specifically, it maintains two estimates S (k) and F (k) for each individual measure. These two estimates parameterise a beta distribution that encodes our beliefs about the average cost reward of the policy pack. In each episode, we track the collected reward r and the number of rounds t before the closing conditions are met. The cumulative reward for episode r is added to the ongoing estimate S (k) of the current policy and an update of the form t \u2212 r."}, {"heading": "3.3 Algorithm 3: warmPSRL", "text": "For both algorithms, pUCB-Exded and pThompsonded Explored, we can simultaneously estimate the places in which we move: in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which places we move, in which we move, in which we move, in which places we hinder, in which we move, in which places we move, in which we move in which we move, in which places we move, in which we move in which places we move, in which we move in which places we move, in which we move in which we move, in which places we move in which we move, in which we move in which we move, in which we move in which we move, in which we move in which we move, in which places we move in which we move, in which we move in which we move, in which we move in which we move, in which we move, in which we move in which we move, in which we move, in which we move in which we move, in which we move in which we move, in which places we move in which we move, in which we move, in which we move, in which places we move in which we move, in which we move we move, in which we move, in which we move in which we move, in which we move, in which we move in which places we move, in which we move in which we move, in which places we move, in which we move, in which we move in which we are in which places we are in which we are in which places we are in which we are in which places we are in which we are"}, {"heading": "4 Numerical Results", "text": "Below, we describe two MDP settings that structure the appropriate optimal policies. Knowing this information makes it easier to search for policies that minimize remorse."}, {"heading": "4.1 The slow server problem", "text": "Consider a service system consisting of K servers and a single queue in which the incoming customers wait before using the service. Customers arrive according to a Poisson process at the \u03bb rate and each customer is obliged to receive the service on one of the parallel servers. Suppose that the service requirement of each incoming customer is a unit and that the time it takes the server k to serve a customer is a random variable that has an exponential distribution at the \u00b5k rate for k = 1., K. This service system reflects many practical scenarios (including call center operation, web server load distribution and others) This problem is well investigated [6, 23, 24] in the environment where K = 2, \u00b51 > \u00b52 and all parameters are known. Specifically, it has been shown that in an optimal access control policy, the faster server should always be occupied if there are customers waiting for the service in the queue, which means that the policy of both is slower."}, {"heading": "4.2 The machine replacement problem", "text": "The setting for this problem is adapted from [25] (Example 1.1.3, page 8) and has various applications in inventory management. Let us consider the problem of efficient operation of a machine. The machine may be in one of the n possible states (S = {1, 2,.., n}). Let us allow the condition 1 of the machine to be in perfect condition and any subsequent condition corresponding to the increasingly deteriorated condition of the machine. Let us allow the average cost g (i) to operate the machine for a period of time in which it is in condition i be low. Due to the increasing likelihood of failure, we can assume that g (1) \u2264 g (2) \u2264 \u00b7 \u00b7 \u00b7 g (n). We can take two measures in each condition without having to operate the machine (C) or to perform maintenance (PM). Once maintenance has been performed, the machine is guaranteed to remain in state 1 for a one-time period. Thus, the cost we accept for maintenance is R + g (R for repair and g)."}, {"heading": "5 Concluding Remarks", "text": "We have built on the proven, easy-to-use UCB and Thompson sampling algorithms to provide competitive remorse minimizing RL algorithms (novel modifications to estimate the long-term average reward \u03c1 (k) and the number of rounds each policy \u03c0k must apply).Such direct use of structural information is not readily possible with current state-of-the-art algorithms (PSRL and UCRL).The presented algorithms are simple, competitive, and many times better than modern methods for cumulative remorse minimization in RL settings, and have significant advantages in terms of computation and sampling costs."}], "references": [{"title": "Puterman, Markov Decision Processes: discrete stochastic dynamic programming", "author": ["L. M"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Reinforcement learning methods for continuous-time Markov decision problems", "author": ["S.J. Duff"], "venue": "Advances in Neural Information Processing Systems 7, vol. 7, p. 393, 1995.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1995}, {"title": "Structured Threshold Policies for Dynamic Sensor Scheduling: A Partially Observed Markov Decision Process Approach", "author": ["V. Krishnamurthy", "D. Djonin"], "venue": "IEEE Transactions on Signal Processing, vol. 55, no. 10, pp. 4938\u20134957, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Structural properties of the optimal resource allocation policy for single-queue systems", "author": ["R. Yang", "S. Bhulai", "R. van der Mei"], "venue": "Annals of Operations Research, vol. 202, no. 1, pp. 211\u2013233, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal control of a queuing system with two heterogeneous servers", "author": ["W. Lin", "P. Kumar"], "venue": "IEEE Trans. on Automatic Control, vol. 29, pp. 696\u2013703, 1984.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1984}, {"title": "Assigning customers to two parallel servers with resequencing", "author": ["N.R. Gogate", "S.S. Panwar"], "venue": "Communications Letters, IEEE, vol. 3, no. 4, pp. 119\u2013121, 1999.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Optimal monotone forwarding policies in delay tolerant mobile ad-hoc networks", "author": ["E. Altman", "T. Ba\u015far", "F. De Pellegrini"], "venue": "Performance Evaluation, vol. 67, no. 4, pp. 299\u2013317, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A discrete semi-Markov decision model to determine the optimal repair/replacement policy under general repairs", "author": ["C. Love", "Z.G. Zhang", "M. Zitron", "R. Guo"], "venue": "European journal of operational research, vol. 125, no. 2, pp. 398\u2013409, 2000.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, vol. 8, no. 3-4, pp. 279\u2013292, 1992.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Near-optimal reinforcement learning in polynomial time", "author": ["M. Kearns", "S. Singh"], "venue": "Machine Learning, vol. 49, no. 2-3, pp. 209\u2013232, 2002.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2002}, {"title": "R-max-a general polynomial time algorithm for near-optimal reinforcement learning", "author": ["R.I. Brafman", "M. Tennenholtz"], "venue": "The Journal of Machine Learning Research, vol. 3, pp. 213\u2013231, 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "Logarithmic online regret bounds for undiscounted reinforcement learning", "author": ["P. Ortner", "R. Auer"], "venue": "Advances in Neural Information Processing Systems, vol. 19, p. 49, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["T. Jaksch", "R. Ortner", "P. Auer"], "venue": "The Journal of Machine Learning Research, vol. 11, pp. 1563\u20131600, 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "more) efficient reinforcement learning via posterior sampling", "author": ["I. Osband", "D. Russo", "B. Van Roy"], "venue": "pp. 3003\u20133011, 2013. 12", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Thompson Sampling for Learning Parameterized Markov Decision Processes", "author": ["A. Gopalan", "S. Mannor"], "venue": "Proceedings of COnference on Learning Theory (COLT), vol. 40, 2015, pp. 1\u201338.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Regret bounds for reinforcement learning with policy advice", "author": ["M.G. Azar", "A. Lazaric", "E. Brunskill"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer, 2013, pp. 97\u2013112.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine learning, vol. 47, no. 2-3, pp. 235\u2013256, 2002.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2002}, {"title": "Analysis of thompson sampling for the multi-armed bandit problem", "author": ["S. Agrawal", "N. Goyal"], "venue": "Proceedings of the Twenty-Fifth Annual Conference on Learning Theory, 2012, pp. 39.1\u201339.26.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "NP-hardness of checking the unichain condition in average cost MDPs", "author": ["J.N. Tsitsiklis"], "venue": "Operations research letters, vol. 35, no. 3, pp. 319\u2013323, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Stochastic processes", "author": ["S.M. Ross"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1996}, {"title": "A note on optimal control of a queuing system with two heterogeneous servers,", "author": ["W. Lin", "P. Kumar"], "venue": "System and Control letters, vol. 4, pp. 131\u2013134, 1984.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1984}, {"title": "A simple proof of the optimality of a threshold policy in a two-server queueing system", "author": ["G. Koole"], "venue": "System and Control letters, vol. 26, pp. 301\u2013303, 1995.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Dynamic Programming and Optimal Control, 3rd ed", "author": ["D.P. Bertsekas"], "venue": "Athena Scientific,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 76, "endOffset": 82}, {"referenceID": 1, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 2, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 3, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 4, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 5, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 147, "endOffset": 162}, {"referenceID": 6, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 189, "endOffset": 192}, {"referenceID": 7, "context": "For example, the multi-period (s, S) policy in inventory control management [1, 2], the threshold and multi-threshold policies in queueing systems [3, 4, 5, 6, 7] and communication systems [8] and the threshold policy in machine replacement problems [9] are MDP problems where the optimal policy is characterized by a special structure.", "startOffset": 250, "endOffset": 253}, {"referenceID": 8, "context": "RL algorithms such as Q-learning [10] and SARSA [11] achieve this objective.", "startOffset": 33, "endOffset": 37}, {"referenceID": 9, "context": "RL algorithms such as Q-learning [10] and SARSA [11] achieve this objective.", "startOffset": 48, "endOffset": 52}, {"referenceID": 10, "context": "EXP3 [12] and Rmax [13] algorithms provide such PAC guarantees while learning the optimal behaviour.", "startOffset": 5, "endOffset": 9}, {"referenceID": 11, "context": "EXP3 [12] and Rmax [13] algorithms provide such PAC guarantees while learning the optimal behaviour.", "startOffset": 19, "endOffset": 23}, {"referenceID": 12, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 81, "endOffset": 89}, {"referenceID": 13, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 81, "endOffset": 89}, {"referenceID": 14, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 96, "endOffset": 100}, {"referenceID": 15, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 116, "endOffset": 120}, {"referenceID": 16, "context": "Algorithms that pursue regret minimization in a finite time horizon are the UCRL [14, 15], PSRL [16], Thompson PSRL [17] and the RLPA [18] algorithms.", "startOffset": 134, "endOffset": 138}, {"referenceID": 13, "context": "Further, in case of UCRL [15] and PSRL [16], certain explored policies chosen by the algorithm need not satisfy the structural property of the optimal policy.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Further, in case of UCRL [15] and PSRL [16], certain explored policies chosen by the algorithm need not satisfy the structural property of the optimal policy.", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "As is the case with UCRL and PSRL, our regret minimization algorithms are also inspired from the regret minimization algorithms of the multi-arm bandit (MAB) problem ([19], [20]).", "startOffset": 167, "endOffset": 171}, {"referenceID": 18, "context": "As is the case with UCRL and PSRL, our regret minimization algorithms are also inspired from the regret minimization algorithms of the multi-arm bandit (MAB) problem ([19], [20]).", "startOffset": 173, "endOffset": 177}, {"referenceID": 17, "context": "As the names suggest, our algorithms are conceptually inspired by the UCB [19] and Thompson sampling [20] algorithms for the MAB problem.", "startOffset": 74, "endOffset": 78}, {"referenceID": 18, "context": "As the names suggest, our algorithms are conceptually inspired by the UCB [19] and Thompson sampling [20] algorithms for the MAB problem.", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "The state of the art PSRL algorithm is based on Thompson sampling [20] and maintains a belief on the transition probability and the reward matrix.", "startOffset": 66, "endOffset": 70}, {"referenceID": 16, "context": "This is a nontrivial technicality that some of the previous algorithms such as the RLPA [18] overlooked by assuming episodes of arbitrary random duration.", "startOffset": 88, "endOffset": 92}, {"referenceID": 19, "context": "A sufficient condition for this to happen (and also assumed in this paper) is that the MDP satisfies the unichain condition [21].", "startOffset": 124, "endOffset": 128}, {"referenceID": 20, "context": "The Renewal-Reward theorem ([22], Chapter 3, Theorem 3.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "If \u03b2(t) is set to a constant (typically value 1), then the decision rule is the same as UCB1 of [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "Note that our updates also rely on conjugacy properties [20] (similar to Beta-Binomial conjugacy used in Thompson sampling).", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Further, PSRL as described in [16] needs to reset to the starting state after the end of the each episode (we take this into account carefully in our experiments).", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "This problem is well studied [6, 23, 24] in the setting where K = 2, \u03bc1 > \u03bc2 and all parameters are known.", "startOffset": 29, "endOffset": 40}, {"referenceID": 21, "context": "This problem is well studied [6, 23, 24] in the setting where K = 2, \u03bc1 > \u03bc2 and all parameters are known.", "startOffset": 29, "endOffset": 40}, {"referenceID": 22, "context": "This problem is well studied [6, 23, 24] in the setting where K = 2, \u03bc1 > \u03bc2 and all parameters are known.", "startOffset": 29, "endOffset": 40}, {"referenceID": 14, "context": "On the other hand, for PSRL, we were able to do this by suitably modifying the Dirichlet distribution [16].", "startOffset": 102, "endOffset": 106}, {"referenceID": 23, "context": "The setting for this problem is adapted from [25] (Example 1.", "startOffset": 45, "endOffset": 49}], "year": 2016, "abstractText": "A recent goal in the Reinforcement Learning (RL) framework is to choose a sequence of actions or a policy to maximize the reward collected or minimize the regret incurred in a finite time horizon. For several RL problems in operation research and optimal control, the optimal policy of the underlying Markov Decision Process (MDP) is characterized by a known structure. The current state of the art algorithms do not utilize this known structure of the optimal policy while minimizing regret. In this work, we develop new RL algorithms that exploit the structure of the optimal policy to minimize regret. Numerical experiments on MDPs with structured optimal policies show that our algorithms have better performance, are easy to implement, have a smaller run-time and require less number of random number generations.", "creator": "LaTeX with hyperref package"}}}