{"id": "1608.04983", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Ensemble of Jointly Trained Deep Neural Network-Based Acoustic Models for Reverberant Speech Recognition", "abstract": "Distant speech recognition is a challenge, particularly due to the corruption of speech signals by reverberation caused by large distances between the speaker and microphone. In order to cope with a wide range of reverberations in real-world situations, we present novel approaches for acoustic modeling including an ensemble of deep neural networks (DNNs) and an ensemble of jointly trained DNNs. First, multiple DNNs are established, each of which corresponds to a different reverberation time 60 (RT60) in a setup step. Also, each model in the ensemble of DNN acoustic models is further jointly trained, including both feature mapping and acoustic modeling, where the feature mapping is designed for the dereverberation as a front-end. In a testing phase, the two most likely DNNs are chosen from the DNN ensemble using maximum a posteriori (MAP) probabilities, computed in an online fashion by using maximum likelihood (ML)-based blind RT60 estimation and then the posterior probability outputs from two DNNs are combined using the ML-based weights as a simple average. Extensive experiments demonstrate that the proposed approach leads to substantial improvements in speech recognition accuracy over the conventional DNN baseline systems under diverse reverberant conditions.", "histories": [["v1", "Wed, 17 Aug 2016 14:43:17 GMT  (2319kb,D)", "http://arxiv.org/abs/1608.04983v1", "9 pages, 8 figures, 1 table"]], "COMMENTS": "9 pages, 8 figures, 1 table", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jeehye lee", "myungin lee", "joon-hyuk chang"], "accepted": false, "id": "1608.04983"}, "pdf": {"name": "1608.04983.pdf", "metadata": {"source": "CRF", "title": "Ensemble of Jointly Trained Deep Neural Network-Based Acoustic Models for Reverberant Speech Recognition", "authors": ["Jeehye Lee", "Myungin Lee"], "emails": ["jchang@hanyang.ac.kr)."], "sections": [{"heading": null, "text": "In fact, most of them are able to go to another world, where they can go to another world, where they can go to another world, where they can go to another world."}, {"heading": "II. REVIEW OF RELATED WORK", "text": "We will start with a brief description of related work in order to provide a sufficient background for understanding our approach. In the ensemble model, we will first discuss the design of the ensemble classifier. In the second subsection, the DNN joint training will be introduced."}, {"heading": "A. Neural network ensemble", "text": "Ensemble classification is known to be an effective approach to improving recognition accuracy [21] - [23] for which individual models are designed independently, and the deciphering of word hypotheses from multiple models is combined to evaluate spoken frames. In a recent study, several sets of data were generated by standardized DNA characteristics using beamforming and speech enhancement techniques, and other language-related characteristics and other auxiliary characteristics are also included [23]. An acoustic model is used by each dataset that builds the acoustic model ensemble as shown in Figure 1. Then, the EAM outputs are combined via a simple rear strategy in which the output probability for the triphonic HMM state k is given as an average of the rear probabilities p (kn | x) of the nth-acoustic model (n = 1, 2, N)."}, {"heading": "III. PROPOSED ENSEMBLE JOINT ACOUSTIC MODEL FOR REVERBERANT SPEECH RECOGNITION", "text": "In this section, we present the proposed algorithm for the DNN ensemble model and the ensemble of jointly trained DNN models, followed by an introduction to how to combine the acoustic ensemble model for which the blind estimation of RT60 is described."}, {"heading": "A. Ensemble of DNN models", "text": "In short, we design seven different classes of neural networks for acoustic modeling, each of which quantifies a unique reverberating condition. After the filter bank function, the characteristics are divided into several training sets, for which the reverberating condition is divided into seven points, from RT60 = 0.3 s to RT60 = 0.9 s, in steps of 0.1 s, each of which is determined separately in the sequence of DNN-based averaging techniques using the fine-tuning techniques described in the previous section. As a result, the EAM consists of seven different DNNs, as shown in Figure 3. In a test step, the downstream averaging technique is applied to combine the results of the DNN ensemble. Assuming that N acoustic models are generated from multiple data sets, the initial probability is most closely defined by pn = p (x), where k denotes the states of the Mpostel-AM with probability."}, {"heading": "B. Ensemble of jointly trained DNN models", "text": "In this subsection, we present the jointly trained DNN for the ensemble model, using both feature mapping and acoustic modeling. Thus, in this approach, the EJAM deals with reverberating speech at seven RT60 points, as in the previous EAM, which can be described as merging clean language with the room impulse response (RIR) at each RT60 point. EJAM is used as an input for feature mapping, for which the reverberating filter bank functions are converted into the goal of clean filter bank functions. Thus, the design of the feature mapping rule is considered a system identification problem, with a series of input and corresponding output function vector sequences. Estimating the DNN model parameters relevant to feature mapping is performed on the basis of three hidden layers with 2048 units in each shift, reducing the MSE function between the input and output range."}, {"heading": "IV. EXPERIMENTS", "text": "This section describes the performance evaluation of the proposed EAM and EJAM for remote voice recognition. To evaluate the proposed method, we present a series of experiments conducted in different resonant environments. Furthermore, the proposed method was compared with a conventional single DNN acoustic model and a single DNN acoustic model corresponding to the topological complexity of the EAM [30]."}, {"heading": "A. Experimental Setup", "text": "The proposed method was evaluated on the basis of TIMIT DB [31], which was first applied to the RIR generator [32] to simulate reverberant environments. As shown in Fig. 6, we created a simulated room with a size of 5 x 3 x 2.5 m3 and different reflection coefficients to generate specific RT60 conditions; the distance between microphone and source was set to 50 cm to avoid near-call scenarios; to simulate the system, reverberation times were increased from 0.3 s to 0.9 s, corresponding to specific RT60 conditions; and over time, 3696 expressions from TIMIT were used to form the training scenario in each RT60 state. Consequently, the number of expressions used for training was 3320 x 7 = 23240, a number that was also used in background models; the development group included 376 expressions mixed with seven RT60 seconds for cross-validation."}, {"heading": "B. Speech recognition results of DNN-based ensemble models", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "V. CONCLUSIONS", "text": "To the best of our knowledge, this study is the first to use an ensemble model for speech recognition under reverberating conditions. Initially, the ensemble structure was constructed, with each RT60 serving as a single DNN-based acoustic model in a specific area. Subsequently, the two most likely DNNs from the ensemble model are selected based on the ML blind estimation for RT60. EAM is further improved through joint training of the acoustic and feature models that serve as dereverberation. Experimental results showed that the DNN-based ensemble structure for acoustic modeling was superior to the single DNN-based background model. In particular, the blind ML estimation for RT60 was successfully responsible for selecting the most likely DNNs in the ensemble model."}], "references": [{"title": "Packet loss concealment based on deep neural networks for digital speech transmission", "author": ["B.-K. Lee", "J.-H. Chang"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 2, pp. 378-387, Feb. 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "EVAM: An eigenvector-based algorithm for multichannel blind deconvolution of input colored signals", "author": ["M.I. Gurelli", "C.L. Nikias"], "venue": "IEEE Trans. Signal Process., vol. 43, no. 1, pp. 134-149, Jan. 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "A two-stage algorithm for one-microphoe reverberant speech enhancement", "author": ["M. Wu", "D. Wang"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 14, no. 3, pp. 774-784, May. 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Supression of late reverberation effect on speech signal using long-term multiple-step linear prediction", "author": ["K. Kinoshita", "M. Delcroix", "T. Nakatani", "M. Mioshi"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 17, no. 4, pp. 534-545, May. 2009.  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING  8 (a) ML (RT60 = 0.61 s)  (b) ML (RT60 = 0.63 s) (c) ML (RT60 = 0.67 s) Fig. 8. An example of the ML of the estimated RT", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Single- and multi-microphone speech dereverberation using spectral enhancement", "author": ["E.A.P. Habets"], "venue": "Ph.D. dissertation, Technische Univ. Eindhoven, Eindhoven, The Netherlands, 2007.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "On the use of lime dereverberation algorithm in an acoustic environment with a noise source", "author": ["M. Delcroix", "T. Hikichi", "M. Miyoshi"], "venue": "Proc. ICASSP, 2006, pp. 825-828.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "Cepstral channel normalization techniques for HMM-based speaker verification", "author": ["A. Rosenberg", "C.-H. Lee", "F. Soong"], "venue": "Proc. ICSLP, 1994, pp. 1835-1838.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1994}, {"title": "Environmentally robust ASR front-end for deep neural network acoustic models", "author": ["T. yoshioka", "M.J.F. Gales"], "venue": "Comput. Speech Language, vol. 31, no. 1, pp. 65-86, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural network-based bottleneck feature and denoising autoencoder-based dereverberation for distant-talking speaker identification", "author": ["Z. Zhang", "L. Wang", "A. Kai", "T. Yamada", "W. Li", "M. Iwahashi"], "venue": "EURASIP J. Audio, Speech, Music. Process., pp. 1-13, May. 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech dereverberation for enhancement and recognition using dynamic features constrained deep neural networks and feature adaptation", "author": ["X. Xiao", "S. Zhao", "D. Nguyen", "X. Zhong", "D.L. Jones", "E. Chng", "H. Li"], "venue": "EURASIP J. Advances in Signal Process., pp. 1-18, Dec. 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring deep neural networks and deep autoencoders in reverberant speech recognition", "author": ["M Mimura", "S Sakai", "T Kawahara"], "venue": "Proc. HSCMA, 2014, pp. 197-201.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning spectral mapping for speech dereverberation and denoising", "author": ["K. Han", "Y. Wang", "D. Wang", "W.S. Woods", "I. Merks", "T. Zhang"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 23, no. 6, pp. 982-992, Jun. 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint training of front-end and back-end deep neural networks for robust speech recognition", "author": ["T. Gao", "J. Du", "L.-R. Dai", "C.-H. Lee"], "venue": "Proc. ICASSP, 2015, pp. 4375-4379.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models", "author": ["C.J. Legetter", "P.C. Woodland"], "venue": "Comput. Speech Language, vol. 9, no. 2, pp. 171-185, 1995.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1995}, {"title": "Maximum a posteriori estimation for mul-  tivariate Gaussian mixture observation of Markov chains", "author": ["J. Gauvain", "C.-H. Lee"], "venue": "IEEE Trans. Speech Audio Process., vol. 2, no. 2, pp. 291-298, 1994.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1994}, {"title": "A universal VAD based on jointly trained deep neural networks", "author": ["Q. Wang", "J. Du", "X. Bao", "Z.-R. Wang", "L.-R. Dai", "C.-H. Lee"], "venue": "Proc. Interspeech, 2015, pp. 2282-2286.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving robustness of deep neural network acoustic models via speech separation and joint adaptive training", "author": ["A. Narayann", "D. Wang"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 23, no. 1, pp. 92-101, Jan. 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Random forests of phonetic decision trees for acoustic modeling in conversational speech recognition", "author": ["J. Xue", "Y. Zhao"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 3, pp. 519-528, Mar. 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Ensemble methods in machine learning", "author": ["T.G. Dietterich"], "venue": "Proc. MCS, 2000, pp. 1-15.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2000}, {"title": "Building an Ensemble of CD-DNN- HMM acoustic model using random forests of phonetic decision trees", "author": ["T. Zhao", "Y. Zhao", "X. Chen"], "venue": "Proc. ISCLP, 2014, pp. 98-102.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Building acoustic model ensembles by data sampling with enhanced trainings and features", "author": ["X. Chen", "Y. Zhao"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 21, no. 3, pp. 498-507, Mar. 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "An information fusion approach to recognizing microphone array speech in the CHIME-3 challenge based on a deep learning framework", "author": ["J. Du", "Q. Wang", "Y.-H. Tu", "X. Biao", "L.-R. Dai", "C.-H. Lee"], "venue": "Proc. ASRU, 2015, pp. 430-435.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Ensemble deep learning for speech recognition.", "author": ["L. Deng", "J.C. Platt"], "venue": "Proc. Interspeech,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Theoretical analysis of diversity in an ensemble of automatic speech recognition systems", "author": ["K. Audhkhasi", "A.M. Zavou", "P.G. Georgiou", "S.S. Narayanan"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 22, no. 3, pp. 711-726, Mar. 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Framewise speech-nonspeech classification by neural networks for voice activity detection with statistical noise supression", "author": ["Y. Obuchi"], "venue": "Proc. ICASSP, 2016, pp. 5715-5719.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Ensemble of deep neural networks using acoustic environment classification for statistical modelbased voice activity detection", "author": ["I. Hwang", "H.-M. Park", "J.-H. Chang"], "venue": "Comput. Speech Language, vol. 38, pp. 1-12, Jul. 2016.  IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING  9", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Blind estimation of reverberation time", "author": ["R. Ratnam", "D.L. Jones", "B.C. Wheeler", "W.D. OBrien", "C.R. Lansing", "A.S. Feng"], "venue": "J. Acoust. Soc. Amer., vol. 114, no. 5, pp. 2877-2892, Nov. 2003.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "An improved algorithm for blind reverberation time estimation", "author": ["H.W. Lollmann", "E. Yilmaz", "M. Jeub", "P. Vary"], "venue": "Proc. IWAENC, 2010, pp. 1-4.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "An investigation of deep neural networks for noise robust speech recognition", "author": ["M.L. Seltzer", "D. Yu", "Y. Wang"], "venue": "Proc. ICASSP, 2013, pp. 7398-7402.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "TIMIT acoustic phonetic continuous speech corpus", "author": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett", "N.L. Dahlgren"], "venue": "Proc. Linguistic Data Consortium, 1993.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1993}, {"title": "Room impulse response generator", "author": ["E.A.P. Habets"], "venue": "Tech. Rep., Technische Univ. Eindhoven, Eindhoven, The Netherlands, 2010.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz", "J. Silovsky", "G. Stemmer", "K. Vesely"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.", "startOffset": 110, "endOffset": 113}, {"referenceID": 3, "context": "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.", "startOffset": 136, "endOffset": 139}, {"referenceID": 4, "context": "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.", "startOffset": 140, "endOffset": 143}, {"referenceID": 5, "context": "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.", "startOffset": 169, "endOffset": 172}, {"referenceID": 6, "context": "This is done using front-end processing techniques such as packet loss concealment [1], linear filtering [2], [3], spectral enhancement [4]-[6], and feature enhancement [7], [8], which reflect long-term acoustic context.", "startOffset": 174, "endOffset": 177}, {"referenceID": 7, "context": "Recently, deep neural networks (DNNs) have been used for matching reverberant speech to its anechoic version [9]-[14].", "startOffset": 109, "endOffset": 112}, {"referenceID": 12, "context": "Recently, deep neural networks (DNNs) have been used for matching reverberant speech to its anechoic version [9]-[14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "This approach is termed the back-endbased speech recognition system, which includes a hidden Markov model (HMM) adaptation, such as the maximum likelihood linear regression (MLLR) [15] and the maximum a posteriori (MAP) adaptation [16].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "This approach is termed the back-endbased speech recognition system, which includes a hidden Markov model (HMM) adaptation, such as the maximum likelihood linear regression (MLLR) [15] and the maximum a posteriori (MAP) adaptation [16].", "startOffset": 231, "endOffset": 235}, {"referenceID": 12, "context": "More recently, neural network has been used to jointly train a single DNN for both feature mapping and acoustic modeling in noisy environments [14], [17], [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 15, "context": "More recently, neural network has been used to jointly train a single DNN for both feature mapping and acoustic modeling in noisy environments [14], [17], [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 16, "context": "More recently, neural network has been used to jointly train a single DNN for both feature mapping and acoustic modeling in noisy environments [14], [17], [18].", "startOffset": 155, "endOffset": 159}, {"referenceID": 17, "context": "neural network ensemble, which builds a set of separately trained neural networks and combines the sets to form the unified prediction model [19]-[27].", "startOffset": 141, "endOffset": 145}, {"referenceID": 25, "context": "neural network ensemble, which builds a set of separately trained neural networks and combines the sets to form the unified prediction model [19]-[27].", "startOffset": 146, "endOffset": 150}, {"referenceID": 26, "context": "As for the MAP criteria, maximum likelihood (ML) estimation is used for the blind estimation of RT60 [28], [29], which is based on a statistical model for representing sound decay under reverberant conditions.", "startOffset": 101, "endOffset": 105}, {"referenceID": 27, "context": "As for the MAP criteria, maximum likelihood (ML) estimation is used for the blind estimation of RT60 [28], [29], which is based on a statistical model for representing sound decay under reverberant conditions.", "startOffset": 107, "endOffset": 111}, {"referenceID": 12, "context": "In addition, inspired by the joint training technique [14], we propose jointly training each model in the ensemble for both feature mapping and acoustic modeling, which enables us to use the back-propagation algorithm up to the feature mapping layer.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "The ensemble classifier is known to be an effective approach for enhancing the recognition accuracy [21]-[23] for which individual models are independently designed, and the decoding word hypotheses of multiple models are combined to score the speech frames.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "The ensemble classifier is known to be an effective approach for enhancing the recognition accuracy [21]-[23] for which individual models are independently designed, and the decoding word hypotheses of multiple models are combined to score the speech frames.", "startOffset": 105, "endOffset": 109}, {"referenceID": 21, "context": "In a recent study, multiple datasets were generated through normalized noisy features by which beamforming and speech enhancement techniques are used, and additional speaker related features as well as other auxiliary features are also included [23].", "startOffset": 245, "endOffset": 249}, {"referenceID": 19, "context": "In averaging posterior probabilities, simple averaging methods [21], [23] and weighted averaging method [22] have been presented.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "In averaging posterior probabilities, simple averaging methods [21], [23] and weighted averaging method [22] have been presented.", "startOffset": 69, "endOffset": 73}, {"referenceID": 20, "context": "In averaging posterior probabilities, simple averaging methods [21], [23] and weighted averaging method [22] have been presented.", "startOffset": 104, "endOffset": 108}, {"referenceID": 21, "context": "For a more detailed discussion of the EAM, please refer to [23].", "startOffset": 59, "endOffset": 63}, {"referenceID": 27, "context": "The weights are determined by the MAP probability computation, which is given by the blind ML estimation of RT60 [29].", "startOffset": 113, "endOffset": 117}, {"referenceID": 27, "context": "Indeed, the diffuse tail of reverberation is instead mathematically modeled as a simplified noise decay curve [29]:", "startOffset": 110, "endOffset": 114}, {"referenceID": 28, "context": "Moreover, the proposed method was compared with a conventional single DNN acoustic model and a single DNN acoustic model, which set the topological complexity equivalent to that of EAM [30].", "startOffset": 185, "endOffset": 189}, {"referenceID": 29, "context": "The proposed method was evaluated on TIMIT DB [31], which was first applied to the RIR generator [32] in order to 0.", "startOffset": 46, "endOffset": 50}, {"referenceID": 30, "context": "The proposed method was evaluated on TIMIT DB [31], which was first applied to the RIR generator [32] in order to 0.", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "Training of HMM parameters and decoding for speech recognition was carried out using the Kaldi software [33].", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "We evaluated the performance of the proposed EAM algorithm in terms of the phone error rate (PER) compared to the single DNN background model (SBM) [30].", "startOffset": 148, "endOffset": 152}, {"referenceID": 27, "context": "8, and we adopted the parameters used in [29].", "startOffset": 41, "endOffset": 45}], "year": 2016, "abstractText": "Distant speech recognition is a challenge, particularly due to the corruption of speech signals by reverberation caused by large distances between the speaker and microphone. In order to cope with a wide range of reverberations in real-world situations, we present novel approaches for acoustic modeling including an ensemble of deep neural networks (DNNs) and an ensemble of jointly trained DNNs. First, multiple DNNs are established, each of which corresponds to a different reverberation time 60 (RT60) in a setup step. Also, each model in the ensemble of DNN acoustic models is further jointly trained, including both feature mapping and acoustic modeling, where the feature mapping is designed for the dereverberation as a front-end. In a testing phase, the two most likely DNNs are chosen from the DNN ensemble using maximum a posteriori (MAP) probabilities, computed in an online fashion by using maximum likelihood (ML)-based blind RT60 estimation and then the posterior probability outputs from two DNNs are combined using the ML-based weights as a simple average. Extensive experiments demonstrate that the proposed approach leads to substantial improvements in speech recognition accuracy over the conventional DNN baseline systems under diverse reverberant conditions.", "creator": "LaTeX with hyperref package"}}}