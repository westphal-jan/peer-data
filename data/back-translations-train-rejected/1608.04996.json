{"id": "1608.04996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Open Problem: Approximate Planning of POMDPs in the class of Memoryless Policies", "abstract": "Planning plays an important role in the broad class of decision theory. Planning has drawn much attention in recent work in the robotics and sequential decision making areas. Recently, Reinforcement Learning (RL), as an agent-environment interaction problem, has brought further attention to planning methods. Generally in RL, one can assume a generative model, e.g. graphical models, for the environment, and then the task for the RL agent is to learn the model parameters and find the optimal strategy based on these learnt parameters. Based on environment behavior, the agent can assume various types of generative models, e.g. Multi Armed Bandit for a static environment, or Markov Decision Process (MDP) for a dynamic environment. The advantage of these popular models is their simplicity, which results in tractable methods of learning the parameters and finding the optimal policy. The drawback of these models is again their simplicity: these models usually underfit and underestimate the actual environment behavior. For example, in robotics, the agent usually has noisy observations of the environment inner state and MDP is not a suitable model.", "histories": [["v1", "Wed, 17 Aug 2016 15:20:35 GMT  (586kb)", "http://arxiv.org/abs/1608.04996v1", "arXiv admin note: substantial text overlap witharXiv:1602.07764"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1602.07764", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["kamyar azizzadenesheli", "alessandro lazaric", "animashree anandkumar"], "accepted": false, "id": "1608.04996"}, "pdf": {"name": "1608.04996.pdf", "metadata": {"source": "CRF", "title": "Open Problem: Approximate Planning of POMDPs in the class of Memoryless Policies", "authors": ["Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar"], "emails": ["KAZIZZAD@UCI.EDU", "ALESSANDRO.LAZARIC@INRIA.FR", "A.ANANDKUMAR@UCI.EDU"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.04 996v 1 [cs.A I] 1 7More complex models such as the Partially Observable Markov Decision Process (POMDP) can compensate for this disadvantage. Adapting this model to the environment, where partial observation is made on the active substance, generally leads to dramatic performance improvements, sometimes unlimited improvements, compared to the MDP. In general, finding the optimal policy for the POMDP model is mathematically insoluble and completely non-convex, even for the class of memoryless policies. The open problem is to find a method to find an exact or approximately optimal stochastic memoryless policy for POMDP models."}, {"heading": "1. Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2. Formal Definition", "text": "A POMDP M is a tuple < X, Y, R, fT, fR, fO = \u00b7 indicator i = \u00b7, where X is a finite state space with cardinality (X | X, A is a finite action space with cardinality | A | = A, Y is a finite observation space with cardinality | Y | = Y, and R is a finite reward space with cardinality | R | = R and maximum reward rmax. Furthermore, fT denotes the transition density, so that fT (x, x, a) is the probability of transition to x (X, a), where fR is the reward density, so that fR (r, a) is the probability of receiving the reward in R according to the value of the indicator vector r, which gives the state-action state (x, a), and fO is the observation density x, so that fO (y) is the probability that we get a reward in R according to the value of the indicator we give."}, {"heading": "3. Related Work", "text": "It is shown that finding the exact optimal policy for POMDP is followed by the curse of dimensionality and the curse of history. People use the point-based value repetition Pineau et al. (2006) to reduce the complexity of planning. It is also common to use the heuristic search value repetition Smith and Simmons (2004) and also the policy tree with limited depth Kaelbling et al. (1998) to reduce planning complexity. For a limited horizon, the computational complexity of optimal policy grows exponentially with the horizon. For an infinite horizon, any vector of state distribution can be any point in the continued space of simple sub-space. This means that planning goes beyond the continuous space that is PSPACE-complete."}], "references": [{"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Peter Auer", "Thomas Jaksch", "Ronald Ortner"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Auer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2009}, {"title": "Reinforcement learning of pomdps using spectral methods", "author": ["Kamyar Azizzadenesheli", "Alessandro Lazaric", "Animashree Anandkumar"], "venue": "arXiv preprint arXiv:1602.07764,", "citeRegEx": "Azizzadenesheli et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Azizzadenesheli et al\\.", "year": 2016}, {"title": "Planning and acting in partially observable stochastic domains", "author": ["Leslie Pack Kaelbling", "Michael L Littman", "Anthony R Cassandra"], "venue": "Artificial intelligence,", "citeRegEx": "Kaelbling et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Kaelbling et al\\.", "year": 1998}, {"title": "Finding optimal memoryless policies of pomdps under the expected average reward criterion", "author": ["Yanjie Li", "Baoqun Yin", "Hongsheng Xi"], "venue": "European Journal of Operational Research,", "citeRegEx": "Li et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Li et al\\.", "year": 2011}, {"title": "Geometry and determinism of optimal stationary control in partially observable markov decision processes", "author": ["Guido Montufar", "Keyan Ghazi-Zahedi", "Nihat Ay"], "venue": "arXiv preprint arXiv:1503.07206,", "citeRegEx": "Montufar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2015}, {"title": "The complexity of markov decision processes", "author": ["Christos Papadimitriou", "John N. Tsitsiklis"], "venue": "Math. Oper. Res.,", "citeRegEx": "Papadimitriou and Tsitsiklis.,? \\Q1987\\E", "shortCiteRegEx": "Papadimitriou and Tsitsiklis.", "year": 1987}, {"title": "Anytime point-based approximations for large pomdps", "author": ["Joelle Pineau", "Geoffrey Gordon", "Sebastian Thrun"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pineau et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Pineau et al\\.", "year": 2006}, {"title": "Model-based bayesian reinforcement learning in partially observable domains", "author": ["P. Poupart", "N. Vlassis"], "venue": "In International Symposium on Artificial Intelligence and Mathematics (ISAIM),", "citeRegEx": "Poupart and Vlassis.,? \\Q2008\\E", "shortCiteRegEx": "Poupart and Vlassis.", "year": 2008}, {"title": "Bayes-adaptive pomdps", "author": ["Stephane Ross", "Brahim Chaib-draa", "Joelle Pineau"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Ross et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ross et al\\.", "year": 2007}, {"title": "Heuristic search value iteration for pomdps", "author": ["Trey Smith", "Reid Simmons"], "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "citeRegEx": "Smith and Simmons.,? \\Q2004\\E", "shortCiteRegEx": "Smith and Simmons.", "year": 2004}, {"title": "The optimal control of partially observable Markov processes", "author": ["E.J. Sondik"], "venue": "PhD thesis, Stanford University,", "citeRegEx": "Sondik.,? \\Q1971\\E", "shortCiteRegEx": "Sondik.", "year": 1971}, {"title": "The optimal control of partially observable markov processes over the infinite horizon: Discounted costs", "author": ["Edward J Sondik"], "venue": "Operations research,", "citeRegEx": "Sondik.,? \\Q1978\\E", "shortCiteRegEx": "Sondik.", "year": 1978}, {"title": "Introduction to reinforcement learning", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "On the computational complexity of stochastic controller optimization in pomdps", "author": ["Nikos Vlassis", "Michael L Littman", "David Barber"], "venue": "ACM Transactions on Computation Theory (TOCT),", "citeRegEx": "Vlassis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Vlassis et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 12, "context": "RL agents learn how to maximize longterm reward using a experience obtained by direct interaction with a stochastic environment (Bertsekas and Tsitsiklis, 1996; Sutton and Barto, 1998).", "startOffset": 128, "endOffset": 184}, {"referenceID": 10, "context": "In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model.", "startOffset": 81, "endOffset": 95}, {"referenceID": 5, "context": ", computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.", "startOffset": 85, "endOffset": 121}, {"referenceID": 0, "context": "A number of exploration\u2013exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.", "startOffset": 159, "endOffset": 178}, {"referenceID": 0, "context": "A number of exploration\u2013exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model. Many challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration\u2013exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time.", "startOffset": 159, "endOffset": 1190}, {"referenceID": 0, "context": "A number of exploration\u2013exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model. Many challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration\u2013exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time.", "startOffset": 159, "endOffset": 1221}, {"referenceID": 0, "context": "A number of exploration\u2013exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model. Many challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration\u2013exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time. The proposed algorithms are such that the Bayesian inference can be done at each step, a POMDP is sampled from the posterior and the corresponding optimal policy is executed. The regret bound and sample complexity are not provided. Recently, the learning POMDP model parameter and imposing trade off between exploration and estimation (elements 1 and 3), are done at Azizzadenesheli et al. (2016). They propose the theoretical guaranty on regret bound given the oracle memoryless policy.", "startOffset": 159, "endOffset": 1787}, {"referenceID": 0, "context": "A number of exploration\u2013exploitation strategies have been shown to have strong performance guarantees for MDPs, either in terms of regret or sample complexity Auer et al. (2009). However, the assumption of full observability of the state evolution is often violated in practice, and the agent may have only noisy observations of the true state of the environment (e.g., noisy sensors in robotics). In this case, it is more appropriate to use the partially-observable MDP (POMDP) (Sondik, 1971) model. Many challenges arise in designing RL algorithms for POMDPs. Unlike in MDPs, the estimation problem (element 1) involves identifying the parameters of a latent variable model (LVM). The planning problem (element 2), i.e., computing the optimal policy for a POMDP with known parameters, is PSPACE-complete (Papadimitriou and Tsitsiklis, 1987), and it requires solving an augmented MDP built on a continuous belief space (i.e., a distribution over the hidden state of the POMDP). Finally, integrating estimation and planning in an exploration\u2013exploitation strategy (element 3) with guarantees is non-trivial and no no-regret strategies are currently known. Previous works Ross et al. (2007) and Poupart and Vlassis (2008) present new active learning algorithms to estimate the belief state in a model-based Bayesian RL approach, where a distribution over possible MDPs is updated over time. The proposed algorithms are such that the Bayesian inference can be done at each step, a POMDP is sampled from the posterior and the corresponding optimal policy is executed. The regret bound and sample complexity are not provided. Recently, the learning POMDP model parameter and imposing trade off between exploration and estimation (elements 1 and 3), are done at Azizzadenesheli et al. (2016). They propose the theoretical guaranty on regret bound given the oracle memoryless policy. Therefore to close the learning, planing, and exploration-exploitation loop, the missing part, planning (element 2), is the remaining part. Therefore, planing is a problem of finding the optimal memoryless policy, under uncertainty, in the class of stochastic memoryless polices. The overview complexity of planing in POMDP domain is discussed in Kaelbling et al. (1998).", "startOffset": 159, "endOffset": 2249}, {"referenceID": 7, "context": "Related Work Planning on uncertainty in a dynamic internal process is studied for infinite horizon Sondik (1978). It is shown that, finding the exact optimal policy for POMDP is followed by the curse of dimensionality and the curse of history.", "startOffset": 99, "endOffset": 113}, {"referenceID": 5, "context": "People uses point-based value iteration Pineau et al. (2006) to reduce the complexity of the planning.", "startOffset": 40, "endOffset": 61}, {"referenceID": 5, "context": "People uses point-based value iteration Pineau et al. (2006) to reduce the complexity of the planning. It is also common to use heuristic search value iteration Smith and Simmons (2004) and also policy tree with limited depth Kaelbling et al.", "startOffset": 40, "endOffset": 186}, {"referenceID": 2, "context": "It is also common to use heuristic search value iteration Smith and Simmons (2004) and also policy tree with limited depth Kaelbling et al. (1998) to reduce the planning complexity.", "startOffset": 123, "endOffset": 147}, {"referenceID": 2, "context": "It is also common to use heuristic search value iteration Smith and Simmons (2004) and also policy tree with limited depth Kaelbling et al. (1998) to reduce the planning complexity. For a finite horizonBut the computation complexity of finding optimal policy grows exponentially by horizon. For an infinite horizon, each vector of state distribution can be any point in the continues space of the simplex subspace. This means the planning is over the continuous space which is PSPACE-complete. Sondik (1978) presented a method to partition the continuous space", "startOffset": 123, "endOffset": 508}, {"referenceID": 11, "context": "Although, it seems to be easier than belief based planning, it is still an NP \u2212 hard problem Vlassis et al. (2012). To breaking down this complexity, Li et al.", "startOffset": 93, "endOffset": 115}, {"referenceID": 3, "context": "To breaking down this complexity, Li et al. (2011) presented a novel method for finding the optimal policy in the class of deterministic memoryless policies.", "startOffset": 34, "endOffset": 51}, {"referenceID": 3, "context": "To breaking down this complexity, Li et al. (2011) presented a novel method for finding the optimal policy in the class of deterministic memoryless policies. Meanwhile, deterministic policies act poorly in the general case of POMDPs. The geometric representation of POMDP planning problem is shown in Montufar et al. (2015) and its geometric structure is well studied.", "startOffset": 34, "endOffset": 324}], "year": 2016, "abstractText": "Planning plays an important role in the broad class of decision theory. Planning has drawn much attention in recent work in the robotics and sequential decision making areas. Recently, Reinforcement Learning (RL), as an agent-environment interaction problem, has brought further attention to planning methods. Generally in RL, one can assume a generative model, e.g. graphical models, for the environment, and then the task for the RL agent is to learn the model parameters and find the optimal strategy based on these learnt parameters. Based on environment behavior, the agent can assume various types of generative models, e.g. Multi Armed Bandit for a static environment, or Markov Decision Process (MDP) for a dynamic environment. The advantage of these popular models is their simplicity, which results in tractable methods of learning the parameters and finding the optimal policy. The drawback of these models is again their simplicity: these models usually underfit and underestimate the actual environment behavior. For example, in robotics, the agent usually has noisy observations of the environment inner state and MDP is not a suitable model. More complex models like Partially Observable Markov Decision Process (POMDP) can compensate for this drawback. Fitting this model to the environment, where the partial observation is given to the agent, generally gives dramatic performance improvement, sometimes unbounded improvement, compared to MDP. In general, finding the optimal policy for the POMDP model is computationally intractable and fully non convex, even for the class of memoryless policies. The open problem is to come up with a method to find an exact or an approximate optimal stochastic memoryless policy for POMDP models.", "creator": "LaTeX with hyperref package"}}}