{"id": "1608.05014", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Path-based vs. Distributional Information in Recognizing Lexical Semantic Relations", "abstract": "Recognizing various semantic relations between terms is crucial for many NLP tasks. While path-based and distributional information sources are considered complementary, the strong results the latter showed on recent datasets suggested that the former's contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. We demonstrate that these two information sources are indeed complementary, and analyze the contributions of each source.", "histories": [["v1", "Wed, 17 Aug 2016 16:27:49 GMT  (144kb)", "http://arxiv.org/abs/1608.05014v1", null], ["v2", "Thu, 6 Oct 2016 09:46:36 GMT  (19kb)", "http://arxiv.org/abs/1608.05014v2", "4 pages"], ["v3", "Thu, 27 Oct 2016 15:27:21 GMT  (149kb)", "http://arxiv.org/abs/1608.05014v3", "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), in COLING 2016"], ["v4", "Wed, 2 Nov 2016 08:57:39 GMT  (151kb)", "http://arxiv.org/abs/1608.05014v4", "5 pages, accepted to the 5th Workshop on Cognitive Aspects of the Lexicon (CogALex-V), in COLING 2016"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["vered shwartz", "ido dagan"], "accepted": false, "id": "1608.05014"}, "pdf": {"name": "1608.05014.pdf", "metadata": {"source": "CRF", "title": "The Roles of Path-based and Distributional Information in Recognizing Lexical Semantic Relations", "authors": ["Vered Shwartz", "Ido Dagan"], "emails": ["vered1986@gmail.com", "dagan@cs.biu.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.05 014v 1 [cs.C L] 17 Aug 201 6Recognition of different semantic relationships between terms is crucial for many NLP tasks. While path-based and distribution-based information sources are considered complementary, the strong results the latter showed on recent datasets suggest that the former's contribution may have become obsolete. We follow the recent success of an integrated neural method for detecting hypernymia (Shwartz et al., 2016) and extend it to detection of multiple relationships. We show that these two sources of information do indeed complement each other, and analyze the contributions of each source."}, {"heading": "1 Introduction", "text": "Automated methods for detecting the lexical semantic relationship between terms are valuable for NLP applications, and several datasets have been developed to train and evaluate these methods (e.g. Baroni and Lenci, 2011; Santus et al., 2015). Two important sources of information have been used to detect lexical semantic relationships: path-based and distribution methods look at the common occurrence of the two terms in a particular pair in the corpus, where the dependency pathways linking the terms are typically used as traits (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013). Distribution methods are based on the disjunctural events of each term and have recently become popular by using word embedding (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term."}, {"heading": "2 Background: HypeNET", "text": "Recently, Shwartz et al. (2016) introduced HypeNET, a hypernymy recognition method based on the integration of the most powerful distribution method for the task with a novel neural path representation, improving on the most modern methods. In HypeNET, a term pair (x, y) is presented as a feature vector consisting of both distributional and path-based features: ~ vxy = [~ vwx, ~ vpaths (x, y), ~ vwy] (1), where ~ vwx and ~ vwy are word embedding vectors providing their distribution-based representation, and ~ vpaths (x, y) is an embedding vector representing all dependency paths linking x and y in the corpus. A binary classifier is formed on these vectors, resulting in c = softmax (W \u00b7 vxy), with hypernypus > my-Y [1] predicting these relationships."}, {"heading": "3 Classification Methods", "text": "We experiment with several methods: Path-based (PB) The path-based HypeNET model is a binary classifier trained solely on path vectors: ~ vpaths (x, y). We adapt this model to classify multiple semantic relationships by changing the network softmax output c to a distribution via k target relationships and classifying a pair to the highest score relation: r = argmaxic [i].Distributional We form an SVM classifier based on the concatenation of x and y word embeddings [~ vwx, ~ vwy] (Baroni et al., 2012) (DS).2 Prior work suggested that such a linear classifier is unable to capture interactions between x and y features, and that it instead learns separate properties of x and y (Levy et al., 2015), e.g. by identifying this animal as a prototype hyper."}, {"heading": "4 Datasets", "text": "We use four common semantic relationship datasets, detailed in Table 1: K & H + N ((Necs, Ulescu et al., 2015), an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015) and ROOT09 (Santus et al., 2016). All datasets extracted instances from semantic resources (Fellbaum, 1998; Speer and Havasi, 2013). BLESS contains the event and attribute relationships that combine a concept with a typical activity / property (e.g. (alligator, swimming) and (alligator, aquatic), with an additional random relationship for negative instances. BLESS contains the event and attribute relationships that combine a concept with a typical activity / property (e.g. alligator, swimming) and (alligator, aquatic)."}, {"heading": "5 Results and Analysis", "text": "Table 2 shows the performance of the different methods (\u00a7 3) on all datasets. Like Shwartz et al. (2016), we adjusted the hyperparameters of all methods to the validation set of each dataset and used Wikipedia as a corpus."}, {"heading": "5.1 Classification Architecture", "text": "The results show that LexNET is actually effective in the multi-class setting, outperforming every single method in all datasets and successfully integrating distribution- and path-based information due to its large number of relationships and small size. DSh significantly improves DS across all datasets, and the only dataset where performance is mediocre is EVALution, which seems to be inherently more difficult for all methods due to its large number of relationships and small size. DSh significantly improves DS across all datasets, and the hidden layer appears to allow interactions between x and y characteristics, which is particularly noticeable in the ROOT09 hypernymia ratio F1 score, which is down from 0.25 in DS to 0.45 in DSh.Nevertheless, we have not observed any similar behavior in LexNETh that works similarly or slightly worse than LexNET. It is possible that the contributions of the hidden layer and path-based source via the distribution signal are redundant. 4 It may also be that the greater number of parameters in NEWh prevents the optimal data sets from facilitating the conversion of large data sets."}, {"heading": "5.2 Analysis of Information Sources", "text": "It is true that it is not the relationship between x and y, but rather the characteristics of x and y, which exist in other countries. (Our analysis shows that this is due to the changing hypernym pairs, which at the distribution level understand the ability to memorize, the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the ability to memorize, the ability to memorize and the expression, the ability to memorize and the ability to memorize, the ability to memorize and the expression, the ability to memorize and the ability to memorize, the ability to memorize and the expression, the ability to memorize and the ability to memorize, the ability to memorize, the ability to memorize and the expression and the ability to memorize, the ability to memorize, the ability to memorize and the expression, the ability to memorize and the ability to memorize and the expression, the ability to memorize, the ability to memorize, the ability to memorize and the expression, the ability to memorize and the ability to memorize and the expression, the ability to memorize, and the ability to memorize, the ability to memorize, and the ability to memorize, the ability, the ability to memorize and the"}, {"heading": "5.3 Analysis of Semantic Relations", "text": "To analyze the contribution of each source of information for specific relationships, we have merged all the records into a single set of WordNet relationships in Table 1 and evaluated all the methods used to create this dataset. 6The confusion matrices (Figure 1) identify the relationships that are recognized by each source of information, and re-evaluate previous results (Mirkin et al., 2006). PB is unsurprisingly weak in detecting synonyms that do not tend to occur side by side. It has also performed poorly for most of these pairs that are classified as random. Using the analysis method of Shwartz et al. (2016), we found that co-hyponymy indicating paths includes many lists (e.g. X, banjo, mandolin, and Y), but most of these pairs that are classified as random suggests that most of the pairs in the dataset have never appeared in such lists. DSh has worked poorly on synonyms and are not connected to each other on synonyms."}, {"heading": "6 Conclusion", "text": "We have presented an adaptation to HypeNET (Shwartz et al., 2016) that classifies term pairs into one of several semantic relationships. Evaluation of common datasets shows that HypeNET is expandable to the multiclass setting and performs better than any single method. Although the distribution information source is dominant in most datasets, it consistently benefits from path-based information, especially when finer modeling of the intermediate relationship is required. Finally, the behavior of the different methods is sensitive to the distribution of instances in each dataset, emphasizing the need to develop large realistic datasets with a natural term distribution."}], "references": [{"title": "How we blessed distributional semantic evaluation", "author": ["Baroni", "Lenci2011] Marco Baroni", "Alessandro Lenci"], "venue": "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics,", "citeRegEx": "Baroni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2011}, {"title": "Entailment above the word level in distributional semantics", "author": ["Baroni et al.2012] Marco Baroni", "Raffaella Bernardi", "Ngoc-Quynh Do", "Chung-chieh Shan"], "venue": "In EACL,", "citeRegEx": "Baroni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2012}, {"title": "Learning distributed word representations for natural logic reasoning", "author": ["Christopher Potts", "Christopher D Manning"], "venue": null, "citeRegEx": "Bowman et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2014}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["Marti A Hearst"], "venue": "In ACL,", "citeRegEx": "Hearst.,? \\Q1992\\E", "shortCiteRegEx": "Hearst.", "year": 1992}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "A semi-supervised method to learn and construct taxonomies using the web", "author": ["Kozareva", "Hovy2010] Zornitsa Kozareva", "Eduard Hovy"], "venue": "In EMNLP,", "citeRegEx": "Kozareva et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kozareva et al\\.", "year": 2010}, {"title": "Do supervised distributional methods really learn lexical inference relations", "author": ["Levy et al.2015] Omer Levy", "Steffen Remus", "Chris Biemann", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Integrating pattern-based and distributional similarity methods for lexical entailment acquisition", "author": ["Ido Dagan", "Maayan Geffet"], "venue": "In COLING and ACL,", "citeRegEx": "Mirkin et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mirkin et al\\.", "year": 2006}, {"title": "Computing lexical contrast", "author": ["Bonnie J Dorr", "Graeme Hirst", "Peter D Turney"], "venue": null, "citeRegEx": "Mohammad et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mohammad et al\\.", "year": 2013}, {"title": "Patty: a taxonomy of relational patterns with semantic types", "author": ["Gerhard Weikum", "Fabian Suchanek"], "venue": "In EMNLP and CoNLL,", "citeRegEx": "Nakashole et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nakashole et al\\.", "year": 2012}, {"title": "Reading between the lines: Overcoming data sparsity for accurate classification of lexical relationships", "author": ["N\u00faria Bel", "Sara Mendes", "David Jurgens", "Roberto Navigli"], "venue": "Lexical and Computational Semantics", "citeRegEx": "Nec\u015fulescu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nec\u015fulescu et al\\.", "year": 2015}, {"title": "Adding semantics to data-driven paraphrasing", "author": ["Johan Bos", "Malvina Nissim", "Charley Beller", "Benjamin Van Durme", "Chris Callison-Burch"], "venue": null, "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Relation extraction with matrix factorization and universal schemas", "author": ["Limin Yao", "Andrew McCallum", "Benjamin M Marlin"], "venue": null, "citeRegEx": "Riedel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2013}, {"title": "Inclusive yet selective: Supervised distributional hypernymy detection", "author": ["Katrin Erk", "Gemma Boleda"], "venue": "In COLING,", "citeRegEx": "Roller et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2014}, {"title": "Unsupervised antonymsynonym discrimination in vector space", "author": ["Santus et al.2014] Enrico Santus", "Qin Lu", "Alessandro Lenci", "Churen Huang"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2014}, {"title": "Evalution 1.0: an evolving semantic dataset for training and evaluation of distributional semantic models", "author": ["Santus et al.2015] Enrico Santus", "Frances Yung", "Alessandro Lenci", "Chu-Ren Huang"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2015}, {"title": "Nine features in a random forest to learn taxonomical semantic relations", "author": ["Santus et al.2016] Enrico Santus", "Alessandro Lenci", "Tin-Shing Chiu", "Qin Lu", "Chu-Ren Huang"], "venue": null, "citeRegEx": "Santus et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Santus et al\\.", "year": 2016}, {"title": "Improving hypernymy detection with an integrated path-based and distributional method", "author": ["Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Shwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shwartz et al\\.", "year": 2016}, {"title": "Learning syntactic patterns for automatic hypernym discovery", "author": ["Snow et al.2004] Rion Snow", "Daniel Jurafsky", "Andrew Y Ng"], "venue": null, "citeRegEx": "Snow et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Snow et al\\.", "year": 2004}, {"title": "Conceptnet 5: A large semantic network for relational knowledge", "author": ["Speer", "Havasi2013] Robert Speer", "Catherine Havasi"], "venue": "In The Peoples Web Meets NLP,", "citeRegEx": "Speer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Speer et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations.", "startOffset": 84, "endOffset": 106}, {"referenceID": 17, "context": "Automated methods to recognize the lexical semantic relation between terms are valuable for NLP applications, and several datasets have been developed for training and evaluating such methods (e.g. Baroni and Lenci, 2011; Santus et al., 2015).", "startOffset": 192, "endOffset": 242}, {"referenceID": 3, "context": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013).", "startOffset": 179, "endOffset": 257}, {"referenceID": 20, "context": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013).", "startOffset": 179, "endOffset": 257}, {"referenceID": 10, "context": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013).", "startOffset": 179, "endOffset": 257}, {"referenceID": 14, "context": "Path-based methods consider the joint occurrences of the two terms in a given pair in the corpus, where the dependency paths that connect the terms are typically used as features (Hearst, 1992; Snow et al., 2004; Nakashole et al., 2012; Riedel et al., 2013).", "startOffset": 179, "endOffset": 257}, {"referenceID": 7, "context": "Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term.", "startOffset": 129, "endOffset": 176}, {"referenceID": 13, "context": "Distributional methods are based on the disjoint occurrences of each term and have recently become popular using word embeddings (Mikolov et al., 2013; Pennington et al., 2014), which provide a distributional representation for each term.", "startOffset": 129, "endOffset": 176}, {"referenceID": 1, "context": "These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al.", "startOffset": 87, "endOffset": 129}, {"referenceID": 15, "context": "These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al.", "startOffset": 87, "endOffset": 129}, {"referenceID": 18, "context": ", 2014), and consistently outperformed other methods (Santus et al., 2016; Nec\u015fulescu et al., 2015).", "startOffset": 53, "endOffset": 99}, {"referenceID": 11, "context": ", 2014), and consistently outperformed other methods (Santus et al., 2016; Nec\u015fulescu et al., 2015).", "startOffset": 53, "endOffset": 99}, {"referenceID": 0, "context": "These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al., 2016; Nec\u015fulescu et al., 2015). While these two sources have been considered complementary, recent results seemed to suggest that path-based methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, yielding notably improved results on a new dataset.", "startOffset": 88, "endOffset": 441}, {"referenceID": 0, "context": "These embedding-based methods were reported to perform well on several common datasets (Baroni et al., 2012; Roller et al., 2014), and consistently outperformed other methods (Santus et al., 2016; Nec\u015fulescu et al., 2015). While these two sources have been considered complementary, recent results seemed to suggest that path-based methods have no marginal contribution over the distributional ones. Recently, however, Shwartz et al. (2016) showed that a good path representation can provide substantial complementary information to the distributional signal in hypernymy detection, yielding notably improved results on a new dataset. In this paper, we investigate the extension of Shwartz et al.\u2019s (2016) method to recognize multiple semantic relations, and reexamine their findings over common datasets for the broader task.", "startOffset": 88, "endOffset": 706}, {"referenceID": 19, "context": "Recently, Shwartz et al. (2016) introduced HypeNET, a hypernymy detection method based on the integration of the best-performing distributional method for the task with a novel neural path representation, improving upon state-of-the-art methods.", "startOffset": 10, "endOffset": 32}, {"referenceID": 19, "context": "Shwartz et al. (2016) showed that this new path representation, when used on its own, outperforms prior path-based methods for hypernymy detection.", "startOffset": 0, "endOffset": 22}, {"referenceID": 1, "context": "Distributional We train an SVM classifier on the concatenation of x and y\u2019s word embeddings [~vwx , ~vwy ] (Baroni et al., 2012) (DS).", "startOffset": 107, "endOffset": 128}, {"referenceID": 6, "context": "2 Prior work suggested that such a linear classifier is incapable of capturing interactions between x and y\u2019s features, and that instead it learns separate properties of x and y (Levy et al., 2015), e.", "startOffset": 178, "endOffset": 197}, {"referenceID": 0, "context": "Distributional We train an SVM classifier on the concatenation of x and y\u2019s word embeddings [~vwx , ~vwy ] (Baroni et al., 2012) (DS). 2 Prior work suggested that such a linear classifier is incapable of capturing interactions between x and y\u2019s features, and that instead it learns separate properties of x and y (Levy et al., 2015), e.g. by memorizing that animal is a prototypical hypernym. To examine the effect of non-linear expressive power on the model, we experiment with a neural network with a single hidden layer trained on [~vwx , ~vwy ] (DSh). This was previously done by Bowman et al. (2014), with promising results, but on a small artificial vocabulary.", "startOffset": 108, "endOffset": 605}, {"referenceID": 19, "context": "3 The technical details of our network are identical to Shwartz et al. (2016). We experimented also with difference ~vwx \u2212~vwy and some other classifiers, and chose concatenation as it yielded the best performance on the validation sets.", "startOffset": 56, "endOffset": 78}, {"referenceID": 11, "context": "We use four common semantic relation datasets, detailed in Table 1: K&H+N ((Nec\u015fulescu et al., 2015), an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al.", "startOffset": 75, "endOffset": 100}, {"referenceID": 17, "context": ", 2015), an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al., 2015), and ROOT09 (Santus et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 18, "context": ", 2015), and ROOT09 (Santus et al., 2016).", "startOffset": 20, "endOffset": 41}, {"referenceID": 6, "context": "To prevent a possible lexical memorization effect (Levy et al., 2015), Santus et al.", "startOffset": 50, "endOffset": 69}, {"referenceID": 10, "context": "We use four common semantic relation datasets, detailed in Table 1: K&H+N ((Nec\u015fulescu et al., 2015), an extension to Kozareva and Hovy (2010)), BLESS (Baroni and Lenci, 2011), EVALution (Santus et al.", "startOffset": 76, "endOffset": 143}, {"referenceID": 6, "context": "To prevent a possible lexical memorization effect (Levy et al., 2015), Santus et al. (2016) added negative switched hyponym-hypernym pairs (e.", "startOffset": 51, "endOffset": 92}, {"referenceID": 19, "context": "Like Shwartz et al. (2016), we tuned the hyper-parameters of all methods on the validation set of each dataset, and used Wikipedia as corpus.", "startOffset": 5, "endOffset": 27}, {"referenceID": 6, "context": "the memorization effect identified by Levy et al. (2015). This is mostly frequent in ROOT09 (405 pairs, which is 72%), where the switched hypernym pairs make the model less able to predict the relation according to the label frequency of a single term.", "startOffset": 38, "endOffset": 57}, {"referenceID": 8, "context": "6 The confusion matrices (Figure 1) identify the relations recognized by each information source, reassessing prior findings (Mirkin et al., 2006).", "startOffset": 125, "endOffset": 146}, {"referenceID": 8, "context": "6 The confusion matrices (Figure 1) identify the relations recognized by each information source, reassessing prior findings (Mirkin et al., 2006). PB is unsurprisingly weak in recognizing synonyms, which do not tend to co-occur. It also performed badly on co-hyponyms and antonyms. Using the analysis procedure of Shwartz et al. (2016), we found that co-hyponymy indicating paths include many lists (e.", "startOffset": 126, "endOffset": 337}, {"referenceID": 16, "context": "It seems worthwhile to try improving the model with insights from prior work on these specific relations (Santus et al., 2014; Mohammad et al., 2013) or by using additional information sources, like multilingual data (Pavlick et al.", "startOffset": 105, "endOffset": 149}, {"referenceID": 9, "context": "It seems worthwhile to try improving the model with insights from prior work on these specific relations (Santus et al., 2014; Mohammad et al., 2013) or by using additional information sources, like multilingual data (Pavlick et al.", "startOffset": 105, "endOffset": 149}, {"referenceID": 12, "context": ", 2013) or by using additional information sources, like multilingual data (Pavlick et al., 2015).", "startOffset": 75, "endOffset": 97}, {"referenceID": 19, "context": "We presented an adaptation to HypeNET (Shwartz et al., 2016) that classifies term-pairs to one of multiple semantic relations.", "startOffset": 38, "endOffset": 60}], "year": 2017, "abstractText": "Recognizing various semantic relations between terms is crucial for many NLP tasks. While path-based and distributional information sources are considered complementary, the strong results the latter showed on recent datasets suggested that the former\u2019s contribution might have become obsolete. We follow the recent success of an integrated neural method for hypernymy detection (Shwartz et al., 2016) and extend it to recognize multiple relations. We demonstrate that these two information sources are indeed complementary, and analyze the contributions of each source.", "creator": "LaTeX with hyperref package"}}}