{"id": "1608.05046", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Aug-2016", "title": "Practical optimal experiment design with probabilistic programs", "abstract": "Scientists often run experiments to distinguish competing theories. This requires patience, rigor, and ingenuity - there is often a large space of possible experiments one could run. But we need not comb this space by hand - if we represent our theories as formal models and explicitly declare the space of experiments, we can automate the search for good experiments, looking for those with high expected information gain. Here, we present a general and principled approach to experiment design based on probabilistic programming languages (PPLs). PPLs offer a clean separation between declaring problems and solving them, which means that the scientist can automate experiment design by simply declaring her model and experiment spaces in the PPL without having to worry about the details of calculating information gain. We demonstrate our system in two case studies drawn from cognitive psychology, where we use it to design optimal experiments in the domains of sequence prediction and categorization. We find strong empirical validation that our automatically designed experiments were indeed optimal. We conclude by discussing a number of interesting questions for future research.", "histories": [["v1", "Wed, 17 Aug 2016 18:59:23 GMT  (200kb,D)", "http://arxiv.org/abs/1608.05046v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["long ouyang", "michael henry tessler", "daniel ly", "noah goodman"], "accepted": false, "id": "1608.05046"}, "pdf": {"name": "1608.05046.pdf", "metadata": {"source": "CRF", "title": "Practical optimal experiment design with probabilistic programs", "authors": ["Long Ouyang", "Michael Henry Tessler", "Daniel Ly", "Noah D. Goodman"], "emails": ["ngoodman}@stanford.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, the majority of people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to move, to dance, to dance, to move, to dance, to dance, to move, to move, to dance, to move, to dance, to dance, to dance, to dance"}, {"heading": "2 Experiment design framework", "text": "Let's start with a concrete example before giving formal details. Let's imagine that we are studying how people predict values for sequence data (e.g. flips of a possibly tricky coin); let's compare two models: mfair, in which people believe that the coin is unbiased; and mbias, in which people believe that the coin has a bias that is unknown (expressed as a uniform before the unit interval), but that can be learned from data. We have a uniform prediction in front of the models and we want to update this belief distribution by showing people four flips of the same coin and asking them to predict what will happen on the next flip. There are 16 possible experiments (all combinations of H and T for 4 flips) and 2n possible outcomes - predictions of heads or tails for each human participant. Each model is a probability distribution to {0, 1} n conditional on the experiment - it describes a prediction of how certain people would react after a flip."}, {"heading": "2.1 Writing models as probabilistic programs", "text": "We use the probabilistic programming language WebPPL (webppl.org), a small but feature-rich probabilistic programming language embedded in JavaScript [2]. However, WebPPL provides a number of primitive distributions (e.g. binomial) that support sample generation and calculation of the probability density of values from the domain. For example, we can test using examples (Binomial (4, 12) using examples (Binomial ({n: 4, p: 0.5})) to determine the intuitive probability of value 2 using this distribution (Binomial ({n: 4, p: 0.5}), 2), 2). We are often interested in subordinate inferences."}, {"heading": "2.2 Writing OED as a probabilistic program", "text": "Amazingly, after expressing the spaces of models, experiments, and answers as probabilistic programs, it is easy to express OED also as a probabilistic program (see listing 1). Equation 1 translates into about 20 lines of WebPPL code and expresses that OED is an inference problem. A rich language such as WebPPL is particularly suitable here, as we rely heavily on the ability to perform nested inferences. Also, writing OED as a probabilistic program gives us access to algorithms that are more complex than previous research has considered (e.g. mixtures of enumeration and HMC for experiment spaces that have continuous and discrete subspaces). Finally, it should be noted that we implement the search for the optimal experiment using inference. This is not strictly necessary - we could also replace the extreme inference function () with an optimization method (e.g., Search ().OED function = Sample, y.Sample, Sample)."}, {"heading": "3 Case study 1: Sequence prediction", "text": "Human judgments about sequences are surprisingly systematic and inconsistent in terms of equally likely outcomes - for example, we could safely assume that the next coin toss will be in the sequence HHTTHHTT H, while we could be unsure about the sequence THHTHT. There are many hypotheses that could be made about what underlies human intuitions about such sequences [3, 4, 5]. At this point, we look at three simple models of people's beliefs: (a) Fair Coin: People assume that the coin is fair, (b) Biased Coin: People believe that the coin has an unknown bias (i.e., the probability of an H result) that they can learn from data, (c) Markov Coin: People believe that the coin has a certain probability to switch between the range of H and T results, which is also evident from the data. As in our previous coin, we must look at the same layout and the next attempt at the four."}, {"heading": "3.1 Formalization", "text": "The model space M is {mfair, mbias, mmarkov}. At the moment, we assume that the experiment will contain data from a single participant, so that the experiment space X is the cartesian product {1} \u00b7 {H, T} 4, which represents the fixed sample size of 1 and the sequence space.3 Finally, Y is the set of answers {H, T} 1. In mfair, we model the participants so that they believe that the coin produces heads or tails with equal probability: var fairCoin = function (seq) {Infer (function () {return flip (0.5)})} Here is Flip (0.5) the abbreviation for sample (Bernoulli ({p: 0.5})). Note the type signature of this model - it takes an experiment as input and returns a distribution of possible results of this experiment. In mbias, people assume that the coin has an unknown preconception, they learn from Coinseq and use it to predict the next observations (Coinseq)."}, {"heading": "3.2 Predictions of optimal experiment design", "text": "In fact, it is such that it is a matter of a way in which people are able to put themselves in the world, in which they are able to put themselves in the world, and in which they are able to put themselves in the world, and in which they are able to put themselves in the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they, in which they are able to put themselves, in the world, in which they are able to put themselves, in the world, in which they are able to put themselves, in the world, in the world, in which they are able to put themselves, in which they are able to put themselves, in the world, in which they are able to put themselves, in which they are able"}, {"heading": "3.3 Empirical validation", "text": "We validated our system by collecting human judgments for all 16 experiments and comparing the expected information gain with the actual information gain from the empirical results. We randomly assigned 351 participants to one experiment (all 16 experiments were conducted by \u2265 20 unique participants), pressing a button to reveal the sequence of 4 flips one by one, and then predicting the next coin toss (either head or tail). For each experiment x and each result y, we calculated the expected information gain from conducting our empirical sample of participants 4 and compared it to the actual information gain, DKL (P (M | Y = y, X = x) VP (M)))), for the three model comparison scenarios. Figure 3 shows that the expected information gain is a reliable predictor for the empirical value of an experiment (minimum r = 0.857), suggesting that one could rely on the OED tool to automatically select good experiments for this study."}, {"heading": "4 Case study 2: Category learning", "text": "Here we are examining a more complex and realistic space of models and experiments. In particular, we are analyzing a classic paper on the psychology of categorizing Medin and Schaffer [6], which aimed to distinguish two competing models of category learning - the pattern model and the prototype model. Using intuition, Medin and Schaffer (MS) designed an experiment (often referred to as the \"5-4 experiment\") in which the models made different predictions and found that the results supported the pattern model. Subsequently, many other authors followed their example, replicated and tested this experiment to test other competing models. At this point, we ask: How good was the MS 5-4 experiment? Could they have conducted an experiment that would have distinguished the two models with less data?"}, {"heading": "4.1 Models", "text": "Both the specimen and the prototype model are classifiers that map input (objects represented as vectors of Boolean characteristics) to a probability distribution of the categorization response (designation: A or B. The specimen model assumes that humans store information about each instance of the category they have observed; therefore, categorizing an object is a function of the similarity of the object to all examples of category A versus the similarity of all examples of B. In contrast, the prototype model assumes that humans store for each category a measure of central tendency - a prototype. The categorization of an object is therefore a function of its similarity to the A prototype versus its similarity to the B prototype. For details and representation of these models in WebPPL, see the supplement."}, {"heading": "4.2 Experiments", "text": "Participants get to know the category structures first in a training phase, in which they learn a subset of the objects under supervision and then in a test phase. During the training, the participants see a subset of the presented objects individually and must label each object. At first, they can only guess the labels, but receive feedback, in order to finally learn the category assignments. After they have reached a learning criterion, they finish the test phase, in which they label all objects (training set and held test set) without feedback. MS used visual stimuli, which varied on 4 binary dimensions (color: red vs. green, shape: triangle vs. circle, size: small vs. large and counting: 1 vs. 2). For technical reasons, they considered only experiments, which have (1) linearly separable decision limits, (2) which contain 5 A's and 4 B's in the training set and (3) the modal A object 1111 and the model B object 0000. There are up to 33 mutations to meet these limitations."}, {"heading": "4.3 Predictions of optimal experimental design", "text": "Using the predictive prediction for p (y; x), we calculated the expected information gain for all 933 experiments and found that the best experiment (for an individual participant) had an expected information gain of 0.08 Nats, while the MS 5-4 experiment had an expected information gain of only 0.03 Nats. Thus, the optimal experiment should be 2.5 times more informative than the MS experiment. In fact, the MS experiment is in the lower third of all experiments (Fig. 4a). Why is the MS experiment ineffective? One reason for this is that Medin and Schaffer prioritized experiments that predict a qualitative categorization difference (i.e., if one model predicts that one object is an A while the other is a B. The experiment they developed actually predicts a qualitative difference for an object, but this difference has a small magnitude and comes at the expense of a small information gain from the remaining objects. The optimal experiment is better able to maximize the information by simultaneously contradicting the models."}, {"heading": "4.4 Empirical validation", "text": "Figure 4b shows that the optimal experiment we found for a single participant is actually better than the MS experiment (n = 1, blue greater than red).For n = 1, the mean actual information gain for the optimal experiment is 0.15, while for the MS experiment it is 0.026.This 5-fold difference in information is even greater than the 2.5-fold difference predicted by the expected information gain. Furthermore, by gradually introducing more data, we observe that both experiments achieve the maximum actual information gain, but the optimal experiment only needs 10 participants to reach this maximum, while the MS experiment requires about 30 participants. Thus, the optimal experiment provides the same amount of information for a third of the experimental cost."}, {"heading": "5 Related work", "text": "The basic intuition behind the OED - finding experiments that maximize some expected metrics for data collection - has been discovered independently in a number of areas, including physics [7], chemistry [8], biology [9, 10], psychology [11], statistics [1], and machine learning [12]. However, this work often implements the OED for relatively limited cases, specializing in certain model classes, and committing to a single inference technique. Thus, Liepe et al. [10], for example, have developed a method in systems biology to find experiments that optimize the information gain for parameters of biomolecular models (ODEs with Gaussian noise).Their information measurement (Shannon entropy) is similar to ours, but they focus on a narrow family of models and commit to a tailor-made inference technique (an ABC scheme based on SMC).Their information measurement (Shannon entropy) is similar to ours, but they focus on a customized inference technique (an ABC scheme based on Gaussian noise), which requires the use of the psychology & Pitt methodology [11] to express a specific function for designing the purpose."}, {"heading": "6 Conclusion", "text": "Our approach partially automates the design of experiments and seeks experiments that maximize beliefs about model distribution; with our approach, the scientist writes her hypotheses as probability programs, outlines a space for possible experiments, and transfers them to the OED for experiment selection; we emphasize that our work complements practitioners; it does not replace them; our tool eliminates the need to manually comb large spaces for good experiments; we hope that this will enable scientists and engineers to work on the more interesting problems - to develop empirical paradigms and to build models. Our approach suggests a number of interesting directions for future work; we raise the OED problem as a problem of inference, and this might suggest certain inference techniques. For example, if a particular reaction is quite unlikely (i.e. p (y), then it is acceptable to have a less precise estimate of the information gain for this answer."}], "references": [{"title": "On a measure of information provided by an experiment", "author": ["D.V. Lindley"], "venue": "Annals of Mathematical Statistics, vol. 27, no. 4, pp. 986\u20131005, 1956.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1956}, {"title": "A psychological interpretation of the results of the Zenith radio experiments in telepathy", "author": ["L.D. Goodfellow"], "venue": "Journal of Experimental Psychology, vol. 23, no. 6, pp. 601\u2013623, 1938.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1938}, {"title": "The perception of randomness", "author": ["R. Falk"], "venue": "International Conference for the Psychology of Mathematics Education, vol. 1, pp. 222\u2013229, 1981.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1981}, {"title": "From algorithmic to subjective randomness", "author": ["T.L. Griffiths", "J.B. Tenenbaum"], "venue": "Advances in Neural Information Processing Systems, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Context Theory of Classification Learning", "author": ["D.L. Medin", "M.M. Schaffer"], "venue": "Psychological Review, vol. 85, no. 3, pp. 207\u2013238, 1978.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1978}, {"title": "Optimal nonlinear Bayesian experimental design: an application to amplitude versus offset experiments", "author": ["J. van Den Berg", "A. Curtis"], "venue": "Geophysical Journal International, vol. 155, no. 2, pp. 411\u2013421, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Accelerated Bayesian experimental design for chemical kinetic models", "author": ["X. Huan"], "venue": "Master\u2019s thesis, Massachusetts Institute of Technology, 2010.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "A Bayesian approach to targeted experiment design", "author": ["J. Vanlier", "C.A. Tiemann", "P.A.J. Hilbers", "N.A.W. van Riel"], "venue": "Bioinformatics, vol. 28, pp. 1136\u20131142, Apr. 2012.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Maximizing the Information Content of Experiments in Systems Biology", "author": ["J. Liepe", "S. Filippi", "M. Komorowski", "M.P.H. Stumpf"], "venue": "PLoS Computational Biology, vol. 9, p. e1002888, Jan. 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal experimental design for model discrimination", "author": ["J.I. Myung", "M.A. Pitt"], "venue": "Psychological review, vol. 116, no. 3, pp. 499\u2013518, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Near-optimal bayesian active learning with noisy observations", "author": ["D. Golovin", "A. Krause", "D. Ray"], "venue": "Advances in Neural Information Processing, 2010. 9", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "The information-theoretic foundations of OED are fairly straightforward [1], but it has not enjoyed widespread use in practice.", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "There are many hypotheses one might have about what underlies human intuitions about such sequences [3, 4, 5].", "startOffset": 100, "endOffset": 109}, {"referenceID": 2, "context": "There are many hypotheses one might have about what underlies human intuitions about such sequences [3, 4, 5].", "startOffset": 100, "endOffset": 109}, {"referenceID": 3, "context": "There are many hypotheses one might have about what underlies human intuitions about such sequences [3, 4, 5].", "startOffset": 100, "endOffset": 109}, {"referenceID": 4, "context": "In particular, we analyze a classic paper on the psychology of categorization by Medin and Schaffer [6] that aimed to distinguish two competing models of category learning \u2013 the exemplar model and the prototype model.", "startOffset": 100, "endOffset": 103}, {"referenceID": 5, "context": "The basic intuition behind OED\u2014to find experiments that maximize some expected measure of informativeness\u2014has been independently discovered in a number of fields, including physics [7], chemistry [8], biology [9, 10], psychology [11], statistics [1], and machine learning [12].", "startOffset": 181, "endOffset": 184}, {"referenceID": 6, "context": "The basic intuition behind OED\u2014to find experiments that maximize some expected measure of informativeness\u2014has been independently discovered in a number of fields, including physics [7], chemistry [8], biology [9, 10], psychology [11], statistics [1], and machine learning [12].", "startOffset": 196, "endOffset": 199}, {"referenceID": 7, "context": "The basic intuition behind OED\u2014to find experiments that maximize some expected measure of informativeness\u2014has been independently discovered in a number of fields, including physics [7], chemistry [8], biology [9, 10], psychology [11], statistics [1], and machine learning [12].", "startOffset": 209, "endOffset": 216}, {"referenceID": 8, "context": "The basic intuition behind OED\u2014to find experiments that maximize some expected measure of informativeness\u2014has been independently discovered in a number of fields, including physics [7], chemistry [8], biology [9, 10], psychology [11], statistics [1], and machine learning [12].", "startOffset": 209, "endOffset": 216}, {"referenceID": 9, "context": "The basic intuition behind OED\u2014to find experiments that maximize some expected measure of informativeness\u2014has been independently discovered in a number of fields, including physics [7], chemistry [8], biology [9, 10], psychology [11], statistics [1], and machine learning [12].", "startOffset": 229, "endOffset": 233}, {"referenceID": 0, "context": "The basic intuition behind OED\u2014to find experiments that maximize some expected measure of informativeness\u2014has been independently discovered in a number of fields, including physics [7], chemistry [8], biology [9, 10], psychology [11], statistics [1], and machine learning [12].", "startOffset": 246, "endOffset": 249}, {"referenceID": 10, "context": "The basic intuition behind OED\u2014to find experiments that maximize some expected measure of informativeness\u2014has been independently discovered in a number of fields, including physics [7], chemistry [8], biology [9, 10], psychology [11], statistics [1], and machine learning [12].", "startOffset": 272, "endOffset": 276}, {"referenceID": 8, "context": "[10] devised a method for finding experiments that optimize information gain for parameters of biomolecular models (ODEs with Gaussian noise).", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "In psychology, Myung & Pitt [11] devised a general design optimization method but this method requires researchers to select their own utility function for the value of an experiment and implement inference on their own.", "startOffset": 28, "endOffset": 32}], "year": 2016, "abstractText": "Scientists often run experiments to distinguish competing theories. This requires patience, rigor, and ingenuity\u2014there is often a large space of possible experiments one could run. But we need not comb this space by hand\u2014if we represent our theories as formal models and explicitly declare the space of experiments, we can automate the search for good experiments, looking for those with high expected information gain. Here, we present a general and principled approach to experiment design based on probabilistic programming languages (PPLs). PPLs offer a clean separation between declaring problems and solving them, which means that the scientist can automate experiment design by simply declaring her model and experiment spaces in the PPL without having to worry about the details of calculating information gain. We demonstrate our system in two case studies drawn from cognitive psychology, where we use it to design optimal experiments in the domains of sequence prediction and categorization. We find strong empirical validation that our automatically designed experiments were indeed optimal. We conclude by discussing a number of interesting questions for future research.", "creator": "LaTeX with hyperref package"}}}