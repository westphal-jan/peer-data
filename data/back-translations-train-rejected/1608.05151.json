{"id": "1608.05151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2016", "title": "Effective Multi-step Temporal-Difference Learning for Non-Linear Function Approximation", "abstract": "Multi-step temporal-difference (TD) learning, where the update targets contain information from multiple time steps ahead, is one of the most popular forms of TD learning for linear function approximation. The reason is that multi-step methods often yield substantially better performance than their single-step counter-parts, due to a lower bias of the update targets. For non-linear function approximation, however, single-step methods appear to be the norm. Part of the reason could be that on many domains the popular multi-step methods TD($\\lambda$) and Sarsa($\\lambda$) do not perform well when combined with non-linear function approximation. In particular, they are very susceptible to divergence of value estimates. In this paper, we identify the reason behind this. Furthermore, based on our analysis, we propose a new multi-step TD method for non-linear function approximation that addresses this issue. We confirm the effectiveness of our method using two benchmark tasks with neural networks as function approximation.", "histories": [["v1", "Thu, 18 Aug 2016 01:21:27 GMT  (63kb,D)", "http://arxiv.org/abs/1608.05151v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["harm van seijen"], "accepted": false, "id": "1608.05151"}, "pdf": {"name": "1608.05151.pdf", "metadata": {"source": "CRF", "title": "Effective Multi-step Temporal-Difference Learning for Non-Linear Function Approximation", "authors": ["Harm van Seijen"], "emails": ["harm.vanseijen@maluuba.com"], "sections": [{"heading": "1 Introduction", "text": "As a result, TD learning goals play an important role (Sutton, 1988) and reinforcement learning (Sutton & Barto, 1998; Szepesv\u00e1ri, 2009).The core concept behind TD learning is to extract the value of one state (or state-pair of actions) from the value of another state (or state-pair of actions).However, a step update achieves the state that is extracted from a single step into the future.With multi-level update goals, bootstrapping occurs in relation to values of states that are further in the future.Control of which states bootstrapping takes place is important because it affects the fundamental trade-off between bias and variance of updates.The trade-off that produces the best performance from domain to domain, but for most domains, the best trade-off lies somewhere between a one-level upgrade target (high default but low variance) and a full-return update."}, {"heading": "2 Related Work", "text": "This work is related to real online learning of the time difference (van Seijen & Sutton, 2014; van Seijen et al., 2015).The non-linear online \u03bb-return algorithm presented in Section 4 is a direct extension of the linear online \u03bb-return algorithm that underlies the true online TD (\u03bb).In the linear case, the mathematically inefficient forward view algorithm can be rewritten into computationally efficient backward view equations, resulting in the true online TD (\u03bb) algorithm. Unfortunately, this is not possible in the non-linear case, since the derivation of the true online equations takes advantage of the fact that the gradient in relation to the value function is independent of the weight vector, which is not the case of the non-linear function approximation."}, {"heading": "3 Background", "text": "Our problem is that of a Markov Decision Process (MDP), which is a 5-fold prediction of the form < S, A, p, r, \u03b3 >, consisting of S, the group of all states; A, the group of all measures; p (s, s, a), the transition probability function, which gives for each state the probability of a transition to the state s, S and action the probability of a transition to the state s, S at the next step; r (s, a, s), the reward function, which gives the expected reward for a transition from (s, a) to s. \u03b3 is the discount factor, which indicates how future rewards are weighted in relation to the immediate reward. An MDP may contain final state, which terminates an episode. Mathematically, a final state can be interpreted as a state with a single action leading to a reward of 0 and a transition to itself.The return at the time t is defined as a discounted sum of the rewards observed."}, {"heading": "4 Analysis of TD(\u03bb)", "text": "This is a multi-level method that is not immediately obvious because its updated equations are different in form from (1), which makes it difficult to determine what the updated target is. TD (\u03bb) is a multi-level method that emerges from the fact that the weights calculated by TD (\u03bb) are similar to those calculated by another algorithm that has a well-defined multi-level updated target called the \"effect algorithm.\" While the traditional return algorithm is similar to TD, the effect algorithm is similar only at the end of an episode. (Sutton & Barto, 1998; Bertsekas & Tsitsiklis, 1996) we specify a more general version that is similar to TD."}, {"heading": "5 Forward TD(\u03bb)", "text": "This has the advantage that updates can be performed immediately, but also results in the calculation time per time step increasing over time. In this section, we present a mathematically efficient method that performs updates using a \"\u03bb return\" with a horizon that includes a fixed number of time steps in the future: G\u03bb | t + Kt with a certain number of time steps in the future: G \u2212 1 time steps do not occur updates. In other words, the effects 0 for 1 \u2264 t < K The weights of K through the duration of T \u2212 1 are defined as follows: The effects of using the updated target G\u03bb | t + Kt with fixed K is that no updates occur during the first K \u2212 1 time steps. < K: The weights of K through the duration of T \u2212 1 are defined as follows: The effects of K + K: = K \u2212 1 are the same."}, {"heading": "6 Empirical Comparisons", "text": "In our first experiment, we evaluate the performance of TD (\u03bb), forward TD (\u03bb) and the results of predicting episodes of prediction, the impact on the standard mountain-car task (Sutton & Barto, 1998).The state-space consists of the position and speed of the car, scaled to numbers within the range [-1, 1].The value of the function is comparable to a neural network that uses the two state variables as input, an initial variable representing the state-value, and a single hidden layer of 50 nodes in between. The back-propagation algorithm is used to obtain the derivation of the value function in terms of weights (similar to that of Tesauro, 1994).The evaluation policy is an almost optimal policy. All rewards are derived from a normal distribution with average -1 and standard deviation 2. We fix the performance = 0.01 and show the performance for different stages."}, {"heading": "7 Conclusions", "text": "Deviations from the general TD update rule make TD (\u03bb) susceptible to deviations from estimates and cause additional deviations that reduce performance. Although the \u03bb return algorithm accurately implements the general update rule, it is not a viable alternative as its computation time per step and memory requirements are much more expensive. To address this, we have introduced a new method called Forward TD (\u03bb), which accurately implements the general update rule (such as the \u03bb return algorithm), but is also very efficient (such as TD (\u03bb). Specifically, its computation time complexity is similar to that of TD (0). While Forward TD (\u03bb) performs its updates with a delay, we have shown empirically that the performance increase due to accurate compliance with the general update rule is greater than the performance reduction due to the update line (QE)."}, {"heading": "Acknowledgements", "text": "The author thanks Itamar Arel for the discussions that led to the development of Forward TD (\u03bb), which was supported in part by grants from Alberta Innovates - Technology Futures and the National Science and Engineering Research Council of Canada."}, {"heading": "A Proof TD(\u03bb) is Similar to the Online \u03bb-Return Algorithm", "text": "Theorem 1 Let us instead always use weights with double indices for the online indicators and update the individual weights with their own indicators. (...) Theorem 1 Let us always use weights with double indicators for the online indicators. (...) We assume that these weights are well defined and continuous everywhere and that we prove the theorems by showing that we do not apply the theory (...). (...) Then we cannot be approximated for all time steps by O (...) / (C + O (...)). (...) As an example, let us cite the theorems by showing that we are not approximated (...). (...)"}], "references": [{"title": "Neuronlike adaptive elements that can solve difficult learning control problems", "author": ["A.G. Barto", "R.S. Sutton", "C.W. Anderson"], "venue": "IEEE Transactions on Systems, Man and Cybernetics,", "citeRegEx": "Barto et al\\.,? \\Q1983\\E", "shortCiteRegEx": "Barto et al\\.", "year": 1983}, {"title": "Truncating temporal differences: On the efficient implementation of TD(\u03bb) for reinforcement learning", "author": ["P. Cichosz"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Cichosz,? \\Q1995\\E", "shortCiteRegEx": "Cichosz", "year": 1995}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "Kumaran", "H. King D", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Reinforcement learning with replacing eligibility traces", "author": ["S.P. Singh", "R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Singh and Sutton,? \\Q1996\\E", "shortCiteRegEx": "Singh and Sutton", "year": 1996}, {"title": "Learning to predict by the methods of temporal differences", "author": ["R.S. Sutton"], "venue": "Machine Learning,", "citeRegEx": "Sutton,? \\Q1988\\E", "shortCiteRegEx": "Sutton", "year": 1988}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto", "year": 1998}, {"title": "Algorithms for Reinforcement Learning", "author": ["C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "Szepesv\u00e1ri,? \\Q2009\\E", "shortCiteRegEx": "Szepesv\u00e1ri", "year": 2009}, {"title": "TD-Gammon, a self-teaching backgammon program, achieves master-level play", "author": ["G. Tesauro"], "venue": "Neural Computation,", "citeRegEx": "Tesauro,? \\Q1994\\E", "shortCiteRegEx": "Tesauro", "year": 1994}, {"title": "True online TD(\u03bb)", "author": ["H. van Seijen", "R.S. Sutton"], "venue": "In International Conference on Machine Learning (ICML),", "citeRegEx": "Seijen and Sutton,? \\Q2014\\E", "shortCiteRegEx": "Seijen and Sutton", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "1 Introduction Multi-step update targets play an important role in TD learning (Sutton, 1988) and reinforcement learning (Sutton & Barto, 1998; Szepesv\u00e1ri, 2009).", "startOffset": 79, "endOffset": 93}, {"referenceID": 6, "context": "1 Introduction Multi-step update targets play an important role in TD learning (Sutton, 1988) and reinforcement learning (Sutton & Barto, 1998; Szepesv\u00e1ri, 2009).", "startOffset": 121, "endOffset": 161}, {"referenceID": 1, "context": "Forward TD(\u03bb) is similar to a method introduced by Cichosz (1995). Specifically, Cichosz\u2019s method is based on the same update target as forward TD(\u03bb).", "startOffset": 51, "endOffset": 66}, {"referenceID": 2, "context": "That delaying updates results in better performance in this case is probably related to the reason that the DQN algorithm uses a separate target network that is updated in a delayed way (Mnih et al., 2015).", "startOffset": 186, "endOffset": 205}, {"referenceID": 0, "context": "Our second control domain is the cart-pole benchmark task, in which a pole has to be balanced upright on a cart for as long as possible (Barto et al., 1983).", "startOffset": 136, "endOffset": 156}], "year": 2016, "abstractText": "Multi-step temporal-difference (TD) learning, where the update targets contain information from multiple time steps ahead, is one of the most popular forms of TD learning for linear function approximation. The reason is that multi-step methods often yield substantially better performance than their single-step counter-parts, due to a lower bias of the update targets. For non-linear function approximation, however, single-step methods appear to be the norm. Part of the reason could be that on many domains the popular multi-step methods TD(\u03bb) and Sarsa(\u03bb) do not perform well when combined with non-linear function approximation. In particular, they are very susceptible to divergence of value estimates. In this paper, we identify the reason behind this. Furthermore, based on our analysis, we propose a new multi-step TD method for non-linear function approximation that addresses this issue. We confirm the effectiveness of our method using two benchmark tasks with neural networks as function approximation.", "creator": "LaTeX with hyperref package"}}}