{"id": "1608.05374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Aug-2016", "title": "DNN-based Speech Synthesis for Indian Languages from ASCII text", "abstract": "Text-to-Speech synthesis in Indian languages has a seen lot of progress over the decade partly due to the annual Blizzard challenges. These systems assume the text to be written in Devanagari or Dravidian scripts which are nearly phonemic orthography scripts. However, the most common form of computer interaction among Indians is ASCII written transliterated text. Such text is generally noisy with many variations in spelling for the same word. In this paper we evaluate three approaches to synthesize speech from such noisy ASCII text: a naive Uni-Grapheme approach, a Multi-Grapheme approach, and a supervised Grapheme-to-Phoneme (G2P) approach. These methods first convert the ASCII text to a phonetic script, and then learn a Deep Neural Network to synthesize speech from that. We train and test our models on Blizzard Challenge datasets that were transliterated to ASCII using crowdsourcing. Our experiments on Hindi, Tamil and Telugu demonstrate that our models generate speech of competetive quality from ASCII text compared to the speech synthesized from the native scripts. All the accompanying transliterated datasets are released for public access.", "histories": [["v1", "Thu, 18 Aug 2016 18:58:39 GMT  (155kb,D)", "http://arxiv.org/abs/1608.05374v1", "6 pages, 5 figures -- Accepted in 9th ISCA Speech Synthesis Workshop"]], "COMMENTS": "6 pages, 5 figures -- Accepted in 9th ISCA Speech Synthesis Workshop", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["srikanth ronanki", "siva reddy", "bajibabu bollepalli", "simon king"], "accepted": false, "id": "1608.05374"}, "pdf": {"name": "1608.05374.pdf", "metadata": {"source": "CRF", "title": "DNN-based Speech Synthesis for Indian Languages from ASCII text", "authors": ["Srikanth Ronanki", "Siva Reddy", "Bajibabu Bollepalli", "Simon King"], "emails": ["srikanth.ronanki@ed.ac.uk"], "sections": [{"heading": "1. Introduction", "text": "In fact, it is the case that most people are able to feel how they want, and that they are able to move around."}, {"heading": "2. Related work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Transliteration of Indian Languages", "text": "Many standard transliteration systems exist for Indian languages. Table 1 shows different transliterations for an example sentence, including CPS (Common Phone Set) [8] and IT3 [9], which are widely used by the language technology community, ITRANS3 [10] is used by publishers, and WX4 [11] by the Natural Language Processing (NLP) community. Although these fonts offer a unique conversion to Indian fonts due to their lack of legibility and the extra effort involved in learning how to use them, people still spell their words using pronunciation. Such transliteration is reflected in the Informal series of Table 1. The most common trend observed in literature is to treat transliterature as a machine translation and discriminatory ranking problem [12]. Our work aims to exploit the fact that translations are phonetically motivated and therefore treat transliterature as a conversion problem."}, {"heading": "2.2. Statistical Speech Synthesis", "text": "Most existing work in language synthesis for Indian languages uses unit selection [13] with syllable-like units [14, 15]. Re-3https: / / en.wikipedia.org / wiki / ITRANS 4https: / / en.wikipedia.org / wiki / WX _ notationar Xiv: 160 8.05 374v 1 [cs.C L] 18 August 201 6cently, based on the observation that Indian languages have many similarities in phonetics, a language-independent telephone set has been proposed, which has been used in the construction of statistical parametric (HMM-based) speech synthesis systems [8]. We use this common telephone set in one of our models. Our work is also based on the recent literature on unattended learning for text-language synthesis, which aims to reduce dependence on human knowledge and the manual effort to build language-specific resources [16, 17, 18]."}, {"heading": "3. Our Approach", "text": "Our speech synthesis pipeline consists of two steps: 1) Conversion of the input ASCII transcribed text into a phonetic script; 2) Learning a DNN-based speech synthesizer from the parallel phonetic text and audio signal."}, {"heading": "3.1. Converting ASCII text to Phonetic Script", "text": "We examine three different approaches, which differ in the degree of monitoring in the definition of a phoneme."}, {"heading": "3.1.1. Uni-Grapheme Model", "text": "In this approach, we assume that each ASCII graphite acts as a phoneme. We assume that the DNN will learn to assign these \"phonemes\" to speech sounds. We normalize the data in lowercase letters and remove all punctuation marks, ensuring that the phone set contains 26 letters and an additional / sil / phone to mark the beginning and end of the sentence."}, {"heading": "3.1.2. Multi-Grapheme Model", "text": "In this approach, in addition to the unigraphs, we also include some frequently occurring bi-graphs as \"phonemes.\" When we manually checked the 50 best bi-graphs, we found that the phonemes that indicate stop consonants such as / kh /, / ch /, / th /, / ph /, / bh / and long vowels such as / aa /, / ii /, / ee /, / oo /, / uu / and dipthongs such as / ai /, / au /, / ou / occur most frequently in all languages. 17 of these bi-graphs were selected as phonemes in addition to the above 27 unigraphs, giving a total of 44 phonemes."}, {"heading": "3.1.3. Grapheme-to-Phoneme (G2P) Model", "text": "In this model, we assume that the phoneme set is given in the three languages. We use the usual telephone set (CPS, [8]) to work with the languages of interest. We use deterministic converters to convert the native text into CPS phonetics [9, 23]. We then align the phonetic transcriptions with the ASCII transcriptions of Mechanical Turkers to create a pronunciation table. Table 2 shows the parallel data with the native text in the first column, the informal ASCII transcriptions in the second column, and the CPS phonetic transcriptions in the third column. In view of the pronunciation dictionary, we train a G2P converter [24] for each language separately with varying n-gram sequences. The corpus used for the training is described in Section 4.1. Figure 1 shows the telephone error rate of the G2P model with varying phone models reaching the Gramm6 in the three languages."}, {"heading": "3.2. DNN Speech Synthesizer", "text": "We use a DNN for learning to synthesize language from the phonetic strings obtained in the previous step. We use two independent DNNs - one for duration and the other for acoustic modeling. Let xi = [xi (1),..., xi (dx)] T and yi = [yi (1),..., yi (dy)] T be static DNN input and output characteristics, where dx and dy denote the dimensions of xi and yi, respectively T denotes transposition.Duration Model: For duration modeling, the input function includes binary characteristics (xp) derived from a subset of questions used by the decision tree clustering in the standard HTS synthesizer. Similar to [20, 21], frame-aligned data for DNN education is generated by forced alignment of the HMM system."}, {"heading": "4. Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Speech Databases", "text": "Our interesting languages are Hindi, Tamil and Telugu, all widely spoken Indian languages. We train and test using data from Blizzard Challenge 2015, which includes about four hours of speech and text for each language. The data set includes 1710 utterances for Hindi, 1462 utterances for Tamil and 2481 utterances for Telugu, using a single speaker per language. 92% of the data was used for training, 4% for development and 4% for testing."}, {"heading": "4.2. Annotation", "text": "Starting with the original transcriptions in native script, we asked crowd-sourced annotators for ASCII transcription using pronunciation as the main motivation for spelling. For Hindi and Tamil, we recruited paid workers via Mechanical Turk who were able to read and speak the language fluently (as reported); for Telugu, we had access to a trusted pool of native speakers. We put each sentence on words with spaces and punctuations as separators. Annotators were provided with a web interface that included a text box for each word, ensuring the transcription of each word in the input set. The total number of annotators for Telugu, Tamil and Hindi is 50, 66 and 82, respectively. We diversified train, dev and test splits by having different sets of annotators available for each split."}, {"heading": "4.3. Experimental Settings", "text": "We used the same DNN architectures (section 3.2) for both duration and acoustic modeling. The number of hidden layers was 6 with 1024 nodes each. As shown in Equation 4, the Tanh function was used as a hidden activation function, and a linear activation function was used at the output level. During the training, the L2 regularization was applied to weights with a tightening factor of 0.00001, the minichar size was 256 for the acoustic model and 64 for the duration model. In the first 10 epochs, the impulse was 0.3 with a fixed learning rate of 0.002. After 10 epochs, the impulse was increased to 0.9 and from then on, the learning rate in each epoch was halved. The learning rate of the top two layers was always half as in other layers. The learning rate was finely tuned in the duration models to achieve the best performance."}, {"heading": "4.4. Our Models", "text": "As described in Section 3.1, we train three different models for each language. The number of questions used in DNN varied from system to system. For the Uni-Grapheme model (referred to as UGM), the questions were based on the Qin-Phone identity, and other questions include suprasegmental characteristics such as syllable, word, phrase, and position characteristics. For the Multi-Grapheme model (referred to as MGM) and the Grapheme-ToPhoneme model (referred to as G2P), additional questions based on position and mode of articulation were added."}, {"heading": "4.5. Benchmark", "text": "As a benchmark, we use the DNN speech synthesizer, which is trained on the phonetic transcription of speech data by CPS. The aim is therefore to synthesize speech from ASCII text that comes as close as possible to this benchmark (BMK)."}, {"heading": "5. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Objective Evaluations", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1. Duration Model", "text": "To evaluate the DNN duration prediction, we calculated the Root Mean Square Correlation (RMSE) and Pearson correlation between reference and predicted duration, estimating the reference time from the forced alignment step in HTS. Tables 4 and 5 show the results based on the test data."}, {"heading": "G2P 0.818 0.564 0.657", "text": "Overall, the benchmark system performed better than other systems in all languages. Among the suggested approaches, G2P performed slightly better in terms of correlation than the other two, while RMSE performance was not consistent in all languages. One possible explanation for this is that G2P uses superior telephone rates defined manually, while UGM and MGM use unattended phones. Nevertheless, the proposed systems are not too far from the benchmark. Compared to Telugu, Hindi and Tamil have lower objective values. Punctuation marks were not retained in the corpus for these two languages, making pauses more difficult to predict. As a result, occasional pauses in acoustics were often adapted to non-stop phones and resulted in errors in reference times. These unpredictable strains inflated the objective measures without too much interfering with actual predictions. (Telugu, on the other hand, used ear pauses based on fixed-x acoustics)."}, {"heading": "5.1.2. Acoustic Model", "text": "We used the following four objective ratings to assess the performance of the proposed methods compared to the benchmark system. \u2022 MCD: Mel-Cepstral Distortion (MCD) to measure the accuracy of the MCC prediction power. \u2022 BAP: to measure the distortion of BAPs. \u2022 F0 RMSE: Root Mean Squared Error (RMSE) to measure the accuracy of the F0 prediction. The error value was calculated on a linear scale instead of the log scale used to model the F0 values. \u2022 V / UV: to measure the accuracy of the F0 prediction. In all of these metrics, a lower value gives the better performance. While the objective metrics do not directly reflect the perception quality, they are often useful for system tuning. Table 3 presents the results on test data. As expected, the benchmark model performs well on most metrics."}, {"heading": "5.2. Subjective Evaluations", "text": "This year, it's about half, but not half, \"he said."}, {"heading": "6. Applications", "text": "The transformation of graphs into phonemes described here enabled us to build Indic-search6, a search engine that helps end users use ASCII to search for pages written in Unicode. Textto language interfaces with ASCII input also allow users to type their own pronunciation rather than adapt to a particular notation."}, {"heading": "7. Conclusions", "text": "Our proposed approach first converts ASCII text into phonetic writing and then learns a DNN to synthesize speech from phonetic writing. We experimented with three approaches that vary in the degree of manual monitoring in defining phonemes. Our results show that the G2P model competes with manually defined phonemes with few assumptions. All data and examples used in the hearing tests are available online at: http: / / srikanthr.in / indic-speech-synthesis.6http: / / srikanthr.in / indic-searchAckledgements: Thanks to Nivedita Chennupati and Spandana Gella for their contribution to data collection with Amazon Mechanical Turk. Also thanks to Sivanada Achanta for evaluating the systems through hearing tests. We thank Gustav Henter for correcting the errors that remain with the authors."}, {"heading": "8. References", "text": "[1] A. N. S. Institute, \"7-bit american standard code for informationinterchange,\" ANSI X3, vol. 4, 1986. [2] U. Z. Ahmed, K. Bali, M. Choudhury, and S. VB, \"Challenges in design input method editors for indian lan-guages: The role of word-origin and context,\" in Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011). Chiang Mai, Thailand: Asian Federation of Natural Language Processing, November 2011, pp. 1-9. [Online]. Available: http: / / www.aclweb.org / anthology / W11-3501 [3]. R. Roy, M. Choudhury, P. Majumder, and K. Agarwal, and K. 2013 Track of the fire 2013 on transliterated search,. \""}], "references": [{"title": "7-bit american standard code for information interchange", "author": ["A.N.S. Institute"], "venue": "ANSI X3, vol. 4, 1986.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Challenges in designing input method editors for indian lan-guages: The role of word-origin and context", "author": ["U.Z. Ahmed", "K. Bali", "M. Choudhury", "S. VB"], "venue": "Proceedings of the Workshop on Advances in Text Input Methods (WTIM 2011). Chiang Mai, Thailand: Asian Federation of Natural Language Processing, November 2011, pp. 1\u20139. [Online]. Available: http://www.aclweb.org/anthology/W11-3501", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Overview of the fire 2013 track on transliterated search", "author": ["R.S. Roy", "M. Choudhury", "P. Majumder", "K. Agarwal"], "venue": "Proceedings of the 5th 2013 Forum on Information Retrieval Evaluation, ser. FIRE \u201913. New York, NY, USA: ACM, 2007, pp. 4:1\u20134:7. [Online]. Available: http://doi.acm.org/10.1145/ 2701336.2701636", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Query expansion for mixed-script information retrieval", "author": ["P. Gupta", "K. Bali", "R.E. Banchs", "M. Choudhury", "P. Rosso"], "venue": "Proceedings of the 37th International ACM SIGIR Conference on Research &#38; Development in Information Retrieval, ser. SIGIR \u201914. New York, NY, USA: ACM, 2014, pp. 677\u2013 686. [Online]. Available: http://doi.acm.org/10.1145/2600428. 2609622", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "POS tagging of English-Hindi code-mixed social media content", "author": ["Y. Vyas", "S. Gella", "J. Sharma", "K. Bali", "M. Choudhury"], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). Doha, Qatar: Association for Computational Linguistics, October 2014, pp. 974\u2013979. [Online]. Available: http://www.aclweb.org/anthology/ D14-1105", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "The blizzard challenge 2014", "author": ["K. Prahallad", "A. Vadapalli", "S. Kesiraju", "H. Murthy", "S. Lata", "T. Nagarajan", "M. Prasanna", "H. Patil", "A. Sao", "S. King"], "venue": "Proc. Blizzard Challenge workshop, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "The blizzard challenge 2013\u2013indian language task", "author": ["K. Prahallad", "A. Vadapalli", "N. Elluru", "G. Mantena", "B. Pulugundla", "P. Bhaskararao", "H. Murthy", "S. King", "V. Karaiskos", "A. Black"], "venue": "Proc. Blizzard Challenge Workshop, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "A common attribute based unified hts framework for speech synthesis in indian languages", "author": ["R. B", "S.L. Christina", "G.A. Rachel", "S. Solomi V", "M.K. Nandwana", "A. Prakash", "A.S. S", "R. Krishnan", "S.K. Prahalad", "K. Samudravijaya", "P. Vijayalakshmi", "T. Nagarajan", "H. Murthy"], "venue": "Proc. SSW, Barcelona, Spain, August 2013, pp. 311\u2013316.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A simple approach for building transliteration editors for indian languages", "author": ["P. Lavanya", "P. Kishore", "G.T. Madhavi"], "venue": "Journal of Zhejiang University Science A, vol. 6, no. 11, pp. 1354\u20131361, 2005.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Om: One tool for many (indian) languages", "author": ["G. Madhavi", "B. Mini", "N. Balakrishnan", "R. Raj"], "venue": "Journal of Zhejiang University Science A, vol. 6, no. 11, pp. 1348\u20131353, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Transliteration among indian languages using wx notation.", "author": ["R. Gupta", "P. Goyal", "S. Diwakar"], "venue": "in Proc. of KONVENS,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Report of news 2009 machine transliteration shared task", "author": ["H. Li", "A. Kumaran", "V. Pervouchine", "M. Zhang"], "venue": "Proceedings of the 2009 Named Entities Workshop: Shared Task on Transliteration, ser. NEWS \u201909. Stroudsburg, PA, USA: Association for Computational Linguistics, 2009, pp. 1\u201318. [Online]. Available: http://dl.acm.org/citation.cfm?id=1699705. 1699707", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Global syllable set for building speech synthesis in indian languages", "author": ["E.V. Raghavendra", "S. Desai", "B. Yegnanarayana", "A.W. Black", "K. Prahallad"], "venue": "Proc. IEEE Spoken Language Technology workshop, 2008, pp. 49\u201352.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "A data driven synthesis approach for indian languages using syllable as basic unit", "author": ["S. Kishore", "R. Kumar", "R. Sangal"], "venue": "Proceedings of Intl. Conf. on NLP (ICON), 2002, pp. 311\u2013316.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "A syllablebased framework for unit selection synthesis in 13 indian languages", "author": ["H. Patil", "T. Patel", "N. Shah", "H. Sailor", "R. Krishnan", "G. Kasthuri", "T. Nagarajan", "L. Christina", "N. Kumar", "V. Raghavendra", "S. Kishore", "S. Prasanna", "N. Adiga", "S. Singh", "K. Anand", "P. Kumar", "B. Singh", "S. Binil Kumar", "T. Bhadran", "T. Sajini", "A. Saha", "T. Basu", "K. Rao", "N. Narendra", "A. Sao", "R. Kumar", "P. Talukdar", "P. Acharyaa", "S. Chandra", "S. Lata", "H. Murthy"], "venue": "Oriental COCOSDA held jointly with 2013 Conference on Asian Spoken Language Research and Evaluation (O- COCOSDA/CASLRE), 2013 International Conference, Nov 2013, pp. 1\u20138.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised learning for text-to-speech synthesis", "author": ["W. Oliver"], "venue": "Ph.D. dissertation, University of Edinburgh, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Bootstrapping text-to-speech for speech processing in languages without an orthography", "author": ["S. Sitaram", "S. Palkar", "Y. Chen", "A. Parlikar", "A.W. Black"], "venue": "Proc. ICASSP, 2013, pp. 7992\u20137996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "The NST\u2013 GlottHMM entry to the Blizzard Challenge 2015", "author": ["O. Watts", "S. Ronanki", "Z. Wu", "T. Raitio", "A. Suni"], "venue": "Proc. Blizzard Challenge Workshop, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G. Dahl", "A. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T. Sainath", "B. Kingsbury"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, Nov 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["H. Zen", "A. Senior", "M. Schuster"], "venue": "Proc. ICASSP, 2013, pp. 7962\u20137966.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Z. Wu", "C. Valentini-Botinhao", "O. Watts", "S. King"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Statistical parametric speech synthesis", "author": ["H. Zen", "K. Tokuda", "A.W. Black"], "venue": "Speech Commun., vol. 51, no. 11, pp. 1039\u2013 1064, 2009.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Text processing for text-to-speech systems in indian languages.", "author": ["A.A. Raj", "T. Sarkar", "S.C. Pammi", "S. Yuvaraj", "M. Bansal", "K. Prahallad", "A.W. Black"], "venue": "in Proc. SSW,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2007}, {"title": "Joint-sequence models for grapheme-tophoneme conversion", "author": ["M. Bisani", "H. Ney"], "venue": "Speech Commun., vol. 50, no. 5, pp. 434 \u2013 451, 2008. [Online]. Available: http://www.sciencedirect.com/ science/article/pii/S0167639308000046", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Tandem-straight: A temporally stable power spectral representation for periodic signals and applications to interference-free spectrum, f0, and aperiodicity estimation", "author": ["H. Kawahara", "M. Morise", "T. Takahashi", "R. Nisimura", "T. Irino", "H. Banno"], "venue": "Proc. ICASSP, March 2008, pp. 3933\u20133936.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Zlzer, BeaqleJS: HTML5 and JavaScript based Framework for the Subjective Evaluation of Audio Quality, Linux Audio", "author": ["U.S. Kraft"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Speech Synthesis of Code Mixed Text", "author": ["S. Sitaram", "A.W. Black"], "venue": "Proc. LREC, 2016, pp. 3422\u20133428.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Though a large number of Indian languages have indigenous scripts, the lack of a standardized keyboard, and the ubiquity of QWERTY keyboards, means that people most often write using ASCII [1] text using spellings motivated largely by pronunciation [2].", "startOffset": 189, "endOffset": 192}, {"referenceID": 1, "context": "Though a large number of Indian languages have indigenous scripts, the lack of a standardized keyboard, and the ubiquity of QWERTY keyboards, means that people most often write using ASCII [1] text using spellings motivated largely by pronunciation [2].", "startOffset": 249, "endOffset": 252}, {"referenceID": 2, "context": "Increasingly, many technologies such as Web search and natural language processing are adapting to this phenomenon [3, 4, 5].", "startOffset": 115, "endOffset": 124}, {"referenceID": 3, "context": "Increasingly, many technologies such as Web search and natural language processing are adapting to this phenomenon [3, 4, 5].", "startOffset": 115, "endOffset": 124}, {"referenceID": 4, "context": "Increasingly, many technologies such as Web search and natural language processing are adapting to this phenomenon [3, 4, 5].", "startOffset": 115, "endOffset": 124}, {"referenceID": 5, "context": "In the area of Speech Synthesis, although the efforts of the 2013, 2014 and 2015 Blizzard Challenges [6, 7] resulted in improvements to the naturalness of speech synthesis of Indian languages, the text was assumed to be written in native script.", "startOffset": 101, "endOffset": 107}, {"referenceID": 6, "context": "In the area of Speech Synthesis, although the efforts of the 2013, 2014 and 2015 Blizzard Challenges [6, 7] resulted in improvements to the naturalness of speech synthesis of Indian languages, the text was assumed to be written in native script.", "startOffset": 101, "endOffset": 107}, {"referenceID": 7, "context": "Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS [10] is used in publishing houses, and WX [11] by the Natural Language Processing (NLP) community.", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS [10] is used in publishing houses, and WX [11] by the Natural Language Processing (NLP) community.", "startOffset": 48, "endOffset": 51}, {"referenceID": 9, "context": "Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS [10] is used in publishing houses, and WX [11] by the Natural Language Processing (NLP) community.", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "Among these, CPS (Common Phone Set) [8] and IT3 [9] are widely used by the speech technology community, ITRANS [10] is used in publishing houses, and WX [11] by the Natural Language Processing (NLP) community.", "startOffset": 153, "endOffset": 157}, {"referenceID": 11, "context": "The most common trend observed in the literature is to treat transliteration as a machine translation and discriminative ranking problem [12].", "startOffset": 137, "endOffset": 141}, {"referenceID": 12, "context": "Most existing work in speech synthesis for Indian languages uses unit selection [13] with syllable-like units [14, 15].", "startOffset": 80, "endOffset": 84}, {"referenceID": 13, "context": "Most existing work in speech synthesis for Indian languages uses unit selection [13] with syllable-like units [14, 15].", "startOffset": 110, "endOffset": 118}, {"referenceID": 14, "context": "Most existing work in speech synthesis for Indian languages uses unit selection [13] with syllable-like units [14, 15].", "startOffset": 110, "endOffset": 118}, {"referenceID": 7, "context": "cently, based on the observation that Indian languages share many commonalities in phonetics, a language independent phone set was proposed, and was used in building statistical parametric (HMM-based) speech synthesis systems [8].", "startOffset": 226, "endOffset": 229}, {"referenceID": 15, "context": "Our work also aligns with the recent literature on unsupervised learning for text-to-speech synthesis which aims to reduce the reliance on human knowledge and the manual effort required for building language-specific resources [16, 17, 18].", "startOffset": 227, "endOffset": 239}, {"referenceID": 16, "context": "Our work also aligns with the recent literature on unsupervised learning for text-to-speech synthesis which aims to reduce the reliance on human knowledge and the manual effort required for building language-specific resources [16, 17, 18].", "startOffset": 227, "endOffset": 239}, {"referenceID": 17, "context": "Our work also aligns with the recent literature on unsupervised learning for text-to-speech synthesis which aims to reduce the reliance on human knowledge and the manual effort required for building language-specific resources [16, 17, 18].", "startOffset": 227, "endOffset": 239}, {"referenceID": 18, "context": "Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model.", "startOffset": 53, "endOffset": 57}, {"referenceID": 19, "context": "Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model.", "startOffset": 72, "endOffset": 84}, {"referenceID": 20, "context": "Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model.", "startOffset": 72, "endOffset": 84}, {"referenceID": 21, "context": "Following the success of DNNs for speech recognition [19] and synthesis [20, 21, 22], we also use a DNN as the acoustic model.", "startOffset": 72, "endOffset": 84}, {"referenceID": 7, "context": "We use the common phone set (CPS, [8]) to work with the languages of interest.", "startOffset": 34, "endOffset": 37}, {"referenceID": 8, "context": "We convert the native text to CPS phonetic text using deterministic converters [9, 23].", "startOffset": 79, "endOffset": 86}, {"referenceID": 22, "context": "We convert the native text to CPS phonetic text using deterministic converters [9, 23].", "startOffset": 79, "endOffset": 86}, {"referenceID": 23, "context": "Given the pronunciation lexicon, we train a G2P transducer [24] for each language separately with varying n-gram sequences.", "startOffset": 59, "endOffset": 63}, {"referenceID": 19, "context": "Similar to [20, 21], frame-aligned data for DNN training is created by forced alignment using the HMM system.", "startOffset": 11, "endOffset": 19}, {"referenceID": 20, "context": "Similar to [20, 21], frame-aligned data for DNN training is created by forced alignment using the HMM system.", "startOffset": 11, "endOffset": 19}, {"referenceID": 24, "context": "Finally, the STRAIGHT vocoder [25] is used to synthesize the waveform.", "startOffset": 30, "endOffset": 34}, {"referenceID": 25, "context": "Three MUSHRA (MUltiple Stimuli with Hidden Reference and Anchor) [26] tests were conducted to assess the naturalness of the synthesized speech.", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "[27] is one such recent attempt for synthesizing speech from code-mixed text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Text-to-Speech synthesis in Indian languages has a seen lot of progress over the decade partly due to the annual Blizzard challenges. These systems assume the text to be written in Devanagari or Dravidian scripts which are nearly phonemic orthography scripts. However, the most common form of computer interaction among Indians is ASCII written transliterated text. Such text is generally noisy with many variations in spelling for the same word. In this paper we evaluate three approaches to synthesize speech from such noisy ASCII text: a naive UniGrapheme approach, a Multi-Grapheme approach, and a supervised Grapheme-to-Phoneme (G2P) approach. These methods first convert the ASCII text to a phonetic script, and then learn a Deep Neural Network to synthesize speech from that. We train and test our models on Blizzard Challenge datasets that were transliterated to ASCII using crowdsourcing. Our experiments on Hindi, Tamil and Telugu demonstrate that our models generate speech of competetive quality from ASCII text compared to the speech synthesized from the native scripts. All the accompanying transliterated datasets are released for public access.", "creator": "LaTeX with hyperref package"}}}