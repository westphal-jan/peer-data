{"id": "1608.05513", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network", "abstract": "Recently, a multi-level fuzzy min max neural network (MLF) was proposed, which improves the classification accuracy by handling an overlapped region (area of confusion) with the help of a tree structure. In this brief, an extension of MLF is proposed which defines a new boundary region, where the previously proposed methods mark decisions with less confidence and hence misclassification is more frequent. A methodology to classify patterns more accurately is presented. Our work enhances the testing procedure by means of data centroids. We exhibit an illustrative example, clearly highlighting the advantage of our approach. Results on standard datasets are also presented to evidentially prove a consistent improvement in the classification rate.", "histories": [["v1", "Fri, 19 Aug 2016 07:05:33 GMT  (110kb,D)", "http://arxiv.org/abs/1608.05513v1", null], ["v2", "Tue, 20 Dec 2016 08:09:40 GMT  (0kb,I)", "http://arxiv.org/abs/1608.05513v2", "This paper has been withdrawn by the author due to crucial evidence that the similar work has already been published"]], "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["shraddha deshmukh", "sagar gandhi", "pratap sanap", "vivek kulkarni"], "accepted": false, "id": "1608.05513"}, "pdf": {"name": "1608.05513.pdf", "metadata": {"source": "CRF", "title": "Data Centroid Based Multi-Level Fuzzy Min-Max Neural Network", "authors": ["Shraddha Deshmukh", "Sagar Gandhi", "Pratap Sanap", "Vivek Kulkarni"], "emails": ["shraddhadeshmukh2@gmail.com;", "gandhi@persistent.co.in;", "sanap@persistent.com;", "kulkarni@persistent.com)"], "sections": [{"heading": null, "text": "This year it has reached the point where it is a new border region, where the previously proposed methods require decisions with less confidence and thus more susceptibility to error. A methodology for classifying patterns is presented more precisely. Our work extends to the test procedure using data entroids. We show an illustrative example that clearly highlights the advantage of our approach. Results on standard datasets also become a consistent improvement in the classification rate.Index Terms - Hyperactivity."}, {"heading": "II. MULTI-LEVEL FUZZY MIN-MAX NEURAL NETWORK", "text": "Multilevel fuzzy min max neural network (MLF) is a classifier that efficiently takes into account the misclassification of patterns belonging to an overlapping region by maintaining a tree structure that is a homogeneous tree [9]. In the MLF training phase, examples are continuously repeated to form the hyperboxes and overlaps, each recursion leading to a plane. This recursive procedure is performed until the predefined maximum depth or overlap is present. Hyperbox expansion based on hyperbox size control parameters (\u03b8) is validated by Equation (1) and expansion is performed by Equation (2)."}, {"heading": "III. DATA CENTROID BASED MULTI-LEVEL FUZZY MIN-MAX NEURAL NETWORK", "text": "Fig. 1. D-MLF Subnet Tree StructureIn this section, we give details of a newly proposed algorithm, in particular, we define a new boundary region generated from a trained network, and propose a solution for correctly classifying the associated test patterns. Figure 1 describes the D-MLF structure, where each node in the S-network contains two segments: hyperbox segment (HBS) and overlapped segments (OLS). HBS represents hyperboxes created at that level, while OLS3 represents overlaps at that level."}, {"heading": "A. D-MLF Training Phase", "text": "Similar to the MLF learning process, D-MLF maintains S networks with HBS and OLS structures. First, all patterns are traversed, leading to the creation and extension of hyperboxes with equations (1) and (2), then each hyperbox is checked with the rest of the hyperboxes to detect the overlap using Equation (4). First, i = 1... D (min (w1b, w2b) \u2212 max (v1b, v2b) > 0) (4) Where w1b and w 2 b are the maximum points and v 1 b and v2b are the minimum points of the two hyperboxes under which overlap is being tested. Furthermore, D-MLF adds a new step in the learning phase, known as data centroid (DC) calculation, where DC of all input patterns belonging to each hyperbox is maintained."}, {"heading": "B. D-MLF Testing Phase", "text": "The original MLF is a decision-making process based on the subnet decision. The selected subnet does not have to be a leaf pattern in the tree. We do not change this model, but extend the process of how the subnet marks the choice. 4 The example function given in Equation (11) is used against overlapped sample fields. After recursive traversing of the OLS, a suitable subnet is discovered to which the test pattern belongs. A membership function explained in Equation (6) is used this time to calculate the membership with the hyperboxes within the selected subnet.bj (Ah) = min ([1 \u2212 f (aih \u2212 w, j i)] [1 \u2212 f \u2212 f \u2212 vji \u2212 a h, \u03b3i)]))) f (x, \u03b3) = 1 when the membership with the hyperboxes within the selected subnet is calculated."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "The performance of the proposed method (D-MLF) will be assessed on the basis of the classification rate. Various experiments have been conducted to test D-MLF on various standard datasets, using standard datasets such as Iris, Glass, Wine, Wisconsin Breast Cancer (WBC), Wisconsin Diagnostic Breast Cancer (WDBC) and Ionosphere. These datasets come from the UCI repository of machine learning databases [11]. In these experiments, the Hyperbox size parameter (\u03b8) was chosen as 0.2, 0.5 and 0.9 to perform measurements across the spectrum. The larger the hyperbox becomes, the greater the number of overlaps, the higher the misclassification rate. We divide the data evenly for training and testing, and the average results are shown over 100 experiments. For each iteration, training and test data are randomly selected.Table 1 shows results we compare with the MLF method as it is better than the methods previously proposed [9]."}, {"heading": "VI. CONCLUSION", "text": "A data-centered method, D-MLF, minimizes the importance of outliers and similar errors in decision-making. It has been proven that the proposal exceeds all previously proposed FMM methods. More importantly, we have proposed a model that is suitable for data in the real world and extends the state of the art. D-MLF will help huge areas of application such as security, natural language processing, biomedical reasoning, etc."}], "references": [{"title": "Fuzzy sets", "author": ["L.A. Zadeh"], "venue": "Information and Control, vol. 8, no. 3, pp. 338-353", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1965}, {"title": "Fuzzy min-max neural networks. Part 1. Classification", "author": ["P.K. Simpson"], "venue": "IEEE Trans. Neural Network,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "Fuzzy Min-Max Neural Networks - Part 2: Clustering", "author": ["P.K. Simpson"], "venue": "IEEE Trans Fuzzy Systems", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1993}, {"title": "General fuzzy min-max neural network for clustering and classification", "author": ["B. Gabrys", "A. Bargiela"], "venue": "IEEE Trans. Neural Networks, vol. 11, pp. 769783", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "An inclusion/exclusion fuzzy hyperbox classifier", "author": ["Bargiela", "W. Pedrycz", "M. Tanaka"], "venue": "Int. J. Knowl. Based Intell. Eng. Syst.,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "A fuzzy min-max neural network classifier with compensatory neuron architecture", "author": ["A.V. Nandedkar", "P.K. Biswas"], "venue": "IEEE Trans. Neural Netw.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Mansourizade, \u201cMulti-level fuzzy min-max neural network classifier", "author": ["R. Davtalab", "M.M.H. Dezfoulian"], "venue": "IEEE Trans. Neural Netw. Learn. Syst.,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Results of an analysis and recognition of vowels by computer using zero-crossing data", "author": ["W. Bezdel", "H.J. Chandler"], "venue": "Proc. Inst. Elec. Eng.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1965}, {"title": "Lichman., UCI machine learning repository", "author": ["M.K. Bache"], "venue": "School Inf. Comput. Sci., Univ. California,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "FUZZY set theory was proposed with the intended use to the fields of pattern classification and information processing [1].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "Simpson [2] presented the fuzzy min max neural network (FMM), which makes the soft decisions to organize hyperboxes by its degree of belongingness to a particular class, which is known as a membership function.", "startOffset": 8, "endOffset": 11}, {"referenceID": 1, "context": "Along with this elegant proposal, [2] also presented the characteristics for a good classifier, among which, nonlinear separability, overlapping classes and tuning parameters have proved to be of a great interest to a research community.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "Simpson also presented a clustering approach using FMM in [3].", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "To address this issue, GFMM [4] brought this generality.", "startOffset": 28, "endOffset": 31}, {"referenceID": 0, "context": "Earlier, the process of contraction [1][4] was employed, which used to eliminate all the overlapping regions.", "startOffset": 36, "endOffset": 39}, {"referenceID": 3, "context": "Earlier, the process of contraction [1][4] was employed, which used to eliminate all the overlapping regions.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "Exclusion/Inclusion Fuzzy Classification (HEFC) network was introduced in [5], which further reduced the number of hyperboxes and increased the accuracy.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "This notion is used as it is in almost all the newly introduced models [6][7][8][9].", "startOffset": 74, "endOffset": 77}, {"referenceID": 6, "context": "This notion is used as it is in almost all the newly introduced models [6][7][8][9].", "startOffset": 80, "endOffset": 83}, {"referenceID": 5, "context": "Fuzzy Min-Max Neural Network Classifier with Compensatory Neurons (FMCN) was acquainted in [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 6, "context": "Multi-level fuzzy min max neural network (MLF) [9] addresses the problem of overlapped region with an elegant approach.", "startOffset": 47, "endOffset": 50}, {"referenceID": 6, "context": "patterns belonging to overlapped region by maintaining a tree structure, which is a homogeneous tree [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "This is a novel finding, and contradictory to what MLF authors have mentioned [9].", "startOffset": 78, "endOffset": 81}, {"referenceID": 7, "context": "If the pattern belongs to the boundary region, Euclidean Distance [10] between test pattern and the data centroids of the selected hyperboxes is computed.", "startOffset": 66, "endOffset": 70}, {"referenceID": 8, "context": "These datasets were obtained from the UCI repository of machine learning databases [11].", "startOffset": 83, "endOffset": 87}, {"referenceID": 6, "context": "method, as it has been already proven to perform better than the previously proposed FMM methods [9].", "startOffset": 97, "endOffset": 100}], "year": 2017, "abstractText": "Recently, a multi-level fuzzy min max neural network (MLF) was proposed, which improves the classification accuracy by handling an overlapped region (area of confusion) with the help of a tree structure. In this brief, an extension of MLF is proposed which defines a new boundary region, where the previously proposed methods mark decisions with less confidence and hence misclassification is more frequent. A methodology to classify patterns more accurately is presented. Our work enhances the testing procedure by means of data centroids. We exhibit an illustrative example, clearly highlighting the advantage of our approach. Results on standard datasets are also presented to evidentially prove a consistent improvement in the classification rate.", "creator": "LaTeX with hyperref package"}}}