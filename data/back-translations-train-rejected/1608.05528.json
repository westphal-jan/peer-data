{"id": "1608.05528", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Automatic Selection of Context Configurations for Improved Class-Specific Word Representations", "abstract": "Recent work has demonstrated that state-of-the-art word embedding models require different context types to produce high-quality representations for different word classes such as adjectives (A), verbs (V), and nouns (N). This paper is concerned with identifying contexts useful for learning A/V/N-specific representations. We introduce a simple yet effective framework for selecting class-specific context configurations that yield improved representations for each class. We propose an automatic A* style selection algorithm that effectively searches only a fraction of the large configuration space. The results on predicting similarity scores for the A, V, and N subsets of the benchmarking SimLex-999 evaluation set indicate that our method is useful for each class: the improvements are 6% (A), 6% (V), and 5% (N) over the best previously proposed context type for each class. At the same time, the model trains on only 14% (A), 26.2% (V), and 33.6% (N) of all dependency-based contexts, resulting in much shorter training time.", "histories": [["v1", "Fri, 19 Aug 2016 08:30:35 GMT  (159kb,D)", "http://arxiv.org/abs/1608.05528v1", null], ["v2", "Wed, 7 Jun 2017 09:26:29 GMT  (160kb,D)", "http://arxiv.org/abs/1608.05528v2", "CoNLL 2017"], ["v3", "Mon, 12 Jun 2017 13:11:53 GMT  (153kb,D)", "http://arxiv.org/abs/1608.05528v3", "CoNLL 2017"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ivan vuli\\'c", "roy schwartz", "ari rappoport", "roi reichart", "anna korhonen"], "accepted": false, "id": "1608.05528"}, "pdf": {"name": "1608.05528.pdf", "metadata": {"source": "CRF", "title": "Automatic Selection of Context Configurations for Improved (and Fast) Class-Specific Word Representations", "authors": ["Ivan Vuli\u0107", "Roy Schwartz", "Ari Rappoport", "Roi Reichart", "Anna Korhonen"], "emails": ["iv250@cam.ac.uk", "alk23@cam.ac.uk", "roys02@cs.huji.ac.il", "arir@cs.huji.ac.il", "roiri@ie.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if it is a matter of a way in which people are able to survive themselves and themselves by going in search of themselves. (...) In fact, it is as if people are able to survive themselves and themselves. (...) It is as if people are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if people are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves. (...) It is as if they are able to survive themselves."}, {"heading": "2 Related Work", "text": "Word representations, also known as vector space models or word embeddings, date from the early 1970s (Salton, 1971). Word representation models typically train on (word, context) pairs. Traditionally, most models use word pocket contexts (BOW) that represent a word with its adjacent words, regardless of the syntactic or semantic relationships between them (Collobert et al., 2011; Mikolov et al., 2013; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014, among others). Recently, several alternative context types have been proposed, motivated by the limitations of BOW contexts, mainly by their focus on topical rather than functional similarity (e.g. coffee vs. coffee: tea), including dependency contexts (Grefenstette, 1994; Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et al, 2010; Schwartz, 2015)."}, {"heading": "3 Methodology", "text": "Intro: representation model After previous work (Schwartz et al., 2016; Vulic \u0301 and Korhonen, 2016), we record the word representation model while only varying the context type. For all context configurations and all other (base-) context types, we opt for the standard and very robust choice (Levy et al., 2015) in vector space modeling: SGNS (Mikolov et al., 2013). In all experiments, we use word2vecf, a new implementation of word2vec that is able to learn from arbitrary (word, context) pairings. 2 To simplify the presentation, we assume that all training data for SGNS exists in the form of such (word, context) pairs (Levy and Goldberg, 2014a; Levy and Goldberg, 2014c), where word is the current target, context, and context-based results (e.g., the BOW) can only be observed in a context."}, {"heading": "3.1 Terminology and Background", "text": "This paper starts from dependency contexts based on dependencies (DEPS), the universal contexts (PADO and Lapata, 2007; Utt and Pad\u00f3, 2014), which have recently been found useful for embedding words (Levy and Goldberg, 2014a; Melamud et al., 2016).Given a parsed training corpus that represents for each target a kind of dependency relationship between the head and the modifier (e.g., amod h, w), the type of dependency relationship between the head and the modifier (e.g., amod) is an inverse relationship between the context elements m1 _ r1,. The naive version of DEPS extracts contexts from the parsed corpus without any post-processing (more on this later).In view of an example from Fig. 1, the DEPS contexts are the DEPS contexts."}, {"heading": "3.2 Individual Context Bags", "text": "After the analysis, steps are taken to obtain a comprehensive list of (linguistic universal) individual bags at the right level of granularity for computational feasibility (Step 1), to improve the expressivity of the context (Steps 2-3) and to remove uninformative contexts (Step 4). (1) Closely related bags are merged into a new, broader bag: direct (dobj) and indirect objects (iobj) are merged into obj. Further, nsubj and nsubjpass are merged into a single bag subj, xcomp and ccomp into Comp, advcl and advmod in adv. Note that these combinations are performed solely for efficiency reasons (i.e. to reduce the search space) without affecting any theoretical implications of the proposed algorithm (see later Alg. 1). (2) We perform the standard procedure of prepositional breakdown of Arc and Goldberg, 2014a."}, {"heading": "3.3 Selection of Class-Specific Configurations", "text": "In order to reduce the number of possible configurations for each word class (again for efficiency reasons), it is possible to automatically edit only single context pockets that are potentially useful. Thus, for example, since amod denotes an adjective modifier of a noun, this bag can be safely removed from the pool of candidate pockets for verbs. In another example, amod contexts for verb representation modeling can be a mere coincidence as a result of incorrect parsing, and may ultimately lead to worsened verb vectors. In another example, appositional changes captured by appos will always show a relationship between nouns, so that they can safely be excluded as candidate pockets for projectors and verbs. The construction of such initial class-specific pools is performed automatically before the start of the configuration search (see line 1 in Alg. 1).Best configuration search Although a bruteforce search is possible over all possible configurations (e.g. for practice)."}, {"heading": "4 Experimental Setup", "text": "Regarding training data, pre-processing, parameter settings, and evaluation our setup is always replicated from a recent related study by Levy et al. (2015).Training Data All the representations in our comparison are induced from the cleaned and tokenised English Polyglot Wikipedia data (Al-Rfou et al., 2013).9 The corpus contains approx. 75M sets and 1.5G word tokens.7The total is 133 and not 127 as we have to include 6 additional 1-group configurations that have to be tested but are not included in the initial pool for verbs, see Tab. 1 and Tab. 2.8We have also experimented with a less conservative algorithm variant which does not stop when lower level configurations are not to improve E; it instead follows the path of the best-scoring lower-level configuration even if its score is lower than the score of its origin."}, {"heading": "5 Results and Discussion", "text": "The results, however, are in Tab. 1. The experiment supports our intuition (see Figure. 3.1): Some context bags are definitely not useful for some classes and can be safely removed if they perform the class-specific SGNS training. For example, the amod bag is indeed important for adjective and noun similarity, and at the same time it does not encode any useful information regarding verb similarity; composed, as expected, is useful only for nouns. Tab. 1 also suggests that somehttp: / www.cs.huji.ac.il / \u0445 roys02 / dr06.htmlcontext (e.g. nummod) has no informative knowledge regarding contexts related to context similarity."}, {"heading": "6 Conclusion", "text": "We have presented a novel framework for the selection of class-specific context configurations that provides improved and fast representations for prominent word classes: adjectives, verbs and nouns. Each word class requires a different configuration to achieve optimal results for the class-specific subset of SimLex-999. We have also proposed an algorithm that is capable of finding a suitable class-specific configuration, while the search across the wide range of possible context configurations is computationally feasible.Our approach is as general as possible and can be applied to other context types, evaluation tasks and languages. It is particularly important to emphasize the linguistic portability of our approach. It is based on linguistic agnostic annotations derived from the Universal Dependencies Initiative, and it also uses language-agnostic POS taggers and dependency savers. We have also conducted preliminary experiments that show the exact verbal configurations with English and Italian 9ctible and adjectival patterns."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In CoNLL,", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["Bansal et al.2014] Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "In ACL,", "citeRegEx": "Bansal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bansal et al\\.", "year": 2014}, {"title": "Strudel: A corpus-based semantic model based on properties and types", "author": ["Baroni et al.2010] Marco Baroni", "Brian Murphy", "Eduard Barbu", "Massimo Poesio"], "venue": "Cognitive Science,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Don\u2019t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors", "author": ["Baroni et al.2014] Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In ACL,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Top accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet"], "venue": "In COLING,", "citeRegEx": "Bohnet.,? \\Q2010\\E", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "It depends: Dependency parser comparison using a Web-based evaluation tool", "author": ["Choi et al.2015] Jinho D. Choi", "Joel Tetreault", "Amanda Stent"], "venue": "In ACL,", "citeRegEx": "Choi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Choi et al\\.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel P. Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Efficient unsupervised discovery of word categories using symmetric patterns and high frequency words", "author": ["Davidov", "Rappoport2006] Dmitry Davidov", "Ari Rappoport"], "venue": "In ACL,", "citeRegEx": "Davidov et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2006}, {"title": "The Stanford typed dependencies representation", "author": ["de Marneffe", "Christopher D. Manning"], "venue": "In Proceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation,", "citeRegEx": "Marneffe et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2008}, {"title": "Universal Stanford dependencies: A cross-linguistic typology", "author": ["Timothy Dozat", "Natalia Silveira", "Katri Haverinen", "Filip Ginter", "Joakim Nivre", "Christopher D. Manning"], "venue": "In LREC,", "citeRegEx": "Marneffe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marneffe et al\\.", "year": 2014}, {"title": "Retrofitting word vectors to semantic lexicons", "author": ["Jesse Dodge", "Sujay Kumar Jauhar", "Chris Dyer", "Eduard Hovy", "Noah A. Smith"], "venue": "In NAACL-HLT,", "citeRegEx": "Faruqui et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "A dynamic oracle for arc-eager dependency parsing", "author": ["Goldberg", "Nivre2012] Yoav Goldberg", "Joakim Nivre"], "venue": "In COLING,", "citeRegEx": "Goldberg et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Goldberg et al\\.", "year": 2012}, {"title": "Explorations in automatic thesaurus discovery", "author": ["Gregory Grefenstette"], "venue": null, "citeRegEx": "Grefenstette.,? \\Q1994\\E", "shortCiteRegEx": "Grefenstette.", "year": 1994}, {"title": "A formal basis for the heuristic determination of minimum cost paths", "author": ["Hart et al.1968] Peter E. Hart", "Nils J. Nilsson", "Bertram Raphael"], "venue": "IEEE Transactions on Systems Science and Cybernetics,", "citeRegEx": "Hart et al\\.,? \\Q1968\\E", "shortCiteRegEx": "Hart et al\\.", "year": 1968}, {"title": "SimLex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Specializing word embeddings for similarity or relatedness", "author": ["Kiela et al.2015] Douwe Kiela", "Felix Hill", "Stephen Clark"], "venue": "In EMNLP,", "citeRegEx": "Kiela et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2015}, {"title": "Solutions to Plato\u2019s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge", "author": ["Landauer", "Dumais1997] Thomas K. Landauer", "Susan T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "Landauer et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Landauer et al\\.", "year": 1997}, {"title": "Separated by an un-common language: Towards judgment language informed vector space modeling. CoRR, abs/1508.00106", "author": ["Leviant", "Reichart2015] Ira Leviant", "Roi Reichart"], "venue": null, "citeRegEx": "Leviant et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Leviant et al\\.", "year": 2015}, {"title": "Dependency-based word embeddings", "author": ["Levy", "Goldberg2014a] Omer Levy", "Yoav Goldberg"], "venue": "In ACL,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Linguistic regularities in sparse and explicit word representations", "author": ["Levy", "Goldberg2014b] Omer Levy", "Yoav Goldberg"], "venue": "In CoNLL,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014c] Omer Levy", "Yoav Goldberg"], "venue": "In NIPS,", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings", "author": ["Levy et al.2015] Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the ACL,", "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Not all contexts are created equal: Better word representations with variable attention", "author": ["Ling et al.2015] Wang Ling", "Yulia Tsvetkov", "Silvio Amir", "Ramon Fermandez", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Chu-Cheng Lin"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Learning semantic word embeddings based on ordinal knowledge constraints", "author": ["Liu et al.2015] Quan Liu", "Hui Jiang", "Si Wei", "Zhen-Hua Ling", "Yu Hu"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Miguel B. Almeida", "Noah A. Smith"], "venue": "In ACL,", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Universal dependency annotation for multilingual parsing", "author": ["mee Lee"], "venue": "In ACL,", "citeRegEx": "Lee.,? \\Q2013\\E", "shortCiteRegEx": "Lee.", "year": 2013}, {"title": "Modeling word meaning in context with substitute vectors", "author": ["Melamud et al.2015] Oren Melamud", "Ido Dagan", "Jacob Goldberger"], "venue": "In NAACL-HLT,", "citeRegEx": "Melamud et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2015}, {"title": "The role of context types and dimensionality in learning word embeddings", "author": ["Melamud et al.2016] Oren Melamud", "David McClosky", "Siddharth Patwardhan", "Mohit Bansal"], "venue": null, "citeRegEx": "Melamud et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Melamud et al\\.", "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Gregory S. Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Learning word embeddings efficiently with noise-contrastive estimation", "author": ["Mnih", "Kavukcuoglu2013] Andriy Mnih", "Koray Kavukcuoglu"], "venue": "In NIPS,", "citeRegEx": "Mnih et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2013}, {"title": "Counter-fitting word vectors to linguistic", "author": ["Mrk\u0161i\u0107 et al.2016] Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Lina Maria Rojas-Barahona", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve J. Young"], "venue": null, "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2016}, {"title": "Universal Dependencies 1.2. LINDAT/CLARIN digital library at Institute of Formal and Applied Linguistics, Charles University in Prague", "author": ["Nivre et al.2015] Joakim Nivre"], "venue": null, "citeRegEx": "Nivre,? \\Q2015\\E", "shortCiteRegEx": "Nivre", "year": 2015}, {"title": "Dependency-based construction of semantic space models", "author": ["Pad\u00f3", "Lapata2007] Sebastian Pad\u00f3", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2007}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A universal part-ofspeech tagset", "author": ["Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan T. McDonald"], "venue": "In LREC,", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "The SMART Retrieval System: Experiments in Automatic Document Processing", "author": ["Gerard Salton"], "venue": null, "citeRegEx": "Salton.,? \\Q1971\\E", "shortCiteRegEx": "Salton.", "year": 1971}, {"title": "Part-of-speech induction from scratch", "author": ["Hinrich Sch\u00fctze"], "venue": "In ACL,", "citeRegEx": "Sch\u00fctze.,? \\Q1993\\E", "shortCiteRegEx": "Sch\u00fctze.", "year": 1993}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction", "author": ["Roi Reichart", "Ari Rappoport"], "venue": "In CoNLL,", "citeRegEx": "Schwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "Symmetric patterns and coordinations: Fast and enhanced representations of verbs and adjectives", "author": ["Roi Reichart", "Ari Rappoport"], "venue": "In NAACL-HLT,", "citeRegEx": "Schwartz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2016}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In NAACL-HLT,", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Word representations: A simple and general method for semi-supervised learning", "author": ["Lev-Arie Ratinov", "Yoshua Bengio"], "venue": "In ACL,", "citeRegEx": "Turian et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turian et al\\.", "year": 2010}, {"title": "Crosslingual and multilingual construction of syntax-based vector space models", "author": ["Utt", "Pad\u00f32014] Jason Utt", "Sebastian Pad\u00f3"], "venue": "Transactions of the ACL,", "citeRegEx": "Utt et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Utt et al\\.", "year": 2014}, {"title": "Is \u201cuniversal syntax\u201d universally useful for learning distributed word representations", "author": ["Vuli\u0107", "Korhonen2016] Ivan Vuli\u0107", "Anna Korhonen"], "venue": "In ACL,", "citeRegEx": "Vuli\u0107 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vuli\u0107 et al\\.", "year": 2016}, {"title": "From paraphrase database to compositional paraphrase model and back", "author": ["Wieting et al.2015] John Wieting", "Mohit Bansal", "Kevin Gimpel", "Karen Livescu"], "venue": "Transactions of the ACL,", "citeRegEx": "Wieting et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Learning syntactic categories using paradigmatic representations of word context", "author": ["Enis Sert", "Deniz Yuret"], "venue": "In EMNLP,", "citeRegEx": "Yatbaz et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Yatbaz et al\\.", "year": 2012}, {"title": "Improving lexical embeddings with semantic knowledge", "author": ["Yu", "Dredze2014] Mo Yu", "Mark Dredze"], "venue": "In ACL,", "citeRegEx": "Yu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 41, "context": "Dense real-valued distributed word representations, or word embeddings, have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014).", "startOffset": 164, "endOffset": 233}, {"referenceID": 7, "context": "Dense real-valued distributed word representations, or word embeddings, have become ubiquitous in NLP, serving as invaluable features in a broad range of NLP tasks (Turian et al., 2010; Collobert et al., 2011; Chen and Manning, 2014).", "startOffset": 164, "endOffset": 233}, {"referenceID": 29, "context": "The omnipresent word2vec skip-gram model with negative sampling (SGNS) (Mikolov et al., 2013) is still considered the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al.", "startOffset": 71, "endOffset": 93}, {"referenceID": 3, "context": ", 2013) is still considered the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015).", "startOffset": 198, "endOffset": 238}, {"referenceID": 22, "context": ", 2013) is still considered the state-of-the-art word representation model, due to its simplicity, fast training, as well as its solid and robust performance across a wide variety of semantic tasks (Baroni et al., 2014; Levy et al., 2015).", "startOffset": 198, "endOffset": 238}, {"referenceID": 1, "context": "Recent work demonstrated that reaching beyond the omnipresent BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Schwartz et al., 2016; Melamud et al., 2016) or symmetric patterns (Schwartz et al.", "startOffset": 119, "endOffset": 185}, {"referenceID": 39, "context": "Recent work demonstrated that reaching beyond the omnipresent BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Schwartz et al., 2016; Melamud et al., 2016) or symmetric patterns (Schwartz et al.", "startOffset": 119, "endOffset": 185}, {"referenceID": 28, "context": "Recent work demonstrated that reaching beyond the omnipresent BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Schwartz et al., 2016; Melamud et al., 2016) or symmetric patterns (Schwartz et al.", "startOffset": 119, "endOffset": 185}, {"referenceID": 38, "context": ", 2016) or symmetric patterns (Schwartz et al., 2015; Schwartz et al., 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V).", "startOffset": 30, "endOffset": 76}, {"referenceID": 39, "context": ", 2016) or symmetric patterns (Schwartz et al., 2015; Schwartz et al., 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V).", "startOffset": 30, "endOffset": 76}, {"referenceID": 23, "context": "1 Further, several recent studies (Ling et al., 2015; Schwartz et al., 2016) suggested that \u201cnot all contexts are created equal\u201d: for instance, Schwartz et al.", "startOffset": 34, "endOffset": 76}, {"referenceID": 39, "context": "1 Further, several recent studies (Ling et al., 2015; Schwartz et al., 2016) suggested that \u201cnot all contexts are created equal\u201d: for instance, Schwartz et al.", "startOffset": 34, "endOffset": 76}, {"referenceID": 15, "context": "Besides their competitive performance on verb and adjective similarity measured on SimLex-999 (Hill et al., 2015), training with such coordination-based contexts is significantly faster due to the pre-training selection step.", "startOffset": 94, "endOffset": 113}, {"referenceID": 1, "context": "Recent work demonstrated that reaching beyond the omnipresent BOW contexts towards contexts based on dependency parses (Bansal et al., 2014; Schwartz et al., 2016; Melamud et al., 2016) or symmetric patterns (Schwartz et al., 2015; Schwartz et al., 2016) yields significant improvements in learning representations for particular word classes such as adjectives (A) and verbs (V). Curiously enough, despite the success story with adjectives and verbs, SGNS with BOW contexts still outperforms all other context types in noun (N) representation learning.1 Further, several recent studies (Ling et al., 2015; Schwartz et al., 2016) suggested that \u201cnot all contexts are created equal\u201d: for instance, Schwartz et al. (2016) show that it is possible to learn highquality representations of verbs and adjectives using only a subset of dependency-based contexts that covers coordination structures for the SGNS training while discarding everything else.", "startOffset": 120, "endOffset": 720}, {"referenceID": 38, "context": "We refer the reader to the work of Schwartz et al. (2016) for a comprehensive overview of results on SimLex-999.", "startOffset": 35, "endOffset": 58}, {"referenceID": 22, "context": "Using a standard experimental setup (Levy et al., 2015), we outperform the best baseline context type for each word class: the improvements are 6 \u03c1 points for adjectives, 6 for verbs, and 5 for nouns over the best previously proposed context type for each word class.", "startOffset": 36, "endOffset": 55}, {"referenceID": 39, "context": "We also demonstrate that the proposed method is robust: the improvements extend to another standard training setup (Schwartz et al., 2016) which uses more training data, as well as a different parser and a different dependency annotation scheme.", "startOffset": 115, "endOffset": 138}, {"referenceID": 36, "context": "Word representations, also known as vector space models or word embeddings, date back to the early 1970s (Salton, 1971).", "startOffset": 105, "endOffset": 119}, {"referenceID": 13, "context": "These include dependency contexts (Grefenstette, 1994; Pad\u00f3 and Lapata, 2007; Levy and Goldberg, 2014a), pattern contexts (Baroni et", "startOffset": 34, "endOffset": 103}, {"referenceID": 45, "context": ", 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015).", "startOffset": 31, "endOffset": 74}, {"referenceID": 27, "context": ", 2015) and substitute vectors (Yatbaz et al., 2012; Melamud et al., 2015).", "startOffset": 31, "endOffset": 74}, {"referenceID": 27, "context": "Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups, and showed that the optimal type largely depends on the task at hand.", "startOffset": 0, "endOffset": 22}, {"referenceID": 27, "context": "Melamud et al. (2016) compared three context types on a set of intrinsic and extrinsic evaluation setups, and showed that the optimal type largely depends on the task at hand. Vuli\u0107 and Korhonen (2016) compared a few variants of", "startOffset": 0, "endOffset": 202}, {"referenceID": 38, "context": "Schwartz et al. (2016), showed that symmetric patterns are very useful contexts for verb and adjective similarity, while BOW works best for nouns, measured on", "startOffset": 0, "endOffset": 23}, {"referenceID": 15, "context": "the benchmarking SimLex-999 (Hill et al., 2015) evaluation set.", "startOffset": 28, "endOffset": 47}, {"referenceID": 44, "context": ", similarity vs relatedness, antonyms) operate in one of the two following frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015); (2) \u201clight\u201d post-processing procedures which use lexical knowledge to refine offthe-shelf pre-trained word vectors (Faruqui et al.", "startOffset": 168, "endOffset": 268}, {"referenceID": 24, "context": ", similarity vs relatedness, antonyms) operate in one of the two following frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015); (2) \u201clight\u201d post-processing procedures which use lexical knowledge to refine offthe-shelf pre-trained word vectors (Faruqui et al.", "startOffset": 168, "endOffset": 268}, {"referenceID": 16, "context": ", similarity vs relatedness, antonyms) operate in one of the two following frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015); (2) \u201clight\u201d post-processing procedures which use lexical knowledge to refine offthe-shelf pre-trained word vectors (Faruqui et al.", "startOffset": 168, "endOffset": 268}, {"referenceID": 23, "context": ", similarity vs relatedness, antonyms) operate in one of the two following frameworks: (1) modifying the prior or the regularisation of the original training procedure (Yu and Dredze, 2014; Wieting et al., 2015; Liu et al., 2015; Kiela et al., 2015; Ling et al., 2015); (2) \u201clight\u201d post-processing procedures which use lexical knowledge to refine offthe-shelf pre-trained word vectors (Faruqui et al.", "startOffset": 168, "endOffset": 268}, {"referenceID": 11, "context": ", 2015); (2) \u201clight\u201d post-processing procedures which use lexical knowledge to refine offthe-shelf pre-trained word vectors (Faruqui et al., 2015; Mrk\u0161i\u0107 et al., 2016).", "startOffset": 124, "endOffset": 167}, {"referenceID": 31, "context": ", 2015); (2) \u201clight\u201d post-processing procedures which use lexical knowledge to refine offthe-shelf pre-trained word vectors (Faruqui et al., 2015; Mrk\u0161i\u0107 et al., 2016).", "startOffset": 124, "endOffset": 167}, {"referenceID": 22, "context": "For all context configurations and all other (baseline) context types, we opt for the standard and very robust choice (Levy et al., 2015) in vector space modeling: SGNS (Mikolov et al.", "startOffset": 118, "endOffset": 137}, {"referenceID": 29, "context": ", 2015) in vector space modeling: SGNS (Mikolov et al., 2013).", "startOffset": 39, "endOffset": 61}, {"referenceID": 35, "context": ", 2014) complemented with the universal POS tagset (Petrov et al., 2012).", "startOffset": 51, "endOffset": 72}, {"referenceID": 39, "context": "(3) Coordination-based contexts are extracted as in prior work (Schwartz et al., 2016), distinguishing between left and right contexts extracted from the conj relation.", "startOffset": 63, "endOffset": 86}, {"referenceID": 14, "context": "inspired by A* search (Hart et al., 1968).", "startOffset": 22, "endOffset": 41}, {"referenceID": 19, "context": "In terms of training data, pre-processing, parameter settings, and evaluation our setup is replicated from a recent related study by Levy et al. (2015).", "startOffset": 133, "endOffset": 152}, {"referenceID": 0, "context": "Training Data All the representations in our comparison are induced from the cleaned and tokenised English Polyglot Wikipedia data (Al-Rfou et al., 2013).", "startOffset": 131, "endOffset": 153}, {"referenceID": 35, "context": "The Wikipedia data were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using state-of-the art TurboTagger (Martins et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 25, "context": ", 2012) using state-of-the art TurboTagger (Martins et al., 2013)10, trained using default settings without any further parameter fine-tuning (SVM MIRA with 20 iterations) on the TRAIN+DEV portion of the UD treebank annotated with UPOS tags.", "startOffset": 43, "endOffset": 65}, {"referenceID": 4, "context": "61 (Bohnet, 2010)12 with standard settings on the TRAIN+DEV UD treebank portion.", "startOffset": 3, "endOffset": 17}, {"referenceID": 38, "context": "Evaluation Following prior work (Schwartz et al., 2015; Schwartz et al., 2016), we experiment with the verb pair (222 pairs), adjective pair (111 pairs), and noun pair (666 pairs) portions of SimLex-999.", "startOffset": 32, "endOffset": 78}, {"referenceID": 39, "context": "Evaluation Following prior work (Schwartz et al., 2015; Schwartz et al., 2016), we experiment with the verb pair (222 pairs), adjective pair (111 pairs), and noun pair (666 pairs) portions of SimLex-999.", "startOffset": 32, "endOffset": 78}, {"referenceID": 19, "context": "Our final evaluation setup is borrowed from Levy et al. (2015): the context configurations are optimised on a development set, which is separate from the unseen test data.", "startOffset": 44, "endOffset": 63}, {"referenceID": 28, "context": "configurations against relevant baseline context types from prior work (Melamud et al., 2016; Schwartz et al., 2016; Vuli\u0107 and Korhonen, 2016): - BOW: Standard bag-of-words contexts.", "startOffset": 71, "endOffset": 142}, {"referenceID": 39, "context": "configurations against relevant baseline context types from prior work (Melamud et al., 2016; Schwartz et al., 2016; Vuli\u0107 and Korhonen, 2016): - BOW: Standard bag-of-words contexts.", "startOffset": 71, "endOffset": 142}, {"referenceID": 6, "context": "com/archive/p/mate-tools/ We opt for the Mate parser due to its speed, simplicity, and state-of-the-art performance, see (Choi et al., 2015).", "startOffset": 121, "endOffset": 140}, {"referenceID": 38, "context": "- SP: Contexts based on symmetric patterns (SPs) (Davidov and Rappoport, 2006; Schwartz et al., 2015), lexico-syntactic constructs such as \u201cX and Y\u201d or \u201cX or Y\u201d; Y is then an SP context instance for X, and vice versa.", "startOffset": 49, "endOffset": 101}, {"referenceID": 39, "context": "The SP contexts, extracted from plain text, are very effective in modeling adjective and verb similarity, but they fall short of BOW and related approaches in noun similarity modeling (Schwartz et al., 2016).", "startOffset": 184, "endOffset": 207}, {"referenceID": 22, "context": "SGNS Preprocessing and Parameters The SGNS preprocessing scheme was replicated from (Levy and Goldberg, 2014a; Levy et al., 2015) with all context types.", "startOffset": 84, "endOffset": 129}, {"referenceID": 38, "context": "For BOW and POSIT, the window size was tuned on the dev set: its value is 2 in all experiments, which is aligned with prior work (Schwartz et al., 2015).", "startOffset": 129, "endOffset": 152}, {"referenceID": 39, "context": "The results indicate that class-specific configurations are not as lightweight and fast as SP or COORD contexts (Schwartz et al., 2016).", "startOffset": 112, "endOffset": 135}, {"referenceID": 38, "context": "Another Training Setup To demonstrate that context configurations generalise to settings with more training data, and other annotation and parser choices, in another experiment we have replicated the experimental setup of Schwartz et al. (2016).", "startOffset": 222, "endOffset": 245}, {"referenceID": 39, "context": "The corpus is parsed with labeled Stanford dependencies (de Marneffe and Manning, 2008) following (Levy and Goldberg, 2014a; Schwartz et al., 2016): Stanford POS Tagger (Toutanova et al.", "startOffset": 98, "endOffset": 147}, {"referenceID": 40, "context": ", 2016): Stanford POS Tagger (Toutanova et al., 2003) and the stack version of the MALT parser (Goldberg and Nivre, 2012).", "startOffset": 29, "endOffset": 53}, {"referenceID": 9, "context": "The straightforward \u201ctranslation\u201d from labelled Stanford dependencies into UD is performed using the mapping from de Marneffe et al. (2014),", "startOffset": 117, "endOffset": 140}, {"referenceID": 38, "context": "Table 7: Results on the A/V/N SimLex-999 subsets, and on the entire set (All) in the training setup from Schwartz et al. (2016). d = 500.", "startOffset": 105, "endOffset": 128}], "year": 2016, "abstractText": "Recent work has demonstrated that stateof-the-art word embedding models require different context types to produce highquality representations for different word classes such as adjectives (A), verbs (V), and nouns (N). This paper is concerned with identifying contexts useful for learning A/V/N-specific representations. We introduce a simple yet effective framework for selecting class-specific context configurations that yield improved representations for each class. We propose an automatic A* style selection algorithm that effectively searches only a fraction of the large configuration space. The results on predicting similarity scores for the A, V, and N subsets of the benchmarking SimLex-999 evaluation set indicate that our method is useful for each class: the improvements are 6% (A), 6% (V), and 5% (N) over the best previously proposed context type for each class. At the same time, the model trains on only 14% (A), 26.2% (V), and 33.6% (N) of all dependency-based contexts, resulting in much shorter training time.", "creator": "LaTeX with hyperref package"}}}