{"id": "1608.05554", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Learning to Start for Sequence to Sequence Architecture", "abstract": "The sequence to sequence architecture is widely used in the response generation and neural machine translation to model the potential relationship between two sentences. It typically consists of two parts: an encoder that reads from the source sentence and a decoder that generates the target sentence word by word according to the encoder's output and the last generated word. However, it faces to the cold start problem when generating the first word as there is no previous word to refer. Existing work mainly use a special start symbol &lt;/s&gt;to generate the first word. An obvious drawback of these work is that there is not a learnable relationship between words and the start symbol. Furthermore, it may lead to the error accumulation for decoding when the first word is incorrectly generated. In this paper, we proposed a novel approach to learning to generate the first word in the sequence to sequence architecture rather than using the start symbol. Experimental results on the task of response generation of short text conversation show that the proposed approach outperforms the state-of-the-art approach in both of the automatic and manual evaluations.", "histories": [["v1", "Fri, 19 Aug 2016 09:48:13 GMT  (231kb,D)", "http://arxiv.org/abs/1608.05554v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["qingfu zhu", "weinan zhang", "lianqiang zhou", "ting liu"], "accepted": false, "id": "1608.05554"}, "pdf": {"name": "1608.05554.pdf", "metadata": {"source": "CRF", "title": "Learning to Start for Sequence to Sequence Architecture", "authors": ["Qingfu Zhu", "Weinan Zhang", "Lianqiang Zhou", "Ting Liu"], "emails": ["qfzhu@ir.hit.edu.cn", "wnzhang@ir.hit.edu.cn", "tliu@ir.hit.edu.cn", "tomcatzhou@tencent.com"], "sections": [{"heading": "1 Introduction", "text": "Recently, the sequence to the sequence (Seq2Seq) architecture has undergone a major evolution as a general neural network method for modeling the potential relationship between two sequences. In the basic Seq2Seq model, each sequence is normally modeled by RNN, and the two RNs for the source sequence and target sequence are called encoders or decoders. The encoder reads from the source set and makes some summaries. In fact, the decoder is a language model that conditions words according to previously predicted words (usually the context vector), indicating that when the decoder attempts to predict a word, the context vector and the word predicted in a predicted time are two necessary inputs that are required. Thus, the initialization question arises: when the decoder generates the first word, there is no predicted word to reference."}, {"heading": "2 Background", "text": "From a probability perspective, the Seq2Seq model maximizes the probability of the target sequence being conditioned with the source sequence during the training process, and searches for a sequence that has a maximum conditioned probability given the source sequence during the prediction process. Because of this highly abstract attribute, many tasks such as response generation, machine translation, and answering questions can all be modeled with this architecture."}, {"heading": "2.1 RNN encoder-decoder", "text": "Typically, a sequence model consists of two parts: encoder and decoder, both of which are often implemented using a family of RNN, such as GRU (Cho et al., 2014a; Chung et al., 2014) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Graves, 2012a), so that a seq2seq model is also referred to as the RNN encoder decoder architecture; the encoder is a normal RNN that reads from a sequence of words and displays their hidden states; these states are also referred to as annotations designated by H, and for each hidden state hi at the time i, it is calculated by its previous hidden state hi \u2212 1, and the word at the time t: ht = f (ht \u2212 1, xt), the last encoded state c: H = {h1, h2, h3,... h1 (T is the hidden length of the source) is not the text."}, {"heading": "2.2 Attention Mechanism in Seq2Seq", "text": "Considering that this vector may not be able to contain all the useful information of the source sequence and thus become a bottleneck of the model, Bahdanau et al. (2014) add the attention mechanism to improve the performance of the Seq2Seq. Compared to the basic architecture, which uses the last hidden state as context vector c, the attention mechanism gives weight to all annotations and then uses them to calculate a weighted sum as a new context vector. It should be noted that in this way the vector c is different for each step in the decoder, because a time-related variable was involved in the calculation, so that here we refer to the result as vector cj = T = i = 1 \u03b1ijhi (5), the context vector being cj \u2212 the vector if we decode the word intuitively in the context, with the forward weight (1) being considered equal to the analogy (1)."}, {"heading": "2.3 Initialization in Seq2Seq Learning", "text": "In the RNN encoder, a state is used to calculate the state next time (see Eq.1), and in this way the initial state has an indirect influence on all the next states. The RNN encoder shares the same situation. In addition, the decoder has an additional variable that should be initialized: predicted word at the last step, because we do not have this input for the first process of generation. Typically, we set the initial hidden state of the encoder to a vector of zero, and people usually use the last hidden state of the encoder to initialize the first hidden states of the decoder: s0 = \u03c3 (WshT) (7), which is a non-linear function, Ws is a feasible parameter. This is intuitively plausible because it describes the relationship between the two sequences that the decoder conditioned with the symbol that we manually generate for the first decoder function."}, {"heading": "3 Learning to start", "text": "In this section, we propose a new model for predicting Seq2Seqs to achieve the original prediction that the probability is higher than the second. We think that the method that uses a start symbol to predict the first word is not very suitable. First, the RNN decoder is essentially a language model (Mikolov et al., 2010) that uses the predicted words to predict a new word, from the perspective of probability, it learns a conditional probability system of the word that contains the last predicted words. While the start symbol and the first predicted word have no such association, since most words can be placed at the first position of a sequence, there is no learnable conditional probability system, so that the result of taking a start symbol of x may cause the model to predict some high frequency words that are also observed during other conversation models with this architecture (Sordoni et al., 2015; Serban et al, 2015, and Le yy, 2015, which we think are a higher probability that we are involved in predicting the number of words)."}, {"heading": "4 Experiment Settings", "text": "To verify effectiveness, our proposed model was tested against the task of response generation of short text conversations. As a kind of neural machine architecture, a big data architecture is always required to achieve good performance. To achieve this, a dialog set was searched as a training set, and for comparison, a basic type of Seq2Seq response generation architecture, the hybrid model proposed by Shang et al. (2015), was implemented."}, {"heading": "4.1 Data", "text": "For the sake of simplicity, the first sentence and the second sentence of a dialogue pair are each referred to as post and answer (Shang et al., 2015). The data set contains one million pairs and about 35,000 words. Compared to the data used by Shang et al. (2015), this data is a one-to-one dataset, one post corresponds to exactly one answer. While combing through some one-to-one data from the microblog in the publication Shang et al. (2015), they then distributed all responses to their post. This is a creative way to build a large dataset, while during our experiments we found that the one-to-one data has a faster convergence rate, so we created our own datasets and trained models on it. Table 1 is an example of our datasets."}, {"heading": "4.2 Models", "text": "We trained two models: the first is a basic Seq2Seq dialog model called the Hybird model (referred to as HYP) (Shang et al., 2015), the other is what we proposed (referred to as LTS); the encoder and decoder were both implemented with the GRU (Cho et al., 2014a; Chung et al., 2014); and we set the parameters of our model to Shang et al. (2015); the hidden size in the encoder was set to 1024; and the embedding size was set to 500; all embedding vectors were pre-trained based on the training data (Mikolov et al., 2013a; Mikolov et al., 2013b); and we sent the data to the model while processing the training using the mini-stack with a stack size of 100, and the RMSprop algorithm was used to update the parameters of the model (and we tracked both over the best set of 201m)."}, {"heading": "5 Result", "text": "To date, there is no uniform evaluation of response generation (Galley et al., 2015; Pietquin and Hastie, 2013; Schatzmann et al., 2005). Firstly, we tested the performance of our model against the statistics of the first predicted word. Secondly, we evaluated the complete answer to see if our model can improve the Seq2Seq architecture. To achieve this goal, we use two metrics: firstly, we used the wildly applied automatic evaluation method-blue (Papineni et al., 2002) in the field of machine translation, and secondly, the human annotation method."}, {"heading": "5.1 First Generated Word Evaluation", "text": "In order to evaluate the generation of the first word, two aspects are taken into account: accuracy rate and diversity; the pairs of dialogs in the test set are referred to as test sample and reference, and the sequence generated by the model is still referred to as response. As mentioned earlier, the test set is a one-to-one set that corresponds to the servo power; we define a set called Rset for each sample that consists of all the first words of that sample; during the test process, the first word of the sample falls into its R set, then it is marked as hit; and the accuracy is the ratio of the hit samples across the entire test set."}, {"heading": "5.2 Bleu Metric", "text": "We use this metric to evaluate the complete reaction of a model, rather than the first text. Table 3 shows that the LTS is in good agreement with human judgment on the task of reaction generation (Sordoni et al., 2015; Li et al., 2015) and the result is shown in Table 3.From Table 3, we can see that the LTS performs better than the HYP in Bleu-1 to Bleu-3. From this table, we can also see that the improvement compared to the Bleu-1 is not as significant as the other two. We analyzed this situation and came to the conclusion that it could be because the Bleu metric calculates overlaps of n-grams between reaction and references, while the unigram is easier to compare compared to other n-grams, so that the Bleu-1 is not completely differentiated.Table 4 shows some results of the Bleu evaluation, two models received similar bleu-1 values, while the Bleu-2 and Bleu-3 measurement values are much more closely mirrored by our measurement magnifiers, which may also reflect the improvement much earlier."}, {"heading": "5.3 Manual Evaluation", "text": "At the same time, we also tested our model's adoption of human annotation method. The evaluation metric refers to Shang (2015). We generated answers using HYP model or LTS, then these answers, along with their original questions, are mixed into a new file to ensure that workers can assess the result fairly. Three workers were involved in assigning these answers in the range of 0 to 2, and the evaluation metric is as follows: 0: This means that the answer may not be perfect, but in a particular scenario can be treated as an appropriate answer, or it is too general an answer, such as \"I don't know if the logic is consistent or relevant with the original post in semantics, it should be a 0-Score.1: This means that the answer may not be perfect, but can be treated as an appropriate answer in a particular scenario, or it is too general an answer, such as\" I don't know that. \"2: This indicates a thoroughly appropriate answer, an answer to this category can only be sorted if we find the scenario is free of fluidity and the grammaticity principle is not three."}, {"heading": "6 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Sequence to sequence for Machine Translation", "text": "Using the sequence model, neural machine translation has already provided a performance comparable to that of traditional methods (Bahdanau et al., 2014) and, as far as we know, was first introduced in this area by Kalchburner and Blunsom (2013), Sutskever et al. (2014), Cho et al. (2014b), Gao et al. (2014). In addition, Cho et al. (2014b) added vector c as an additional input to each step of the decoder, giving all steps, not just the first, complete information about the context vector in this way. Furthermore, Bahdanau et al. (2014) proposed a novel method for calculating a weighted sum of all the encoder's annotations. This mechanism can be regarded as a kind of attention, which means when we decipher a word to which we should pay more attention."}, {"heading": "6.2 Sequence to sequence in Response Generation", "text": "Broadly speaking, dialogue systems can be divided into two classes (Serban et al., 2016): goal-driven represented by systems Ga\u0161ic \u0301 et al. (2013) and non-goal-driven systems. Neural network methods are mainly used in the latter, as a large scale of data in this area is easier to achieve. Ritter et al. (2011) first combined microblogging data with generative probability models, then Shang et al. (2015) used this type of data on the Seq2Seq to create a short maintenance machinery, followed by Serban et al. (2016), who developed the Nerual Network hierarchical model with the aim of modelling the utterances and interactive structure to build a multi-round dialogue system. At the same time, Banband (2012) proposed methods that use a different type of data, the movie dialogue. Building on this, Ameixa et al. (2014) found that the use of the on-demand system and the subtitles of movies can also improve performance."}, {"heading": "7 Conclusions", "text": "In this paper, we proposed a new approach to the sequence model for generating the first word. It has been shown that our proposed model can enhance both the accuracy and diversity of generating the first word, thereby improving the overall performance of the generation. Experiments in the response generation task confirmed the effectiveness of our model, while our proposed method is not a method for a specific task, but a general framework that can also be used for other tasks."}], "references": [{"title": "Luke, i am your father: dealing with out-of-domain requests by using movies subtitles", "author": ["Ameixa et al.2014] David Ameixa", "Luisa Coheur", "Pedro Fialho", "Paulo Quaresma"], "venue": "In International Conference on Intelligent Virtual Agents,", "citeRegEx": "Ameixa et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ameixa et al\\.", "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Movie-dic: a movie dialogue corpus for research and development", "author": ["Rafael E Banchs"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Banchs.,? \\Q2012\\E", "shortCiteRegEx": "Banchs.", "year": 2012}, {"title": "Audio chord recognition with recurrent neural networks", "author": ["Yoshua Bengio", "Pascal Vincent"], "venue": "In ISMIR,", "citeRegEx": "Boulanger.Lewandowski et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Boulanger.Lewandowski et al\\.", "year": 2013}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Cho et al.2014a] Kyunghyun Cho", "Bart Van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "Computer Science", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine", "author": ["Cho et al.2014b] Kyunghyun Cho", "Bart Van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "Caglar Gulcehre", "Kyung Hyun Cho", "Yoshua Bengio"], "venue": "Eprint Arxiv", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "deltableu: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Galley et al.2015] Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06863", "citeRegEx": "Galley et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Galley et al\\.", "year": 2015}, {"title": "Learning continuous phrase representations for translation modeling", "author": ["Gao et al.2014] Jianfeng Gao", "Xiaodong He", "Wen-tau Yih", "Li Deng"], "venue": null, "citeRegEx": "Gao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gao et al\\.", "year": 2014}, {"title": "On-line policy optimisation of bayesian spoken dialogue systems via human interaction", "author": ["Ga\u0161i\u0107 et al.2013] M Ga\u0161i\u0107", "Catherine Breslin", "Matthew Henderson", "Dongho Kim", "Martin Szummer", "Blaise Thomson", "Pirros Tsiakoulis", "Steve Young"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "Ga\u0161i\u0107 et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161i\u0107 et al\\.", "year": 2013}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Gers et al.2000] Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Neural networks. In Supervised Sequence Labelling with Recurrent Neural Networks, pages", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Sequence transduction with recurrent neural networks. arXiv preprint arXiv:1211.3711", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["Kalchbrenner", "Blunsom2013] Nal Kalchbrenner", "Phil Blunsom"], "venue": "In EMNLP,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2013}, {"title": "A diversity-promoting objective function for neural conversation models. arXiv preprint arXiv:1510.03055", "author": ["Li et al.2015] Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "In Interspeech,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "A survey on metrics for the evaluation of user simulations. The knowledge engineering review, 28(01):59\u201373", "author": ["Pietquin", "Hastie2013] Olivier Pietquin", "Helen Hastie"], "venue": null, "citeRegEx": "Pietquin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pietquin et al\\.", "year": 2013}, {"title": "Data-driven response generation in social media", "author": ["Ritter et al.2011] Alan Ritter", "Colin Cherry", "William B Dolan"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Quantitative evaluation of user simulation techniques for spoken dialogue systems", "author": ["Kallirroi Georgila", "Steve Young"], "venue": "In 6th SIGdial Workshop on DISCOURSE and DIALOGUE", "citeRegEx": "Schatzmann et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schatzmann et al\\.", "year": 2005}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808", "citeRegEx": "Serban et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2015}, {"title": "Building end-to-end dialogue systems using generative hierarchical neural network models", "author": ["Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "In Proceedings of the 30th AAAI Conference on Artificial Intelligence (AAAI-16)", "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "Computer Science", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "A neural network approach to context-sensitive generation of conversational responses", "author": ["Michel Galley", "Michael Auli", "Chris Brockett", "Yangfeng Ji", "Margaret Mitchell", "Jian-Yun Nie", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06714", "citeRegEx": "Sordoni et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "A neural conversational model. arXiv preprint arXiv:1506.05869", "author": ["Vinyals", "Le2015] Oriol Vinyals", "Quoc Le"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 27, "context": "Typically, previous work use a start symbol \u201c</s>\u201d to generation the first word (Sutskever et al., 2014).", "startOffset": 80, "endOffset": 104}, {"referenceID": 6, "context": "1 RNN encoder-decoder Typically, a sequence to sequence model consists of two parts: encoder and decoder, both of which are often implemented using a family of RNN, such as GRU (Cho et al., 2014a; Chung et al., 2014) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al.", "startOffset": 177, "endOffset": 216}, {"referenceID": 10, "context": ", 2014) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2000; Graves, 2012a), so a seq2seq model is also called RNN encoder-decoder architecture.", "startOffset": 17, "endOffset": 85}, {"referenceID": 1, "context": "Considering that vector may not be able to contain all the useful information of the source sequence, thus becoming a bottleneck of the model,Bahdanau et al. (2014) add the attention mechanism to improve the Seq2Seq\u2019s performance.", "startOffset": 142, "endOffset": 165}, {"referenceID": 16, "context": "First, the decoder RNN is essentially a language model (Mikolov et al., 2010), which use the previous predicted words to predict a new word, from the perspective of probability, it learns a conditional probability of word that given last predicted words.", "startOffset": 55, "endOffset": 77}, {"referenceID": 26, "context": "While the start symbol and the first predicted word do not have such association, because most words can be put at the first position of a sequence, there is not a learnable conditional probability, so the result of taking a start symbol may cause the model prefer to predict some high frequency words, which is also observed during other conversation models using this architecture (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 383, "endOffset": 448}, {"referenceID": 23, "context": "While the start symbol and the first predicted word do not have such association, because most words can be put at the first position of a sequence, there is not a learnable conditional probability, so the result of taking a start symbol may cause the model prefer to predict some high frequency words, which is also observed during other conversation models using this architecture (Sordoni et al., 2015; Serban et al., 2015; Vinyals and Le, 2015).", "startOffset": 383, "endOffset": 448}, {"referenceID": 25, "context": "And to be compared with, a basic kind of Seq2Seq architecture for response generation called hybrid model proposed by Shang et al. (2015) was implemented.", "startOffset": 118, "endOffset": 138}, {"referenceID": 25, "context": "For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively.", "startOffset": 109, "endOffset": 129}, {"referenceID": 25, "context": "For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively. The data set contains one million pairs, and about 35 thousands words. It should be noted that compared with the data used by Shang et al. (2015), this crawled data is a one-to-one data set, one post is corresponding to exact one response.", "startOffset": 110, "endOffset": 289}, {"referenceID": 25, "context": "For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively. The data set contains one million pairs, and about 35 thousands words. It should be noted that compared with the data used by Shang et al. (2015), this crawled data is a one-to-one data set, one post is corresponding to exact one response. While in the Shang et al. (2015) paper, they crawled some one-to-many data from microblog, then distributed all the responses to the its post.", "startOffset": 110, "endOffset": 416}, {"referenceID": 25, "context": "For convenience, first sentence and the second sentence of one dialogue pair are denoted as post and response(Shang et al., 2015)respectively. The data set contains one million pairs, and about 35 thousands words. It should be noted that compared with the data used by Shang et al. (2015), this crawled data is a one-to-one data set, one post is corresponding to exact one response. While in the Shang et al. (2015) paper, they crawled some one-to-many data from microblog, then distributed all the responses to the its post. This is a creative way to build a big data set, while during our experiments, we found that the one-to-one data has a more rapid rate of convergence, so we created our own data set and trained models on it.Table 1 is an example of our data. As for the test set, considering that one of our evaluation method\u2013Bleu, which will be introduced in detail in next section, should has more than one reference for every candidate, our one-to-one data is not very suitable, so we select 100 posts and their corresponding responses in Shang et al. (2015)\u2019s data-set to build our test set.", "startOffset": 110, "endOffset": 1070}, {"referenceID": 25, "context": "The first one is a basic Seq2Seq model for dialogue called Hybird Model(denoted as HYP)(Shang et al., 2015) , the other is what we have proposed(denoted as LTS).", "startOffset": 87, "endOffset": 107}, {"referenceID": 6, "context": "The encoder and decoder were both implemented using the GRU (Cho et al., 2014a; Chung et al., 2014).", "startOffset": 60, "endOffset": 99}, {"referenceID": 3, "context": "After that, we used the beam search algorithm to search for the N-best result of response for one paritular sentence (Graves, 2012b; Boulanger-Lewandowski et al., 2013).", "startOffset": 117, "endOffset": 168}, {"referenceID": 3, "context": "The encoder and decoder were both implemented using the GRU (Cho et al., 2014a; Chung et al., 2014). and we set our model\u2019s parameters reference to Shang et al. (2015). The hidden size in the encoder was set to 1024.", "startOffset": 61, "endOffset": 168}, {"referenceID": 7, "context": "Until now, there is still not a uniform evaluation for response generation (Galley et al., 2015; Pietquin and Hastie, 2013; Schatzmann et al., 2005).", "startOffset": 75, "endOffset": 148}, {"referenceID": 22, "context": "Until now, there is still not a uniform evaluation for response generation (Galley et al., 2015; Pietquin and Hastie, 2013; Schatzmann et al., 2005).", "startOffset": 75, "endOffset": 148}, {"referenceID": 19, "context": "to achieve that goal, we use two metrics: for one hand, we employed the wildly used automatic evaluation method\u2013blue (Papineni et al., 2002) in the area of machine translation, for the other hand, we employed the human annotation method.", "startOffset": 117, "endOffset": 140}, {"referenceID": 26, "context": "which is proved to agree well with human judgement on response generation task (Sordoni et al., 2015; Li et al., 2015).", "startOffset": 79, "endOffset": 118}, {"referenceID": 15, "context": "which is proved to agree well with human judgement on response generation task (Sordoni et al., 2015; Li et al., 2015).", "startOffset": 79, "endOffset": 118}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014).", "startOffset": 180, "endOffset": 203}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al.", "startOffset": 181, "endOffset": 298}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al.", "startOffset": 181, "endOffset": 322}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al.", "startOffset": 181, "endOffset": 341}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al. (2014). Besides, Cho et al.", "startOffset": 181, "endOffset": 359}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al. (2014). Besides, Cho et al. (2014b) added the vector c as an extra input to every time step of the decoder, by doing like this, all the steps not only the first one, can get full information of the context vector.", "startOffset": 181, "endOffset": 388}, {"referenceID": 1, "context": "1 Sequence to sequence for Machine Translation Using the sequence to sequence model, neural machine translation has already got a comparable performance to the traditional methods (Bahdanau et al., 2014). As far as we know, it was first introduced into this area by Kalchbrenner and Blunsom (2013),Sutskever et al. (2014),Cho et al. (2014b),Gao et al. (2014). Besides, Cho et al. (2014b) added the vector c as an extra input to every time step of the decoder, by doing like this, all the steps not only the first one, can get full information of the context vector. Furthermore, Bahdanau et al. (2014) proposed a novel method to calculated a weighted sum of all the annotations of the encoder.", "startOffset": 181, "endOffset": 602}, {"referenceID": 24, "context": "2 Sequence to sequence in Response Generation General speaking, dialogue systems can be sorted into two classes (Serban et al., 2016): goal-driven represented by systems Ga\u0161i\u0107 et al.", "startOffset": 112, "endOffset": 133}, {"referenceID": 7, "context": ", 2016): goal-driven represented by systems Ga\u0161i\u0107 et al. (2013) and non-goal-driven systems.", "startOffset": 44, "endOffset": 64}, {"referenceID": 7, "context": ", 2016): goal-driven represented by systems Ga\u0161i\u0107 et al. (2013) and non-goal-driven systems. The neural networks methods are mainly used in the later, because a large scale of data is more easily to get in that area. Ritter et al. (2011) first combine micro-blogging data with the generative probabilistic models, then Shang et al.", "startOffset": 44, "endOffset": 238}, {"referenceID": 7, "context": ", 2016): goal-driven represented by systems Ga\u0161i\u0107 et al. (2013) and non-goal-driven systems. The neural networks methods are mainly used in the later, because a large scale of data is more easily to get in that area. Ritter et al. (2011) first combine micro-blogging data with the generative probabilistic models, then Shang et al. (2015) used this type of data on the Seq2Seq to build a short conservation machine.", "startOffset": 44, "endOffset": 339}, {"referenceID": 7, "context": ", 2016): goal-driven represented by systems Ga\u0161i\u0107 et al. (2013) and non-goal-driven systems. The neural networks methods are mainly used in the later, because a large scale of data is more easily to get in that area. Ritter et al. (2011) first combine micro-blogging data with the generative probabilistic models, then Shang et al. (2015) used this type of data on the Seq2Seq to build a short conservation machine. followed by Serban et al. (2016), who came up with the Hierarchical Nerual Network model, aiming to model the utterances and interactive structure to build a multi-round dialogue system.", "startOffset": 44, "endOffset": 449}, {"referenceID": 1, "context": "At the same time, Banchs (2012) proposed methods using a different type of data,the movie dialogue.", "startOffset": 18, "endOffset": 32}, {"referenceID": 0, "context": "Based on that, Ameixa et al. (2014) find using the retrieal system and movie subtitles can also improvement the performance.", "startOffset": 15, "endOffset": 36}], "year": 2016, "abstractText": "The sequence to sequence architecture is widely used in the response generation and neural machine translation to model the potential relationship between two sentences. It typically consists of two parts: an encoder that reads from the source sentence and a decoder that generates the target sentence word by word according to the encoder\u2019s output and the last generated word. However, it faces to the \u201ccold start\u201d problem when generating the first word as there is no previous word to refer. Existing work mainly use a special start symbol \u201c</s>\u201d to generate the first word. An obvious drawback of these work is that there is not a learnable relationship between words and the start symbol. Furthermore, it may lead to the error accumulation for decoding when the first word is incorrectly generated. In this paper, we proposed a novel approach to learning to generate the first word in the sequence to sequence architecture rather than using the start symbol. Experimental results on the task of response generation of short text conversation show that the proposed approach outperforms the state-of-the-art approach in both of the automatic and manual evaluations.", "creator": "LaTeX with hyperref package"}}}