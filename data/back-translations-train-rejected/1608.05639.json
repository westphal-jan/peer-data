{"id": "1608.05639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Aug-2016", "title": "Operator-Valued Bochner Theorem, Fourier Feature Maps for Operator-Valued Kernels, and Vector-Valued Learning", "abstract": "This paper presents a framework for computing random operator-valued feature maps for operator-valued positive definite kernels. This is a generalization of the random Fourier features for scalar-valued kernels to the operator-valued case. Our general setting is that of operator-valued kernels corresponding to RKHS of functions with values in a Hilbert space. We show that in general, for a given kernel, there are potentially infinitely many random feature maps, which can be bounded or unbounded. Most importantly, given a kernel, we present a general, closed form formula for computing a corresponding probability measure, which is required for the construction of the Fourier features, and which, unlike the scalar case, is not uniquely and automatically determined by the kernel. We also show that, under appropriate conditions, random bounded feature maps can always be computed. Furthermore, we show the uniform convergence, under the Hilbert-Schmidt norm, of the resulting approximate kernel to the exact kernel on any compact subset of Euclidean space. Our convergence requires differentiable kernels, an improvement over the twice-differentiability requirement in previous work in the scalar setting. We then show how operator-valued feature maps and their approximations can be employed in a general vector-valued learning framework. The mathematical formulation is illustrated by numerical examples on matrix-valued kernels.", "histories": [["v1", "Fri, 19 Aug 2016 15:34:43 GMT  (48kb)", "http://arxiv.org/abs/1608.05639v1", "31 pages"]], "COMMENTS": "31 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["ha quang minh"], "accepted": false, "id": "1608.05639"}, "pdf": {"name": "1608.05639.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["minh.haquang@iit.it"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.05 639v 1 [cs.L G] 19 A"}, {"heading": "1. Introduction", "text": "Much work has been done lately on these kernels and their associated RKHS functions, both theoretically and practically, see e.g. (Micchelli and Pontil, 2005; Carmeli et al., 2011; Reisert and Burkhardt, 2007; Caponnetto et al., 2008; Brouard et al., 2011; Kadri et al., 2011; Minh and Sindhwani et al., 2012; Sindhwani et al.) While rich in theory and potentially powerful applications, one of the most important challenges in the application of operator-rated kernels is that they are applied computationally to large datasets."}, {"heading": "2. Background", "text": "Throughout the work, we are working with shift-invariant positive definite nuclei K to Rn \u00b7 Rn = > function (so that K (x, t) = k (x \u2212 t). < & \u2212 K for any function k: Rn \u2192 R, which is then positively defined as a positive definite function on Rn.Random Fourier functions on Rn if and only if it is the Fourier transformation of a finite, positive measurement on Rn, which isk (x) = \u00b5 (x) = Rn e \u2212 i < i < p > function k on Rn positively defined. (1) For our purposes, we are only looking at the real-rated setting on k."}, {"heading": "2.1 Operator-Valued Feature Maps for Operator-Valued Kernels", "text": "Let FK be a separable Hilbert space and L (W, FK) represent the Banach space of all limited linear operators mapped from W to FK. < K (\"we\") a feature card for K with corresponding feature space. < K (\"we\") a feature card for K. < K (\"we\") a feature card for K. < K (\"we\") a feature card for K. < K (\"we\") a feature card for K. < K (\"we\") a feature card for K. < K (\"we\") a feature card for K. < K (\"we\") a feature card for K. < K (\"we\") a feature card for K. < K (\"we\") a feature card for K. < K (\"we\") a feature card. < K (\"we\" we \") a feature card. < K (\" we \""}, {"heading": "3. Random Operator-Valued Feature Maps", "text": "We will now introduce the generalization of the random Fourier function board from the scalar setting to the operator-weighted setting. Let's start by checking Bochner's theorem in the operator-weighted setting in Section 3.1, which immediately leads to the formal creation of the Fourier function boards in Section 3.2. As we said in the operator-weighted setting, we must explicitly construct the required probability value, individually for some specific cores in Section 3.3 and for a general kernel in Section 3.4."}, {"heading": "3.1 Operator-Valued Bochner Theorem", "text": "The user version of Bochner's theorem that we present here is from (Neeb, 1998), see also (Falb, 1969; Carmeli et al., 2010) Let's be a divisible Hilbert space in this section. Let L (H) be the Banach space of limited linear operators on H, sym (H), positive operators on H, sym (H) on H, sym (H) on H, sym + (H) on H, sym + (H) on H, sym (H) on H, positive operators on H, it is said that the user class of A (H), if the user class of K (A) 1 < ek (A) 1 / 2ek > < the user base for each orthonorthonormal base {ek} k = 1 in H. If A (H), then the user class of A (H), then the user class of A)."}, {"heading": "3.2 Formal Construction of Approximate Fourier Feature Maps", "text": "Assuming that we currently have a pair that fulfills the factorization in Eq. (21), then Eq. (20) takes the formK. (x, t). (22) Leave {\u03c9j} Dj = 1, D N, D points in Rn. (<) Leave the K (x, t) by the empirical sumK. (x, t) = k D (x \u2212 t) = 1DD. l = 1cos (< l, x \u2212 t >)."}, {"heading": "3.3 Probability Measure and Feature Map Construction in Some Special Cases", "text": "We will first consider several examples of operator-weighted kernels resulting from scalar-weighted kernels. (For these examples, both the Sym + (H) -weighted metrics and the probability metrics are derived from the corresponding probability metrics (see below). An important aspect we note is that the approach to calculating the probability measurement in this section is specific to each kernel and cannot be generalized. We will return to these examples in the general setting of Section 3.4, where we will consider a general formula for calculating a general kernel k.Example 1 (separable kernels), where the operator-weighted positive function k has (x)."}, {"heading": "3.4 First Main Result: Probability Measure Construction in the General Case", "text": "We now show how to construct a general k-measurement definition (under suitable assumptions). (Furthermore, we show that the corresponding function card is limited in the sense that it is a limited function (see the exact specification in Corollary 10).Proposition 8 Let K: Rn \u2192 L (H) be at ultraweak continuous displacement invariant Positive value definition. Let us define the unique Sym + (H) -weighted measurement variable in Corollary 10).Proposition 8 Let K: Rn \u2192 L (H) be at ultraweak continuous displacement invariant Positive value definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-Definition-"}, {"heading": "3.5 Second Main Result: Uniform Convergence Analysis", "text": "I would like to thank all those who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me throughout my career, who have supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who supported me through my struggles, who helped me through my struggles, who helped me through my struggles, who helped me through my struggles, who helped me through my struggles."}, {"heading": "4. Vector-Valued Learning with Operator-Valued Feature Maps", "text": "After discussing operator-weighted features and their random approximations, we now show how they can be applied in the context of learning in RKHS of vector-weighted functions. Let W, Y be two Hilbert spaces, C: W \u2192 Y be a limited operator, K: Rn \u00b7 Rn \u2192 L (W) be a positive definitive defined kernel with the corresponding RKHS functions, and V be a convex loss function. Let's consider the following general learning problem (Minh et al., 2016) fz, \u03b3 = argminf. HK 1ll, i = 1V (yi, Cf (xi)) + \u03b3A | f | 2HK +.M f > Wu > Wu + l. (67) Here z = (y) = (xi, yi) HK 1ll."}, {"heading": "5. Numerical Experiments", "text": "In this section, we report on several experiments to illustrate the numerical properties of the characteristics maps just created = q. Since the characteristics of the separable characteristics maps follow directly from those of the corresponding scalar nuclei, we mainly focus on the curl-free and div-free distribution in R3, which is used in the calculation of the approximate kernel [\u2212 1, 1] 3. On this basis, we first calculated the kernel-free and div-free kernel cards induced by the Gaussian kernel. (33) We compiled the characteristic maps with Iq. (25) For the curl-free kernel-free kernel construction, we become 0.4."}, {"heading": "6. Conclusion and Future Work", "text": "We have presented a framework for creating random operator-evaluated feature maps for cores with the operator-evaluated version of Bochner's theorem. We have shown that due to the non-uniqueness of the probability measure in this setting, in general many feature maps can be calculated that may be unlimited or limited. Under certain conditions that are fulfilled for many common cores such as curl-free and div-free cores, limited feature maps can always be calculated. Subsequently, we showed the uniform convergence of the approximate core in the Hilbert Schmidt standard with the limited maps and enhanced previous results in the scalar setting. Finally, we showed how a general vector-evaluated learning formulation can be expressed in the form of feature sketches and demonstrated them experimentally. A comprehensive empirical evaluation of the proposed formulation is left to future work."}, {"heading": "Appendix A. Proofs of Main Mathematical Results", "text": "For a function f = > HK of the form f = = > K = > K = > K = < K = < K (x) < K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x), K (x, K (x), K (x), K (x), K (x, K (x), K (x), K (x, K (x), K (x), K (x, K (x), K (x, K (x), K (x), K (x, K (x), K (x, K (x), K (x), K (x, K (x), K (x), K (x, K (x), K (x, K (x), K (x), K (x, K (x), K (x), K (x, K (x, K (x), K (x), K (x), K (x, K (x), K (x), K (x, K (x, K (x), K (x), K (x), K (x, < K = < K = < K)"}], "references": [{"title": "Universal multi-task kernels", "author": ["A. Caponnetto", "C. Micchelli", "M. Pontil", "Y. Ying"], "venue": null, "citeRegEx": "Caponnetto et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Caponnetto et al\\.", "year": 2011}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In NIPS,", "citeRegEx": "Rahimi and Recht.,? \\Q2007\\E", "shortCiteRegEx": "Rahimi and Recht.", "year": 2007}, {"title": "On the error of random Fourier features", "author": ["D. Sutherland", "J. Schneider"], "venue": "Neural Information Processing Systems,", "citeRegEx": "Sutherland and Schneider.,? \\Q2015\\E", "shortCiteRegEx": "Sutherland and Schneider.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "In the scalar setting, one of the most powerful approaches for scaling up kernel methods is Random Fourier Features (Rahimi and Recht, 2007), which applies Bochner\u2019s Theorem and the Inverse Fourier Transform to build random features that approximate a given shift-invariant kernel.", "startOffset": 116, "endOffset": 140}, {"referenceID": 1, "context": "The approach in (Rahimi and Recht, 2007) has been improved both in terms of computational speed (Le et al.", "startOffset": 16, "endOffset": 40}, {"referenceID": 2, "context": ", 2013) and rates of convergence (Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015).", "startOffset": 33, "endOffset": 96}, {"referenceID": 1, "context": "Thirdly, when restricting to the scalar setting, our convergence holds for differentiable kernels, which is an improvement over the hypothesis of (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015; Brault et al., 2016), which all require the kernels to be twice-differentiable.", "startOffset": 146, "endOffset": 254}, {"referenceID": 2, "context": "Thirdly, when restricting to the scalar setting, our convergence holds for differentiable kernels, which is an improvement over the hypothesis of (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015; Brault et al., 2016), which all require the kernels to be twice-differentiable.", "startOffset": 146, "endOffset": 254}, {"referenceID": 1, "context": ", 2016) and previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015).", "startOffset": 51, "endOffset": 138}, {"referenceID": 2, "context": ", 2016) and previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015).", "startOffset": 51, "endOffset": 138}, {"referenceID": 1, "context": "Random Fourier features for scalar-valued kernels (Rahimi and Recht, 2007).", "startOffset": 50, "endOffset": 74}, {"referenceID": 1, "context": "This is weaker than the assumptions in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015), which all require that \u222b Rn ||\u03c9||2d\u03c1(\u03c9) < \u221e, that is k is twice-differentiable.", "startOffset": 39, "endOffset": 126}, {"referenceID": 2, "context": "This is weaker than the assumptions in (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015), which all require that \u222b Rn ||\u03c9||2d\u03c1(\u03c9) < \u221e, that is k is twice-differentiable.", "startOffset": 39, "endOffset": 126}, {"referenceID": 1, "context": "Furthermore, as with previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015), the convergence in (Brault et al.", "startOffset": 60, "endOffset": 147}, {"referenceID": 2, "context": "Furthermore, as with previous results in the scalar setting (Rahimi and Recht, 2007; Sutherland and Schneider, 2015; Sriperumbudur and Szab\u00f3, 2015), the convergence in (Brault et al.", "startOffset": 60, "endOffset": 147}], "year": 2016, "abstractText": "This paper presents a framework for computing random operator-valued feature maps for operator-valued positive definite kernels. This is a generalization of the random Fourier features for scalar-valued kernels to the operator-valued case. Our general setting is that of operator-valued kernels corresponding to RKHS of functions with values in a Hilbert space. We show that in general, for a given kernel, there are potentially infinitely many random feature maps, which can be bounded or unbounded. Most importantly, given a kernel, we present a general, closed form formula for computing a corresponding probability measure, which is required for the construction of the Fourier features, and which, unlike the scalar case, is not uniquely and automatically determined by the kernel. We also show that, under appropriate conditions, random bounded feature maps can always be computed. Furthermore, we show the uniform convergence, under the Hilbert-Schmidt norm, of the resulting approximate kernel to the exact kernel on any compact subset of Euclidean space. Our convergence requires differentiable kernels, an improvement over the twice-differentiability requirement in previous work in the scalar setting. We then show how operator-valued feature maps and their approximations can be employed in a general vector-valued learning framework. The mathematical formulation is illustrated by numerical examples on matrix-valued kernels.", "creator": "LaTeX with hyperref package"}}}