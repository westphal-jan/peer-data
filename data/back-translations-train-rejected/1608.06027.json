{"id": "1608.06027", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Surprisal-Driven Feedback in Recurrent Networks", "abstract": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.39 BPC.", "histories": [["v1", "Mon, 22 Aug 2016 01:42:45 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v1", "v1, ICLR format"], ["v2", "Fri, 2 Sep 2016 17:26:02 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v2", "v2, ICLR format, added new results (feedback + zoneout)"], ["v3", "Mon, 5 Sep 2016 04:42:02 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v3", "ICLR format, added new results (feedback + zoneout)"], ["v4", "Wed, 19 Oct 2016 04:32:46 GMT  (188kb,D)", "http://arxiv.org/abs/1608.06027v4", "ICLR 2017 submission, fixed some equations"]], "COMMENTS": "v1, ICLR format", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["kamil m rocki"], "accepted": false, "id": "1608.06027"}, "pdf": {"name": "1608.06027.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Kamil Rocki"], "emails": ["kmrocki@us.ibm.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "Based on human performance on the same task, an important ingredient missing in modern variants of recurrent networks is thought to be top-down feedback. Despite their existence, it is not entirely clear how mammalian brains could implement such a mechanism. It is important to understand what kind of top-down interaction contributes to improved predictive skills to address more difficult AI problems that require an interpretation of deeper contextual information. Furthermore, it could provide clues to what makes human cognitive skills so unique. Existing approaches that take top-down feedback into account in neural networks focus primarily on stacked layers of neurons in which superordinate representations represent a top-down signal source. In this paper, we suggest that the discrepancy between recent predictions and observations could effectively be used as a feedback signal that influences further predictions. It is very common to apply such a discrepancy to the data during the learning phase, but not to use it as a minimization signal during the learning phase of the long flow phase."}, {"heading": "1.1 SUMMARY OF CONTRIBUTIONS", "text": "The most important contributions of this paper are: \u2022 the introduction of a novel method to incorporate the latest false prognosis as an additional input signal \u2022 the extension of modern capabilities in character-level text modeling using the Hutter Wikipedia dataset."}, {"heading": "1.2 RELATED WORK", "text": "An important difference between the architecture proposed here and its architecture is the source of the feedback signal. In GF-RNN it is assumed that there are higher-level layers of representation that form the source of feedback. Xiv: 160 8.06 027v 1 [cs.L G] 22 Aug 201 6On the other hand, feedback here depends directly on the discrepancy between past predictions and current observations and even functions within a single layer. Another related concept is conductor networks (Rasmus et al., 2015), where top-down connections contribute to improved semi-monitored learning performance."}, {"heading": "2 FEEDBACK: MISPREDICTION-DRIVEN PREDICTION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 NOTATION", "text": "Throughout the section the following notation is used: x - inputs h - hidden units y - outputs p - output probabilities (normalized y) s - surprise value t - time step W - feed x \u2192 h connection matrix U - recursive h \u2192 h connection matrix V - feedback s \u2192 h connection matrix S - shortened BPTT length M - number of inputs N - number of hidden units \u00b7 denotes matrix multiplication as elementary multiplication \u03c3 (\u00b7), tanh (\u00b7) - elementary nonlinearity \u03b4x = 20s E \u2202 xIn the case of LSTM the following concatenated representations are used: gt = itftot ut b = b i bf bo bu U = U i Uf Uo Uu W = W i Wf Where Wu V = V i Vf Vo Vu (1)"}, {"heading": "2.2 SIMPLE RNN WITHOUT FEEDBACK", "text": "First, we show a simple recursive neural network architecture without feedback, which serves as a basis for demonstrating our approach. It is illustrated in Fig. 2 and formulated as follows: ht = tanh (W \u00b7 xt + U \u00b7 ht \u2212 1 + b) (2)"}, {"heading": "2.3 FEEDBACK AUGMENTED RECURRENT NETWORKS", "text": "Figure 3 shows the main idea of a surprisingly driven feedback in recurrent networks. In addition to the feedbacks W and U, we have added an additional matrix V. Another input signal, V \u00b7 st, is taken into account when updating hidden states of the network. We suggest that the discrepancy st between the most recent predictions pt \u2212 1 and observations xt could effectively be used as a feedback signal that influences further predictions. Such information is normally used as an error signal during the learning phase, but not during inference. Our hypothesis is that it is an important source of information that can be used during the inference phase, and that it brings benefits in the form of improved generalization capability. Figure 1 shows examples of feedback signals that are taken into account. If the feedback is surprisingly close to zero, the sum of the input signals is the same as in a typical RNN. Next subsections provide a mathematical description of the time of the feedback architecture (through feedback architecture in relation to propagation)."}, {"heading": "2.4 FORWARD PASS", "text": "Set h0, c0 to zero, and p0 to an even distribution, or transfer the last state to the full BPTT."}, {"heading": "2.5 BACKWARD PASS", "text": "For t = S-1: Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Et = Predictions Et = Predictions Et = Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Predictions Et = Et = Predictions Et = Predictions Et = Predictions Et = Et"}, {"heading": "3 EXPERIMENTS", "text": "We conducted experiments with the enwik8 dataset, which represented the first 108 bytes of the English Wikipedia dump (with all the additional symbols present in XML), also known as the Hutter Price Challenge dataset, 1. The first 90% of each corpus was used for training, the next 5% for validation, and the last 5% for reporting test accuracy. In each iteration sequence with a length of 10,000 were randomly selected. The learning algorithm used was Adagrad1 with a learning rate of 0.001. The weights were initialized using the so-called Xavier initialization Glorot & Bengio (2010). The sequence length for BPTT was 100 and the batch size 128, the states were adopted for the entire sequence of 10,000, emulating the full BPTT. Forget bias was originally set to 1. Other parameters were set to zero. The algorithm was written to GUDA + 10 and GUDA + 10 and GTX in days."}, {"heading": "4 SUMMARY", "text": "We have introduced a network architecture that takes advantage of the temporal nature of the data and monitors the discrepancy between predictions and observations.This prediction error information, also known as surprises, is used in new guesses. We have shown that the combination of commonly used feedback signals and such feedback signals improves the generalization capacities of the long short term memory network.It surpasses other stochastic and completely deterministic approaches to prediction at the enwik8 character level, reaching 1.39 BPC.1 with a modification that only takes into account the most recent window of gradient updates1http: / / mattmahoney.net / dc / text.html 2our"}, {"heading": "5 FURTHER WORK", "text": "It is not yet clear what the feedback should really consist of and how it should interact with lower-level neurons (additive, multiplicative, or some other type of connection), and further improvements may be possible through additional regularization, or by incorporating sparseness in order to improve the unbundling of sources of variation in temporal data."}, {"heading": "ACKNOWLEDGEMENTS", "text": "This work was supported in part by the Defense Advanced Research Projects Agency (DARPA)."}], "references": [{"title": "Gated feedback recurrent neural networks", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1502.02367,", "citeRegEx": "Chung et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS10). Society for Artificial Intelligence and Statistics,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves"], "venue": "CoRR, abs/1507.01526,", "citeRegEx": "Kalchbrenner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Semisupervised learning with ladder", "author": ["Antti Rasmus", "Harri Valpola", "Mikko Honkala", "Mathias Berglund", "Tapani Raiko"], "venue": "network. CoRR,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Recurrent memory array structures", "author": ["Kamil Rocki"], "venue": "arXiv preprint arXiv:1607.03085,", "citeRegEx": "Rocki.,? \\Q2016\\E", "shortCiteRegEx": "Rocki.", "year": 2016}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey Hinton"], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11),", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Backpropagation through time: what does it do and how to do it", "author": ["P. Werbos"], "venue": "In Proceedings of IEEE,", "citeRegEx": "Werbos.,? \\Q1990\\E", "shortCiteRegEx": "Werbos.", "year": 1990}, {"title": "On multiplicative integration with recurrent neural networks", "author": ["Yuhuai Wu", "Saizheng Zhang", "Ying Zhang", "Yoshua Bengio", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1606.06630,", "citeRegEx": "Wu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "Recurrent highway networks, 2016", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutnk", "Jrgen Schmidhuber"], "venue": null, "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "One such architecture is Gated-Feedback RNN (Chung et al., 2015).", "startOffset": 44, "endOffset": 64}, {"referenceID": 4, "context": "Another related concept is Ladder Networks (Rasmus et al., 2015), where top-down connections contribute to improved semi-supervised learning performance.", "startOffset": 43, "endOffset": 64}, {"referenceID": 7, "context": "Next subsections provide mathematical description of the feedback architecture in terms of forward and backward passes for the Back Propagation Through Time (BPTT) (Werbos, 1990) algorithm.", "startOffset": 164, "endOffset": 178}, {"referenceID": 6, "context": "BPC mRNN(Sutskever et al., 2011) 1.", "startOffset": 8, "endOffset": 32}, {"referenceID": 0, "context": "60 GF-RNN (Chung et al., 2015) 1.", "startOffset": 10, "endOffset": 30}, {"referenceID": 3, "context": "58 Grid LSTM (Kalchbrenner et al., 2015) 1.", "startOffset": 13, "endOffset": 40}, {"referenceID": 8, "context": "45 MI-LSTM (Wu et al., 2016) 1.", "startOffset": 11, "endOffset": 28}, {"referenceID": 9, "context": "44 Recurrent Highway Networks (Zilly et al., 2016) 1.", "startOffset": 30, "endOffset": 50}, {"referenceID": 5, "context": "42 Array LSTM (Rocki, 2016) 1.", "startOffset": 14, "endOffset": 27}], "year": 2017, "abstractText": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper, we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.39 BPC.", "creator": "LaTeX with hyperref package"}}}