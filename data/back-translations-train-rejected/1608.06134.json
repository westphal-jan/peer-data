{"id": "1608.06134", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Median-Based Generation of Synthetic Speech Durations using a Non-Parametric Approach", "abstract": "This paper proposes a new approach to duration modelling for statistical parametric speech synthesis in which a recurrent statistical model is trained to output a phone transition probability at each timestep (acoustic frame). Unlike conventional approaches to duration modelling -- which assume that duration distributions have a particular form (e.g., a Gaussian) and use the mean of that distribution for synthesis -- our approach can in principle model any distribution supported on the non-negative integers. Generation from this model can be performed in many ways; here we consider output generation based on the median predicted duration. The median is more typical (more probable) than the conventional mean duration, is robust to training-data irregularities, and enables incremental generation. Furthermore, a frame-level approach to duration prediction is consistent with a longer-term goal of modelling durations and acoustic features together. Results indicate that the proposed method is competitive with baseline approaches in approximating the median duration of held-out natural speech.", "histories": [["v1", "Mon, 22 Aug 2016 11:52:55 GMT  (60kb,D)", "http://arxiv.org/abs/1608.06134v1", "7 pages, 1 figure -- Under review at IEEE Workshop on Spoken Language Technology (SLT 2016)"], ["v2", "Fri, 11 Nov 2016 13:24:44 GMT  (61kb,D)", "http://arxiv.org/abs/1608.06134v2", "7 pages, 1 figure -- Accepted for presentation at IEEE Workshop on Spoken Language Technology (SLT 2016)"]], "COMMENTS": "7 pages, 1 figure -- Under review at IEEE Workshop on Spoken Language Technology (SLT 2016)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["srikanth ronanki", "oliver watts", "simon king", "gustav eje henter"], "accepted": false, "id": "1608.06134"}, "pdf": {"name": "1608.06134.pdf", "metadata": {"source": "CRF", "title": "MEDIAN-BASED GENERATION OF SYNTHETIC SPEECH DURATIONS USING A NON-PARAMETRIC APPROACH", "authors": ["Srikanth Ronanki", "Oliver Watts", "Simon King", "Gustav Eje Henter"], "emails": ["srikanth.ronanki@ed.ac.uk"], "sections": [{"heading": null, "text": "Index Terms - Text-to-Speech, Speech Synthesis, Permanent Modelling, Nonparametric Models, LSTMs."}, {"heading": "1. INTRODUCTION", "text": "This paper represents a new approach to the modeling and generation of language segments, a feature of language that contributes to the perception of its rhythm. Creating appropriate rhythms and melodies is a challenging but indispensable step in the creation of natural-sounding synthetic languages in recent years [1, 2]. As we expect, there have been lengthy improvements in statistical parameter synthesis (SPSS), especially in the adoption of deep and recurring machine techniques in recent years [1, 3]. The dependencies are important for the prosodic structure of speech, recurring models such as LSTMs are well suited for the prosodic sequence of problem modeling."}, {"heading": "2. BACKGROUND", "text": "In this section, we give a brief overview of how speech durations were generated in synthetic speech, with a particular emphasis on the use of statistical methods for this task, which forms the context for the method proposed in Section 3."}, {"heading": "2.1. A Brief Review of Modelling and Generating Durations", "text": "This year, there will be no significant change in the first half of the year."}, {"heading": "2.2. Median-Based Generation", "text": "All methods in Table 1 that predict distribution of duration automatically support a wide variety of methods for generating performance durations from this distribution. While in principle natural language is a random sample drawn from the true duration distribution, the output naturalness of scanning methods in language synthesis has been shown to be poor unless high-precision models are used [15]. As a result, most SPSS systems use deterministic generation methods that are synonymous in practice with generation of mean duration. We are looking at a scheme in which the durations generated during synthesis are based on the median - rather than the conventional mean - of predicted duration distribution. Since we no longer assume that the duration distribution is symmetrical, the median value will typically differ from the means of distribution."}, {"heading": "2.3. Frame-Level Duration Prediction", "text": "Watts et al. [20] described a common model of duration and speech parameters, in which a deep neural network was trained to simultaneously output acoustic parameters and a 5-dimensional (telephone) duration vector for each image. Synthesis with this type of network leads to a chicken-and-egg problem in which state durations are both input and output of the sequence prediction network. This has been solved by iteratively refining state duration predictions over multiple iterations, which is slow. In our proposed method, a single pass is sufficient for duration generation. Moreover, despite the granularity of the approach in [20] it is not easy to identify a likely interpretation of their schemes. A general advantage of duration predictions at the image level is that they can be combined with the (traditionally different) prediction of acoustic characteristics that are important for pitch and pitch."}, {"heading": "3. THEORY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Preliminaries", "text": "Let p {1,.., P} be a telephone index, and let t {1,.., T} be an index in frames. Further, let Dp - a random variable - be the duration of the phone p, and let dp, Z > 0 be a result of the Dp.1Natural voice telephones have different permanent distributions depending on the input text. In TTS, the properties of the distribution Dp are predicted from contextual linguistic attributes lp extracted from the text by the synthesizer frontend. Specifically, permanent modeling is the task of mapping the sequence of linguistic attributes (l1,.., lP) to a sequence of predicted permanent distributions (D1,..., DP). Permanent generation, on the other hand, is the task of mapping the figure (l1,.., lP) to a sequence of generated permanent distributions (D1,.., DP) to a sequence of predicted permanent distributions (l1, DP)."}, {"heading": "3.2. Conventional Duration Modelling", "text": "In statistical synthesizers that generate durations at the state or telephony level, the conventional approach is to assume that the durations Dp follow some parametric family fD, with a parameter representing the number of degrees of freedom (N is the number of degrees of freedom), i.e. P (Dp = d) = fD (d; \u03b8p). (1) The prediction of the distribution Dp is then reduced to the stochastic regression problem of predicting the distribution parameter successp from the telephone level parallel input-output datasetDp = ((l1,.., lP), (d1,., dP). (2) 1In many TTS systems, Dp is a vector of the per-phone-state durations, but we limit ourselves to the telephone-level predictions in this work; a state-based formulation is a simple extension."}, {"heading": "3.3. Non-Parametric Duration Modelling", "text": "We will now describe a scheme that, unlike conventional phonelevel approaches with parametric families fD (d; \u03b8 =), is able to model and predict arbitrary duration distributions for D (limited only by predictions of the machine learning method used). The key idea is to make predictions at the frame level at which it was mapped. We can then define a frame sequence, the Lt of the linguistic sequence Lt = (l1,. lt) = (lp (1),., lp (t)) to t, which is constant for all frames within each phone. For Brevity, we will write p for the current phone."}, {"heading": "3.4. From Transitions to Median Durations", "text": "After we have defined a telephone duration distribution in (11) and trained a model to predict this distribution from data, we have to take into account how durations d are to be generated predicted from this distribution. As discussed in Section 2.2, the sample typically results in poor naturalness, while the middle generation is sensitive to the tails of the distribution and unsuitable for the sequential generation. On the other hand, it is straightforward to derive the right tail probability of the telephone duration distribution induced by the values of the median duration observed to date. asP (Dp > nt | Lt) = t0 + nt \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s \u00b2 s = s \u00b2 s = s \u00b2 s = s \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 t \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s"}, {"heading": "3.5. Adding External Memory", "text": "In the proposal described so far, progress through the current phone is tracked exclusively through the internal state of the predictor used (ct for an LSTM), in contrast to the approach used by the hidden Semi-Markov models in HTS, which achieved more general distributions of duration than normal HMMs by maintaining an external counter for the number of frames spent in each state and using it to calculate the transition probability. However, nothing prevents us from adding these extended features Lt., which count the frame number within the current phone, as input into the neural network x (\u00b7 W), which predicts the transition probabilities at the frame level, in addition to the regular, linguistic features Lt. We call these extended features l \u2032 t and L \u2032 t = ([l '1, n1], as input frames."}, {"heading": "4. EXPERIMENTAL SET-UP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data", "text": "As an initial investigation into the feasibility of our frame-level duration prediction approach, we conducted some experiments with a language database created for the Blizzard Challenge 2016.2 The database - provided to the Challenge by Usborne Publishing Ltd. - consists of the speeches and texts from 50 children's audiobooks spoken by a British speaker. We used a segmentation of the audiobooks provided to the Challenge by another Challenge3 and kindly2http: / / www.synsig.org / index.php / Blizzard Challenge 2016 3Innoetics: https: / / www.innoetics.com, which were provided to other participants. The total duration of the audio was approximately 4.33 hours after the segmentation. For the work presented here, 4% of the data was set aside as a test set. The test set consists of three complete short stories: Goldilocks and the Three Bears, The Boy Who Called the Wolf and The Enormous Tournament, with a total durance of approximately 10 minutes."}, {"heading": "4.2. Feature extraction", "text": "Festvox'ehmm [24] was used to insert pauses in annotated telephone sequences based on acoustics to improve the accuracy of the enforced time periods. Each phone was then characterized by a vector of 481 text-based binary and numeric characteristics: These characteristics are a subset of the characteristics used in the public HTS demonstration to cluster decision trees [11]; numerical characteristics queried by these questions were used directly where possible; all inputs were normalized to the range [0.01, 0.99]; the phone duration used to train conventional baselines was achieved by counting frames in a phone. Unlike our previous work [19], sub-phone states were not used in DNN training nor in predictions, which is consistent with our longer-term goal of freeing ourselves from dependence on often arbitrary HMM partial tunings."}, {"heading": "4.3. System training", "text": "Four systems were trained: two phone-level predictors (which we identify as Phone-DNN and Phone-LSTM) to provide benchmarks and evaluate the effects of recurring modeling on long-term prediction, and two experimental systems to implement the new idea (which we identify as Frame-LSTM-I and Frame-LSTM-E)."}, {"heading": "4.3.1. Phone-DNN and Phone-LSTM", "text": "Phone-DNN is a feedback DNN with six layers of 1024 nodes each. The hidden nodes used tanh activation and the output was linear. Phone-LSTM was configured with five feedback layers of 1024 nodes each and a final unidirectional SLSTM [25] layer consisting of 512 nodes."}, {"heading": "4.3.2. Frame-LSTM-I and Frame-LSTM-E", "text": "Both proposed systems used the same architecture as PhoneLSTM, but were - unlike the base systems - trained with 1 data point per frame instead of per phone. Targets presented in the training were 0.0 for non-phone end frames and 1.0 for phone end frames. Frame LSTM-I only took the vector representing the current phone; at runtime, it was therefore necessary to rely on its internal memory (hidden state and LSTM cell state) to determine the probability of phone transition in each given frame. Frame LSTM-I inputs are identical for all frames in a given phone. Frame LSTM-E therefore implemented the idea described in Section 3.5: Inputs encoding the current phone context are augmented by a frame counter indicating the number of frames that have passed since the phone started. Frame LSTM-E can therefore rely on this external memory in addition to the current memory if there is as much internal memory as there is."}, {"heading": "4.4. Synthesis", "text": "At synthesis time, the ehmm telephone sequences derived from the test data were used as input for each duration prediction model. This corresponds to the use of an oracle pause strategy, but does not provide the forecasters with any other acoustically derived information. Generating from Phone-DNN and Phone-LSTM is simple: one (effective mean) output is generated for each phone. Mean-based duration predictions were obtained from FrameLSTM-I and Frame-LSTM-E by repeatedly executing the models via frame-level inputs and using the technique explained in Section 3.4 and summarized in Pseudocode in Algorithm 1 to decide when the median was reached and thus when we should proceed to the next phone. By tracking the time spent in each phone, we obtain mean-based predictions of the duration of the phone using either external memory of the time expired in the current phone (Case-procedure-leave procedure procedure procedure procedure procedure procedure procedure procedure procedure-procedure-procedure-procedure-procedure-1), or by saying MASE-1 (MASE-1: MASE MASE procedure-1: 1) or MASE-1 (MASE-1: MASE-1): MASE-1: 1: MASE-1: MASE procedure procedure procedure-1: 1:"}, {"heading": "5. RESULTS", "text": "Table 2 indicates RMSE, the mean absolute error (MAE) and Pearson correlation, which is calculated by comparing the predicted runtimes with the reference provided. Silence segments are excluded from these calculations. First, the results for the benchmark systems show that PhoneLSTM Phone-DNN exceeds in all respects, confirming the importance of the recurrence of LSTM for duration modeling. Regarding the experimental systems, it can be seen that the mean-based predictions of the LSTM and DNN baselines clearly exceed the proposed methods for RMSE, but the gap is much smaller when it comes to MAE. Mean-based generation methods are likely to perform better than RMSE because - as discussed in Section 3.4 - the median is the theoretical minimizer of the MAE and DNN baselines, while the mean is the (R) MSE.The breakdown of the gap between the larger phonetic class results results results is logically a larger."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "We have described a new paradigm of long-term modeling with LSTMs that works in frame-level speech synthesis to predict duration. Experiments conducted with audiobook data have proven to be competitive compared to a base-level LSTM system at the telephone level. Future work will include joint modeling of duration and acoustic characteristics, as well as subjective evaluation of synthetic language."}, {"heading": "7. ACKNOWLEDGEMENTS", "text": "This research was supported by the EPSRC Programme Grant EP / I031022 / 1, Natural Speech Technology (NST) and the NST Research Data Collection is available at http: / / hdl.handle. net / 10283 / 786."}, {"heading": "8. REFERENCES", "text": "[1] Heiga Zen, Andrew Senior, and Mike Schuster, \"Statistical parametric speech synthesis, using deep neural networks,\" in Proc. ICASSP, 2013, pp. 7962-7966. [2] Zhizheng Wu, Cassia Valentini-Botinhao, Oliver Watts, and Simon King, \"Deep neural networks employing multi-task learning and stacked homes long-term memory recurrent memory recurrent output layer for speech synthesis,\" in Proc. ICASSP, 2015, pp. 4460-4464. [3] Heiga Zen and Has."}], "references": [{"title": "Statistical parametric speech synthesis using deep neural networks", "author": ["Heiga Zen", "Andrew Senior", "Mike Schuster"], "venue": "Proc. ICASSP, 2013, pp. 7962\u20137966.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep neural networks employing multi-task learning and stacked bottleneck features for speech synthesis", "author": ["Zhizheng Wu", "Cassia Valentini-Botinhao", "Oliver Watts", "Simon King"], "venue": "Proc. ICASSP, 2015, pp. 4460\u20134464.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis", "author": ["Heiga Zen", "Ha\u015fim Sak"], "venue": "Proc. ICASSP, 2015, pp. 4470\u20134474.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "A study of speaker adaptation for DNN-based speech synthesis", "author": ["Zhizheng Wu", "Pawel Swietojanski", "Christophe Veaux", "Steve Renals", "Simon King"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Sentence-level control vectors for deep neural network speech synthesis", "author": ["Oliver Watts", "Zhizheng Wu", "Simon King"], "venue": "Proc. Interspeech, 2015, pp. 2217\u20132221.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Review of text-to-speech conversion for English", "author": ["Dennis H. Klatt"], "venue": "J. Acoust. Soc. Am., vol. 82, no. 3, pp. 737\u2013793, 1987.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1987}, {"title": "Joint prosody prediction and unit selection for concatenative speech synthesis", "author": ["Ivan Bulyko", "Mari Ostendorf"], "venue": "Proc. ICASSP, 2001, vol. 2, pp. 781\u2013784.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Statistical parametric speech synthesis", "author": ["Heiga Zen", "Keiichi Tokuda", "Alan W. Black"], "venue": "Speech Commun., vol. 51, no. 11, pp. 1039\u20131064, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "An introduction to statistical parametric speech synthesis", "author": ["Simon King"], "venue": "Sadhana, vol. 36, no. 5, pp. 837\u2013852, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Hidden semi-markov model based speech synthesis", "author": ["Heiga Zen", "Keiichi Tokuda", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura"], "venue": "Proc. Interspeech, 2004, pp. 1393\u2013 1396.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "The HMM-based speech synthesis system (HTS) version 2.0", "author": ["Heiga Zen", "Takashi Nose", "Junichi Yamagishi", "Shinji Sako", "Takashi Masuko", "Alan Black", "Keiichi Tokuda"], "venue": "Proc. SSW, 2007, vol. 6, pp. 294\u2013299.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Syllable-level duration determination", "author": ["W. Nick Campbell"], "venue": "Proc. Eurospeech, 1989, pp. 2698\u20132701.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1989}, {"title": "A statistical model of duration control for speech synthesis", "author": ["K. Huber"], "venue": "Proc. EUSIPCO, 1990, pp. 1127\u20131130.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1990}, {"title": "Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices", "author": ["Heiga Zen", "Yannis Agiomyrgiannakis", "Niels Egberts", "Fergus Henderson", "Przemys\u0142aw Szczepaniak"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Measuring the perceptual effects of modelling assumptions in speech synthesis using stimuli constructed from repeated natural speech", "author": ["Gustav Eje Henter", "Thomas Merritt", "Matt Shannon", "Catherine Mayo", "Simon King"], "venue": "Proc. Interspeech, 2014, pp. 1504\u20131508.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "The mean, median, mode inequality and skewness for a class of densities", "author": ["H.L. MacGillivray"], "venue": "Australian Journal of Statistics, vol. 23, no. 2, pp. 247\u2013250, 1981.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1981}, {"title": "The mean, median, and mode of unimodal distributions: a characterization", "author": ["Sanjib Basu", "Anirban DasGupta"], "venue": "Theory Probab. Appl., vol. 41, no. 2, pp. 210\u2013223, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Speech parameter generation algorithms for HMM-based speech synthesis", "author": ["Keiichi Tokuda", "Takayoshi Yoshimura", "Takashi Masuko", "Takao Kobayashi", "Tadashi Kitamura"], "venue": "Proc. ICASSP, 2000, vol. 3, pp. 1315\u20131318.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Robust TTS duration modelling using DNNs", "author": ["Gustav Eje Henter", "Srikanth Ronanki", "Oliver Watts", "Mirjam Wester", "Zhizheng Wu", "Simon King"], "venue": "Proc. ICASSP, 2016, vol. 41, pp. 5130\u20135134.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "The NST\u2013GlottHMM entry to the Blizzard Challenge 2015", "author": ["Oliver Watts", "Srikanth Ronanki", "Zhizheng Wu", "Tuomo Raitio", "Antti Suni"], "venue": "Proc. Blizzard Challenge Workshop, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Prosody contour prediction with long short-term memory, bi-directional, deep recurrent neural networks", "author": ["Raul Fernandez", "Asaf Rendel", "Bhuvana Ramabhadran", "Ron Hoory"], "venue": "Proc. Interspeech, 2014, pp. 2268\u20132272.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "A template-based approach for speech synthesis intonation generation using LSTMs", "author": ["Srikanth Ronanki", "Gustav Eje Henter", "Zhizheng Wu", "Simon King"], "venue": "Proc. Interspeech, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Mel-cepstral distance measure for objective speech quality assessment", "author": ["Robert F. Kubichek"], "venue": "Proc. IEEE Pac. Rim Conf. Commun. Comput. Signal Process., 1993, vol. 1, pp. 125\u2013128.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1993}, {"title": "Sub-phonetic modeling for capturing pronunciation variations for conversational speech synthesis", "author": ["Kishore Prahallad", "Alan W. Black", "Ravishankhar Mosur"], "venue": "Proc. ICASSP, 2006, pp. I\u2013853\u2013I\u2013856.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Investigating gated recurrent networks for speech synthesis", "author": ["Zhizheng Wu", "Simon King"], "venue": "Proc. ICASSP, 2016, pp. 5140\u20135144.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Steady improvements have been made in statistical parametric speech synthesis (SPSS), in particular through the adoption of deep and recurrent machine-learning techniques in recent years [1, 2].", "startOffset": 187, "endOffset": 193}, {"referenceID": 1, "context": "Steady improvements have been made in statistical parametric speech synthesis (SPSS), in particular through the adoption of deep and recurrent machine-learning techniques in recent years [1, 2].", "startOffset": 187, "endOffset": 193}, {"referenceID": 2, "context": "As we expect that high-level, long-range dependencies are of importance for the prosodic structure of speech, recurrent models such as LSTMs are well-suited to prosodic sequence modelling problems [3].", "startOffset": 197, "endOffset": 200}, {"referenceID": 2, "context": "Another possible weakness of current approaches towards duration generation is that they typical operate as an initial stage, separate from the generation of acoustic features [3].", "startOffset": 176, "endOffset": 179}, {"referenceID": 3, "context": "A major motivation for this is that such a joint model would allow the simultaneous adaptation [4] and control [5] of rhythmic, melodic and phonetic characteristics in a stable and consistent way.", "startOffset": 95, "endOffset": 98}, {"referenceID": 4, "context": "A major motivation for this is that such a joint model would allow the simultaneous adaptation [4] and control [5] of rhythmic, melodic and phonetic characteristics in a stable and consistent way.", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "In early, formant-based synthesis systems, phone durations were generated by rule [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 6, "context": "That said, some approaches allowed predicted durations to be incorporated into the target cost [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "The rise of statistical parametric speech synthesis (SPSS) [8, 9] has introduced a new methodology for duration generation, in which a statistical model (probability distribution) is created to describe speech-sound durations.", "startOffset": 59, "endOffset": 65}, {"referenceID": 8, "context": "The rise of statistical parametric speech synthesis (SPSS) [8, 9] has introduced a new methodology for duration generation, in which a statistical model (probability distribution) is created to describe speech-sound durations.", "startOffset": 59, "endOffset": 65}, {"referenceID": 9, "context": "[10] introduced the idea of using hidden semiMarkov models (HSMMs) to describe durations in the context of speech synthesis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Typically, HSMM durations would be assumed to follow a parametric distribution of some sort; the widely-used HMM-based speech synthesis system (HTS) [11] defaults to Gaussian duration distributions, for instance, although the skewness and non-negativity of speech durations mean that other choices of distribution (log-normal, gamma) might be more suitable [12, 13].", "startOffset": 149, "endOffset": 153}, {"referenceID": 11, "context": "Typically, HSMM durations would be assumed to follow a parametric distribution of some sort; the widely-used HMM-based speech synthesis system (HTS) [11] defaults to Gaussian duration distributions, for instance, although the skewness and non-negativity of speech durations mean that other choices of distribution (log-normal, gamma) might be more suitable [12, 13].", "startOffset": 357, "endOffset": 365}, {"referenceID": 12, "context": "Typically, HSMM durations would be assumed to follow a parametric distribution of some sort; the widely-used HMM-based speech synthesis system (HTS) [11] defaults to Gaussian duration distributions, for instance, although the skewness and non-negativity of speech durations mean that other choices of distribution (log-normal, gamma) might be more suitable [12, 13].", "startOffset": 357, "endOffset": 365}, {"referenceID": 2, "context": ", [3, 14].", "startOffset": 2, "endOffset": 9}, {"referenceID": 13, "context": ", [3, 14].", "startOffset": 2, "endOffset": 9}, {"referenceID": 14, "context": "While in principle, natural speech is a random sample drawn from the true duration distribution, the output naturalness of sampling methods in speech synthesis has been found to perform poorly unless highly accurate models are used [15].", "startOffset": 232, "endOffset": 236}, {"referenceID": 15, "context": "[16, 17].", "startOffset": 0, "endOffset": 8}, {"referenceID": 16, "context": "[16, 17].", "startOffset": 0, "endOffset": 8}, {"referenceID": 17, "context": "This implies that, in the spirit of most likely output parameter generation [18], median-based duration prediction is likely closer to the peak density \u2013 the \u201cmost typical\u201d outcome \u2013 than the mean is.", "startOffset": 76, "endOffset": 80}, {"referenceID": 18, "context": "Statistical robustness is compelling for speech synthesis [19], particularly for big and found datasets, as it reduces the sensitivity to errors and unexpected behaviour in the training corpus.", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "[20] described a joint model of duration and speech parameters, where a deep neural network was trained to simultaneously output acoustic parameters and a 5-dimensional (phone) state-duration vector for each frame.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "Furthermore, despite the frame-level granularity of the approach in [20], it is not straightforward to identify a probabilistic interpretation of their scheme.", "startOffset": 68, "endOffset": 72}, {"referenceID": 19, "context": "Such joint modelling of durations with acoustic properties of speech was a major factor in motivating the set-up in [20].", "startOffset": 116, "endOffset": 120}, {"referenceID": 20, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 21, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "As long as the transition probabilities satisfy \u03c0t \u2208 [0, 1] and", "startOffset": 53, "endOffset": 59}, {"referenceID": 22, "context": "Interestingly, summing absolute rather than squared errors is a common component of the mel-cepstral distortion (MCD) [23] often used to evaluate acoustic models in speech synthesis, but the same idea is less commonly seen for evaluating generated durations.", "startOffset": 118, "endOffset": 122}, {"referenceID": 23, "context": "Festvox\u2019s ehmm [24] was used to insert pauses into the annotated phone sequences based on the acoustics, to improve the accuracy of forced-aligned durations.", "startOffset": 15, "endOffset": 19}, {"referenceID": 10, "context": "Each phone was then characterised by a vector of 481 text-derived binary and numerical features: these features are a subset of the features used in decision-tree clustering questions from the HTS public demo [11]; numerical features queried by those questions were used directly where possible.", "startOffset": 209, "endOffset": 213}, {"referenceID": 18, "context": "Contrary to our previous work [19], sub-phone states were not used in either DNN training or prediction.", "startOffset": 30, "endOffset": 34}, {"referenceID": 24, "context": "Phone-LSTM was configured with five feed-forward layers of 1024 nodes each and a final unidirectional SLSTM [25] hidden layer consisting of 512 nodes.", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "This paper proposes a new approach to duration modelling for statistical parametric speech synthesis in which a recurrent statistical model is trained to output a phone transition probability at each timestep (acoustic frame). Unlike conventional approaches to duration modelling \u2013 which assume that duration distributions have a particular form (e.g., a Gaussian) and use the mean of that distribution for synthesis \u2013 our approach can in principle model any distribution supported on the non-negative integers. Generation from this model can be performed in many ways; here we consider output generation based on the median predicted duration. The median is more typical (more probable) than the conventional mean duration, is robust to training-data irregularities, and enables incremental generation. Furthermore, a frame-level approach to duration prediction is consistent with a longer-term goal of modelling durations and acoustic features together. Results indicate that the proposed method is competitive with baseline approaches in approximating the median duration of held-out natural speech.", "creator": "LaTeX with hyperref package"}}}