{"id": "1608.06235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Aug-2016", "title": "Adaptive Probabilistic Trajectory Optimization via Efficient Approximate Inference", "abstract": "Robotic systems must be able to quickly and robustly make decisions when operating in uncertain and dynamic environments. While Reinforcement Learning (RL) can be used to compute optimal policies with little prior knowledge about the environment, it suffers from slow convergence. An alternative approach is Model Predictive Control (MPC), which optimizes policies quickly, but also requires accurate models of the system dynamics and environment. In this paper we propose a new approach, adaptive probabilistic trajectory optimization, that combines the benefits of RL and MPC. Our method uses scalable approximate inference to learn and updates probabilistic models in an online incremental fashion while also computing optimal control policies via successive local approximations. We present two variations of our algorithm based on the Sparse Spectrum Gaussian Process (SSGP) model, and we test our algorithm on three learning tasks, demonstrating the effectiveness and efficiency of our approach.", "histories": [["v1", "Mon, 22 Aug 2016 17:49:50 GMT  (1865kb,D)", "http://arxiv.org/abs/1608.06235v1", null], ["v2", "Sun, 11 Sep 2016 23:11:23 GMT  (1875kb,D)", "http://arxiv.org/abs/1608.06235v2", null]], "reviews": [], "SUBJECTS": "cs.RO cs.LG", "authors": ["yunpeng pan", "xinyan yan", "evangelos theodorou", "byron boots"], "accepted": false, "id": "1608.06235"}, "pdf": {"name": "1608.06235.pdf", "metadata": {"source": "CRF", "title": "Adaptive Probabilistic Trajectory Optimization via Efficient Approximate Inference", "authors": ["Yunpeng Pan", "Xinyan Yan", "Evangelos Theodorou", "Byron Boots"], "emails": ["ypan37@gatech.edu,", "xyan43@gatech.edu,", "evangelos.theodorou@gatech.edu,", "bboots@cc.gatech.edu"], "sections": [{"heading": "1 Introduction", "text": "Over the last decade, Enhanced Learning (RL) has begun to be successfully applied to robotics and autonomous systems. While model-free RL has shown promising results, it typically requires human expert demonstrations or relies on many direct interactions with physical systems. Model-based RL was developed to explicitly solve the problem of inefficiency of learning processes from data that allows for better generalization [2, 1]. However, these deterministic model-based methods suffer from errors that come together in the creation of long-term prognoses. Recent model-based RL methods overcome this problem by achieving state-of-the-art performance [3-5]."}, {"heading": "2 Trajectory Optimization", "text": "We will consider a general unknown dynamic system described by the following differential equation: k = Q (x, u) dt + Cd\u03c9, x (t0) = x0, dp \u00b2 N (0, \u03b5\u03c9), (1) where x \u2212 Rn is the state, u \u00b2 Rm is the control and \u03c9 \u00b2 Rp is standard Brownian noise. The goal of optimal control and amplification of learning is to find the control of policy \u03c0 (x (t), t) which minimizes the expected cost (t0))) = Ex [h (t), DP (T) +, T t0 L (t), \u03c0 (t), xjectory, t] where h (T) minimizes the terminal cost, and L (t), \u03c0 (x (t) is the instantaneous cost rate. Tax policy u (t) = \u03c0 (t), t) is a function which defines the states and time of control."}, {"heading": "3 Probabilistic Model Learning and Inference", "text": "In iLQG-LD [12] and Minimax DDP [13], dynamics are learned using locally weighted projection regression (LWPR) [14] and receptive field weighted regression (RFWR) [15]. However, both methods require a large amount of training data collected from many interactions with the physical system. Recently, probabilistic methods such as PDDP [4] and AGP-iLQR [16] have shown impressive data efficiency using Gaussian process (GP) dynamics models. However, GP inference in PDDP [4] is too expensive for online optimization and incremental updates. AGP-iLQR [16] uses subranges of regression (SOR) that exhibit faster inference and have dynamic models in PDDP [4]."}, {"heading": "3.1 Model learning via sparse spectrum Gaussian processes", "text": "It is not as if it is a powerful regression technology that imposes significant practical constraints on learning and inference to large datasets due to its O (N3) compression and O (N2) complexity, which is a direct consequence of storage and inversion efficiency."}, {"heading": "3.2 Approximate Bayesian inference", "text": "When performing long-term predictions using the SSGP models, the input state control pair x-z becomes uncertain. Here, we define the common distribution over the state control pair in a time step as p (x-z) = p (x-z) = p (x-z). This predictive distribution cannot be calculated analytically, because the nonlinear mapping of an input Gaussian distribution results in a non-Gaussian predictive distribution. Therefore, we have to resort to approximate methods. Deterministic approximate inference methods approach the posterior with a Gaussian one [22, 23, 3], e.g. 0 x = f (x-z). However, these methods scale quadratically with the number of training samples, making them unsuitable for online learning with moderate sets of data (e.g. 500 or more data points).In this section, we present two methods that compare GPs in a similar way as we offer the predictive results in relation to the number of parameters in the number of parameters."}, {"heading": "3.2.1 Exact moment matching (SSGP-EMM)", "text": "Taking into account the law of iterated expectation, the approximate mean (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x) + (x), (x) + (x) + (x), (x) + (x), (x), (x), (x), (x), (x), (x), (x) [x), (x), (x), x (x), x (x), x [(x), x [x), (x), x [x [x), x [x), x (x), x (x), x [x [x), x [x), x, x (x), x (x), x (x), x (x), x [x [x), x [x), x [x), x, x (x), x, x (x), x [x), x (x), x, x (x), x (x), x (x), x [x), x (x), x [x), x [x), x [x [x), x [x), x [x [x), x [x), x [x), x, x (x), x [x), x (x), x [x), x (x [x), x [x), x [, x [, x), x [x), x [, x), x (x [, x), x [, x), x [, x [, x [, x), x), x [, x), x [, x [, x) x [, x [, x) x [, x [, x), x [, x) x [, x"}, {"heading": "3.2.2 Linearization (SSGP-Lin)", "text": "Another approach to approximate the probable distribution under uncertain input is the linearization of the posterior SSGP mean function. (First, we derive the partial derivation of the predictive mean Ef [f (x)] to the input x (x), and the predictive mean of the first order Taylor expansion to the input mean Ef (f (x) x (x) x (x) x x (x) x (x) x (x) x (x) x (x) x (x) x (x) x (5), where Di (x) = x (x) x (x) x) x (x) x (x), x), x), x (x), x (x), x, x, (x), x (x), x (x), x, x (), x, x, x ()."}, {"heading": "4 Online Probabilistic Trajectory Optimization", "text": "In order to explicitly take into account the uncertainty of the dynamic model, we perform a path optimization in the faith space. In our proposed framework, we create a local model along a nominal orbit through the faith space (v-k, u-k) including i) a linear approximation of the faith dynamic model; ii) a local approximation of the value function. We define the faith and control nominal orbit (v-k, u-1: H) and deviations from this orbit (vk-v-k, \u03b4uk = uk-u-k). The linear approximation of the faith dynamics along the nominal orbit is vk + 1 = zw-k."}, {"heading": "4.1 Online optimization: a receding horizon scheme", "text": "The trajectory optimization approach with learned dynamics can be used for episodic RL. However, this requires interactions with the physical system over a long time frame (trajectory) and is not responsive to tasks or model variations occurring in short time frames (e.g. at each time step).Here we propose an online approach in the sense of model predictive control (MPC).In view of a solution for a single H-step trajectory optimization problem starting with x1, we only apply the first element of the control sequence u1 and proceed to solve another H-step trajectory optimization problem starting with x2. In contrast to the offline approach, we initialize with the previous optimized trajectory model. The H-step nominal control sequence for warm start becomes u2, u3, uH, uH, uH. Online optimization would develop much faster than offline, as long as the new target is not far away from the previous target."}, {"heading": "4.2 Relation to prior work", "text": "Our method has some similarities with methods such as iLQG-LD [12], AGP-iLQR [16], PDDP [4] and Minimax DDP [13], all of which are based on DDP or iLQR and learned dynamics models. Our method differs in both model and controller learning. GP models are more robust for modeling errors than LWPR (iLQG-LD [12]) or RFWR (Minimax DDP [13]). However, the main obstacle to the application of GPs is the high computational effort required to perform inferences. Our method integrates scalable approximate inferences (see Section 3.2) into the optimization of trajectory. In contrast, related methods either use a computationally intensive inference approach (PDDP [4]) or reduce uncertainty in the dynamic model (AGP-iLQR [16], which is less robust for modeling errors), even if our method is based on the ability to adapt."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Approximate inference performance", "text": "We compare the proposed approximate follow-up methods with three existing approaches: the full GP-precise moment matching (GP-EMM) approach (GP-EMM) used in iLQG-LD [22, 23, 3], the subset of regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12]. We consider two multi-level prediction tasks using the dynamic models of a quadrotor (16 state dimensions, 4 control dimensions) and a Puma-560 manipulator (12 state dimensions, 6 control dimensions). In the following, we evaluate the performance in terms of predictive accuracy of a quadrotor (16 state dimensions, 4 control dimensions) and a Puma-560 manipulator (12 state dimensions, 6 control dimensions)."}, {"heading": "5.2 Trajectory optimization performance", "text": "We evaluate the performance of our methods using three models of predictable control (MPC).PUMA-560 Task: Various target and model parameter changes The task is to guide the end effector to the desired position and orientation. The desired state is time-consuming, as in fig.2b. We collected 1000 data points offline and sampled 50 random features for both of our methods. We also used 50 reference points for AGP-iLQR. To show the effect of the online adaptation, we increased the mass of the end effector by 500% at the beginning of online learning (it is fixed during learning).2 (a) shows the cost reduction results averaged over 3 independent studies."}, {"heading": "6 Conclusion", "text": "Similar to RL, our method can efficiently learn from experience and adapt to new situations. Unlike most RL algorithms, our method updates control guidelines and dynamics models online step by step, with tasks and dynamics variants. To perform robust and fast planning, we have introduced two scalable approximate inference methods. Both methods have demonstrated superior performance in terms of computational efficiency and long-term predictive accuracy compared to known methods. Our adaptive probabilistic system for track optimization combines the advantages of efficient inference and model predictive control (MPC) and is applicable to a wide range of learning and control problems."}], "references": [{"title": "A survey on policy search for robotics", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "A comparison of direct and model-based reinforcement learning", "author": ["C.G. Atkeson", "J.C. Santamaria"], "venue": "In In International Conference on Robotics and Automation. Citeseer,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Gaussian processes for data-efficient learning in robotics and control", "author": ["M. Deisenroth", "D. Fox", "C. Rasmussen"], "venue": "IEEE Transsactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Probabilistic differential dynamic programming", "author": ["Y. Pan", "E. Theodorou"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Model-based contextual policy search for data-efficient generalization of robot skills", "author": ["A. Kupcsik", "M.P. Deisenroth", "J. Peters", "AP Loh", "P. Vadakkepat", "G. Neumann"], "venue": "Artificial Intelligence,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Differential dynamic programming", "author": ["D. Jacobson", "D. Mayne"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1970}, {"title": "An application of reinforcement learning to aerobatic helicopter", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A. Y Ng"], "venue": "flight. NIPS,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Control-limited differential dynamic programming", "author": ["Y. Tassa", "N. Mansard", "E. Todorov"], "venue": "ICRA", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Sparse spectrum gaussian process regression", "author": ["M. L\u00e1zaro-Gredilla", "J. Qui\u00f1onero-Candela", "C.E. Rasmussen", "A.R. Figueiras-Vidal"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "A generalized iterative lqg method for locally-optimal feedback control of constrained nonlinear stochastic systems", "author": ["E. Todorov", "W. Li"], "venue": "In American Control Conference,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Adaptive optimal feedback control with learned internal dynamics models", "author": ["D. Mitrovic", "S. Klanke", "S. Vijayakumar"], "venue": "In From Motor Learning to Interaction Learning in Robots,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Minimax differential dynamic programming: An application to robust biped walking", "author": ["J. Morimoto", "CG Atkeson"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2002}, {"title": "Incremental online learning in high dimensions", "author": ["Sethu Vijayakumar", "Aaron D\u2019souza", "Stefan Schaal"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Constructive incremental learning from only local information", "author": ["Stefan Schaal", "Christopher G Atkeson"], "venue": "Neural computation,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1998}, {"title": "Approximate real-time optimal control based on sparse gaussian process models", "author": ["J. Boedecker", "JT. Springenberg", "J. Wulfing", "M. Riedmiller"], "venue": "ADPRL", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Real-time model learning using incremental sparse spectrum gaussian process regression", "author": ["A. Gijsberts", "G. Metta"], "venue": "Neural Networks,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2007}, {"title": "Fourier analysis on groups", "author": ["W. Rudin"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1962}, {"title": "Gaussian processes for machine learning", "author": ["C.K.I Williams", "C.E. Rasmussen"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Propagation of uncertainty in bayesian kernel models-application to multiple-step ahead forecasting", "author": ["J. Quinonero Candela", "A. Girard", "J. Larsen", "C.E. Rasmussen"], "venue": "In IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}, {"title": "Gaussian process priors with uncertain inputs application to multiple-step ahead time series forecasting", "author": ["A. Girard", "C.E. Rasmussen", "J. Quinonero-Candela", "R. Murray-Smith"], "venue": "In NIPS,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Receding horizon differential dynamic programming", "author": ["Y. Tassa", "T. Erez", "W.D. Smart"], "venue": "In NIPS,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Steady-state cornering equilibria and stabilisation for a vehicle during extreme operating conditions", "author": ["E. Velenis", "E. Frazzoli", "P. Tsiotras"], "venue": "International Journal of Vehicle Autonomous Systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "While model-free RL has demonstrated promising results [1], it typically requires human expert demonstrations or relies on lots of direct interactions with physical systems.", "startOffset": 55, "endOffset": 58}, {"referenceID": 1, "context": "Model-based RL was developed to address the issue of sample inefficiency by learning dynamics models explicitly from data, which can provide better generalization [2, 1].", "startOffset": 163, "endOffset": 169}, {"referenceID": 0, "context": "Model-based RL was developed to address the issue of sample inefficiency by learning dynamics models explicitly from data, which can provide better generalization [2, 1].", "startOffset": 163, "endOffset": 169}, {"referenceID": 2, "context": "Recent probabilistic model-based RL methods overcome this issue, achieving state-of-the-art performance [3\u20135].", "startOffset": 104, "endOffset": 109}, {"referenceID": 3, "context": "Recent probabilistic model-based RL methods overcome this issue, achieving state-of-the-art performance [3\u20135].", "startOffset": 104, "endOffset": 109}, {"referenceID": 4, "context": "Recent probabilistic model-based RL methods overcome this issue, achieving state-of-the-art performance [3\u20135].", "startOffset": 104, "endOffset": 109}, {"referenceID": 5, "context": "More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7\u20139].", "startOffset": 93, "endOffset": 96}, {"referenceID": 6, "context": "More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7\u20139].", "startOffset": 198, "endOffset": 203}, {"referenceID": 7, "context": "More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7\u20139].", "startOffset": 198, "endOffset": 203}, {"referenceID": 8, "context": "More precisely, we propose a method that relies on local trajectory optimization such as DDP [6], which is scalable and has been shown to work well on challenging learning control tasks in robotics [7\u20139].", "startOffset": 198, "endOffset": 203}, {"referenceID": 2, "context": "In previous GP-related RL methods, approximate inference is the major computational bottleneck [3\u20135].", "startOffset": 95, "endOffset": 100}, {"referenceID": 3, "context": "In previous GP-related RL methods, approximate inference is the major computational bottleneck [3\u20135].", "startOffset": 95, "endOffset": 100}, {"referenceID": 4, "context": "In previous GP-related RL methods, approximate inference is the major computational bottleneck [3\u20135].", "startOffset": 95, "endOffset": 100}, {"referenceID": 9, "context": "Therefore, we develop two novel scalable approximate inference algorithms based on Sparse Spectrum Gaussian Processes (SSGPs) [10].", "startOffset": 126, "endOffset": 130}, {"referenceID": 10, "context": "In DDP, and related methods such as iLQG [11], a local model is constructed based on i) a first or second-order linear approximation of the dynamics model; ii) a second-order local approximation of the value function along a nominal trajectory.", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "DDP-related methods have been widely used for solving control problem in robotics tasks [7, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 7, "context": "DDP-related methods have been widely used for solving control problem in robotics tasks [7, 8].", "startOffset": 88, "endOffset": 94}, {"referenceID": 11, "context": "In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15].", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15].", "startOffset": 31, "endOffset": 35}, {"referenceID": 13, "context": "In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15].", "startOffset": 114, "endOffset": 118}, {"referenceID": 14, "context": "In iLQG-LD [12] and Minimax DDP[13], the dynamics are learned using Locally Weighted Projection Regression (LWPR) [14] and Receptive Field Weighted Regression (RFWR) [15].", "startOffset": 166, "endOffset": 170}, {"referenceID": 3, "context": "Recently, probabilistic methods such as PDDP [4] and AGP-iLQR [16] have demonstrated impressive data efficiency with Gaussian process (GP) dynamics models.", "startOffset": 45, "endOffset": 48}, {"referenceID": 15, "context": "Recently, probabilistic methods such as PDDP [4] and AGP-iLQR [16] have demonstrated impressive data efficiency with Gaussian process (GP) dynamics models.", "startOffset": 62, "endOffset": 66}, {"referenceID": 3, "context": "However, GP inference in PDDP [4] is too computationally expensive for online optimization and incremental updates.", "startOffset": 30, "endOffset": 33}, {"referenceID": 15, "context": "AGP-iLQR [16] employs subset of regression (SOR) approximations that feature faster GP inference and incremental model adaptation, but neglects the predictive uncertainty when performing multi-step predictions using the learned forward dynamics.", "startOffset": 9, "endOffset": 13}, {"referenceID": 9, "context": "In order to perform efficient and robust trajectory optimization, we introduce a learning and inference scheme based on Sparse Spectrum Gaussian Processes (SSGPs) [10, 17].", "startOffset": 163, "endOffset": 171}, {"referenceID": 16, "context": "In order to perform efficient and robust trajectory optimization, we introduce a learning and inference scheme based on Sparse Spectrum Gaussian Processes (SSGPs) [10, 17].", "startOffset": 163, "endOffset": 171}, {"referenceID": 2, "context": "can be leveraged to learn the dynamics model [3\u20135].", "startOffset": 45, "endOffset": 50}, {"referenceID": 3, "context": "can be leveraged to learn the dynamics model [3\u20135].", "startOffset": 45, "endOffset": 50}, {"referenceID": 4, "context": "can be leveraged to learn the dynamics model [3\u20135].", "startOffset": 45, "endOffset": 50}, {"referenceID": 9, "context": "Sparse Spectrum GP Regression (SSGPR) SSGP [10] is a recent approach that provides a principled approximation of GPR by employing a random Fourier feature approximation of the kernel function [18].", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "Sparse Spectrum GP Regression (SSGPR) SSGP [10] is a recent approach that provides a principled approximation of GPR by employing a random Fourier feature approximation of the kernel function [18].", "startOffset": 192, "endOffset": 196}, {"referenceID": 18, "context": "Based on Bochner\u2019s theorem [19], any shift-invariant kernel functions can be represented as the Fourier transform of a unique measure k(x\u0303i \u2212 x\u0303j) = \u222b Rn+m e i\u03c9T(x\u0303i\u2212x\u0303j)p(\u03c9)d\u03c9 = E\u03c9[\u03c6\u03c9(x\u0303i) \u03c6\u03c9(x\u0303j)].", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "We consider the popular Squared Exponential (SE) covariance function with Automatic Relevance Determination (ARD) distance measure as it has been applied successfully in learning dynamics and optimal control [17, 16], k(x\u0303i, x\u0303j) = \u03c3 f exp(\u2212 1 2 (x\u0303i \u2212 x\u0303j) P(x\u0303i \u2212 x\u0303j)), where P = diag({l i } n+m i=1 ).", "startOffset": 208, "endOffset": 216}, {"referenceID": 15, "context": "We consider the popular Squared Exponential (SE) covariance function with Automatic Relevance Determination (ARD) distance measure as it has been applied successfully in learning dynamics and optimal control [17, 16], k(x\u0303i, x\u0303j) = \u03c3 f exp(\u2212 1 2 (x\u0303i \u2212 x\u0303j) P(x\u0303i \u2212 x\u0303j)), where P = diag({l i } n+m i=1 ).", "startOffset": 208, "endOffset": 216}, {"referenceID": 19, "context": "The hyper-parameters can be learned by maximizing the log-likelihood of the training outputs given the inputs using numerical methods such as conjugate gradient [20].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "Instead, we keep track of its upper triangular Cholesky factor A = RR [17].", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "Deterministic approximate inference methods approximate the posterior with a Gaussian [22, 23, 3], e.", "startOffset": 86, "endOffset": 97}, {"referenceID": 21, "context": "Deterministic approximate inference methods approximate the posterior with a Gaussian [22, 23, 3], e.", "startOffset": 86, "endOffset": 97}, {"referenceID": 2, "context": "Deterministic approximate inference methods approximate the posterior with a Gaussian [22, 23, 3], e.", "startOffset": 86, "endOffset": 97}, {"referenceID": 20, "context": "See Table 1 for a comparison between our methods and exact moment matching approach for GPs (GP-EMM) [22, 23, 3].", "startOffset": 101, "endOffset": 112}, {"referenceID": 21, "context": "See Table 1 for a comparison between our methods and exact moment matching approach for GPs (GP-EMM) [22, 23, 3].", "startOffset": 101, "endOffset": 112}, {"referenceID": 2, "context": "See Table 1 for a comparison between our methods and exact moment matching approach for GPs (GP-EMM) [22, 23, 3].", "startOffset": 101, "endOffset": 112}, {"referenceID": 20, "context": "Computational complexity GP-EMM [22, 23, 3] O ( Nn(n+m) )", "startOffset": 32, "endOffset": 43}, {"referenceID": 21, "context": "Computational complexity GP-EMM [22, 23, 3] O ( Nn(n+m) )", "startOffset": 32, "endOffset": 43}, {"referenceID": 2, "context": "Computational complexity GP-EMM [22, 23, 3] O ( Nn(n+m) )", "startOffset": 32, "endOffset": 43}, {"referenceID": 5, "context": "The Q function can be approximated as a quadratic model along the nominal trajectory [6].", "startOffset": 85, "endOffset": 88}, {"referenceID": 10, "context": "In our implementation we apply line search to guarantee convergence to a locally optimal control policy [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Control constraints are taken into account using the method in [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 11, "context": "2 Relation to prior work Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] .", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "2 Relation to prior work Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] .", "startOffset": 104, "endOffset": 108}, {"referenceID": 3, "context": "2 Relation to prior work Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] .", "startOffset": 114, "endOffset": 117}, {"referenceID": 12, "context": "2 Relation to prior work Our method shares some similarities with methods such as iLQG-LD[12], AGP-iLQR [16], PDDP[4] and Minimax DDP[13] .", "startOffset": 133, "endOffset": 137}, {"referenceID": 11, "context": "GP models are more robust to modeling error than LWPR (iLQG-LD [12]) or RFWR (Minimax DDP [13]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "GP models are more robust to modeling error than LWPR (iLQG-LD [12]) or RFWR (Minimax DDP [13]).", "startOffset": 90, "endOffset": 94}, {"referenceID": 3, "context": "In contrast, related methods either employs computation-intensive inference approach (PDDP [4]), or drops uncertainty in the dynamics model (AGP-iLQR [16]), which becomes less robust to modeling errors.", "startOffset": 91, "endOffset": 94}, {"referenceID": 15, "context": "In contrast, related methods either employs computation-intensive inference approach (PDDP [4]), or drops uncertainty in the dynamics model (AGP-iLQR [16]), which becomes less robust to modeling errors.", "startOffset": 150, "endOffset": 154}, {"referenceID": 20, "context": "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].", "startOffset": 174, "endOffset": 185}, {"referenceID": 21, "context": "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].", "startOffset": 174, "endOffset": 185}, {"referenceID": 2, "context": "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].", "startOffset": 174, "endOffset": 185}, {"referenceID": 19, "context": "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].", "startOffset": 220, "endOffset": 224}, {"referenceID": 15, "context": "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].", "startOffset": 242, "endOffset": 246}, {"referenceID": 13, "context": "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].", "startOffset": 257, "endOffset": 261}, {"referenceID": 11, "context": "1 Approximate inference performance We compare the proposed approximate inference methods with three existing approaches: the full GP exact moment matching (GP-EMM) approach [22, 23, 3], Subset of Regressors GP (SoR-GP) [20] used in AGP-iLQR [16], and LWPR [14] used in iLQG-LD [12].", "startOffset": 278, "endOffset": 282}, {"referenceID": 2, "context": "Our methods are more scalable than GP-EMM, which is the major computational bottleneck for probabilistic model-based RL approaches [3, 4].", "startOffset": 131, "endOffset": 137}, {"referenceID": 3, "context": "Our methods are more scalable than GP-EMM, which is the major computational bottleneck for probabilistic model-based RL approaches [3, 4].", "startOffset": 131, "endOffset": 137}, {"referenceID": 22, "context": "The receding-horizon DDP (RH-DDP) [24] with full knowledge of the dynamics model was used as a baseline.", "startOffset": 34, "endOffset": 38}, {"referenceID": 23, "context": "This problem has been studied in [25] where the authors developed a LQR control scheme based on analytic linearization of the dynamics model.", "startOffset": 33, "endOffset": 37}, {"referenceID": 23, "context": "We applied our method to this task under unknown dynamics with 2500 offline data points, which were sampled from the empirical vehicle model in [25].", "startOffset": 144, "endOffset": 148}, {"referenceID": 23, "context": "Results and comparisons with the solution in [25] are shown in fig.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "Blue lines are the analytic LQR solution in [25]", "startOffset": 44, "endOffset": 48}], "year": 2017, "abstractText": "Robotic systems must be able to quickly and robustly make decisions when operating in uncertain and dynamic environments. While Reinforcement Learning (RL) can be used to compute optimal policies with little prior knowledge about the environment, it suffers from slow convergence. An alternative approach is Model Predictive Control (MPC), which optimizes policies quickly, but also requires accurate models of the system dynamics and environment. In this paper we propose a new approach, adaptive probabilistic trajectory optimization, that combines the benefits of RL and MPC. Our method uses scalable approximate inference to learn and updates probabilistic models in an online incremental fashion while also computing optimal control policies via successive local approximations. We present two variations of our algorithm based on the Sparse Spectrum Gaussian Process (SSGP) model, and we test our algorithm on three learning tasks, demonstrating the effectiveness and efficiency of our approach.", "creator": "LaTeX with hyperref package"}}}