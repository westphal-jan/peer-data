{"id": "1608.06374", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But Also Parameters", "abstract": "This paper emphasizes the significance to jointly exploit the problem structure and the parameter structure, in the context of deep modeling. As a specific and interesting example, we describe the deep double sparsity encoder (DDSE), which is inspired by the double sparsity model for dictionary learning. DDSE simultaneously sparsities the output features and the learned model parameters, under one unified framework. In addition to its intuitive model interpretation, DDSE also possesses compact model size and low complexity. Extensive simulations compare DDSE with several carefully-designed baselines, and verify the consistently superior performance of DDSE. We further apply DDSE to the novel application domain of brain encoding, with promising preliminary results achieved.", "histories": [["v1", "Tue, 23 Aug 2016 03:50:01 GMT  (427kb,D)", "http://arxiv.org/abs/1608.06374v1", null], ["v2", "Sun, 2 Oct 2016 03:01:51 GMT  (426kb,D)", "http://arxiv.org/abs/1608.06374v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["zhangyang wang", "thomas s huang"], "accepted": false, "id": "1608.06374"}, "pdf": {"name": "1608.06374.pdf", "metadata": {"source": "CRF", "title": "Deep Double Sparsity Encoder: Learning to Sparsify Not Only Features But Also Parameters", "authors": ["Zhangyang Wang", "Liang Zhang", "Yingzhen Yang", "Jiayu Zhou", "Georgios B. Giannakis", "Thomas S. Huang"], "emails": [], "sections": [{"heading": "Introduction", "text": "It was gradually recognized that the problem structure could be integrated into the design of deep architectures, and such tailored deep architectures can benefit from their problem-specific regulations and improve performance. In particular, there has been a flourishing interest in bridging low-cost coding (Wang et al. 2016b), (Wang et al. 2016d), which develops similar ideas on fast-traceable regressors and constructed network adjustments to solve variants of low-cost coding models. (Xin et al. 2016) demonstrated both theoretically and empirically that a trained deep network is potentially capable of restoring zero-based low-cost representations under milder conditions."}, {"heading": "Double Sparsity Model for Dictionary Learning", "text": "A crucial consideration in the use of the sparse coding model (1) is the choice of dictionary D. It has been observed that the dictionary atoms learned are highly structured, with remarkably regular patterns (Peng et al. 2015), hence the hypothesis that the dictionary atoms themselves may have a sparse structure compared to a more basic dictionary. (Rubinstein, Zibulevsky and Elad 2010) suggested a double thrift model, suggesting that each atom of the dictionary itself has a sparse representation of some pre-defined base dictionary D0. Thus, the dictionary is expressed as follows: D = D0S, | S (:, i) | 0 \u2264 s, \u0441i, (3), where S is the sparse atomic representation matrix that has no more than s unspecified base dictionary D0. The dictionary is expressed as follows: \u00d7 \u00d7 Remm = Remn."}, {"heading": "The Proposed Model", "text": "Given D0 and S, we replace (3) in (2) to obtain: L1 (x) = STDT0 x (1 x), L2 (zk) = (I \u2212 STDT0 D0S) zk, (4) with the iterative formula of zk and the form of N. Compared with (2), S now becomes the traable parameter instead of D. In order to simplify (4), we first eliminate DT0 D0 of L2 (zk). In view of training data Xn \u00b7 = {xi}, = 1, 2,..., and assuming that XX\u03a3 has no mean, we choose D0 as the (complete) intrinsic matrix of X\u0445XT\u0442 (i.e., the covector matrix of Xxi = {xi}, i = 1, 2,..., and we assume that X\u0445\u043e\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0441\u0441\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u0441\u0438\u043d\u0438\u043d\u0438\u043d\u0435\u0441\u0438\u043d\u0435\u043d\u0435\u0441\u0438\u043d\u0435\u0441\u0438\u0441\u0438\u043d\u0435\u0441\u0438\u043d\u0435\u0441\u0438\u043d\u0435\u0441\u0438\u0441\u0438\u043d\u0435\u0441\u0441\u0441\u0441\u0441\u0441\u0438\u043d\u0438\u043d\u0435\u0441\u0441\u0441\u0441\u0438smynismysmyst = = =), L1, L1, 2)."}, {"heading": "The Projected Gradient Descent Algorithm", "text": "Let G specify the nonlinear mapping of the data to the last hidden attribute before loss, the optimization problem of 1Another parameter that needs to be learned together is the threshold \u03bb inN. It is handled identically as in (Wang, Ling and Huang 2016).Training DDSE could be written as follows: min {W1, W2, W3, \u03b8} F\u03b8 (G (X\u03a3 | W1, W2, W3))), s.t. | | W1 (i,:) | 0 \u2264 s, | W2 (:, j) | 0 \u2264 s, | W3 (k,:) | 0 \u2264 s, \u0394i, j, k. (6) Apart from the limitations, the target in (6) is usually minimized by stochastic gradient descent (SGD)."}, {"heading": "Complexity Analysis", "text": "The total amount of parameters in DDSE is (2k + 1) sm. In contrast, the LISTA network in Figure 1 (b) assumes mn + km2 parameters, provided that its L2 parameters are also not bound by iterations. Since s m, n, the parameter ratio turns out to be (2k + 1) smmn + km2 = (2k + 1) s + km \u2192 2s m 1, than k \u2192. DDSE can thus be stored and loaded much more compactly, due to the sparse structure of Wls. More importantly, DSE can ensure sufficient capacity and flexibility of learning by using large m + km \u2192 2s m 1, and now effectively regulates the learning process by selecting small s. Consequences Time-complexity The efficient multiplication of a sparse matrix with elements of Wls."}, {"heading": "Relationship to Existing Techniques", "text": "Many regulatory techniques have been proposed to reduce the overuse of deep learning, such as Dropconnect (Krizhevsky, Sutskever, and Hinton 2012), which sets a randomly selected subset of activations to zero within each level. (Wan et al. 2013) further introduced Dropconnect to regulate fully connected layers, which instead sets a randomly selected subset of weights to zero during training. The proposed DDSE model implies an adaptive regime for Dropconnect, whereby the selection of \"dropped\" weights is not decided randomly, but by data-based hard thresholds. Furthermore, both Dropout and Dropconnect are applied only to training and are not able to reduce the actual model size. DDSE could be considered as an alternative to having a weight loss penalty enforced by hard \"0 constraints."}, {"heading": "Implementation", "text": "The proposed DDSE is implemented with the CUDA ConvNet package (Krizhevsky, Sutskever and Hinton 2012). We use a constant learning rate of 0.01, with the impulse parameter set to 0.9, and a batch size of 128. Neither Dropout nor Dropconnect are used unless otherwise stated. We manually reduce the learning rate when the network does not improve as in (Krizhevsky, Sutskever and Hinton 2012) according to a schedule set at a validation stage. As proposed in (5), we first subtract the mean and perform PCA via the training data X\u03a3. We adopt the multi-level updating strategy in (Jin et al. 2016), namely updating Wl by SGD without cardinality limitations for several (15 default) iterations, before performing the projection Pl (l = 1, 2, 3)."}, {"heading": "Simulation and Comparison", "text": "This year, it has come to the point where it will be able to take the lead at a time when it is not as far as it has been in the past, when it has never come so far."}, {"heading": "Application to Brain Encoding", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves."}], "references": [{"title": "C", "author": ["R.G. Baraniuk", "V. Cevher", "M.F. Duarte", "Hegde"], "venue": "2010. Model-based compressive sensing. IEEE Transactions on Information Theory 56(4):1982\u2013", "citeRegEx": "Baraniuk et al. 2010", "shortCiteRegEx": null, "year": 2001}, {"title": "M", "author": ["T. Blumensath", "Davies"], "venue": "E.", "citeRegEx": "Blumensath and Davies 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep big simple neural nets excel on handwritten digit recognition. http://arxiv. org/pdf/1003.0358", "author": ["Ciresan"], "venue": null, "citeRegEx": "Ciresan,? \\Q2010\\E", "shortCiteRegEx": "Ciresan", "year": 2010}, {"title": "A", "author": ["A. Coates", "Ng"], "venue": "Y.", "citeRegEx": "Coates and Ng 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Why does unsupervised pre-training help deep learning? JMLR", "author": ["Erhan"], "venue": null, "citeRegEx": "Erhan,? \\Q2010\\E", "shortCiteRegEx": "Erhan", "year": 2010}, {"title": "and LeCun", "author": ["K. Gregor"], "venue": "Y.", "citeRegEx": "Gregor and LeCun 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["He"], "venue": null, "citeRegEx": "He,? \\Q2016\\E", "shortCiteRegEx": "He", "year": 2016}, {"title": "Training skinny deep neural networks with iterative hard thresholding methods", "author": ["Jin"], "venue": "arXiv preprint arXiv:1607.05423", "citeRegEx": "Jin,? \\Q2016\\E", "shortCiteRegEx": "Jin", "year": 2016}, {"title": "J", "author": ["K.N. Kay", "T. Naselaris", "R.J. Prenger", "Gallant"], "venue": "L.", "citeRegEx": "Kay et al. 2008", "shortCiteRegEx": null, "year": 2008}, {"title": "B", "author": ["K.N. Kay", "J. Winawer", "A. Rokem", "A. Mezer", "Wandell"], "venue": "A.", "citeRegEx": "Kay et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "G", "author": ["A. Krizhevsky", "I. Sutskever", "Hinton"], "venue": "E.", "citeRegEx": "Krizhevsky. Sutskever. and Hinton 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "L", "author": ["U. Meier", "D.C. Ciresan", "Gambardella"], "venue": "M.; and Schmidhuber, J.", "citeRegEx": "Meier et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "D", "author": ["B.A. Olshausen", "Field"], "venue": "J.", "citeRegEx": "Olshausen and Field 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Connections between nuclear norm and frobenius norm based representation. arXiv preprint arXiv:1502.07423", "author": ["Peng"], "venue": null, "citeRegEx": "Peng,? \\Q2015\\E", "shortCiteRegEx": "Peng", "year": 2015}, {"title": "Deep subspace clustering with sparsity prior", "author": ["Peng"], "venue": "In The 25th International Joint Conference on Artificial Intelligence", "citeRegEx": "Peng,? \\Q2016\\E", "shortCiteRegEx": "Peng", "year": 2016}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Sutskever"], "venue": null, "citeRegEx": "Sutskever,? \\Q2013\\E", "shortCiteRegEx": "Sutskever", "year": 2013}, {"title": "Y", "author": ["L. Wan", "M. Zeiler", "S. Zhang", "Cun"], "venue": "L.; and Fergus, R.", "citeRegEx": "Wan et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "T", "author": ["Z. Wang", "J. Yang", "H. Zhang", "Z. Wang", "Y. Yang", "D. Liu", "Huang"], "venue": "S.", "citeRegEx": "Wang et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked approximated regression machine: A simple deep learning approach", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2016\\E", "shortCiteRegEx": "Wang", "year": 2016}, {"title": "2016b. D3: Deep dual-domain based fast restoration of jpeg-compressed images", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2016\\E", "shortCiteRegEx": "Wang", "year": 2016}, {"title": "Learning deep `\u221e encoder for hashing", "author": ["Wang"], "venue": "In IJCAI", "citeRegEx": "Wang,? \\Q2016\\E", "shortCiteRegEx": "Wang", "year": 2016}, {"title": "Learning deep `0 encoders", "author": ["Ling Wang", "Z. Huang 2016] Wang", "Q. Ling", "T. Huang"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Maximal sparsity with deep networks? NIPS", "author": ["Xin"], "venue": null, "citeRegEx": "Xin,? \\Q2016\\E", "shortCiteRegEx": "Xin", "year": 2016}], "referenceMentions": [], "year": 2017, "abstractText": "This paper emphasizes the significance to jointly exploit the problem structure and the parameter structure, in the context of deep modeling. As a specific and interesting example, we describe the deep double sparsity encoder (DDSE), which is inspired by the double sparsity model for dictionary learning. DDSE simultaneously sparsities the output features and the learned model parameters, under one unified framework. In addition to its intuitive model interpretation, DDSE also possesses compact model size and low complexity. Extensive simulations compare DDSE with several carefully-designed baselines, and verify the consistently superior performance of DDSE. We further apply DDSE to the novel application domain of brain encoding, with promising preliminary results achieved.", "creator": "LaTeX with hyperref package"}}}