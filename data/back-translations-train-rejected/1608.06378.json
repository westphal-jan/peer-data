{"id": "1608.06378", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine", "abstract": "Multimedia or spoken content presents more attractive information than plain text content, but it's more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It's highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors.", "histories": [["v1", "Tue, 23 Aug 2016 04:27:41 GMT  (7038kb,D)", "http://arxiv.org/abs/1608.06378v1", "Accepted conference paper: \"The Annual Conference of the International Speech Communication Association (Interspeech), 2016\""]], "COMMENTS": "Accepted conference paper: \"The Annual Conference of the International Speech Communication Association (Interspeech), 2016\"", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bo-hsiang tseng", "sheng-syun shen", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1608.06378"}, "pdf": {"name": "1608.06378.pdf", "metadata": {"source": "CRF", "title": "Towards Machine Comprehension of Spoken Content: Initial TOEFL Listening Comprehension Test by Machine", "authors": ["Bo-Hsiang Tseng", "Sheng-Syun Shen", "Hung-Yi Lee", "Lin-Shan Lee"], "emails": ["r02942037@ntu.edu.tw,", "r03942071@ntu.edu.tw,", "tlkagkb93901106@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": null, "text": "As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It is highly attractive to develop a machine that automatically understands spoken content and summarizes the key information that humans can browse through. In this endeavor, we propose a new task of machine understanding spoken content. We define the original goal as the listening comprehension test of TOEFL, a demanding academic English test for non-native English learners. In addition, for this task, we propose an attention-based Multi-Hop Recurrent Neural Network (AMRNN) architecture that achieves encouraging results in the initial tests. Initial results have also shown that word-level attention is likely to be more robust than sentence-level attention in this task with ASR errors."}, {"heading": "1. Introduction", "text": "With the popularity of shared videos, social networks, online courses, etc., the quantity of multimedia or spoken content is growing much faster than what people can see or hear. Accessing large collections of multimedia or spoken content is difficult and time consuming for people, even if these materials are more attractive to people than mere text information. Therefore, it will be great if the machine can automatically listen to and understand the spoken content and even visualize the key information for people. This paper represents a first attempt towards the above goal: the machine capture of spoken content. In an initial task, we want the machine to be able to hear and understand an audio story, and answer the questions related to that audio content."}, {"heading": "2. Task Definition and Contributions", "text": "In this thesis, we develop and propose a new task of machine understanding spoken content that has never before been associated with our knowledge. We take the TOEFL listening comprehension test as a corpus for this work. TOEFL is an English exam that tests the knowledge and skills of academic English for learners whose native languages are not English. In this exam, subjects would first listen to an audio story for about five minutes and then answer several questions according to this story. History is related to college life, such as the conversation between the student and Professor Xiv: 160 8,06 378v 1 [cs.C L] 23 Aug 201 6sor or a lecture in class. Each question has four possibilities where only one is correct. A real example in the TOEFL exam is shown in Figure 1. The upper part is the manual transcription of a small portion of the audio history."}, {"heading": "3. Proposed Approach", "text": "The general structure of the proposed model is shown in Figure 2. Entering the model includes the transcription of an audio story, a question, and four possible answers, all of which are presented as word sequences. First, the word sequence of the input question is presented in Section 3.1 as a question vector VQ. Using the question vector VQ, the attention mechanism is applied to extract the question-related information from the story in Section 3.2. The machine then goes through the story several times through the attention mechanism and receives a response vector VQn in Section 3.3. This response vector VQn is finally used in Section 3.4 to evaluate the safety of each choice, and the choice with the highest score is taken as a result. All the model parameters in the above procedure are trained together with the target, with 1 being used for the correct choice and 0 otherwise."}, {"heading": "3.1. Question Representation", "text": "Fig. 3 (A) shows the procedure of encoding the input query into a vector representation VQ. The input query is a sequence of T-words, w1, w2,..., wT, each word representing Wi in 1-Of-N encoding. A bidirectional gated recurrent unit (GRU) network [26-28] takes one word from the input query one at a time. In Fig. 3 (A), the hidden layer output of the forward GRU (green rectangle) to the time index t is denoted by yf (t), and that of the backward GRU (blue rectangle) by yb (t). After looking at all the words in the question, the hidden layer output of the forward GRU network is denoted by the last time index yf (T) and that of the backward GRU network by the first time index yb (1) to the question vector representation VQ or Vyb (1) (T)."}, {"heading": "3.2. Story Attention Module", "text": "Fig. 3 (B) shows the attention mechanism that takes the question vector VQ obtained in Fig. 3 (A) and the transcriptions of the story as input to encode the whole story into a vector representation VS. The transcription of the story is a very long word sequence with many sentences, so that we only show two sentences of 4 words each for simplicity. There is a bidirectional GRU in Fig. 3 (B) that encodes the whole story into a vector representation VS. The vector representation of the t-th word St is constructed by linking the hidden layered outputs of the forward and backward GRU networks, that is St = [yf (t).yb (t)]]. Then the attention value t for each time index t is the cosmic similargement between the question vector VQ and the word vector VQ and the vector St for each word attention Q2."}, {"heading": "3.3. Hopping", "text": "The overall picture of the proposed model is shown in Fig. 2, where Fig. 3 (A) and (B) are component modules (identified as Fig. 3 (A) and (B) of the complete proposed model. To the left of Fig. 2, the input query is first converted by the module in Fig. 3 (A) into a question vector VQ0. This VQ0 is used to calculate the attention values \u03b1t to obtain the story vector VS1 through the module in Fig. 3 (B). Subsequently, VQ0 and VS1 are added to a new question vector VQ1. This process is referred to in Fig. 2 as the first hop (Hop 1). Output of the first hop VQ1 can be used to calculate the new attention to obtain a new story vector VS1. This can be considered the machine going through the story again to refocus the story with a new question vector."}, {"heading": "3.4. Answer Selection", "text": "As in the upper part of Fig. 2, the same method is used here as in Fig. 3 (A) before the question was encoded in VQ to encode four selection vectors in selection vector representations VA, VB, VC, VD. Then the cosinal similarity between the output of the last hop VQn and the selection vectors is calculated and the choice with the greatest similarity is selected."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Experimental Setup", "text": "\u2022 Dataset Collection: The TOEFL data set collected contained a total of 963 examples (717 for training, 124 for validation, 122 for testing), each containing a story, a question and 4 choices. In addition to the audio recording of each story, the manual transcriptions of the story are also available. We used a Pydub library [29] to segment the complete audio recording into utterances. Each audio recording has an average of 57.9 utterances. There are an average of 657.7 words in a story, 12.01 words in question and 10.35 words in each choice. \u2022 Speech Recognition: We used the CMU voice recognition - Sphinx [30] to transcribe the audio history. The detection word error rate (WHO) was 34.32%. \u2022 Pre-processing: We used a pre-rehearsed 300-dimension glove vector model [31] to get the vector for each word to be represented as a vector choice and expression in each question."}, {"heading": "4.2. Baselines", "text": "We compared the proposed model with some commonly used simple baselines in [25] and the memory network [17]. \u2022 Selection length: The most na\u00efve baseline is to select the selection based on the number of words in it, without listening to the stories and looking at the questions, which included: (i) selecting the longest choice in memory, (ii) selecting the shortest choice, or (iii) selecting the length that is most different from the other choices. \u2022 Selection similarity: using the vector representations for the decisions in the pre-processing of section 4.1, we calculated the cosinal distance between the four decisions and chose the one that is (i) most similar to the other, or (ii) the one that is most different from the others. \u2022 Selection similarity: Using the vector representations for the decisions in section 4.1, we selected the vector representations for the decisions and the questions in the pre-processing of section 4.1, the choice with the highest cosmic similarity to the choice of the cosmic similarity."}, {"heading": "4.3. Results", "text": "The results are shown in Table 1. We trained the model on the manual transcriptions of the stories, while we tested the model on the test set with the two manual transcriptions (column labeled \"Manual\") and ASR transcriptions (column labeled \"ASR\"). \u2022 Choice length: Part (a) shows the performance of three models for selecting the answer with the longest, shortest or most different length, ranging from 23% to 35%. \u2022 Within Choice similarity: Part (b) shows the performance of two models for selection, which differs from the others."}, {"heading": "4.4. Analysis on a typical example", "text": "Fig. 4 shows the visualization of the attention weights obtained for a typical sample story in the test sentence, with the proposed AMRNN model at the word or sentence level drawing attention to manual or ASR transcriptions. The darker the color, the higher the weights. Only a small portion of the story is shown where the model's response made a good difference. This story was mainly about the thick cloud and some mysteries on Venus. The question for this story is \"What is a possible origin of Venus's clouds?\" and the correct choice is \"Gases released as a result of volcanic activity.\" In the manual transcription cases (left half of Fig. 4), both models with word or sentence level attention correctly answered the question and focused on the core and informative words / sentences on the question. The sentence model has successfully captured the sentence, including... \"volcanic eruptions that often ignore gases\"; while the key word model \"we look at some key terms incorrectly as in R.\""}, {"heading": "5. Conclusions", "text": "In this thesis, we create a new task using the TOEFL corpus. TOEFL is an English exam in which the learner is asked to listen to a story for up to 5 minutes and then answer some relevant questions. To answer the question, the learner must perform deduction, logic, and summary. We developed a model that is capable of completing this challenging task. For manual transcriptions, the proposed model achieved an accuracy of 51.56%, while the very powerful storage network achieved only 39.17% accuracy. Even with ASR transcriptions with WHO of 34.32%, the proposed model still delivered an accuracy of 48.33%. We also found that although attention at the sentence level achieved the best results at the manual transcription level, attention at the word level exceeded that at the sentence level when ASR errors occurred."}, {"heading": "6. References", "text": "[1] P. R. C. i Umbert, Factoid Question Answering for spoken docu-ments P. Jibert, Ph.D. Dissertation, Universitat Polite de Catalunya, 2012. [2] P. R. C. i Umbert, J. T. Borra, and L. M. Villodre, \"answer spoken questions.\" [3] S.-R. Shiang, H.-Y.-Lee, and L.-S. Lee, \"answer spoken questions with tree-structured conditional random fields and two-layered random ways.\" (S.-S. S. Shiang, S. S., H.-Y.-Y.) S. S. S. S. S. S. S. S. S. S. S. S. S., S. S. S. S. S. S. S. S. S., S. S. S. S. S. S. S. S. S. S. S. S. S. S. S., S. S., S. S. S., S., S. S., S., S. S., S. S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., and S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S., S.,"}], "references": [{"title": "Factoid question answering for spoken documents", "author": ["P.R.C. i Umbert"], "venue": "Ph.D. dissertation, Universitat Polit\u00e8cnica de Catalunya, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Spoken question answering using tree-structured conditional random fields and two-layer random walk.", "author": ["S.-R. Shiang", "H.-y. Lee", "L.-s. Lee"], "venue": "INTERSPEECH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Sibyl, a factoid question-answering system for spoken documents", "author": ["P.R. Comas", "J. Turmo", "L. M\u00e0rquez"], "venue": "ACM Trans. Inf. Syst., 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Multilingual Information Access Evaluation I. Text Retrieval Experiments: 10th Workshop of the Cross-Language Evaluation Forum, CLEF 2009, Corfu, Greece, September 30 - October 2, 2009, Revised Selected Papers", "author": ["J. Turmo", "P.R. Comas", "S. Rosset", "O. Galibert", "N. Moreau", "D. Mostefa", "P. Rosso", "D. Buscaldi"], "venue": "ch. Overview of QAST", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Overview of QAST 2008", "author": ["J. Turmo", "P. Comas", "S. Rosset", "L. Lamel", "N. Moreau", "D. Mostefa"], "venue": "Working Notes for the CLEF 2008 Workshop,, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-Language Evaluation Forum", "author": ["D. Giampiccolo", "P. Forner", "J. Herrera", "A. Pe\u00f1as", "C. Ayache", "C. Forascu", "V. Jijkoun", "P. Osenova", "P. Rocha", "B. Sacaleanu", "R. Sutcliffe"], "venue": "ch. Overview of the CLEF 2007 Multilingual Question Answering Track,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2008}, {"title": "Improving deep neural networks for lvcsr using rectified linear units and dropout", "author": ["G.E. Dahl", "T.N. Sainath", "G.E. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8609\u20138613.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2013}, {"title": "New types of deep neural network learning for speech recognition and related applications: An overview", "author": ["L. Deng", "G. Hinton", "B. Kingsbury"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8599\u20138603.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "arXiv preprint arXiv:1404.2188, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2493\u20132537, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Teaching machines to read and comprehend", "author": ["K.M. Hermann", "T. Kocisky", "E. Grefenstette", "L. Espeholt", "W. Kay", "M. Suleyman", "P. Blunsom"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1684\u20131692.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Largescale simple question answering with memory networks", "author": ["A. Bordes", "N. Usunier", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1506.02075, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end memory networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2431\u20132439.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "arXiv preprint arXiv:1506.07285, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["A.M. Rush", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1509.00685, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1997}, {"title": "Question answering with subgraph embeddings", "author": ["A. Bordes", "S. Chopra", "J. Weston"], "venue": "arXiv preprint arXiv:1406.3676, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "A factoid question answering system using answer pattern matching.", "author": ["N.P. Er", "I. Cicekli"], "venue": "in IJCNLP,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 1156\u20131165.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Movieqa: Understanding stories in movies through question-answering", "author": ["M. Tapaswi", "Y. Zhu", "R. Stiefelhagen", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "arXiv preprint arXiv:1512.02902, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Sphinx-4: A flexible open source framework for speech recognition", "author": ["W. Walker", "P. Lamere", "P. Kwok", "B. Raj", "R. Singh", "E. Gouvea", "P. Wolf", "J. Woelfel"], "venue": "2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Glove: Global vectors for word representation.", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "in EMNLP, vol", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["T. Tieleman", "G. Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning, vol. 4, p. 2, 2012.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Maintaining stream statistics over sliding windows", "author": ["M. Datar", "A. Gionis", "P. Indyk", "R. Motwani"], "venue": "SIAM Journal on Computing, vol. 31, no. 6, pp. 1794\u20131813, 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1813}], "referenceMentions": [{"referenceID": 0, "context": "The listening comprehension task considered here is highly related to Spoken Question Answering (SQA) [1, 2].", "startOffset": 102, "endOffset": 108}, {"referenceID": 1, "context": "SQA usually worked with ASR transcripts of the spoken content, and used information retrieval (IR) techniques [3] or relied on knowledge bases [4] to find the proper answer.", "startOffset": 110, "endOffset": 113}, {"referenceID": 2, "context": "Sibyl [5], a", "startOffset": 6, "endOffset": 9}, {"referenceID": 3, "context": "Question Answering in Speech Transcripts (QAST) [6\u20138] has been a well-known evaluation program of SQA for years.", "startOffset": 48, "endOffset": 53}, {"referenceID": 4, "context": "Question Answering in Speech Transcripts (QAST) [6\u20138] has been a well-known evaluation program of SQA for years.", "startOffset": 48, "endOffset": 53}, {"referenceID": 5, "context": "Question Answering in Speech Transcripts (QAST) [6\u20138] has been a well-known evaluation program of SQA for years.", "startOffset": 48, "endOffset": 53}, {"referenceID": 6, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 108, "endOffset": 114}, {"referenceID": 7, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 108, "endOffset": 114}, {"referenceID": 8, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 108, "endOffset": 114}, {"referenceID": 9, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 128, "endOffset": 135}, {"referenceID": 10, "context": "With the fast development of deep learning, neural networks have successfully applied to speech recognition [9\u201311] or NLP tasks [12,13].", "startOffset": 128, "endOffset": 135}, {"referenceID": 11, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 12, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 13, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 14, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 15, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 16, "context": "A number of recent efforts have explored various ways to understand multimedia in text form [14\u201319].", "startOffset": 92, "endOffset": 99}, {"referenceID": 14, "context": "They incorporated attention mechanisms [17] with Long ShortTerm Memory based networks [20].", "startOffset": 39, "endOffset": 43}, {"referenceID": 17, "context": "They incorporated attention mechanisms [17] with Long ShortTerm Memory based networks [20].", "startOffset": 86, "endOffset": 90}, {"referenceID": 18, "context": "In Question Answering field, most of the works focused on understanding text documents [21\u201324].", "startOffset": 87, "endOffset": 94}, {"referenceID": 19, "context": "In Question Answering field, most of the works focused on understanding text documents [21\u201324].", "startOffset": 87, "endOffset": 94}, {"referenceID": 20, "context": "In Question Answering field, most of the works focused on understanding text documents [21\u201324].", "startOffset": 87, "endOffset": 94}, {"referenceID": 21, "context": "Even though [25] tried to answer the question related to the movie, they only used the text and image in the movie for that.", "startOffset": 12, "endOffset": 16}, {"referenceID": 22, "context": "A bidirectional Gated Recurrent Unit (GRU) network [26\u201328] takes one word from the input question sequentially at a time.", "startOffset": 51, "endOffset": 58}, {"referenceID": 23, "context": "A bidirectional Gated Recurrent Unit (GRU) network [26\u201328] takes one word from the input question sequentially at a time.", "startOffset": 51, "endOffset": 58}, {"referenceID": 24, "context": "A bidirectional Gated Recurrent Unit (GRU) network [26\u201328] takes one word from the input question sequentially at a time.", "startOffset": 51, "endOffset": 58}, {"referenceID": 25, "context": "\u2022 Speech Recognition: We used the CMU speech recognizer - Sphinx [30] to transcribe the audio story.", "startOffset": 65, "endOffset": 69}, {"referenceID": 26, "context": "\u2022 Pre-processing: We used a pre-trained 300 dimension glove vector model [31] to obtain the vector representation for each word.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "We used RmsProp [32] with initial learning rate of 1e-5 with momentum 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 21, "context": "We compared the proposed model with some commonly used simple baselines in [25] and the memory network [17].", "startOffset": 75, "endOffset": 79}, {"referenceID": 14, "context": "We compared the proposed model with some commonly used simple baselines in [25] and the memory network [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "\u2022 Sliding Window [25, 33]: This model try to found a window of W utterances in the story with the maximum similarity to the question.", "startOffset": 17, "endOffset": 25}, {"referenceID": 28, "context": "\u2022 Sliding Window [25, 33]: This model try to found a window of W utterances in the story with the maximum similarity to the question.", "startOffset": 17, "endOffset": 25}, {"referenceID": 14, "context": "\u2022 Memory Network [17]: We implemented the memory network with some modifications for this task to find out if memory network was able to deal it.", "startOffset": 17, "endOffset": 21}, {"referenceID": 14, "context": "The embedding size of the memory network was set 128, stochastic gradient descent was used as [17] with initial learning rate of 0.", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "Multimedia or spoken content presents more attractive information than plain text content, but it\u2019s more difficult to display on a screen and be selected by a user. As a result, accessing large collections of the former is much more difficult and time-consuming than the latter for humans. It\u2019s highly attractive to develop a machine which can automatically understand spoken content and summarize the key information for humans to browse over. In this endeavor, we propose a new task of machine comprehension of spoken content. We define the initial goal as the listening comprehension test of TOEFL, a challenging academic English examination for English learners whose native language is not English. We further propose an Attention-based Multi-hop Recurrent Neural Network (AMRNN) architecture for this task, achieving encouraging results in the initial tests. Initial results also have shown that word-level attention is probably more robust than sentence-level attention for this task with ASR errors.", "creator": "LaTeX with hyperref package"}}}