{"id": "1608.06386", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Which techniques does your application use?: An information extraction framework for scientific articles", "abstract": "Every field of research consists of multiple application areas with various techniques routinely used to solve problems in these wide range of application areas. With the exponential growth in research volumes, it has become difficult to keep track of the ever-growing number of application areas as well as the corresponding problem solving techniques. In this paper, we consider the computational linguistics domain and present a novel information extraction system that automatically constructs a pool of all application areas in this domain and appropriately links them with corresponding problem solving techniques. Further, we categorize individual research articles based on their application area and the techniques proposed/used in the article. k-gram based discounting method along with handwritten rules and bootstrapped pattern learning is employed to extract application areas. Subsequently, a language modeling approach is proposed to characterize each article based on its application area. Similarly, regular expressions and high-scoring noun phrases are used for the extraction of the problem solving techniques. We propose a greedy approach to characterize each article based on the techniques. Towards the end, we present a table representing the most frequent techniques adopted for a particular application area. Finally, we propose three use cases presenting an extensive temporal analysis of the usage of techniques and application areas.", "histories": [["v1", "Tue, 23 Aug 2016 05:27:46 GMT  (1467kb,D)", "http://arxiv.org/abs/1608.06386v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["soham dan", "sanyam agarwal", "mayank singh", "pawan goyal", "animesh mukherjee"], "accepted": false, "id": "1608.06386"}, "pdf": {"name": "1608.06386.pdf", "metadata": {"source": "CRF", "title": "Which techniques does your application use?: An information extraction framework for scientific articles", "authors": ["Soham Dan", "Sanyam Agarwal", "Mayank Singh", "Pawan Goyal", "Animesh Mukherjee"], "emails": ["sohamd@cse.iitkgp.ernet.in", "sanyama@cse.iitkgp.ernet.in", "mayank.singh@cse.iitkgp.ernet.in", "pawang@cse.iitkgp.ernet.in", "animeshm@cse.iitkgp.ernet.in"], "sections": [{"heading": null, "text": "CCS Concepts \u2022 Information Systems \u2192 Information Gathering; Access Problems and Objectives; Information Extraction; Keywords, Techniques, Language Models, Citation Context, Time AnalysisACM ISBN 978-1-4503-2138-9.DOI: 10.1145 / 1235"}, {"heading": "1. INTRODUCTION", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2. RELATED WORK", "text": "Information extraction (IE) from scientific articles combines approaches from natural language processing and data mining and generates considerable research interest in recent times. In particular, there has been burgeoning research interest in the field of biomedical documents. Jones et al. [25] extracted keywords from the full text of biomedical articles and claims that there is an extraction of information that exhibits heterogeneity in the keywords from different sections. Muller et al. [19] have developed the Textpresso framework that explores the use of ontologies for obtaining and extracting information. In a similar paper, Fukuda et al. [4] have proposed an IE system for protein name extraction. There has been significant work in the field of protein structure analysis. Gaizauskas et al. [5] PASTA, a structural system for developing a Friedman, is being proposed."}, {"heading": "3. DATASET", "text": "We use AAN (ACL Anthology Network) [23] dataset, which consists of 21,213 full-text articles in the field of computer linguistics and natural language processing. The dataset includes essays from 1965 - 2013 from 342 ACL sites. We also edit the full-text articles to eliminate OCR errors. Data set and code are available online at http: / / tinyurl.com / hhrpfge."}, {"heading": "4. METHODOLOGY", "text": "In this section, we describe a method of mapping between a list of areas and a list of techniques. As we have already explained in the introduction, the mapping task is divided into five steps: (1) creating a ranking of areas, (2) categorizing work based on areas, (3) creating a ranking of techniques, (4) categorizing work based on techniques, and (5) finally mapping between the list of areas on the list of techniques. The current work makes a twofold contribution. First, we are developing a system that automatically extracts areas and techniques used in an essay. Second, we are mapping the area to the corresponding popular techniques. Together, they can be considered structured metadata for individual scientific articles as well as a field of research.Next, we briefly describe five phases in detail:"}, {"heading": "4.1 Building the repertoire of areas", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4.2 Categorizing papers on the basis of application area", "text": "From the first step, we get a pool of scopes, and we have to assign individual papers to one of these scopes. Individual papers from the entire corpus are categorized to their corresponding scopes on the basis of two strategies - direct match and relevance according to the language models defined for different scopes (see Figure 1). Direct match: In the direct approach, we look for an explicit match of the string between the title or abstraction and one of the scopes. A non-trivial task here is to identify the position and text in the summary of a paper. We achieve this by using handwritten rules [digit]? [digit]] to [digit] [digit] [digit \"introduction\"] (where the \"section number and optional\" is used) to match the part of the paper between the summary and introduction. Also, we have converted the text to lowercase during pre-processing."}, {"heading": "4.3 Extracting the repertoire of techniques", "text": "This year, more than ever before in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a, a place, a, a place, a place, a, a, a place, a place, a place, a place, a, a, a place, a place, a place, a, a, a, a, a place, a, a place, a, a, a place, a, a, a, a, a, a place, a, a, a, a, a, a, a, a, a, a, a, a"}, {"heading": "4.4 Extracting techniques for an individual", "text": "We have built a similar vector of these noun expressions, where the ith component of the vector is the raw number of the noun expression drawn from the global vector presented in the previous section. If a particular noun expression is missing from the global vector, its weight is set to zero. Once this vector is created for method paper X, we simply take a point product between this local vector of X and the global vector to create a ranking of possible techniques for X. Now, we simply read the top K techniques on that ranking list, as the techniques for which the method paper X is used appear as a statistical technique. Note, however, that we have taken a point product between the local and global vector of X to create a ranking of possible techniques for X. These noun expressions, which often occur in almost all papers, appear as a statistical technique, even if they occur only once in the local vector, we will ignore a point product between the global noun printing and we do not use for the local noun printing. \""}, {"heading": "4.5 Mapping between area and the list of techniques", "text": "In Algo-Result: Map by area: List of techniques for this area Initialization T \u2190 \u03c6; for P-Corpus doA-Area (P) T \u2190 \u03c6 MSet \u2190 MethodPapersCitedBy (P) for M-MSet doT-Technique (M) end T-A-Tend Algorithm 1: Mapping Areas to List of Techniquesrithm 1, Area (P) is a function that returns the area of a paper P and similarly Technique (M) is a function that returns the techniques introduced or improved by Method Paper M. Furthermore, MethodPapersCitedBy (P) is a function that returns all method papers cited by Paper P in its method section."}, {"heading": "5. EVALUATION RESULTS", "text": "In this paper we have presented a variety of experiments that require careful analysis of the results in order to assess the effectiveness of the algorithms. The first four stages are crucial for the construction of the information extraction system and therefore a critical evaluation is presented. Creating the mapping table is a result of the first four stages and examples have been presented to illustrate the nature of the mapping table."}, {"heading": "5.1 Evaluation of the ranked list of areas", "text": "First, we present the relative performance of the three schemes (in terms of precision) for the compilation of the ranking, in Table 1. Since Scheme 3 gave the highest precision (assignments were judged by an expert) on the list of the top 30 areas, we used this scheme for the compilation of the ranking in subsequent stages. In this part of the evaluation, we have a ranking of potential areas in the computational linguistics domain extracted from the ACL Corpora. We use standard precision recall measures for the purpose of evaluation. However, it should be noted that there is no easy way to measure the recall of the algorithm on the entire ACL Corporate, since all possible areas in the Corpus are not known a-priori. This is because the only way to construct a definitive list of all possible areas that are manually identified by each individual paper in the Corpus."}, {"heading": "5.2 Evaluating the extraction of areas from individual papers", "text": "Once we have a probable range for all work in the ACL corpus, we need to validate the accuracy of our assignment algorithm. By means of an online survey, we have validated these tasks by a team of domain experts. A set of 120 papers has been validated by these domain experts. The accuracy of our method is calculated as x120, where x is the number of scope assignments marked as correct by the jurors. Of the 120 papers to be evaluated, the jurors have identified 88 ratings as correct, so the accuracy of our method is 73.3%."}, {"heading": "5.3 Evaluating the list of techniques used in the corpus", "text": "In this case, the calculation of the callback is difficult if we work with the best K techniques for each method work. To simplify the task, we calculate the callback only for the highest rated technique for each method work. The experiment is constructed as follows - for a randomly selected set of 30 papers, we aggregate all their citation correlations from the method sections of the quotation work. The smaller corpus of 30 introduced or improved papers for 26 different techniques is assessed by a domain expert. Afterwards, we executed the noun sentence extraction algorithm to get the list of techniques. 21 of these 26 techniques could be extracted, so the callback of our algorithm is about 81%. The precision calculation is made by finding out what percentage of the best K techniques is real computational linguistics domain techniques as assessed by domain experts."}, {"heading": "5.4 Evaluating the extraction of techniques from a method paper", "text": "Once we have the repertoire of tasks for the entire corpus through the use of the point product-based ranking, we can identify the specific techniques for which a methodology paper is used. We now need to evaluate the accuracy of this mapping of techniques for a methodology paper. To ensure the simplicity of the evaluation, we report only the best-rated technique for a methodology paper. For the evaluation, a sample of 60 papers was randomly selected and the assessments of a group of domain experts collected through an online survey were used to test the accuracy of our methodology. Thus, each domain expert evaluated the technique assigned to him for the papers submitted and reported how many of them were correct. The accuracy of our algorithm is then calculated as x60, where x is the number of correct assignments of techniques to the methodology papers. In this evaluation, our algorithm had 36 correct assignments and thus the accuracy of our method is 60%."}, {"heading": "5.5 Mapping from areas to list of techniques", "text": "In this section, we present some of the entries we have obtained from the mapping table for some of the higher ranked areas of computational linguistics. As we can see from the examples, the extracted techniques consist of subtasks, tools and datasets popularly used in one area. In addition, the techniques are quite detailed and describe many techniques that are very specific to the areas - for example, Bleu score and hurricane algorithm: machine translation, Nivres-Arc-zealous: dependency parsing and spin model: opinion survey. It is also interesting to observe that the extracted techniques cover a wide range of time periods - for example: Collins Parser, Berkeley Parser, Charniak Parser, Stanford Parser, MST Parser and Malt Parser: Dependency Parsing have been presented to the Computational Linguistics (CL) community at significantly different times."}, {"heading": "6. USE CASE: TEMPORAL ANALYSIS", "text": "In this section, we present three use cases. Each use case is a chronological examination of areas and techniques. We analyze the development of areas of application and related techniques over a certain period of time. In the first use case, we examine the development of areas in the field of computer linguistics. The next use case deals with the development of techniques for a certain area. Finally, we examine the development of important areas in the top conferences."}, {"heading": "6.1 Evolution of areas", "text": "From the list of popular domains (based on the total number of works published in one domain) in aan, we present six representative domains, namely \"machine translation,\" \"dependency parsing,\" \"speech recognition,\" \"information extraction,\" \"summary\" and \"semantic role marking,\" and examine their popularity (percentage of work published in that period of time in the total number of work published in that period) between 1980 and 2013 in five-year windows. Figure 2 shows the temporal fluctuations for these domains and how they evolve over time. Observations: While domains such as \"machine translation\" and \"dependency parsing\" are on the rise, \"information extraction\" and \"semantic role labelling\" are declining. Another interesting observation is that until 1994 the ACL community had great interest in \"speech recognition,\" which may then experience a steep decline due to the fact that the language community was slowly splitting off."}, {"heading": "6.2 Evolution of techniques in areas", "text": "For this analysis, we divide the timeline into fixed buckets of w years. Next, we extract for each bucket popular techniques (based on the number of times each paper has cited this technique) using our proposed system. Interestingly, however, we found several trends and figures in this study. Table 7 presents the popular techniques for five sample areas. Some of the interesting trends from Table 7 are listed below: \u2022 Dependency Parsing: New techniques (malt parser, minimal tree approach (MST) parser, etc.) originated 2005 - 2009. The next year buckets, these parsers overcome the popularity of earlier parsers such as Collin's Parser, Berkeley Parser and are almost on par with Charniak Parser. In addition, we observe that the Penn tree base is widely used for dependency phases."}, {"heading": "6.3 Evolution of major areas in top conferences", "text": "We shortlisted two of the top computer linguistics conferences, the Association of Computational Linguistics (ACL) Annual Meeting and the International Conference on Computational Linguistics (COLING). We study 40 years of conference history. For each conference, the 40-year period is divided into four 10-year buckets. Next, for each conference, we extract the ten most popular areas (based on citations) for each 10-year bucket. Figure 3 presents phrase clouds that represent the evolution of the areas in these two conferences compared to the full AAN dataset itself. Some of the interesting observations from this analysis are: \u2022 Full AAN dataset: Here, we observe that areas such as \"semantic role labeling,\" \"natural language evaluation\" and \"language recognition\" have prevailed in recent decades."}, {"heading": "7. CONCLUSION AND FUTURE WORK", "text": "In this paper, we have proposed a novel system of data extraction for scientific articles. It extracts a ranking of all application areas in the field of computational linguistics. At a more granular level, it also extracts the application area for a particular paper. In addition, it extracts a ranking of all techniques as well as paper-based technology extraction. Finally, we construct a mapping of application areas for all techniques. We evaluate our system with technical experts and prove that it performs reasonably well both in terms of precision and on-demand. As a use case, we present a comprehensive analysis of the temporal variation in the popularity of techniques for a particular area. Some of the interesting observations we make here are that the areas such as \"machine translation\" and \"dependency parameters\" are on the increase in popularity, while areas such as \"speech recognition,\" \"linguistic knowledge sources\" and \"evolution of natural language\" are aimed at building on the future of data parameters, which we plan to base on several areas of application."}, {"heading": "8. REFERENCES", "text": "[1] S. A. Caraballo, et al. Toward information extraction: fying protein names from Citological R. ou. [1] S. A. Tamura, T. Takagi. Automatic construction of ahypernym-labeled noun Hierarchie from text. [2] A. M. Cohen, and W. R. Hersh. A survey of current work in biomedical text mining. Briefings in bioinformatics, 6 (1): 57-71, 2005. [3] C. Friedman, P. Kra, H. Yu, M. Krauthammer, and A. Rzhetsky. Geniuses: a natural-language processing system for the extraction of molecular pathways from journal articles. Bioinformatics, 17 (suppl 1): S74-S82, 2001. [4] K.-i. Fukuda, T. Tsunoda."}], "references": [{"title": "Automatic construction of a hypernym-labeled noun hierarchy from text", "author": ["S.A. Caraballo"], "venue": "Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 120\u2013126. Association for Computational Linguistics,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1999}, {"title": "A survey of current work in biomedical text mining", "author": ["A.M. Cohen", "W.R. Hersh"], "venue": "Briefings in bioinformatics, 6(1):57\u201371,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Genies: a natural-language processing system for the extraction of molecular pathways from journal articles", "author": ["C. Friedman", "P. Kra", "H. Yu", "M. Krauthammer", "A. Rzhetsky"], "venue": "Bioinformatics, 17(suppl 1):S74\u2013S82,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Protein structures and information extraction from biological texts: the pasta system", "author": ["R. Gaizauskas", "G. Demetriou", "P.J. Artymiuk", "P. Willett"], "venue": "Bioinformatics, 19(1):135\u2013143,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Analyzing the dynamics of research by extracting key aspects of scientific papers", "author": ["S. Gupta", "C.D. Manning"], "venue": "IJCNLP, pages 1\u20139,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Using word dependent transition models in hmm based word alignment for statistical machine translation", "author": ["X. He"], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, pages 80\u201387. Association for Computational Linguistics,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Automatic acquisition of hyponyms from large text corpora", "author": ["M.A. Hearst"], "venue": "Proceedings of the 14th Conference on Computational Linguistics - Volume 2, COLING \u201992, pages 539\u2013545, Stroudsburg, PA, USA,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Mining scientific terms and their definitions: A study of the ACL anthology", "author": ["Y. Jin", "M.-Y. Kan", "J.-P. Ng", "X. He"], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 780\u2013790, Seattle, Washington, USA, October", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning to extract entities from labeled and unlabeled text", "author": ["R. Jones"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Semeval-2010 task 5: Automatic keyphrase extraction from scientific articles", "author": ["S.N. Kim", "O. Medelyan", "M.-Y. Kan", "T. Baldwin"], "venue": "Proceedings of the 5th International Workshop on Semantic Evaluation, pages 21\u201326. Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens"], "venue": "In Proceedings of the 45th annual meeting of the ACL on interactive poster and demonstration sessions,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Linking genes to literature: text mining, information extraction, and retrieval applications for biology", "author": ["M. Krallinger", "A. Valencia", "L. Hirschman"], "venue": "Genome biology, 9(Suppl 2):1\u201314,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2008}, {"title": "Improving english-russian sentence alignment through pos tagging and damerau-levenshtein distance", "author": ["A. Kutuzov"], "venue": "ACL 2013, page 63,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Humb: Automatic key term extraction from scientific articles in grobid", "author": ["P. Lopez", "L. Romary"], "venue": "Proceedings of the 5th international workshop on semantic evaluation, pages 248\u2013251. Association for Computational Linguistics,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Fbk: cross-lingual textual entailment without translation", "author": ["Y. Mehdad", "M. Negri", "J.G.C. de Souza"], "venue": "In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Decision procedures for dependency parsing using graded constraints", "author": ["W. Menzel", "I. Schr\u00f6der"], "venue": "in proceedings of ACL\u201990. Citeseer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1998}, {"title": "Textpresso for neuroscience: searching the full text of thousands of neuroscience research papers", "author": ["H.-M. M\u00fcller", "A. Rangarajan", "T.K. Teal", "P.W. Sternberg"], "venue": "Neuroinformatics, 6(3):195\u2013204,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Scientific paper summarization using citation summary networks", "author": ["V. Qazvinian", "D.R. Radev"], "venue": " Proceedings of the 22nd International Conference on Computational Linguistics-Volume 1, pages 689\u2013696. Association for Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Citation summarization through keyphrase extraction", "author": ["V. Qazvinian", "D.R. Radev", "A. \u00d6zg\u00fcr"], "venue": "Proceedings of the 23rd International Conference on Computational Linguistics, pages 895\u2013903. Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Non-monotonic sentence alignment via semisupervised learning", "author": ["X. Quan", "C. Kit", "Y. Song"], "venue": "ACL (1), pages 622\u2013630. Citeseer,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "The ACL anthology network corpus", "author": ["D.R. Radev", "P. Muthukrishnan", "V. Qazvinian"], "venue": "Proceedings, ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries, Singapore,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Training nondeficient variants of ibm-3 and ibm-4 for word alignment", "author": ["T. Schoenemann"], "venue": "ACL (1), pages 22\u201331,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Information extraction from full text scientific articles: Where are the keywords", "author": ["P.K. Shah", "C. Perez-Iratxeta", "P. Bork", "M.A. Andrade"], "venue": "BMC Bioinformatics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Ontology-based information extraction: An introduction and a survey of current approaches", "author": ["D.C. Wimalasuriya", "D. Dou"], "venue": "Journal of Information Science,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A study of smoothing methods for language models applied to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "ACM Transactions on Information Systems (TOIS), 22(2):179\u2013214,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 21, "context": "For example, in \u201cTraining Nondeficient Variants of IBM-3 and IBM-4 for Word Alignment\u201d [24], \u2018word alignment\u2019 is an area but in ar X iv :1 60 8.", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "\u201cUsing Word-Dependent Transition Models in HMM-Based Word Alignment for Statistical Machine Translation\u201d [8], \u2018word alignment\u2019 is a technique for machine translation.", "startOffset": 105, "endOffset": 108}, {"referenceID": 22, "context": "[25] extracted keywords from full text of biomedical articles and claim that there exist a heterogeneity in the keywords from different sections.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[19] have developed the Textpresso framework, that leverage ontologies for information retrieval and extraction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[5] proposed PASTA, an IE system developed and evaluated for the protein structure domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] have developed a similar system to extract structure information about cellular pathways using a knowledge model.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "\u2019s [2] survey on biomedical text mining, Krallinger et al.", "startOffset": 3, "endOffset": 6}, {"referenceID": 11, "context": "\u2019s [14] survey on information extraction and applications for biology and Wimalasuriya et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 23, "context": "[27] on ontology based information extraction are examples of some of the popular surveys on IE for biomedical domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "[1] have extended previous work of automatically building semantic lexicons to automatic construction of a hierarchy of nouns and their hypernyms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[12] and Lopez et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[16] are two popular works in automatic keyphrase extraction from scientific articles.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[20] have explored summarization of scientific papers using citation summary networks and citation summarization through keyphrase extraction [21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] have explored summarization of scientific papers using citation summary networks and citation summarization through keyphrase extraction [21].", "startOffset": 142, "endOffset": 146}, {"referenceID": 8, "context": "Jones [11] introduced an approach for entity extraction from labeled and unlabeled text.", "startOffset": 6, "endOffset": 10}, {"referenceID": 4, "context": "In [6], they investigated the dynamics of a research community by extracting key aspects from scientific papers and showed how extracting key information help in analyzing the influence of one community on another.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "[10] proposed a supervised sequence labeling system that identifies scientific terms and their accompanying definition.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "We use AAN (ACL Anthology Network) [23] dataset which consists of 21,213 full text papers from the domain of computational linguistics and natural language processing.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "For example, paper title, \u201cMoses: Open source toolkit for statistical machine translation\u201d [13] represents an instance of the form X for Y, where Y is the application area.", "startOffset": 91, "endOffset": 95}, {"referenceID": 15, "context": ", in \u201cDecision procedures for dependency parsing using graded constraints\u201d [18]) are likely to contain the name of an application area.", "startOffset": 75, "endOffset": 79}, {"referenceID": 19, "context": "For example, given a word \u2018via\u2019 in the seed set and paper title \u201cNonMonotonic Sentence Alignment via Semisupervised Learning\u201d [22], we extract the leading phrase (k-gram) before \u2018via\u2019.", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "For example, given a paper title \u201cImproving English Russian sentence alignment through POS tagging and Damerau Levenshtein distance\u201d [15] and the above extracted phrase (k-gram) alignment, we enrich the seed set by the functional keyword \u2018through\u2019.", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "As validated by [28], the parameter \u03bb for JM smoothing should be high for long queries, which is true in our case.", "startOffset": 16, "endOffset": 20}, {"referenceID": 14, "context": "A probable explanation for this could be the introduction of techniques which perform cross-lingual textual entailment without machine translation - for example, FBK: Cross-Lingual Textual Entailment Without Translation [17].", "startOffset": 220, "endOffset": 224}], "year": 2016, "abstractText": "Every field of research consists of multiple application areas with various techniques routinely used to solve problems in these wide range of application areas. With the exponential growth in research volumes, it has become difficult to keep track of the ever-growing number of application areas as well as the corresponding problem solving techniques. In this paper, we consider the computational linguistics domain and present a novel information extraction system that automatically constructs a pool of all application areas in this domain and appropriately links them with corresponding problem solving techniques. Further, we categorize individual research articles based on their application area and the techniques proposed/used in the article. k-gram based discounting method along with handwritten rules and bootstrapped pattern learning is employed to extract application areas. Subsequently, a language modelling approach is proposed to characterize each article based on its application area. Similarly, regular expressions and high-scoring noun phrases are used for the extraction of the problem solving techniques. We propose a greedy approach to characterize each article based on the techniques. Towards the end, we present a table representing the most frequent techniques adopted for a particular application area. Finally, we propose three use cases presenting an extensive temporal analysis of the usage of techniques and application areas.", "creator": "LaTeX with hyperref package"}}}