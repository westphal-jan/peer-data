{"id": "1608.06408", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Online Learning to Rank with Top-k Feedback", "abstract": "We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over $T$ rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner's performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top $k$ ranked items, where $k \\ll m$. The first setting is \\emph{non-contextual}, where the list of items to be ranked is fixed. The second setting is \\emph{contextual}, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve $O(T^{2/3})$ regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. $k=1$. We empirically demonstrate the performance of our algorithms on simulated and real world datasets.", "histories": [["v1", "Tue, 23 Aug 2016 07:40:08 GMT  (484kb,D)", "http://arxiv.org/abs/1608.06408v1", "Under review in JMLR"]], "COMMENTS": "Under review in JMLR", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sougata chaudhuri", "ambuj tewari"], "accepted": false, "id": "1608.06408"}, "pdf": {"name": "1608.06408.pdf", "metadata": {"source": "CRF", "title": "Online Learning to Rank with Top-k Feedback", "authors": ["Sougata Chaudhuri", "Ambuj Tewari"], "emails": ["sougata@umich.edu", "tewria@umich.edu"], "sections": [{"heading": null, "text": "Keywords: ranking learning, online learning, partial monitoring, online bandits, learning theory"}, {"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2. Online Ranking with Restricted Feedback- Non Contextual Setting", "text": "All the evidence that is not in the main text is in Appendix A."}, {"heading": "2.1 Notation and Preliminaries", "text": "The defined m items to be evaluated are numbered {1, 2,., m}. A permutation \u03c3 gives an assignment of rankings to items and their inverted \u03c3 \u2212 1 gives an assignment of items to rankings. Thus \u03c3 \u2212 1 (i) = j items i is placed at position j, while \u03c3 (i) = j Regret items j at position i. Monitoring takes the form of a relevance vector R = {0, 1,.., n} m, which represents the relevance of each document for the query. If n = 1, the relevance vector is staggered binary. For n > 1, the relevance vector is multigraded in the form of a relevance vector. R (i) denotes the ith component of R. The subscript t is used exclusively to denote time t. We denote {1,.., n} by [n]. The learner can be selected from m! actions (permutations), while opponents can be selected from nature."}, {"heading": "2.2 Ranking Measures", "text": "We consider ranking measures, which can be expressed in the form f (\u03c3) \u00b7 R, where the function f: Rm \u2192 Rm is composed of m copies of a univariate, monotonous, scalar value function. Thus, f (\u03c3) = [fs (\u03c3 \u2212 1 (1))), fs (\u03c3 \u2212 1 (2),., f s (\u03c3 \u2212 1 (m)]], where fs: R \u2192 R. Monotonous (increasing) means fs (\u03c3 \u2212 1 (i))) \u2265 fs (\u03c3 \u2212 1 (j))))), whenever \u03c3 \u2212 1 (i) > \u03c3 \u2212 1 (j), Monotonic (decreasing) is similarly defined. The following popular ranking measures can be expressed in the form f (\u03c3): PairwiseLoss & SumLoss: PairwiseLoss is limited to binary relevance and defined as: PairwiseLoss is limited to binary relevance and defined as: Pm is limited to binary relevance ectors, defined as Pr = = 1 m."}, {"heading": "2.3 Relevant Definitions from Partial Monitoring", "text": "We develop all results in the context of SumLoss, with binary relevance vector. We then extend the results to other ranking metrics. Our most important results on repentance limits are based on some of the theory for abstract partial monitoring games developed by Bartok et al. (2014) and Foster and Rakhlin (2012). To facilitate understanding, we reproduce the relevant notations and definitions in the context of SumLoss. We will explicitly mention when deriving the results for Top-k feedback matrix, and when limiting ourselves to Top-1 feedback matrix. Loss and feedback matrices: The online learning game with the SumLoss metrics and Top-1 feedback can be expressed in the form of a pair of loss matrix and feedback matrix. The Loss matrix L is an m! x m \u00b7 m-dimensional matrix, with lines pointing to the actions of the learner's relevance (the permutance of actions and the splits)."}, {"heading": "2.4 Minimax Regret for SumLoss", "text": "The minimax remorse for SumLoss, limited to top-1 feedback, is established by showing that: a) SumLoss satisfies global observability and b) it does not satisfy local observability."}, {"heading": "2.4.1 Global Observability", "text": "Definition 3: The condition of global observation applies, w.r.t. Loss matrix L = > Feedback matrix L = > Feedback matrix H = > 2. (> Feedback matrix H = > 2.), if for each pair of actions of the learner the (vector) loss difference between each pair of actions of the learner must belong to the vector space spanned by columns of (transposed) signal matrices corresponding to all possible actions of the learner. We derive from this the following theorem on global observation for SumLoss.Theorem 1. The global observation condition, according to definition 3, holds w.r.t. Loss of matrix L and feedback matrix H defined for SumLoss, for each m \u00b2 1. Proof. For each action of the learner Rb."}, {"heading": "2.4.2 Local Observability", "text": "Definition 4: Two Pareto-optimal actions for SumLoss.Lemma 2. For Sumoss, each of the two actions is an optimal action. Definition 4: Two Pareto-optimal actions for SumLoss.LemLoss. Definition 4: Two Pareto-optimal actions i and j are called neighboring actions if Ci-Cj is a (2) dimensional polytopic (where Ci-Ck). Definition 5: A pair of adjacent (learning) actions i and j are called locally observable when \"i-j\" j actions are called N + i, jCol (S > k). Definition 5: A pair of adjacent (learning) actions i and j are called locally observable when \"i-j\" actions are called. \"First, we present the following two Lemmas to characterize Pareto-optimal actions and adjacent actions for SumLoss.LemLoss."}, {"heading": "2.5 Minimax Regret Bound", "text": "In fact, the fact that the game is globally observable (Theorem 1), combined with Theorem 3), is an opposing strategy that generates relevance vectors so that the learner's expected regret can be increased (T 2 / 3). The fact that the game is globally observable (Theorem 1), combined with Theorem 3), is able to generate relevance vectors so that the learner's expected regret is increased (T 2 / 3). The fact that the game is globally observable (Theorem 1), combined with Theorem 3), giving the height of relevance vectors, gives an algorithm that was originally given in Piccolboni and Schindelhauer."}, {"heading": "2.6 Algorithm for Obtaining Minimax Regret under SumLoss with Top k Feedback", "text": "We first provide a general algorithmic framework to obtain an O (T 2 / 3) remorse for SumLoss, with feedback on the best placed items per round, for k \u2265 1. Then we create a specific algorithm that spends O (m logm) time per round (i.e. highly efficient) and receives a remorse via rate O (poly (m) T 2 / 3)."}, {"heading": "2.6.1 General Algorithmic Framework", "text": "Within each block, we will assign a small number of rounds for pure exploration, which will allow us to estimate the information received at the beginning of the block in order to maintain the distribution over the respective opponent in that block. The estimated average vector is cumulated across blocks and then transmitted to a complete information algorithm for the next block. The randomized complete information algorithm uses the information received at the beginning of the block to maintain the distribution (learner's actions). In each round in the new block, actions are selected and presented according to the distribution. The key feature of the randomized complete information algorithm is this: for each online game in an adversarial setting played via T rounds, if the loss of each action is known at the end of each round (complete information), the algorithm should have an expected rate of O (C-T), where the difference between the loss and the action exists."}, {"heading": "2.6.2 Computationally Efficient Algorithm with FTPL", "text": "This results in an easy-to-implement algorithm implementation that is only performed in a single round. (The structure of our problem allows to be selected during the exploitation round.) This leads to an easy-to-implement algorithm implementation that is only performed in a single round. (The structure of our problem allows to be selected during the exploitation round.) This leads to an easy-to-implement algorithm implementation that is only performed in a single round. (The structure of our problem allows to be selected during the exploitation.) This leads to an easy-to-implement algorithm implementation. (The structure of our problem allows to be selected during the exploitation.) This results in an easy-to-implement algorithm implementation that is only performed in a single round. (The structure of our problem allows to be selected during the exploitation.)"}, {"heading": "2.7 Regret Bounds for PairwiseLoss, DCG and Precision@n", "text": "PairwiseLoss: As we saw in Eq. 2, the regret of SumLoss is the same as the regret of PairwiseLoss (1). (For example, SumLoss in Corollary 9 can be replaced with PairwiseLoss to achieve exactly the same result. DCG: All SumLoss results can be extended to DCG (see Appendix A). In addition, the results can even be extended to multi-level relevance vectors. The main differences between SumLoss and DCG are the following: the former is a loss function; the latter is a win function. Also, for DCG, f (s) 6 = \u2212 1 (see definition in Sec.2.2) and if relevance is multi-level, DCG cannot be expressed."}, {"heading": "2.8 Non-Existence of Sublinear Regret Bounds for NDCG, AP and AUC", "text": "As explained in paragraph 2.2, NDCG, AP and AUC are normalized versions of the measurands DCG, Precision @ n and PairwiseLoss. We have the following problem for all these normalized ranking measurands. Tagma 12. The condition of global observability according to Definition 1 fails for NDCG, AP and AUC if feedback is limited to peak values. Combining the above problem with Theorem 2 by Bartok et al. (2014), we come to the conclusion that there can be no algorithm that exhibits sublinear regret for one of the following measurands: NDCG, AP or AUC if it is based on Top-1 feedback. Theorem 13. There is an online game for NDCG with top-1 feedback, so that there is an algorithm for each learner that generates relevance vectors so that the expected regret of the learner is greater (T)."}, {"heading": "3. Online Ranking with Restricted Feedback- Contextual Setting", "text": "All evidence not included in the main text is in Appendix B."}, {"heading": "3.1 Problem Setting and Learning to Rank Algorithm", "text": "First, we introduce some additional notations to Section 2.1. In contextual placement, each question and associated dots (documents) are presented together as a feature matrix. (The feature matrices are presented as feature vectors in Rd.) The feature matrices are considered as ancillary information (context) and represent different terms that are in contrast to the fixed terms in the first part of our work. Xi: denotes a ranking of X. We assume that the feature vectors that represent documents are limited by RD in \"2 Norm. The relevance vectors are the same as before.As per traditional learning to ranking with query document matrices, documents are ranked according to a ranking order. The prevailing technique is to represent a ranking order and maintain ranking."}, {"heading": "3.2 Unbiased Estimators of Gradients of Surrogates", "text": "Algorithm 2 can be implemented for any ranking surrogate, as long as an unbiased estimator of the gradient can be constructed from random feedback. In order to obtain formal guarantees of remorse, we will construct the unbiased estimator of four major ranking surrogates, three of which are popular convex surrogates, one each from the three most important learning-to-rank methods, i.e. pointwise, pairwise and list-wise methods. The fourth method is a popular non-convex surrogate. Shorthand notations: We note that by chain rule we focus on unbiased estimators of \"swtt\" (Xtw, Rt) = \"swtt\" (s wt, Rt) and take a matrix vector product with \"Xt.\" In order to reduce the notation in our environment, we focus on unbiased estimators of \"swtt\" (product, wt, Rt and Rt)."}, {"heading": "3.2.1 Convex Surrogates", "text": "We will summarize the unbiased estimate of the gradient of the square loss (Cossock and Zhang, 2006) as follows: (1) The unbiased estimate of R (s, R) is 2 (s, R) 2 (s, R) 2 (s, R) 2 (s, R) 2 (s, R) 2 (s, R) 2 (s, R) 2 (s, R) 2 (s) 2 (s, R) 2 (s, R) 2 (s) 2 (s, R) 2 (s, S) 2 (s, S) 2 (s, S) 2 (s, S) 2 (s) 2 (s).Pairwise Method: We will use the unbiased estimate of the gradient of the hinelike in RankSVM (s, 2002): The unbiased estimate of the gradient in RankSVM (s, 2002): (s, R) 6 (s) 1 (R) > i) We will use the unbiased estimate of the gradient of the M (s, 2002)"}, {"heading": "3.2.2 Non-convex Surrogate", "text": "We will give an example of a non-convex surrogate for which Alg. 2 is applicable (but will have no guarantees of remorse due to non-convection). We will choose the SmoothDCG surrogate specified in (Chapelle and Wu, 2010), which has proven to be very competitive. We will consider SmoothDCG @ 1, the smooth version of DCG @ 1 (i.e., DCG, which focuses only on the best-rated document). The surrogate is defined as: SD (s, R) = 1 x x x m j = 1 exp (j) (j) (s) / h exp (s), where p (hs) exp (s) exp (s), where it is a (known) smoothing parameter and a (R) = 1 x x (R = 1 x) exp (s (j) = 1 x) (i = 1 x) (i = 1 x) (j = 1 x)."}, {"heading": "3.3 Computational Complexity of Algorithm 2", "text": "Three of the four key steps regulating the complexity of algorithm 2, i.e., construction of s-t, \u03c3-t and sorting can all be performed in O (m log (m)) time. The only bottleneck could have been calculations of p (2001) in square loss, (modified) ListNet loss and SmoothDCG loss, and p (2001) in RankSVM loss, as they contain sums of permutations. However, they have a compact representation, i.e. p (2001) = (2001), p (2001) = (2001) + ccm) 1 (2001) + \u0421T (2001) + \u0421T (2001) + \u0421T (2001) + \u0421T (2001) (2001) (2001) 6 = \u0445T (1)) and p (2001)."}, {"heading": "3.4 Regret Bounds", "text": "The underlying Deterministrization of our algorithms is in each round, in which we learn the rate and the expectation of all random quantities in the algorithms.However, from the perspective of loss of algorithms (Rt) we are not able to achieve the underlying Deterministrization of algorithms.The underlying Deterministrization of our algorithms is not able to achieve the underlying Deterministrization of our algorithms.The underlying Deterministrization of our algorithms is not in a position to achieve the underlying Deterministrization of our algorithms.The underlying Deterministrization of our algorithms is not in a position to achieve the underlying Deterministrization of our algorithms.The underlying Deterministrification of our algorithms is not in a way to achieve the underlying Deterministrization of our algorithms.The underlying Deterministrization of our algorithms is not in a way to achieve the underlying, underlying Deterministrization of our algorithms.The underlying, underlying, the underlying Deterministrization of our algorithms is not in a way to achieve the underlying, underlying, the underlying Deterministrization of our algorithms.The underlying, the underlying, the underlying, the underlying Deterministrimentalization of our algorithms is in a way, underlying, the underlying, the underlying Algorithms is in a way, in a way to achieve the basis, in an algorithms."}, {"heading": "3.5 Impossibility of Sublinear Regret for NDCG Calibrated Surrogates", "text": "This means that a function with a low expected loss of invisible data should expect a low target loss of invisible data. We focus on NDCG calibrated surrogates (both convex and non-convex), those of Ravikumar et al. (2011). We first specify the necessary and sufficient condition for a surrogate of invisible data. We focus on NDCG calibrated surrogates (both convex and non-convex), which are characterized by Ravikumar et al. (2011). We first specify the necessary and sufficient condition for a surrogate of calibrated data. We focus on NDCG calibrated surrogates (both convex and non-convex), characterized by Ravikumar et al."}, {"heading": "4. Experiments", "text": "We conducted experiments with simulated and commercial datasets to demonstrate the performance of our algorithms."}, {"heading": "4.1 Non Contextual Setting", "text": "We had the following objectives while we carried out experiments in the algorithm noncontextual, on-line ranking practice algorithm: \"This is a way like the performance of algorithm 1 of the size of algorithm 1 of the size of algorithm 2 of the size of algorithm 2 of the size of algorithm 2 of the size of algorithm 3 of the size of algorithm 3 of the size of algorithm 3 of the size of algorithm 3 of the size of algorithm 4 of the size of algorithm 4 of algorithm 5 of the size of algorithm 5 of the size of algorithm 4 of algorithm 6 of the size of algorithm 6 of the algorithm 6 of the size of algorithm 6 of the algorithm 6 of the size of algorithm 6 of the size of algorithm 6 of the size of algorithm 6 of the size of algorithm 7 of 7 of 7 of 7 of 7 of 7 of 7 of the algorithm 7 of 7 of 7 of 7 of the 7 of the algorithm 7 of the 7 of the 7 of the algorithm 7 of the 7 of the algorithm 7 of the algorithm 7 of the size of the algorithm 7 of the size of algorithm 6 of the algorithm 6 of the size of algorithm 6 of the size of algorithm 6 of the algorithm 6 of the size of algorithm 6 of the algorithm 6 of the size of the algorithm 3 of the size of the algorithm 3 of the size of algorithm 3 of the algorithm 3 of the size of algorithm 3 of the size of algorithm 3 of the algorithm 3 of the size of the algorithm 3 of the size of algorithm 3 of the algorithm 3 of the size of the algorithm 3 of the size of algorithm 3 of the algorithm 3 of the size of algorithm 3 of the algorithm 3 of the size of the size of algorithm 3 of the algorithm 3 of the algorithm 3 of the size of the algorithm 3 of the size of the algorithm 3 of the size of the algorithm 3 of the algorithm 3 of the size of the size of the algorithm"}, {"heading": "4.2 Contextual Setting", "text": "The goal was to show that it is possible to learn a good ranking function, even with severely limited feedback, if users only care about the ranking that is presented to them, and in fact the algorithm interacts with users by presenting rankings and receiving feedback on the top positions (s). We tested the quality of the ranking lists, and therefore the performance of the ranking functions is only of interest. Users are only interested in the ranking that is presented to them, and in fact the algorithm interacts with users by presenting rankings and receiving feedback on the top positions (s). We tested the quality of the ranking lists, and thus the performance of the ranking functions that are directed against the full relevance of the vectors, via ranking measurement NDCG, at the 10th point. NDCG, cutoff at one point n, is defined as follows: NDCGn (R) = 1Zn (R)."}, {"heading": "5. Conclusion and Future Directions", "text": "We have the problem of online surveys of people who are able to recognise that they are able to acquire their identity."}, {"heading": "Acknowledgments", "text": "The authors acknowledge the NSF's support under the funding programmes IIS-1319810 and CAREER IIS1452099."}, {"heading": "Appendix A.", "text": "We provide technical details of the results of the online rankings with restricted feedback Non Contextual Setting.Proof of Theorem 4: Proof. We will explicitly show that local observability condition fails by looking at the case when the number of objects m = 3. It is obvious that the set of probabilities for which E [R (1)] [R (2)] [R (3)] any other action {S) is not a subset of any C3, C4, C6} blocks either an object 2 at the top or an object 3 at the top. It is obvious that the set of probabilities for which E [R (1)] [R (2)] cannot be a subset of actions of C3, C4, C6}. From Def. 4, the neighborhood action set of actions {S 1, 2} is precisely defined and contains no further actions."}, {"heading": "Appendix B.", "text": "We provide technical details of the results of the online ranking list: F () x () x () i.K = i.K = i.K = i.K = i.K = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D (i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D =: =.D = i.D = i.D =: =.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = = = i.D = i.D = = i.D = i.D = i.D = = = = i.D = = i.D = = = i.D: i.D = = i.D = = i.D = = i.D = i.D = i.D = = = i.D = = i.D = = i.D = = i.D = i.D = = i.D = = i.D = = = i.D = i.D = i.D = = i.D = i.D = i.D = = i.D = i.D = = = i.D = i.D = = = i.D = i.D = i.D = = = i.D = i.D = i.D = = = i.D = i.D = i.D = = = i.D = i.D = i.D = i.D = = = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D = i.D"}], "references": [{"title": "Diversifying search results", "author": ["Rakesh Agrawal", "Sreenivas Gollapudi", "Alan Halverson", "Samuel Ieong"], "venue": "In Proceedings of the Second ACM International Conference on Web Search and Data Mining,", "citeRegEx": "Agrawal et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2009}, {"title": "Improved bounds for online learning over the permutahedron and other ranking polytopes", "author": ["Nir Ailon"], "venue": "In Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ailon.,? \\Q2014\\E", "shortCiteRegEx": "Ailon.", "year": 2014}, {"title": "Modern information retrieval, volume 463", "author": ["Ricardo Baeza-Yates", "Berthier Ribeiro-Neto"], "venue": "ACM Press,", "citeRegEx": "Baeza.Yates and Ribeiro.Neto.,? \\Q1999\\E", "shortCiteRegEx": "Baeza.Yates and Ribeiro.Neto.", "year": 1999}, {"title": "Convexity, classification, and risk bounds", "author": ["Peter L Bartlett", "Michael I Jordan", "Jon D McAuliffe"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "Bartlett et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 2006}, {"title": "Partial monitoring with side information", "author": ["G\u00e1bor Bart\u00f3k", "Csaba Szepesv\u00e1ri"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Bart\u00f3k and Szepesv\u00e1ri.,? \\Q2012\\E", "shortCiteRegEx": "Bart\u00f3k and Szepesv\u00e1ri.", "year": 2012}, {"title": "Partial monitoringclassification, regret bounds, and algorithms", "author": ["Gabor Bartok", "Dean P. Foster", "David Pal", "Alexander Rakhlin", "Csaba Szepesvari"], "venue": "Mathematics of Operations Research,", "citeRegEx": "Bartok et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bartok et al\\.", "year": 2014}, {"title": "Approximating matrix p-norms", "author": ["Aditya Bhaskara", "Aravindan Vijayaraghavan"], "venue": "In Proceedings of the twenty-second annual ACM-SIAM symposium on Discrete Algorithms,", "citeRegEx": "Bhaskara and Vijayaraghavan.,? \\Q2011\\E", "shortCiteRegEx": "Bhaskara and Vijayaraghavan.", "year": 2011}, {"title": "Learning, regret minimization, and equilibria", "author": ["A. Blum", "Y. Mansour"], "venue": null, "citeRegEx": "Blum and Mansour.,? \\Q2007\\E", "shortCiteRegEx": "Blum and Mansour.", "year": 2007}, {"title": "Multi-scale exploration of convex functions and bandit convex optimization", "author": ["S\u00e9bastien Bubeck", "Ronen Eldan"], "venue": "arXiv preprint arXiv:1507.06580,", "citeRegEx": "Bubeck and Eldan.,? \\Q2015\\E", "shortCiteRegEx": "Bubeck and Eldan.", "year": 2015}, {"title": "On the (non-) existence of convex, calibrated surrogate losses for ranking", "author": ["Cl\u00e9ment Calauzenes", "Nicolas Usunier", "Patrick Gallinari"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Calauzenes et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Calauzenes et al\\.", "year": 2012}, {"title": "Learning to rank: from pairwise approach to listwise approach", "author": ["Zhe Cao", "Tao Qin", "Tie-Yan Liu", "Ming-Feng Tsai", "Hang Li"], "venue": "In Proceedings of the 24th International conference on Machine learning,", "citeRegEx": "Cao et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2007}, {"title": "Prediction, learning, and games", "author": ["Nicolo Cesa-Bianchi"], "venue": null, "citeRegEx": "Cesa.Bianchi.,? \\Q2006\\E", "shortCiteRegEx": "Cesa.Bianchi.", "year": 2006}, {"title": "Yahoo! learning to rank challenge overview", "author": ["Olivier Chapelle", "Yi Chang"], "venue": "Journal of Machine Learning Research-Proceedings Track,", "citeRegEx": "Chapelle and Chang.,? \\Q2011\\E", "shortCiteRegEx": "Chapelle and Chang.", "year": 2011}, {"title": "Gradient descent optimization of smoothed information retrieval metrics", "author": ["Olivier Chapelle", "Mingrui Wu"], "venue": "Information retrieval,", "citeRegEx": "Chapelle and Wu.,? \\Q2010\\E", "shortCiteRegEx": "Chapelle and Wu.", "year": 2010}, {"title": "Large margin optimization of ranking measures", "author": ["Olivier Chapelle", "Quoc Le", "Alex Smola"], "venue": "In NIPS Workshop: Machine Learning for Web Search,", "citeRegEx": "Chapelle et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Chapelle et al\\.", "year": 2007}, {"title": "Auc optimization vs. error rate minimization", "author": ["Corinna Cortes", "Mehryar Mohri"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Cortes and Mohri.,? \\Q2004\\E", "shortCiteRegEx": "Cortes and Mohri.", "year": 2004}, {"title": "Subset ranking using regression", "author": ["David Cossock", "Tong Zhang"], "venue": "In Conference on Learning theory,", "citeRegEx": "Cossock and Zhang.,? \\Q2006\\E", "shortCiteRegEx": "Cossock and Zhang.", "year": 2006}, {"title": "Statistical analysis of bayes optimal subset ranking", "author": ["David Cossock", "Tong Zhang"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Cossock and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Cossock and Zhang.", "year": 2008}, {"title": "On the consistency of ranking algorithms", "author": ["John C. Duchi", "Lester W. Mackey", "Michael I. Jordan"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Online convex optimization in the bandit setting", "author": ["Abraham D Flaxman", "Adam Tauman Kalai", "H Brendan McMahan"], "venue": "In Proceedings of the sixteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "Flaxman et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Flaxman et al\\.", "year": 2005}, {"title": "No internal regret via neighborhood watch", "author": ["Dean P. Foster", "Alexander Rakhlin"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Foster and Rakhlin.,? \\Q2012\\E", "shortCiteRegEx": "Foster and Rakhlin.", "year": 2012}, {"title": "On multilabel classification and ranking with bandit feedback", "author": ["Claudio Gentile", "Francesco Orabona"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gentile and Orabona.,? \\Q2014\\E", "shortCiteRegEx": "Gentile and Orabona.", "year": 2014}, {"title": "Balancing exploration and exploitation in listwise and pairwise online learning to rank", "author": ["Katja Hofmann", "Shimon Whiteson", "Maarten de Rijke"], "venue": "Information Retrieval,", "citeRegEx": "Hofmann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann et al\\.", "year": 2013}, {"title": "IR evaluation methods for retrieving highly relevant documents", "author": ["Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "In Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.,? \\Q2000\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.", "year": 2000}, {"title": "Cumulated gain-based evaluation of IR techniques", "author": ["Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems,", "citeRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin and Kek\u00e4l\u00e4inen.", "year": 2002}, {"title": "Optimizing search engines using clickthrough data", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the 8th ACM SIGKDD,", "citeRegEx": "Joachims.,? \\Q2002\\E", "shortCiteRegEx": "Joachims.", "year": 2002}, {"title": "Efficient algorithms for online decision problems", "author": ["Adam Kalai", "Santosh Vempala"], "venue": "Journal of Computer and System Sciences,", "citeRegEx": "Kalai and Vempala.,? \\Q2005\\E", "shortCiteRegEx": "Kalai and Vempala.", "year": 2005}, {"title": "The value of knowing a demand curve: Bounds on regret for online posted-price auctions", "author": ["Robert Kleinberg", "Tom Leighton"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Kleinberg and Leighton.,? \\Q2003\\E", "shortCiteRegEx": "Kleinberg and Leighton.", "year": 2003}, {"title": "The epoch-greedy algorithm for multi-armed bandits with side information", "author": ["John Langford", "Tong Zhang"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Langford and Zhang.,? \\Q2008\\E", "shortCiteRegEx": "Langford and Zhang.", "year": 2008}, {"title": "Combinatorial partial monitoring game with linear feedback and its applications", "author": ["Tian Lin", "Bruno Abrahao", "Robert Kleinberg", "John Lui"], "venue": "In Proceedings of the 31th International Conference on Machine Learning,", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Learning to rank for information retrieval", "author": ["Tie-Yan Liu"], "venue": "Springer Science & Business Media,", "citeRegEx": "Liu.,? \\Q2011\\E", "shortCiteRegEx": "Liu.", "year": 2011}, {"title": "Letor: Benchmark dataset for research on learning to rank for information retrieval", "author": ["Tie-Yan Liu", "Jun Xu", "Tao Qin", "Wenying Xiong", "Hang Li"], "venue": "In Proceedings of SIGIR 2007 workshop on learning to rank for information retrieval,", "citeRegEx": "Liu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2007}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Antonio Piccolboni", "Christian Schindelhauer"], "venue": "In Computational Learning Theory,", "citeRegEx": "Piccolboni and Schindelhauer.,? \\Q2001\\E", "shortCiteRegEx": "Piccolboni and Schindelhauer.", "year": 2001}, {"title": "Discrete prediction games with arbitrary feedback and loss", "author": ["Antonio Piccolboni", "Christian Schindelhauer"], "venue": "In COLT,", "citeRegEx": "Piccolboni and Schindelhauer.,? \\Q2001\\E", "shortCiteRegEx": "Piccolboni and Schindelhauer.", "year": 2001}, {"title": "Learning diverse rankings with multi-armed bandits", "author": ["Filip Radlinski", "Robert Kleinberg", "Thorsten Joachims"], "venue": "In Proceedings of the 25th International conference on Machine learning,", "citeRegEx": "Radlinski et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2008}, {"title": "Redundancy, diversity and interdependent document relevance", "author": ["Filip Radlinski", "Paul N Bennett", "Ben Carterette", "Thorsten Joachims"], "venue": "In ACM SIGIR,", "citeRegEx": "Radlinski et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Radlinski et al\\.", "year": 2009}, {"title": "On NDCG consistency of listwise ranking methods", "author": ["Pradeep Ravikumar", "Ambuj Tewari", "Eunho Yang"], "venue": "In Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Ravikumar et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ravikumar et al\\.", "year": 2011}, {"title": "Test collection based evaluation of information retrieval systems, volume 13", "author": ["Mark Sanderson"], "venue": "Now Publishers Inc,", "citeRegEx": "Sanderson.,? \\Q2010\\E", "shortCiteRegEx": "Sanderson.", "year": 2010}, {"title": "A cross-benchmark comparison of 87 learning to rank methods", "author": ["Niek Tax", "Sander Bockting", "Djoerd Hiemstra"], "venue": "Information Processing and Management,", "citeRegEx": "Tax et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tax et al\\.", "year": 2015}, {"title": "Efficient learning in large-scale combinatorial semi-bandits", "author": ["Zheng Wen", "Branislav Kveton", "Azin Ashkan"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Wen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wen et al\\.", "year": 2014}, {"title": "A support vector method for optimizing average precision", "author": ["Yisong Yue", "Thomas Finley", "Filip Radlinski", "Thorsten Joachims"], "venue": "In Proceedings of ACM SIGIR,", "citeRegEx": "Yue et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yue et al\\.", "year": 2007}, {"title": "Online convex programming and generalized infinitesimal gradient ascent", "author": ["Martin Zinkevich"], "venue": "In Proceedings of the 20th International Conference on Machine Learning", "citeRegEx": "Zinkevich.,? \\Q2003\\E", "shortCiteRegEx": "Zinkevich.", "year": 2003}], "referenceMentions": [{"referenceID": 30, "context": "Learning to rank (Liu, 2011) is a supervised machine learning problem, where the output space consists of rankings of objects.", "startOffset": 17, "endOffset": 28}, {"referenceID": 23, "context": "The accuracy of a ranked list, in comparison to the actual relevance of the documents, is measured by various ranking measures, such as Discounted Cumulative Gain (DCG) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2000), Average Precision (AP) (Baeza-Yates and Ribeiro-Neto, 1999) and others.", "startOffset": 169, "endOffset": 200}, {"referenceID": 2, "context": "The accuracy of a ranked list, in comparison to the actual relevance of the documents, is measured by various ranking measures, such as Discounted Cumulative Gain (DCG) (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2000), Average Precision (AP) (Baeza-Yates and Ribeiro-Neto, 1999) and others.", "startOffset": 225, "endOffset": 261}, {"referenceID": 37, "context": "In certain applications, such as deploying a new web app or developing a custom search engine, collecting large amount of high quality labeled data might be infeasible (Sanderson, 2010).", "startOffset": 168, "endOffset": 185}, {"referenceID": 22, "context": "One type of online ranking models learn from implicit feedback inferred from user clicks on ranked lists (Hofmann et al., 2013).", "startOffset": 105, "endOffset": 127}, {"referenceID": 25, "context": "Moreover, a clicked item might not actually be relevant to the user and there is also the problem of bias towards top ranked items in inferring feedback from user clicks (Joachims, 2002).", "startOffset": 170, "endOffset": 186}, {"referenceID": 0, "context": "Non-contextual setting: Existing work loosely related to ranking of a fixed set of items to satisfy diverse user preferences (Radlinski et al., 2008, 2009; Agrawal et al., 2009; Wen et al., 2014) has focused on learning an optimal ranking of a subset of items, to be presented to an user, with performance judged by a simple 0-1 loss.", "startOffset": 125, "endOffset": 195}, {"referenceID": 39, "context": "Non-contextual setting: Existing work loosely related to ranking of a fixed set of items to satisfy diverse user preferences (Radlinski et al., 2008, 2009; Agrawal et al., 2009; Wen et al., 2014) has focused on learning an optimal ranking of a subset of items, to be presented to an user, with performance judged by a simple 0-1 loss.", "startOffset": 125, "endOffset": 195}, {"referenceID": 11, "context": "The appropriate framework to study the problem is that of partial monitoring (Cesa-Bianchi, 2006).", "startOffset": 77, "endOffset": 97}, {"referenceID": 29, "context": "A very recent paper shows another practical application of partial monitoring in the stochastic setting (Lin et al., 2014).", "startOffset": 104, "endOffset": 122}, {"referenceID": 5, "context": "Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability (Bartok et al., 2014; Foster and Rakhlin, 2012), where observability is of two kinds: local and global.", "startOffset": 208, "endOffset": 255}, {"referenceID": 20, "context": "Recent advances in the classification of partial monitoring games tell us that the minimax regret, in an adversarial setting, is governed by a property of the loss and feedback functions called observability (Bartok et al., 2014; Foster and Rakhlin, 2012), where observability is of two kinds: local and global.", "startOffset": 208, "endOffset": 255}, {"referenceID": 18, "context": "We prove that, for some ranking measures, namely PairwiseLoss (Duchi et al., 2010), DCG and Precision@n (Liu et al.", "startOffset": 62, "endOffset": 82}, {"referenceID": 31, "context": ", 2010), DCG and Precision@n (Liu et al., 2007), global observability holds.", "startOffset": 29, "endOffset": 47}, {"referenceID": 15, "context": "For example, the normalized versions of PairwiseLoss, DCG and Precision@n are called AUC (Cortes and Mohri, 2004), NDCG (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) and AP respectively.", "startOffset": 89, "endOffset": 113}, {"referenceID": 24, "context": "For example, the normalized versions of PairwiseLoss, DCG and Precision@n are called AUC (Cortes and Mohri, 2004), NDCG (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) and AP respectively.", "startOffset": 120, "endOffset": 151}, {"referenceID": 10, "context": ", the cross entropy surrogate in ListNet (Cao et al., 2007) and hinge surrogate in RankSVM (Joachims, 2002), instead of discontinuous ranking measures like DCG, or AP, because the latter lead to intractable optimization problems in the query-documents setting.", "startOffset": 41, "endOffset": 59}, {"referenceID": 25, "context": ", 2007) and hinge surrogate in RankSVM (Joachims, 2002), instead of discontinuous ranking measures like DCG, or AP, because the latter lead to intractable optimization problems in the query-documents setting.", "startOffset": 39, "endOffset": 55}, {"referenceID": 17, "context": "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method (Cossock and Zhang, 2008), hinge loss used in the pairwise RankSVM (Joachims, 2002) method, and (modified) cross-entropy surrogate used in the listwise ListNet (Cao et al.", "startOffset": 120, "endOffset": 145}, {"referenceID": 25, "context": "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method (Cossock and Zhang, 2008), hinge loss used in the pairwise RankSVM (Joachims, 2002) method, and (modified) cross-entropy surrogate used in the listwise ListNet (Cao et al.", "startOffset": 187, "endOffset": 203}, {"referenceID": 10, "context": "The convex surrogates considered are from three major learning to ranking methods: squared loss from a pointwise method (Cossock and Zhang, 2008), hinge loss used in the pairwise RankSVM (Joachims, 2002) method, and (modified) cross-entropy surrogate used in the listwise ListNet (Cao et al., 2007) method.", "startOffset": 280, "endOffset": 298}, {"referenceID": 14, "context": "For example, the normalized versions of PairwiseLoss, DCG and Precision@n are called AUC (Cortes and Mohri, 2004), NDCG (J\u00e4rvelin and Kek\u00e4l\u00e4inen, 2002) and AP respectively. We show an unexpected result for the normalized versions: they do not admit sub-linear regret algorithms with top 1 feedback. This is despite the fact that the opposite is true for their unnormalized counterparts! Intuitively, the normalization makes it hard to construct an unbiased estimators of the (unobserved) relevance vectors. Surprisingly, we are able to translate this intuitive hurdle into a provable impossibility. We also present some preliminary experiments on simulated datasets to explore the performance of our efficient algorithm and compare its regret to its full information counterpart. Contextual Setting: The requirement of having a fixed set of items to rank, in the first part of our work, somewhat limits practical applicability. In fact, in the classic multiarmed bandit problem, while non-contextual bandits have received a lot of attention, the authors Langford and Zhang (2008) mention that \u201csettings with no context information are rare in practice\u201d.", "startOffset": 90, "endOffset": 1080}, {"referenceID": 13, "context": "SmoothDCG surrogate (Chapelle and Wu, 2010).", "startOffset": 20, "endOffset": 43}, {"referenceID": 36, "context": "The convex surrogates we mentioned above are widely used but are known to fail to be calibrated with respect to NDCG (Ravikumar et al., 2011).", "startOffset": 117, "endOffset": 141}, {"referenceID": 9, "context": "We only focus on NDCG calibrated surrogates for the impossibility results since no (convex) surrogate can be calibrated for AP and ERR (Calauzenes et al., 2012).", "startOffset": 135, "endOffset": 160}, {"referenceID": 27, "context": "Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bart\u00f3k and Szepesv\u00e1ri, 2012; Gentile and Orabona, 2014).", "startOffset": 135, "endOffset": 185}, {"referenceID": 11, "context": "Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bart\u00f3k and Szepesv\u00e1ri, 2012; Gentile and Orabona, 2014).", "startOffset": 135, "endOffset": 185}, {"referenceID": 4, "context": "Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bart\u00f3k and Szepesv\u00e1ri, 2012; Gentile and Orabona, 2014).", "startOffset": 202, "endOffset": 258}, {"referenceID": 21, "context": "Note, however, that there does exist work on partial monitoring problems with continuous learner actions, but without side information (Kleinberg and Leighton, 2003; Cesa-Bianchi, 2006), and vice versa (Bart\u00f3k and Szepesv\u00e1ri, 2012; Gentile and Orabona, 2014).", "startOffset": 202, "endOffset": 258}, {"referenceID": 1, "context": "It has been shown by Ailon (2014) that regret under the two measures are equal:", "startOffset": 21, "endOffset": 34}, {"referenceID": 5, "context": "Our main results on regret bounds build on some of the theory for abstract partial monitoring games developed by Bartok et al. (2014) and Foster and Rakhlin (2012).", "startOffset": 113, "endOffset": 134}, {"referenceID": 5, "context": "Our main results on regret bounds build on some of the theory for abstract partial monitoring games developed by Bartok et al. (2014) and Foster and Rakhlin (2012). For ease of understanding, we reproduce the relevant notations", "startOffset": 113, "endOffset": 164}, {"referenceID": 5, "context": "The following definitions, given for abstract problems by Bartok et al. (2014), has been refined to fit our problem context.", "startOffset": 58, "endOffset": 79}, {"referenceID": 5, "context": "First, we get a lower bound by combining our Theorem 4 with Theorem 4 of Bartok et al. (2014).", "startOffset": 73, "endOffset": 94}, {"referenceID": 11, "context": "1 in Cesa-Bianchi (2006), gives an algorithm (inspired by the algorithm originally given in Piccolboni and Schindelhauer (2001a)) obtaining O(T 2/3) regret.", "startOffset": 5, "endOffset": 25}, {"referenceID": 11, "context": "1 in Cesa-Bianchi (2006), gives an algorithm (inspired by the algorithm originally given in Piccolboni and Schindelhauer (2001a)) obtaining O(T 2/3) regret.", "startOffset": 5, "endOffset": 129}, {"referenceID": 11, "context": "1 in Cesa-Bianchi (2006), gives an algorithm (inspired by the algorithm originally given in Piccolboni and Schindelhauer (2001a)) obtaining O(T 2/3) regret. Corollary 6. The algorithm in Figure 1 of Cesa-Bianchi (2006) achieves O(T 2/3) regret bound for SumLoss.", "startOffset": 5, "endOffset": 219}, {"referenceID": 11, "context": "However, the algorithm in Cesa-Bianchi (2006) is intractable in our setting since the algorithm necessarily enumerates all the actions of the learner in each round, which is exponential in m in our case (m! to be exact).", "startOffset": 26, "endOffset": 46}, {"referenceID": 5, "context": "Theorem 4 of Bartok et al. (2014) says the following: A partial monitoring game which is both globally and locally observable has minimax regret \u0398(T 1/2), while a game which is globally observable but not locally observable has minimax regret \u0398(T 2/3).", "startOffset": 13, "endOffset": 34}, {"referenceID": 7, "context": "Our algorithm is motivated by the reduction from bandit-feedback to full feedback scheme given in Blum and Mansour (2007). However, the reduction cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action.", "startOffset": 98, "endOffset": 122}, {"referenceID": 7, "context": "Our algorithm is motivated by the reduction from bandit-feedback to full feedback scheme given in Blum and Mansour (2007). However, the reduction cannot be directly applied to our problem, because we are not in the bandit setting and hence do not know loss of any action. Further, the algorithm of Blum and Mansour (2007) necessarily spends N rounds per block to try out each of the N available actions \u2014 this is impractical in our setting since N = m!.", "startOffset": 98, "endOffset": 322}, {"referenceID": 26, "context": "2 Computationally Efficient Algorithm with FTPL We instantiate our general algorithm with Follow The Perturbed Leader (FTPL) full information algorithm (Kalai and Vempala, 2005).", "startOffset": 152, "endOffset": 177}, {"referenceID": 5, "context": "Combining the above lemma with Theorem 2 of Bartok et al. (2014), we conclude that there cannot exist any algorithm which has sub-linear regret for any of the following measures: NDCG, AP or AUC, when restricted to top 1 feedback.", "startOffset": 44, "endOffset": 65}, {"referenceID": 16, "context": "1 Convex Surrogates Pointwise Method: We will construct the unbiased estimator of the gradient of squared loss (Cossock and Zhang, 2006): \u03c6sq(s,R) = \u2016s \u2212 R\u20162.", "startOffset": 111, "endOffset": 136}, {"referenceID": 25, "context": "Pairwise Method: We will construct the unbiased estimator of the gradient of hingelike surrogate in RankSVM (Joachims, 2002): \u03c6svm(s,R) = \u2211 i 6=j=1 1(R(i) > R(j)) max(0, 1+ s(j)\u2212 s(i)).", "startOffset": 108, "endOffset": 124}, {"referenceID": 10, "context": "We will focus on the cross-entropy surrogate used in the highly cited ListNet (Cao et al., 2007) ranking algorithm and show how a very natural modification to the surrogate makes its gradient estimable in our partial feedback setting.", "startOffset": 78, "endOffset": 96}, {"referenceID": 14, "context": "For example, the class of popular listwise surrogates that are developed from structured prediction perspective (Chapelle et al., 2007; Yue et al., 2007) cannot have unbiased estimator of gradients from top-k feedback since they are based on maps from full relevance vectors to full rankings and thus cannot be decomposed over k = 1 or 2 coordinates of R.", "startOffset": 112, "endOffset": 153}, {"referenceID": 40, "context": "For example, the class of popular listwise surrogates that are developed from structured prediction perspective (Chapelle et al., 2007; Yue et al., 2007) cannot have unbiased estimator of gradients from top-k feedback since they are based on maps from full relevance vectors to full rankings and thus cannot be decomposed over k = 1 or 2 coordinates of R.", "startOffset": 112, "endOffset": 153}, {"referenceID": 13, "context": "We choose the SmoothDCG surrogate given in (Chapelle and Wu, 2010), which has been shown to have very competitive empirical performance.", "startOffset": 43, "endOffset": 66}, {"referenceID": 13, "context": "SmoothDCG, like ListNet, defines a family of surrogates, based on the cut-off point of DCG (see original paper (Chapelle and Wu, 2010) for details).", "startOffset": 111, "endOffset": 134}, {"referenceID": 41, "context": "4 Regret Bounds The underlying deterministic part of our algorithm is online gradient descent (OGD) (Zinkevich, 2003).", "startOffset": 100, "endOffset": 117}, {"referenceID": 19, "context": "1 of (Flaxman et al., 2005), in our problem setting is:", "startOffset": 5, "endOffset": 27}, {"referenceID": 6, "context": "To get bound on Et\u2016z\u0303t\u20162, we used the following norm relation that holds for any matrix X (Bhaskara and Vijayaraghavan, 2011): \u2016X\u2016p\u2192q = sup v 6=0 \u2016Xv\u2016q \u2016v\u2016p , where q is the dual exponent of p (i.", "startOffset": 90, "endOffset": 125}, {"referenceID": 8, "context": "For bandit online convex optimization problems with Lipschitz, convex surrogates, the best regret rate known so far, that can be achieved by an efficient algorithm, is O(T 3/4) (however, see the work of Bubeck and Eldan (2015) for a non-constructive O(log(T ) \u221a T ) bound).", "startOffset": 203, "endOffset": 227}, {"referenceID": 3, "context": "t the target (Bartlett et al., 2006).", "startOffset": 13, "endOffset": 36}, {"referenceID": 3, "context": "t the target (Bartlett et al., 2006). Intuitively, it means that a function with small expected surrogate loss on unseen data should have small expect target loss on unseen data. We focus on NDCG calibrated surrogates (both convex and non-convex) that have been characterized by Ravikumar et al. (2011). We first state the necessary and sufficient condition for a surrogate to be calibrated w.", "startOffset": 14, "endOffset": 303}, {"referenceID": 36, "context": "15 states that argsort(s\u03c6(\u03b7)) \u2286 argsort(ER\u223c\u03b7 [ G(R) Z(R) ] ) Ravikumar et al. (2011) give concrete examples of NDCG calibrated surrogates, including how some of the popular surrogates can be converted into NDCG calibrated ones: e.", "startOffset": 61, "endOffset": 85}, {"referenceID": 32, "context": "3 of Piccolboni and Schindelhauer (2001b) cannot be directly extended to prove the impossibility result because it relies on constructing a connected graph on vertices defined by neighboring actions of learner.", "startOffset": 5, "endOffset": 42}, {"referenceID": 38, "context": "ListNet is not only one of the most cited ranking algorithms (over 700 citations according to Google Scholar), but also one of the most validated algorithms (Tax et al., 2015).", "startOffset": 157, "endOffset": 175}, {"referenceID": 12, "context": "They were Yahoo\u2019s Learning to Rank Challenge dataset (Chapelle and Chang, 2011) and a dataset published by Russian search engine Yandex (IM-2009).", "startOffset": 53, "endOffset": 79}], "year": 2016, "abstractText": "We consider two settings of online learning to rank where feedback is restricted to top ranked items. The problem is cast as an online game between a learner and sequence of users, over T rounds. In both settings, the learners objective is to present ranked list of items to the users. The learner\u2019s performance is judged on the entire ranked list and true relevances of the items. However, the learner receives highly restricted feedback at end of each round, in form of relevances of only the top k ranked items, where k m. The first setting is non-contextual, where the list of items to be ranked is fixed. The second setting is contextual, where lists of items vary, in form of traditional query-document lists. No stochastic assumption is made on the generation process of relevances of items and contexts. We provide efficient ranking strategies for both the settings. The strategies achieve O(T ) regret, where regret is based on popular ranking measures in first setting and ranking surrogates in second setting. We also provide impossibility results for certain ranking measures and a certain class of surrogates, when feedback is restricted to the top ranked item, i.e. k = 1. We empirically demonstrate the performance of our algorithms on simulated and real world datasets.", "creator": "LaTeX with hyperref package"}}}