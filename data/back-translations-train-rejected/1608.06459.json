{"id": "1608.06459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Tracking Amendments to Legislation and Other Political Texts with a Novel Minimum-Edit-Distance Algorithm: DocuToads", "abstract": "Political scientists often find themselves tracking amendments to political texts. As different actors weigh in, texts change as they are drafted and redrafted, reflecting political preferences and power. This study provides a novel solution to the prob- lem of detecting amendments to political text based upon minimum edit distances. We demonstrate the usefulness of two language-insensitive, transparent, and efficient minimum-edit-distance algorithms suited for the task. These algorithms are capable of providing an account of the types (insertions, deletions, substitutions, and trans- positions) and substantive amount of amendments made between version of texts. To illustrate the usefulness and efficiency of the approach we replicate two existing stud- ies from the field of legislative studies. Our results demonstrate that minimum edit distance methods can produce superior measures of text amendments to hand-coded efforts in a fraction of the time and resource costs.", "histories": [["v1", "Tue, 23 Aug 2016 10:55:23 GMT  (154kb,D)", "http://arxiv.org/abs/1608.06459v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.CY", "authors": ["henrik hermansson", "james p cross"], "accepted": false, "id": "1608.06459"}, "pdf": {"name": "1608.06459.pdf", "metadata": {"source": "CRF", "title": "Tracking Amendments to Legislation and Other Political Texts with a Novel Minimum-Edit-Distance Algorithm: DocuToads", "authors": ["Henrik Hermansson", "James P. Cross"], "emails": ["(hajh@ifs.ku.dk).", "(james.cross@ucd.ie)."], "sections": [{"heading": null, "text": "Political scientists often follow changes in policy texts, and the more different actors get involved, the more texts change when they are designed and redesigned, reflecting political preferences and power. This study offers a novel solution to the problem of detecting changes in policy texts on the basis of minimal editing distances. We demonstrate the usefulness of two language-insensitive, transparent, and efficient algorithms with minimal editing distances that are suitable for this task. These algorithms are able to provide a representation of the types (insertions, deletions, substitutions, and transpositions) and the substantial amount of changes between text versions. To illustrate the usefulness and efficiency of the approach, we replicate two existing studies in the field of legislative science. Our results show that methods with minimal editing distances can provide superior yardsticks for text changes to achieve hand-coded efforts in a fraction of time and resource costs."}, {"heading": "Introduction", "text": "In fact, most of them will be able to play by the rules they have set for their policies."}, {"heading": "Existing literature", "text": "Measuring the evolution of policy documents has so far proved difficult in terms of the replicability and validity of the measures used to record such changes, as well as the time and resources required to compile them. Ideally, political scientists want accurate metrics to capture the nature and extent of the changes, so that the success of individual actors in influencing outcomes can be studied. As a result, enormous efforts have been made in a number of sub-areas of political science to create such measures, the institutional context in which such positions are taken, and the end result achieved, resulting in a comprehensive account of the decision-making process. As a result, huge efforts have been made in a number of sub-areas of political science to create such measures. This literature differs significantly in the methodologies applied, with methods such as accurate manual reading of relevant documents and extensive quantitative analysis of policy texts and speeches. Here, we give a brief overview of examples from this literature that are organized by the methodologies applied to capture policy texts."}, {"heading": "Manual coding", "text": "To date, perhaps the most fruitful and convincing attempts to capture the changes introduced by the actors, and their success in incorporating these changes into policy texts involved6The oft-quoted phrase \"garbage in, garbage out\" is particularly relevant here; the most important hand-picked measures in multi-party government political systems (Martin and Vanberg, 2005, 2004). The authors argue that \"given the technical nature of most modern legislation, the political significance of changes needs to be improved by classifying the essential content and language of such changes (Martin and Vanberg, 2004)."}, {"heading": "Quantitative text analysis", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move into another world, in which they are able to move into another world, in which they are able to move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they are able to live, in which they, in which they, in which they, in which they, in which they, in which they are"}, {"heading": "The Conciliation Committee of the EU", "text": "This year, it is at an all-time high in the history of the European Union."}, {"heading": "Parliamentary amendments in Germany", "text": "In this context, it should be noted that the persons in question are people who are not able to take care of people who are able, who are able, who are able to move, and who are able to move."}, {"heading": "Conclusions", "text": "In this study, we demonstrated the usefulness of minimum algorithms for quantifying differences between versions of policy texts. We show that these algorithms are particularly suitable for detecting changes or changes in legislation. Furthermore, we introduced a new minimum algorithm that can handle text transfers, i.e., the substantial change in algorithms could be noted. In the empirical section of the study, we identified the results of the DocuToads algorithms with the substantially existing amendments and coded them manually."}], "references": [{"title": "Dynamic Programming", "author": ["R. Bellman"], "venue": null, "citeRegEx": "Bellman,? \\Q1957\\E", "shortCiteRegEx": "Bellman", "year": 1957}, {"title": "council of ministers of the european union", "author": ["J.P. Cross", "J. B\u00f8lstad"], "venue": "Journal of European Public Policy,", "citeRegEx": "Cross and B\u00f8lstad,? \\Q2015\\E", "shortCiteRegEx": "Cross and B\u00f8lstad", "year": 2015}, {"title": "rupted time series analysis", "author": ["M.O. Dayhoff", "R.M. Schwartz"], "venue": "European Union Politics,", "citeRegEx": "Dayhoff and Schwartz,? \\Q1978\\E", "shortCiteRegEx": "Dayhoff and Schwartz", "year": 1978}, {"title": "Interorganizational Negotiation and Intraorganizational", "author": ["H. Farrell", "A. H\u00e9ritier"], "venue": "at-a-distance measurements. Political Psychology,", "citeRegEx": "Farrell and H\u00e9ritier,? \\Q2004\\E", "shortCiteRegEx": "Farrell and H\u00e9ritier", "year": 2004}, {"title": "Comparative Political Studies", "author": ["Parliament", "Council"], "venue": "Construction of phylogenetic trees", "citeRegEx": "Parliament and Council.,? \\Q1967\\E", "shortCiteRegEx": "Parliament and Council.", "year": 1967}, {"title": "Explaining negotiations in the conciliation committee", "author": ["F. Franchino", "C. Mariotto"], "venue": null, "citeRegEx": "284", "shortCiteRegEx": "284", "year": 2012}, {"title": "Amino acid substitution matrices from protein", "author": ["J.G. Henikoff"], "venue": null, "citeRegEx": "Henikoff and Henikoff,? \\Q1992\\E", "shortCiteRegEx": "Henikoff and Henikoff", "year": 1992}, {"title": "Methods for identifying versioned and plagiarized documents", "author": ["T.C. Hoad", "J. Zobel"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Hoad and Zobel,? \\Q2003\\E", "shortCiteRegEx": "Hoad and Zobel", "year": 2003}, {"title": "Computational dialectology in irish gaelic", "author": ["B. Kessler"], "venue": "In Proceedings of the seventh confer-", "citeRegEx": "Kessler,? \\Q1995\\E", "shortCiteRegEx": "Kessler", "year": 1995}, {"title": "Binary codes capable of correcting deletions, insertions and reversals", "author": [], "venue": null, "citeRegEx": "Levenshtein,? \\Q1966\\E", "shortCiteRegEx": "Levenshtein", "year": 1966}, {"title": "Introduction to information retrieval", "author": ["C.D. 10:707. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Policing the bargain: Coalition government and parliamen", "author": ["L.W. Cambridge. Martin", "G. Vanberg"], "venue": null, "citeRegEx": "Martin and Vanberg,? \\Q2004\\E", "shortCiteRegEx": "Martin and Vanberg", "year": 2004}, {"title": "Coalition policymaking and legislative review", "author": ["L.W. Martin", "G. Vanberg"], "venue": "tary scrutiny. American Journal of Political Science,", "citeRegEx": "Martin and Vanberg,? \\Q2005\\E", "shortCiteRegEx": "Martin and Vanberg", "year": 2005}, {"title": "Parliaments and coalitions: The role of legislative institu", "author": ["L.W. Martin", "G. Vanberg"], "venue": "Political Science Review,", "citeRegEx": "Martin and Vanberg,? \\Q2011\\E", "shortCiteRegEx": "Martin and Vanberg", "year": 2011}, {"title": "Measuring dialect distance phonetically", "author": ["J. Press. Nerbonne", "W. Heeringa"], "venue": "Proksch, S.-O. and Slapin, J. B", "citeRegEx": "Nerbonne and Heeringa,? \\Q1997\\E", "shortCiteRegEx": "Nerbonne and Heeringa", "year": 1997}, {"title": "Party system dynamics in post-war Japan", "author": ["Proksch", "S.-O", "J.B. Slapin", "M.F. Thies"], "venue": "Journal of Political Science,", "citeRegEx": "Proksch et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Proksch et al\\.", "year": 2011}, {"title": "A quantitative content analysis of electoral pledges", "author": ["K. Ritter", "M.J. Medhurst"], "venue": "Electoral Studies,", "citeRegEx": "Ritter and Medhurst,? \\Q2004\\E", "shortCiteRegEx": "Ritter and Medhurst", "year": 2004}, {"title": "White House ghosts: Presidents and their speechwriters", "author": ["R. Press. Schlesinger"], "venue": "Simon and Schuster. Slapin, J. B. and Proksch, S.-O", "citeRegEx": "Schlesinger,? \\Q2008\\E", "shortCiteRegEx": "Schlesinger", "year": 2008}, {"title": "Look who\u2019s talking: Parliamentary debate in the European", "author": ["J.B. Slapin", "S.O. Proksch"], "venue": null, "citeRegEx": "Slapin and Proksch,? \\Q2010\\E", "shortCiteRegEx": "Slapin and Proksch", "year": 2010}, {"title": "Computing patterns in strings", "author": ["G. Education. Tsebelis", "C.B. Jensen", "A. Kalandrakis", "A. Kreppel"], "venue": "Union. European Union Politics,", "citeRegEx": "Tsebelis et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Tsebelis et al\\.", "year": 2003}, {"title": "The string-to-string correction problem", "author": ["R.A. Wagner", "M.J. Fischer"], "venue": null, "citeRegEx": "Wagner and Fischer,? \\Q1974\\E", "shortCiteRegEx": "Wagner and Fischer", "year": 1974}, {"title": "Bounds for the string editing problem", "author": ["Wong", "C.-K", "A.K. Chandra"], "venue": null, "citeRegEx": "Wong et al\\.,? \\Q1976\\E", "shortCiteRegEx": "Wong et al\\.", "year": 1976}], "referenceMentions": [{"referenceID": 16, "context": "Examples of this process include political speeches drafted by speechwriters and amended by the politician giving the speech, or legislative proposals drafted by committees and amended in plenary (Ritter and Medhurst, 2004; Vaughn and Villalobos, 2006; Dille, 2000; Schlesinger, 2008).", "startOffset": 196, "endOffset": 284}, {"referenceID": 17, "context": "Examples of this process include political speeches drafted by speechwriters and amended by the politician giving the speech, or legislative proposals drafted by committees and amended in plenary (Ritter and Medhurst, 2004; Vaughn and Villalobos, 2006; Dille, 2000; Schlesinger, 2008).", "startOffset": 196, "endOffset": 284}, {"referenceID": 1, "context": "Capturing amendments made to a text, especially if broken down by topic, can also be a useful indicator of extent and focus of political censorship (Cross, 2014, 2013; Cross and B\u00f8lstad, 2015).", "startOffset": 148, "endOffset": 192}, {"referenceID": 11, "context": "In a comparative politics context, assessing how actors in different legislatures seek to amend and influence political texts has provided important insights into how the process of legislative review can ameliorate agency problems in political systems with multiparty governments (Martin and Vanberg, 2005, 2011, 2004). The authors argued that \u201cgiven the technical nature of most modern legislation, grasping the policy significance of changes to a draft bill by classifying the substantive content and language of such changes requires extensive expertise in the policy areas dealt with by the bill. [...] Any measure based on our perceptions of substantive policy impact is therefore bound to be highly unreliable, especially when applied to a large number of bills across a variety of policy areas\u201d. Instead, they develop \u201ca more objective measure of change, [defined] as the number of article changes made to the draft version of a government bill\u201d (Martin and Vanberg, 2005, p.9). The data used in these studies was collected through hand coding the changes to legislative texts between the initial proposal and the final piece of legislation decided upon, providing a quantitative, objective and reliable account of how legislation evolved and was amended. In the Martin and Vanberg (2011) study, five distinct Parliaments are considered, with a total of 1,300 legislative proposals across these Parliaments examined.", "startOffset": 282, "endOffset": 1297}, {"referenceID": 19, "context": "Tsebelis et al. (2001) were the first to examine the amendment success of different EU institutions in the legislative process.", "startOffset": 0, "endOffset": 23}, {"referenceID": 19, "context": "Tsebelis et al. (2001) were the first to examine the amendment success of different EU institutions in the legislative process. The authors examined nearly 5,000 separate amendments to a selection of 231 examples of EU legislation negotiated under both co-decision (79) and co-operation (152) between 1988 and 1997. They focused on the exact amendments offered by the actors and the degree to which these are adopted or rejected in the final text. The work by Tsebelis et al. (2001) demonstrates that tracking amendments offers a way to objectively measure salient policy developments and the realisation of policy preferences.", "startOffset": 0, "endOffset": 483}, {"referenceID": 18, "context": "Building upon the efforts of Tsebelis et al. (2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU.", "startOffset": 29, "endOffset": 52}, {"referenceID": 18, "context": "Building upon the efforts of Tsebelis et al. (2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU.", "startOffset": 29, "endOffset": 83}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ.", "startOffset": 90, "endOffset": 537}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.", "startOffset": 90, "endOffset": 604}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.e. which actor\u2019s position is most reflected in the final text) similar to those of hand-coding efforts for a subset (9/20) of the most clear-cut cases, leaving much room for improvement for automated text analysis methods. One important question that arises when applying automated text analysis to political documents, is whether or not the chosen method is appropriate for the task at hand, given the datagenerating processes under consideration. The Wordfish algorithm designed by Slapin and Proksch (2008) aims to place actors responsible for particular political texts on latent ideological policy dimensions, based upon the word frequencies found in the political texts of interest.", "startOffset": 90, "endOffset": 1197}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.e. which actor\u2019s position is most reflected in the final text) similar to those of hand-coding efforts for a subset (9/20) of the most clear-cut cases, leaving much room for improvement for automated text analysis methods. One important question that arises when applying automated text analysis to political documents, is whether or not the chosen method is appropriate for the task at hand, given the datagenerating processes under consideration. The Wordfish algorithm designed by Slapin and Proksch (2008) aims to place actors responsible for particular political texts on latent ideological policy dimensions, based upon the word frequencies found in the political texts of interest. An important assumption of such an approach made clear by the authors themselves is that the language used in each document can be reduced to a word frequency distribution and that differences between these distributions represent differences between actors on the latent policy dimension being estimated. When one considers legislative texts, in which language is highly formalised, this is a rather strong assumption, which may go some way to explaining the rather disappointing success rate the algorithm had in replicating the hand-coding efforts of Franchino and Mariotto (2012). As argued in Grimmer\u2019s (2013) review of the state-of-the-art of automated text analysis, one must pay", "startOffset": 90, "endOffset": 1960}, {"referenceID": 18, "context": "(2001), Franchino and Mariotto (2012) utilised automated text analysis methods (Wordfish (Slapin and Proksch, 2008)) developed for ascertaining (ideological) policy positions from political texts, to compare the bargaining success of the Commission, Council, and Parliament in the conciliation committees of the EU. This study is especially useful for the purposes of studying the effectiveness of automated text analysis methods, as the authors make a significant effort to compare the hand-coding scheme used by Tsebelis et al. (2001) to the automated method they employ. Franchino and Mariotto (2012) demonstrate that Wordfish is capable of producing document-level binary results (i.e. which actor\u2019s position is most reflected in the final text) similar to those of hand-coding efforts for a subset (9/20) of the most clear-cut cases, leaving much room for improvement for automated text analysis methods. One important question that arises when applying automated text analysis to political documents, is whether or not the chosen method is appropriate for the task at hand, given the datagenerating processes under consideration. The Wordfish algorithm designed by Slapin and Proksch (2008) aims to place actors responsible for particular political texts on latent ideological policy dimensions, based upon the word frequencies found in the political texts of interest. An important assumption of such an approach made clear by the authors themselves is that the language used in each document can be reduced to a word frequency distribution and that differences between these distributions represent differences between actors on the latent policy dimension being estimated. When one considers legislative texts, in which language is highly formalised, this is a rather strong assumption, which may go some way to explaining the rather disappointing success rate the algorithm had in replicating the hand-coding efforts of Franchino and Mariotto (2012). As argued in Grimmer\u2019s (2013) review of the state-of-the-art of automated text analysis, one must pay", "startOffset": 90, "endOffset": 1991}, {"referenceID": 18, "context": "Slapin & Proksch, and those using their Wordfish method appropriately have been very successful in providing important insights into position taking in different parliamentary contexts (Slapin and Proksch, 2010; Proksch and Slapin, 2009; Proksch et al., 2011).", "startOffset": 185, "endOffset": 259}, {"referenceID": 15, "context": "Slapin & Proksch, and those using their Wordfish method appropriately have been very successful in providing important insights into position taking in different parliamentary contexts (Slapin and Proksch, 2010; Proksch and Slapin, 2009; Proksch et al., 2011).", "startOffset": 185, "endOffset": 259}, {"referenceID": 20, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 95, "endOffset": 159}, {"referenceID": 8, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 239, "endOffset": 283}, {"referenceID": 14, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 239, "endOffset": 283}, {"referenceID": 2, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 343, "endOffset": 428}, {"referenceID": 6, "context": "They have successfully been applied to problems as diverse as creating accurate spell checkers (Wagner, 1974; Wagner and Fischer, 1974; Wong and Chandra, 1976), assessing differences between different dialects in computational linguistics (Kessler, 1995; Nerbonne and Heeringa, 1997), and assessing genetic alignments in computational biology (Fitch and Margoliash, 1967; Dayhoff and Schwartz, 1978; Henikoff and Henikoff, 1992).", "startOffset": 343, "endOffset": 428}, {"referenceID": 13, "context": "All of that being said, when such algorithms are applied to the types of texts for which Slapin and Proksch (2008) originally intended them, the usefulness of automated text analysis become clear.", "startOffset": 89, "endOffset": 115}, {"referenceID": 7, "context": "The first solution, referred to as fingerprinting, randomly draws sample words from two texts and infers their similarity based on the similarity of the samples (Hoad and Zobel, 2003).", "startOffset": 161, "endOffset": 183}, {"referenceID": 9, "context": "The Levenshtein minimum edit distance algorithm is used to assess the differences between two strings of text units, and is calculated as the minimum number of editing operations required to change one string into another (Levenshtein, 1966).", "startOffset": 222, "endOffset": 241}, {"referenceID": 9, "context": "The Levenshtein minimum edit distance algorithm is used to assess the differences between two strings of text units, and is calculated as the minimum number of editing operations required to change one string into another (Levenshtein, 1966).8 Three distinct editing operations are allowed by the algorithm, and each has an assigned weight. The allowed editing operations are the deletion of a unit of text (weighted 1), the insertion of a unit of text (weighted 1), or the substitution of DOCUument Transpose Or Add, Delete, Substitute For those interested in a more detailed exposition of the Levenshtein distance algorithm we recommend Manning et al. (2008)", "startOffset": 4, "endOffset": 661}, {"referenceID": 0, "context": "The minimum edit distance is computed using a dynamic programming approach, which is a method for solving larger problems by considering a larger problem to be the sum of the solutions to a series of sub-problems (Bellman, 1957).", "startOffset": 213, "endOffset": 228}, {"referenceID": 9, "context": "We aim to demonstrate that automated methods can perform just as well as and sometimes better than hand-coding efforts, regardless of the chosen algorithm, and that our new DocuToads algorithm outperforms the existing Levenshtein algorithm due to its ability to detect transpositions in legislative texts. We then move on to compare the DocuToads algorithm to a sample of the hand-coding performed by Martin and Vanberg (2005), aiming to replicate their results and to compare the counting of article changes to counting edit operations.", "startOffset": 218, "endOffset": 427}, {"referenceID": 3, "context": "This decision-making mechanism has become an important institutional feature of the EU (Farrell and H\u00e9ritier, 2004).", "startOffset": 87, "endOffset": 115}, {"referenceID": 3, "context": "This decision-making mechanism has become an important institutional feature of the EU (Farrell and H\u00e9ritier, 2004). Essentially, the different versions of a piece of legislation put forward by the Council and Parliament reveal overt conflict between the central institutions and the final text drafted by the Conciliation Committee represents the resolution of that conflict. Taking the Council of Minister\u2019s version as a reference document, Franchino and Mariotto (2012) have encoded substantive differences between the three versions of twenty legislative acts that were resolved by Conciliation Committee on an article-by-article basis, using the same methodology and terminology as Tsebelis et al.", "startOffset": 88, "endOffset": 473}, {"referenceID": 3, "context": "This decision-making mechanism has become an important institutional feature of the EU (Farrell and H\u00e9ritier, 2004). Essentially, the different versions of a piece of legislation put forward by the Council and Parliament reveal overt conflict between the central institutions and the final text drafted by the Conciliation Committee represents the resolution of that conflict. Taking the Council of Minister\u2019s version as a reference document, Franchino and Mariotto (2012) have encoded substantive differences between the three versions of twenty legislative acts that were resolved by Conciliation Committee on an article-by-article basis, using the same methodology and terminology as Tsebelis et al. (2001). All-in-all, the authors categorised 525 substantive amendments in the recitals and articles of the texts and associated them with particular legislative articles.", "startOffset": 88, "endOffset": 710}, {"referenceID": 3, "context": "This decision-making mechanism has become an important institutional feature of the EU (Farrell and H\u00e9ritier, 2004). Essentially, the different versions of a piece of legislation put forward by the Council and Parliament reveal overt conflict between the central institutions and the final text drafted by the Conciliation Committee represents the resolution of that conflict. Taking the Council of Minister\u2019s version as a reference document, Franchino and Mariotto (2012) have encoded substantive differences between the three versions of twenty legislative acts that were resolved by Conciliation Committee on an article-by-article basis, using the same methodology and terminology as Tsebelis et al. (2001). All-in-all, the authors categorised 525 substantive amendments in the recitals and articles of the texts and associated them with particular legislative articles.16 In order to demonstrate that minimum edit distance measures are capable of replicating these hand-coding efforts, we show the similarity of the Levenshtein and DocuToads minimum edit distances to this hand-coded scheme in two ways. First, we take each document-level dyad (e.g. Council-Parliament, Council-final text, Parliament-final text) as the unit of analysis, resulting in 60 document combinations to analyse (20 dossiers with three document combinations in each). For each such dyad, we add up the number of amendments found by Franchino and Mariotto (2012) and compare what they find to the Levenshtein and DocuToads minimum edit distances between the documents of interest.", "startOffset": 88, "endOffset": 1441}, {"referenceID": 9, "context": "The Levenshtein algorithm, unable to correct for transpositions, commits a type-I error by recording these examples as first deleted and later re-added sections of the text, overestimating the substantive importance of these rather cosmetic changes that had been ignored by hand coders. In conclusion, DocuToads is to be preferred in cases where cut-paste operations are common or expected while the Levenshtein algorithm is slightly faster and equivalent in the absence of such operations. In cases where no transpositions but overall high numbers of edit operations are to be expected, the Levenshtein algorithm will also provide a backtrace more closely corresponding to a (hypothetical) log of actual changes performed by the writer. In conclusion to this section, we are very confident in the ability of minimum edit distance algorithms in general and DocuToads in particular to replicate and replace hand coding for the coding of substantive amendments to legislative text. Our results are highly consistent with hand coding, and much more consistent than the automated method employed by Franchino and Mariotto (2012). The differences that do exist are almost as likely to be the result of human error as of the failures of minimum edit distance algorithms.", "startOffset": 4, "endOffset": 1125}, {"referenceID": 12, "context": "While the authors did not provide us with the original texts they had worked with, we were able to identify and attain 66 out of 148 document pairs from their work on the German Bundestag (Martin and Vanberg, 2005).", "startOffset": 188, "endOffset": 214}, {"referenceID": 9, "context": "In the empirical section of the study we compared the results of the DocuToads algorithm and the previously existing Levenshtein algorithm with the substantive amendments identified and hand-coded by Franchino and Mariotto (2012), who followed the same procedure as Tsebelis et al.", "startOffset": 117, "endOffset": 230}, {"referenceID": 9, "context": "In the empirical section of the study we compared the results of the DocuToads algorithm and the previously existing Levenshtein algorithm with the substantive amendments identified and hand-coded by Franchino and Mariotto (2012), who followed the same procedure as Tsebelis et al. (2001). We found a strong correlation between both MED results and the hand-coded measure on the document as well as article level.", "startOffset": 117, "endOffset": 289}], "year": 2016, "abstractText": "Political scientists often find themselves tracking amendments to political texts. As different actors weigh in, texts change as they are drafted and redrafted, reflecting political preferences and power. This study provides a novel solution to the problem of detecting amendments to political text based upon minimum edit distances. We demonstrate the usefulness of two language-insensitive, transparent, and efficient minimum-edit-distance algorithms suited for the task. These algorithms are capable of providing an account of the types (insertions, deletions, substitutions, and transpositions) and substantive amount of amendments made between version of texts. To illustrate the usefulness and efficiency of the approach we replicate two existing studies from the field of legislative studies. Our results demonstrate that minimum edit distance methods can produce superior measures of text amendments to hand-coded efforts in a fraction of the time and resource costs. \u2217Post-Doctoral Researcher at the Centre for European Politics, Department of Political Science, University of Copenhagen (hajh@ifs.ku.dk). \u2020Lecturer in European Public Policy, School of Politics and International Relations, University College Dublin (james.cross@ucd.ie). ar X iv :1 60 8. 06 45 9v 1 [ cs .C L ] 2 3 A ug 2 01 6", "creator": "LaTeX with hyperref package"}}}