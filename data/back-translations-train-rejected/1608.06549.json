{"id": "1608.06549", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Using Semantic Similarity for Input Topic Identification in Crawling-based Web Application Testing", "abstract": "To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, in existing crawlers, rules for identifying the topics of input text fields, such as login ids, passwords, emails, dates and phone numbers, have to be manually configured. Moreover, the rules for one application are very often not suitable for another. In addition, when several rules conflict and match an input text field to more than one topics, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identify the topics of encountered input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus. In our evaluation with 100 real-world forms, the proposed approach demonstrated comparable performance to the rule-based one. Our experiments also show that the accuracy of the rule-based approach can be improved by up to 19% when integrated with our approach.", "histories": [["v1", "Tue, 23 Aug 2016 15:34:55 GMT  (1264kb)", "http://arxiv.org/abs/1608.06549v1", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.SE cs.CL", "authors": ["jun-wei lin", "farn wang"], "accepted": false, "id": "1608.06549"}, "pdf": {"name": "1608.06549.pdf", "metadata": {"source": "CRF", "title": "Using Semantic Similarity for Input Topic Identification in Crawling-based Web Application Testing", "authors": ["Jun-Wei Lin", "Farn Wang"], "emails": ["farn}@ntu.edu.tw"], "sections": [{"heading": null, "text": "In existing crawlers, however, rules for identifying the topics of input text fields, such as login IDs, passwords, emails, data and phone numbers, must be configured manually. Furthermore, the rules for an application are very often unsuitable for another application. If several rules conflict with each other and an input text field matches more than one topic, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identifying the topics encountered by input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus.In our evaluation of 100 real forms, the proposed approach has shown a comparable performance to the rules-based one. Our experiments also show that the accuracy of the rules-based approach can be improved by up to 19% when integrated with our approach. CCS concepts \u2022 Software and its software and its rule-based engineering; keyword verification; search engine identification; and error identification;"}, {"heading": "1. INTRODUCTION", "text": "It is important that we have no problems in our financial, social and other day-to-day activities because their behavior is determined by the interactions between programs written in different languages and running simultaneously in the front end and the back end. To avoid dealing with these complex interactions, the test engineers treat the application as a black box and abstract the DOMs (Document Object Models) presented to the end user in the browser as if they were performing the application's behavior patterns as a state chart. Since manual state research is often labor-intensive and incomplete, they are looking for crawling-based techniques [9, 14, 15, 24, 29] which will systematically and automatically examine the states of web applications. Although such techniques largely automate the testing of complicated web applications, they are generated in valid input value."}, {"heading": "In Browser:", "text": "In fact, we will be able to assert ourselves, we will be able to assert ourselves, we will be able to assert ourselves in the world."}, {"heading": "2. BACKGROUND AND MOTIVATION", "text": "This year is the highest in the history of the country."}, {"heading": "3. APPROACH", "text": "Once the topics are identified, the corresponding values can be selected from a pre-created database or generated by data models such as Smart Profile [7]. Our approach includes two phases, which are illustrated in Figure 2. First, during the training phase, we extract feature vectors from collected input fields to build a training corpus, derive vector space models from the corpus and label each vector in the corpus according to its theme. It should be noted that the labeling may not be carried out individually because the vectors are caused by a heuristics suggested in Section 3.3. As a result, we can label a group of vectors as the same topic at a time. Second, the artifacts from the training phase fully automate the inference phase."}, {"heading": "3.1 Feature Extraction", "text": "The first step of the proposed approach is to extract the feature vector from an input field encountered, which is expressed in a DOM element. New in this paper is that we consider not only the attributes, but also the nearby labels or descriptions of the DOM element during feature extraction. Algorithm 1 shows how it is achieved. First, we specify DOM attributes such as id, name, placeholders, and maximum lengths that affect the topic identification in an attribute list, and the matching attributes and their values of the DOM elements are inserted into the feature vector (lines 2 to 4). To find the corresponding descriptions, we search the siblings of the DOM element for tags such as span and label in an attribute list, and insert the texts enclosed by the tags into the feature vector (lines 11 to 18). If no such descriptions are found, the search is continued on the parent element (20)."}, {"heading": "3.2 Vector Transformation", "text": "In fact, it is a purely mental game, which is about finding a solution, which is about finding a solution, which is about finding a solution, which is about finding a solution, which is about finding a solution, which is about finding a solution, which is about finding a solution, in order to find a solution, which is about finding a solution, which is about finding a solution, which is about finding a solution, in order to find a solution, in order to find a solution, which is about finding a solution, in order to find a solution, in order to find a solution that is capable of finding a solution, in order to find a solution for a solution."}, {"heading": "3.3 Topic Identification and Labeling", "text": "At this stage, each input field in the training corpus is labeled by its theme, and we can use the results from earlier stages to facilitate the labeling process. First, conceptually similar input fields are expected to be located near the latent vector space. For example, the converted vectors of the second and third input fields in Figure 4 are near: [0, 0.913020), (1, \u2212 0.094889), (3, \u2212 0.000177), (5, \u2212 0.013603), (11, \u2212 0.396488), [0, 0.913239), (1, \u2212 0.092972), (2, \u2212 0.000165), (3), (4), (4), 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5."}, {"heading": "3.4 Inference with the Models and Topics", "text": "To calculate the similarity between two vectors, we assume cosinal similarity, i.e. the cosine of the angle, because it is reported as a good measure for obtaining information [28]. The cosinal similarity of two vectors A and B is: \u2219 Algorithm 2 describes how we determine the input topic based on its cosinal similarities to training data. First, the topic of the vector in latent space that is most similar to the one encountered (lines 3 to 4) is selected. If the difference in similarities between the five most similar vectors is less than one threshold, the topic is determined by a selection procedure within the top 5 vectors (lines 5 to 8)."}, {"heading": "3.5 Integration with the Rule-based Approach", "text": "The questions of the rules-based approach mentioned in Section 2 can be solved by integrating with the proposed approach. Algorithm 3 provides the details. First, for the input fields that are not identified by the rules-based method, we output the answer found by our technique (lines 4 to 5). Second, for the input fields that correspond to several rules for different topics, we select the answer using the natural language technique (lines 6 to 11). In particular, if the natural language answer appears in the candidates, the answer is selected (lines 8 to 9), or a random selection is made from all candidates, including the natural address (line 11). In Section 5, we evaluated the effectiveness of the integration."}, {"heading": "4. IMPLEMENTATION", "text": "We implemented the proposed method with Python 2.7. A Python library, gensim [26], is used for vector-related operations such as vector transformation and similarity calculation, interaction with web applications is supported by Selenium Webdriver [2], and BeautifulSoup [1] is used for parsing and manipulating DOMs."}, {"heading": "5. EVALUATION", "text": "To evaluate the effectiveness of the proposed approach, we conducted two controlled experiments with 100 real-world forms. In the first experiment, we analyzed and labeled the input fields in the forms, used some forms to create training bodies and derive rules, and evaluated the performance of the proposed and rules-based approaches. In the second experiment, 35 simple forms from the 100 forms were actually tested with identified input topics and corresponding values using different methods, answering two research questions: Q1. How effective is the proposed approach compared to the rules-based approach? How much training data is required? Q2. Can the proposed approach be used to improve the rules-based approach? Our experimental data along with the implementation are publicly available [3]."}, {"heading": "5.1 Subject Forms", "text": "We have collected 100 application forms for graduate programs in 9 countries of the world (the full list is included in the appendix), and two examples of the forms are in Figure 5. There are a total of 958 input fields in the forms, ranging from two to fifty-eight for each form, and 62 input topics such as password, email, first name and zip code are labeled. Table 3 shows the labeled topics and the number of input fields for each topic. These topics must be distinguished from each other in order to forward the forms. For example, we select application forms for several reasons as subject data. First, they usually contain many different topics of input formats such as user profile, date or yyyyy for input fields in different forms with date information. We select application forms as subject data for several reasons. First, they usually include many different topics of input formats such as user profile, date or year yyyyy for input fields."}, {"heading": "5.2 Experimental Setup", "text": "In the first experiment, to understand how the proportion of training data influences the performance of the methods under evaluation, we randomly selected 10%, 20%, 30%, 40% and 50% of the test forms as training data. We then derived artifacts such as labeled corpus, vector models (for the proposed approach) and rules (for the rules-based approach) from the input fields of the training forms and used the artifacts to derive the input fields in the remaining forms. (2) RB, the manual, rules-based approach was calculated to show the percentage of correctly identified input fields in the remaining forms. Experiments were repeated 1,000 times. Five methods were evaluated in the first experiments: (1) NL, the proposed natural language approach. (2) RB, the rules-based approach. (3) RB + NL-n (no match), NL-values to identify input fields that are not recognized by the RB approach."}, {"heading": "5.3 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.3.1 Experiments with topic data", "text": "Table 4 shows the average accuracy that each method achieves when the percentages considered are used as training data. Firstly, with only 10% of subjects as training data, the proposed approach and the rules-based approach increase both in deriving the RB input fields 90%, with average accuracy 69.84% and 75.11%, respectively, the average accuracy of the proposed approach increases with the percentage used as training data, but the performance of the rules-based approach slightly decreases as the share of training data increases. As a result, the proposed approach performs similar to the rules-based approach with 50% as training data. Secondly, while RB + NL-n achieves some improvements with 10% as training data, where the improvement by identifying the no-match elements with the proposed approach is not significant."}, {"heading": "5.3.2 Experiments with Real Forms", "text": "The results of the second experiments are presented in Table 7. In these 10 studies, we can see that the randomly generated values are not helpful in passing the forms and only 1.7 out of 28 forms were passed on average. On the other hand, the proposed method (NL) performs better on average and in some cases than the rule-based method (RB), and the effectiveness is relatively stable in terms of the number of forms passed. Furthermore, it can be noted that RB + NLn performs worse than the original RB. By examining the forms passed in RB, but failing in RB + NL-n, we found that the input fields that did not comply with any rule were assigned random values in RB, but in RB + NL-n specific values were assigned to the fields, such as emails or passwords that contain special characters. Observation suggests that incorrectly identified input topics may undermine the exploratory ability of a crawler. Finally, the results of RB + NL-n show that the approach proposed in Table 7 can be significantly improved with the first approach that is consistent with the proposed."}, {"heading": "5.3.3 Threats to Validity", "text": "Implementation of the proposed approach could affect the validity of the results. To ensure accuracy, we have used mature and open source libraries such as gensim [26], BeautifulSoup [1] and Selenium Webdriver [2] in important steps of our implementation. Furthermore, the selected topic forms and the structure of the evaluation, such as the defined topics and derived rules, could affect the generality of the results. To make the experimental data representative, we have collected real-world forms in nine countries around the world. We also open our source code and data to the public [3] for verification and replication."}, {"heading": "6. RELATED WORK", "text": "As a matter of fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to move, to move, to move, to move, to move, to, to move, to move, to, to move, to move, to, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to, to move, to move, to move, to, to move, to move, to"}, {"heading": "7. CONCLUSION", "text": "In this paper, we proposed a natural language technology for entering topics in web application tests. Using vector-space models and topics derived from training corpus, the topics of the input fields encountered are followed based on their semantic similarities to vectors in training corpus. http: / / www.http: / / www.vectors.org / world / index.phase approach can then be significantly improved with the proposed approach. [The proposed approach addresses the problems of the rules-based approach in existing crawlers to make it more applicable.] Our evaluation shows that the proposed method can be significantly improved with the proposed technique, comparable to the rules-based method and the accuracy of the rules-based approach. In the experiment with real forms, the proposed technique performs better than the rules-based method in some cases, and its effectiveness is relatively stable. In the future, we plan to conduct experiments with more data and explore the possibility of applying our technique in other contexts such as clicks."}], "references": [{"title": "Using GUI Ripping for Automated Testing of Android Applications", "author": ["D. Amalfitano", "A.R. Fasolino", "P. Tramontana", "S. De Carmine", "A. Memon"], "venue": "In Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2012}, {"title": "Statistical Debugging Using Latent Topic Models", "author": ["D Andrzejewski", "A. Mulhern", "B. Liblit", "X. Zhu"], "venue": "In Proceedings of the 18th European Conference on Machine Learning. ECML \u201907. Berlin, Heidelberg: Springer-Verlag,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "VeriWeb: Automatically Testing Dynamic Web Sites", "author": ["M. Benedikt", "J. Freire", "P. Godefroid"], "venue": "In International World Wide Web Conference (WWW). Honolulu, HI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2002}, {"title": "Latent Dirichlet Allocation", "author": ["Blei", "David M", "Ng", "Andrew Y", "Jordan", "Michael I"], "venue": "J. Mach. Learn. Res", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2003}, {"title": "CrossCheck: Combining Crawling and Differencing to Better Detect Cross-browser Incompatibilities in Web Applications", "author": ["S.R. Choudhary", "M.R. Prasad", "A. Orso"], "venue": "IEEE Fifth International Conference on Software Testing, Verification and Validation (ICST)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "WebMate: Web Application Test Generation in the Real World", "author": ["V. Dallmeier", "B. Pohl", "M. Burger", "M. Mirold", "A. Zeller"], "venue": "In 2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops (ICSTW)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Concept-based Failure Clustering", "author": ["N. DiGiuseppe", "J.A. Jones"], "venue": "In Proceedings of the ACM SIGSOFT 20th  International Symposium on the Foundations of Software Engineering. FSE \u201912", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Semantic Fault Diagnosis: Automatic Natural-language Fault Descriptions", "author": ["N. DiGiuseppe", "J.A. Jones"], "venue": "In Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering. FSE \u201912", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "AJAX Crawl: Making AJAX Applications Searchable", "author": ["C. Duda", "G. Frey", "D. Kossmann", "R. Matter", "C. Zhou"], "venue": "In IEEE 25th International Conference on Data Engineering,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Feedback-directed exploration of web applications to derive test models", "author": ["A.M. Fard", "A. Mesbah"], "venue": "IEEE 24th International Symposium on Software Reliability Engineering (ISSRE)", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Leveraging Existing Tests in Automated Test Generation for Web Applications", "author": ["A.M. Fard", "M. Mirzaaghaei", "A. Mesbah"], "venue": "In Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering. ASE \u201914", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Linear Algebra, 4th Edition 4th edition., Upper Saddle River, N.J: Pearson", "author": ["Friedberg", "Stephen H", "Insel", "Arnold J", "Spence", "Lawrence E"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "A systematic mapping study of web application testing", "author": ["V. Garousi", "A. Mesbah", "A. Betin-Can", "S. Mirshokraie"], "venue": "Information and Software Technology 55,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "Statistics for Management and Economics", "author": ["G. Keller", "B. Warrack"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "Bug localization using latent Dirichlet allocation", "author": ["S.K. Lukins", "N.A. Kraft", "L.H. Etzkorn"], "venue": "Information and Software Technology 52,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Dynodroid: An Input Generation System for Android Apps", "author": ["A. Machiry", "R. Tahiliani", "M. Naik"], "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2013}, {"title": "Introduction to Information Retrieval", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2008}, {"title": "State-Based Testing of Ajax Web Applications.pdf", "author": ["A. Marchetto", "P. Tonella", "F. Ricca"], "venue": "In 2008 1st International Conference on Software Testing, Verification, and Validation", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "GUI ripping: reverse engineering of graphical user interfaces for testing", "author": ["A. Memon", "I. Banerjee", "A. Nagarajan"], "venue": "In 10th Working Conference on Reverse Engineering,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2003}, {"title": "Crawling Ajax-Based Web Applications Through Dynamic Analysis of User Interface State Changes", "author": ["A. Mesbah", "A. van Deursen", "S. Lenselink"], "venue": "ACM Trans. Web 6,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Invariant- Based Automatic Testing of Modern Web Applications", "author": ["A. Mesbah", "A. van Deursen", "D. Roest"], "venue": "IEEE Transactions on Software Engineering", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["R. \u0158eh\u016f\u0159ek", "P. Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2010}, {"title": "Mining Behavior Models from Enterprise Web Applications", "author": ["M. Schur", "A. Roth", "A. Zeller"], "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Modern Information Retrieval: A Brief Overview", "author": ["Singhal", "Amit"], "venue": "IEEE Data Eng. Bull. 24,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}], "referenceMentions": [{"referenceID": 4, "context": "Since manual state exploration is often labor-intensive and incomplete, crawling-based techniques [9, 10, 13, 14, 15, 24, 25, 27, 29] are introduced to systematically and automatically explore the state spaces of web applications.", "startOffset": 98, "endOffset": 133}, {"referenceID": 5, "context": "Since manual state exploration is often labor-intensive and incomplete, crawling-based techniques [9, 10, 13, 14, 15, 24, 25, 27, 29] are introduced to systematically and automatically explore the state spaces of web applications.", "startOffset": 98, "endOffset": 133}, {"referenceID": 8, "context": "Since manual state exploration is often labor-intensive and incomplete, crawling-based techniques [9, 10, 13, 14, 15, 24, 25, 27, 29] are introduced to systematically and automatically explore the state spaces of web applications.", "startOffset": 98, "endOffset": 133}, {"referenceID": 9, "context": "Since manual state exploration is often labor-intensive and incomplete, crawling-based techniques [9, 10, 13, 14, 15, 24, 25, 27, 29] are introduced to systematically and automatically explore the state spaces of web applications.", "startOffset": 98, "endOffset": 133}, {"referenceID": 10, "context": "Since manual state exploration is often labor-intensive and incomplete, crawling-based techniques [9, 10, 13, 14, 15, 24, 25, 27, 29] are introduced to systematically and automatically explore the state spaces of web applications.", "startOffset": 98, "endOffset": 133}, {"referenceID": 19, "context": "Since manual state exploration is often labor-intensive and incomplete, crawling-based techniques [9, 10, 13, 14, 15, 24, 25, 27, 29] are introduced to systematically and automatically explore the state spaces of web applications.", "startOffset": 98, "endOffset": 133}, {"referenceID": 20, "context": "Since manual state exploration is often labor-intensive and incomplete, crawling-based techniques [9, 10, 13, 14, 15, 24, 25, 27, 29] are introduced to systematically and automatically explore the state spaces of web applications.", "startOffset": 98, "endOffset": 133}, {"referenceID": 22, "context": "Since manual state exploration is often labor-intensive and incomplete, crawling-based techniques [9, 10, 13, 14, 15, 24, 25, 27, 29] are introduced to systematically and automatically explore the state spaces of web applications.", "startOffset": 98, "endOffset": 133}, {"referenceID": 16, "context": "We then build a training corpus with the feature vectors, and apply a series of transformations including Bag-of-words, Tf-idf (Term frequency with inverse document frequency) and LSI (Latent Semantic Indexing) [21] to the corpus, to represent the feature vectors with real numbers.", "startOffset": 211, "endOffset": 215}, {"referenceID": 12, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 97, "endOffset": 101}, {"referenceID": 4, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 139, "endOffset": 174}, {"referenceID": 5, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 139, "endOffset": 174}, {"referenceID": 8, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 139, "endOffset": 174}, {"referenceID": 9, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 139, "endOffset": 174}, {"referenceID": 10, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 139, "endOffset": 174}, {"referenceID": 19, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 139, "endOffset": 174}, {"referenceID": 20, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 139, "endOffset": 174}, {"referenceID": 22, "context": "To capture the behaviors of such applications, crawling-based technique plays a significant role [17] in automated web application testing [9, 10, 13, 14, 15, 24, 25, 27, 29].", "startOffset": 139, "endOffset": 174}, {"referenceID": 9, "context": "Although exhaustive crawling can cause state explosion problem in most industrial web applications that have huge state spaces, the navigational diversity of the crawling is still important because we hope for deriving a test model with adequate functionality coverage [14].", "startOffset": 269, "endOffset": 273}, {"referenceID": 10, "context": "However, achieving this diversity is challenging because many web applications require specific inputs to reach the hidden states behind input fields or forms [15].", "startOffset": 159, "endOffset": 163}, {"referenceID": 8, "context": "While there was a crawling technique ignoring text input [13], most existing crawlers [10, 24, 27] handle the input fields with randomly generated or user-specified data.", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "While there was a crawling technique ignoring text input [13], most existing crawlers [10, 24, 27] handle the input fields with randomly generated or user-specified data.", "startOffset": 86, "endOffset": 98}, {"referenceID": 19, "context": "While there was a crawling technique ignoring text input [13], most existing crawlers [10, 24, 27] handle the input fields with randomly generated or user-specified data.", "startOffset": 86, "endOffset": 98}, {"referenceID": 22, "context": "While there was a crawling technique ignoring text input [13], most existing crawlers [10, 24, 27] handle the input fields with randomly generated or user-specified data.", "startOffset": 86, "endOffset": 98}, {"referenceID": 16, "context": "In unsupervised document analysis, vector transformations such as Tf-idf and LSI are algorithms that project a text corpus to a vector space by examining the word statistical co-occurrence patterns [21].", "startOffset": 198, "endOffset": 202}, {"referenceID": 11, "context": "Furthermore, LSI is used to reduce the rank of a word-document matrix by applying Singular Value Decomposition [16], a mathematical technique in linear algebra.", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": ", the cosine of the angle) between them [28].", "startOffset": 40, "endOffset": 44}, {"referenceID": 2, "context": "Once the topics are identified, the corresponding values can then be selected from a pre-established databank or generated by data models such as smart profile [7].", "startOffset": 160, "endOffset": 163}, {"referenceID": 16, "context": "After all input fields for training are represented as feature vectors in a corpus, three transformations are applied to the vectors sequentially: Bag-of-words, Tf-idf and LSI [21].", "startOffset": 176, "endOffset": 180}, {"referenceID": 11, "context": "The transformation reduces the dimension of the vector space constructed by the words and documents in a corpus using a mathematical technique called Singular Value Decomposition [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 23, "context": ", the cosine of the angle, because it is reported a good measure in information retrieval [28].", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "A Python library, gensim [26], is used for vector space related operations such as vector transformation and similarity calculation.", "startOffset": 25, "endOffset": 29}, {"referenceID": 13, "context": "To determine whether the improvements we observed in average accuracies are statistically significant, we conducted a t-test for matched pairs [18] for the NL, RB and RB+NL-b approaches.", "startOffset": 143, "endOffset": 147}, {"referenceID": 21, "context": "To ensure the correctness, we adopted mature and open-sourced libraries such as gensim [26] , BeautifulSoup [1] and Selenium Webdriver [2] in key steps of our implementation.", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 72, "endOffset": 92}, {"referenceID": 8, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 72, "endOffset": 92}, {"referenceID": 17, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 72, "endOffset": 92}, {"referenceID": 19, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 72, "endOffset": 92}, {"referenceID": 22, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 72, "endOffset": 92}, {"referenceID": 4, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 105, "endOffset": 124}, {"referenceID": 9, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 105, "endOffset": 124}, {"referenceID": 10, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 105, "endOffset": 124}, {"referenceID": 20, "context": "Crawling-based techniques for modern web applications have been studied [10, 13, 22, 24, 27] and adopted [9, 14, 15, 25, 29] in automated web application testing.", "startOffset": 105, "endOffset": 124}, {"referenceID": 8, "context": "[13] proposed algorithms to crawl AJAX-based web applications and index the states.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[24] called Crawjax tries to infer a finite state machine of an AJAX-based web application through dynamically analyzing the changes of the DOM contents.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "The tool is also used for detecting and reporting violated invariants [25] and cross-browser incompatibilities [9] in web applications.", "startOffset": 70, "endOffset": 74}, {"referenceID": 4, "context": "The tool is also used for detecting and reporting violated invariants [25] and cross-browser incompatibilities [9] in web applications.", "startOffset": 111, "endOffset": 114}, {"referenceID": 22, "context": "[27] presented a crawler called ProCrawl to extract abstract behavior models from multi-user web applications, focusing on building a model close to business logic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[10], WebMate, can autonomously explore a web application, and use existing test cases as an exploration base.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[22] extracts a finite state machine from existing execution traces of web applications, and generates test cases consisting of dependent event sequences.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[15] combined the knowledge inferred from manual test suites with automatic crawling in test case generation for web applications.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "A couple of metrics such as JavaScript code coverage, path diversity and DOM diversity are also proposed to evaluate the test model derived by a crawler [14].", "startOffset": 153, "endOffset": 157}, {"referenceID": 0, "context": "Studies on GUI ripping for testing purpose [5, 23] and automatic black-box testing on mobile applications [4, 20] are also related to our work in terms of how they explore the interfaces of the applications and derive test models with dynamic analysis.", "startOffset": 43, "endOffset": 50}, {"referenceID": 18, "context": "Studies on GUI ripping for testing purpose [5, 23] and automatic black-box testing on mobile applications [4, 20] are also related to our work in terms of how they explore the interfaces of the applications and derive test models with dynamic analysis.", "startOffset": 43, "endOffset": 50}, {"referenceID": 15, "context": "Studies on GUI ripping for testing purpose [5, 23] and automatic black-box testing on mobile applications [4, 20] are also related to our work in terms of how they explore the interfaces of the applications and derive test models with dynamic analysis.", "startOffset": 106, "endOffset": 113}, {"referenceID": 1, "context": "[6] approached debugging using a variant of Latent-Dirichlet Allocation [8] to identify weak bug topics from strong interference.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[6] approached debugging using a variant of Latent-Dirichlet Allocation [8] to identify weak bug topics from strong interference.", "startOffset": 72, "endOffset": 75}, {"referenceID": 14, "context": "[19] on a developer\u2019s input such as a bug report to localize faults statistically.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Later, DiGiuseppe and Jones [11, 12] adopted natural-language techniques such as feature extraction and Tf-idf in fault description and clustering.", "startOffset": 28, "endOffset": 36}, {"referenceID": 7, "context": "Later, DiGiuseppe and Jones [11, 12] adopted natural-language techniques such as feature extraction and Tf-idf in fault description and clustering.", "startOffset": 28, "endOffset": 36}], "year": 2016, "abstractText": "To automatically test web applications, crawling-based techniques are usually adopted to mine the behavior models, explore the state spaces or detect the violated invariants of the applications. However, in existing crawlers, rules for identifying the topics of input text fields, such as login ids, passwords, emails, dates and phone numbers, have to be manually configured. Moreover, the rules for one application are very often not suitable for another. In addition, when several rules conflict and match an input text field to more than one topics, it can be difficult to determine which rule suggests a better match. This paper presents a natural-language approach to automatically identify the topics of encountered input fields during crawling by semantically comparing their similarities with the input fields in labeled corpus. In our evaluation with 100 real-world forms, the proposed approach demonstrated comparable performance to the rule-based one. Our experiments also show that the accuracy of the rule-based approach can be improved by up to 19% when integrated with our approach.", "creator": "Microsoft\u00ae Word 2010"}}}