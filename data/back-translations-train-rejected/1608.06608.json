{"id": "1608.06608", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Aug-2016", "title": "Infinite-Label Learning with Semantic Output Codes", "abstract": "We develop a new statistical machine learning paradigm, named infinite-label learning, to annotate a data point with more than one relevant labels from a candidate set, which pools both the finite labels observed at training and a potentially infinite number of previously unseen labels. The infinite-label learning fundamentally expands the scope of conventional multi-label learning, and better models the practical requirements in various real-world applications, such as image tagging, ads-query association, and article categorization. However, how can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning, where the key is to represent a class/label by a vector of semantic codes, as opposed to treating them as atomic labels. We validate the infinite-label learning by a PAC bound in theory and some empirical studies on both synthetic and real data.", "histories": [["v1", "Tue, 23 Aug 2016 19:14:47 GMT  (257kb,D)", "http://arxiv.org/abs/1608.06608v1", null], ["v2", "Wed, 18 Oct 2017 02:47:58 GMT  (322kb,D)", "http://arxiv.org/abs/1608.06608v2", null], ["v3", "Sat, 21 Oct 2017 00:56:08 GMT  (321kb,D)", "http://arxiv.org/abs/1608.06608v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yang zhang", "rupam acharyya", "ji liu", "boqing gong"], "accepted": false, "id": "1608.06608"}, "pdf": {"name": "1608.06608.pdf", "metadata": {"source": "CRF", "title": "Infinite-Label Learning with Semantic Output Codes", "authors": ["Yang Zhang", "Rupam Acharyya"], "emails": ["yangzhang@knigths.ucf.edu", "racharyy@cs.rochester.edu", "jliu@cs.rochester.edu", "bgong@crcv.ucf.edu"], "sections": [{"heading": null, "text": "ar Xiv: 1"}, {"heading": "1 Introduction", "text": "This year it will be able to inherit the aforementioned brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated brainconsecrated csrteeaeFnln."}, {"heading": "2 Problem statement, modeling, and algorithmic solutions", "text": "In this section, we formally present the problem of Infinite Label Learning, followed by a modeling assumption about the corresponding data generation process. We also offer some exemplary algorithmic solutions. Finally, we contrast Infinite Label Learning with closely related Multi-Label Learning and Zero Shot Learning."}, {"heading": "2.1 Problem statement", "text": "Suppose we have a set of semantic codes of labels L = {\u03bbl \u0433Rn} Ll = 1 and a learning sample S = {(xm, ym) \u0445 Rd \u00b7 {\u2212 1, + 1} L} Mm = 1, where the note yml = + 1 indicates that the l-th label, which is semantically encoded as \u03bbl, is relevant to the m-th data point xm. Semantic codes could be the distributed representations of English words, the phoneme composition of words in the language, the vector of incidental occurrence, or visual attributes of objects. For short, we refer to the semantic codes of labels L = {\u03bbl} Ll = 1 as labels in the rest of the paper. Use U = {prehl} l} l > L to designate another group of labels that are detached from L. Labels in U do not correspond to any data points in the learning sample and are therefore often referred to as zero-shot in the rest of the paper."}, {"heading": "2.2 A modeling assumption", "text": "We consider the following distributions about the data in infinite-label learning, x \u0445 PX =. \"This means that the following distributions about the data in infinite-label learning, x-q =\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "2.3 Algorithmic solutions", "text": "Considering the modeling in Section 2.2, the solution for learning with infinite labels boils down to how to define the labeling function h (x) = {h (\u03bb; x) | \u03bbL-U}. We give some examples and will examine Example 1 both theoretically (Section 3) and empirically (Section 4). Example 1. Assuming that each labeling is represented by a vector (e.g. word vectors, attributes, or click-through statistics) and h (\u03bb; x) = sgn < V x, \u03bb >, we essentially try to find a hyperplane V x to classify either seen or not seen labels by minimizing the empirical risk in equivalent (3). Example 2. We can define a new hypothesis by replacing V x in Example 1 with a neural network nn (x) so that h (\u03bb; sgn < < l (Example) and (Example) is useful (l)."}, {"heading": "2.4 Related work", "text": "The modeling of data, labels and label assignments in infinite label studies also stands in sharp contrast to the prevailing modeling assumptions for traditional classification problems or closely related multi-label learning problems, none of which induces L labels and label assignment by separate distributions. In particular, we compare infinite label learning with multi-label learning [21, 23, 3] in Table 1. Multi-label learning generally assumes that the training sample S = {(xm, ym)} Mm = 1 i.i.d. is taken from the common distribution PXY, completely ignoring the distribution of L labels. Consequently, multi-label classifiers cannot handle invisible labels in U. We can understand infinite label learning as a natural extension of zero-shot learning [15, 19, 8, 17, both rely on external codification for the training of the codification classes in addition."}, {"heading": "3 Infinite-label learning under the PAC learning framework", "text": "In this section, we examine the theoretical properties of infinite-label learning under the PAC learning framework = > log (max. / 8), considering the learning sample S = {(xm, ym) \u0445 Rd \u00b7 {\u2212 1, + 1} L = 1 and the semantic codes of the seen terms L = {\u03bbl, Rn} Ll = 1, the theorem below sheds lights on the numbers of data points and labels that are necessary of error, and the supplementary materials give narrower limits under some sparse regulations on the hypothesis that H.Theorem has put forward."}, {"heading": "4 Empirical studies", "text": "While the theoretical result in Section 3 in some way justifies its \"learning capability,\" there are many other questions that are of interest for the practical application of Infinite Label Learning. We focus on the following two questions and provide some empirical insights using synthetic data and real data. 1. After learning a labeling function from the training set, how many and what types of invisible labels can we confidently handle this labeling function? 2. What effect does it have to vary the number of labels seen L if we get a firm connection between seen and unseen labels L and U? Namely, the same labels L and U are used in the test phase, but we learn different labeling functions by varying the labels seen L."}, {"heading": "4.1 Synthetic experiments", "text": "We generate some synthetic data to answer the first question. This gives us the flexibility to control the number of labels. (We generate some synthetic data to answer the first question.) We generate some synthetic data to answer the first question. (We generate the labels of the labels of the labels of the labels Labels of the labels Labels of the labels Labels of the labels Labels of the labels Labels of the labels Labels of the labels Labels of the labels Labels Labels of the labels Labels of the labels Labels of the labels Labels of the labels Labels of the labels Labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of the labels of"}, {"heading": "4.2 Image tagging", "text": "We are experimenting with image tagging to find empirical answers to the second question that is raised at the beginning of this section. [Image tagging] is a real scenario that can benefit from the proposed work. [Image tag] is often defined as ranking a set of labels for a query image, but the main challenge in practice is that the candidate could be very large - there are about 53M labels on Flickr. [Image label] labels that you do not need to use all labels during training to highlight the images. Instead, we can learn a labeling function from a small number of seen labels and then apply it to all sorts of labels. In this case, how many seen labels should we use to achieve the same results as the labels used for training? This is exactly the second question we ask at the beginning of this section. We are conducting our experiments with NUS WIDE datasets through."}, {"heading": "5 Conclusion", "text": "In this paper, we have proposed a new paradigm of machine learning, which we have conceived as infinite label learning. [It fundamentally expands the scope of multi-label learning, in which, at the test stage, the learned label function can assign a potentially infinite number of relevant labels to a data point.] Infinite label learning is made feasible by representing the labels with semantic codes. There are many avenues for future work to further explore Infinite label learning. We note that our current label learning is likely to be improved. Theoretical understanding of its performance under MiAP evaluation is also necessary for MiAP to prevail in evaluating multi-label results. A particularly interesting application of Infinite label learning is to the extreme multi-label classification problems."}], "references": [{"title": "The landmark selection method for multiple output prediction", "author": ["Krishnakumar Balasubramanian", "Guy Lebanon"], "venue": "In Proceedings of the 29th International Conference on Machine Learning", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["Kush Bhatia", "Himanshu Jain", "Purushottam Kar", "Manik Varma", "Prateek Jain"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Efficient multi-label classification with many labels", "author": ["Wei Bi", "James Kwok"], "venue": "In Proceedings of the 30th International Conference on Machine Learning", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Learning to rank using gradient descent", "author": ["Chris Burges", "Tal Shaked", "Erin Renshaw", "Ari Lazier", "Matt Deeds", "Nicole Hamilton", "Greg Hullender"], "venue": "In Proceedings of the 22nd international conference on Machine learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Nus-wide: a real-world web image database from national university of singapore", "author": ["Tat-Seng Chua", "Jinhui Tang", "Richang Hong", "Haojie Li", "Zhiping Luo", "Yantao Zheng"], "venue": "In Proceedings of the ACM international conference on image and video retrieval,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Describing objects by their attributes", "author": ["Alireza Farhadi", "Ian Endres", "Derek Hoiem", "David Forsyth"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Multi-label prediction via compressed sensing", "author": ["Daniel Hsu", "Sham Kakade", "John Langford", "Tong Zhang"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Attribute-based classification for zeroshot visual object categorization", "author": ["Christoph H Lampert", "Hannes Nickisch", "Stefan Harmeling"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Socializing the semantic gap: A comparative survey on image tag assignment, refinement and retrieval", "author": ["Xirong Li", "Tiberio Uricchio", "Lamberto Ballan", "Marco Bertini", "Cees GM Snoek", "Alberto Del Bimbo"], "venue": "arXiv preprint arXiv:1503.08248,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "Predicting unseen labels using label hierarchies in large-scale multi-label learning", "author": ["Jinseok Nam", "Eneldo Loza Menc\u00eda", "Hyunwoo J Kim", "Johannes F\u00fcrnkranz"], "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["Mohammad Norouzi", "Tomas Mikolov", "Samy Bengio", "Yoram Singer", "Jonathon Shlens", "Andrea Frome", "Greg S Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1312.5650,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Zero-shot learning with semantic output codes", "author": ["Mark Palatucci", "Dean Pomerleau", "Geoffrey E Hinton", "Tom M Mitchell"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "An embarrassingly simple approach to zero-shot learning", "author": ["Bernardino Romera-Paredes", "PHS Torr"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Zero-shot learning through cross-modal transfer", "author": ["Richard Socher", "Milind Ganjoo", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Efficient pairwise learning using kernel ridge regression: an exact two-step method", "author": ["Michiel Stock", "Tapio Pahikkala", "Antti Airola", "Bernard De Baets", "WillemWaegeman"], "venue": "arXiv preprint arXiv:1606.04275,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Mining multi-label data. In Data mining and knowledge discovery handbook, pages 667\u2013685", "author": ["Grigorios Tsoumakas", "Ioannis Katakis", "Ioannis Vlahavas"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Modular construction of time-delay neural networks for speech recognition", "author": ["Alex Waibel"], "venue": "Neural computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1989}, {"title": "A review on multi-label learning algorithms. Knowledge and Data Engineering", "author": ["Min-Ling Zhang", "Zhi-Hua Zhou"], "venue": "IEEE Transactions on,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Fast zero-shot image tagging", "author": ["Yang Zhang", "Boqing Gong", "Mubarak Shah"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Multi-label output codes using canonical correlation analysis", "author": ["Yi Zhang", "Jeff G Schneider"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}], "referenceMentions": [{"referenceID": 19, "context": "The above problems are actually often used to benchmark different (extreme) multi-label learning algorithms [21, 23, 3].", "startOffset": 108, "endOffset": 119}, {"referenceID": 21, "context": "The above problems are actually often used to benchmark different (extreme) multi-label learning algorithms [21, 23, 3].", "startOffset": 108, "endOffset": 119}, {"referenceID": 1, "context": "The above problems are actually often used to benchmark different (extreme) multi-label learning algorithms [21, 23, 3].", "startOffset": 108, "endOffset": 119}, {"referenceID": 13, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 210, "endOffset": 225}, {"referenceID": 17, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 210, "endOffset": 225}, {"referenceID": 6, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 210, "endOffset": 225}, {"referenceID": 15, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 210, "endOffset": 225}, {"referenceID": 13, "context": "How can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning [15, 19, 8, 17], where the key is to represent a class by a vector of semantic codes [15], as opposed to treating them as atomic labels.", "startOffset": 295, "endOffset": 299}, {"referenceID": 20, "context": "Thanks to the studies in linguistics, words in speech recognition are represented by combinations of phonemes [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "In visual object recognition, a class is described by a set of visual attributes [10, 7].", "startOffset": 81, "endOffset": 88}, {"referenceID": 5, "context": "In visual object recognition, a class is described by a set of visual attributes [10, 7].", "startOffset": 81, "endOffset": 88}, {"referenceID": 10, "context": "More recently, the distributed representations of English words [12, 16] have found their applications in a variety of tasks.", "startOffset": 64, "endOffset": 72}, {"referenceID": 14, "context": "More recently, the distributed representations of English words [12, 16] have found their applications in a variety of tasks.", "startOffset": 64, "endOffset": 72}, {"referenceID": 8, "context": "The labels in U correspond to no data points in the training sample and thus often termed zero-shot labels [10, 24].", "startOffset": 107, "endOffset": 115}, {"referenceID": 22, "context": "The labels in U correspond to no data points in the training sample and thus often termed zero-shot labels [10, 24].", "startOffset": 107, "endOffset": 115}, {"referenceID": 19, "context": "We particularly compare the infinite-label learning to multi-label learning [21, 23, 3] in Table 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 21, "context": "We particularly compare the infinite-label learning to multi-label learning [21, 23, 3] in Table 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 1, "context": "We particularly compare the infinite-label learning to multi-label learning [21, 23, 3] in Table 1.", "startOffset": 76, "endOffset": 87}, {"referenceID": 13, "context": "We can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17].", "startOffset": 87, "endOffset": 102}, {"referenceID": 17, "context": "We can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17].", "startOffset": 87, "endOffset": 102}, {"referenceID": 6, "context": "We can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17].", "startOffset": 87, "endOffset": 102}, {"referenceID": 15, "context": "We can understand infinite-label learning as a natural extension of zero-shot learning [15, 19, 8, 17].", "startOffset": 87, "endOffset": 102}, {"referenceID": 7, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 36, "endOffset": 49}, {"referenceID": 23, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 36, "endOffset": 49}, {"referenceID": 1, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 36, "endOffset": 49}, {"referenceID": 0, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 78, "endOffset": 84}, {"referenceID": 2, "context": "The recent works of label embedding [9, 25, 3, 1] and landmark label sampling [2, 4] are particularly related to ours.", "startOffset": 78, "endOffset": 84}, {"referenceID": 11, "context": "The most recent works [13, 24] can be regarded as specific instantiations of infinite-label learning, while we formally formulate the problem and provide both theoretical and empirical studies as follows.", "startOffset": 22, "endOffset": 30}, {"referenceID": 22, "context": "The most recent works [13, 24] can be regarded as specific instantiations of infinite-label learning, while we formally formulate the problem and provide both theoretical and empirical studies as follows.", "startOffset": 22, "endOffset": 30}, {"referenceID": 18, "context": "In addition, the pairwise learning [20] is a very interesting framework which encapsulates matrix completion, collaborative filtering, zero-shot and the infinite-label learning.", "startOffset": 35, "endOffset": 39}, {"referenceID": 18, "context": "As a result, our analyses apply to pairwise learning when yh is binary in [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 0, "context": "k=1 \u03c0kN ( \u03bck,UkU T k , ) , \u03bb \u223c N ([ 2 3 ] , [ 1 1.", "startOffset": 34, "endOffset": 41}, {"referenceID": 1, "context": "k=1 \u03c0kN ( \u03bck,UkU T k , ) , \u03bb \u223c N ([ 2 3 ] , [ 1 1.", "startOffset": 34, "endOffset": 41}, {"referenceID": 21, "context": "We borrow from multi-label classification [23] the Hamming loss as the evaluation metric.", "startOffset": 42, "endOffset": 46}, {"referenceID": 4, "context": "We conduct our experiments using the NUS-WIDE dataset [6].", "startOffset": 54, "endOffset": 57}, {"referenceID": 16, "context": "The image features x are l2-normalized VGGNet-19 [18] last fully-connected-layer activations.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "We obtain their semantic codes by the pre-trained GloVe word vectors [16].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "We report the results measured by four popular metrics: Mean image Average Precision (MiAP) [11] and the top-3 precision, recall, and F1-score.", "startOffset": 92, "endOffset": 96}, {"referenceID": 3, "context": "Accordingly, in order to impose the ranking property to our labeling function, we learn it using the RankNet loss [5],", "startOffset": 114, "endOffset": 117}, {"referenceID": 12, "context": "[14] and Akata et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "One particularly interesting application of infinite-label learning is on the extreme multi-label classification problems [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "[2] Krishnakumar Balasubramanian and Guy Lebanon.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] Kush Bhatia, Himanshu Jain, Purushottam Kar, Manik Varma, and Prateek Jain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] Wei Bi and James Kwok.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] Tat-Seng Chua, Jinhui Tang, Richang Hong, Haojie Li, Zhiping Luo, and Yantao Zheng.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Alireza Farhadi, Ian Endres, Derek Hoiem, and David Forsyth.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Tomas Mikolov, et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Daniel Hsu, Sham Kakade, John Langford, and Tong Zhang.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] Christoph H Lampert, Hannes Nickisch, and Stefan Harmeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Xirong Li, Tiberio Uricchio, Lamberto Ballan, Marco Bertini, Cees GM Snoek, and Alberto Del Bimbo.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] Jinseok Nam, Eneldo Loza Menc\u00eda, Hyunwoo J Kim, and Johannes F\u00fcrnkranz.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] Mohammad Norouzi, Tomas Mikolov, Samy Bengio, Yoram Singer, Jonathon Shlens, Andrea Frome, Greg S Corrado, and Jeffrey Dean.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] Mark Palatucci, Dean Pomerleau, Geoffrey E Hinton, and Tom M Mitchell.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] Jeffrey Pennington, Richard Socher, and Christopher D Manning.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] Bernardino Romera-Paredes and PHS Torr.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] Karen Simonyan and Andrew Zisserman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] Richard Socher, Milind Ganjoo, Christopher D Manning, and Andrew Ng.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] Michiel Stock, Tapio Pahikkala, Antti Airola, Bernard De Baets, and WillemWaegeman.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] Grigorios Tsoumakas, Ioannis Katakis, and Ioannis Vlahavas.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] Alex Waibel.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] Min-Ling Zhang and Zhi-Hua Zhou.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] Yang Zhang, Boqing Gong, and Mubarak Shah.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] Yi Zhang and Jeff G Schneider.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We develop a new statistical machine learning paradigm, named infinite-label learning, to annotate a data point with more than one relevant labels from a candidate set, which pools both the finite labels observed at training and a potentially infinite number of previously unseen labels. The infinite-label learning fundamentally expands the scope of conventional multi-label learning, and better models the practical requirements in various real-world applications, such as image tagging, ads-query association, and article categorization. However, how can we learn a labeling function that is capable of assigning to a data point the labels omitted from the training set? To answer the question, we seek some clues from the recent work on zero-shot learning, where the key is to represent a class/label by a vector of semantic codes, as opposed to treating them as atomic labels. We validate the infinite-label learning by a PAC bound in theory and some empirical studies on both synthetic and real data. 1 ar X iv :1 60 8. 06 60 8v 1 [ cs .L G ] 2 3 A ug 2 01 6", "creator": "LaTeX with hyperref package"}}}