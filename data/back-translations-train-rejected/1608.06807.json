{"id": "1608.06807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "Efficient Training for Positive Unlabeled Learning", "abstract": "Positive unlabeled learning (PU learning) refers to the task of learning a binary classifier from only positive and unlabeled data [1]. This problem arises in various practical applications, like in multimedia/information retrieval [2], where the goal is to find samples in an unlabeled data set that are similar to the samples provided by a user, as well as for applications of outlier detection [3] or semi-supervised novelty detection [4]. The works in [5] and [6] have recently shown that PU learning can be formulated as a risk minimization problem. In particular, expressing the risk with a convex loss function, like the double Hinge loss, allows to achieve better classification performance than those ones obtained by using other loss functions. Nevertheless, the works have only focused in analysing the generalization performance obtained by using different loss functions, without considering the efficiency of training. In that regard, we propose a novel algorithm, which optimizes efficiently the risk minimization problem stated in [6]. In particular, we show that the storage complexity of our approach scales only linearly with the number of training samples. Concerning the training time, we show experimentally on different benchmark data sets that our algorithm exhibits the same quadratic behaviour of existing optimization algorithms implemented in highly-efficient libraries. The rest of the paper is organized as follows. In Section 2 we review the formulation of the PU learning problem and we enunciate for the first time the Representer theorem. In Section 3 we derive the convex formulation of the problem by using the double Hinge loss function. In Section 4 we propose an algorithm to solve the optimization problem and we finally conclude with the last section by describing the experimental evaluation.", "histories": [["v1", "Wed, 24 Aug 2016 13:38:16 GMT  (698kb,D)", "https://arxiv.org/abs/1608.06807v1", "Technical Report"], ["v2", "Mon, 10 Oct 2016 20:14:08 GMT  (1180kb)", "http://arxiv.org/abs/1608.06807v2", "Submitted to IEEE TPAMI"], ["v3", "Wed, 12 Oct 2016 14:53:15 GMT  (1180kb)", "http://arxiv.org/abs/1608.06807v3", "Submitted to IEEE TPAMI"]], "COMMENTS": "Technical Report", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["emanuele sansone", "francesco g b de natale", "zhi-hua zhou"], "accepted": false, "id": "1608.06807"}, "pdf": {"name": "1608.06807.pdf", "metadata": {"source": "CRF", "title": "Efficient Training for Positive Unlabeled Learning", "authors": ["Emanuele Sansone", "Francesco G. B. De Natale"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 8.06 807v 3 [cs.L G] 12 Oct 201 6Index terms - machine learning, single-class classification, positive unlabelled learning, open set recognition, core methods"}, {"heading": "1 INTRODUCTION", "text": "POSITIVE unlabeled learning (PU learning) refers to the task of learning a binary classifier of only positive and unlabeled data [1]. This classification problem arises in various practical situations. For example: \u2022 Retrieval [2], where the goal is to find samples in an unlabeled data set that is too diverse, and it is therefore difficult to collect and label enough negative samples, where the goal is to find outliers from an unlabeled data set. \u2022 One-vs-Rest classification [4], which is typical of problems where the negative class occurs. \u2022 Open Sentence Recognition [5], where test classes are unknown and therefore the use of unlabeled data."}, {"heading": "2 LITERATURE REVIEW", "text": "PU learning is well known in the machine learning community because it is used in a variety of tasks ranging from matrix completion [9], multi-view learning [10] to semi-supervised learning [11]. It is also used in data mining to classify data streams [12] or time series [13] and to detect events such as coexistence in diagrams [14]. The majority of existing work in PU learning can be divided into two broad categories, which are characterized by two different types of utilization of unlabeled data. The first category consists of two-stage approaches [15], [16], [17], [18] which first extract a number of reliable negative samples from the unlabeled data and secondly use them together with the available positive data to train a binary classifier. These methods are mainly heuristic and their performance depends heavily on the quality of the extraction of negative samples and then classify the second category as negative, all of which consists of these two approaches."}, {"heading": "2.1 Comparison with one-class classification", "text": "In fact, it is the case that most of them will be able to survive on their own, without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process, a process in which there is a process in which there is a process in which there is a process in which there is a process, a process in which there is a process, a process in which there is a process, a process, a process, a process, a process and a process in which there is a process, a process and a process, a process, a process and a process, a process, a process, a process, a process, a process, a process, a process and a process in which there is a process, a process, a process, a process, a process, a process, a process, a process, a process, a process, and a process, and a process, and a process, and a process, and a process."}, {"heading": "2.2 Comparison with semi-supervised learning", "text": "In fact, the fact is that most of them are able to assert themselves, that they are able to assert themselves, that they are able to assert themselves, and that they are able to assert themselves."}, {"heading": "2.3 Theoretical studies about PU learning", "text": "Inspired by the groundbreaking work in [57] and by the first studies on OCC [27], [28], authors in [58] and later with an extended version in [59], the first to define and theoretically analyze the problem of PU learning are the authors. In particular, they propose a framework based on the statistical query model [57] to have theoretical guarantees for classification performance and to derive algorithms based on decision trees. In particular, the authors examine the problem of learning functions characterized by monotonic conjuctions, which are particularly useful in text classification, where documents can be represented with binary vectors to model the presence / absence of words from an available dictionary. Instead of taking binary characteristics into account, authors in [60] propose a Naive-Bayes classification of categorical characteristics in noisy environments. Their work is based on the assumption that attributes are independent, which is highly useful in spaces."}, {"heading": "3 PU LEARNING FORMULATION", "text": "Suppose we get a training dataset Db = (xi, y) dx = (1) between the individual classes (f x). (c) Suppose we get a training dataset Db = (xi, y, y) dx = (f x, y) dx = (f x, y) dx = (f x, y) dx = (f x).Suppose we get a training dataset Db = (xi, y, y) dx = (f x, y) dx = (f x).Suppose a training f = 1 (x) x (x).Suppose we get a training dataset f = 1 (x).Suppose that the training dataset f = 1 (x) x (f x).Zero we can get a training f = 1 (x).Zero we get a training f = 1 (x).Zero we can get a training f = 1 (x)."}, {"heading": "4 USMO ALGORITHM", "text": "In order not to store the gram matrix, we propose an iterative algorithm that converts problem (6) into a sequence of smaller QP sub-problems. Here, the smaller one refers to the consideration of a subset of training samples whose indexes define the working set, and the calculation and storage of the gram matrix temporarily only for this reduced set. Each iteration of the USMO algorithm consists of three main operations: selecting the working set, called S, calculating the gram matrix only for samples associated with indexes in S, and resolving a QP sub-problem where only terms are considered as a function of S. The details of the general algorithm are given in algorithm. It is important to mention that this strategy in principle allows a reduction in the storage requirement for the exponents in S and the resolution of a QP sub-problem. See [65] for the analysis of QP Elements 2 Initiates: MUSS1: General Algorithms: MUSSO: 1:"}, {"heading": "4.1 QP Subproblem", "text": "We begin by considering the following dilemma (evidence in Annex C): 1. Given S = {i, j}, any optimal solution \u03c3 S = [\u03c3 \u0445 i \u03c3 j] T, \u03b4 S = [\u03b4 i \u03b4 j] T of the QP partial problem (8) must fulfill the following condition: that the optimal solution (\u03c3 S = 0 \u2264 u \u2264 c2, either \u03c3 u = c2 \u2212 \u03b4 u 2 or \u03c3 u = \u03b4 u 2. This tells us that the optimal solution (\u03c3 S, \u03b4 S) assumes a specific form and therefore its calculation can be per-6formed by exploiting it in a smaller space. In particular, four possible sub-spaces can be identified for the search: Case 1: \u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441\u0441icicicicicicicicicicicicicicicicicicicicicicicicicicicice ice ice ice ice ice ice ice ice ice ice ice ice ice-ice ice-ice-ice-ice-ice-ice-ice-ice-ice-ice-ice-ice-ice-ice-ice-ice-ice-ice"}, {"heading": "4.2 Optimality Conditions", "text": "A problem of all optimization algorithms is the determination of the stop condition. In algorithm 1, the search for the solution is stopped as soon as certain stationary conditions are met. These conditions, called Karush-Kuhn-Tucker (KKT), constitute the certificates of optimisation for the obtained solution. In the case of (6), they are both necessary and sufficient, since the target is convex and the limitations are affine functions [66]. Specifically, an optimal solution must satisfy the following relations: either the following F certificates, (6) or (6) are both necessary and sufficient, since the target is convex and the limitations are affine functions [66]. Specifically, an optimal solution must satisfy the following relations: either the F certificate or (6)."}, {"heading": "4.3 Working Set Selection", "text": "A natural choice for selecting the work set is to look for pairs that violate the terms of \u03c4 \u2212 optimality conditions. In particular, definition 1. Each pair (xi, xj) of Dn is an infringing pair, if and only if it fulfils the following relationships: xi D1, xj D3 \u21d2 f (xi) \u2212 f (xj) \u2212 f (xi) \u2212 f (xj) < \u2212 \u03c4, xi D2, xj D3 \u21d2 f (xi) \u2212 f (xj) \u2212 f (5) \u2212 f (5) \u2212 f (5) \u2212 f (5) \u2212 f (xj D2 \u21d2 f (xj), xi D1, xj f (xi) \u2212 f (xj) \u2212 f (xj) \u2212 f (5) \u2212 f (5) \u2212 f (5) \u2212 f (xf (xj) \u2212 \u2212 \u2212 \u2212)) \u2212 f (xf) xj) xf (xj) xj \u2212 f (xj) xj \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xj \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xf (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xf (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f \u2212 f (xf (xj) \u2212 f (xj) \u2212 f \u2212 f \u2212 f (xj) \u2212 f \u2212 f (xj) \u2212 f (xj) \u2212 f (xj) \u2212 f (xf \u2212 f \u2212 f (xj) \u2212 f \u2212 f \u2212 f \u2212 f (xj) \u2212 f \u2212 f \u2212 f \u2212 f (xj) \u2212 f (xj), xj xf \u2212 f (xj \u2212 f \u2212 f (xj \u2212 f) \u2212 f \u2212 f \u2212 f \u2212 f \u2212 f (xj \u2212 f), xj \u2212 f \u2212 f \u2212 f \u2212 f), xj xj \u2212 f \u2212 f \u2212 f \u2212 f"}, {"heading": "4.4 Function Cache and Bias Update", "text": "Remember that each iteration of the USMO algorithm is composed of three main operations, namely: selecting the work set, resolving the associated QP sub-problem, and updating the most critical values based on the solution obtained. It is interesting to note that all of these operations require the calculation of the target function f (xu) for all xu operations belonging either to the unbound set or to the total set Dn, depending on the approach chosen by this iteration. In fact, the level of workset7 results. The term unbound operations results from the fact that 0 < c2 for all xu operations Dn.8. This can only be verified if the second approach is used and then requires the entire set Dn scanned.selection to evaluate the conditions in (14) for sample pairs according to f; the equations used to solve the QP sub-problem, shown in Table 1, depends on vector = KSS."}, {"heading": "4.5 Initialization", "text": "As already mentioned, the proposed algorithm is characterized by iterations that focus either on the entire training data set or on a smaller set of unbound samples. < < < < < < < < < < < < < < < < > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > > >"}, {"heading": "5 THEORETICAL ANALYSIS", "text": "In this section we present the main theoretical result, namely that algorithm 1 = result 1 = result 1 = result 1 = result 2 = result 2 (result 2). It is important to remember that each iteration of the USMO corresponds to a line that lies in the two-dimensional plane that depends on a single variable. In particular, if xi and xj correspond to the selected pair of points on an iteration, then the solution space corresponds to a line that is defined in the two-dimensional plane. The feasible region in this plane can be divided into four parts that are defined in Figure 2. These regions are considered as closed sets, including boundary points, such as edges and corners. To look only at the interior of each specified U, we use the notation int U. Based on these considerations it is possible to prove the following situation."}, {"heading": "6 EXPERIMENTAL RESULTS", "text": "In this section, comprehensive evaluations are performed to verify the effectiveness of USMO. The proposed algorithm is compared against [7] and [6]. USMO code is developed in MATLAB and is therefore available at XXXX XXXX.12. Competitors are also implemented in MATLAB to ensure fair comparisons. In particular, the method in [7] solves a problem (6) with the ramp loss function, which uses quadprog function as quadprog method. [69] Experiments are performed on a machine with 16 2.40 GHz cores and 64GB of RAM. A large collection of real data sets from the UCI repository is used, namely 17 sets of data, some containing hundreds of thousands of samples, while the remaining 5 are consistently larger."}, {"heading": "7 CONCLUSION", "text": "This paper proposes an efficient algorithm for PU learning. Theoretical analysis ensures that the obtained solution regains the optimum objective function. Experimental evaluation evaluates the efficiency of the proposed method and shows its potential applicability to real problems related to PU learning. Future research will expand this work and consider both one-time learning and representative learning.12"}, {"heading": "APPENDIX A PROOF OF THE REPRESENTER THEOREM", "text": "Analogous to [63], we define \u03a6 as the set whose elements are the representatives of the training data set D = Dp * Dn, i.e. its orthogonal complement, so that each function f * Hk | g = x * i: xi * D\u03b1ixi, \"i * R\" H = {h * Hk | < h, g > Hk = 0, \"g * Hk\" H * i: xi * D\u03b1ixi, \"i\" R \"H * B = {h * Hk | < h, g > Hk = 0,\" g \"HQ\" H \"Therefore, each function f * Hk can be divided into two orthogonal components, namely f = f * Hk * Hk and f * HQ * Hk * HQ = i * HQ =. The evaluation of the function f * Hk is performed by evaluating the previous properties, viz.f (xj)."}, {"heading": "APPENDIX B", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "PU LEARNING FORMULATION", "text": "Taking into account the definition of the double Hinge loss function and its composite loss, namely its (x, y) = max {\u2212 xy, max {0, 12 \u2212 xy 2} and its (x, y) = \u2212 xy, we can functionally express the empirical risk (3) as follows: \u2212 xi p \u00b2 i: xi \u00b2 Dpf (xi) + 1n \u00b2 i: xi \u00b2 Dnmax {f (xi), max {0, 12 + f (xi) 2} (29) and using the result given in the representator theorem, the optimization problem (4) becomes: min \u03b1, \u03b2 \u2212 c1 \u00b2 n: xi \u00b2 Dp (xi), xi \u00b2 Dp (xi), xi j, xj + j (xi), xj + 2 (xi): xi \u00b2 + 2 (xj): xxi) n \u00b2."}, {"heading": "APPENDIX C PROOF OF LEMMA 1", "text": "By the introduction of the following notation, namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk = namelyk"}, {"heading": "ACKNOWLEDGMENTS", "text": "This research was supported in part by NSFC (61333014) and the Collaborative Innovation Center of Novel Software Technology and Industrialization. We are grateful for the support of NVIDIA Corporation by donating a Titan X Pascal machine to support this research."}], "references": [{"title": "Learning Classifiers from Only Positive and Unlabeled Data", "author": ["C. Elkan", "K. Noto"], "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2008, pp. 213\u2013220.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "One Class Support Vector Machine Based Non-Relevance Feedback Document Retrieval", "author": ["T. Onoda", "H. Murata", "S. Yamada"], "venue": "IEEE International Joint Conference on Neural Networks, vol. 1. IEEE, 2005, pp. 552\u2013557.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Inlier-Based Outlier Detection via Direct Density Ratio Estimation", "author": ["S. Hido", "Y. Tsuboi", "H. Kashima", "M. Sugiyama", "T. Kanamori"], "venue": "2008 Eighth IEEE International Conference on Data Mining. IEEE, 2008, pp. 223\u2013232.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "A Positive and Unlabeled Learning Algorithm for One-Class Classification of Remote-Sensing Data", "author": ["W. Li", "Q. Guo", "C. Elkan"], "venue": "IEEE Transactions on Geoscience and Remote Sensing, vol. 49, no. 2, pp. 717\u2013725, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Toward Open Set Recognition", "author": ["W.J. Scheirer", "A. de Rezende Rocha", "A. Sapkota", "T.E. Boult"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 35, no. 7, pp. 1757\u20131772, 2013.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Analysis of Learning from Positive and Unlabeled Data", "author": ["M. Du Plessis", "G. Niu", "M. Sugiyama"], "venue": "NIPS, 2014, pp. 703\u2013711.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Convex Formulation for Learning from Positive and Unlabeled Data", "author": ["\u2014\u2014"], "venue": "ICML, 2015, pp. 1386\u20131394.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "An Overview of Statistical Learning Theory", "author": ["V.N. Vapnik"], "venue": "Neural Networks, IEEE Transactions on, pp. 988\u2013999, 1999.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1999}, {"title": "PU Learning for Matrix Completion", "author": ["C.-J. Hsieh", "N. Natarajan", "I.S. Dhillon"], "venue": "ICML, 2015, pp. 2445\u20132453.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-View Positive and Unlabeled Learning", "author": ["J.T. Zhou", "S.J. Pan", "Q. Mao", "I.W. Tsang"], "venue": "ACML, 2012, pp. 555\u2013570.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Beyond the Low-Density Separation Principle: A Novel Approach to Semi- Supervised Learning", "author": ["T. Sakai", "M.C. d. Plessis", "G. Niu", "M. Sugiyama"], "venue": "arXiv preprint arXiv:1605.06955, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Positive Unlabeled Learning for Data Stream Classification", "author": ["X. Li", "S.Y. Philip", "B. Liu", "S.-K. Ng"], "venue": "SDM, vol. 9. SIAM, 2009, pp. 257\u2013268.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "Positive Unlabeled Leaning for Time Series Classification", "author": ["M.N. Nguyen", "X.-L. Li", "S.-K. Ng"], "venue": "IJCAI, vol. 11, 2011, pp. 1421\u2013 1426.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Hypergraph-Based Anomaly Detection of High-Dimensional Co-Occurrences", "author": ["J. Silva", "R. Willett"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 31, no. 3, pp. 563\u2013569, 2009.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "Partially Supervised Classification of Text Documents", "author": ["B. Liu", "W.S. Lee", "P.S. Yu", "X. Li"], "venue": "ICML, vol. 2, 2002, pp. 387\u2013 394.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "PEBL: Positive Example Based Learning for Web Page Classification Using SVM", "author": ["H. Yu", "J. Han", "K.C.-C. Chang"], "venue": "International Conference on Knowledge Discovery and Data Mining. ACM, 2002, pp. 239\u2013248.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning to Classify Texts Using Positive and Unlabeled Data", "author": ["X. Li", "B. Liu"], "venue": "IJCAI, vol. 3, 2003, pp. 587\u2013592.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Single-Class Classification with Mapping Convergence", "author": ["H. Yu"], "venue": "Machine Learning, vol. 61, no. 1-3, pp. 49\u201369, 2005.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Building Text Classifiers Using Positive and Unlabeled Examples", "author": ["B. Liu", "Y. Dai", "X. Li", "W.S. Lee", "P.S. Yu"], "venue": "Third IEEE International Conference on Data Mining. IEEE, 2003, pp. 179\u2013186.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Single-class classifier learning using neural networks: An application to the prediction of mineral deposits", "author": ["A. Skabar"], "venue": "Machine Learning and Cybernetics, 2003 International Conference on, vol. 4. IEEE, 2003, pp. 2127\u20132132.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-Supervised Novelty Detection", "author": ["G. Blanchard", "G. Lee", "C. Scott"], "venue": "Journal of Machine Learning Research, vol. 11, no. Nov, pp. 2973\u20133009, 2010.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Domain Anomaly Detection in Machine Perception: A System Architecture and Taxonomy", "author": ["J. Kittler", "W. Christmas", "T. De Campos", "D. Windridge", "F. Yan", "J. Illingworth", "M. Osman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 5, pp. 845\u2013859, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Novelty Detection: A ReviewPart 1: Statistical Approaches", "author": ["M. Markou", "S. Singh"], "venue": "Signal processing, vol. 83, no. 12, pp. 2481\u2013 2497, 2003.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Novelty Detection: A ReviewPart 2: Neural Network Based Approaches", "author": ["\u2014\u2014"], "venue": "Signal processing, vol. 83, no. 12, pp. 2499\u20132521, 2003.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Authorship Verification as a One-Class Classification Problem", "author": ["M. Koppel", "J. Schler"], "venue": "ICML, 2004, p. 62.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "One-Class Collaborative Filtering", "author": ["R. Pan", "Y. Zhou", "B. Cao", "N.N. Liu", "R. Lukose", "M. Scholz", "Q. Yang"], "venue": "Eighth IEEE International Conference on Data Mining. IEEE, 2008, pp. 502\u2013511.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2008}, {"title": "SV Estimation of a Distributions Support", "author": ["B. Sch\u00f6lkopf", "R. Williamson", "A. Smola", "J. Shawe-Taylor"], "venue": "NIPS, vol. 12, 1999.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "Support Vector Domain Description", "author": ["D.M. Tax", "R.P. Duin"], "venue": "Pattern Recognition Letters, vol. 20, no. 11, pp. 1191\u20131199, 1999.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1999}, {"title": "Network Constraints and Multi- Objective Optimization for One-Class Classification", "author": ["M.M. Moya", "D.R. Hush"], "venue": "Neural Networks, vol. 9, no. 3, pp. 463\u2013474, 1996.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Kernel Method for Percentile Feature Extraction", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "A.J. Smola"], "venue": "2000.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2000}, {"title": "Uniform Object Generation for Optimizing One-Class Classifiers", "author": ["D.M. Tax", "R.P. Duin"], "venue": "Journal of Machine Learning Research, vol. 2, no. Dec, pp. 155\u2013173, 2001.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2001}, {"title": "A Linear Programming Approach to Novelty Detection", "author": ["C. Bennett", "K. Campbell"], "venue": "NIPS, vol. 13, no. 13, p. 395, 2001.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2001}, {"title": "One-Class LP Classifiers for Dissimilarity Representations", "author": ["E. Pekalska", "D. Tax", "R. Duin"], "venue": "NIPS, 2002, pp. 761\u2013768.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "One-Class SVMs for Document Classification", "author": ["L.M. Manevitz", "M. Yousef"], "venue": "Journal of Machine Learning Research, vol. 2, no. Dec, pp. 139\u2013154, 2001.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2001}, {"title": "One-Class Classification", "author": ["D.M. Tax"], "venue": "TU Delft, Delft University of Technology,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2001}, {"title": "Combining One-Class Classifiers", "author": ["D.M. Tax", "R.P. Duin"], "venue": "International Workshop on Multiple Classifier Systems. Springer, 2001, pp. 299\u2013308.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2001}, {"title": "Ensembles of One Class Support Vector Machines", "author": ["A.D. Shieh", "D.F. Kamm"], "venue": "International Workshop on Multiple Classifier Systems. Springer, 2009, pp. 181\u2013190.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Constructing Boosting Algorithms from SVMs: An Application to One-Class Classification", "author": ["G. Ratsch", "S. Mika", "B. Scholkopf", "K.-R. Muller"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 9, pp. 1184\u20131199, 2002.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-Class Supervised Novelty Detection", "author": ["V. Jumutc", "J.A. Suykens"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 36, no. 12, pp. 2510\u20132523, 2014.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2014}, {"title": "Theoretical Comparisons of Learning from Positive-Negative, Positive-Unlabeled, and Negative-Unlabeled Data", "author": ["G. Niu", "M.C. d. Plessis", "T. Sakai", "M. Sugiyama"], "venue": "arXiv preprint arXiv:1603.03130, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "One-Class Classification: Taxonomy of Study and Review of Techniques", "author": ["S.S. Khan", "M.G. Madden"], "venue": "The Knowledge Engineering Review, vol. 29, no. 03, pp. 345\u2013374, 2014.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "The Effect of Unlabeled Samples in Reducing the Small Sample Size Problem and Mitigating the Hughes Phenomenon", "author": ["B.M. Shahshahani", "D.A. Landgrebe"], "venue": "IEEE Transactions on Geoscience and remote sensing, vol. 32, no. 5, pp. 1087\u20131095, 1994.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1994}, {"title": "A Mixture of Experts Classifier with Learning Based on Both Labelled and Unlabelled Data", "author": ["D.J. Miller", "H.S. Uyar"], "venue": "NIPS, 1997, pp. 571\u2013577.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1997}, {"title": "The Value of Unlabeled Data for Classification Problems", "author": ["T. Zhang", "F. Oles"], "venue": "ICML, 2000, pp. 1191\u20131198.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2000}, {"title": "Semi-Supervised Learning (Chapelle, O. et al., Eds.; 2006)[Book reviews", "author": ["O. Chapelle", "B. Scholkopf", "A. Zien"], "venue": "IEEE Transactions on Neural Networks, vol. 20, no. 3, pp. 542\u2013542, 2009.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2009}, {"title": "Classtering: Joint Classification and Clustering with Mixture of Factor Analysers", "author": ["E. Sansone", "A. Passerini", "F.G.B.D. Natale"], "venue": "European Conference on Aritificial Intelligence (ECAI), 2016, pp. 1089\u20131095.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-Supervised Classification by Low Density Separation.", "author": ["O. Chapelle", "A. Zien"], "venue": "AISTATS,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2005}, {"title": "Manifold Regularization: A Geometric Framework for Learning from Labeled and Unlabeled Examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning Research, vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi- Supervised Learning with Deep Generative Models", "author": ["D.P. Kingma", "S. Mohamed", "D.J. Rezende", "M. Welling"], "venue": "NIPS, 2014, pp. 3581\u20133589.  15", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-Supervised Learning by Disagreement", "author": ["Z.-H. Zhou", "M. Li"], "venue": "Knowledge and Information Systems, vol. 24, no. 3, pp. 415\u2013 439, 2010.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2010}, {"title": "Convex and Scalable Weakly Labeled SVMs", "author": ["Y.-F. Li", "I.W. Tsang", "J.T. Kwok", "Z.-H. Zhou"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 2151\u20132188, 2013.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2013}, {"title": "LIBSVM: A Library for Support Vector Machines", "author": ["C.-C. Chang", "C.-J. Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), vol. 2, no. 3, p. 27, 2011.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2011}, {"title": "A Study on SMO-Type Decomposition Methods for Support Vector Machines", "author": ["P.-H. Chen", "R.-E. Fan", "C.-J. Lin"], "venue": "IEEE Transactions on Neural Networks, vol. 17, no. 4, pp. 893\u2013908, 2006.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-Supervised Learning in Gigantic Image Collections", "author": ["R. Fergus", "Y. Weiss", "A. Torralba"], "venue": "NIPS, 2009, pp. 522\u2013530.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2009}, {"title": "Large-Scale Manifold Learning", "author": ["A. Talwalkar", "S. Kumar", "H. Rowley"], "venue": "CVPR. IEEE, 2008, pp. 1\u20138.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning with Augmented Class by Exploiting Unlabeled Data", "author": ["Q. Da", "Y. Yu", "Z.-H. Zhou"], "venue": "AAAI, 2014, pp. 1760\u20131766.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2014}, {"title": "Efficient Noise-Tolerant Learning from Statistical Queries", "author": ["M. Kearns"], "venue": "Journal of the ACM (JACM), vol. 45, no. 6, pp. 983\u20131006, 1998.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1998}, {"title": "Positive and Unlabeled Examples Help Learning", "author": ["F. De Comit\u00e9", "F. Denis", "R. Gilleron", "F. Letouzey"], "venue": "International Conference on Algorithmic Learning Theory. Springer, 1999, pp. 219\u2013230.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning from Positive and Unlabeled Examples", "author": ["F. Letouzey", "F. Denis", "R. Gilleron"], "venue": "International Conference on Algorithmic Learning Theory. Springer, 2000, pp. 71\u201385.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2000}, {"title": "Naive Bayes Classifier for Positive Unlabeled Learning with Uncertainty", "author": ["J. He", "Y. Zhang", "X. Li", "Y. Wang"], "venue": "SDM. SIAM, 2010, pp. 361\u2013372.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "On Discriminative vs. Generative Classifiers: A Comparison of Logistic Regression and Naive Bayes", "author": ["A. Jordan"], "venue": "pp. 841\u2013848, 2002.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2002}, {"title": "Theory of Reproducing Kernels", "author": ["N. Aronszajn"], "venue": "Transactions of the American Mathematical Society, pp. 337\u2013404, 1950.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1950}, {"title": "A Generalized Representer Theorem", "author": ["B. Sch\u00f6lkopf", "R. Herbrich", "A.J. Smola"], "venue": "COLT, 2001, pp. 416\u2013426.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2001}, {"title": "A Unifying View of Representer Theorems", "author": ["A. Argyriou", "F. Dinuzzo"], "venue": "ICML, 2014, pp. 748\u2013756.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2014}, {"title": "Invexity and the Kuhn\u2013Tucker Theorem", "author": ["M.A. Hanson"], "venue": "Journal of Mathematical Analysis and Applications, vol. 236, no. 2, pp. 594\u2013 604, 1999.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1999}, {"title": "Estimating the Support of a High-dimensional Distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation, pp. 1443\u20131471, 2001.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2001}, {"title": "On the Implementation of a Primal-Dual Interior Point Method", "author": ["S. Mehrotra"], "venue": "SIAM Journal on Optimization, vol. 2, no. 4, pp. 575\u2013601, 1992.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1992}, {"title": "The Concave-Convex Procedure (CCCP)", "author": ["A.L. Yuille", "A. Rangarajan"], "venue": "NIPS, 2002, pp. 1033\u20131040.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning with Positive and Unlabeled Examples Using Weighted Logistic Regression", "author": ["W.S. Lee", "B. Liu"], "venue": "ICML, vol. 3, 2003, pp. 448\u2013455.", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "POSITIVE unlabeled learning (PU learning) refers to the task of learning a binary classifier from only positive and unlabeled data [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "\u2022 Retrieval [2], where the goal is to find samples in an unlabeled data set similar to samples provided by a user.", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "\u2022 Inlier-based outlier detection [3], where the goal is to detect outliers from an unlabeled data set based on inlier samples.", "startOffset": 33, "endOffset": 36}, {"referenceID": 3, "context": "\u2022 One-vs-rest classification [4], which is typical of problems where the negative class is too diverse and it is therefore difficult to collect and label enough negative samples.", "startOffset": 29, "endOffset": 32}, {"referenceID": 4, "context": "\u2022 Open set recognition [5], where testing classes are unknown at training time and therefore the exploitation of unlabeled data may help to learn more robust concepts.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "The recent works in [6] and [7] formulate PU learning as optimization problems under the framework of statistical learning theory [8].", "startOffset": 20, "endOffset": 23}, {"referenceID": 6, "context": "The recent works in [6] and [7] formulate PU learning as optimization problems under the framework of statistical learning theory [8].", "startOffset": 28, "endOffset": 31}, {"referenceID": 7, "context": "The recent works in [6] and [7] formulate PU learning as optimization problems under the framework of statistical learning theory [8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 8, "context": "PU learning is very well known in the machine learning community, since it is used in a variety of tasks ranging frommatrix completion [9], multi-view learning [10], as well as semi-supervised learning [11].", "startOffset": 135, "endOffset": 138}, {"referenceID": 9, "context": "PU learning is very well known in the machine learning community, since it is used in a variety of tasks ranging frommatrix completion [9], multi-view learning [10], as well as semi-supervised learning [11].", "startOffset": 160, "endOffset": 164}, {"referenceID": 10, "context": "PU learning is very well known in the machine learning community, since it is used in a variety of tasks ranging frommatrix completion [9], multi-view learning [10], as well as semi-supervised learning [11].", "startOffset": 202, "endOffset": 206}, {"referenceID": 11, "context": "It is also applied in data mining to classify data streams [12] or time series [13] and to detect events, like co-occurrences, in graphs [14].", "startOffset": 59, "endOffset": 63}, {"referenceID": 12, "context": "It is also applied in data mining to classify data streams [12] or time series [13] and to detect events, like co-occurrences, in graphs [14].", "startOffset": 79, "endOffset": 83}, {"referenceID": 13, "context": "It is also applied in data mining to classify data streams [12] or time series [13] and to detect events, like co-occurrences, in graphs [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "The first category consists of two-stage approaches [15], [16], [17], [18], which firstly extract a set of reliable negative samples from the unlabeled data and secondly use them, together with the available positive data, to train a binary classifier.", "startOffset": 52, "endOffset": 56}, {"referenceID": 15, "context": "The first category consists of two-stage approaches [15], [16], [17], [18], which firstly extract a set of reliable negative samples from the unlabeled data and secondly use them, together with the available positive data, to train a binary classifier.", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "The first category consists of two-stage approaches [15], [16], [17], [18], which firstly extract a set of reliable negative samples from the unlabeled data and secondly use them, together with the available positive data, to train a binary classifier.", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "The first category consists of two-stage approaches [15], [16], [17], [18], which firstly extract a set of reliable negative samples from the unlabeled data and secondly use them, together with the available positive data, to train a binary classifier.", "startOffset": 70, "endOffset": 74}, {"referenceID": 0, "context": "Positive and negative data are then used to train different classifiers based on SVM [1], [19], neural networks [20] or kernel density estimators [21].", "startOffset": 85, "endOffset": 88}, {"referenceID": 18, "context": "Positive and negative data are then used to train different classifiers based on SVM [1], [19], neural networks [20] or kernel density estimators [21].", "startOffset": 90, "endOffset": 94}, {"referenceID": 19, "context": "Positive and negative data are then used to train different classifiers based on SVM [1], [19], neural networks [20] or kernel density estimators [21].", "startOffset": 112, "endOffset": 116}, {"referenceID": 20, "context": "Positive and negative data are then used to train different classifiers based on SVM [1], [19], neural networks [20] or kernel density estimators [21].", "startOffset": 146, "endOffset": 150}, {"referenceID": 21, "context": "OCC is applied to many real-world problems, involving for example anomaly/novelty detection (see [22] for a recent survey and definition of anomaly detection and see [23], [24] for reviews about novelty detection).", "startOffset": 97, "endOffset": 101}, {"referenceID": 22, "context": "OCC is applied to many real-world problems, involving for example anomaly/novelty detection (see [22] for a recent survey and definition of anomaly detection and see [23], [24] for reviews about novelty detection).", "startOffset": 166, "endOffset": 170}, {"referenceID": 23, "context": "OCC is applied to many real-world problems, involving for example anomaly/novelty detection (see [22] for a recent survey and definition of anomaly detection and see [23], [24] for reviews about novelty detection).", "startOffset": 172, "endOffset": 176}, {"referenceID": 24, "context": "Other possible applications of OCC range from author verification in text documents [25], document retrieval [2], as well as collaborative filtering in social networks [26].", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "Other possible applications of OCC range from author verification in text documents [25], document retrieval [2], as well as collaborative filtering in social networks [26].", "startOffset": 109, "endOffset": 112}, {"referenceID": 25, "context": "Other possible applications of OCC range from author verification in text documents [25], document retrieval [2], as well as collaborative filtering in social networks [26].", "startOffset": 168, "endOffset": 172}, {"referenceID": 26, "context": "Authors is [27], [28] are among the first to develop OCC algorithms.", "startOffset": 11, "endOffset": 15}, {"referenceID": 27, "context": "Authors is [27], [28] are among the first to develop OCC algorithms.", "startOffset": 17, "endOffset": 21}, {"referenceID": 26, "context": "In particular, the study in [27] proposes a classifier which finds the hyperplane separating data from the origin with the maximum margin, while authors in [28] propose a classifier which finds the mimimum radius hypersphere enclosing data.", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "In particular, the study in [27] proposes a classifier which finds the hyperplane separating data from the origin with the maximum margin, while authors in [28] propose a classifier which finds the mimimum radius hypersphere enclosing data.", "startOffset": 156, "endOffset": 160}, {"referenceID": 26, "context": "Despite the difference between these two approaches, it is proved in [27] that, for specific choices of kernel function (namely, in the case of translationinvariant kernels, like the Gaussian kernel), the obtained solutions are the same.", "startOffset": 69, "endOffset": 73}, {"referenceID": 29, "context": "In fact, authors in [30] modify the model of [27] by incorporating a small training set of anomalies and using the centroid of this set, instead of the origin, as the reference point to find the hyperplane.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "In fact, authors in [30] modify the model of [27] by incorporating a small training set of anomalies and using the centroid of this set, instead of the origin, as the reference point to find the hyperplane.", "startOffset": 45, "endOffset": 49}, {"referenceID": 30, "context": "Authors in [31] propose a strategy to automatically select the hyperparameters defined in [28] to increase the usability of the framework.", "startOffset": 11, "endOffset": 15}, {"referenceID": 27, "context": "Authors in [31] propose a strategy to automatically select the hyperparameters defined in [28] to increase the usability of the framework.", "startOffset": 90, "endOffset": 94}, {"referenceID": 26, "context": "Rather than repelling samples from a specific point, as in [27], [30], authors in [32] propose a strategy that attract samples towards the centroid.", "startOffset": 59, "endOffset": 63}, {"referenceID": 29, "context": "Rather than repelling samples from a specific point, as in [27], [30], authors in [32] propose a strategy that attract samples towards the centroid.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "Rather than repelling samples from a specific point, as in [27], [30], authors in [32] propose a strategy that attract samples towards the centroid.", "startOffset": 82, "endOffset": 86}, {"referenceID": 32, "context": "Authors in [33] propose a similar strategy based on linear programming, where data are represented in a similarity/dissimilarity space.", "startOffset": 11, "endOffset": 15}, {"referenceID": 33, "context": "To mention a few, authors in [34] propose a neural network-based approach, where the goal is to learn the identity function.", "startOffset": 29, "endOffset": 33}, {"referenceID": 34, "context": "Authors in [35] propose a one-class nearest neighbour, where a test sample is accepted as a member of the target class only when the distance from its neighbours is comparable to their local density.", "startOffset": 11, "endOffset": 15}, {"referenceID": 28, "context": "More precisely, the term OCC was coined in 1996 [29].", "startOffset": 48, "endOffset": 52}, {"referenceID": 35, "context": "Solutions to improve classification perfomance are obtained by applying classical strategies, like ensemble methods [36], bagging [37] or boosting [38] strategies.", "startOffset": 116, "endOffset": 120}, {"referenceID": 36, "context": "Solutions to improve classification perfomance are obtained by applying classical strategies, like ensemble methods [36], bagging [37] or boosting [38] strategies.", "startOffset": 130, "endOffset": 134}, {"referenceID": 37, "context": "Solutions to improve classification perfomance are obtained by applying classical strategies, like ensemble methods [36], bagging [37] or boosting [38] strategies.", "startOffset": 147, "endOffset": 151}, {"referenceID": 38, "context": "Authors in [39] argue that existing one-class classifiers fail when dealing with mixture distributions.", "startOffset": 11, "endOffset": 15}, {"referenceID": 20, "context": "As discussed in [21], standard OCC algorithms are not designed to use unlabeled data and therefore make the implicit assumption that they are uniformly distributed on the support of nominal distribution, which does not hold in general.", "startOffset": 16, "endOffset": 20}, {"referenceID": 39, "context": "The recent work in [40] proves that, under some simple conditions, large amount of unlabeled data can boost performance of OCC even in comparison with completely supervised approaches.", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "Furthermore, the exploitation of unlabeled data allows building OCC classifiers in the context of open set recognition [5], where it is essential to learn robust concepts/functions.", "startOffset": 119, "endOffset": 122}, {"referenceID": 40, "context": "PU learning can be regarded as a generalization of OCC [41], in the sense that it can deal and manage unlabeled data coming from more general distributions than the uniform one.", "startOffset": 55, "endOffset": 59}, {"referenceID": 41, "context": "2 Comparison with semi-supervised learning The idea of exploiting unlabeled data in semi-supervised learning is originally proposed by [42].", "startOffset": 135, "endOffset": 139}, {"referenceID": 42, "context": "Authors in [43] are among the first to analyze this aspect from a generative perspective.", "startOffset": 11, "endOffset": 15}, {"referenceID": 43, "context": "Authors in [44] extend this analysis and consider that data can be correctly described by the more general class of parametric models.", "startOffset": 11, "endOffset": 15}, {"referenceID": 44, "context": "The work in [45] provides a reference taxonomy of semi-supervised learning algorithms.", "startOffset": 12, "endOffset": 16}, {"referenceID": 45, "context": "In particular, we distinguish among generative approaches, like the one in [46], which exploit the unlabeled data to better estimate the class-conditional densities and infer/predict the unknown labels based on the learnt model,", "startOffset": 75, "endOffset": 79}, {"referenceID": 46, "context": "low-density separation methods, like the work in [47], which look for decision boundaries that correctly classify labeled data and are placed in regions with few unlabeled samples (the so called low-density regions), graph-based methods, like in [48], which exploit unlabeled data to build a similarity graph and then propagate labels based on the smoothness assumption, methods based on dimensionality reduction, like in [49], which use the unlabeled samples for representation learning and then perform classification on the learnt feature representation, and disagreement-based methods, discussed in [50], which exploit the disagreement among multiple base learners to learn more robust ensemble classifiers.", "startOffset": 49, "endOffset": 53}, {"referenceID": 47, "context": "low-density separation methods, like the work in [47], which look for decision boundaries that correctly classify labeled data and are placed in regions with few unlabeled samples (the so called low-density regions), graph-based methods, like in [48], which exploit unlabeled data to build a similarity graph and then propagate labels based on the smoothness assumption, methods based on dimensionality reduction, like in [49], which use the unlabeled samples for representation learning and then perform classification on the learnt feature representation, and disagreement-based methods, discussed in [50], which exploit the disagreement among multiple base learners to learn more robust ensemble classifiers.", "startOffset": 246, "endOffset": 250}, {"referenceID": 48, "context": "low-density separation methods, like the work in [47], which look for decision boundaries that correctly classify labeled data and are placed in regions with few unlabeled samples (the so called low-density regions), graph-based methods, like in [48], which exploit unlabeled data to build a similarity graph and then propagate labels based on the smoothness assumption, methods based on dimensionality reduction, like in [49], which use the unlabeled samples for representation learning and then perform classification on the learnt feature representation, and disagreement-based methods, discussed in [50], which exploit the disagreement among multiple base learners to learn more robust ensemble classifiers.", "startOffset": 422, "endOffset": 426}, {"referenceID": 49, "context": "low-density separation methods, like the work in [47], which look for decision boundaries that correctly classify labeled data and are placed in regions with few unlabeled samples (the so called low-density regions), graph-based methods, like in [48], which exploit unlabeled data to build a similarity graph and then propagate labels based on the smoothness assumption, methods based on dimensionality reduction, like in [49], which use the unlabeled samples for representation learning and then perform classification on the learnt feature representation, and disagreement-based methods, discussed in [50], which exploit the disagreement among multiple base learners to learn more robust ensemble classifiers.", "startOffset": 603, "endOffset": 607}, {"referenceID": 50, "context": "For example, the work in [51] proposes a framework to solve a mixed-integer programming problem, which runs multiple times the SVM algorithm.", "startOffset": 25, "endOffset": 29}, {"referenceID": 51, "context": "State-of-the-art implementations of SVM (see for example LIBSVM [52]) are based mainly on decomposition methods [53], like our proposed approach.", "startOffset": 64, "endOffset": 68}, {"referenceID": 52, "context": "State-of-the-art implementations of SVM (see for example LIBSVM [52]) are based mainly on decomposition methods [53], like our proposed approach.", "startOffset": 112, "endOffset": 116}, {"referenceID": 53, "context": "Other semi-supervised approaches look for approximations of the fitness function involved in the optimization problem, like [54], [55].", "startOffset": 124, "endOffset": 128}, {"referenceID": 54, "context": "Other semi-supervised approaches look for approximations of the fitness function involved in the optimization problem, like [54], [55].", "startOffset": 130, "endOffset": 134}, {"referenceID": 55, "context": "To the best of our knowledge, only few works like the study in [56] propose semi-supervised methods capable to handle this situation.", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "On the contrary, recent works in [11], [40], show that it is possible to apply PU learning algorithms to solve semi-supervised learning tasks, even in the case of open set environment.", "startOffset": 33, "endOffset": 37}, {"referenceID": 39, "context": "On the contrary, recent works in [11], [40], show that it is possible to apply PU learning algorithms to solve semi-supervised learning tasks, even in the case of open set environment.", "startOffset": 39, "endOffset": 43}, {"referenceID": 56, "context": "Inspired by the seminal work in [57] and by the first studies about OCC [27], [28], authors in [58], and later with an extended version in [59], are the first to define and theoretically analyze the problem of PU learning.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "Inspired by the seminal work in [57] and by the first studies about OCC [27], [28], authors in [58], and later with an extended version in [59], are the first to define and theoretically analyze the problem of PU learning.", "startOffset": 72, "endOffset": 76}, {"referenceID": 27, "context": "Inspired by the seminal work in [57] and by the first studies about OCC [27], [28], authors in [58], and later with an extended version in [59], are the first to define and theoretically analyze the problem of PU learning.", "startOffset": 78, "endOffset": 82}, {"referenceID": 57, "context": "Inspired by the seminal work in [57] and by the first studies about OCC [27], [28], authors in [58], and later with an extended version in [59], are the first to define and theoretically analyze the problem of PU learning.", "startOffset": 95, "endOffset": 99}, {"referenceID": 58, "context": "Inspired by the seminal work in [57] and by the first studies about OCC [27], [28], authors in [58], and later with an extended version in [59], are the first to define and theoretically analyze the problem of PU learning.", "startOffset": 139, "endOffset": 143}, {"referenceID": 56, "context": "In particular, the authors propose a framework based on the statistical query model [57] to have theoretical guarantees about classification performance and to derive algorithms based on decision trees.", "startOffset": 84, "endOffset": 88}, {"referenceID": 59, "context": "Instead of considering binary features, authors in [60] propose a Naive-Bayes classifier for categorical features in noisy environments.", "startOffset": 51, "endOffset": 55}, {"referenceID": 60, "context": "Their work is subject to the attribute independence assumption, which is useful for estimating class-conditional densities in high-dimensional spaces, but rather limiting when compared to discriminative approaches, which directly focus on classification and avoid performing density estimation [61].", "startOffset": 294, "endOffset": 298}, {"referenceID": 20, "context": "In order to understand the criticality of this issue, consider the theoretical result of consistency presented in [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 5, "context": "Recently, authors in [6], [7] propose frameworks based on the statistical learning theory [8].", "startOffset": 21, "endOffset": 24}, {"referenceID": 6, "context": "Recently, authors in [6], [7] propose frameworks based on the statistical learning theory [8].", "startOffset": 26, "endOffset": 29}, {"referenceID": 7, "context": "Recently, authors in [6], [7] propose frameworks based on the statistical learning theory [8].", "startOffset": 90, "endOffset": 93}, {"referenceID": 7, "context": "According to statistical learning theory [8], the", "startOffset": 41, "endOffset": 44}, {"referenceID": 6, "context": "where l\u0303(f(x), 1) = l(f(x), 1) \u2212 l(f(x),\u22121) is called the composite loss [7].", "startOffset": 73, "endOffset": 76}, {"referenceID": 61, "context": "For an overview of RKHS and their properties, see the work in [62] where \u03b1i \u2208 R for all i.", "startOffset": 62, "endOffset": 66}, {"referenceID": 62, "context": "Just to mention a few, authors in [63] have provided a generalized version of the classical representer theorem for classification and regression tasks, while authors in [48] have derived the representer theorem for semi-supervised learning.", "startOffset": 34, "endOffset": 38}, {"referenceID": 47, "context": "Just to mention a few, authors in [63] have provided a generalized version of the classical representer theorem for classification and regression tasks, while authors in [48] have derived the representer theorem for semi-supervised learning.", "startOffset": 170, "endOffset": 174}, {"referenceID": 63, "context": "More recently, the study in [64] has proposed a unified view of existing representer theorems, identifying the relations between these theorems and certain classes of regularization penalties.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "Authors in [7] have analysed the properties of loss functions for the PU learning problem and shown that a necessary condition for convexity is that the composite loss function in (3) is affine.", "startOffset": 11, "endOffset": 14}, {"referenceID": 6, "context": "In particular, better generalization performance can be achieved by using the double Hinge loss [7].", "startOffset": 96, "endOffset": 99}, {"referenceID": 5, "context": "Even, the comparison with non-convex loss functions [6], [7] suggests to use the double Hinge loss for the PU learning problem.", "startOffset": 52, "endOffset": 55}, {"referenceID": 6, "context": "Even, the comparison with non-convex loss functions [6], [7] suggests to use the double Hinge loss for the PU learning problem.", "startOffset": 57, "endOffset": 60}, {"referenceID": 64, "context": "In case of (6) they are both necessary and sufficient conditions, since the objective is convex and the constraints are affine functions [66].", "startOffset": 137, "endOffset": 141}, {"referenceID": 65, "context": "Labeled samples are used to train a oneclass SVM [67], that is in turn used to rank the unlabeled samples according to their value of estimated target function.", "startOffset": 49, "endOffset": 53}, {"referenceID": 6, "context": "The proposed algorithm is compared against [7] and [6].", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "The proposed algorithm is compared against [7] and [6].", "startOffset": 51, "endOffset": 54}, {"referenceID": 6, "context": "In particular, the method in [7] solves problem (6) using the MATLAB built-in function quadprog, combined with the second-order primal-dual interior point algorithm [68], while the method in [6] solves problem (4) with the ramp loss function using the quadprog function combined with the concave-convex procedure [69].", "startOffset": 29, "endOffset": 32}, {"referenceID": 66, "context": "In particular, the method in [7] solves problem (6) using the MATLAB built-in function quadprog, combined with the second-order primal-dual interior point algorithm [68], while the method in [6] solves problem (4) with the ramp loss function using the quadprog function combined with the concave-convex procedure [69].", "startOffset": 165, "endOffset": 169}, {"referenceID": 5, "context": "In particular, the method in [7] solves problem (6) using the MATLAB built-in function quadprog, combined with the second-order primal-dual interior point algorithm [68], while the method in [6] solves problem (4) with the ramp loss function using the quadprog function combined with the concave-convex procedure [69].", "startOffset": 191, "endOffset": 194}, {"referenceID": 67, "context": "In particular, the method in [7] solves problem (6) using the MATLAB built-in function quadprog, combined with the second-order primal-dual interior point algorithm [68], while the method in [6] solves problem (4) with the ramp loss function using the quadprog function combined with the concave-convex procedure [69].", "startOffset": 313, "endOffset": 317}, {"referenceID": 6, "context": "Since both USMO and [7] solve the same optimization problem, we firstly investigate whether the two algorithms achieve same solutions.", "startOffset": 20, "endOffset": 23}, {"referenceID": 51, "context": "Except for the initialization, which uses LIBSVM [52] to run the one-class classifier.", "startOffset": 49, "endOffset": 53}, {"referenceID": 20, "context": "Methods like [21], [70], [71] can be used to estimate it.", "startOffset": 13, "endOffset": 17}, {"referenceID": 68, "context": "Methods like [21], [70], [71] can be used to estimate it.", "startOffset": 19, "endOffset": 23}, {"referenceID": 6, "context": "This fact is a direct consequence of the theory proved in Section 5, since USMO is guaranteed to converge to the same value of objective function obtained in [7].", "startOffset": 158, "endOffset": 161}, {"referenceID": 6, "context": "In all cases USMO is therefore able to achieve the same solution of [7] in a more efficient way.", "startOffset": 68, "endOffset": 71}, {"referenceID": 5, "context": "Secondly, we investigate the quality of solutions obtained by USMO and [6].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "On average, USMO is performing better than [6].", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "This is mainly due to the fact that the optimization problem solved in [6] is not convex, meaning that multiple local solutions are available.", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "Furthermore, it is important to mention that USMO has a linear storage complexity instead of quadratic, as for [6].", "startOffset": 111, "endOffset": 114}, {"referenceID": 6, "context": "It is evident that the advantage of USMO over [7] and [6], in terms of both speed and storage, increases with the number of unlabeled samples.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "It is evident that the advantage of USMO over [7] and [6], in terms of both speed and storage, increases with the number of unlabeled samples.", "startOffset": 54, "endOffset": 57}, {"referenceID": 6, "context": "When the number of unlabeled samples exceeds 10000 units, the method in [7] and [6] becomes computationally intractable (in terms of storage) and only USMO can still provide a solution.", "startOffset": 72, "endOffset": 75}, {"referenceID": 5, "context": "When the number of unlabeled samples exceeds 10000 units, the method in [7] and [6] becomes computationally intractable (in terms of storage) and only USMO can still provide a solution.", "startOffset": 80, "endOffset": 83}, {"referenceID": 62, "context": "APPENDIX A PROOF OF THE REPRESENTER THEOREM Similarly to [63], define \u03a6 as the set, whose elements are the representers of the training dataset D = Dp \u222a Dn, namely \u03a8 = { \u03c6xi \u2208 Hk|i : xi \u2208 D }", "startOffset": 57, "endOffset": 61}], "year": 2016, "abstractText": "Positive unlabeled (PU) learning is useful in various practical situations, where there is a need to learn a classifier for a class of interest from an unlabeled data set, which may contain anomalies as well as samples from unknown classes. The learning task can be formulated as an optimization problem under the framework of statistical learning theory. Recent studies have theoretically analyzed its properties and generalization performance, nevertheless, little effort has been made to consider the problem of scalability, especially when large sets of unlabeled data are available. In this work we propose a novel scalable PU learning algorithm that is theoretically proven to provide the optimal solution, while showing superior computational and memory performance. Experimental evaluation confirms the theoretical evidence and shows that the proposed method can be successfully applied to a large variety of real-world problems involving PU learning.", "creator": "LaTeX with hyperref package"}}}