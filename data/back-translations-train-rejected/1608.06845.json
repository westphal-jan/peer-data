{"id": "1608.06845", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "Effect of Incomplete Meta-dataset on Average Ranking Method", "abstract": "One of the simplest metalearning methods is the average ranking method. This method uses metadata in the form of test results of a given set of algorithms on given set of datasets and calculates an average rank for each algorithm. The ranks are used to construct the average ranking. We investigate the problem of how the process of generating the average ranking is affected by incomplete metadata including fewer test results. This issue is relevant, because if we could show that incomplete metadata does not affect the final results much, we could explore it in future design. We could simply conduct fewer tests and save thus computation time. In this paper we describe an upgraded average ranking method that is capable of dealing with incomplete metadata. Our results show that the proposed method is relatively robust to omission in test results in the meta datasets.", "histories": [["v1", "Wed, 24 Aug 2016 14:44:33 GMT  (33kb)", "http://arxiv.org/abs/1608.06845v1", "8 pages, two figures and 6 tables"]], "COMMENTS": "8 pages, two figures and 6 tables", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["salisu mamman abdulrahman", "pavel brazdil"], "accepted": false, "id": "1608.06845"}, "pdf": {"name": "1608.06845.pdf", "metadata": {"source": "CRF", "title": "Effect of Incomplete Meta-dataset on Average Ranking Method", "authors": ["Salisu Mamman Abdulrahman", "Abdulrahman Brazdil"], "emails": ["salisu.abdul@gmail.com", "pbrazdil@inescporto.pt"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.06 845v 1 [cs.A I] 2 4Keywords: average ranking, totality of rankings, incomplete metadata"}, {"heading": "1. Introduction", "text": "The task of recommending the most suitable algorithms for a given task (Brazdil et al. (2008); Smith-Miles (2008))) has drawn a lot of attention to the problem of the robustness of a particular version of an average ranking procedure that uses incomplete rankings as input, which arise when we have incomplete test results in the metadata set. We have examined how much performance decreases under such circumstances. The rest of this paper is organized as follows. In the next section, we present an overview of existing work in related areas. Section 3 gives details of the proposed aggregation method for incomplete metadata, the experimental results and future workshops."}, {"heading": "2. Related Work", "text": "In this paper, we deal with a specific case of the algorithm selection problem, based on the selection of classification algorithms. Various researchers have addressed this problem over the last 25 years. An approach to algorithm selection / recommendation is based on metalearning. The simplest method only uses performance results on different sets of data in the form of rankings, which are then aggregated to obtain a single aggregated ranking. Therefore, the aggregated ranking can be used as a simple model that the user can follow to test the top candidates in order to identify the algorithm to be used. This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008). A more advanced approach, often considered a classical metalworking approach, uses, in addition to the results, a number of measures that characterize data sets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-2008) (Miles)."}, {"heading": "3. Effect of Incomplete Meta-dataset on Average Ranking Method", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to hide ourselves, and that we are able, that we will be able, that we will be able, that we will be able, that we will be able."}, {"heading": "3.1. Aggregation Method for Incomplete Rankings (AR-MTA)", "text": "Before describing the method for calculating the average ranking, which can process incomplete rankings, we will consider a motivating example (see Table 4), which illustrates why we cannot simply use the usual average ranking method (Lin (2010)), which is commonly used in comparative studies in the ranking machine learning. Let's compare the rankings R1 (Table 4a) and R2 (Table 4b). We note that algorithm a2 ranks in the ranking R1, but rank 1 in the ranking R2 has rank 1. If we use the usual method, the final ranking of a2 would be the mean of the two ranks, i.e. (4 + 1) / 2 = 2.5. This does not seem to be intuitively correct, since the information in the ranking R2 is incomplete. If we only perform one test and get ranking R2 as a result, this information is obviously inferior in order to have performed more tests to perform the ranking tests, as the tests should be performed by the 1, to suggest that the individual observations should be taken into account."}, {"heading": "3.2. Results on the Effects of Omissions in the Meta-Dataset", "text": "This year, it has reached the point where it is only half as much as it is half."}, {"heading": "Acknowledgements", "text": "This work is supported by the TETFund 2012 Intervention for Kano University of Science and Technology, Wudil, Kano State, Nigeria for PhD Overseas Training of the Nigerian Federal Government. The authors also acknowledge the support of the project NanoSTIMA: Macro-to-Nano Human Sensing: Towards Integrated Multimodal Health Monitoring and Analytics / NORTE01-0145-FEDER-000016, which is financed by the Regional Operational Programme Northern Portugal (NORTE 2020) under the Partnership Agreement PORTUGAL 2020 and by the European Regional Development Fund (ERDF)."}], "references": [{"title": "Metalearning: Applications to data mining", "author": ["Pavel Brazdil", "Christophe Giraud-Carrier", "Carlos Soares", "Ricardo Vilalta"], "venue": "Springer Science & Business Media,", "citeRegEx": "Brazdil et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brazdil et al\\.", "year": 2008}, {"title": "The WEKA Data Mining Software: An Update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Rank aggregation methods", "author": ["S. Lin"], "venue": "WIREs Computational Statistics,", "citeRegEx": "Lin.,? \\Q2010\\E", "shortCiteRegEx": "Lin.", "year": 2010}, {"title": "Tell me who can learn you and I can tell you who you are: Landmarking various learning algorithms", "author": ["B. Pfahringer", "H. Bensusan", "Ch. Giraud-Carrier"], "venue": "In Proc. of the 17th Int. Conf. on Machine Learning,", "citeRegEx": "Pfahringer et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Pfahringer et al\\.", "year": 2000}, {"title": "RankAggreg, an R package for weighted rank aggregation", "author": ["V. Pihur", "S. Datta", "S. Susmita Datta"], "venue": "Department of Bioinformatics and Biostatistics,", "citeRegEx": "Pihur et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pihur et al\\.", "year": 2014}, {"title": "The Algorithm Selection Problem", "author": ["J.R. Rice"], "venue": "Advances in Computers,", "citeRegEx": "Rice.,? \\Q1976\\E", "shortCiteRegEx": "Rice.", "year": 1976}, {"title": "Cross-disciplinary Perspectives on Meta-Learning for Algorithm Selection", "author": ["K.A. Smith-Miles"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Smith.Miles.,? \\Q2008\\E", "shortCiteRegEx": "Smith.Miles.", "year": 2008}, {"title": "Fast Algorithm Selection using Learning Curves", "author": ["J.N. van Rijn", "S.M. Abdulrahman", "P. Brazdil", "J. Vanschoren"], "venue": "In Advances in Intelligent Data Analysis XIV. Springer,", "citeRegEx": "Rijn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rijn et al\\.", "year": 2015}, {"title": "OpenML: networked science in machine learning", "author": ["J. Vanschoren", "J.N. van Rijn", "B. Bischl", "L. Torgo"], "venue": "ACM SIGKDD Explorations Newsletter,", "citeRegEx": "Vanschoren et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vanschoren et al\\.", "year": 2014}, {"title": "Data Mining: Practical machine learning tools and techniques", "author": ["Ian H. Witten", "Eibe Frank"], "venue": null, "citeRegEx": "Witten and Frank.,? \\Q2005\\E", "shortCiteRegEx": "Witten and Frank.", "year": 2005}], "referenceMentions": [{"referenceID": 4, "context": "The algorithm selection problem, originally described by Rice (1976), has attracted a great deal of attention, as it endeavours to select and apply the best or near best algorithm(s) for a given task (Brazdil et al.", "startOffset": 57, "endOffset": 69}, {"referenceID": 0, "context": "The algorithm selection problem, originally described by Rice (1976), has attracted a great deal of attention, as it endeavours to select and apply the best or near best algorithm(s) for a given task (Brazdil et al. (2008); Smith-Miles (2008)).", "startOffset": 201, "endOffset": 223}, {"referenceID": 0, "context": "The algorithm selection problem, originally described by Rice (1976), has attracted a great deal of attention, as it endeavours to select and apply the best or near best algorithm(s) for a given task (Brazdil et al. (2008); Smith-Miles (2008)).", "startOffset": 201, "endOffset": 243}, {"referenceID": 2, "context": "Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010).", "startOffset": 84, "endOffset": 95}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)).", "startOffset": 62, "endOffset": 84}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al.", "startOffset": 62, "endOffset": 285}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)).", "startOffset": 62, "endOffset": 308}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)).", "startOffset": 62, "endOffset": 328}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here. Aggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods.", "startOffset": 62, "endOffset": 854}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here. Aggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods. The last category includes, for instance, Cross Entropy Monte Carlo, CEMC method. Some of the approaches require that the elements that do not appear in list Li of k elements be attributed a concrete rank (e.g. k + 1). This does not seem to be correct. We should not be forced to assume that some information exists, if in fact we have none. We have considered using a package of R RankAggreg (Pihur et al. (2014)), but unfortunately we would have to attribute a concrete rank (e.", "startOffset": 62, "endOffset": 1419}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here. Aggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods. The last category includes, for instance, Cross Entropy Monte Carlo, CEMC method. Some of the approaches require that the elements that do not appear in list Li of k elements be attributed a concrete rank (e.g. k + 1). This does not seem to be correct. We should not be forced to assume that some information exists, if in fact we have none. We have considered using a package of R RankAggreg (Pihur et al. (2014)), but unfortunately we would have to attribute a concrete rank (e.g. k + 1) to all missing elements. We have therefore developed a simple heuristic method based on Borda\u2019s method described in Lin (2010). In our view it serves well our purpose.", "startOffset": 62, "endOffset": 1622}, {"referenceID": 0, "context": "This strategy is sometimes referred to as the Top-N strategy (Brazdil et al. (2008)). A more advanced approach often considered as the classical metalearning approach uses, in addition to performance results, also a set of measures that characterize datasets (Pfahringer et al. (2000); Brazdil et al. (2008); Smith-Miles (2008)). However, this line is not followed up here. Aggregation of rankings involving complete rankings is a simple matter. Normally it just involves calculating the average rank for all items in the ranking (Lin, 2010). Complete rankings are those in which k items are ranked N times and no value in this set is missing. Incomplete rankings arise when only some ranks are known in some of the rankings. These arise quite often in practice. Many diverse methods exist for aggregation of incomplete rankings. According to Lin (2010) methods applicable to long lists can be divided into three categories: Heuristic algorithms, Markov chain methods and stochastic optimization methods. The last category includes, for instance, Cross Entropy Monte Carlo, CEMC method. Some of the approaches require that the elements that do not appear in list Li of k elements be attributed a concrete rank (e.g. k + 1). This does not seem to be correct. We should not be forced to assume that some information exists, if in fact we have none. We have considered using a package of R RankAggreg (Pihur et al. (2014)), but unfortunately we would have to attribute a concrete rank (e.g. k + 1) to all missing elements. We have therefore developed a simple heuristic method based on Borda\u2019s method described in Lin (2010). In our view it serves well our purpose. As Lin (2010) pointed out simple methods often compete quite well with other more complex approaches.", "startOffset": 62, "endOffset": 1677}, {"referenceID": 2, "context": "Aggregation Method for Incomplete Rankings (AR-MTA) Before describing the method for the calculation of the average ranking that can process incomplete rankings, let us consider a motivating example (see Table 4), illustrating why we cannot simply use the usual average ranking method (Lin (2010)), often used in comparative studies in machine learning literature.", "startOffset": 286, "endOffset": 297}, {"referenceID": 6, "context": "Results on the Effects of Omissions in the Meta-Dataset The data used in the experiments involves the meta-dataset constructed from evaluation results retrieved from OpenML (Vanschoren et al. (2014)), a collaborative science platform for machine learning.", "startOffset": 174, "endOffset": 199}, {"referenceID": 1, "context": "This dataset contains the results of 53 parameterized classification algorithms from the Weka workbench (Hall et al. (2009)) on 39 datasets.", "startOffset": 105, "endOffset": 124}, {"referenceID": 1, "context": "This dataset contains the results of 53 parameterized classification algorithms from the Weka workbench (Hall et al. (2009)) on 39 datasets. In the leave-oneout mode, 38 datasets are used to generate the model (e.g. average ranking), while the dataset left out is used for evaluation. Characterization of our Meta-Dataset: We are interested to analyze rankings of classification algorithms on different datasets and in particular how these differ for pairs of datasets, using Spearman correlation coefficient. Fig.1 shows a histogram characterizing the meta-dataset used. The histogram is accompanied by expected value, standard deviation and coefficient of variation calculated as the ratio of standard deviation to the expected value (mean) (Witten and Frank (2005)).", "startOffset": 105, "endOffset": 768}, {"referenceID": 1, "context": "This dataset contains the results of 53 parameterized classification algorithms from the Weka workbench (Hall et al. (2009)) on 39 datasets. In the leave-oneout mode, 38 datasets are used to generate the model (e.g. average ranking), while the dataset left out is used for evaluation. Characterization of our Meta-Dataset: We are interested to analyze rankings of classification algorithms on different datasets and in particular how these differ for pairs of datasets, using Spearman correlation coefficient. Fig.1 shows a histogram characterizing the meta-dataset used. The histogram is accompanied by expected value, standard deviation and coefficient of variation calculated as the ratio of standard deviation to the expected value (mean) (Witten and Frank (2005)). These measures are shown in Table 5. Results: The aim is to investigate how certain omissions in the meta-datasets affect the performance. The results are presented in the form of loss-time curve (van Rijn et al. (2015)) which show how performance loss depends on time.", "startOffset": 105, "endOffset": 990}], "year": 2016, "abstractText": "One of the simplest metalearning methods is the average ranking method. This method uses metadata in the form of test results of a given set of algorithms on given set of datasets and calculates an average rank for each algorithm. The ranks are used to construct the average ranking. We investigate the problem of how the process of generating the average ranking is affected by incomplete metadata including fewer test results. This issue is relevant, because if we could show that incomplete metadata does not affect the final results much, we could explore it in future design. We could simply conduct fewer tests and save thus computation time. In this paper we describe an upgraded average ranking method that is capable of dealing with incomplete metadata. Our results show that the proposed method is relatively robust to omission in test results in the meta datasets.", "creator": "gnuplot 5.0 patchlevel 1"}}}