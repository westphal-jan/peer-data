{"id": "1608.06954", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "State Duration and Interval Modeling in Hidden Semi-Markov Model for Sequential Data Analysis", "abstract": "Sequential data modeling and analysis have become indispensable tools for analyzing sequential data such as time-series data because a larger amount of sensed event data have become available. These methods capture the sequential structure of data of interest, such as input- output relationship and correlation among datasets. However, since most studies in this area are specialized or limited for their respective applications, rigorous requirement analysis on such a model has not been examined in a general point of view. Hence, we particularly examine the structure of sequential data, and extract the necessity of \"state duration\" and \"state duration\" of events for efficient and rich representation of sequential data. Specifically addressing the hidden semi-Markov model (HSMM) that represents such state duration inside a model, we attempt to newly add representational capability of state interval of events onto HSMM. To this end, we propose two extended models; one is interval state hidden semi-Markov model (IS-HSMM) to express the length of state interval with a special state node designated as \"interval state node\". The other is interval length probability hidden semi-Markov model (ILP-HSMM) which repre- sents the length of state interval with a new probabilistic parameter \"interval length probability.\" From exhaustive simulations, we show superior performances of the proposed models in comparison with HSMM. To the best of our knowledge, our proposed models are the first extensions of HMM to support state interval representation as well as state duration representation.", "histories": [["v1", "Wed, 24 Aug 2016 20:11:14 GMT  (1098kb,D)", "http://arxiv.org/abs/1608.06954v1", "30 pages, 20 figures"]], "COMMENTS": "30 pages, 20 figures", "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hiromi narimatsu", "hiroyuki kasai"], "accepted": false, "id": "1608.06954"}, "pdf": {"name": "1608.06954.pdf", "metadata": {"source": "CRF", "title": "State Duration and Interval Modeling in Hidden Semi-Markov Model for Sequential Data Analysis", "authors": ["Hiromi Narimatsu", "Hiroyuki Kasai"], "emails": ["narimatsu@appnet.is.uec.ac.jp)", "kasai@is.uec.ac.jp)"], "sections": [{"heading": "1 Introduction", "text": "The significant advancement of portable devices and portable devices with multifunctional sensors has enabled us to capture all sensor data and capture all observed events and phenomena. This situation motivates us to analyze such recorded data, and many studies have examined a wide range of data acquisition requirement profiles, speech recognition, behavioral analysis, and time series analysis."}, {"heading": "2 Related Work", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "3 Analysis of Sequential Data Modeling", "text": "This section presents an analysis of sequential data modeling and derives the model requirements for sequential data analysis. Subsequently, the satisfaction of the extended models of HMM for the model requirements is examined."}, {"heading": "3.1 Requirement for Model Description", "text": "This section discusses the requirements for the model description using time series data: representative data of sequential data. To this end, we start from the situation in which several different sequences are generated independently of five sensors, as shown in Figure 1. Here, there is an observed event whose value exceeds a predefined threshold, as a \"state\" represented in a block. The continuous duration of each event is represented by the length of the block. Since events are not observed one after the other, namely no observation period, there is between two consecutive states in certain periods, the length of such observation periods is represented as distancing between two blocks. In this example, we also assume that a series of four black-colored blocks, {S1, S2, S4} express an extracted multiple state that forms a certain group. Now, we extract the requirements for the model description. First, by addressing this formation of four blocks, we ensure that these states are observed in a prescribed sequence."}, {"heading": "3.2 Requirement Verification for Extended HMM Models", "text": "This section examines whether HMM and the extended variants of HMM meet these requirements. Table 1 summarizes a comparison between the existing HMM models in terms of the model requirements described above. As the HMM base model describes the sequence of states (R1), all the extended HMM models inherit this capability. FO-HMM specializes in treating the ambiguity of observation symbols and does not contribute to our model requirement. IO-HMM is a hybrid model of generative and discriminatory models for treating the estimated probability frequently used for input sequences and observations. Therefore, it does not meet the theremaining requirements. HSMM models the time required to remain in a single state [7], and its variants, including HMM selftrans and EDM [19, 20, 21, 22] do not meet the same requirements: state order (R1) and state duration (R3) of the HMM model. As a result of the detailed analysis of the HMM model, we also state that both are not complete."}, {"heading": "4 Hidden Semi-Markov Model (HSMM)", "text": "HMM has been researched as a powerful model for speech recognition, because the important parts for modeling are the characteristics of each phoneme and the order of phonemics [28, 29, 21]. The model parameters of HMM consist of the initial probability, the transition probability between states and the emission probability of observation elements from each state. While the model training phase calculates the optimal values of the model parameters, the detection phase for each model calculates the probabilities that an observed sequence will generate when the sequence from each model is observed using the trained model parameters, and finally selects the most suitable model as the recognition result. Among the model parameters, the distinguishing feature of HMM is to model the transition probability of each pair of two states. However, the time remaining in each state is also indispensable for modeling in some useful applications such as online handwriting."}, {"heading": "4.1 Notations", "text": "The HSMM structure is shown in Figure 2, which is compared with that of HMM. In the following, we assume that each unit of time in time t has a corresponding probability of observation. The observation sequence from time t = t1 to t = t2 is called ot1: t2 = ot1,..., ot2. A series of output symbols that are actually observed is expressed as Y = {y1, \u00b7 \u00b7, M}, where M is the number of state types, and the hidden state sequence of time t = T is expressed as S1: T = S1,..., ST, where St represents a state in time t. While HMM allows each state node to emit an observation symbol, HSMM allows to emit several observation symbols."}, {"heading": "4.2 Model Training (Inference)", "text": "This section describes how to build the model of HSMM on the basis of sequences, i.e. how to estimate the number of parameters, each containing the duration in each element. In general, HSMM models the model using the Tree-Welch algorithm [14] in the same way as HMM, using a recursive forward-backward algorithm. Specific calculation algorithm for HSMM is detailed as follows: Calculation of the forward probabilities starts from t = 1 to t = T, and calculation of the backward probabilities from t = T to t = 1. These two-way calculations repeat themselves to the extent of the increase of the previous probability."}, {"heading": "4.3 Recognition using HSMM", "text": "For the detection phase in which the model most likely to generate a particular target observation sequence is found, the probability of generating an observation sequence plays an essential role. To this end, we first assume that each group of sequences is assigned a label in advance, and the detection step is defined to search for the most appropriate label for a particular sequence group by calculating the label of the model with the highest probability of being the detection result. The probability of generating the target observation sequence is calculated using the forward algorithm used in HMM. For each model, it recursively calculates the forward variable and the probability for each state using P (o1: T) = \u2211 M i = 1\u03b1T (i, di), which is the limit probability distribution, where t (j, dj) = [M, i) = 1 \u03b1t \u2212 1 (i, di (j, j) = 1\u03b1t, di: di (t)."}, {"heading": "5 State Interval Modeling in HSMM", "text": "Before going into the details, we describe the state interval representation in a sequence. HSMM's base model ignores the period during which no event is observed, as the occurrence of events and the sequence of events are indispensable for sequence data modeling. However, we also consider this period, i.e. the non-observation period, as it is also indispensable for the sequential data model described in Section 3.1. Therefore, we consider this period in this essay as a state interval and assign a new symbol \"interval symbol\" to this period. Figure 3 illustrates an example of the state interval representation in which \"a\" and \"b\" are symbols that are actually observed in the original sequence, and \"i\" is the interval symbol used to fill the state interval. Section 5.1 examines approaches to state interval modeling using HSMM, and the problems that arise due to the filled sequence in Section 5.2 with the state interval."}, {"heading": "5.1 Two Approaches for State Interval Modeling", "text": "In order to treat state intervals with HSMM, two approaches can be considered as represented in Figure 4. Since each state of the HSMM can represent its duration, which remains in a single state, this new approach describes the length of the state interval by introducing the new state interval, which explicitly indicates the state interval. On the other hand, the other approach presents the state interval as a new probable parameter, as shown in Figure 4 (b).For both approaches, three variations of the state interval can be taken into consideration: the first approach models the state interval with the previous state (a) -1, (b) -1), and the second model with the subsequent state (a) -2, (b) -2).The latter variant models the length of the interval with both preceding and subsequent states (a) -3).In comparison between three variations, the first two models b, (b), -2, (b), (3) and (3) can show the last connections exactly."}, {"heading": "5.2 Problems of State Interval Modeling", "text": "The structure of the first approach is illustrated in Figure 5, where the interval state node is represented as a black node iS. Although this approach treats the state interval in a simple way, it causes major bias in the transition probability if there are many groups of observed interval symbols in a sequence, as in Figure 6. Details of the problem are examined using an example shown in the same figure. Figure 6 (a) shows an example sequence for explaining the interval probability. Each sequence shows the original observation sequence and the state probability of the state model. Figure 6 (b) shows an example sequence filled with state interval nodes of the interval symbol i. The tables to the right of the figure show the transition frequency from one state interval to another state interval, which is represented with the original / complemented state probability calculated in a period of time. \""}, {"heading": "6 Interval State Hidden Semi-Markov Model (IS-HSMM)", "text": "To solve this problem, we propose an advanced model, IS-HSMM, to obtain the transition probability of the original sequence. Figure 7 illustrates a conceptual structure of the ISHSMM. For an easy-to-understand explanation, we use the first half of the three states shown in Figure 7 as an example, if q1 and q2 are original hidden states and q3 is the interval node. While the original HSMM indicates the transition probability in the sequence q1, iq2 and q3, the proposed IS-HSMM mediates the transition probability using two transition probabilities not only from iq2 to q3, but also from the previous q1 to q3, in order to detect the transition of the original train to the MIS-M."}, {"heading": "6.1 Model Training in IS-HSMM", "text": "The difference to the base model of HSMM appears in the calculation of the forward variables and the backward variables in the recursive calculation step. The state transition probability from state i to state j, in which the interval state between state i and state j is inserted, is defined as asa (i, di) (is, id) (j, dj): = P (St + 1: t + dj = j | St + 1: t + id = i s, St \u2212 di + 1: t = i), where the duration of the interval state is called id and the duration of state i and j is called di or dj. The forward variable is expressed as as\u03b1t (j, dj) = planned i (S)\\ {j}, di) \u2212 1 (i, di) a (i, di) (is, id) (j) bj (ot \u2212 dj + 1: t)."}, {"heading": "6.2 Recognition using IS-HSMM", "text": "While the calculation of probability follows the original HSMM if the preceding state is not the interval state node, it differs if the preceding state is the interval state node. The probability of the observation sequence if the preceding state is the interval state node is calculated as P (o1: T) = \u2211 M i = 1 \u03b1T (i, di), where \u03b1t (j, dj) = [c \u2211 i = 1\u03b1t \u2212 1 (i, di) a (i, di) (is, id) (j, dj)] bj, dj (ot \u2212 dj + 1: t). (9) The total algorithm is represented in algorithm 2."}, {"heading": "7 Interval Length Probability HSMM (ILP-HSMM)", "text": "In this section, it is proposed that ILP-HSMM reintroduces the interval length probability into the transition probability in order to handle the state interval between two states. It should be noted that the interval length probability corresponds to the probability distribution of the interval length of two states, to be technically precise, the distinct difference between HSMM and ILP-HSMM is that while the state j begins immediately after the end time of state i in the original HSMM, the state j begins after a period of time, Li, j passes since the end time of state i in the ILP-HSMM. The conceptual model structure of the ILP-HSMM is illustrated in Figure 8. Although the structure of the ILP-HSMM is similar to the structure of the HSMM in Figure 2, the interval length probability is added anew to HSMM as shown in Figure 8, where Li, j represents the time difference between the end state of the start time and the state of the j."}, {"heading": "7.1 Model Training (Inference) in ILP-HSMM", "text": "The slash-structured blocks represent the data sequence of the training data set. First, the probability density distribution of the interval length of Li, j is expressed by the Gaussian distribution p (Li, j) asp (Li, j) = 1 \u221a 2\u03c0\u03c32 e \u2212 (Li, j \u2212 \u00b5) 22\u03c32, (10), whereby \u03c3 and \u00b5 represent the variance or mean value of Li. However, it should be noted that the Gaussian distribution is used as a probability density distribution for simplification. However, other distributions and functions could be adopted for the ILP-HSMM probability without changing other parameters. Accordingly, the quantity of parameters used in ILP-HSMM is defined as the probability density distribution: = A, B, L}, whereby the elements of the parameters of the ILP-HSMM probability are adopted without changing other parameters."}, {"heading": "7.2 Recognition using ILP-HSMM", "text": "The Viterbi algorithm is used to estimate the probability of a model [22]. The pair of the model with the probability of interval length and its probable designation is stored as a candidate for the estimation. The recognition label indicating the estimated result is selected when the model has the maximum probability estimate by calculating for each state in each model. First, we calculate p (Li, j) beforehand. If Li, j is outside the range, the probability density distribution asp (Li, j) = min i, j (Li, j) \u00b7 c is determined, where c = 0 \u2264 c \u2264 1. Then the forward variable is calculated to estimate the maximum probability duration as\u03b1t (j, dj): = max s1: t \u2212 dj P (s1: t \u2212 dj, st \u2212 di \u2212 interval probability, o1: dj \u2212 dj probability)."}, {"heading": "8 Evaluations", "text": "After explaining the specification of the experimental data in Section 8.1, Section 8.2 and Section 8.3, we present the experimental results of comparing execution time and detection performance between HSMM, IS-HSMM and ILP-HSMM. Finally, we evaluate the reproducibility comparison between IS-HSMM and ILP-HSMM in terms of modeling performance in Section 8.4."}, {"heading": "8.1 Experimental Data", "text": "Considering that the sequential data contain state duration and state interval, we use music sound data that is different from instruments. For example, if the same music is played by different instruments, even if the music rhythm is the same, the length of each sound varies for the same note. For example, while the sound power spectrum played by the organ and drum for the same music is nearly the same in Figure 10. The horizontal axis represents time, and the vertical axis represents sound power, i.e. the volume of sound. While the power of each note played by the drum is almost the same during sound, the sound sequence played by the drum decreases rapidly after tapping. We generate the observation sequence from the music sound data described below based on the characteristics of the sound continuous time. Step 1 Set the threshold b1 and b2 to separate the observation symbols into three types of volume."}, {"heading": "8.2 Execution Time Evaluation", "text": "In this section the execution time for training and recognition is shown. For the evaluation we generate 35 sequences, changing the number of training data. Training time results are shown in Figure 12. The x-axis shows the number of training data and the y-axis shows the execution time for training. The top, middle and bottom row represent the results of IS-HSMM, ILP-HSMM and HSMM, respectively the results show that the three diagrams run mostly parallel, which shows that the difference between the results of HSMM and ILP-HSMM and the difference between the results of HSMM are both of a certain degree. Therefore, the training time of IS-HSMM and ILP-HSMM requires additional time, but the amount of additional time does not increase the recognition."}, {"heading": "8.3 Recognition Performance Evaluation", "text": "This section presents the evaluation results of the recognition by comparing IS-HSMM and ILP-HSMM with HSMM. The evaluation yardsticks are the detection accuracy based on F-measurements calculated with the help of (2). (3) However, the number of models collected in PP / AP is very high. (4) The number of designated models in PP is the number of models likely to be calculated (6). (5) The number of models collected in PP / PP and the number of models actually covered in PPP / AP is the number of designated models. (5) The results are shown in Figure 14 and in Figure 15. The x-axis represents precision, and the f-axis represents the evaluation of the y-axis represents the number of points collected in PP."}, {"heading": "8.4 Reproducibility Performance Evaluations between IS-HSMM and ILP-HSMM", "text": "In this context, it must be stated that this is not an isolated case, but a case in which it is an isolated case. In this case, it is an isolated case, in which it is an isolated case. In this case, it is a case in which it is an isolated case. In this case, it is a case in which a person is able to kill himself. In this case, it is a case in which it is a case. In this case, it is a case in which it is a case in which it is a case in which a person is able to kill himself. In this case, it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is not a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which it is a case in which is a case in which it is a case in which it is a case in which it is a case in which it is a case"}, {"heading": "9 Summary and Future Work", "text": "We specifically investigated a hidden Semi-Markov Model (HSMM) to handle such sequential data, and propose two advanced models to handle state intervals in a sequence: IS-HSMM and ILP-HSMM. On the other hand, ILP-HSMM uses a special calculation technique to handle interval data, where if the previous state is an interval state, it simultaneously models the transition from the second preceding state to the current state. On the other hand, ILP-HSMM uses Gaussian distribution as a length parameter and trains with both preceding and subsequent states. By comparing detection performance and elapsed time between IS-HSMM, ILP-HSMM and HSMM, both proposed models give higher performance than HSMM, although they require additional calculation costs. Comparing IS-HSMM and ILP-HSMM with respect to the actual problem that we vary the MILM-delegate data, MILM-MILM often results in higher MILM performance than MILM over time."}], "references": [{"title": "Finding sequential patterns from large sequence data", "author": ["M. Esmaeili", "F. Gabor"], "venue": "International Journal of Computer Science Issues (IJSC), vol. 7, no. 1, pp. 43\u201346, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "A sequential algorithm for training text classifiers", "author": ["D.D. Lewis", "W.A. Gale"], "venue": "Proc. of ACM the 17th Annual International Conference on Research and Development in Information Retrieval (ACM SIGIR), pp. 3\u201312, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "A sequential clustering algorithm with application to gene expression data", "author": ["J. Song", "D.L. Nicolae"], "venue": "Journal of the Korean Statistical Society, vol. 38, no. 2, pp. 175\u2013184, 2009.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2009}, {"title": "Data mining for wearable sensors in health monitoring systems: A review of recent trends and challenges", "author": ["H. Banaee", "M.U. Ahmed", "A. Loutfi"], "venue": "Sensors 2013, vol. 13, no. 12, pp. 17 472\u201317 500, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Sequential bayesian estimation with censored data for multi-sensor systems", "author": ["Y. Zheng", "R. Niu", "P.K. Varshney"], "venue": "IEEE Trans. on Signal Processing, vol. 62, no. 10, pp. 2626\u20132641, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning and recognizing the hierarchical and sequential structure of human activities", "author": ["H.T. Cheng"], "venue": "Ph.D. dissertation, Carnegie Mellon University, Dec. 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Hidden semi-markov models", "author": ["S.Z. Yu"], "venue": "Elsevier Artificial Intelligence, vol. 174, no. 2, pp. 215\u2013243, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Duration and interval hidden markov model for sequential data analysis", "author": ["H. Narimatsu", "H. Kasai"], "venue": "Proc. of International Joint Conference on Neural Networks (IJCNN2015), pp. 3743\u2013 2751, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A dynamic programming approach to continuous speech recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "Proc. of 7th International Congress on Acoustics (ICA) 1971, vol. C13, 1971.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1971}, {"title": "Statistical learning theory", "author": ["V.N. Vapnik"], "venue": "John Wiley and Sons, New York, 1995.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1995}, {"title": "Support vector machines for pattern classification", "author": ["S. Abe"], "venue": "Springer Science and Business Media, July 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Bayesian regression with parametric models for heteroscedasticity", "author": ["W.J. Boscardin", "A. Gelman"], "venue": "Advances in Econometrics, vol. 11A, pp. 87\u2013109, 1996.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1996}, {"title": "The regression analysis of binary sequences", "author": ["D.R. Cox"], "venue": "Journal of the Royal Statistical Society, vol. 20, pp. 215\u2013242, 1958.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1958}, {"title": "Statistical inference for probabilistic functions of finite state markov chains", "author": ["L.E. Baum", "T. Petrie"], "venue": "The Annals of Mathematical Statistics, vol. 37, no. 6, pp. 1554\u20131563, 1966.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1966}, {"title": "An inequality with applications to statistical estimation for probabilistic functions of a markov process and to a model for ecology", "author": ["L.E. Baum", "J.A. Egon"], "venue": "Bulletin of the American Mathematical Society, vol. 73, no. 3, pp. 360\u2013363, 1967.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1967}, {"title": "Hidden morkov models combining discrete symbols and continuous attributes in handwriting recognition", "author": ["H. Xue", "V. Govindaraju"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, vol. 28, no. 3, pp. 458\u2013462, 2006. 29", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Input-output hmm for sequence processing", "author": ["Y. Bengio", "P. Frasconi"], "venue": "IEEE Trans. on Neural Networks, vol. 7, no. 5, 1996.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1996}, {"title": "Non-stationary fuzzy markov chain", "author": ["F. Salzenstein", "C. Collet", "S. Lecam", "M. Hatt"], "venue": "Pattern Recognition Letters, vol. 28, no. 16, pp. 2201\u20132208, 2007.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient forward-backward algorithm for an explicit duration hidden markov model", "author": ["S.Z. Yu", "H. Kobayashi"], "venue": "IEEE Signal Processing Letters, vol. 10, no. 1, pp. 11\u201314, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling duration in a hidden markov model with the exponential family", "author": ["C.D. Mitchell", "L.H. Jamieson"], "venue": "Proc. of 1993 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), vol. 2, pp. 331\u2013334, Apr. 1993.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1993}, {"title": "Modeling state duration in hidden markov models for automatic speech recognition", "author": ["P. Ramesh", "J.G. Wilpon"], "venue": "Proc. of 1992 IEEE International Conference on Acoustic, Speech, and Signal Processing (ICASSP), vol. 1, pp. 381\u2013384, Mar. 1992.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1992}, {"title": "Dynamic bayesian networks: Representation, inference and learning", "author": ["K. Murphy"], "venue": "Ph.D. dissertation, Dept. Computer Science, UC Berkeley, 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Hidden semi-markov models (hsmms)", "author": ["\u2014\u2014"], "venue": "http://www.ai.mit.edu/murphyk, Nov. 2002, (accessed 2016-02-19).", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Application of hidden markov models and hidden semi-markov models to financial time series", "author": ["J. Bulla"], "venue": "Ph.D. dissertation, Georg-August-University of Gottingen, June 2006.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2006}, {"title": "Hidden semi-markov model-based methodology for multi-sensor equipment health diagnosis and prognosis", "author": ["M. Dong", "D. He"], "venue": "European Journal of Operational Research, vol. 178, no. 3, pp. 858\u2013878, April 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Implementation of hidden semi-markov models", "author": ["N.A. Dasu"], "venue": "Ph.D. dissertation, University of Nevada, May 2011.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiple alignment using hidden markov models.", "author": ["S.R. Eddy"], "venue": "Proc. of AAAI Third International Conference on Intelligent Systems for Modecular Biology,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1995}, {"title": "Map speaker adaptation of state duration distribution for speech recognition", "author": ["Y.B. Yoma", "J.S. Sanchez"], "venue": "IEEE Trans. on Speech and Audio Processing, vol. 10, no. 7, pp. 443\u2013450, 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Modeling acoustic transitions in speech by modified hidden markov models with state duration and state duration-dependent observation probabilities", "author": ["Y.K. Park", "C.K. UN", "O.W. Kwon"], "venue": "IEEE Trans. on Speech and Audio Processing, vol. 4, no. 5, pp. 389\u2013392, 1996.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1996}, {"title": "Hidden semi-markov models and efficient forward-backward algorithms", "author": ["H. Kobayashi", "S.Z. Yu"], "venue": "Proc. of 2007 Hawaii and SITA Joint Conference on Information Theory, vol. 174, pp. 41\u201346, May 2007. 30", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "categorized three types of sequential patterns after theoretical investigation for a large amount of data [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 1, "context": "proposed a sequential algorithm using queries to train text classifiers [2].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "proposed a sequential clustering algorithm for gene data [3].", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "More recently, studies using sensor data analysis for human behavior recognition and video sequence understanding have received significant attention because of the significant progress on wearable devices and the widespread of video surveillance systems [4, 5, 6].", "startOffset": 255, "endOffset": 264}, {"referenceID": 4, "context": "More recently, studies using sensor data analysis for human behavior recognition and video sequence understanding have received significant attention because of the significant progress on wearable devices and the widespread of video surveillance systems [4, 5, 6].", "startOffset": 255, "endOffset": 264}, {"referenceID": 5, "context": "More recently, studies using sensor data analysis for human behavior recognition and video sequence understanding have received significant attention because of the significant progress on wearable devices and the widespread of video surveillance systems [4, 5, 6].", "startOffset": 255, "endOffset": 264}, {"referenceID": 6, "context": "Next, (b) we propose two new sequential models by extending hidden semi-Markov model (HSMM) [7] to support both state duration and interval of events efficiently.", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "Preliminary studies of ILP-HSMM was proposed in our earlier work as DI-HMM [8].", "startOffset": 75, "endOffset": 78}, {"referenceID": 8, "context": "For sequential pattern matching and sequential pattern detection, the Dynamic Programming (DP) algorithm [9] provides an optimized search algorithm that calculates the cost of a path in a grid and which thereby finds the least costly path.", "startOffset": 105, "endOffset": 108}, {"referenceID": 9, "context": "For sequential pattern classification, Support Vector Machine (SVM) [10, 11] is a classifier that converts the n-class problem into multiple two-class problems.", "startOffset": 68, "endOffset": 76}, {"referenceID": 10, "context": "For sequential pattern classification, Support Vector Machine (SVM) [10, 11] is a classifier that converts the n-class problem into multiple two-class problems.", "startOffset": 68, "endOffset": 76}, {"referenceID": 11, "context": "As for Regression Model (RM)[12], the logistic regression model [13] is a representative model that is powerful binary classification model when the model parameters are independent each other.", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "As for Regression Model (RM)[12], the logistic regression model [13] is a representative model that is powerful binary classification model when the model parameters are independent each other.", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "Hidden Markov model (HMM), originally proposed in [14, 15], is a statistical tool used for modeling generative sequences.", "startOffset": 50, "endOffset": 58}, {"referenceID": 14, "context": "Hidden Markov model (HMM), originally proposed in [14, 15], is a statistical tool used for modeling generative sequences.", "startOffset": 50, "endOffset": 58}, {"referenceID": 15, "context": "proposed transition-emitting HMMs (TEHMMs) and state-emitting HMMs (SE-HMMs) to treat the discontinuous symbol [16], of which application is an off-line handwriting word recognition.", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "focused on mapping input sequences to the output sequences [17].", "startOffset": 59, "endOffset": 63}, {"referenceID": 17, "context": "dealt with a statistical model based on Fuzzy Markov random chains for image segmentations in the context of stationary and non-stationary data [18].", "startOffset": 144, "endOffset": 148}, {"referenceID": 18, "context": "proposed the explicit-duration hidden Markov model [19, 20, 21, 22].", "startOffset": 51, "endOffset": 67}, {"referenceID": 19, "context": "proposed the explicit-duration hidden Markov model [19, 20, 21, 22].", "startOffset": 51, "endOffset": 67}, {"referenceID": 20, "context": "proposed the explicit-duration hidden Markov model [19, 20, 21, 22].", "startOffset": 51, "endOffset": 67}, {"referenceID": 21, "context": "proposed the explicit-duration hidden Markov model [19, 20, 21, 22].", "startOffset": 51, "endOffset": 67}, {"referenceID": 6, "context": "Addressing the difference of duration in each state, hidden semi-Markov Model (HSMM) is proposed to treat the duration and multiple observations produced in one single state [7, 23].", "startOffset": 174, "endOffset": 181}, {"referenceID": 22, "context": "Addressing the difference of duration in each state, hidden semi-Markov Model (HSMM) is proposed to treat the duration and multiple observations produced in one single state [7, 23].", "startOffset": 174, "endOffset": 181}, {"referenceID": 23, "context": "procedure to the right-censored HSMM for modeling financial time-series data using conditional Gaussian distributions for the HSMM parameters [24].", "startOffset": 142, "endOffset": 146}, {"referenceID": 24, "context": "prioritized the weights for each sensor to treat multiple sensor results, and showed that the proposed model of HSMM gave higher performances than the original HSMM [25].", "startOffset": 165, "endOffset": 169}, {"referenceID": 25, "context": "Recently, Dasu analyzed the technique of HSMM, and described how to implement HSMM for a practical application in detail [26].", "startOffset": 121, "endOffset": 125}, {"referenceID": 26, "context": "HMM (baseline) [27] HMM-selftrans [16] X HSMM [7, 23] X FO-HMM [18] EDM [19, 20] X IO-HMM [17] IS-HSMM and ILP-HSMM (proposal) X X", "startOffset": 15, "endOffset": 19}, {"referenceID": 15, "context": "HMM (baseline) [27] HMM-selftrans [16] X HSMM [7, 23] X FO-HMM [18] EDM [19, 20] X IO-HMM [17] IS-HSMM and ILP-HSMM (proposal) X X", "startOffset": 34, "endOffset": 38}, {"referenceID": 6, "context": "HMM (baseline) [27] HMM-selftrans [16] X HSMM [7, 23] X FO-HMM [18] EDM [19, 20] X IO-HMM [17] IS-HSMM and ILP-HSMM (proposal) X X", "startOffset": 46, "endOffset": 53}, {"referenceID": 22, "context": "HMM (baseline) [27] HMM-selftrans [16] X HSMM [7, 23] X FO-HMM [18] EDM [19, 20] X IO-HMM [17] IS-HSMM and ILP-HSMM (proposal) X X", "startOffset": 46, "endOffset": 53}, {"referenceID": 17, "context": "HMM (baseline) [27] HMM-selftrans [16] X HSMM [7, 23] X FO-HMM [18] EDM [19, 20] X IO-HMM [17] IS-HSMM and ILP-HSMM (proposal) X X", "startOffset": 63, "endOffset": 67}, {"referenceID": 18, "context": "HMM (baseline) [27] HMM-selftrans [16] X HSMM [7, 23] X FO-HMM [18] EDM [19, 20] X IO-HMM [17] IS-HSMM and ILP-HSMM (proposal) X X", "startOffset": 72, "endOffset": 80}, {"referenceID": 19, "context": "HMM (baseline) [27] HMM-selftrans [16] X HSMM [7, 23] X FO-HMM [18] EDM [19, 20] X IO-HMM [17] IS-HSMM and ILP-HSMM (proposal) X X", "startOffset": 72, "endOffset": 80}, {"referenceID": 16, "context": "HMM (baseline) [27] HMM-selftrans [16] X HSMM [7, 23] X FO-HMM [18] EDM [19, 20] X IO-HMM [17] IS-HSMM and ILP-HSMM (proposal) X X", "startOffset": 90, "endOffset": 94}, {"referenceID": 6, "context": "HSMM models the time length to remain in one single state [7], and its variants including HMM-selftrans and EDM [19, 20, 21, 22] satisfy the same requirements: state order (R1) and state duration (R3).", "startOffset": 58, "endOffset": 61}, {"referenceID": 18, "context": "HSMM models the time length to remain in one single state [7], and its variants including HMM-selftrans and EDM [19, 20, 21, 22] satisfy the same requirements: state order (R1) and state duration (R3).", "startOffset": 112, "endOffset": 128}, {"referenceID": 19, "context": "HSMM models the time length to remain in one single state [7], and its variants including HMM-selftrans and EDM [19, 20, 21, 22] satisfy the same requirements: state order (R1) and state duration (R3).", "startOffset": 112, "endOffset": 128}, {"referenceID": 20, "context": "HSMM models the time length to remain in one single state [7], and its variants including HMM-selftrans and EDM [19, 20, 21, 22] satisfy the same requirements: state order (R1) and state duration (R3).", "startOffset": 112, "endOffset": 128}, {"referenceID": 21, "context": "HSMM models the time length to remain in one single state [7], and its variants including HMM-selftrans and EDM [19, 20, 21, 22] satisfy the same requirements: state order (R1) and state duration (R3).", "startOffset": 112, "endOffset": 128}, {"referenceID": 27, "context": "HMM has been studied as a powerful model for speech recognition because the important parts for modeling are the features of each phoneme and the order of phonemics [28, 29, 21].", "startOffset": 165, "endOffset": 177}, {"referenceID": 28, "context": "HMM has been studied as a powerful model for speech recognition because the important parts for modeling are the features of each phoneme and the order of phonemics [28, 29, 21].", "startOffset": 165, "endOffset": 177}, {"referenceID": 20, "context": "HMM has been studied as a powerful model for speech recognition because the important parts for modeling are the features of each phoneme and the order of phonemics [28, 29, 21].", "startOffset": 165, "endOffset": 177}, {"referenceID": 13, "context": "In general, HSMM trains the model using Baum-Welch algorithm [14] as the same way as HMM, where a recursive forward-backward algorithm is used.", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "The forward-backward algorithm is an inference algorithm used for HMM, and the an extended algorithm special for HSMM is also proposed[30].", "startOffset": 134, "endOffset": 138}, {"referenceID": 21, "context": "The Viterbi algorithm is used to estimate the probability of a model [22].", "startOffset": 69, "endOffset": 73}], "year": 2016, "abstractText": "Sequential data modeling and analysis have become indispensable tools for analyzing sequential data such as time-series data because a larger amount of sensed event data have become available. These methods capture the sequential structure of data of interest, such as inputoutput relationship and correlation among datasets. However, since most studies in this area are specialized or limited for their respective applications, rigorous requirement analysis on such a model has not been examined in a general point of view. Hence, we particularly examine the structure of sequential data, and extract the necessity of \u201cstate duration\u201d and \u201cstate duration\u201d of events for efficient and rich representation of sequential data. Specifically addressing the hidden semi-Markov model (HSMM) that represents such state duration inside a model, we attempt to newly add representational capability of state interval of events onto HSMM. To this end, we propose two extended models; one is interval state hidden semi-Markov model (IS-HSMM) to express the length of state interval with a special state node designated as \u201cinterval state node\u201d. The other is interval length probability hidden semi-Markov model (ILP-HSMM) which represents the length of state interval with a new probabilistic parameter \u201cinterval length probability.\u201d From exhaustive simulations, we show superior performances of the proposed models in comparison with HSMM. To the best of our knowledge, our proposed models are the first extensions of HMM to support state interval representation as well as state duration representation.", "creator": "LaTeX with hyperref package"}}}