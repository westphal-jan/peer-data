{"id": "1608.06984", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Aug-2016", "title": "Learning an Optimization Algorithm through Human Design Iterations", "abstract": "There is evidence that humans can be more efficient than existing algorithms at searching for good solutions in high-dimensional and non-convex design or control spaces, potentially due to our prior knowledge and learning capability. This work attempts to quantify the search strategy of human beings to enhance a Bayesian optimization (BO) algorithm for an optimal design and control problem. We consider the sequence of human solutions (called a search trajectory) as generated from BO, and propose to recover the algorithmic parameters of BO through maximum likelihood estimation. The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem. We learn BO parameters from a player who achieved fast improvement in his/her solutions and show that applying the learned parameters to BO achieves better convergence than using a self-adaptive BO. The proposed method is different from inverse reinforcement learning in that it only requires a good search strategy, rather than near-optimal solutions from humans.", "histories": [["v1", "Wed, 24 Aug 2016 23:22:06 GMT  (1914kb,D)", "http://arxiv.org/abs/1608.06984v1", "28 pages, 7 figures, to be submitted to Journal of Mechanical Design"], ["v2", "Fri, 26 Aug 2016 17:40:37 GMT  (1898kb,D)", "http://arxiv.org/abs/1608.06984v2", "28 pages, 7 figures, to be submitted to Journal of Mechanical Design"], ["v3", "Tue, 6 Dec 2016 17:30:21 GMT  (1758kb,D)", "http://arxiv.org/abs/1608.06984v3", "submitted to Journal of Mechanical Design"], ["v4", "Wed, 26 Apr 2017 18:00:36 GMT  (1861kb,D)", "http://arxiv.org/abs/1608.06984v4", "accepted to Journal of Mechanical Design"]], "COMMENTS": "28 pages, 7 figures, to be submitted to Journal of Mechanical Design", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["thurston sexton", "max yi ren"], "accepted": false, "id": "1608.06984"}, "pdf": {"name": "1608.06984.pdf", "metadata": {"source": "CRF", "title": "Learning Human Search Strategies from a Crowdsourcing Game", "authors": ["Thurston Sexton", "Yi Ren"], "emails": ["tbsexton@asu.edu", "yiren@asu.edu"], "sections": [{"heading": "1 Introduction", "text": "In this context, it should be noted that this project is a project which is primarily a project which aims to put the needs of people at the heart of society and which aims to put the needs of people at the heart of society."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Terminologies and notations", "text": "Let us consider an optimization problem as MinxX f (x), where X Rp is the solution space, i.e. the common space for design variables d and control variables \u03b8. A search curve with K iterations can be represented by hK: = < XK, fK >, where XK and fK represent the collection of K samples in X and their objective values. h0: = < X0, f0 > represents an initial exploration set with K0 samples. Human knowledge is represented by algorithmic parameters \u03bb that determine the search behavior: During the search, each new solution xk + 1 is determined by hk and \u03bb by maximizing a measure of quality of the solution: xk + 1 = argmaxx-XQ (x)."}, {"heading": "2.2 Bayesian optimization", "text": "BO is an iterative search algorithm with two main steps in each iteration. Model update: BO first updates a Gaussian process model (GP) [15] to predict objective values based on current observations hk and Gaussian parameters \u03bb. Without taking random noise into account when evaluating the lens, the GP model can be derived as f (x; hk, \u03bb) = b + rTR \u2212 1 (fk \u2212 b), where b = 1T R \u2212 1T R \u2212 11, r is a slit vector with elements ri = exp (\u2212 xi) T (x \u2212 xi)) = b + rTR \u2212 1 (fk \u2212 b), k \u2212 perjective is a symmetric matrix with Rij = exp (\u2212 xj) T \u2212 11, r is a slit vector with elements ri = exp (\u2212 xi)."}, {"heading": "3 Inverse BO", "text": "To do this, we minimize a cost function consisting of the exploration cost of h0, known as LINI, and the cost of the remaining hK space. And LBO: = \u2212 logDp (hK \u2212 h0) = \u2212 logDp (X0) = \u2212 logDp (XK \u2212 h0) is the common probability of the exploration quantity and the solution space. And LBO: = \u2212 logDp (hK \u2212 h0) = \u2212 logDp \u2212 k \u2212 k (xk + 1) is the size of the solution space. And here Log (\u00b7) stands for natural logarithm. The derivative of LINI and LBO is as follows: To calculate a new sample during the exploration phase, we assume that each new sample for i = 1, \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 KBO is a natural logarithm."}, {"heading": "5 Simulation studies", "text": "We use a simulation study to show that (1) IBO can correctly identify the true \u03bb for a given search curve, unless the trajectory resembles a random search, and (2) learning from others (i.e. estimating by IBO an observed effective search curve) can lead to better BO convergence than self-learning (i.e. updating \u03bb by maximizing the probability of observations)."}, {"heading": "5.1 IBO performance", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1.1 Simulation settings and results", "text": "The simulation study is detailed as follows: We apply BO to a 30-dimensional Rosenbrock function, which is limited by X: = [\u2212 2, 2] 30. To initialize BO, we use LHS to draw 10 samples from X. BO ends when the expected improvement for the next iteration is less than 10 \u2212 3. At each iteration, the expected improvement is maximized by a multi-start gradient descend algorithm [18] with 100 LHS initial rates. A number of BO parameters, B = 0.01I, 0.1I, 1.0I, 10.0I, are used to perform the search, where I am the identity matrix. For each of the four settings, 30 independent studies are reordered. For each BO setting, the candidate estimator K = 5,..., 20, we solve equivalents (4) using a grid search with G\u03b1BO: = 0.01, 0.1, 10.0, and GJ \u00b7 2: all cases are equal and 0.0K = all."}, {"heading": "5.1.2 Analysis of the results", "text": "To understand the results of Figure 3, we will first examine the properties of the costs l = > BO (1) for the EQ (3) for the EQ (3) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4) for the EQ (4)."}, {"heading": "5.2 Learning from others vs. self-adaptation", "text": "The above study showed that the correct BO setting \u03bb can be learned by IBO. This subsection continues to show the advantage of \"learning from others,\" i.e. updating \u03bb by IBO, over \"self-adjustment,\" i.e. finding the MLE of \u03bb by hk. The settings follow the last study and the results1 But why does the assumption of \u0432 = 10.0I lead to a significant reduction of L in the other three cases? This is because BO in these cases does not resemble random sampling, i.e. the sequences of the samples are more clustered. If a new sample is in this cluster, its similarities with existing ones are not zero, even if a large number of them is assumed, due to the small euclidean distance between the pairs. And again, the expected improvement function has peaks within the clusters, and remains consistently far away from them. As a result, the optimal value of l (x, BO) in relation to which becomes negative even if BO is erroneous."}, {"heading": "6 A case study on ecoRacer", "text": "With the support of simulation studies, this section examines how IBO can improve the effectiveness of BO in finding the optimal design and control solution for the ecoRacer game when observing effective searches through crowdsourcing. To this end, we first apply Independent Component Analysis (ICA) to reduce the dimensionality of all players \"crowdsourcing solutions, then use these ICA bases to encode the games of a player who had a rapid convergence but did not have the best solution, and the resulting data is then used to obtain the IBO Estimator \u03bb and a GP Estimator \u0445 GP. Convergence performance when playing ecoRacer under these two conditions is compared."}, {"heading": "6.1 Dimension reduction for player\u2019s control signals", "text": "In fact, the control signals we have received in individual countries are ineffective because they are able to solve high-dimensional problems. In a previous study, we did what we have to do to solve them; in a second study, we did what we have to do to solve them; in a third study, we did what we have to do to solve them."}, {"heading": "6.2 Derivation of \u03bb\u0302 and \u03bb\u0302GP", "text": "Based on the hypothesis that a player has a good search strategy if he / she improves his / her performance quickly, we select the search history of a player referred to as \"P2\" who has scored the second highest score within 31 games, much less than the 150 games of which the highest score has been achieved. To apply IBO, we first encode all control solutions from P2 using the learned ICA basics. Together with the final drive ratios, all 31 solutions are then normalized so that they are dominated within [\u2212 1, 1] 31. We also found that the probability that P2 has followed the max-min sampling scheme is lower than the following BO, since the minimum values of l (xk, \u03b1INI) for k = 2,..., 31 (with respect to its INI) are dominated by those of l (xk, \u03b1BO). This indicates that either the player has not carried out a pure exploration according to the min-scheme or is worse at modelling this BO than the explanation of the BO."}, {"heading": "6.3 Comparison of BO performance", "text": "Fig.6 compares the BO performance under the conditions \u03bb, \u03bb, GP and \u0430 = I. In any case, we start with the first two pieces from P2 and perform 180 BO iterations. Similar to the simulation study, the results are reported on the basis of 20 experiments, which is due to the stochastic nature of the BO. Due to the small number of experiments, a bootstrap variance estimate (the shades around the average) is given."}, {"heading": "7 Discussion", "text": "The above study provides a starting point for studies on the quantitative inclusion of human solution-seeking data in optimization algorithms, but many pressing questions remain unanswered in this work. In this section, some notable questions are addressed: First, we will discuss the link between IBO and the existing work on Inverse Reinforcement Learning [26-28] (IRL, also referred to as apprenticeship learning [29,30] and inverse optimal governance [31]). Understanding this link is important as we will use analogies from IRL to explain various types of IBO failure and its potential use."}, {"heading": "7.1 The difference between learning from searches and learning from solutions", "text": "The proposed IBO approach can be seen as a way to design an optimization algorithm (hereinafter referred to as DO), and is conceptually related to IRL. To explain the similarity and difference between the two, we first present the Markov Decision Process (MDP) and Reinforcement Learning (RL), and establish analogies between (1) MDP and the optimization process, and (2) RL and DO."}, {"heading": "7.1.1 Preliminaries on MDP and RL", "text": "Formally, an MDP consists of a tuple < S, A, T, R, \u03b3, b0 > where: S is a finite set of states; A is a finite set of actions; the state transitional function T (s, a, s) determines the probability that there will be a change of states s to s when measures a are taken; R (s, a) is the immediate reward for measures a in states s; q [0, 1) is the discount factor of the cumulative reward; b0 (s) indicates the probability of starting the process in states s. In RL, a control policy \u03c0 is a mapping of a state to an action, i.e., \u03c0: S \u2192 A. The long-term value of \u03c0 for a given state s can be calculated by V \u03c0 (s) = R (s) = R (s, \u03c0 (s) + \u03b3 s \u00b2 s \u00b2 s \u00b2, s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s \u00b2 s, and therefore the expectation that Qs is a uniform (E)."}, {"heading": "7.1.2 MDP vs. optimization process", "text": "We first note that an optimization process can be modeled as a sequential decision process and shares elements with the MDP: its immediate reward is the improvement of the objective value achieved by each new sample, and the cumulative reward represents the total improvement of the objective value within a finite number of iterations. Its state contains the current solution (in X), the corresponding objective value, and potentially the gradients and derivatives of higher order of the objective function; its action is the next solution to be evaluated; and the transition of the state is determined by the optimization algorithm parameterized by some algorithmic settings (e.g. BO settings), analogous to the transition of the state in the MDP, which is influenced by the control parameters."}, {"heading": "7.1.3 RL vs. DO", "text": "The relationship between DO and an optimization process is analogous to that between RL and an MDP: Since RL searches for the control parameters that maximize the value of an MDP, DO similarly searches for the settings of an optimization algorithm that maximizes the improvement of the target. Since MDP is a kind of decision process, RL belongs to DO. Furthermore, DO is recursive: The optimal design of any DO algorithm is inherently a DO problem. We will point out that there is a significant difference between RL and DO: While RL algorithms can be developed based on the Bellman optimization principle and the Markovian property of MDP, DO is generally done empirically and manually. For example, good algorithmic parameters are suggested for either gradient-based methods (e.g. sequential quadratic programming) or for heuristic methods (e.g. genetic algorithms) based on experience rather than an optimization problem."}, {"heading": "7.1.4 IRL vs. IBO", "text": "While RL proposes an optimal control policy for an MDP with a given reward function, a reward function is hardly defined, for example, the reward for \"good drives\" cannot be explicitly defined. Alternatively, it is easier for a human expert to identify actions that are nearly optimal for a given task. [26,29,32,33] IRL techniques are therefore designed to eliminate the inherent reward (and thus the Q function) that explains human demonstrations, either by estimating the reward parameters so that the policy demonstrated has a higher value than any other policy with a margin [26,29,32,33], or by maximizing the likelihood of control parameters by adopting approximately optimal control of the demonstration. [27,34] The IBO approach introduced in this paper as a type of DO is closely related to the latter type of IRLs, and in particular to the maximum entropy method proposed by Ziebart [27]."}, {"heading": "7.2 The potential value of IBO", "text": "We saw that IBO has limited appeal to the presented ecoRacer case, as it only resulted in a marginal improvement in the BO situation after 31 solutions of P2 were observed. In fact, a faster convergence with a standard BO (\u03bb = 1.0 I) could be achieved by initializing the search with these solutions. Below, we will discuss three situations where IBO and DO will generally fail, and what future research is needed to avoid failures. The first situation is when the human search strategy cannot be explained by BO. However, studies in cognitive science have identified some of the core components of human intelligence, including intuitive physics [35-38], problem-solving capabilities [39-41], the ability of learning-to-learn [42], and others. While evidence has shown the link between BO and human search, suitable models of human search strategies may most likely depend on problems."}, {"heading": "8 Conclusions", "text": "To test this hypothesis, we introduced the method of inverse Bayesian optimization, which attempts to mimic human searches through Bayesian optimization. Simulation studies of the 30D Rosenbrock function showed that IBO can correctly identify the best BO parameters based on a small number of observations, and the resulting BO can surpass the method with self-adaptive parameters. We then applied IBO to the real human search data collected in the game ecoRacer. The marginal improvement that IBO achieved through standard and self-adaptive BO settings showed the limitations of this approach. Potential solutions to these limitations are discussed in detail, using an analogy between IBO and IRL.We conclude with a larger picture. Design Crowdsourcing faces a dilemma: These problems are thought to be a success due to the high quality of crowdsourcing solutions, which represents a high-quality solution due to the high cost of crowdsourcing research."}, {"heading": "Acknowledgement", "text": "This work was supported by the National Science Foundation under grant number CMMI-1266184 and the start-up funding of Arizona State University and is gratefully received."}], "references": [{"title": "ecoracer: Game-based optimal electric vehicle design and driver control using human players", "author": ["Y. Ren", "A.E. Bayrak", "P.Y. Papalambros"], "venue": "In ASME 2015 International Design Engineering Technical Conferences and Computers and Information in Engineering Con-", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Foldit. http: //fold.it", "author": ["S. Cooper", "F. Khatib", "A. Treuille", "J. Barbero", "J. Lee", "M. Beenen", "A. Leaver-Fay", "D. Baker", "Z Popovi\u0107"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Algorithm discovery by protein folding game players", "author": ["F. Khatib", "S. Cooper", "M.D. Tyka", "K. Xu", "I. Makedon", "Z. Popovi\u0107", "D. Baker", "F. Players"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2011}, {"title": "eterna. http://eterna.cmu.edu", "author": ["J. Lee", "W. Kladwang", "M. Lee", "D. Cantu", "M. Azizyan", "H. Kim", "A. Limpaecher", "S. Yoon", "A. Treuille", "R. Das"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Rna design rules from a massive open laboratory", "author": ["J. Lee", "W. Kladwang", "M. Lee", "D. Cantu", "M. Azizyan", "H. Kim", "A. Limpaecher", "S. Yoon", "A. Treuille", "R. Das"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Phylo: a citizen science approach for improving multiple sequence alignment", "author": ["A. Kawrykow", "G. Roumanis", "A. Kam", "D. Kwak", "C. Leung", "C. Wu", "E. Zarour", "L. Sarmenta", "M. Blanchette", "J Waldisp\u00fchl"], "venue": "PloS one,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Robobarista: Object part based transfer of manipulation trajectories from crowd-sourcing in 3d pointclouds", "author": ["J. Sung", "S.H. Jin", "A. Saxena"], "venue": "arXiv preprint arXiv:1504.03071", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Frenzy: collaborative data organization for creating conference sessions", "author": ["L.B. Chilton", "J. Kim", "P. Andr\u00e9", "F. Cordeiro", "J.A. Landay", "D.S. Weld", "S.P. Dow", "R.C. Miller", "H. Zhang"], "venue": "In Proceedings of the 32nd annual ACM conference on Human factors in computing systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Games with a purpose", "author": ["L. Von Ahn"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Efficient global optimization of expensive black-box functions", "author": ["D. Jones", "M. Schonlau", "W. Welch"], "venue": "Journal of Global Optimization,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "A tutorial on bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. De Freitas"], "venue": "arXiv preprint arXiv:1012.2599", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Bayesian gait optimization for bipedal locomotion", "author": ["R. Calandra", "N. Gopalan", "A. Seyfarth", "J. Peters", "M.P. Deisenroth"], "venue": "In International Conference on Learning and Intelligent Optimization,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Bayesian optimization explains human active search", "author": ["A. Borji", "L. Itti"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Gaussian processes for machine learning", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Lipschitzian optimization without the lipschitz constant", "author": ["D.R. Jones", "C.D. Perttunen", "B.E. Stuckman"], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1993}, {"title": "Baron: A general purpose global optimization software package", "author": ["N.V. Sahinidis"], "venue": "Journal of global optimization,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1996}, {"title": "L-bfgs-b: Fortran subroutines for large scale bound constrained optimization", "author": ["C. Zhu", "R.H. Byrd", "P. Lu", "J. Nocedal"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1994}, {"title": "A unified attentional bottleneck in the human brain", "author": ["M.N. Tombu", "C.L. Asplund", "P.E. Dux", "D. Godwin", "J.W. Martin", "R. Marois"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2011}, {"title": "Discretionary task interleaving: heuristics for time allocation in cognitive foraging.", "author": ["S.J. Payne", "G.B. Duggan", "H. Neth"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2007}, {"title": "Is study time allocated selectively to a region of proximal learning?", "author": ["J. Metcalfe"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2002}, {"title": "Blind source separation and independent component analysis: A review", "author": ["S. Lee"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2005}, {"title": "Independent component analysis", "author": ["J.V. Stone"], "venue": "Wiley Online Library", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2004}, {"title": "The \u201dindependent components\u201d of natural scenes are edge filters", "author": ["A.J. Bell", "T.J. Sejnowski"], "venue": "Vision research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1997}, {"title": "An empirical comparison of information-theoretic criteria in estimating the number of independent components of fmri data", "author": ["M. Hui", "J. Li", "X. Wen", "L. Yao", "Z. Long"], "venue": "PloS one,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Algorithms for inverse reinforcement learning.", "author": ["A.Y. Ng", "Russell", "S. J"], "venue": "In Icml,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "Maximum entropy inverse reinforcement learning.", "author": ["B.D. Ziebart", "A.L. Maas", "J.A. Bagnell", "A.K. Dey"], "venue": "In AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Nonlinear inverse reinforcement learning with gaussian processes", "author": ["S. Levine", "Z. Popovic", "V. Koltun"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "In Proceedings of the twenty-first international conference on Machine learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2004}, {"title": "Autonomous helicopter aerobatics through apprenticeship learning", "author": ["P. Abbeel", "A. Coates", "A.Y. Ng"], "venue": "The International Journal of Robotics Research", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2010}, {"title": "Inverse optimal control with linearly-solvable mdps", "author": ["K. Dvijotham", "E. Todorov"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2010}, {"title": "Maximum margin planning", "author": ["N.D. Ratliff", "J.A. Bagnell", "M.A. Zinkevich"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2006}, {"title": "A game-theoretic approach to apprenticeship learning", "author": ["U. Syed", "R.E. Schapire"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2007}, {"title": "Bayesian inverse reinforcement learning", "author": ["D. Ramachandran", "E. Amir"], "venue": null, "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2007}, {"title": "The development of object perception.", "author": ["E.S. Spelke", "G. Gutheil", "G. Van de Walle"], "venue": null, "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1995}, {"title": "An account of infants\u2019 physical reasoning", "author": ["R. Baillargeon", "J. Li", "W. Ng", "S. Yuan"], "venue": "Learning and the infant mind,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2009}, {"title": "Humans predict liquid dynamics using probabilistic simulation", "author": ["C.J. Bates", "I. Yildirim", "J.B. Tenenbaum", "P.W. Battaglia"], "venue": "In Proceedings of the 37th annual conference of the cognitive science society", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Computational rationality: A converging paradigm for intelligence in brains, minds, and machines", "author": ["S.J. Gershman", "E.J. Horvitz", "J.B. Tenenbaum"], "venue": "Science, 349(6245),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Recognition-by-components: a theory of human image understanding.", "author": ["I. Biederman"], "venue": "Psychological review,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 1987}, {"title": "Hierarchical deep reinforcement learning: Integrating temporal abstraction and intrinsic motivation", "author": ["T.D. Kulkarni", "K.R. Narasimhan", "A. Saeedi", "J.B. Tenenbaum"], "venue": "arXiv preprint arXiv:1604.06057", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2016}, {"title": "The formation of learning sets.", "author": ["H.F. Harlow"], "venue": "Psychological review,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1949}, {"title": "Building machines that learn and think like people", "author": ["B.M. Lake", "T.D. Ullman", "J.B. Tenenbaum", "S.J. Gershman"], "venue": "arXiv preprint arXiv:1604.00289", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem [1].", "startOffset": 136, "endOffset": 139}, {"referenceID": 1, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 82, "endOffset": 88}, {"referenceID": 2, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 82, "endOffset": 88}, {"referenceID": 3, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 104, "endOffset": 110}, {"referenceID": 4, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 104, "endOffset": 110}, {"referenceID": 5, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 138, "endOffset": 141}, {"referenceID": 6, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 163, "endOffset": 166}, {"referenceID": 7, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 273, "endOffset": 276}, {"referenceID": 0, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 296, "endOffset": 304}, {"referenceID": 8, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 296, "endOffset": 304}, {"referenceID": 9, "context": "Examples can be found from crowdsourcing scientific solutions for protein folding [2, 3], RNA synthesis [4, 5], genome sequence alignment [6], robot arm movements [7], to name a few, as well as from our daily achievements in packing luggage, scheduling conference sessions [8], and playing games [1,9,10].", "startOffset": 296, "endOffset": 304}, {"referenceID": 0, "context": "As a particular example, our previous study investigated the value of crowdsourcing optimal design and control problems with nondeterministic polynomial time by comparing the search performance of an anonymous crowd with a Bayesian Optimization (BO) algorithm on an electric vehicle time-trial game [1].", "startOffset": 299, "endOffset": 302}, {"referenceID": 10, "context": "Specifically, we assume that a player\u2019s search trajectory is produced from BO (also called \u201cEfficient Global Optimization\u201d [11]) where each new trial optimizes an expected improvement function learned and adjusted by the player during the search.", "startOffset": 123, "endOffset": 127}, {"referenceID": 10, "context": "We make the assumption for two reasons: First, BO has been widely used for non-convex optimization problems with high evaluation costs [11,12], which includes the design of dynamical systems where the evaluation of the objective involves long-term simulation of the", "startOffset": 135, "endOffset": 142}, {"referenceID": 11, "context": "We make the assumption for two reasons: First, BO has been widely used for non-convex optimization problems with high evaluation costs [11,12], which includes the design of dynamical systems where the evaluation of the objective involves long-term simulation of the", "startOffset": 135, "endOffset": 142}, {"referenceID": 11, "context": "system [12,13].", "startOffset": 7, "endOffset": 14}, {"referenceID": 12, "context": "system [12,13].", "startOffset": 7, "endOffset": 14}, {"referenceID": 13, "context": "Secondly, it is recently found that BO best resembles human search, which outperforms an extensive set of optimization algorithms in 1D problems [14].", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "Image is reproduced from [1].", "startOffset": 25, "endOffset": 28}, {"referenceID": 14, "context": "Model update: BO first updates a Gaussian Process (GP) model [15] to predict objective values, based on current observations hk and Gaussian parameters \u03bb.", "startOffset": 61, "endOffset": 65}, {"referenceID": 15, "context": "Note that QEI is non-convex with respect to x and its maximization may involve a nested global optimization routine, such as Genetic Algorithm, DIRECT [16], and BARON [17].", "startOffset": 151, "endOffset": 155}, {"referenceID": 16, "context": "Note that QEI is non-convex with respect to x and its maximization may involve a nested global optimization routine, such as Genetic Algorithm, DIRECT [16], and BARON [17].", "startOffset": 167, "endOffset": 171}, {"referenceID": 10, "context": "The exploration set h0 that is necessary to initialize the GP model is usually created by Latin Hypercube sampling (LHS, see [11] for details).", "startOffset": 125, "endOffset": 129}, {"referenceID": 0, "context": "Image is modified from [1].", "startOffset": 23, "endOffset": 26}, {"referenceID": 17, "context": "At each iteration, the expected improvement is maximized using a multi-start gradient descent algorithm [18] with 100 LHS initial guesses.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "study on ecoRacer [1], this was done by introducing state-dependent basis functions (i.", "startOffset": 18, "endOffset": 21}, {"referenceID": 18, "context": "In fact, concurrent performance of multiple tasks consistently leads to impairment in one or more of those tasks [19].", "startOffset": 113, "endOffset": 117}, {"referenceID": 19, "context": "Such a view is derived from optimal foraging theory, which understands animal foraging as an optimization of the rate of energy gain, and then viewing human behavioral solutions as ones that optimize the rate of information gain in the problem [20].", "startOffset": 244, "endOffset": 248}, {"referenceID": 20, "context": "An example of this is a studying strategy while preparing for an exam, and the tendency we have to switch between difficult and easy topics depending on our perceived productivity at learning them [21].", "startOffset": 197, "endOffset": 201}, {"referenceID": 21, "context": "separation [22], which can be elegantly addressed using ICA [23].", "startOffset": 11, "endOffset": 15}, {"referenceID": 22, "context": "separation [22], which can be elegantly addressed using ICA [23].", "startOffset": 60, "endOffset": 64}, {"referenceID": 23, "context": "of edge-filter-like bases [24], the components arrived at using ICA are naturally correlated with spatial locations, much like wavelets, with each having a peak at some unique position (see Fig.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "While it is theoretically possible to find some \u201cmost likely\u201d number of bases using information-theoretic criteria for model selection [25]2, the choice of 30 basis is reasonable because (1) over 95% of the variance is explained, and (2) the resultant solution space (30 control variables and one design variable) is small enough for BO to be effective.", "startOffset": 135, "endOffset": 139}, {"referenceID": 24, "context": "For completeness, we used 1000 PCA components as preprocessing to obtain the most likely number of ICA components under MDL [25], AIC, and KIC information criteria as 187, 464, and 373, respectively, using the method from [25].", "startOffset": 124, "endOffset": 128}, {"referenceID": 24, "context": "For completeness, we used 1000 PCA components as preprocessing to obtain the most likely number of ICA components under MDL [25], AIC, and KIC information criteria as 187, 464, and 373, respectively, using the method from [25].", "startOffset": 222, "endOffset": 226}, {"referenceID": 18, "context": ", given that the game takes 36 seconds, a decision interval of 36s/187 = 192ms is close to the range for the time-frame of attentional blink, which is 200-500 ms [19]), the resultant high-dimensional solution spaces are unfavorable for BO.", "startOffset": 162, "endOffset": 166}, {"referenceID": 25, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 146, "endOffset": 153}, {"referenceID": 26, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 146, "endOffset": 153}, {"referenceID": 27, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 146, "endOffset": 153}, {"referenceID": 28, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 196, "endOffset": 203}, {"referenceID": 29, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 196, "endOffset": 203}, {"referenceID": 30, "context": "This section will address a few notable ones: We discuss first the connection between IBO and the existing work on Inverse Reinforcement Learning [26\u201328] (IRL, also called apprenticeship learning [29,30] and inverse optimal control [31]).", "startOffset": 232, "endOffset": 236}, {"referenceID": 25, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 264, "endOffset": 277}, {"referenceID": 28, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 264, "endOffset": 277}, {"referenceID": 31, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 264, "endOffset": 277}, {"referenceID": 32, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 264, "endOffset": 277}, {"referenceID": 26, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 387, "endOffset": 394}, {"referenceID": 33, "context": "IRL techniques have thus been developed to identify the inherent reward (and thus the Q-function) that explains human demonstrations, either by estimating the reward parameters so that the demonstrated policy has a higher value than any other policies by a margin [26,29,32,33], or by maximizing the likelihood of control parameters by assuming near-optimal control of the demonstration [27,34].", "startOffset": 387, "endOffset": 394}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": ", as in [34]) cannot be applied to optimize the likelihood function since the partition values for two different samples of \u03bb do not cancel.", "startOffset": 8, "endOffset": 12}, {"referenceID": 26, "context": "(8), where N is a large number [27].", "startOffset": 31, "endOffset": 35}, {"referenceID": 34, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 125, "endOffset": 132}, {"referenceID": 35, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 125, "endOffset": 132}, {"referenceID": 36, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 125, "endOffset": 132}, {"referenceID": 37, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 125, "endOffset": 132}, {"referenceID": 38, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 163, "endOffset": 170}, {"referenceID": 39, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 163, "endOffset": 170}, {"referenceID": 40, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 205, "endOffset": 209}, {"referenceID": 41, "context": "Studies in cognitive science have identified some of the core ingredients of human intelligence, including intuitive physics [35\u201338], problem decomposition skills [39\u201341], the ability of learning-to-learn [42], and others [43].", "startOffset": 222, "endOffset": 226}, {"referenceID": 13, "context": "While evidence have shown the connection between BO and human search [14], suitable models of human search strategies can very likely be problem dependent.", "startOffset": 69, "endOffset": 73}], "year": 2017, "abstractText": "There is evidence that humans can be more efficient than existing algorithms at searching for good solutions in high-dimensional and nonconvex design or control spaces, potentially due to our prior knowledge and learning capability. This work attempts to quantify the search strategy of human beings to enhance a Bayesian optimization (BO) algorithm for an optimal design and control problem. We consider the sequence of human solutions (called a search trajectory) as generated from BO, and propose to recover the algorithmic parameters of BO through maximum likelihood estimation. The method is first verified through simulation studies and then applied to human solutions crowdsourced from a gamified design problem [1]. We learn BO parameters from a player who achieved fast improvement in his/her solutions and show that applying the learned parameters to BO achieves better convergence than using a self-adaptive BO. The proposed method is different from inverse reinforcement learning in that it only requires a good search strategy, rather than near-optimal solutions from humans.", "creator": "LaTeX with hyperref package"}}}