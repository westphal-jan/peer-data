{"id": "1608.06993", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Densely Connected Convolutional Networks", "abstract": "Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1)/2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74% test error on CIFAR-10, 19.25% on CIFAR-100 and 1.59% on SVHN).", "histories": [["v1", "Thu, 25 Aug 2016 00:44:55 GMT  (1175kb,D)", "http://arxiv.org/abs/1608.06993v1", "12 pages"], ["v2", "Tue, 29 Nov 2016 14:50:55 GMT  (8685kb,D)", "http://arxiv.org/abs/1608.06993v2", "12 pages"], ["v3", "Sat, 3 Dec 2016 08:35:43 GMT  (3119kb,D)", "http://arxiv.org/abs/1608.06993v3", "12 pages"], ["v4", "Sun, 27 Aug 2017 02:56:24 GMT  (6957kb,D)", "http://arxiv.org/abs/1608.06993v4", "12 pages"]], "COMMENTS": "12 pages", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["gao huang", "zhuang liu", "kilian q weinberger", "laurens van der maaten"], "accepted": false, "id": "1608.06993"}, "pdf": {"name": "1608.06993.pdf", "metadata": {"source": "CRF", "title": "DENSELY CONNECTED CONVOLUTIONAL NETWORKS", "authors": ["Gao Huang", "Zhuang Liu"], "emails": ["gh349@cornell.edu", "liuzhuang13@mails.tsinghua.edu.cn", "kqw4@cornell.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "In fact, most of them are able to play by the rules that they have set themselves in order to play by the rules."}, {"heading": "2 RELATED WORK", "text": "This year it is more than ever before."}, {"heading": "3 DENSENETS", "text": "This year, it is only a matter of time before that happens, until that happens, until an agreement is reached."}, {"heading": "4 EXPERIMENTS", "text": "We demonstrate empirically the effectiveness of DenseNet on several benchmark datasets and compare it with modern network architectures, in particular ResNet and its variants, implemented in Torch 7 (Collobert et al., 2011).The code for reproducing the results is available at https: / / github.com / liuzhuang13 / DenseNet."}, {"heading": "4.1 DATASETS", "text": "The two CIFAR datasets (Krizhevsky & Hinton, 2009) consist of 32 x 32 pixel color natural scenery images. CIFAR-10 (C10) images are drawn from 10 and CIFAR-100 (C100) images from 100 classes. Train and test sets contain 50,000 and 10,000 images, respectively, and we have 5,000 training images available as a validation set. A standard data augmentation scheme is widely used for these datasets (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; Er et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-added with 4 pixels on each side, then randomly cropped."}, {"heading": "4.2 CLASSIFICATION RESULTS.", "text": "This year it is more than ever before in the history of the city."}, {"heading": "5 DISCUSSION", "text": "In fact, most of them are able to outdo themselves, and that is why they are not able to outdo themselves. (...) Most of them are able to outdo themselves. (...) Most of them are able to outdo themselves. \"(...) Most of them are able to outdo themselves. (...) Most of them are able to outdo themselves.\" (...) Most of them are able to outdo themselves. \"(...) Most of them are able to outdo themselves.\" (...) Most of them are able to outdo themselves. \"(...) Most of them are able to outdo themselves.\" (...)"}, {"heading": "6 CONCLUSION", "text": "We have proposed a new architecture of convolutionary neural networks, which we call the Dense Convolutional Network (DenseNet), which introduces direct connections within all layers of a \"dense block\" in the network. We have shown that DenseNets naturally scale to over a hundred layers with no optimization difficulties. In our experiments, DenseNets tend to achieve consistent improvement in accuracy as the number of parameters increases, with no signs of performance deterioration or overhauling. Under multiple settings, DenseNets achieve state-of-the-art results across multiple datasets. Although DenseNets follow a simple connectivity rule, they naturally integrate the nice features of identity mapping, deep monitoring, and diversified depth. They allow for reuse of functions across the network, allowing us to learn more compact and accurate models based on our experiments. Because of their compact internal representations and reduced functional redundancy, DenseNets could be feature extractors for a wide range of monitoring tasks in 2015, or if we believe in a year of monitoring component volume in 2015."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors are partially supported by the III-1618134, III-1526012, IIS-1149882 Scholarship of the National Science Foundation. Gao Huang is supported by the International Postdoctoral Exchange Fellowship Program of China Postdoctoral Council (No.20150015). Zhuang Liu is supported by the National Basic Research Program of China Grants 2011CBA00300, 2011CBA00301, the National Natural Science Foundation of China Grant 61361136003. We also thank Geoff Pleiss and Yu Sun for many insightful discussions."}], "references": [{"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop, number EPFL-CONF-192376,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "The cascade-correlation learning architecture", "author": ["Scott E Fahlman", "Christian Lebiere"], "venue": null, "citeRegEx": "Fahlman and Lebiere.,? \\Q1989\\E", "shortCiteRegEx": "Fahlman and Lebiere.", "year": 1989}, {"title": "Deep manifold traversal: Changing labels with convolutional features", "author": ["Jacob R Gardner", "Matt J Kusner", "Yixuan Li", "Paul Upchurch", "Kilian Q Weinberger", "John E Hopcroft"], "venue": "arXiv preprint arXiv:1511.06421,", "citeRegEx": "Gardner et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gardner et al\\.", "year": 2015}, {"title": "A neural algorithm of artistic style", "author": ["Leon A Gatys", "Alexander S Ecker", "Matthias Bethge"], "venue": "arXiv preprint arXiv:1508.06576,", "citeRegEx": "Gatys et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gatys et al\\.", "year": 2015}, {"title": "Deep sparse rectifier neural networks", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio"], "venue": "In Aistats,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Training and investigating residual nets, 2016", "author": ["Sam Gross", "Michael Wilber"], "venue": "URL http:// torch.ch/blog/2016/02/04/resnets.html", "citeRegEx": "Gross and Wilber.,? \\Q2016\\E", "shortCiteRegEx": "Gross and Wilber.", "year": 2016}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1603.05027,", "citeRegEx": "He et al\\.,? \\Q2016\\E", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Deep networks with stochastic depth", "author": ["Gao Huang", "Yu Sun", "Zhuang Liu", "Daniel Sedra", "Kilian Weinberger"], "venue": "arXiv preprint arXiv:1603.09382,", "citeRegEx": "Huang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning multiple layers of features from tiny images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": null, "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Fractalnet: Ultra-deep neural networks without residuals", "author": ["Gustav Larsson", "Michael Maire", "Gregory Shakhnarovich"], "venue": "arXiv preprint arXiv:1605.07648,", "citeRegEx": "Larsson et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Larsson et al\\.", "year": 2016}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Yann LeCun", "Bernhard Boser", "John S Denker", "Donnie Henderson", "Richard E Howard", "Wayne Hubbard", "Lawrence D Jackel"], "venue": "Neural computation,", "citeRegEx": "LeCun et al\\.,? \\Q1989\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1989}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Bridging the gaps between residual learning, recurrent neural networks and visual cortex", "author": ["Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1604.03640,", "citeRegEx": "Liao and Poggio.,? \\Q2016\\E", "shortCiteRegEx": "Liao and Poggio.", "year": 2016}, {"title": "Reading digits in natural images with unsupervised feature learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "Deconstructing the ladder network architecture", "author": ["Mohammad Pezeshki", "Linxi Fan", "Philemon Brakel", "Aaron Courville", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1511.06430,", "citeRegEx": "Pezeshki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pezeshki et al\\.", "year": 2015}, {"title": "Semisupervised learning with ladder networks", "author": ["Antti Rasmus", "Mathias Berglund", "Mikko Honkala", "Harri Valpola", "Tapani Raiko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Rasmus et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rasmus et al\\.", "year": 2015}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Imagenet large scale visual recognition challenge", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "venue": "International Journal of Computer Vision,", "citeRegEx": "Russakovsky et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Convolutional neural networks applied to house numbers digit classification", "author": ["Pierre Sermanet", "Soumith Chintala", "Yann LeCun"], "venue": "In Pattern Recognition (ICPR),", "citeRegEx": "Sermanet et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sermanet et al\\.", "year": 2012}, {"title": "Striving for simplicity: The all convolutional net", "author": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806,", "citeRegEx": "Springenberg et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "On the importance of initialization and momentum in deep learning", "author": ["Ilya Sutskever", "James Martens", "George Dahl", "Geoffrey Hinton"], "venue": "In Proceedings of the 30th international conference on machine learning", "citeRegEx": "Sutskever et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2013}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Rethinking the inception architecture for computer vision", "author": ["Christian Szegedy", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew Wojna"], "venue": "arXiv preprint arXiv:1512.00567,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Resnet in resnet: Generalizing residual architectures", "author": ["Sasha Targ", "Diogo Almeida", "Kevin Lyman"], "venue": "arXiv preprint arXiv:1603.08029,", "citeRegEx": "Targ et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Targ et al\\.", "year": 2016}, {"title": "Neural network learning without backpropagation", "author": ["Bogdan M Wilamowski", "Hao Yu"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "Wilamowski and Yu.,? \\Q2010\\E", "shortCiteRegEx": "Wilamowski and Yu.", "year": 2010}, {"title": "Wide residual networks", "author": ["Sergey Zagoruyko", "Nikos Komodakis"], "venue": "arXiv preprint arXiv:1605.07146,", "citeRegEx": "Zagoruyko and Komodakis.,? \\Q2016\\E", "shortCiteRegEx": "Zagoruyko and Komodakis.", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "Although they have been originally introduced over 20 years ago (LeCun et al., 1989) only recent improvements in computer hardware and network structure have enabled the training of truly deep CNNs.", "startOffset": 64, "endOffset": 84}, {"referenceID": 15, "context": "The original LeNet5 (LeCun et al., 1998) consisted of 5 layers, VGG-net featured 19 (Russakovsky et al.", "startOffset": 20, "endOffset": 40}, {"referenceID": 21, "context": ", 1998) consisted of 5 layers, VGG-net featured 19 (Russakovsky et al., 2015), and only last year Highway Networks (Srivastava et al.", "startOffset": 51, "endOffset": 77}, {"referenceID": 9, "context": "Stochastic Depth (Huang et al., 2016) shortens ResNets by randomly dropping layers during training to allow better information and gradient flow.", "startOffset": 17, "endOffset": 37}, {"referenceID": 6, "context": ", 2015) and Residual Networks (ResNets) (He et al., 2015b) have surpassed the 100 layers barrier. As CNNs become increasingly deep, a new research problem emerges: as information about the input or gradient passes through many layers, it can vanish and \u201cwash out\u201d by the time it reaches the end (or beginning) of the network. Many recent publications address this or related problems. ResNets (He et al., 2015b) and Highway Networks (Srivastava et al., 2015) bypass signal from one layer to the next via identity connections. Stochastic Depth (Huang et al., 2016) shortens ResNets by randomly dropping layers during training to allow better information and gradient flow. Recently, Larsson et al. (2016) introduced FractalNets, which repeatedly combine several parallel layer sequences with different number of convolutional blocks to obtain a large nominal depth, while main\u2217Authors contribute equally.", "startOffset": 41, "endOffset": 704}, {"referenceID": 9, "context": "Prior work (Huang et al., 2016) has shown that there is great redundancy within the feature maps of the individual layers in ResNets.", "startOffset": 11, "endOffset": 31}, {"referenceID": 9, "context": "Recent variations of ResNets (Huang et al., 2016) show that many layers make only very small changes, and in fact can be randomly dropped during training.", "startOffset": 29, "endOffset": 49}, {"referenceID": 9, "context": "(2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016).", "startOffset": 70, "endOffset": 90}, {"referenceID": 19, "context": "Highway Networks (Srivastava et al., 2015) were amongst the first architectures that provided a means to effectively train end-to-end networks with more than 100 layers. By introducing the bypassing layers along with gating units, Srivastava et al. (2015) showed that Highway Networks with hundreds of layers can be optimized with SGD effectively.", "startOffset": 18, "endOffset": 256}, {"referenceID": 6, "context": "This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al.", "startOffset": 43, "endOffset": 368}, {"referenceID": 6, "context": "This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016). Stochastic depth is a mechanism to improve the training of deep residual networks by dropping layers randomly. It also shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. This paper was in part inspired by this observation. He et al. (2016) introduces ResNets with pre-activation, which similar to stochastic depth allow state-of-the-art performance with > 1000 layers.", "startOffset": 43, "endOffset": 769}, {"referenceID": 6, "context": "This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016). Stochastic depth is a mechanism to improve the training of deep residual networks by dropping layers randomly. It also shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. This paper was in part inspired by this observation. He et al. (2016) introduces ResNets with pre-activation, which similar to stochastic depth allow state-of-the-art performance with > 1000 layers. An orthogonal approach to making networks deeper (e.g., with the help of skip connections) is to increase the network width. The GoogLeNet (Szegedy et al., 2015a;b) uses an \u201cInception module\u201d which concatenates feature maps produced by filters of different sizes. Targ et al. (2016) propose a variant of ResNets with wide generalized residual blocks.", "startOffset": 43, "endOffset": 1181}, {"referenceID": 6, "context": "This point is further supported by ResNet (He et al., 2015b), in which pure identity mappings are used as bypassing layers. ResNets have achieved impressive (often record breaking) achievements on most challenging image recognition, localization and detection tasks, such as ImageNet and COCO object detection dataset (He et al., 2015b). Recently, Huang et al. (2016) successfully trained a 1202-layer ResNet with stochastic depth (Huang et al., 2016). Stochastic depth is a mechanism to improve the training of deep residual networks by dropping layers randomly. It also shows that not all layers may be needed and highlights that there is a great amount of redundancy in deep (residual) networks. This paper was in part inspired by this observation. He et al. (2016) introduces ResNets with pre-activation, which similar to stochastic depth allow state-of-the-art performance with > 1000 layers. An orthogonal approach to making networks deeper (e.g., with the help of skip connections) is to increase the network width. The GoogLeNet (Szegedy et al., 2015a;b) uses an \u201cInception module\u201d which concatenates feature maps produced by filters of different sizes. Targ et al. (2016) propose a variant of ResNets with wide generalized residual blocks. In fact, simply increasing the number of filers in each layer of ResNets can improve its performance provided the depth is sufficient (Zagoruyko & Komodakis, 2016). The FractalNet proposed by Larsson et al. (2016) also achieves competitive results on several benchmark datasets using a wide network structure.", "startOffset": 43, "endOffset": 1463}, {"referenceID": 19, "context": "The Ladder Networks (Rasmus et al., 2015; Pezeshki et al., 2015) introduce lateral connections into autoencoders, leading to impressive accuracy on semi-supervised learning tasks .", "startOffset": 20, "endOffset": 64}, {"referenceID": 18, "context": "The Ladder Networks (Rasmus et al., 2015; Pezeshki et al., 2015) introduce lateral connections into autoencoders, leading to impressive accuracy on semi-supervised learning tasks .", "startOffset": 20, "endOffset": 64}, {"referenceID": 4, "context": "H`(\u00b7) can be a composite function of operations such as Batch Normalization (BN) (Ioffe & Szegedy, 2015), rectified linear units (ReLU) (Glorot et al., 2011), Pooling (LeCun et al.", "startOffset": 136, "endOffset": 157}, {"referenceID": 15, "context": ", 2011), Pooling (LeCun et al., 1998), or Convolution (Conv).", "startOffset": 17, "endOffset": 37}, {"referenceID": 12, "context": "Traditional convolutional feed-forward networks connect the output of the ` layer as input to the (`+1) layer (Krizhevsky et al., 2012), which gives rise to the following layer transition x` = H`(x`\u22121).", "startOffset": 110, "endOffset": 135}, {"referenceID": 4, "context": "(2016) we define H`(\u00b7) as a composite function of three consecutive operations: Batch Normalization (BN) (Ioffe & Szegedy, 2015), followed by a rectified linear unit (ReLU) (Glorot et al., 2011), and then Convolution (Conv).", "startOffset": 173, "endOffset": 194}, {"referenceID": 5, "context": "Motivated by He et al. (2016) we define H`(\u00b7) as a composite function of three consecutive operations: Batch Normalization (BN) (Ioffe & Szegedy, 2015), followed by a rectified linear unit (ReLU) (Glorot et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 0, "context": "The implementation is in Torch 7 (Collobert et al., 2011).", "startOffset": 33, "endOffset": 57}, {"referenceID": 20, "context": "A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32\u00d732 images; half of the images are then horizontally mirrored.", "startOffset": 68, "endOffset": 237}, {"referenceID": 23, "context": "A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32\u00d732 images; half of the images are then horizontally mirrored.", "startOffset": 68, "endOffset": 237}, {"referenceID": 9, "context": "A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32\u00d732 images; half of the images are then horizontally mirrored.", "startOffset": 68, "endOffset": 237}, {"referenceID": 13, "context": "A standard data augmentation scheme is widely used for this dataset (Lin et al., 2013; Romero et al., 2014; Lee et al., 2015; Springenberg et al., 2014; Srivastava et al., 2015; He et al., 2015b; Huang et al., 2016; Larsson et al., 2016): the images are first zero-padded with 4 pixels on each side, then randomly cropped to again produce 32\u00d732 images; half of the images are then horizontally mirrored.", "startOffset": 68, "endOffset": 237}, {"referenceID": 17, "context": "The Street View House Numbers (SVHN) dataset (Netzer et al., 2011) contains 32\u00d732 colored digit images coming from Google Street View.", "startOffset": 45, "endOffset": 66}, {"referenceID": 22, "context": "Following common practice (Sermanet et al., 2012; Goodfellow et al., 2013; Lin et al., 2013; Lee et al., 2015; Huang et al., 2016) we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set.", "startOffset": 26, "endOffset": 130}, {"referenceID": 9, "context": "Following common practice (Sermanet et al., 2012; Goodfellow et al., 2013; Lin et al., 2013; Lee et al., 2015; Huang et al., 2016) we use all the training data without any data augmentation, and a validation set with 6,000 images is split from the training set.", "startOffset": 26, "endOffset": 130}, {"referenceID": 23, "context": "35 All-CNN (Springenberg et al., 2014) - - 9.", "startOffset": 11, "endOffset": 38}, {"referenceID": 13, "context": "39 FractalNet (Larsson et al., 2016) 21 38.", "startOffset": 14, "endOffset": 36}, {"referenceID": 9, "context": "01 ResNet with Stochastic Depth (Huang et al., 2016) 110 1.", "startOffset": 32, "endOffset": 52}, {"referenceID": 8, "context": "64 ResNet (pre-activation) (He et al., 2016) 164 1.", "startOffset": 27, "endOffset": 44}, {"referenceID": 6, "context": "87 ResNet (He et al., 2015b) 110 1.7M - 6.61 - - ResNet (reported by Huang et al. (2016)) 110 1.", "startOffset": 11, "endOffset": 89}, {"referenceID": 25, "context": "9 and Nesterov momentum (Sutskever et al., 2013) with 0 dampening.", "startOffset": 24, "endOffset": 48}, {"referenceID": 6, "context": "We adopt the weight initialization introduced by He et al. (2015a), and also equip the network with batch normalization (Ioffe & Szegedy, 2015).", "startOffset": 49, "endOffset": 67}, {"referenceID": 6, "context": "It also indicates that they do not suffer from overfitting or the optimization difficulties pointed out by He et al. (2015b). In fact, it is possible that a deeper DenseNet with more parameters could achieve even better results.", "startOffset": 107, "endOffset": 125}, {"referenceID": 8, "context": "7M parameters (He et al., 2016).", "startOffset": 14, "endOffset": 31}, {"referenceID": 12, "context": "In comparison with other popular network architectures, like AlexNet (Krizhevsky et al., 2012) or VGG-net (Russakovsky et al.", "startOffset": 69, "endOffset": 94}, {"referenceID": 21, "context": ", 2012) or VGG-net (Russakovsky et al., 2015), ResNets with pre-activation use fewer parameters while typically achieving better results (He et al.", "startOffset": 19, "endOffset": 45}, {"referenceID": 8, "context": ", 2015), ResNets with pre-activation use fewer parameters while typically achieving better results (He et al., 2016).", "startOffset": 99, "endOffset": 116}, {"referenceID": 2, "context": ", Gardner et al. (2015) or Gatys et al.", "startOffset": 2, "endOffset": 24}, {"referenceID": 2, "context": ", Gardner et al. (2015) or Gatys et al. (2015). Finally, we believe that there may be further gains in accuracy obtainable through more detailed fine-tuning of hyper-parameters or learning rate schedules.", "startOffset": 2, "endOffset": 47}], "year": 2016, "abstractText": "Recent work has shown that convolutional networks can be substantially deeper, more accurate and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper we embrace this observation and introduce the Dense Convolutional Network (DenseNet), where each layer is directly connected to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections, one between each layer and its subsequent layer (treating the input as layer 0), our network has L(L+1) 2 direct connections. For each layer, the feature maps of all preceding layers are treated as separate inputs whereas its own feature maps are passed on as inputs to all subsequent layers. Our proposed connectivity pattern has several compelling advantages: it alleviates the vanishing gradient problem and strengthens feature propagation; despite the increase in connections, it encourages feature reuse and leads to a substantial reduction of parameters; its models tend to generalize surprisingly well. We evaluate our proposed architecture on five highly competitive object recognition benchmark tasks. The DenseNet obtains significant improvements over the state-of-the-art on all five of them (e.g., yielding 3.74% test error on CIFAR-10, 19.25% on CIFAR-100 and 1.59% on SVHN).", "creator": "LaTeX with hyperref package"}}}