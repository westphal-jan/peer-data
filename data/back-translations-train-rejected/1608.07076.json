{"id": "1608.07076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "A Context-aware Natural Language Generator for Dialogue Systems", "abstract": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users' way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.", "histories": [["v1", "Thu, 25 Aug 2016 10:43:56 GMT  (125kb,D)", "http://arxiv.org/abs/1608.07076v1", "Accepted as a short paper for SIGDIAL 2016"]], "COMMENTS": "Accepted as a short paper for SIGDIAL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ond\\v{r}ej du\\v{s}ek", "filip jur\\v{c}\\'i\\v{c}ek"], "accepted": false, "id": "1608.07076"}, "pdf": {"name": "1608.07076.pdf", "metadata": {"source": "CRF", "title": "A Context-aware Natural Language Generator for Dialogue Systems", "authors": ["Ond\u0159ej Du\u0161ek"], "emails": ["odusek@ufal.mff.cuni.cz", "jurcicek@ufal.mff.cuni.cz"], "sections": [{"heading": "1 Introduction", "text": "In a conversation, speakers are influenced by previous statements of their colleagues and tend to adapt their way of speaking to each other (\"Align, Entr\u00e9e\") by reusing lexical elements and syntactic structures (\"Reitter et al., 2006\"). In spoken dialogue systems (\"SDS\"), users are naturally and unconsciously encouraged to have successful conversations (\"Parent\" and \"Eskenazi,\" 2010). The function of \"Natural Language Generation\" (\"NLG et al., 2008\") is to produce a natural source of variation in dialogues (\"SDS\")."}, {"heading": "2 Our generator", "text": "Our seq2seq generator is an improved version of the Seq2seq generator by Dus-ek and Jurc-ic-ek (2016b), which in turn is carefully based on the seq2seq model (Bahdanau et al., 2015, see Figure 2) as implemented in the framework of TensorFlow (Abadi et al., 2015).1 We first describe the base model in Section 2.1 and then list our context-related improvements in Section 2.2."}, {"heading": "2.1 Baseline Seq2seq NLG with Attention", "text": "The generation has two stages: The first, encoder stage uses a recurrent neural network (RNN), consisting of maximum short-term memory cells (LSTM) (Hochreiter and Schmidhuber, 1997; Graves, 2013), to convert a sequence of input tokens2 x = {x1,.., xn} into a sequence of hidden states h = {h1,..., hn}: ht = lstm (xt, ht \u2212 1) (1) The second, decoder stage then uses the hidden states h = {y1,.,."}, {"heading": "2.2 Making the Generator Context-aware", "text": "We have implemented three different modifications to our generator that make its output dependent on the previous context: 4Prepending context. The previous user statement is simply prefixed to the DA and fed into the encoder (see fig. 2). The dictionary of context statements is different from the DA token dictionary.Context encoder. We add another, separate encoder for the context statements. The hidden states of both encoders are concatenated, and the decoder then works with double-sized vectors both on the input and in the attention model (see fig. 2).n-gram match reranchor. We added a second reranchor for the k-best outputs of the generator, and the decoder then works with double-sized vectors on both the input and the attention model (see fig. 2)."}, {"heading": "3 Experiments", "text": "We are experimenting with the publicly available dataset of Dus-ek and Jurc-i-c-ek (2016a) 6 for NLG in the pub-5We are not using a shortening penalty because we do not want to degrade shorter output sequences. However, the addition to the formula in our preliminary experiments led to results similar to those presented here. 6The dataset will be released at http: / / hdl.handle. net / 11234 / 1-1675; we used a newer version of GitHub (https: / github.com / UFAL-DSG / alexlic Transport Information Domain, which includes the previous context along with each pair of input-DA and target set of natural language. It contains more than 5,500 expressions, i.e. three images for each of the over 1,800 combinations of input-DA and context-user education."}, {"heading": "3.1 Evaluation Using Automatic Metrics", "text": "Table 1 lists our results on the test data relating to the BLEU and NIST metrics (Papineni et al., 2002; Doddington, 2002). We can see that while the n-gram match reranchor improves the BLEU score by using context-dependent or separate encoder results that are lower than baseline.10 However, we believe that the use of the n-gram match reranchor together with the context-dependent or separate encoder brings significant improvements of about 2.8 BLEU points in both cases, better than the use of the n-gram match reranchor alone. 11 We believe that adding the context information to the decoder separate encoder kbest lists increases the likelihood that context-based output will appear on the decoder kit lists, but it also leads to much greater uncertainty and therefore the corresponding output may not be at the top of the decoder list alone."}, {"heading": "3.2 Human Evaluation", "text": "In a blind, pair-by-pair preference test with untrained judges recruited with on9w, the value was set to 5 if the n-gram match reranchor itself is executed or combined with the separate encoder. 10 In our experiments with development data, all three methods brought a mild BLEU improvement. 11 The statistical significance at 99% level was evaluated using pair-by-pair bootstrap resampling (Koehn, 2004). The CrowdFlower crowdsourcing platform gave the judges the context and system output for the baseline and the context-sensitive system, and they were asked to select the variant that sounded more natural. We used a random sample of 1,000 pairs of different system issues across all 5 random initializations of the networks and collected 3 judgments for each of them."}, {"heading": "4 Related Work", "text": "Our system is an evolutionary improvement over the LSTM seq2seq system from Dus-Eek and Jurc-Escek (2016b), and as such it is architecturally most closely related to other newer RNN-based approaches to NLG that are not context-aware: the RNN generation with a revolutionary reanchor from Wen et al. (2015a) and an improved LSTM-based version (Wen et al., 2015b), as well as the LSTM encoder aligner decoder NLG system from Mei et al. (2015). The most recent end-to-end portable SDS from Wen et al. (2016) has an implicit access to the previous context, but the authors do not focus on its influence on the responses generated. There have been several attempts to model the drainage in dialogue (Brockmann et al, 2005; Reitter et al, 2006; Buschmeier) and the successful rain systems from Lopyentmeier (12.com)."}, {"heading": "5 Conclusions and Further Work", "text": "We introduced an improvement to our natural language generator based on the sequence-to-equence approach (Dus-ek and Jurc-ic-ek, 2016b), which enabled it to exploit previous context statements to adapt to the user's speech style and provide more context-aware and less repetitive responses. We used two different methods to feed the previous context into the generator and an anchor based on an N-gram match against the context. Evaluation of our context-aware data set (Dus-ek and Jurc-ic-ic-ek, 2016a) showed a significant improvement in the BLEU score for combining the two approaches, which was confirmed in a subsequent human pair preference test. Our generator is available on GitHub at the following address: https: / / github.com / UFAL-DSG / tgenIn future work, we plan to improve the forms of the same word preference test to allow the automated matching of the QS with the previous context, for example."}, {"heading": "Acknowledgments", "text": "This work was financed by the Ministry of Education, Youth and Sport of the Czech Republic under the funding agreement LK11221 and nuclear research funding, the SIA project 260 333 and the GAUK project 2058214 of Charles University in Prague using language resources stored and distributed in the LINDAT / CLARIN project of the Ministry of Education, Youth and Sport of the Czech Republic (project LM2015071)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "International Conference on Learning Representations. arXiv:1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A neural probabilistic language model", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin."], "venue": "Journal of Machine Learning Research, 3:1137\u20131155.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Modelling alignment for affective dialogue", "author": ["C. Brockmann", "A. Isard", "J. Oberlander", "M. White."], "venue": "Workshop on Adapting the Interaction Style to Affective Factors at the 10th International Conference on User Modeling.", "citeRegEx": "Brockmann et al\\.,? 2005", "shortCiteRegEx": "Brockmann et al\\.", "year": 2005}, {"title": "Modelling and evaluation of lexical and syntactic alignment with a priming-based microplanner", "author": ["H. Buschmeier", "K. Bergmann", "S. Kopp."], "venue": "Empirical Methods in Natural Language Generation, number 5790 in Lecture Notes in Computer", "citeRegEx": "Buschmeier et al\\.,? 2010", "shortCiteRegEx": "Buschmeier et al\\.", "year": 2010}, {"title": "Automatic evaluation of machine translation quality using N-gram cooccurrence statistics", "author": ["G. Doddington."], "venue": "Proceedings of the Second International Conference on Human Language Technology Research, pages 138\u2013145.", "citeRegEx": "Doddington.,? 2002", "shortCiteRegEx": "Doddington.", "year": 2002}, {"title": "Jur\u010d\u0131\u0301\u010dek. 2016a. A context-aware natural language generation dataset for dialogue systems", "author": ["F.O. Du\u0161ek"], "venue": "In Workshop on Collecting and Generating Resources for Chatbots and Conversational Agents - Development and Evaluation,", "citeRegEx": "Du\u0161ek,? \\Q2016\\E", "shortCiteRegEx": "Du\u0161ek", "year": 2016}, {"title": "Jur\u010d\u0131\u0301\u010dek. 2016b. Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings. arXiv:1606.05491", "author": ["F.O. Du\u0161ek"], "venue": "To appear in Proceedings of ACL", "citeRegEx": "Du\u0161ek,? \\Q2016\\E", "shortCiteRegEx": "Du\u0161ek", "year": 2016}, {"title": "Lexical entrainment and success in student engineering groups", "author": ["H. Friedberg", "D. Litman", "S.B.F. Paletz."], "venue": "Proc. of SLT, pages 404\u2013409.", "citeRegEx": "Friedberg et al\\.,? 2012", "shortCiteRegEx": "Friedberg et al\\.", "year": 2012}, {"title": "Generating sequences with recurrent neural networks", "author": ["A. Graves."], "venue": "arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Long shortterm memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Entrainment in pedestrian direction giving: How many kinds of entrainment", "author": ["Z. Hu", "G. Halberg", "C. Jimenez", "M. Walker."], "venue": "Proc. of IWSDS, pages 90\u2013101.", "citeRegEx": "Hu et al\\.,? 2014", "shortCiteRegEx": "Hu et al\\.", "year": 2014}, {"title": "Alex: A statistical dialogue systems framework", "author": ["F. Jur\u010d\u0131\u0301\u010dek", "O. Du\u0161ek", "O. Pl\u00e1tek", "L. \u017dilka"], "venue": "In Proc. of Text, Speech and Dialogue,", "citeRegEx": "Jur\u010d\u0131\u0301\u010dek et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jur\u010d\u0131\u0301\u010dek et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba."], "venue": "International Conference on Learning Representations. arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Statistical significance tests for machine translation evaluation", "author": ["P. Koehn."], "venue": "Proceedings of EMNLP, pages 388\u2013395.", "citeRegEx": "Koehn.,? 2004", "shortCiteRegEx": "Koehn.", "year": 2004}, {"title": "Automated two-way entrainment to improve spoken dialog system performance", "author": ["J. Lopes", "M. Eskenazi", "I. Trancoso."], "venue": "Proc. of ICASSP, pages 8372\u20138376.", "citeRegEx": "Lopes et al\\.,? 2013", "shortCiteRegEx": "Lopes et al\\.", "year": 2013}, {"title": "From rule-based to data-driven lexical entrainment models in spoken dialog systems", "author": ["J. Lopes", "M. Eskenazi", "I. Trancoso."], "venue": "Computer Speech & Language, 31(1):87\u2013112.", "citeRegEx": "Lopes et al\\.,? 2015", "shortCiteRegEx": "Lopes et al\\.", "year": 2015}, {"title": "What to talk about and how? selective generation using LSTMs with coarse-to-fine alignment", "author": ["H. Mei", "M. Bansal", "M.R. Walter."], "venue": "arXiv:1509.00838.", "citeRegEx": "Mei et al\\.,? 2015", "shortCiteRegEx": "Mei et al\\.", "year": 2015}, {"title": "High frequency word entrainment in spoken dialogue", "author": ["A. Nenkova", "A. Gravano", "J. Hirschberg."], "venue": "Proc. of ACL-HLT, pages 169\u2013172.", "citeRegEx": "Nenkova et al\\.,? 2008", "shortCiteRegEx": "Nenkova et al\\.", "year": 2008}, {"title": "BLEU: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu."], "venue": "Proc. of ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Lexical entrainment of real users in the Let\u2019s Go spoken dialog system", "author": ["G. Parent", "M. Eskenazi."], "venue": "Proc. of Interspeech, pages 3018\u20133021.", "citeRegEx": "Parent and Eskenazi.,? 2010", "shortCiteRegEx": "Parent and Eskenazi.", "year": 2010}, {"title": "Computational modelling of structural priming in dialogue", "author": ["D. Reitter", "F. Keller", "J.D. Moore."], "venue": "Proc. of NAACL-HLT: Short Papers, pages 121\u2013 124.", "citeRegEx": "Reitter et al\\.,? 2006", "shortCiteRegEx": "Reitter et al\\.", "year": 2006}, {"title": "Lexical and syntactic priming and their impact in deployed spoken dialog systems", "author": ["S. Stoyanchev", "A. Stent."], "venue": "Proc. of NAACL-HLT, pages 189\u2013 192.", "citeRegEx": "Stoyanchev and Stent.,? 2009", "shortCiteRegEx": "Stoyanchev and Stent.", "year": 2009}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q. VV Le."], "venue": "Advances in Neural Information Processing Systems, pages 3104\u20133112. arXiv:1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Stochastic language generation in dialogue using recurrent neural networks with convolutional sentence reranking", "author": ["T.-H. Wen", "M. Gasic", "D. Kim", "N. Mrksic", "P.-H. Su", "D. Vandyke", "S. Young."], "venue": "Proc. of SIGDIAL, pages 275\u2013284.", "citeRegEx": "Wen et al\\.,? 2015a", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "Semantically conditioned LSTM-based natural language generation for spoken dialogue systems", "author": ["T.-H. Wen", "M. Gasic", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "S. Young."], "venue": "Proc. of EMNLP, pages 1711\u20131721.", "citeRegEx": "Wen et al\\.,? 2015b", "shortCiteRegEx": "Wen et al\\.", "year": 2015}, {"title": "A network-based endto-end trainable task-oriented dialogue system", "author": ["T.-H. Wen", "M. Ga\u0161i\u0107", "N. Mrk\u0161i\u0107", "L.M. RojasBarahona", "P.-H. Su", "S. Ultes", "D. Vandyke", "S. Young."], "venue": "arXiv:1604.04562.", "citeRegEx": "Wen et al\\.,? 2016", "shortCiteRegEx": "Wen et al\\.", "year": 2016}, {"title": "The hidden information state model: A practical framework for POMDP-based spoken dialogue management", "author": ["S. Young", "M. Ga\u0161i\u0107", "S. Keizer", "F. Mairesse", "J. Schatzmann", "B. Thomson", "K. Yu."], "venue": "Computer Speech & Language, 24(2):150\u2013", "citeRegEx": "Young et al\\.,? 2010", "shortCiteRegEx": "Young et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 20, "context": "In a conversation, speakers are influenced by previous utterances of their counterparts and tend to adapt (align, entrain) their way of speaking to each other, reusing lexical items as well as syntactic structure (Reitter et al., 2006).", "startOffset": 213, "endOffset": 235}, {"referenceID": 7, "context": "Entrainment occurs naturally and subconsciously, facilitates successful conversations (Friedberg et al., 2012; Nenkova et al., 2008), and forms a natural source of variation in dialogues.", "startOffset": 86, "endOffset": 132}, {"referenceID": 17, "context": "Entrainment occurs naturally and subconsciously, facilitates successful conversations (Friedberg et al., 2012; Nenkova et al., 2008), and forms a natural source of variation in dialogues.", "startOffset": 86, "endOffset": 132}, {"referenceID": 19, "context": "In spoken dialogue systems (SDS), users were reported to entrain to system prompts (Parent and Eskenazi, 2010).", "startOffset": 83, "endOffset": 110}, {"referenceID": 26, "context": "The function of natural language generation (NLG) components in task-oriented SDS typically is to produce a natural language sentence from a dialogue act (DA) (Young et al., 2010) representing an action, such as inform or request, along with one or more attributes (slots) and their values (see Fig.", "startOffset": 159, "endOffset": 179}, {"referenceID": 21, "context": "NLG is an important component of SDS which has a great impact on the perceived naturalness of the system; its quality can also influence the overall task success (Stoyanchev and Stent, 2009; Lopes et al., 2013).", "startOffset": 162, "endOffset": 210}, {"referenceID": 14, "context": "NLG is an important component of SDS which has a great impact on the perceived naturalness of the system; its quality can also influence the overall task success (Stoyanchev and Stent, 2009; Lopes et al., 2013).", "startOffset": 162, "endOffset": 210}, {"referenceID": 11, "context": "To avoid repetition and add variation into the outputs, they typically alternate between a handful of preset variants (Jur\u010d\u0131\u0301\u010dek et al., 2014) or use overgeneration and random sampling from a k-best list of outputs (Wen et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 24, "context": ", 2014) or use overgeneration and random sampling from a k-best list of outputs (Wen et al., 2015b).", "startOffset": 80, "endOffset": 99}, {"referenceID": 0, "context": "Our system is an extension of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b)\u2019s generator based on sequence-to-sequence (seq2seq) models with attention (Bahdanau et al., 2015).", "startOffset": 132, "endOffset": 155}, {"referenceID": 4, "context": "Our system is an extension of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b)\u2019s generator based on sequence-to-sequence (seq2seq) models with attention (Bahdanau et al.", "startOffset": 30, "endOffset": 58}, {"referenceID": 0, "context": "Our system is an extension of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b)\u2019s generator based on sequence-to-sequence (seq2seq) models with attention (Bahdanau et al., 2015). It is, to our knowledge, the first fully trainable entrainment-enabled NLG system for SDS. We also present our first results on the dataset of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016a), which includes the preceding user utterance along with each data instance (i.", "startOffset": 133, "endOffset": 328}, {"referenceID": 4, "context": "Our seq2seq generator is an improved version of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b)\u2019s generator, which itself is based on the seq2seq model with attention (Bahdanau et al.", "startOffset": 48, "endOffset": 76}, {"referenceID": 9, "context": "The generation has two stages: The first, encoder stage uses a recurrent neural network (RNN) composed of long-short-term memory (LSTM) cells (Hochreiter and Schmidhuber, 1997; Graves, 2013) to encode a sequence of input tokens2 x = {x1, .", "startOffset": 142, "endOffset": 190}, {"referenceID": 8, "context": "The generation has two stages: The first, encoder stage uses a recurrent neural network (RNN) composed of long-short-term memory (LSTM) cells (Hochreiter and Schmidhuber, 1997; Graves, 2013) to encode a sequence of input tokens2 x = {x1, .", "startOffset": 142, "endOffset": 190}, {"referenceID": 0, "context": "See (Du\u0161ek and Jur\u010d\u0131\u0301\u010dek, 2016b) and (Bahdanau et al., 2015) for a more formal description of the base model.", "startOffset": 37, "endOffset": 60}, {"referenceID": 1, "context": "Embeddings are used (Bengio et al., 2003), i.", "startOffset": 20, "endOffset": 41}, {"referenceID": 22, "context": "3 The generator supports greedy decoding as well as beam search which keeps track of top k most probable output sequences at each time step (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 140, "endOffset": 187}, {"referenceID": 0, "context": "3 The generator supports greedy decoding as well as beam search which keeps track of top k most probable output sequences at each time step (Sutskever et al., 2014; Bahdanau et al., 2015).", "startOffset": 140, "endOffset": 187}, {"referenceID": 18, "context": ", BLEU-2 (Papineni et al., 2002) without brevity penalty.", "startOffset": 9, "endOffset": 32}, {"referenceID": 5, "context": "We experiment on the publicly available dataset of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016a)6 for NLG in the pub-", "startOffset": 51, "endOffset": 79}, {"referenceID": 23, "context": ", are replaced by placeholders (Wen et al., 2015a).", "startOffset": 31, "endOffset": 50}, {"referenceID": 12, "context": "Same as Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b), we train the seq2seq models by minimizing cross-entropy on the training set using the Adam optimizer (Kingma and Ba, 2015), and we measure BLEU on the development set after each pass over the training data, selecting the best-performing parameters.", "startOffset": 138, "endOffset": 159}, {"referenceID": 5, "context": "Same as Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b), we train the seq2seq models by minimizing cross-entropy on the training set using the Adam optimizer (Kingma and Ba, 2015), and we measure BLEU on the development set after each pass over the training data, selecting the best-performing parameters.", "startOffset": 8, "endOffset": 36}, {"referenceID": 18, "context": "Table 1 lists our results on the test data in terms of the BLEU and NIST metrics (Papineni et al., 2002; Doddington, 2002).", "startOffset": 81, "endOffset": 122}, {"referenceID": 4, "context": "Table 1 lists our results on the test data in terms of the BLEU and NIST metrics (Papineni et al., 2002; Doddington, 2002).", "startOffset": 81, "endOffset": 122}, {"referenceID": 24, "context": "In addition to BLEU and NIST scores, we measured the slot error rate ERR (Wen et al., 2015b), i.", "startOffset": 73, "endOffset": 92}, {"referenceID": 13, "context": "Statistical significance at 99% level has been assessed using pairwise bootstrap resampling (Koehn, 2004).", "startOffset": 92, "endOffset": 105}, {"referenceID": 24, "context": "(2015a) and an improved LSTM-based version (Wen et al., 2015b), as well as the LSTM encoder-aligner-decoder NLG system of Mei et al.", "startOffset": 43, "endOffset": 62}, {"referenceID": 5, "context": "Our system is an evolutionary improvement over the LSTM seq2seq system of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al.", "startOffset": 74, "endOffset": 102}, {"referenceID": 5, "context": "Our system is an evolutionary improvement over the LSTM seq2seq system of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al. (2015a) and an improved LSTM-based version (Wen et al.", "startOffset": 74, "endOffset": 300}, {"referenceID": 5, "context": "Our system is an evolutionary improvement over the LSTM seq2seq system of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al. (2015a) and an improved LSTM-based version (Wen et al., 2015b), as well as the LSTM encoder-aligner-decoder NLG system of Mei et al. (2015). The recent end-to-end trainable SDS of Wen et al.", "startOffset": 74, "endOffset": 432}, {"referenceID": 5, "context": "Our system is an evolutionary improvement over the LSTM seq2seq system of Du\u0161ek and Jur\u010d\u0131\u0301\u010dek (2016b) and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by Wen et al. (2015a) and an improved LSTM-based version (Wen et al., 2015b), as well as the LSTM encoder-aligner-decoder NLG system of Mei et al. (2015). The recent end-to-end trainable SDS of Wen et al. (2016) does have an implicit access to previous context, but the authors do not focus on its influence on the generated responses.", "startOffset": 74, "endOffset": 490}, {"referenceID": 2, "context": "There have been several attempts at modelling entrainment in dialogue (Brockmann et al., 2005; Reitter et al., 2006; Buschmeier et al., 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 20, "context": "There have been several attempts at modelling entrainment in dialogue (Brockmann et al., 2005; Reitter et al., 2006; Buschmeier et al., 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 3, "context": "There have been several attempts at modelling entrainment in dialogue (Brockmann et al., 2005; Reitter et al., 2006; Buschmeier et al., 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al.", "startOffset": 70, "endOffset": 141}, {"referenceID": 10, "context": ", 2010) and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses (Hu et al., 2014) or increased naturalness and task success (Lopes et al.", "startOffset": 176, "endOffset": 193}], "year": 2016, "abstractText": "We present a novel natural language generation system for spoken dialogue systems capable of entraining (adapting) to users\u2019 way of speaking, providing contextually appropriate responses. The generator is based on recurrent neural networks and the sequence-to-sequence approach. It is fully trainable from data which include preceding context along with responses to be generated. We show that the context-aware generator yields significant improvements over the baseline in both automatic metrics and a human pairwise preference test.", "creator": "LaTeX with hyperref package"}}}