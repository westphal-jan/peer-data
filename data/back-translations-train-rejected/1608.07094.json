{"id": "1608.07094", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "A Novel Term_Class Relevance Measure for Text Categorization", "abstract": "In this paper, we introduce a new measure called Term_Class relevance to compute the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis.", "histories": [["v1", "Thu, 25 Aug 2016 11:46:06 GMT  (477kb)", "http://arxiv.org/abs/1608.07094v1", "12 pages, 6 figures, 2 tables"], ["v2", "Wed, 14 Sep 2016 12:51:50 GMT  (311kb)", "http://arxiv.org/abs/1608.07094v2", "12 pages, 6 figures, 2 tables"]], "COMMENTS": "12 pages, 6 figures, 2 tables", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["d s guru", "mahamad suhil"], "accepted": false, "id": "1608.07094"}, "pdf": {"name": "1608.07094.pdf", "metadata": {"source": "CRF", "title": "A Novel Term_Class Relevance Measure for Text Categorization", "authors": ["D S Guru", "Mahamad Suhil"], "emails": [], "sections": [{"heading": null, "text": "the relevance of a term in the classification of a document into a particular class. The proposed measure estimates the degree of relevance of a given term by placing an unlabelled document as a member of a known class, as a product of Class _ Term weight and Class _ Term density; where the Class _ Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term, and the Class _ Term density is the relative density of the occurrence of the term in the class to the overall occurrence of the term in the total population. Unlike other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term in all documents of the class in the total population. To demonstrate the meaning of the proposed measure, experiments were conducted with the 20 newsgroups."}, {"heading": "1. Introduction", "text": "For several decades, the automatic content-based classification of documents from vast collections has become an active field of research due to the fact that electronic data has grown uncontrollably large over the Internet and is growing exponentially day by day. Manual, day-based classification has become less important due to the enormous size of the data to be processed and the inability of tags to describe the content of documents. Various text classification applications that are currently in demand, such as filtering spam in emails, classifying e-books, classifying news documents, classifying text data from social networks, etc., have also led researchers to explore various ways of analysing and displaying this data, so that it can be quickly and efficiently recovered and managed."}, {"heading": "1.1 A review of the available term weighting schemes", "text": "Since our work focuses on proposing a new weighting scheme, but not on the classification framework, we are looking at the literature only on different weighting schemes. Terms are the basic units of information of each text document. Therefore, all weighting schemes developed in the literature measure the weight of a term when representing the content of a document [1-5. Depending on whether or not the document belongs to predefined categories intended to determine the weight of a term, the weighting schemes are generally divided into two classes, namely unattended weighting schemes and supervised weighting schemes. In the following sections, we give an overview of both the weighting schemes and the techniques they have adopted."}, {"heading": "1.1.1 Unsupervised term weighting schemes", "text": "The traditional evaluation methods borrowed from the IR, such as binary, term frequency (TF), TFIDF and their various variants, are unattended schemes [2]. The TF-IDF and its variants proposed by Jones are the most commonly used term weighting schemes for text classification. Some of the variants of TF are Raw Term Frequency, Log (TF), Log (TF + 1) or Log (TF) + 1 [1-2]. If ni is the number of documents containing the term and N is the number of documents in the collection, then the variants of IDF are 1 / ni, Log (1 / ni), Log (N / ni) + 1 and Log (N / ni) [1]."}, {"heading": "1.1.2 Supervised term weighting schemes", "text": "The weighted terms were developed primarily for text categorization because the fact that a verified knowledge is provided on the class labels of the training copies (1-4). (All verified term weight programs are further classified into subcategories based on whether the weighting estimates of a term are used in the preservation of document contents or the relevance of a term in the placement of a document as a member of a class.) It will be more effective to use weighting schemes that are used to measure the relevance of a term in the preservation of document contents and those that can be used to categorize the relevance of a document as a member of a class. Term-Document Relevance Measurement These measures are useful to select a discriminatory subset of terms for the presentation of a document by weighting the terms according to their relevance in the document class."}, {"heading": "2. A New Term_Class Relevance Measure", "text": "This year it has come to the point where it will be able to realize the aforementioned hreeisrcW, \"he said.\" We have to be able, \"he said.\" We have to be in the position we are in, \"he said.\" We have to be in the position we are in, \"he said.\" We have to be in the position we are in, \"he said.\" We have to be in the position we are in, \"he said.\" We have to be in the position we are in, \"he said."}, {"heading": "3. Classification with SVM and k-NN classifiers", "text": "In order to measure the applicability of the proposed concept of relevance, we use SVM as a learning algorithm to perform classifications on the basis of its good generalization capability. In addition, the training burden for SVM is very low, although the time for training is directly proportional, since the representatives of the class are equal. We are considering the effectiveness of the proposed relevance measure, which we have discussed with the SVM classifier."}, {"heading": "4. Comparative Analysis", "text": "In this section, we offer a quantitative comparative analysis of the proposed Term _ Class Relevance yardstick with the results of Isa et al., [20] in Table 3. The results that correspond [20] were extracted directly from the paper, since the proposed Term _ Class Relevance yardstick is the same in both papers, and they also provided the results of the same 20 Newsgroups dataset, using only SVM classifiers with different cores. Table 3 also shows that the proposed Term _ Class Relevance ystick exceeds the measurement yardstick used by Isa et al., [20]. Together with SVM, we compare the results using the results of the k-NN classifier with k = 10. Table 3 shows that the k-NN classifier with k = 10 has improved results when compared to SVM with all cores for both the relevance yardstick and the relevance yardstick. Therefore, we recommend using a better classification yardstick for N than SVM."}], "references": [{"title": "Term-Weighting Approaches in Automatic Text Retrieval, Information", "author": ["G. Salton", "C. Buckley"], "venue": "Processing and Management,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1988}, {"title": "Supervised Term Weighting for Automated Text Categorization", "author": ["F Debole", "F. Sebastiani"], "venue": "Proceedings of the 2003 ACM symposium on applied computing,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "Class-indexing-based term weighting for automatic text classification", "author": ["F Ren", "G. Sohrab M"], "venue": "Information Sciences", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Representation and Classification of Text Documents: A Brief Review", "author": ["S. Harish B", "S. Guru D", "S. Manjunath"], "venue": "IJCA Special Issue on \u201cRecent Trends in Image Processing and Pattern Recognition\u201d", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K.S. Jones"], "venue": "Journal of Documentation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1972}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": ["K.S. Jones"], "venue": "Journal of Documentation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2004}, {"title": "Analytical evaluation of term weighting schemes for text categorization", "author": ["H Alt\u0131n\u00e7ay", "Z. Erenel"], "venue": "Pattern Recognition Letters", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2010}, {"title": "Imbalanced text classification: A term weighting approach", "author": ["Y. Liu", "H.T. Loh", "A. Sun"], "venue": "Expert Systems with Applications", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Feature selection on hierarchy of web documents", "author": ["D. Mladenic", "M. Grobelnik"], "venue": "Decision Support Syst", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Machine learning in automated text categorization", "author": ["F. Sebastiani"], "venue": "ACM Comput. Surveys", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2002}, {"title": "A comparative study on feature selection in text categorization", "author": ["Y. Yang", "J.O. Pedersen"], "venue": "Proc. ICML\u201997, 14th Internat. Conf. on Machine Learning", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Toward integrating feature selection algorithms for classification and clustering", "author": ["H. Liu", "L. Yu"], "venue": "IEEE Trans. Knowledge Data Eng", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2005}, {"title": "Text categorization with class-based and corpusbased keyword selection", "author": ["A. Ozgur", "L. Ozgur", "T. Gungor"], "venue": "Proc. 20th Internat. Symp. on Computer and Information Sciences. Lecture Notes in Computer Science,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2005}, {"title": "Exploiting likely-positive and unlabeled data to improve the identification of protein\u2013protein interaction articles", "author": ["R.T. Tsai", "H. Hung", "H. Dai", "Y. Lin", "W. Hsu"], "venue": "BMC Bioinform", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2008}, {"title": "Inverse-Category-Frequency Based Supervised Term Weighting Schemes for Text Categorization", "author": ["D Wang", "H. Zhang"], "venue": "Journal of Information Science and Engineering", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams", "author": ["J Reed", "Y. Jiao", "E. Potok T", "B Klump", "M Elmore", "A Hurson"], "venue": "5th International Conference on Machine Learning and Applications. pp.258-263", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "AIR/X \u2013 A rule-based multistage indexing system for large subject fields", "author": ["N. Fuhr", "S. Hartmann", "G. Lustig", "M. Schwantner", "K. Tzeras", "Darmstadt", "T. H"], "venue": "Proceedings of the proceedings of RIAO(pp", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1991}, {"title": "Mineau, \u201cBeyond tfidf Weighting for Text Categorization in the Vector Space Model,\u201dProc", "author": ["G.W.P. Soucy"], "venue": "Int\u2019l Joint Conf. Artificial Intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2005}, {"title": "Text document preprocessing with the Bayes formula for classification using the support vector machine", "author": ["D. Isa", "L.H. Lee", "V.P. Kallimani", "R. Raj Kumar"], "venue": "IEEE Transactions on Knowledge and Data Engineering", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Using the self-organizing map for clustering of text documents", "author": ["D. Isa", "V.P. Kallimani", "L.H. Lee"], "venue": "Expert Systems with Applications", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Symbolic representation of text documents", "author": ["S. Guru D", "S. Harish B", "S. Manjunath"], "venue": "In Proceedings of Third Annual ACM Bangalore Conference", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5].", "startOffset": 125, "endOffset": 130}, {"referenceID": 1, "context": "So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5].", "startOffset": 125, "endOffset": 130}, {"referenceID": 2, "context": "So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5].", "startOffset": 125, "endOffset": 130}, {"referenceID": 3, "context": "So, all weighting schemes developed in the literature measure the weight of a term in representing the content of a document [1-5].", "startOffset": 125, "endOffset": 130}, {"referenceID": 0, "context": "The traditional term weighting methods borrowed from IR, such as binary, term frequency (TF), TFIDF, and its various variants are unsupervised schemes [2].", "startOffset": 151, "endOffset": 154}, {"referenceID": 4, "context": "The TF-IDF proposed by Jones [6, 7] and its variants are the most widely used term weighting schemes for text classification.", "startOffset": 29, "endOffset": 35}, {"referenceID": 5, "context": "The TF-IDF proposed by Jones [6, 7] and its variants are the most widely used term weighting schemes for text classification.", "startOffset": 29, "endOffset": 35}, {"referenceID": 0, "context": "Some of the variants of TF are Raw term frequency, log(TF), log(TF+1), or log(TF)+1[1-2].", "startOffset": 83, "endOffset": 88}, {"referenceID": 16, "context": "In [18], a novel inverse corpus frequency (ICF) based technique is proposed which computes the document representation in linear time.", "startOffset": 3, "endOffset": 7}, {"referenceID": 0, "context": "Supervised term weighting schemes were developed especially for text categorization because of the fact that a supervised knowledge on the class labels of the training samples is provided [1-4].", "startOffset": 188, "endOffset": 193}, {"referenceID": 1, "context": "Supervised term weighting schemes were developed especially for text categorization because of the fact that a supervised knowledge on the class labels of the training samples is provided [1-4].", "startOffset": 188, "endOffset": 193}, {"referenceID": 2, "context": "Supervised term weighting schemes were developed especially for text categorization because of the fact that a supervised knowledge on the class labels of the training samples is provided [1-4].", "startOffset": 188, "endOffset": 193}, {"referenceID": 0, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 1, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 2, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 6, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 7, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 8, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 9, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 10, "context": "Most frequently used techniques to replace IDF include chi-square measure (X ), Information Gain(IG), Gain Ratio, Mutual Information(MI), Odds Ratio(OR) [1-4,8-12].", "startOffset": 153, "endOffset": 163}, {"referenceID": 11, "context": "From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16].", "startOffset": 96, "endOffset": 106}, {"referenceID": 12, "context": "From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16].", "startOffset": 96, "endOffset": 106}, {"referenceID": 13, "context": "From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16].", "startOffset": 96, "endOffset": 106}, {"referenceID": 14, "context": "From past few years, many researchers have proposed alternative term-document relevance schemes [1, 13-16].", "startOffset": 96, "endOffset": 106}, {"referenceID": 12, "context": "In [14], a comparison of corpusbased and class-based keyword selection is proposed by using TF-IDF as weighting scheme.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "In [4], a class-indexing-based term weighting for automatic text classification is proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 18, "context": ", [20] using Bayes posterior probability.", "startOffset": 2, "endOffset": 6}, {"referenceID": 9, "context": "Though, some works make use of Bayes probability for representation, they have not clearly stated the advantage of the measure in classification [11, 18].", "startOffset": 145, "endOffset": 153}, {"referenceID": 16, "context": "Though, some works make use of Bayes probability for representation, they have not clearly stated the advantage of the measure in classification [11, 18].", "startOffset": 145, "endOffset": 153}, {"referenceID": 18, "context": "After [20], this measure was extensively used for term weighting [21, 22].", "startOffset": 6, "endOffset": 10}, {"referenceID": 19, "context": "After [20], this measure was extensively used for term weighting [21, 22].", "startOffset": 65, "endOffset": 73}, {"referenceID": 20, "context": "After [20], this measure was extensively used for term weighting [21, 22].", "startOffset": 65, "endOffset": 73}, {"referenceID": 18, "context": ", [20] also propose a text representation scheme which works with the reduced dimension for each document at the time of representation itself.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ",[20].", "startOffset": 1, "endOffset": 5}, {"referenceID": 18, "context": ", [20] make use of SVM as the classifier.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ", [20] as explained in section 1.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": ",[20] in Table 3.", "startOffset": 1, "endOffset": 5}, {"referenceID": 18, "context": "The results corresponding to [20] have been extracted directly from the paper as the representation scheme is same in both the works and they also have provided the results on the same 20Newsgroups dataset using only SVM classifier with different kernels.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": ",[20].", "startOffset": 1, "endOffset": 5}, {"referenceID": 18, "context": ",[20].", "startOffset": 1, "endOffset": 5}, {"referenceID": 18, "context": "Percentage of Training Results from [20] with SVM Results of Proposed Method SVM k-NN Linear RBF Polynomial Linear RBF Polynomial 30 83.", "startOffset": 36, "endOffset": 40}], "year": 2016, "abstractText": "In this paper, we introduce a new measure called Term_Class relevance to compute the relevancy of a term in classifying a document into a particular class. The proposed measure estimates the degree of relevance of a given term, in placing an unlabeled document to be a member of a known class, as a product of Class_Term weight and Class_Term density; where the Class_Term weight is the ratio of the number of documents of the class containing the term to the total number of documents containing the term and the Class_Term density is the relative density of occurrence of the term in the class to the total occurrence of the term in the entire population. Unlike the other existing term weighting schemes such as TF-IDF and its variants, the proposed relevance measure takes into account the degree of relative participation of the term across all documents of the class to the entire population. To demonstrate the significance of the proposed measure experimentation has been conducted on the 20 Newsgroups dataset. Further, the superiority of the novel measure is brought out through a comparative analysis.", "creator": "Microsoft\u00ae Word 2013"}}}