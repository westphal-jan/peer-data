{"id": "1608.07115", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Aligning Packed Dependency Trees: a theory of composition for distributional semantics", "abstract": "We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and generalization.", "histories": [["v1", "Thu, 25 Aug 2016 12:44:05 GMT  (294kb,D)", "http://arxiv.org/abs/1608.07115v1", "To appear in Special issue of Computational Linguistics - Formal Distributional Semantics"]], "COMMENTS": "To appear in Special issue of Computational Linguistics - Formal Distributional Semantics", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["david weir", "julie weeds", "jeremy reffin", "thomas kober"], "accepted": false, "id": "1608.07115"}, "pdf": {"name": "1608.07115.pdf", "metadata": {"source": "CRF", "title": "Aligning Packed Dependency Trees: a theory of composition for distributional semantics", "authors": ["David Weir", "Julie Weeds", "Jeremy Reffin", "Thomas Kober"], "emails": ["d.j.weir@sussex.ac.uk", "j.e.weeds@sussex.ac.uk", "j.p.reffin@sussex.ac.uk", "t.kober@sussex.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where there is only one occasion when there is a scandal, and that is when there is a scandal."}, {"heading": "2 The Distributional Lexicon", "text": "In this section, we begin the formalization of our proposal by describing the distribution dictionary: a collection of entries that characterize the distribution semantics of lexemes. Table 1 provides a summary of the spelling we use. Let V be a finite alphabet of lexemes2 in which each lexemes contains a portion of the speech tag; let R be a finite alphabet of grammatical dependency relationships; and let TV, R be the set of dependency trees in which each node is labeled with a V element, and each directed edge with an element of R. Figure 1 shows eight examples of dependency relationships."}, {"heading": "2.1 Typed Co-occurrences", "text": "& & # 8220; J & # 8222; J & # 8222; J & # 8222; J & # 8222; J & # 8222; J & # 8222; J & # 8222; J & # 8222; J & # 8222; J & # 8222; J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 8222; -J & # 82J; -J & # 8222; -J & # 222; -J & # 222; -J & # 2J; -2J; -222 & # 222 & # 2J; -2J; -2J & # 222 & # 222 & # 2J; -2J & # 222 & # 2J; -2J & # 222 & # 2J; -J & # 222 & # 2J & # 222 & # 2J; -J & # 2J; -2J & # 222 & # 2J & # 222 & # 2J; -J & # 2J; -2J & # 2J & # 222 & # 2J & # 222 & # 2J; -J & # 2J & # 2J & # 222 & # 2J; -J & # 2J & # 2J & 222 & # 2J & 2J & 2J & 2J; -J & 2J; -J & 2J & 2J & 222 & 2J & # 2J; -J & 2J & 2J & 2J; -J & 2J & 2J & 2J & 222 & 2J & 2J & 2J & 222 & 2J # 2J; J & 2J & 2J & 222 & 2J & 2J & 2J; J & 2J & 2J & 2J & 2J & 222 & J & 2J; J & 2J & J &"}, {"heading": "2.2 Anchored Packed Trees", "text": "Considering a dependency tree, whether it is a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a tree, or a clean tree, or a tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a clean tree, or a tree, or a clean tree, or a tree, or a clean tree, or a tree, or a clean tree, or a tree, or a tree, or a tree, or a clean tree, or a tree, or a tree, or a tree, or a clean tree, or a tree, or a tree, or a tree, or a tree, or a tree, or a clean tree, or a tree, or a tree, or a tree, or a tree, or a clean tree, or a tree, or a tree, or a tree, or a tree, or a clean tree, or a tree, or a clean tree, or a tree, or a tree, or a clean tree, or a tree, or a tree, or a tree, or a tree, or a pure, or a tree, or a tree, or a tree, or a pure, or a tree, or a tree, or a clean"}, {"heading": "3 Apt Similarity", "text": "One of the most fundamental aspects of any treatment of distribution semantics is that it supports a method of measuring distribution similarity (2005). In this section, we describe the way in which the similarity of two apts can be measured by mapping apts. (3) The vector space we use to define distribution characteristics includes one dimension for each element of the feed, and we use the pair in which they refer to their corresponding dimensions. (3) The vector space we use includes one dimension for each element of the feed, and we use the corresponding dimensions. (5) In the face of an apt A, we denote the vectorized representation of A with the value that the vector synchronizes."}, {"heading": "4 Distributional Composition", "text": "In this section, we turn to the central theme of the paper, namely distribution composition. We begin with an informal explanation of our approach and then present a more precise formalization."}, {"heading": "4.1 Discussion of Approach", "text": "The question of the \"why,\" according to the question of the \"why,\" \"why,\" \"why,\" and \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \"why,\" \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\", \"\", \"\", \",\" \",\", \",\" \",\", \",\", \"\", \",\", \"\", \",\" \",\" \",\", \"\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\" \",\" \",\", \"\" \"\" \"\", \"\" \",\" \"\" \"\", \"\" \",\", \"\" \"\", \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\" \",\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \",\" \"\" \"\", \"\" \"\" \"\", \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\""}, {"heading": "4.2 Offset Apts", "text": "Considering a certain offset, a string in R \u043c R \u043c, the apt A, if balanced by D. Considering the offset of an apt by D. involves moving the anchor to the position achieved by following the path from the original anchor position. To define the apt A, we must define the same node for each of these addresses (weighted lexeme multiset). As shown in Equation 7 below, the path compensation can be specified by using the co-event introduced in Section 2.2. Considering a string in R \u0432 R and an apt \u2032), the same node (weighted lexeme multiset) results. Equation 7 below shows that the path compensation can be specified by using the co-event type reduction operator introduced in Section 2.2."}, {"heading": "4.3 Syntax-driven Apt Alignment", "text": "We use offset apts as defined in Eq.7 as a way to align all apts with a dependency tree. Consider the following scenario: \u2022 w1... wn is a phrase (or sentence) in which each wi-V for 1 \u2264 i \u2264 n; \u2022 t-TV, R is a dependency analysis of the string w1... wn; \u2022 wh is the lexeme at the root of t. In other words, h is the position (index) in the phrase at which the head appears; \u2022 wi-T is the elementary apt for each i, 1 \u2264 i \u2264 n; and \u2022 i, the offset of wi in relation to the root, is the path in wi to wh. In other words, wi-i-i, wh-T is a co-occurrence in t for each i, 1 \u2264 i."}, {"heading": "4.4 Merging Aligned Apts", "text": "The question of how to implement the function that appears in Equation 9 is not new. \"We have a series of aligned apts,\" he says. \"There are only two possibilities.\" \"There is only one apt.\" \"There is only one apt,\" he says. \"There is only one apt that represents the semantic meaning of the dependency tree.\" \"So there is the question of how to merge the different nodes that are aligned with each other and form the nodes of the apt that is produced.\" \"The elementary apt for a lexeme expresses these co-compatible contexts.\" \"If the lexemes are articulated in some formulations, our goal is to grasp the extent to which the co-arrangements in the elementary apts as a whole are compatible.\""}, {"heading": "5 Experiments", "text": "In this section, we look at some empirical evidence in support of Apts. First, we look at some of the different ways in which Apts can be instantiated. Second, we present a number of case studies that show the unambiguous effect of Apts composition in adjective composition. Finally, we evaluate the model using the phrase-based compositivity benchmarks of Mitchell and Lapata (2008) and Mitchell and Lapata (2010)."}, {"heading": "5.1 Instantiating Apts", "text": "We have described apt lexicons of three different corpus types and dependencies. \u2022 It is a corpus that is used for case studies in 5.2 billion dollars. (These corpus types are usually able to identify themselves.) It is part of the POS tagged, lemmatized and dependency sparsed as in Grefenstette et al. (2013) and contains about 0.1 billion tokens. \u2022 BNC is a linkage of ukWaC corpus (Ferraresi et al.), 2008), a mid-2009 dump of English Wikipedia and the British National Corpus. This corpus is tokenized, lemmatized and dependent."}, {"heading": "5.2 Disambiguation", "text": "This year, it is only a matter of time before agreement is reached."}, {"heading": "5.3 Phrase-based Composition Tasks", "text": "Here we look at the performance of an instance of the apt framework using two benchmark tasks for the phrase-based composition."}, {"heading": "5.3.1 Experiment 1: the M&L2010 dataset", "text": "The first experiment uses the M & L2010 method of Baroni and Zamparelli (2010) and the associated method (2010), which contains human similarity judgments for adjective-noun (AN), noun-noun (NN), and verb-object (VO) combinations on a seven-point evaluation scale. It contains 108 combinations in each category, such as the social activity, economic condition, and economic situation of people moving in this area. This dataset has been used in a series of evaluations of compositional methods, including Mitchell and Lapata (2010), Blacoe and Lapata (2012), Turney (2012), Turney (2012), Hermann and Blunsom (2013), and Clark (2014). Blacoe and Lapata (2012) show that multiplication in a simple distribution space (referred to here as an untyped VSM) affects people's distributional memories."}, {"heading": "5.3.2 Experiment 2: the M&L2008 dataset", "text": "The second experiment uses the M & L2008 dataset, introduced by Mitchell and Lapata (2008), which contains pairs of intransitive sensitivities along with human judgments of similarity. The dataset contains 120 unique subjects, verb, landmark triples with a different number of human judgments per article. On average, each is rated three times out of 30 participants. The task is to assess the similarity of the subject to the verb and perception, which is able to clarify the context of the subject. In the context of the subject, one might expect it to be close to burnt but not close to irradiated persons. In the context of the subject, one might expect it to be close to irradiated and not close to burned.This dataset was used in the evaluations carried out by Grefenstette et al. (2013) and Dinu, Pham, and Baroni (2013). These evaluations clearly follow the experimental procedure of Mitchell and Lapata."}, {"heading": "6 Related Work", "text": "Our work combines two strands that are usually treated as separate but related problems: the representation of phrase meanings by creating distributional representations through composition and the representation of word meanings in context by modifying the distributional representation of a word. Together with other work on lexical distributional similarity, we use a typed space of incidental occurrence. However, we propose the use of higher order grammatical dependencies to allow the representation of phrase meanings and the representation of word meanings in context."}, {"heading": "6.1 Representing Phrasal Meaning", "text": "The problem of the representation of phrasal meanings has traditionally been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them with some functions to create a data structure that represents the phrase or sentence. Mitchell and Lapata (2008, 2010) found that simple additive and multiplicative functions applied to proximity-based vector representations were no less effective than more complex functions when performance was judged against human similarity. (2013a, 2013b) are currently among the most popular forms of distributed word representation. Word embeddings learned through the Continuous Term Model (CBOW) and the Continuous Skipgram Model proposed by Mikolov et al. (2013a, 2013b) are currently among the most popular forms of distributed word representation. While using a neural network architecture, the intuitions behind such representations are comparable to those in traditional representations of distributed words."}, {"heading": "6.2 Typed Co-occurrence Models", "text": "However, in untyped models of co-occurrence such as Mitchell and Lapata (2008, 2010), co-occurrences are simple, untyped pairs of words that occur together (usually within a window of proximity, but possibly within a grammatical relationship), but the lack of typing enables vectors to be composed by addition and multiplication. In calculating lexical distribution similarity by means of grammatical dependency relationships, it was typical (Lin, 1998; Lee, 1999; Weeds and Weir, 2005) to consider the type of co-occurrence (for example, does the dog appear with food as a direct object or its subject?) as part of the trait space. The distinction between vector spaces based on untyped and typed co-occurrences was typed by Pado and Lapata (2007) and Baroni and Lenci (2010)."}, {"heading": "6.3 Representing Word Meaning in Context", "text": "However, a long-standing topic in distributional semantics is the modification of a canonical representation (2008) and the meaning of a lexeme to reflect the context in which it is found. Typically, a canonical vector is used for a lexeme of all corpus events and the then modified vector to reflect the context (Lund and Burgess, 1996; Erk and Pado, 2008; Erk, Dinu and Pinkal, 2009; Thater, Fu \u00bc rstenau and Pinkal, 2010; Thater, Fu \u00bc rstenau and Pinkal, 2011; Van de Cruys, Poibeau and Korhonen, 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors were typically modified with simple additive and multiplicative functions."}, {"heading": "7 Directions for Future Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Representations", "text": "There are a number of obvious limitations to our approach, which are simply a reflection of our decision to adopt dependency-based syntactical analysis."}, {"heading": "7.2 Applications", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "8 Conclusions", "text": "This paper presents a new theory of compositional distribution semantics. It uses a single structure, the apt, which can represent the distribution semantics of lexemes, phrases and even sentences. By maintaining a higher-level grammatical structure in the representations of lexemes, the apt captures the composition of mutual ambiguities and mutual generalizations of components. Apts allow the comparison of lexemes and phrases in isolation or in context. Furthermore, we have shown how an instantiation of this theory can produce results that are very competitive with state-of-the-art results based on benchmark phrases. As we have already discussed, apts have a wide range of potential applications, including literal induction, literal disambiguation, analysis-repetition, analysis of dependencies and language modeling in general, as well as paraphrase recognition. Further work is needed to understand which instantiations of the theory are suitable for each of these applications."}, {"heading": "9 Acknowledgements", "text": "This work was funded by the British EPSRC project EP / IO37458 / 1 \"A Unified Model of Compositional and Distributional Compositional Semantics: Theory and Applications.\" We would like to thank all members of the DISCO project team. Special thanks go to Miroslav Batchkarov, Stephen Clark, Daoud Clarke, Roland Davis, Bill Keller, Tamara Polajnar, Laura Rimell, Mehrnoosh Sadrzadeh, David Sheldrick and Andreas Vlachos. We would also like to thank the anonymous reviewers for their helpful comments."}], "references": [{"title": "Don\u2019t count, predict! a systematic comparison of contextcounting vs. context-predicting semantic vectors", "author": ["Baroni", "Dinu", "Kruszewski2014] Baroni", "Marco", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Distributional memory: A general framework for corpus-based semantics", "author": ["Baroni", "Lenci2010] Baroni", "Marco", "Alessandro Lenci"], "venue": "Computational Linguistics,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010] Baroni", "Marco", "Roberto Zamparelli"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Semantic parsing via paraphrasing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Berant", "Liang2014] Berant", "Jonathan", "Percy Liang"], "venue": null, "citeRegEx": "Berant et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Berant et al\\.", "year": 2014}, {"title": "A comparison of vectorbased representations for semantic composition", "author": ["Blacoe", "Lapata2012] Blacoe", "William", "Mirella Lapata"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Blacoe et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Blacoe et al\\.", "year": 2012}, {"title": "Multimodal distributional semantics", "author": ["Bruni", "Tran", "Baroni2014] Bruni", "Elia", "Nam Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Context-theoretic Semantics for Natural Language: an Algebraic Framework", "author": ["Clarke", "Daoud"], "venue": "Ph.D. thesis,", "citeRegEx": "Clarke and Daoud.,? \\Q2007\\E", "shortCiteRegEx": "Clarke and Daoud.", "year": 2007}, {"title": "A context-theoretic framework for compositionality in distributional semantics", "author": ["Clarke", "Daoud"], "venue": "Computational Linguistics,", "citeRegEx": "Clarke and Daoud.,? \\Q2012\\E", "shortCiteRegEx": "Clarke and Daoud.", "year": 2012}, {"title": "Mathematical foundations for a compositional distributed model of meaning", "author": ["Coecke", "Sadrzadeh", "Clark2011] Coecke", "Bob", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": "Linguistic Analysis,", "citeRegEx": "Coecke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2011}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Collobert", "Weston2008] Collobert", "Ronan", "Jason Weston"], "venue": "In Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "Collobert et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "From Distributional to Semantic Similarity", "author": ["Curran", "James"], "venue": "Ph.D. thesis,", "citeRegEx": "Curran and James.,? \\Q2004\\E", "shortCiteRegEx": "Curran and James.", "year": 2004}, {"title": "Similarity-based estimation of word cooccurrence probabilities", "author": ["Dagan", "Pereira", "Lee1994] Dagan", "Ido", "Fernando Pereira", "Lillian Lee"], "venue": "In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Dagan et al\\.,? \\Q1994\\E", "shortCiteRegEx": "Dagan et al\\.", "year": 1994}, {"title": "General estimation and evaluation of compositional distributional semantic models", "author": ["Dinu", "Pham", "Baroni2013] Dinu", "Georgiana", "Nghia The Pham", "Marco Baroni"], "venue": "In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,", "citeRegEx": "Dinu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2013}, {"title": "Saarland: Vector-based models of semantic textual similarity", "author": ["Dinu", "Thater2012] Dinu", "Georgiana", "Stefan Thater"], "venue": "*SEM", "citeRegEx": "Dinu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dinu et al\\.", "year": 2012}, {"title": "Vector space models of word meaning and phrase meaning: A survey", "author": ["Erk", "Katrin"], "venue": "Language and Linguistics Compass,", "citeRegEx": "Erk and Katrin.,? \\Q2012\\E", "shortCiteRegEx": "Erk and Katrin.", "year": 2012}, {"title": "A structured vector space model for word meaning in context", "author": ["Erk", "Pad\u00f32008] Erk", "Katrin", "Sebastian Pad\u00f3"], "venue": "In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Erk et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Erk et al\\.", "year": 2008}, {"title": "Introducing and evaluating ukwac, a very large web-derived corpus of english", "author": ["Ferraresi et al.2008] Ferraresi", "Adriano", "Eros Zanchetta", "Marco Baroni", "Silvia Bernardini"], "venue": "In Proceedings of the WAC4 Workshop at LREC", "citeRegEx": "Ferraresi et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Ferraresi et al\\.", "year": 2008}, {"title": "Placing search in context: The concept revisited", "author": ["Lev", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th International Conference on World Wide Web, WWW", "citeRegEx": "Finkelstein et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Integrating logical representations with probabilistic information using markov logic", "author": ["Garrette", "Erk", "Mooney2011] Garrette", "Dan", "Katrin Erk", "Raymond Mooney"], "venue": "In Proceedings of the Ninth International Conference on Computational Semantics,", "citeRegEx": "Garrette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Garrette et al\\.", "year": 2011}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["Edward", "Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni"], "venue": "Proceedings of the Tenth International Conference on Computational Semantics", "citeRegEx": "Grefenstette et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "A regression model of adjective-noun compositionality in distributional semantics", "author": ["Guevara", "Emiliano"], "venue": "In Proceedings of the ACL GEMS Workshop,", "citeRegEx": "Guevara and Emiliano.,? \\Q2010\\E", "shortCiteRegEx": "Guevara and Emiliano.", "year": 2010}, {"title": "The role of syntax in vector space models of compositional semantics. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["Hermann", "Blunsom2013] Hermann", "Karl Moritz", "Phil Blunsom"], "venue": "Association for Computational Linguistics", "citeRegEx": "Hermann et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2013}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill", "Reichart", "Korhonen2015] Hill", "Felix", "Roi Reichart", "Anna Korhonen"], "venue": "Computational Linguistics,", "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "A systematic study of semantic vector space model parameters", "author": ["Kiela", "Clark2014] Kiela", "Douwe", "Stephen Clark"], "venue": "In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Type grammar revisited", "author": ["J. Lambek"], "venue": "Logical Aspects of Computational Linguistics,", "citeRegEx": "Lambek,? \\Q1999\\E", "shortCiteRegEx": "Lambek", "year": 1999}, {"title": "Measures of distributional similarity", "author": ["Lee", "Lillian"], "venue": "In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Lee and Lillian.,? \\Q1999\\E", "shortCiteRegEx": "Lee and Lillian.", "year": 1999}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["Levy", "Goldberg2014] Levy", "Omer", "Yoav Goldberg"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Levy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2014}, {"title": "Improving distributional similarity with lessons learned from word embeddings. Transactions of the Association for Computational Linguistics, 3:211\u2013225", "author": ["Levy", "Goldberg", "Dagan2015] Levy", "Omer", "Yoav Goldberg", "Ido Dagan"], "venue": null, "citeRegEx": "Levy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Levy et al\\.", "year": 2015}, {"title": "Combined distributional and logical semantics", "author": ["Lewis", "Steedman2013] Lewis", "Mike", "Mark Steedman"], "venue": "In Transactions of the Association for Computational Linguistics,", "citeRegEx": "Lewis et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lewis et al\\.", "year": 2013}, {"title": "Automatic retrieval and clustering of similar words", "author": ["Lin", "Dekang"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics,", "citeRegEx": "Lin and Dekang.,? \\Q1998\\E", "shortCiteRegEx": "Lin and Dekang.", "year": 1998}, {"title": "Producing high-dimensional semantic spaces from lexical co-occurrence", "author": ["Lund", "Burgess1996] Lund", "Kevin", "Curt Burgess"], "venue": "Behavior Research Methods, Instruments,", "citeRegEx": "Lund et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Lund et al\\.", "year": 1996}, {"title": "Semeval-2007 task 10: English lexical substitution task", "author": ["McCarthy", "Navigli2007] McCarthy", "Diana", "Robert Navigli"], "venue": "In Proceedings of the 4th International Workshop on Semantic Evaluations", "citeRegEx": "McCarthy et al\\.,? \\Q2007\\E", "shortCiteRegEx": "McCarthy et al\\.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov et al.2013a] Mikolov", "Tomas", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov et al.2013b] Mikolov", "Tomas", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Mikolov", "Yih", "Zweig2013] Mikolov", "Tomas", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Mitchell", "Jeff", "Mirella Lapata"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technology Conference,", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "Composition in distributional models of semantics", "author": ["Mitchell", "Lapata2010] Mitchell", "Jeff", "Mirella Lapata"], "venue": "Cognitive Science,", "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "English as a formal language", "author": ["Montague", "Richard"], "venue": null, "citeRegEx": "Montague and Richard.,? \\Q1970\\E", "shortCiteRegEx": "Montague and Richard.", "year": 1970}, {"title": "Dependency-based construction of semantic space models", "author": ["Pad\u00f3", "Lapata2007] Pad\u00f3", "Sebastian", "Mirella Lapata"], "venue": "Computational Linguistics,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2007}, {"title": "Glove: Global vectors for word representation", "author": ["Pennington", "Socher", "Manning2014] Pennington", "Jeffrey", "Richard Socher", "Christopher Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Socher", "Richard", "Eric H. Huang", "Jeffrey Pennington", "Christopher D Manning", "Andrew Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Semantic compositionality through recursive matrix-vector spaces", "author": ["Socher et al.2012] Socher", "Richard", "Brody Huval", "Christopher D. Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "Semi-supervised recursive autoencoders for predicting sentiment distributions", "author": ["Socher et al.2011] Socher", "Richard", "Jeffrey Pennington", "Eric H. Huang", "Andrew Y. Ng", "Christopher D. Manning"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher et al.2013] Socher", "Richard", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D. Manning", "Andrew Ng", "Christopher Potts"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Ranking paraphrases in context", "author": ["Thater", "Dinu", "Pinkal2009] Thater", "Stefan", "Georgiana Dinu", "Manfred Pinkal"], "venue": "In Proceedings of the 2009 ACL Workshop on Applied Textual Inference,", "citeRegEx": "Thater et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Thater et al\\.", "year": 2009}, {"title": "Contextualizing semantic representations using syntactically enriched vector models", "author": ["Thater", "F\u00fcrstenau", "Pinkal2010] Thater", "Stefan", "Hagen F\u00fcrstenau", "Manfred Pinkal"], "venue": "In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Thater et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Thater et al\\.", "year": 2010}, {"title": "Word meaning in context: A simple and effective vector model", "author": ["Thater", "F\u00fcrstenau", "Pinkal2011] Thater", "Stefan", "Hagen F\u00fcrstenau", "Manfred Pinkal"], "venue": "In Proceedings of 5th International Joint Conference on Natural Language Processing,", "citeRegEx": "Thater et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Thater et al\\.", "year": 2011}, {"title": "Domain and function: A dual-space model of semantic relations and compositions", "author": ["Turney", "Peter D"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney and D.,? \\Q2012\\E", "shortCiteRegEx": "Turney and D.", "year": 2012}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Turney", "Pantel2010] Turney", "Peter D", "Patrick Pantel"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Turney et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Turney et al\\.", "year": 2010}, {"title": "Latent vector weighting for word meaning in context", "author": ["Van de Cruys", "Poibeau", "Korhonen2011] Van de Cruys", "Tim", "Thierry Poibeau", "Anna Korhonen"], "venue": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Cruys et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Cruys et al\\.", "year": 2011}, {"title": "Measures and Applications of Lexical Distributional Similarity", "author": ["Weeds", "Julie"], "venue": "Ph.D. thesis,", "citeRegEx": "Weeds and Julie.,? \\Q2003\\E", "shortCiteRegEx": "Weeds and Julie.", "year": 2003}, {"title": "Co-occurrence retrieval: a flexible framework for distributional similarity", "author": ["Weeds", "Weir2005] Weeds", "Julie", "David Weir"], "venue": "Computational Linguistics,", "citeRegEx": "Weeds et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2005}, {"title": "Characterising measures of lexical distributional similarity", "author": ["Weeds", "Weir", "McCarthy2004] Weeds", "Julie", "David Weir", "Diana McCarthy"], "venue": "In Proceedings of the 20th International Conference on Computational Linguistics,", "citeRegEx": "Weeds et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2004}, {"title": "Distributional composition using higher-order dependency vectors", "author": ["Weeds", "Weir", "Reffin2014] Weeds", "Julie", "David Weir", "Jeremy Reffin"], "venue": "In Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC),", "citeRegEx": "Weeds et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Weeds et al\\.", "year": 2014}, {"title": "The unknown perils of mining wikipedia. https://blog.lateral.io/2015/06/the-unknown-perils-of-mining-wikipedia/, June", "author": ["Wilson", "Benjamin"], "venue": null, "citeRegEx": "Wilson and Benjamin.,? \\Q2015\\E", "shortCiteRegEx": "Wilson and Benjamin.", "year": 2015}, {"title": "A challenge set for advancing language modeling", "author": ["Zweig", "Burges2012] Zweig", "Geoffrey", "Chris JC Burges"], "venue": "In Proceedings of the NAACL-HLT", "citeRegEx": "Zweig et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zweig et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 19, "context": "It has been tokenised, POS-tagged, lemmatised and dependency-parsed as described in Grefenstette et al. (2013) and contains approximately 0.", "startOffset": 84, "endOffset": 111}, {"referenceID": 16, "context": "\u2022 concat is a concatenation of the ukWaC corpus (Ferraresi et al., 2008), a mid-2009 dump of the English Wikipedia and the British National Corpus.", "startOffset": 48, "endOffset": 72}, {"referenceID": 16, "context": "\u2022 concat is a concatenation of the ukWaC corpus (Ferraresi et al., 2008), a mid-2009 dump of the English Wikipedia and the British National Corpus. This corpus has been tokenised, POS-tagged, lemmatised and dependency-parsed as described in Grefenstette et al. (2013) and contains about 2.", "startOffset": 49, "endOffset": 268}, {"referenceID": 17, "context": "(Levy, Goldberg, and Dagan, 2015), we tuned on a number of popular word similarity tasks: MEN (Bruni, Tran, and Baroni, 2014); WordSim-353 (Finkelstein et al., 2001); and SimLex-999 (Hill, Reichart, and Korhonen, 2015).", "startOffset": 139, "endOffset": 165}, {"referenceID": 17, "context": "(Levy, Goldberg, and Dagan, 2015), we tuned on a number of popular word similarity tasks: MEN (Bruni, Tran, and Baroni, 2014); WordSim-353 (Finkelstein et al., 2001); and SimLex-999 (Hill, Reichart, and Korhonen, 2015). In these tuning experiments, we found that context distribution smoothing gave mixed results. However, shifting PPMI (k = 10) gave optimal results across all of the word similarity tasks. Therefore we report results here for vanilla PPMI (shift k = 1) and shifted PPMI (shift k = 10). For composition, we report results for both \u2294uni and \u2294int. Results are shown in Table 5. For this task and with this corpus \u2294uni consistently outperforms \u2294int. Shifting PPMI by log 10 consistently improves results for \u2294uni, but has a large negative effect on the results for \u2294int. We believe that this is due to the relatively small size of the corpus. Shifting PPMI reduces the number of non-zero dimensions in each vector which increases the likelihood of a zero intersection. In the case of AN composition, all of the intersections were zero for this setting, making it impossible to compute a correlation. Comparing these results with the state-of-the-art, we can see that \u2294uni clearly outperforms DM and NLM as tested by Blacoe and Lapata (2012). This method of composition is also achieving close to the best results in Mitchell and Lapata (2010) and Blacoe and Lapata (2012).", "startOffset": 140, "endOffset": 1256}, {"referenceID": 17, "context": "(Levy, Goldberg, and Dagan, 2015), we tuned on a number of popular word similarity tasks: MEN (Bruni, Tran, and Baroni, 2014); WordSim-353 (Finkelstein et al., 2001); and SimLex-999 (Hill, Reichart, and Korhonen, 2015). In these tuning experiments, we found that context distribution smoothing gave mixed results. However, shifting PPMI (k = 10) gave optimal results across all of the word similarity tasks. Therefore we report results here for vanilla PPMI (shift k = 1) and shifted PPMI (shift k = 10). For composition, we report results for both \u2294uni and \u2294int. Results are shown in Table 5. For this task and with this corpus \u2294uni consistently outperforms \u2294int. Shifting PPMI by log 10 consistently improves results for \u2294uni, but has a large negative effect on the results for \u2294int. We believe that this is due to the relatively small size of the corpus. Shifting PPMI reduces the number of non-zero dimensions in each vector which increases the likelihood of a zero intersection. In the case of AN composition, all of the intersections were zero for this setting, making it impossible to compute a correlation. Comparing these results with the state-of-the-art, we can see that \u2294uni clearly outperforms DM and NLM as tested by Blacoe and Lapata (2012). This method of composition is also achieving close to the best results in Mitchell and Lapata (2010) and Blacoe and Lapata (2012).", "startOffset": 140, "endOffset": 1358}, {"referenceID": 17, "context": "(Levy, Goldberg, and Dagan, 2015), we tuned on a number of popular word similarity tasks: MEN (Bruni, Tran, and Baroni, 2014); WordSim-353 (Finkelstein et al., 2001); and SimLex-999 (Hill, Reichart, and Korhonen, 2015). In these tuning experiments, we found that context distribution smoothing gave mixed results. However, shifting PPMI (k = 10) gave optimal results across all of the word similarity tasks. Therefore we report results here for vanilla PPMI (shift k = 1) and shifted PPMI (shift k = 10). For composition, we report results for both \u2294uni and \u2294int. Results are shown in Table 5. For this task and with this corpus \u2294uni consistently outperforms \u2294int. Shifting PPMI by log 10 consistently improves results for \u2294uni, but has a large negative effect on the results for \u2294int. We believe that this is due to the relatively small size of the corpus. Shifting PPMI reduces the number of non-zero dimensions in each vector which increases the likelihood of a zero intersection. In the case of AN composition, all of the intersections were zero for this setting, making it impossible to compute a correlation. Comparing these results with the state-of-the-art, we can see that \u2294uni clearly outperforms DM and NLM as tested by Blacoe and Lapata (2012). This method of composition is also achieving close to the best results in Mitchell and Lapata (2010) and Blacoe and Lapata (2012). It is interesting to note that our model does substantially better than the stateof-the-art on verb-object composition, but is considerably worse at noun-noun composition.", "startOffset": 140, "endOffset": 1387}, {"referenceID": 19, "context": "This dataset was used in the evaluations carried out by Grefenstette et al. (2013) and Dinu, Pham, and Baroni (2013).", "startOffset": 56, "endOffset": 83}, {"referenceID": 19, "context": "This dataset was used in the evaluations carried out by Grefenstette et al. (2013) and Dinu, Pham, and Baroni (2013). These evaluations clearly follow the experimental procedure of Mitchell and Lapata and do not evaluate against mean scores.", "startOffset": 56, "endOffset": 117}, {"referenceID": 19, "context": "This dataset was used in the evaluations carried out by Grefenstette et al. (2013) and Dinu, Pham, and Baroni (2013). These evaluations clearly follow the experimental procedure of Mitchell and Lapata and do not evaluate against mean scores. Instead, separate points are created for each human annotator, as discussed in Section 5.3.1. The multi-step regression algorithm of Grefenstette et al. (2013) achieved \u03c1 = 0.", "startOffset": 56, "endOffset": 402}, {"referenceID": 19, "context": "This dataset was used in the evaluations carried out by Grefenstette et al. (2013) and Dinu, Pham, and Baroni (2013). These evaluations clearly follow the experimental procedure of Mitchell and Lapata and do not evaluate against mean scores. Instead, separate points are created for each human annotator, as discussed in Section 5.3.1. The multi-step regression algorithm of Grefenstette et al. (2013) achieved \u03c1 = 0.23 on this dataset. In the evaluation of Dinu, Pham, and Baroni (2013), the lexical function algorithm, which learns a matrix representation for each functor and defines composition as matrix-vector multiplication, was the best performing compositional algorithm at this task.", "startOffset": 56, "endOffset": 488}, {"referenceID": 19, "context": "This dataset was used in the evaluations carried out by Grefenstette et al. (2013) and Dinu, Pham, and Baroni (2013). These evaluations clearly follow the experimental procedure of Mitchell and Lapata and do not evaluate against mean scores. Instead, separate points are created for each human annotator, as discussed in Section 5.3.1. The multi-step regression algorithm of Grefenstette et al. (2013) achieved \u03c1 = 0.23 on this dataset. In the evaluation of Dinu, Pham, and Baroni (2013), the lexical function algorithm, which learns a matrix representation for each functor and defines composition as matrix-vector multiplication, was the best performing compositional algorithm at this task. With optimal parameter settings, it achieved around \u03c1 = 0.26. In this evaluation, the full additive model of Guevara (2010) achieved \u03c1 < 0.", "startOffset": 56, "endOffset": 818}, {"referenceID": 19, "context": "This dataset was used in the evaluations carried out by Grefenstette et al. (2013) and Dinu, Pham, and Baroni (2013). These evaluations clearly follow the experimental procedure of Mitchell and Lapata and do not evaluate against mean scores. Instead, separate points are created for each human annotator, as discussed in Section 5.3.1. The multi-step regression algorithm of Grefenstette et al. (2013) achieved \u03c1 = 0.23 on this dataset. In the evaluation of Dinu, Pham, and Baroni (2013), the lexical function algorithm, which learns a matrix representation for each functor and defines composition as matrix-vector multiplication, was the best performing compositional algorithm at this task. With optimal parameter settings, it achieved around \u03c1 = 0.26. In this evaluation, the full additive model of Guevara (2010) achieved \u03c1 < 0.05. In order to make our results directly comparable with these previous evaluations, we have used the same corpus to construct our Apt lexicons, namely the concat corpus described in Section 5.1. Otherwise, the Apt lexicon was constructed as described in Section 5.3.1. As before note that k = 1 in shifted PPMI is equivalent to not shifting PPMI. Results are shown in Table 6. We see that \u2294uni is highly competitive with the optimised lexical function model which was the best performing model in the evaluation of Dinu, Pham, and Baroni (2013). In that evaluation, the lexical function model achieved between 0.", "startOffset": 56, "endOffset": 1380}, {"referenceID": 19, "context": "23 Grefenstette et al. (2013)", "startOffset": 3, "endOffset": 30}, {"referenceID": 19, "context": "23 which equals the performance of the multi-step regression algorithm Grefenstette et al. (2013). Here, however, shifting PPMI has a negative impact on performance.", "startOffset": 71, "endOffset": 98}, {"referenceID": 32, "context": "The word embeddings learnt by the continuous bag-of-words model (CBOW) and the continuous skip-gram model proposed by Mikolov et al. (2013a, 2013b) are currently among the most popular forms of distributional word representations. Whilst using a neural network architecture, the intuitions behind such distributed representations of words are the same as in traditional distributional representations. As argued by Pennington et al. (2014), both count-based and prediction-based models probe the underlying corpus co-occurrences statistics.", "startOffset": 118, "endOffset": 440}, {"referenceID": 32, "context": "The word embeddings learnt by the continuous bag-of-words model (CBOW) and the continuous skip-gram model proposed by Mikolov et al. (2013a, 2013b) are currently among the most popular forms of distributional word representations. Whilst using a neural network architecture, the intuitions behind such distributed representations of words are the same as in traditional distributional representations. As argued by Pennington et al. (2014), both count-based and prediction-based models probe the underlying corpus co-occurrences statistics. For example, the CBOW architecture predicts the current word based on context (which is viewed as a bag-of-words) and the skip-gram architecture predicts surrounding words given the current word. Mikolov et al. (2013c) showed that it is possible to use these models to efficiently learn low-dimensional representations for words which appear to capture both syntactic and semantic regularities.", "startOffset": 118, "endOffset": 760}, {"referenceID": 32, "context": "The word embeddings learnt by the continuous bag-of-words model (CBOW) and the continuous skip-gram model proposed by Mikolov et al. (2013a, 2013b) are currently among the most popular forms of distributional word representations. Whilst using a neural network architecture, the intuitions behind such distributed representations of words are the same as in traditional distributional representations. As argued by Pennington et al. (2014), both count-based and prediction-based models probe the underlying corpus co-occurrences statistics. For example, the CBOW architecture predicts the current word based on context (which is viewed as a bag-of-words) and the skip-gram architecture predicts surrounding words given the current word. Mikolov et al. (2013c) showed that it is possible to use these models to efficiently learn low-dimensional representations for words which appear to capture both syntactic and semantic regularities. Mikolov et al. (2013b) also demonstrated the possibility of composing skip-gram representations using addition.", "startOffset": 118, "endOffset": 959}, {"referenceID": 24, "context": "Separately, Coecke, Sadrzadeh, and Clark (2011) proposed a broader compositional framework that incorporated from formal semantics the notion of function application derived from syntactic structure (Montague, 1970; Lambek, 1999).", "startOffset": 199, "endOffset": 229}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013).", "startOffset": 198, "endOffset": 225}, {"referenceID": 41, "context": "The payoff for this increased flexibility has come with impressive performance in sentiment analysis (Socher et al., 2012; Socher et al., 2013).", "startOffset": 101, "endOffset": 143}, {"referenceID": 43, "context": "The payoff for this increased flexibility has come with impressive performance in sentiment analysis (Socher et al., 2012; Socher et al., 2013).", "startOffset": 101, "endOffset": 143}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013). The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix.", "startOffset": 199, "endOffset": 268}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013). The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix.", "startOffset": 199, "endOffset": 311}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013). The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke, Sadrzadeh, and Clark (2011) in using syntax to guide the order of phrasal composition.", "startOffset": 199, "endOffset": 485}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013). The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke, Sadrzadeh, and Clark (2011) in using syntax to guide the order of phrasal composition. This model, however, was made much more flexible by requiring and using task-specific labelled training data to create taskspecific distributional data structures, and by allowing non-linear relationships between component data structures and the composed result. The payoff for this increased flexibility has come with impressive performance in sentiment analysis (Socher et al., 2012; Socher et al., 2013). However, whilst these approaches all pay attention to syntax, they all require large amounts of training data. For example, running regression models to accurately predict the matrix or tensor for each individual adjective or verb requires a large number of exemplar compositions containing that adjective or verb. Socher\u2019s MV-RNN model further requires task-specific labelled training data. Our approach, on the other hand, is purely count-based and directly aggregates information about each word from the corpus. Other approaches have been proposed. Clarke (2007, 2012) suggested a context-theoretic semantic framework, incorporating a generative model that assigned probabilities to arbitrary word sequences. This approach shared with Coecke, Sadrzadeh, and Clark (2011) an ambition to provide a bridge between compositional distributional semantics and formal logic-based semantics.", "startOffset": 199, "endOffset": 1728}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013). The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke, Sadrzadeh, and Clark (2011) in using syntax to guide the order of phrasal composition. This model, however, was made much more flexible by requiring and using task-specific labelled training data to create taskspecific distributional data structures, and by allowing non-linear relationships between component data structures and the composed result. The payoff for this increased flexibility has come with impressive performance in sentiment analysis (Socher et al., 2012; Socher et al., 2013). However, whilst these approaches all pay attention to syntax, they all require large amounts of training data. For example, running regression models to accurately predict the matrix or tensor for each individual adjective or verb requires a large number of exemplar compositions containing that adjective or verb. Socher\u2019s MV-RNN model further requires task-specific labelled training data. Our approach, on the other hand, is purely count-based and directly aggregates information about each word from the corpus. Other approaches have been proposed. Clarke (2007, 2012) suggested a context-theoretic semantic framework, incorporating a generative model that assigned probabilities to arbitrary word sequences. This approach shared with Coecke, Sadrzadeh, and Clark (2011) an ambition to provide a bridge between compositional distributional semantics and formal logic-based semantics. In a similar vein, Garrette, Erk, and Mooney (2011) combined word-level distributional vector representations with logic-based representation using a probabilistic reasoning framework.", "startOffset": 199, "endOffset": 1893}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013). The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke, Sadrzadeh, and Clark (2011) in using syntax to guide the order of phrasal composition. This model, however, was made much more flexible by requiring and using task-specific labelled training data to create taskspecific distributional data structures, and by allowing non-linear relationships between component data structures and the composed result. The payoff for this increased flexibility has come with impressive performance in sentiment analysis (Socher et al., 2012; Socher et al., 2013). However, whilst these approaches all pay attention to syntax, they all require large amounts of training data. For example, running regression models to accurately predict the matrix or tensor for each individual adjective or verb requires a large number of exemplar compositions containing that adjective or verb. Socher\u2019s MV-RNN model further requires task-specific labelled training data. Our approach, on the other hand, is purely count-based and directly aggregates information about each word from the corpus. Other approaches have been proposed. Clarke (2007, 2012) suggested a context-theoretic semantic framework, incorporating a generative model that assigned probabilities to arbitrary word sequences. This approach shared with Coecke, Sadrzadeh, and Clark (2011) an ambition to provide a bridge between compositional distributional semantics and formal logic-based semantics. In a similar vein, Garrette, Erk, and Mooney (2011) combined word-level distributional vector representations with logic-based representation using a probabilistic reasoning framework. Lewis and Steedman (2013) also attempted to combine distributional and logical semantics by learning a lexicon for CCG (Combinatory Categorial Grammar (Steedman, 2000)) which first maps natural language to a deterministic logical form and then performs a distributional clustering over logical predicates based on arguments.", "startOffset": 199, "endOffset": 2052}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013). The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke, Sadrzadeh, and Clark (2011) in using syntax to guide the order of phrasal composition. This model, however, was made much more flexible by requiring and using task-specific labelled training data to create taskspecific distributional data structures, and by allowing non-linear relationships between component data structures and the composed result. The payoff for this increased flexibility has come with impressive performance in sentiment analysis (Socher et al., 2012; Socher et al., 2013). However, whilst these approaches all pay attention to syntax, they all require large amounts of training data. For example, running regression models to accurately predict the matrix or tensor for each individual adjective or verb requires a large number of exemplar compositions containing that adjective or verb. Socher\u2019s MV-RNN model further requires task-specific labelled training data. Our approach, on the other hand, is purely count-based and directly aggregates information about each word from the corpus. Other approaches have been proposed. Clarke (2007, 2012) suggested a context-theoretic semantic framework, incorporating a generative model that assigned probabilities to arbitrary word sequences. This approach shared with Coecke, Sadrzadeh, and Clark (2011) an ambition to provide a bridge between compositional distributional semantics and formal logic-based semantics. In a similar vein, Garrette, Erk, and Mooney (2011) combined word-level distributional vector representations with logic-based representation using a probabilistic reasoning framework. Lewis and Steedman (2013) also attempted to combine distributional and logical semantics by learning a lexicon for CCG (Combinatory Categorial Grammar (Steedman, 2000)) which first maps natural language to a deterministic logical form and then performs a distributional clustering over logical predicates based on arguments. The CCG formalism was also used by Hermann and Blunsom (2013) as a means for incorporating syntax-sensitivity into vector space representations of sentential semantics based on recursive auto-encoders (Socher et al.", "startOffset": 199, "endOffset": 2413}, {"referenceID": 19, "context": "These two approaches were subsequently combined and extended to incorporate simple transitive and intransitive sentences, with functions represented by tensors, and arguments represented by vectors (Grefenstette et al., 2013). The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke, Sadrzadeh, and Clark (2011) in using syntax to guide the order of phrasal composition. This model, however, was made much more flexible by requiring and using task-specific labelled training data to create taskspecific distributional data structures, and by allowing non-linear relationships between component data structures and the composed result. The payoff for this increased flexibility has come with impressive performance in sentiment analysis (Socher et al., 2012; Socher et al., 2013). However, whilst these approaches all pay attention to syntax, they all require large amounts of training data. For example, running regression models to accurately predict the matrix or tensor for each individual adjective or verb requires a large number of exemplar compositions containing that adjective or verb. Socher\u2019s MV-RNN model further requires task-specific labelled training data. Our approach, on the other hand, is purely count-based and directly aggregates information about each word from the corpus. Other approaches have been proposed. Clarke (2007, 2012) suggested a context-theoretic semantic framework, incorporating a generative model that assigned probabilities to arbitrary word sequences. This approach shared with Coecke, Sadrzadeh, and Clark (2011) an ambition to provide a bridge between compositional distributional semantics and formal logic-based semantics. In a similar vein, Garrette, Erk, and Mooney (2011) combined word-level distributional vector representations with logic-based representation using a probabilistic reasoning framework. Lewis and Steedman (2013) also attempted to combine distributional and logical semantics by learning a lexicon for CCG (Combinatory Categorial Grammar (Steedman, 2000)) which first maps natural language to a deterministic logical form and then performs a distributional clustering over logical predicates based on arguments. The CCG formalism was also used by Hermann and Blunsom (2013) as a means for incorporating syntax-sensitivity into vector space representations of sentential semantics based on recursive auto-encoders (Socher et al. (2011a, 2011b)). They achieved this by representing each combinatory step in a CCG parse tree with an auto-encoder function, where it is possible to parameterise both the weight matrix and bias on the combinatory rule and the CCG category. Turney (2012) offered a model that incorporated assessments of word-level semantic relations in order to determine phrasal-level similarity.", "startOffset": 199, "endOffset": 2821}, {"referenceID": 32, "context": "for synonym detection and concept categorisation, it has also been shown (Levy and Goldberg, 2014) that the skip-gram model with negative sampling as introduced in Mikolov et al. (2013a) is equivalent to implicit factorisation of the PPMI matrix.", "startOffset": 164, "endOffset": 187}, {"referenceID": 32, "context": "for synonym detection and concept categorisation, it has also been shown (Levy and Goldberg, 2014) that the skip-gram model with negative sampling as introduced in Mikolov et al. (2013a) is equivalent to implicit factorisation of the PPMI matrix. Levy, Goldberg, and Dagan (2015) also demonstrated how traditional count-based methods could be improved by transferring hyperparameters used by the prediction-based methods (such as context distribution smoothing and negative sampling).", "startOffset": 164, "endOffset": 280}, {"referenceID": 32, "context": "For example, in order to identify good candidate paraphrases for questions in a questionanswering task, Berant and Liang (2014) employ a paraphrase model based on adding word embeddings constructed using the CBOW model of Mikolov et al. (2013). Whilst the authors achieve state-of-the-art using a mixture of methods, a paraphrase model based on the addition of vectors of untyped co-occurrences alone cannot distinguish meanings where syntax is important.", "startOffset": 222, "endOffset": 244}], "year": 2016, "abstractText": "We present a new framework for compositional distributional semantics in which the distributional contexts of lexemes are expressed in terms of anchored packed dependency trees. We show that these structures have the potential to capture the full sentential contexts of a lexeme and provide a uniform basis for the composition of distributional knowledge in a way that captures both mutual disambiguation and gen-", "creator": "LaTeX with hyperref package"}}}