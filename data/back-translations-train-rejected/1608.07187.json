{"id": "1608.07187", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Semantics derived automatically from language corpora contain human-like biases", "abstract": "Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language---the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model---namely, the GloVe word embedding---trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.", "histories": [["v1", "Thu, 25 Aug 2016 15:07:17 GMT  (119kb,D)", "http://arxiv.org/abs/1608.07187v1", "14 pages, 3 figures"], ["v2", "Tue, 30 Aug 2016 18:23:06 GMT  (119kb,D)", "http://arxiv.org/abs/1608.07187v2", "14 pages, 3 figures"], ["v3", "Tue, 9 May 2017 19:03:45 GMT  (119kb,D)", "http://arxiv.org/abs/1608.07187v3", "14 pages, 3 figures"], ["v4", "Thu, 25 May 2017 17:50:31 GMT  (119kb,D)", "http://arxiv.org/abs/1608.07187v4", "14 pages, 3 figures"]], "COMMENTS": "14 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CY cs.LG", "authors": ["aylin caliskan", "joanna j bryson", "arvind narayanan"], "accepted": false, "id": "1608.07187"}, "pdf": {"name": "1608.07187.pdf", "metadata": {"source": "CRF", "title": "Semantics derived automatically from language corpora necessarily contain human biases", "authors": ["Aylin Caliskan-Islam", "Joanna J. Bryson", "Arvind Narayanan"], "emails": ["aylinc@princeton.edu,", "bryson@conjugateprior.org,", "arvindn@cs.princeton.edu."], "sections": [{"heading": null, "text": "Artificial intelligence and machine learning are in a period of astonishing growth. However, there are concerns that these technologies could be used, either intentionally or unintentionally, to perpetuate the prejudices and injustices to which, unfortunately, many human institutions are exposed. Here, for the first time, we show that human-like semantic distortions arise from the application of standard machine learning to ordinary language - the same kind of language that humans are exposed to on a daily basis. We replicate a spectrum of human distortions as revealed by the implicit association test and other well-known psychological studies. We replicate them using a widely used, purely statistical model of machine learning - namely, GloVe embedding - trained on a corpus of text from the Web. Our findings suggest that language itself contains reproducible and accurate imprints of our historical biases, whether they are morally neutral like insects or flowers, problematic as status quo, gender or even flippable."}, {"heading": "Introduction", "text": "We are amazed at the human capabilities that are visible in recent advances in artificial intelligence (AI), and need to be comforted to know the source of this progress. Machine learning, which exploits the universality of computation (Turing, 1950), is able to capture the knowledge and computation that is discovered and transmitted by humans and human culture. However, while it leads to spectacular advances, this strategy undermines the assumption of machine neutrality. The default assumption for many was that the computation that would come from mathematics would be pure and neutral, would provide a fairness that goes beyond what is available in human society. Instead, concerns about machine bias are now coming to the fore - concerns that our historical biases and prejudices are being reified in machines. Documented cases of automated biases range from online advertising (Sweeney, 2013) to criminal convictions (Angwin et al., 2016).Most experts, and certainly, recommend that I always and transparently."}, {"heading": "Meaning and Bias in Humans and Machines", "text": "In fact, we are able to put ourselves in a situation in which we are able, in which we are able to assert ourselves, in which we are able, in which we are able, in which we are in and in which we are able to assert ourselves, in which we are able to change the world."}, {"heading": "Results", "text": "Using the techniques described in the Methods section, we have found every linguistic bias documented in the psychology we have been looking for. Below, we find a sample that we believe to be convincing, and we have not chosen it because of its effectiveness - it is consistently high. Rather, we have chosen it to illustrate our assertion that we can explain a variety of implicit human biases purely on the basis of linguistic laws, and that these are in fact part and parcel of the meaning of language. We demonstrate this by showing that the same measures that replicate implicit bias also replicate adverse hiring practices and provide further verifiable information about employment and naming practices in contemporary America. We ensure impartiality in our approach by using the benchmarks and keywords that are established in known and highly cited works of human sciences, psychology and sociology, which we have found standard associations with, namely, a state-of-the-art-the-2014 and-Pennington-wide use."}, {"heading": "Baseline: Replication of Associations That Are Universally Accepted", "text": "The first results presented in the initial IAT publication (Greenwald et al., 1998) concerned biases that have been shown to be universal in humans and for which there is no societal interest, enabling the methodology to be introduced and clarified and validated on relatively morally neutral topics. We begin by replicating these harmless results for the same purposes."}, {"heading": "Flowers and Insects", "text": "The reason for this is that conflicts have occurred time and again in the past years, such as in the USA, Great Britain, France, Great Britain, France, Great Britain, Great Britain, France, Great Britain, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy, Italy,"}, {"heading": "Racial Biases", "text": "We use the same technique to show that machine learning absorbs prejudice just as easily as other biases among American biases. Here, we replicate not only the original IAT findings on racial bias, but also the recent and surprising finding that names alone have a huge impact on the likelihood of job candidates being called for an interview. \u2022 Jack Implicit Associations for Valence Original Finding: Greenwald et al. (1998, p. 1475) find extreme effects of race, as they are simply indicated by name. A bunch of names associated with being European-Americans turned out to be much easier to associate with pleasant than uncomfortable terms, compared with a bunch of African-American names. With 26 topics, Greenwald et al. show that European American names implicitly are pleasant with an efficacy of 1.17 and a p-value of 10 \u2212 6.Our Finding: We were again able to replicate two attitudes by looking semantic vagueness in glow."}, {"heading": "Replicating the Bertrand and Mullainathan (2004) Re\u0301sume\u0301 Study", "text": "Original search: Bertrand and Mullainathan (2004) sent nearly 5,000 identical search terms to 1,300 job advertisements, with only one change made to the search term: the names of the candidates. They found that European American candidates were 50% more likely to be offered the opportunity to be interviewed. Our search: Perhaps unsurprisingly, we found a significant result for the names used by Bertrand and and Mullainathan again. As before, we had to delete some less common names. We also assumed semantic proximity to convenience as a correlation for an invitation to an interview. We did this with two different types of \"pleasant / unpleasant\" stimuli: the names from the original IAT paper, and also a revised shorter set recently found in Nosek et al. (2002a)."}, {"heading": "Gender Biases", "text": "We will now turn to gender biases and stereotypes, starting with the return to prejudice as demonstrated by the IAT. \u2022 ms81 Words of the family, but then we will turn to matching the prejudices we have used against verifiable information from published US government statistics.Replicating Implicit Associations for Career and Family Whether it is appropriate for women to have careers has been a matter of significant cultural conflict. Historically, the consensus has been that they should not do so; today, most, but by no means all Americans, consider it appropriate for a woman to have a career as a man and family. Likewise, there have been historical prejudices against men who choose to take on domestic roles. The IAT study we are comparing has been conducted online and thus has a vastly larger subject pool. However, as it is more difficult to ensure that online subjects complete their task with attention, it has also examined far fewer keywords. We are able to replicate the key results with these even word-reduced ones."}, {"heading": "Comparison to Real-World Data: Occupational Statistics", "text": "It has been suggested that implicit gender bias is linked to gender gaps in occupational participation (Nosek et al., 2009); however, the relationship between these is complex and can be mutually reinforcing. Here, we examine the correlation between the gender association of occupational words and labor force participation data.Pearson's correlation coefficient B = 0.90 with p-value < 10 \u2212 18.6 / 14Original data: The x-axis of Figure 1 comes from the 2015 U.S. Bureau of Labor Statistics3, which provides information on occupational categories and the percentage of women who have certain occupations in these categories. We generated single-word occupational names (as explained in the Methods section) based on available data and calculated the percentage of women for the set of single-word occupational titles. Our search: Using WEFAT, we are able to use word embedding to predict the percentage of women in the 50 most relevant occupations of the technician."}, {"heading": "Comparison to Real-World Data: Androgynous Names", "text": "Similarly, we considered the verified assignment of sex to androgynous names, i.e. names sometimes used by both sexes. In this case, the most recent information we could find, the census name 1990 and gender statistics. Perhaps due to the age of our name data, our correlation was weaker than at employment statistics of 2015, but still highly significant. The correlation coefficient of Pearson corresponds to 0.84 with p-value < 10 -13. Original data: The x-axis of image 2 is derived from the US census data of 1990, which provides first names and gender information in the population @ @ @ @ @ @ Our finding: The y-axis mirrors our calculation of prejudice for this, as male or female each of the names is. By application of WEFAT we are able to provide the percentage of people with a name, the women with pearls correlation-coefficients of Jamie with a goltvalue @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @ @"}, {"heading": "Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Data and training", "text": "A word embedding is a representation of words as dots in a vector space. Loosely, embedding satisfies the property that vectors that are close to each other represent semantically \"similar\" words. Word embedding derives its power more from the discovery that vector spaces with around 300 dimensions are sufficient to capture most aspects of similarity, which allows a mathematically comprehensible representation of all or most words in large text corpora (Bengio et al., 2003; Lowe, 1997). Starting in 2013, the word2vec family of word embedding techniques has gained due to a new set of computational techniques for creating word embedding from large training corporations of text, with superior speed and predictable performance in various natural language processing tasks (Mikolov et al., 2013; Mikolov et al and Dean, 2013).Most famous word embedding is synchronized between word solutions because \"the algorithms are aligned with word solving tasks.\""}, {"heading": "Word Embedding Association Test (WEAT)", "text": "To demonstrate and quantify the bias, we use the permutation test. Following the IAT literature, we consider two sets of target words (e.g. programmer, engineer, scientist,... and nurse, teacher, librarian,...) and two sets of attribute words (e.g. man, man,... and woman, woman, woman...).The null hypothesis is that there is no difference between the two groups of target words in terms of their relative similarity to the two groups of attribute words.The permutation test measures the (in) probability of the null hypothesis by calculating the probability that a random permutation of the attribute word words will cause the observed (or greater) difference in the sample means.Formally, we let X and Y be two sets of target words of the same size, and A, B the two sets of attribute word word words."}, {"heading": "Word Embedding Factual Association Test (WEFAT)", "text": "In order to understand and show that the need for human bias is discussed in words and in writing, we must also address the question of the extent to which it is a real property of the world, such as the proportion of workers in the world of work who work in the world of work. In principle, we could use all algorithms, but in the world of work we are testing the mapping of terms embedded in property rights, that is, there is an algorithm that restricts property rights and restricts property rights."}, {"heading": "Discussion", "text": "In fact, most of them are able to survive on their own."}, {"heading": "Effects of bias in NLP applications", "text": "In order to better understand the possible effects of bias in word embedding, we look at applications where it has been used. Sentiment analysis classifies text as positive, negative, or neutral; two of its applications are in marketing to quantify customer satisfaction (for example, from a series of product reviews) and in the financial industry to predict market trends (for example, from company tweets); and if we look at a technique for sentiment analysis based on word embedding: let's calculate the value of each word based on its association with certain positive and negative words, then we sum up the sentiment scores. Now, let's look at the application of this technique to film reviews. Our results show that European-American names have a more positive value than African-American names in a state-of-the-art word embedding. This means that a sentence containing a European-American name will have a higher sentiment score than a sentence with that name replaced by an African-American name; in other words, the pre-acting will have a racial character in its output."}, {"heading": "Challenges in addressing bias", "text": "It is not that we are in a situation where we are unable to find a solution."}, {"heading": "Acknowledgements", "text": "We are grateful to the following people: Will Lowe for his significant assistance in designing our significance tests, Tim Macfarlane for his pilot research as part of his bachelor thesis, Solon Barocas and Miles Brundage for excellent comments on an early version of this paper. 6Our use of this term is inspired by Dwork et al. (2012), but we use it somewhat differently, and our reasoning differs from theirs."}], "references": [{"title": "Machine bias: There\u2019s software used across the country", "author": ["J. Angwin", "J. Larson", "S. Mattu", "L. Kirchner"], "venue": null, "citeRegEx": "Angwin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Angwin et al\\.", "year": 2016}, {"title": "criminals. and it\u2019s biased against blacks", "author": ["S. Barocas", "A.D. Selbst"], "venue": "California Law Review,", "citeRegEx": "Barocas and Selbst,? \\Q2014\\E", "shortCiteRegEx": "Barocas and Selbst", "year": 2014}, {"title": "Intuition, deliberation, and the evolution of cooperation", "author": ["A. Bear", "D.G. Rand"], "venue": "B: Biological Sciences,", "citeRegEx": "Bear and Rand,? \\Q2016\\E", "shortCiteRegEx": "Bear and Rand", "year": 2016}, {"title": "A neural probabilistic language model. journal of machine", "author": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Are emily and greg more employable than lakisha and jamal? a field experiment", "author": ["M. Bertrand", "S. Mullainathan"], "venue": null, "citeRegEx": "Bertrand and Mullainathan,? \\Q2004\\E", "shortCiteRegEx": "Bertrand and Mullainathan", "year": 2004}, {"title": "Pattern Recognition and Machine Learning", "author": ["C.M. Bishop"], "venue": "labor market discrimination. The American Economic Review,", "citeRegEx": "Bishop,? \\Q2006\\E", "shortCiteRegEx": "Bishop", "year": 2006}, {"title": "homemaker? debiasing word embeddings", "author": ["K. Crawford"], "venue": "The New York Times. Dessel, A", "citeRegEx": "Crawford,? \\Q2016\\E", "shortCiteRegEx": "Crawford", "year": 2016}, {"title": "Fairness through awareness", "author": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R. Zemel"], "venue": null, "citeRegEx": "Dwork et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Dwork et al\\.", "year": 2012}, {"title": "Certifying and removing", "author": ["M. ACM. Feldman", "S.A. Friedler", "J. Moeller", "C. Scheidegger", "S. Venkatasubramanian"], "venue": "Innovations in Theoretical Computer Science Conference,", "citeRegEx": "Feldman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feldman et al\\.", "year": 2015}, {"title": "Kin selection and \u2018mother tongues\u2019: A neglected component in language evolution", "author": ["W.T. ACM. Fitch"], "venue": null, "citeRegEx": "Fitch,? \\Q2004\\E", "shortCiteRegEx": "Fitch", "year": 2004}, {"title": "Measuring individual differences in implicit cognition", "author": ["A.G. MA. Greenwald", "D.E. McGhee", "J.L. Schwartz"], "venue": null, "citeRegEx": "Greenwald et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Greenwald et al\\.", "year": 1998}, {"title": "With malice toward none and charity for some: Ingroup favoritism", "author": ["A.G. Greenwald", "T.F. Pettigrew"], "venue": "Journal of personality and social psychology,", "citeRegEx": "Greenwald and Pettigrew,? \\Q2014\\E", "shortCiteRegEx": "Greenwald and Pettigrew", "year": 2014}, {"title": "Robot task planning and explanation in open and uncertain", "author": ["H. Zender", "Kruijff", "G.-J", "N. Hawes", "J.L. Wyatt"], "venue": null, "citeRegEx": "M. et al\\.,? \\Q2015\\E", "shortCiteRegEx": "M. et al\\.", "year": 2015}, {"title": "stereotypes influence women\u2019s susceptibility to stereotype threat", "author": ["K.D. Kinzler", "E. Dupoux", "E.S. Spelke"], "venue": "Journal of Experimental Social Psychology,", "citeRegEx": "Kinzler et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Kinzler et al\\.", "year": 2007}, {"title": "Racial bias and the validity of the implicit association test", "author": ["D.J. Lee"], "venue": "Academy of Sciences,", "citeRegEx": "Lee,? \\Q2016\\E", "shortCiteRegEx": "Lee", "year": 2016}, {"title": "Extracting semantics from the enron", "author": [], "venue": "In Proceedings of the Twentieth", "citeRegEx": "Macfarlane,? \\Q2013\\E", "shortCiteRegEx": "Macfarlane", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. LEA. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Annual Conference of the Cognitive Science Society,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": null, "citeRegEx": "Mikolov and Dean,? \\Q2013\\E", "shortCiteRegEx": "Mikolov and Dean", "year": 2013}, {"title": "Implicit and explicit stigmatizing attitudes and stereotypes about depression", "author": ["L.L. Monteith", "J.W. Pettit"], "venue": null, "citeRegEx": "Monteith and Pettit,? \\Q2011\\E", "shortCiteRegEx": "Monteith and Pettit", "year": 2011}, {"title": "Harvesting implicit group attitudes and beliefs from a demonstration", "author": ["Culture", "B.A. 19. Nosek", "M. Banaji", "A.G. Greenwald"], "venue": null, "citeRegEx": "Culture et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Culture et al\\.", "year": 2002}, {"title": "Math= male, me= female, therefore math", "author": ["B.A. Nosek", "M.R. Banaji", "A.G. Greenwald"], "venue": null, "citeRegEx": "Nosek et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Nosek et al\\.", "year": 2002}, {"title": "National differences in gender\u2013science stereotypes predict national sex differences in science and math", "author": [], "venue": null, "citeRegEx": "K,? \\Q2009\\E", "shortCiteRegEx": "K", "year": 2009}, {"title": "Norman stanley fletcher and the case of the proprietary algorithmic risk assessment", "author": ["M. Oswald", "J. Grace"], "venue": "achievement. Proceedings of the National Academy of Sciences,", "citeRegEx": "Oswald and Grace,? \\Q2016\\E", "shortCiteRegEx": "Oswald and Grace", "year": 2016}, {"title": "Glove: Global vectors for word representation", "author": ["J. Insight. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Hierarchical decision processes that operate over distinct timescales underlie choice", "author": ["B.A. Purcell", "R. Kiani"], "venue": null, "citeRegEx": "Purcell and Kiani,? \\Q2016\\E", "shortCiteRegEx": "Purcell and Kiani", "year": 2016}, {"title": "Implicit race attitudes predict trustworthiness", "author": ["D. A", "P. Sokol-Hessner", "M.R. Banaji", "E.A. Phelps"], "venue": "in northern ireland. Frontiers in Psychology,", "citeRegEx": "A. et al\\.,? \\Q2011\\E", "shortCiteRegEx": "A. et al\\.", "year": 2011}, {"title": "Learning fair representations", "author": ["R.S. Zemel", "Y. Wu", "K. Swersky", "T. Pitassi", "C. Dwork"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Zemel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zemel et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Documented cases of automated prejudice range from online advertising (Sweeney, 2013) to criminal sentencing (Angwin et al., 2016).", "startOffset": 109, "endOffset": 130}, {"referenceID": 22, "context": "Transparency should allow courts, companies, citizen watchdogs, and others to understand, monitor, and suggest improvements to algorithms (Oswald and Grace, 2016).", "startOffset": 138, "endOffset": 162}, {"referenceID": 6, "context": "Another recommendation has been diversity among AI developers, to address insensitive or under-informed training of machine learning algorithms (Sweeney, 2013; Noble, 2013; Barr, 2015; Crawford, 2016).", "startOffset": 144, "endOffset": 200}, {"referenceID": 5, "context": "In AI and machine learning, bias refers to prior information, a necessary prerequisite for intelligence (Bishop, 2006).", "startOffset": 104, "endOffset": 118}, {"referenceID": 10, "context": "First introduced by Greenwald et al. (1998), the IAT demonstrates enormous differences in response times when subjects are asked to pair two concepts they find similar, in contrast to two concepts they find different.", "startOffset": 20, "endOffset": 44}, {"referenceID": 23, "context": "We use a state-of-the-art and widely used word embedding, namely GloVe, made available by Pennington et al. (2014). We used one of GloVe\u2019s standard semantic models trained on standard corpora of ordinary language use found on the World Wide Web.", "startOffset": 90, "endOffset": 115}, {"referenceID": 10, "context": "Baseline: Replication of Associations That Are Universally Accepted The first results presented in the initial publication on the IAT (Greenwald et al., 1998) concerned biases that were found to be universal in humans and about which there is no social concern.", "startOffset": 134, "endOffset": 158}, {"referenceID": 4, "context": "Replicating the Bertrand and Mullainathan (2004) R\u00e9sum\u00e9 Study Original Finding: Bertrand and Mullainathan (2004) sent nearly 5,000 identical r\u00e9sum\u00e9s to 1,300 job advertisements with only one change made to the r\u00e9sum\u00e9s: the names of the candidates.", "startOffset": 16, "endOffset": 49}, {"referenceID": 4, "context": "Replicating the Bertrand and Mullainathan (2004) R\u00e9sum\u00e9 Study Original Finding: Bertrand and Mullainathan (2004) sent nearly 5,000 identical r\u00e9sum\u00e9s to 1,300 job advertisements with only one change made to the r\u00e9sum\u00e9s: the names of the candidates.", "startOffset": 16, "endOffset": 113}, {"referenceID": 4, "context": "Our Finding: Perhaps unsurprisingly, we again found a significant result for the names used by Bertrand and Mullainathan. As before, we had to delete some low-frequency names. We also assumed semantic nearness to pleasantness as the correlate for an invitation to interview. We did this with two different sets of \u2018pleasant/unpleasant\u2019 stimuli: those from the original IAT paper, and also a revised shorter set used more recently, found in Nosek et al. (2002a). For both sets of attributes, European American names are more likely than African American names to be invited for interviews (closer to pleasant than to unpleasant).", "startOffset": 95, "endOffset": 461}, {"referenceID": 4, "context": "Our Finding: Perhaps unsurprisingly, we again found a significant result for the names used by Bertrand and Mullainathan. As before, we had to delete some low-frequency names. We also assumed semantic nearness to pleasantness as the correlate for an invitation to interview. We did this with two different sets of \u2018pleasant/unpleasant\u2019 stimuli: those from the original IAT paper, and also a revised shorter set used more recently, found in Nosek et al. (2002a). For both sets of attributes, European American names are more likely than African American names to be invited for interviews (closer to pleasant than to unpleasant). Using the Greenwald et al. (1998) attributes, the effect size is 1.", "startOffset": 95, "endOffset": 663}, {"referenceID": 4, "context": "Our Finding: Perhaps unsurprisingly, we again found a significant result for the names used by Bertrand and Mullainathan. As before, we had to delete some low-frequency names. We also assumed semantic nearness to pleasantness as the correlate for an invitation to interview. We did this with two different sets of \u2018pleasant/unpleasant\u2019 stimuli: those from the original IAT paper, and also a revised shorter set used more recently, found in Nosek et al. (2002a). For both sets of attributes, European American names are more likely than African American names to be invited for interviews (closer to pleasant than to unpleasant). Using the Greenwald et al. (1998) attributes, the effect size is 1.50 and p-value < 10\u22124; and using the updated Nosek et al. (2002a) attributes, the effect size is 1.", "startOffset": 95, "endOffset": 762}, {"referenceID": 20, "context": "In another laboratory study, Nosek et al. (2002b) found that female terms are less associated with the sciences, and male terms less associated with the arts.", "startOffset": 29, "endOffset": 50}, {"referenceID": 23, "context": "A 2D projection (first two principal components) of the 300-dimensional vector space of the GloVe word embedding (Pennington et al., 2014).", "startOffset": 113, "endOffset": 138}, {"referenceID": 3, "context": "Word embeddings derive their power from the discovery that vector spaces with around 300 dimensions suffice to capture most aspects of similarity, enabling a computationally tractable representation of all or most words in large corpora of text (Bengio et al., 2003; Lowe, 1997).", "startOffset": 245, "endOffset": 278}, {"referenceID": 16, "context": "Starting in 2013, the word2vec family of word embedding techniques has gained popularity due to a new set of computational techniques for generating word embeddings from large training corpora of text, with superior speed and predictive performance in various natural-language processing tasks (Mikolov et al., 2013; Mikolov and Dean, 2013).", "startOffset": 294, "endOffset": 340}, {"referenceID": 17, "context": "Starting in 2013, the word2vec family of word embedding techniques has gained popularity due to a new set of computational techniques for generating word embeddings from large training corpora of text, with superior speed and predictive performance in various natural-language processing tasks (Mikolov et al., 2013; Mikolov and Dean, 2013).", "startOffset": 294, "endOffset": 340}, {"referenceID": 23, "context": "For all results in this paper we use the state-of-the-art GloVe word embedding method, in which, at a high level, the similarity between a pair of vectors is related to the probability that the words co-occur close to each other in text (Pennington et al., 2014).", "startOffset": 237, "endOffset": 262}, {"referenceID": 15, "context": "In pilot-work experiments along the lines of those presented here (on free associations rather than implicit associations) raw co-occurrence probabilities were shown to lead to substantially weaker results (Macfarlane, 2013).", "startOffset": 206, "endOffset": 224}, {"referenceID": 17, "context": "For example, we repeated all the WEAT and WEFAT experiments presented above using a different pre-trained embedding: word2vec on a Google News corpus (Mikolov and Dean, 2013).", "startOffset": 150, "endOffset": 174}, {"referenceID": 17, "context": "For example, we repeated all the WEAT and WEFAT experiments presented above using a different pre-trained embedding: word2vec on a Google News corpus (Mikolov and Dean, 2013). In all experiments, we observed statistically significant effects and high effect sizes. Further, we found that the gender association strength of occupation words is highly correlated between the GloVe embedding and the word2vec embedding (Pearson \u03c1 = 0.88; Spearman \u03c1 = 0.86). In concurrent work, Bolukbasi et al. (2016) compared the same two embeddings, using a different measure of the gender bias of occupation words, also finding a high correlation (Spearman \u03c1 = 0.", "startOffset": 151, "endOffset": 499}, {"referenceID": 7, "context": "A recent line of work on fairness in machine learning tries to minimize or avoid such biases (Dwork et al., 2012; Feldman et al., 2015; Zemel et al., 2013; Barocas and Selbst, 2014).", "startOffset": 93, "endOffset": 181}, {"referenceID": 8, "context": "A recent line of work on fairness in machine learning tries to minimize or avoid such biases (Dwork et al., 2012; Feldman et al., 2015; Zemel et al., 2013; Barocas and Selbst, 2014).", "startOffset": 93, "endOffset": 181}, {"referenceID": 26, "context": "A recent line of work on fairness in machine learning tries to minimize or avoid such biases (Dwork et al., 2012; Feldman et al., 2015; Zemel et al., 2013; Barocas and Selbst, 2014).", "startOffset": 93, "endOffset": 181}, {"referenceID": 1, "context": "A recent line of work on fairness in machine learning tries to minimize or avoid such biases (Dwork et al., 2012; Feldman et al., 2015; Zemel et al., 2013; Barocas and Selbst, 2014).", "startOffset": 93, "endOffset": 181}, {"referenceID": 11, "context": "Our work lends credence to the highly parsimonious theory that all that is needed to create prejudicial discrimination is not malice towards others, but preference for one\u2019s ingroup (Greenwald and Pettigrew, 2014).", "startOffset": 182, "endOffset": 213}, {"referenceID": 13, "context": "It has been known for some time that even newborn infants attend foremost to speakers sharing their mother\u2019s dialect (Kinzler et al., 2007); it has been conjectured that such ingroup signaling may even account for the origins of music and language (Fitch, 2004).", "startOffset": 117, "endOffset": 139}, {"referenceID": 9, "context": ", 2007); it has been conjectured that such ingroup signaling may even account for the origins of music and language (Fitch, 2004).", "startOffset": 116, "endOffset": 129}, {"referenceID": 10, "context": "This may account for why in the IAT, Koreans and Japanese people living in their own countries each associate the other as \u2018less pleasant,\u2019 but African Americans show European-American-oriented biases, though not as strongly as European Americans (Greenwald et al., 1998).", "startOffset": 247, "endOffset": 271}, {"referenceID": 9, "context": "This may account for why in the IAT, Koreans and Japanese people living in their own countries each associate the other as \u2018less pleasant,\u2019 but African Americans show European-American-oriented biases, though not as strongly as European Americans (Greenwald et al., 1998). The dominance of European-American orientation may change as the American demography changes; indeed it would be interesting to examine corpora consisting of newspapers or other public language in towns or cities with different demographic makeup, particularly where racial diversity is also represented consistently in public offices and media. Of course, neither our work nor any other theory explaining the origins of prejudice justify prejudiced behavior. Humans are (or can be) good at using explicit knowledge to better cooperate, including choosing to behave fairly. Lee (2016) has shown very recently that the level of implicit bias displayed by subjects in the IAT does not predict cooperative performance.", "startOffset": 248, "endOffset": 858}, {"referenceID": 1, "context": "Besides, bias is known to creep in indirectly, by proxy (Barocas and Selbst, 2014).", "startOffset": 56, "endOffset": 82}, {"referenceID": 18, "context": "To give one example, Monteith and Pettit (2011) using the IAT show that people with mental illnesses are stigmatized compared to people with physical illnesses \u2014 a result we have also replicated in word embeddings (but not reported above).", "startOffset": 21, "endOffset": 48}, {"referenceID": 14, "context": "Instead, we take inspiration from the fact that humans can express behavior different from their implicit biases (Lee, 2016).", "startOffset": 113, "endOffset": 124}, {"referenceID": 24, "context": "Human intelligence is typified by behavior integrating multiple forms of memory and evidence (Purcell and Kiani, 2016; Bear and Rand, 2016).", "startOffset": 93, "endOffset": 139}, {"referenceID": 2, "context": "Human intelligence is typified by behavior integrating multiple forms of memory and evidence (Purcell and Kiani, 2016; Bear and Rand, 2016).", "startOffset": 93, "endOffset": 139}, {"referenceID": 7, "context": "6Our use of this term is inspired by Dwork et al. (2012), but we use it slightly differently, and our argument is different from theirs.", "startOffset": 37, "endOffset": 57}], "year": 2017, "abstractText": "Artificial intelligence and machine learning are in a period of astounding growth. However, there are concerns that these technologies may be used, either with or without intention, to perpetuate the prejudice and unfairness that unfortunately characterizes many human institutions. Here we show for the first time that human-like semantic biases result from the application of standard machine learning to ordinary language\u2014the same sort of language humans are exposed to every day. We replicate a spectrum of standard human biases as exposed by the Implicit Association Test and other well-known psychological studies. We replicate these using a widely used, purely statistical machine-learning model\u2014namely, the GloVe word embedding\u2014trained on a corpus of text from the Web. Our results indicate that language itself contains recoverable and accurate imprints of our historic biases, whether these are morally neutral as towards insects or flowers, problematic as towards race or gender, or even simply veridical, reflecting the status quo for the distribution of gender with respect to careers or first names. These regularities are captured by machine learning along with the rest of semantics. In addition to our empirical findings concerning language, we also contribute new methods for evaluating bias in text, the Word Embedding Association Test (WEAT) and the Word Embedding Factual Association Test (WEFAT). Our results have implications not only for AI and machine learning, but also for the fields of psychology, sociology, and human ethics, since they raise the possibility that mere exposure to everyday language can account for the biases we replicate here.", "creator": "LaTeX with hyperref package"}}}