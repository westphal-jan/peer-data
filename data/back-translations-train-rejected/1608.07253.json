{"id": "1608.07253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Aug-2016", "title": "Learning Latent Vector Spaces for Product Search", "abstract": "We introduce a novel latent vector space model that jointly learns the latent representations of words, e-commerce products and a mapping between the two without the need for explicit annotations. The power of the model lies in its ability to directly model the discriminative relation between products and a particular word. We compare our method to existing latent vector space models (LSI, LDA and word2vec) and evaluate it as a feature in a learning to rank setting. Our latent vector space model achieves its enhanced performance as it learns better product representations. Furthermore, the mapping from words to products and the representations of words benefit directly from the errors propagated back from the product representations during parameter estimation. We provide an in-depth analysis of the performance of our model and analyze the structure of the learned representations.", "histories": [["v1", "Thu, 25 Aug 2016 18:57:50 GMT  (782kb,D)", "http://arxiv.org/abs/1608.07253v1", "CIKM2016, Proceedings of the 25th ACM International Conference on Information and Knowledge Management. 2016"]], "COMMENTS": "CIKM2016, Proceedings of the 25th ACM International Conference on Information and Knowledge Management. 2016", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["christophe van gysel", "maarten de rijke", "evangelos kanoulas"], "accepted": false, "id": "1608.07253"}, "pdf": {"name": "1608.07253.pdf", "metadata": {"source": "META", "title": "Learning Latent Vector Spaces for Product Search", "authors": ["Christophe Van Gysel", "Maarten de Rijke", "Evangelos Kanoulas"], "emails": ["cvangysel@uva.nl", "derijke@uva.nl", "e.kanoulas@uva.nl", "permissions@acm.org."], "sections": [{"heading": null, "text": "Keywords Entity Retrieval; Latent Spatial Models; Representation Learning"}, {"heading": "1. INTRODUCTION", "text": "In fact, the fact is that most of them will be able to put themselves in a situation where they are able to survive themselves, where they are able to get caught up in a situation where they are able, in which they are able, in which they are able to hold their own, and in which they are able, in which they are able to hold their own, and in which they are able to hold their own, and in which they are able to hold their own, in which they are able to hold their own, in which they are able to hold their own."}, {"heading": "2. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Product retrieval", "text": "Nurmi et al. [48] note a discrepancy between shoppers \"shopping lists and the way retail stores maintain information; they introduce a food retrieval system that retrieves products using shopping lists written in natural language; product resolution [1] is an important task for e-commerce aggregation platforms, such as the verticals of major web search engines and price comparison portals; Duan et al. [19] propose a probabilistic blending model for analyzing product search protocols at the attribute level; they focus on structured aspects of product units, while in this work we learn representations from unstructured documents; Duan et al. [20] expand the language modeling approach to product databases by incorporating the ability to condition product search protocols (e.g. light products); they note that languages such as SQL can be effectively used to solve these problems from duplicate databases that users are not using only for their purposes."}, {"heading": "2.2 Latent semantic information retrieval", "text": "The mismatch between queries and documents is a key challenge in the search [34]. Latent semantic models (LSMs) enable queries based on conceptual content rather than exact word matches. LSMs have become popular with the introduction of latent semantic indexing (LSI) [15], followed by probabilistic LSI (pLSI) [28]. Salakhutdinov and Hinton [52] use a deep auto-encoder for the unattended learning of latent semantic document bitpatterns. Deep semantic models [29, 54] use click data to predict the relevance of a document for a query. Methods based on neural networks have also been used for machine-learned ranking [11, 17, 36]. Van Gysel et al. [57] introduce an LSM for query entities, with the emphasis on identifying expert parameters; it is indicated that the school parameter is only increased by the number of random samples."}, {"heading": "2.3 Representation learning", "text": "Recently, there has been a growing interest in neural probabilistic language models (LMs) for word sequence modelling [8, 43, 44]. Distributed representations [27] of words learned through neural LMs, also known as word embedding, include syntactic and semantic information [42, 45, 49] as a side effect of their ability to reduce dimensionality. Forward-looking [13] and recurring [42] neural networks do a good job in various NLP tasks. Recently, there has been an increased interest in multimodal neural language models [33] used for the task of automated caption. Learning representations of entities is not new. Bordes et al. [10] use structured relationships recorded in knowledge bases (KB) for entity presentations, learn and evaluate their representations based on link prediction tasks."}, {"heading": "3. LATENT VECTOR SPACES FOR ENTITY RETRIEVAL", "text": "First, we introduce a generalized formalism and notation for entity-oriented latent vector space models. Then, in \u00a7 3.2, we introduce latent semantic entities, a latent vector space model that collectively learns representations of words, entities, and a mapping between the two directly, based on the idea that entities are characterized by the words to which they are associated and vice versa. Product representations are constructed on the basis of the n-grams that the products are likely to generate based on their description and verification, while word representations are based on the entities to which they are associated and the context in which they appear. We explicitly model the relationship between word and product representations so that we can predict the product representation for a hitherto invisible word order."}, {"heading": "3.1 Background", "text": "We focus on a product retrieval setting in which a user wants to retrieve the most relevant products on an e-commerce platform. As in typical information retrieval scenarios, the user encodes his or her information needs as a query q and passes it to a search engine. Product searches describe characteristics of the product the user is looking for, such as a set of terms describing the product category [51]. In the following, X refers to the group of entities we are looking at. For each xi-X, we assume that we have a set of associated documents Dxi. The exact relationship between the entity and its documents depends on the issue. In this paper, entities products [18, 48] and documents associated with these products are descriptions and product views.Latent vector space models rely on a function f: V + \u2192 E that represents a sequence of words (e.g. a query q during the retrieval) from a continuous vocabulary V related to an e-space platform."}, {"heading": "3.2 Latent semantic entities", "text": "In fact, it is not that we are able to assert ourselves that we are able to assert ourselves that we are able to assert ourselves in the world. (...) It is not that we are able to establish ourselves in the world. (...) It is not that we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is as if we are in the world. (...) It is in the world. (...) It is as if we are in the world. (...) It is in the world."}, {"heading": "3.3 Parameter estimation", "text": "For a particular document d-Dxi associated with entity xi, we generate n-grams wj, 1,.., wj, n, where n (window size) remains fixed during training. For each n-gram wj, 1,., wj, n, we calculate its projected representation f (wj, 1,., wj, n) in E using f (eq. 2). The goal is then to find the similarity between the vector representation of the entity ei and the projected n-programs f (wj, 1,.,., wj, n) in relation to Sc (\u00a7 3.1), while the similarity between f (wj, n) and the projected n-programs f (wj, 1) in relation to the projected n-programs f (wj, 1,.)."}, {"heading": "4. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Research questions", "text": "In this paper, we investigate the problem of constructing a latent vector model of words and entities by directly modelling the discriminatory relationship between entities and word context. We try to answer the following research questions: RQ1 How do the parameters of LSE affect their effectiveness? In \u00a7 3, we introduced various hyperparameters along with the definition of latent semantic entities. We have the size of word representations eV and the dimensionality of entity representations eE. During the parameter estimation, the window size n affects the context width presented as evidence of a particular entity. What influence do these parameters have on the effectiveness of LSE and can we identify relationships between parameters? RQ2 How does LSE compare latent vector models based on LDA, LSI and word2vec? Is there a single method that always works best or does the effectiveness differ per domain?"}, {"heading": "4.2 Experimental design", "text": "In order to answer the research questions raised in \u00a7 4.1, we evaluate LSE in an entity retrieval setting organized around Amazon products (see \u00a7 4.3). We choose to experiment with examples of Amazon product data [38, 39] for the following reasons: (1) The collection contains heterogeneous types of evidence associated with each entity: descriptions and ratings. (2) Each department (e.g. Home & Kitchen) represents a separate, self-contained domain. (3) Within each department, there is a hierarchical taxonomy that subdivides the space of the entities into a rich structure. We can use the labels associated with those partitions and the partitions themselves as the basis of truth during evaluation. (4) Each department consists of a large number of products categorized across a large number of categories."}, {"heading": "4.3 Product search benchmarks", "text": "We evaluate 3 (Amazon divisions) based on four samples from different product ranges, each with an increasing number of products: Home & Kitchen (8,192 products), Clothing, Shoes & Jewelry (16,384 products), Pet Supplies (32,768 products) and Sports & Outdoors (65,536 products); see Table 1. The documents associated with each product consist of the product description plus ratings provided by Amazon customers. Rowley [51, p. 24] describes targeted product searches as users who search by \"the name of a manufacturer, a brand or a set of terms describing the category of the product.\" Following this observation, the test topics are extracted from the categories to which each product belongs. Category hierarchies of less than two levels are ignored, as the first level in the category hierarchy is often not descriptive for the product (e.g. in Clothing, Shoes & Jewelry, this is the gender that is determined to belong to the category of products considered as belonging to a sub-category)."}, {"heading": "4.4 Evaluation measures and significance", "text": "In order to measure the effectiveness of the query, we report on Normalized Discounted Cumulative Gain (NDCG). For RQ4, we also report on Precision @ k (k = 5, 10). Unless otherwise stated, the significance of observed differences is determined by a two-stage paired t test [55] (\u043a \u043a \u043a \u0445 p < 0.01; \u043c \u043c \u043c p < 0.05; \u0445 p < 0.1)."}, {"heading": "4.5 Methods used in comparisons", "text": "We compare latent semantic entities with latent vector space models for entity retrieval [34]. We also perform a contrastive analysis between LSE and smoothed language models with exact matching capabilities. Vector Space Models for entity finding. Demartini et al. [16] propose a formal model for finding entities using document vector space models (\u00a7 3.1). We compare the retrievable effectiveness of LSE with baseline latent vector space models created using (1) Latent semantic indexing (LSI) [15] with TF-IDF term weighting, (2) Latent Dirichlet Allocation (LDA) [9] with \u03b1 = \u03b2 = 0.1, where a document is represented by its theme distribution, and (3) word2vec x elvec [42] with CBOW and negative sampling, where a query / document is represented by the average of its embeddings (Simileys)."}, {"heading": "5. RESULTS AND DISCUSSION", "text": "We start with a high-level overview of our experimental results (RQ1 and RQ2), followed by a comparison with lexical matching methods (RQ3) and the use of LSE as a ranking feature (RQ4) (see Section 4.2 for an overview of experimental design)."}, {"heading": "5.1 Overview of experimental results", "text": "RQ1: Fig. 3 shows that neither extreme values for the dimensionality of the entity representations nor the context width alone achieve the highest performance on the validation sets. Instead, we see that a low-dimensional entity space (128- and 256-dimensional) in combination with a medium-sized context window (4- and 8-gram) achieves the highest NDCG. In the two largest benchmarks (Fig. 3c, 3d), we see that a low-dimensional entity space (128- and 256-dimensional) actually decreases as the dimensionality of the entity space increases. This is due to the model that corresponds to the optimization goal (Eq. 5), which we use as an unattended surrogate of relevance (Fig. 3c, 3d)."}, {"heading": "5.2 A feature for machine-learned ranking", "text": "We examine the use of LSE as a trait in a hierarchy [36]. Latent vector space models are known to provide a means of semantic linkage as opposed to a purely lexical linkage [34, 57]. To determine to what extent this is actually the case, we perform a topic-based comparison between LSE and a lexical language model. In the case of LSE, we choose the model that works best in Fig 3, as described in \u00a7 4.5. We optimize the parameters of LSE and QLM for each benchmark (Table 1)."}, {"heading": "6. ANALYSIS OF REPRESENTATIONS", "text": "Next, we analyze the ideal representation of the vector space, which is independent of the textual representations, which with empirical lower limits aim at their maximum retrieval performance. (So it is only a perspective that shows the effectiveness of word-to-entity figure f, Figure 3 and 4, which levels of performance can be achieved to generate a ranking of textual queries. (It is only a perspective.) As the entities according to their similarity to the projected quantum vector f (qc), the performance of the retrieval entities w.r.The textual representation of a topic depends on the structure of the entity space E, the ideal retrieval vector e, E (i.e.) the vector that optimizes the retrieval performance, and the similarity between the trieval entities w.rt is the performance eval, and the trieval reeval is the performance. (The retrieval theme e, the ideal retrieval vector e, E (the vector e, E (the i.e.) is the vector that optimizes the the retrieval performance, and the retrieval performance is the retrieval, and the retrieval is the performance)."}, {"heading": "7. CONCLUSIONS", "text": "This year it is so far that it will be able to reete.n the aforementioned hreeisrcnlrVo"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "We introduce a novel latent vector space model that jointly learns<lb>the latent representations of words, e-commerce products and a<lb>mapping between the two without the need for explicit annotations.<lb>The power of the model lies in its ability to directly model the dis-<lb>criminative relation between products and a particular word. We<lb>compare our method to existing latent vector space models (LSI,<lb>LDA and word2vec) and evaluate it as a feature in a learning to<lb>rank setting. Our latent vector space model achieves its enhanced<lb>performance as it learns better product representations. Further-<lb>more, the mapping from words to products and the representations<lb>of words benefit directly from the errors propagated back from the<lb>product representations during parameter estimation. We provide<lb>an in-depth analysis of the performance of our model and analyze<lb>the structure of the learned representations.", "creator": "LaTeX with hyperref package"}}}