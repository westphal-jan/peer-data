{"id": "1608.07400", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2016", "title": "Collaborative Filtering with Recurrent Neural Networks", "abstract": "We show that collaborative filtering can be viewed as a sequence prediction problem, and that given this interpretation, recurrent neural networks offer very competitive approach. In particular we study how the long short-term memory (LSTM) can be applied to collaborative filtering, and how it compares to standard nearest neighbors and matrix factorization methods on movie recommendation. We show that the LSTM is competitive in all aspects, and largely outperforms other methods in terms of item coverage and short term predictions.", "histories": [["v1", "Fri, 26 Aug 2016 09:20:21 GMT  (39kb)", "http://arxiv.org/abs/1608.07400v1", null], ["v2", "Tue, 3 Jan 2017 07:41:44 GMT  (39kb)", "http://arxiv.org/abs/1608.07400v2", null]], "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["robin devooght", "hugues bersini"], "accepted": false, "id": "1608.07400"}, "pdf": {"name": "1608.07400.pdf", "metadata": {"source": "CRF", "title": "Collaborative Filtering with Recurrent Neural Networks", "authors": ["Robin Devooght"], "emails": ["robin.devooght@ulb.ac.be", "bersini@ulb.ac.be"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.07 400v 1 [cs.I R] 26 Aug 201 6We show that collaborative filtering can be seen as a problem of sequence prediction, and that, based on this interpretation, recurrent neural networks offer a very competitive approach. In particular, we study how long-term memory (LSTM) can be applied to collaborative filtering and how it compares with standard methods of closest neighbors and matrix factorization for film recommendation. We show that LSTM is competitive in all aspects and largely surpasses other methods in terms of article coverage and short-term prediction. Keywords: collaborative filtering, recommendation systems, relapsing neural network, LSTM, deep learning."}, {"heading": "1 Introduction", "text": "Common filtering is the problem of recommending items to users based on past interactions between users and items. Intuition is that if two users have had similar interactions in the past, they should seek each other out for recommendations. Nearest Neighbors \"(KNN) approaches are based on this idea and offer several mechanisms for identifying similar users and pool recommendations. In addition to KNN, the other main approach to collaborative filtering is matrix factorization (MF), which frames recommendations as a dimension reduction problem. MF tries to represent users and items according to an optimization criterion the interaction between users and items (i.e. users should be close to the items they like). Although useful, these methods are far from perfect and improvements are coming more slowly. Furthermore, they are not suitable for grasping the temporal aspects of recommendations, such as evolving taste or context interests."}, {"heading": "2 Collaborative filtering as a se-", "text": "The goal of the recommendation system is to predict the items in St + i as a function of St \u2212 i. In this environment, which we call static, the order in which the items are consumed is irrelevant to the recommendation system. Instead, we suggest that the recommendations should be based not only on the items the user has consumed, but on the order in which he has consumed them. The user is represented by his sequence of actions: he consumes x2, then x3, and the goal is to predict his next actions (x4, x5, etc.) based on the beginning of the sequence in which the items are consumed."}, {"heading": "3 Sequence prediction favors di-", "text": "As stated in the previous section, methods that ignore the sequence of events are trained to produce long-term predictions, while methods based on sequence predictions are typically trained to produce short-term predictions. Typical metrics for evaluating long-term predictions are precision and retrieval. If Pi is the prediction based on St \u2212 i, then precision is | Pi-St + i | and the retrieval is | Pi-St + i | (we usually calculate precision and call \"at k,\" where k | Pi |). To evaluate the quality of short-term predictions, we propose a simple metric that we call \"sequence prediction success at k\" (sps @ k): | Pi-xi} |, where xi is the first element of the sequence where k | Pi | i | | is. To evaluate the quality of the short-term prediction, we suggest a measurement quantity called \"success at @.\""}, {"heading": "4 Methods comparison", "text": "Recurrent neural networks (RNNs) may be the most versatile methods of sequence prediction, and their recent successes in speech recognition, translation, and other areas make them good candidates for the referral problem. In this section, we evaluate RNN for an item recommendation problem for two film recommendation records and compare it to cooperative filtering methods (which do not use sequence information)."}, {"heading": "4.1 Datasets", "text": "Our biggest limitation when finding records is the presence of information about the sequence of events (often in the form of a timestamp). Unfortunately, this information is missing from most common filter records, but is available in two well-known film recommendations records: \u2022 Movielens 1M: a rather small version of the Movielens dataset with 6040 users and 1,000,209 reviews of 3,706 movies. This dataset has additional information about users (age, gender, profession) and about the movies (year, genre) evaluated in Section 5. \u2022 Netflix: A much larger dataset with about 480,000 users and 100,000 reviews of 17,770 movies. Each dataset has been divided into training sessions, validations and test subsets. These subsets were obtained by dividing users into 3 groups: N randomly selected users and all their ratings to constitute the test set."}, {"heading": "4.2 Recurrent neural networks", "text": "We have taken a similar approach, considering each element as a word, the catalog of elements as the complete vocabulary, and the history of each user as an example sequence. The RNN passes through the sequence of elements consumed by a user, point by point. At each step, the input is the unique encoding of the current element, and the output is a softmax layer with a neuron for each element in the catalog. The k-elements whose neurons are most activated are used as k-recommendations. The state of the art in recursive neural networks is what is called \"gated\" RNNs, in which the internal state of the RNN is controlled by one or more small neural networks, which are called gates. The original gated RNN is the LSTM [9], but it has produced several variants [3, 7].We trained the NN to correct the categorical loss, which we categorize with the only approach, the one wheel."}, {"heading": "4.3 Competing methods", "text": "We compare the RNN with two static methods of the TopN recommendation: one based on nearest neighbors and one based on matrix factorization. We also compare the RNN with a simple Markov chain that can be considered a starting point for the sequence prediction approach."}, {"heading": "4.3.1 Markov chain", "text": "In this simple method, the behavior of the user is modeled by a Markov chain whose states are the various items. Transition probabilities between items are derived from the transition probabilities observed in the training set. At any time, the state of a user corresponds to the last item he consumed, and the recommendations for that user are the k items with the highest transition probabilities from that state. In other words, if the last item consumed by a user is item j, the k-first recommendations of the Markov model are the k items that most often followed item j in the sequences originating from other users. This corresponds to a bigram model in language modeling in which the words become the items."}, {"heading": "4.3.2 User-based nearest neighbors", "text": "User-based closest neighbors or users KNN is one of the oldest methods of cooperative filtering, but still represents a strong basis for Top-N recommendations. A score sij is calculated between a user i and a user j: sij = \u2211 u-Nk (i) ciu1 (j-Su) (1) Where ciu is the similarity between user i and u, Nk (i) is the user closest to i according to similarity measure c, and 1 (j-Su) is the indicator function that evaluates to 1 whether the user j belongs to the order of users u, or otherwise to 0. We used the cosine similarity measure commonly preferred for product recommendations: ciu = | Si-Su | | Ku | (2) The size of the neighborhood (k) has been optimized using a validation set."}, {"heading": "4.3.3 BPR-MF", "text": "BPR-MF is a state-of-the-art matrix factorization method for Top-N recommendations developed by [14]. It is based on the personalized Bayesian ranking: an objective function similar to the AUC (range under the ROC curve) and can be trained by stochastic gradient descent. We used the original implementation of BPR-MF available in the MyMediaLite framework [5]. BPR-MF has many parameters; we selected the most important (number of features and iterations) based on a validation set and maintained the default values of MyMediaLite for the others."}, {"heading": "4.4 Metrics", "text": "We compare the different methods through a series of indicators that are finely tuned to capture different qualities of the recommendation systems. \u2022 sps. Short-term prediction success captures the method's ability to predict the next element. It is 1 if the next element is present in the recommendations, otherwise 0. \u2022 callback. The usual indicators for Top N recommendations capture the method's ability to make long-term predictions. \u2022 User coverage. The fraction of users who have received at least one correct recommendation. Average callback (and precision) conceal the success distribution among users. A high callback could still mean that many users do not get a good recommendation. \u2022 Article coverage. The number of different recommendations that have been correctly recommended. It captures the method's ability to make diverse, successful recommendations. All these indicators are calculated \"by 10,\" i.e. in an environment in which the recommendation system produces ten recommendations for each user."}, {"heading": "4.5 Testing procedure", "text": "For each user of the test set, the method can base its recommendations on the first half of the user's ratings and on the model previously built on the training set (or on the training set directly, in the case of the next adjacent methods), and these recommendations will be evaluated on the basis of the key figures described in Section 4.4, based on the second half of the user's rating, and the key figures will then be averaged across all test users. BPR-MF is the only method that requires the user's ratings during the construction of the model. Therefore, BPR-MF is trained on a larger training set than the other methods, which includes the first half of each test user's ratings. This is an unfair but unfortunately unavoidable advantage for the BPR-MF method. Since BPR-MF and RNN produce stochastic models, Table 1 indicates the average and standard deviation of the results over ten models."}, {"heading": "4.6 Analysis", "text": "The results are in Table 1. The methods that use the sequence information are impressively much better than the others in terms of sps. It is worth considering the quality of these results: In the light of ten studies (i.e. ten recommendations), the RNN is able to predict the next movie seen by 33% of Movielens users and 40% of Netflix users, while methods that are not based on the sequence are less than 15%. As predicted in Section 3, methods with the best sps also have greater article coverage, which is an important but often overlooked aspect of the recommendations. In particular, the article coverage of the RNN is more than twice that of the user KNN. As a global observation, the RNN is proving to be a promising method for all metrics considered. It dominates the other methods in all aspects of the Movielens data set and is only surpassed in terms of the recall of BPRMF and user KNN on Netflix."}, {"heading": "5 Variations of the recurrent neu-", "text": "When applying RNN to a new problem, it is worth examining how to adapt it to the specifics of the new problem. In this section, we will examine technical details of the implementation such as the learning rate and the number of hidden neurons, then we will present a modification of the loss function that leads to more diverse recommendations, and we will examine how the RNN can use additional information such as user age or rating value to build better models."}, {"heading": "5.1 Learning method", "text": "Modern neural networks are rarely trained with the vanilla SGD approach: a number of mechanisms have been developed to speed up learning and plan the reduction in learning rate. Two mechanisms have generally proven useful: \u2022 Momentum: smoothes out gradient variations over time to avoid \"zig-zags\" during learning [13]. \u2022 Adaptive Learning: reducing the learning rate for frequently updated parameters (Adagrad [4], rmsprop). Some methods, such as Adadelta [18] and Adam [10] combine both approaches. We observed that adaptive learning appears more important in this context than dynamics, with the Adagrad, rmsprop and Adam methods working particularly well. Adagrad is particularly attractive because it only needs to adjust one parameter: the initial learning rate, while rmsprop requires two, and Adam three. Adagrad updates are made in the following way: the initial parameter is + 1, where the initial parameter is + 3, the initial parameter is + 1)."}, {"heading": "5.2 Influence of the cell size", "text": "Another important parameter of the LSTM is the number of neurons in the cell. LSTM will not be able to learn a sufficiently rich model if the number of neurons is too small, but more neurons lead to slower learning and can increase the risk of overadjustment. A good first choice for the number of neurons in the LSTM cell is the number of features that would be used to solve the same problem with matrix factorization. Figure 3 shows the validation error during training for several cell sizes. We observe that the model with 10 neurons is severely limited and the highest sps are reached with about 100 neurons."}, {"heading": "5.3 Architecture", "text": "Although we have used a vanilla LSTM in all our experiments, many other RNN architectures could have been used. In this section we briefly examine some of the other possible types of RNNs: \u2022 Bidirectional LSTM: Two LSTM are used in parallel, with one reading the inputs in chronological order and the other in reverse order. The output of both LSTM is fed into the last (Softmax) output layer. \u2022 2-layer LSTM: RNN can be stacked, the output of one LSTM feeds the next. We have tried the simplest version, with two layers of LSTM, the first reading the input and the second producing the output of the previous layer and the predictions. \u2022 GRU: the gated recurrent unit is simpler than the LSTM, with fewer gates and fewer parameters for adjustment [3]. Figure 4 compares the different architectures with the vanilla LSTM."}, {"heading": "5.4 Diversity bias", "text": "The difficulty stems mainly from the fact that the distribution of popularity among items is usually very distorted, in the presence of very few popular items and much rarer items. Of course, any model trained to optimize the chance of correct items will learn to frequently suggest the most popular items. Unfortunately, these popular items usually turn out to be trivial, useless recommendations. We present here a small modification of the objective function of the RNN, which aims to increase the variety of items without losing too much precision. In section 4, we have used categorical cross-entropy as the objective function of the RNN: L = \u2212 log (ocorrect), incorrectly indicating the value of the output neuron corresponding to the correct item. Here, we are testing a slightly different objective function that lowers the error associated with the most popular items. The same underlying reason that the item should be distorted to the most popular is the one that causes the most selective item."}, {"heading": "5.5 Extra features", "text": "The input of the RNN used in section 4 consisted only of the sequence of elements (in the form of a single hot encoding of the last element in each time step). However, many data sets have much richer information and this information could improve the model discovered by the RNN. We distinguish three types of additional information: those that concern a user (age, gender, etc.), those that concern an element (category, price, etc.) and those that concern a specific interaction between user and object (rating, review, etc.). Movielens 1M has some of each category: \u2022 the user characteristics: age (approximately by seven ages), sex and occupation (selected from twenty-two options) \u2022 the characteristics of the articles: year of publication (we used only the decade) and the genre (selected from eighteen categories) \u2022 the user characteristics: rating value (ten possible values), sex and occupation (selected from twenty-two options) \u2022 the characteristics of the article (we used only the decade of publication) \u2022 the characteristics of the article (we used only the ten)."}, {"heading": "6 Related Work", "text": "In the early 2000s, a problem called TOP1 was introduced, which they developed for collaborative filtering. Interestingly, they used objective functions other than categorical cross-entropy, namely Bayesian's personalized ranking (the same as that used by BPR-MF) and another ranking loss called TOP1, which they developed for collaborative filtering. Some previous work has referred to collaborative filtering as sequence forecasting to solve the problem. In the early 2000s, an individual problem called TOP1 was introduced, which they designed for the task. Some previous work has portrayed collaborative filtering as a sequence forecasting problem and used simpler Markov chain methods to solve it."}, {"heading": "7 Conclusion", "text": "Our experiments have shown that the LSTM delivers very good results in terms of motion and Netflix data sets, and is particularly good in terms of short-term prediction and article coverage. Better performance could still be achieved by designing RNNs specifically for the collaborative filtering task, especially at the objective function level, but the fact that the standard LSTM already works so well is further evidence of its ability to solve common problems."}, {"heading": "Acknowledgments", "text": "R. Devooght is supported by the Belgian Fonds pour la Recherche dans l'Industrie et l'Agriculture (FRIA, 1.E041.14)."}], "references": [{"title": "Recurrent neural networks for collaborative filtering, 2014. [Online; accessed 20-Mai- 2016", "author": ["E. Bernhardsson"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Recommendation as a stochastic sequential decision problem", "author": ["R.I. Brafman", "D. Heckerman", "G. Shani"], "venue": "In ICAPS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2003}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "D. Bahdanau", "Y. Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "MyMediaLite: A free recommender system library", "author": ["Z. Gantner", "S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "In Proceedings of the 5th ACM Conference on Recommender Systems (RecSys", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.-r. Mohamed", "G. Hinton"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Lstm: A search space odyssey", "author": ["K. Greff", "R.K. Srivastava", "J. Koutn\u00edk", "B.R. Steunebrink", "J. Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Session-based recommendations with recurrent neural networks", "author": ["B. Hidasi", "A. Karatzoglou", "L. Baltrunas", "D. Tikk"], "venue": "CoRR, abs/1511.06939,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1997}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Using sequential and non-sequential patterns in predictive web usage mining tasks", "author": ["B. Mobasher", "H. Dai", "T. Luo", "M. Nakagawa"], "venue": "In Data Mining,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "On the momentum term in gradient descent learning algorithms", "author": ["N. Qian"], "venue": "Neural networks,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "In Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["S. Rendle", "C. Freudenthaler", "L. Schmidt- Thieme"], "venue": "In Proceedings of the 19th international conference on World wide web,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "An mdp-based recommender system", "author": ["G. Shani", "R.I. Brafman", "D. Heckerman"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Adadelta: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Using temporal data for making recommendations", "author": ["A. Zimdars", "D.M. Chickering", "C. Meek"], "venue": "In Proceedings of the Seventeenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2001}], "referenceMentions": [{"referenceID": 5, "context": "Recurrent neural networks (RNNs) might be the most versatile methods of sequence prediction, and their recent success in speech recognition, translation and other domains [6, 17] make them good candidates for the recommendation problem.", "startOffset": 171, "endOffset": 178}, {"referenceID": 16, "context": "Recurrent neural networks (RNNs) might be the most versatile methods of sequence prediction, and their recent success in speech recognition, translation and other domains [6, 17] make them good candidates for the recommendation problem.", "startOffset": 171, "endOffset": 178}, {"referenceID": 10, "context": "RNNs are commonly used for language modeling, where they are trained to learn sequences of words [11].", "startOffset": 97, "endOffset": 101}, {"referenceID": 8, "context": "The original gated RNN is the LSTM[9], but it has spawned multiple variants [3, 7].", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "The original gated RNN is the LSTM[9], but it has spawned multiple variants [3, 7].", "startOffset": 76, "endOffset": 82}, {"referenceID": 6, "context": "The original gated RNN is the LSTM[9], but it has spawned multiple variants [3, 7].", "startOffset": 76, "endOffset": 82}, {"referenceID": 3, "context": "The results shown here are obtained using Adagrad [4] with a learning rate of 0.", "startOffset": 50, "endOffset": 53}, {"referenceID": 13, "context": "BPR-MF is a state-of-the-art matrix factorization method for top-N recommendation devised by [14].", "startOffset": 93, "endOffset": 97}, {"referenceID": 4, "context": "We used the original implementation of BPR-MF, available in the MyMediaLite framework [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 12, "context": "\u2022 Momentum: smooths the gradients variations over time in order to avoid \u201czig-zags\u201d during learning[13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 3, "context": "\u2022 Adaptive learning: decrease the learning rate for frequently updated parameter (Adagrad[4], rmsprop).", "startOffset": 89, "endOffset": 92}, {"referenceID": 17, "context": "Some methods, such as Adadelta[18] and Adam[10] combine both approaches.", "startOffset": 30, "endOffset": 34}, {"referenceID": 9, "context": "Some methods, such as Adadelta[18] and Adam[10] combine both approaches.", "startOffset": 43, "endOffset": 47}, {"referenceID": 2, "context": "\u2022 GRU: the gated recurrent unit is simpler than the LSTM, with fewer gates and fewer parameters to tune[3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 0, "context": "using it as far back as 2014 [1] to build playlists.", "startOffset": 29, "endOffset": 32}, {"referenceID": 7, "context": "More recently, [8] has applied gated RNN to session based collaborative filtering.", "startOffset": 15, "endOffset": 18}, {"referenceID": 18, "context": "In the early 2000s, [19] used a simple Markov model and tested it for web-page recommendation.", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "[12] adopted a similar approach, using sequential pattern mining.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "In [16, 2], Brafman et al.", "startOffset": 3, "endOffset": 10}, {"referenceID": 1, "context": "In [16, 2], Brafman et al.", "startOffset": 3, "endOffset": 10}, {"referenceID": 15, "context": "defended the view of recommendation systems as a Markov decision process, and although the predictive model was not their main focus, they did present in [16] a Markov chain approach, improved by some heuristics such as skipping and clustering.", "startOffset": 154, "endOffset": 158}, {"referenceID": 14, "context": "More recently, [15] introduced a rather fair approach to build personalized Markov chain, exploiting matrix factorization to fight the sparsity problem.", "startOffset": 15, "endOffset": 19}], "year": 2016, "abstractText": "We show that collaborative filtering can be viewed as a sequence prediction problem, and that given this interpretation, recurrent neural networks offer very competitive approach. In particular we study how the long short-term memory (LSTM) can be applied to collaborative filtering, and how it compares to standard nearest neighbors and matrix factorization methods on movie recommendation. We show that the LSTM is competitive in all aspects, and largely outperforms other methods in terms of item coverage and short term predictions.", "creator": "gnuplot 4.6 patchlevel 4"}}}