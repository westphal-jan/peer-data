{"id": "1608.07625", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Aug-2016", "title": "Large Scale Behavioral Analytics via Topical Interaction", "abstract": "We propose the split-diffuse (SD) algorithm that takes the output of an existing dimension reduction algorithm, and distributes the data points uniformly across the visualization space. The result, called the topic grids, is a set of grids on various topics which are generated from the free-form text content of any domain of interest. The topic grids efficiently utilizes the visualization space to provide visual summaries for massive data. Topical analysis, comparison and interaction can be performed on the topic grids in a more perceivable way.", "histories": [["v1", "Fri, 26 Aug 2016 23:07:43 GMT  (1450kb,D)", "http://arxiv.org/abs/1608.07625v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["shih-chieh su"], "accepted": false, "id": "1608.07625"}, "pdf": {"name": "1608.07625.pdf", "metadata": {"source": "CRF", "title": "Large Scale Behavioral Analytics via Topical Interaction", "authors": ["Shih-Chieh Su"], "emails": ["shihchie@qualcomm.com"], "sections": [{"heading": null, "text": "There are several measurements for each sample, the data is described in a high-dimensional space H. For example, the samples collected by a network of sensors can have the dimensionality of the number of sensors. Each master symbol can be described by several quantifiers that form the data space. In these cases, it is possible that H has tens of thousands of dimensions. To make this high-dimensional data visible to humans, a word is embedded (or a dimension reduction) technique used to map the data to a low-dimensional space. Usually L is a two-dimensional (2D) or three-dimensional (3D) space."}, {"heading": "II. Background", "text": "Consider the case to visualize the traffic drift of a network node over two time periods. There could be hundreds of millions of entries in the activity log during each time period. Instead of showing these entries line by line, a common approach is to classify the entries into topics based on the entry content. Then, we can compare the current metrics over different time periods. Some sample metrics can be entry numbers, total file size, total package size, counts in source or destination units. Likewise, one may be interested in comparing shipping patterns on millions of shipments to New York City. Or comparing the reimbursement patterns before and after a policy change to a health insurance plan. In these cases, the massive amount of entries directly in the charts can be overwhelming and difficult to compare. Instead, we want to perform a visual summary of the behavioral content and visualization method."}, {"heading": "III. The Split-Diffuse Algorithm", "text": "We propose an algorithm called the Split Diffuse (SD) algorithm to realize the above strategy. The idea of the SD algorithm is shown as a simple example in Figure 1. There are 16 points that must be placed over a 4 \u00d7 4 layout space. The SD algorithm first selects the X dimension to split it, and splits the data points into two groups that are smaller or equal to the median, and the others are not. Each group goes through this split step again over the Y dimension, as in Figure 1 (b). In this case, we iteratively divide the points in the X and Y dimensions until there is only one point in the current recursion. We track the split path in the string c. At the end of the recursion, the placement of the single point p in {p} indices is resolved as an indication of the S shape."}, {"heading": "IV. Behavior Anomaly and the Topic Grids", "text": "In fact, it is so that most of them are able to survive themselves. (...) It is not so that they are able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...) It is as if they were able to survive themselves. (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (. (.).). (. (.). (.). (. (.).). (.). (. (.). (.). (.). (.). (.). (.). (). (). (). (.). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). (). ().). (). ().). (). ().). (). (). (). ().). (). ().).). (). (). ().). (). (). ().).). ().).)."}, {"heading": "V. Performance of the Split-Diffuse Algorithm", "text": "In order to measure the performance of the split-diffuse algorithm, we randomly generate the topics in 2D space and let the algorithm distribute these topics evenly over a predefined layout in the space of the same dimensionality. Random topics are generated over a 2D space using two types of sampling approaches, namely the uniform approach, U (\u03c1): [X Y] T [0 0 1], X, Y (\u2212 0.5, 0.5) (3) and the Gaussian approach. G (X Y): [X Y] T [0 0 0 0 1] [cos \u03b8 \u2212 sin \u03b8 cos \u03b8], [X Y] N ([0], I2) (4) As mentioned in Section II, the best attempts are made to maintain the topology of these points in front of the S figure. That is, if ltpi dimensions are left of pj, S (pi) to the left of S (pj), i-dimensions to Losen (S)."}, {"heading": "A. Errors on Grid Layouts", "text": "The error rates for split diffusing U (1) and G (4\u03c0, 2) across different grid layouts are shown in Table I. ErrI decreases with increasing size of the grid set schemes at both U (1) and G (4\u03c0, 2) sampling. ErrII decreases with increasing size of the grid set schemes at U (1) sampling. Another observation is that ErrI decreases with increasing size of the grid set schemes at both U (1) and G (4\u03c0, 2) sampling, but the decrease rate varies. ErrI decreases by almost 90% with U (1) sampling when the layout decreases from 4 \u00d7 4 to 64 \u00d7 64. However, ErrI decreases only about half of its value during the same layout growth at G (4\u03c0, 2) sampling."}, {"heading": "B. Errors on Distribution of Input Data Points", "text": "The error rates for split, diffuse different sampling schemes over the same 8 \u00d7 8 error structure are shown in Figure 7. The error ratio reflects the proportion of unfulfilled constraints in the specific dimension. We also ask the SD algorithm to begin the division in the y dimension. When tested over U (\u03c1), the points distributed evenly within a unit square are shrunk to their x values by the ratio \u03c1 (Equation 3). If the SD algorithm first begins the division in the y dimension, ErrI (y) < ErrI (x) and ErrII (y) < ErrII (x) will not affect all error metrics. If the SD algorithm decides to split in the y dimension, the resulting error rates in the x and y dimensions in Figure 7 (e) will not affect all error metrics. If we decide to start the ratio of the SD-x in the xy dimension first, the algorithm will be replaced in the xx dimension."}, {"heading": "C. Error Bounds", "text": "As mentioned above, each pair of points has a constraint on each dimension within the room. In 2D space, there are completely n (n \u2212 1) constraints, where n is the number of points in {p}. Let's start by inspecting the constraints toward a single point. Figure 1 (c) shows the split path of a particular (black) data point in a 4 \u00d7 4 layout. Column # 1 ensures that the 8 blue points to the left of the split # 1 line to the left of the mapped black point are mapped. Thus, 8 constraints are met in the x dimension (the split # 2 line guarantees 4 constraints in the y dimension, and so on. On the final column regarding the black point, that is, when separated from all other points, a total of 15 constraints in the x dimension are met."}, {"heading": "D. Error on Non-square Layout", "text": "The upper part of Figure 8 (a) illustrates the strategy of splitting iteratively across X and Y dimensions, starting with the splitting in the y dimension as the default; the lower part of Figure 8 (a) incorporates the splitting strategies of the SD algorithm with Argmaxa (ga); the error in Theorem 1 adheres to both splitting strategies, as both strategies ensure that the splitting ends with each point in a single-point cell until the end. We exercise both strategies in Figure 8 (a) on an example set of 8 data points as shown in Figure 8 (b); considering the errors caused by the first splitting, the iterative strategy in the upper part of Figure 8 (b) guarantees the limitations of the y dimension (equation 5) between the above groups."}, {"heading": "VI. Conclusion and Future Work", "text": "In this paper, we introduce the topic grids to organize and visualize vast amounts of behavioral data. Once trained, the same topic grids can be used to visualize different metrics over different time periods and to compare the difference. Humans can easily perceive the difference by the location of the integer indexed grids. Current details can be further illustrated when the user interacts with the topic grids. Although we use the square layout in the essay, the technique can generate layouts in rectangles, cubes, or cuboids. The goal behind the technology is to use the visualization and interaction space as much as possible to make topical comparisons. We present the use case to analyze behavioral risk across network activities. Beyond the risk, the technology can be used to analyze current trends, expertise, learning rates, or other metrics of interest."}, {"heading": "A. Conditioned Topic Placement", "text": "The SD algorithm delegates the maintenance of the H-to-L point relationship to the existing word embedding algorithms of choice. There may be a way to map and maintain the H-to-L point relationship while enforcing the uniform spacing in L. Such an algorithm is, if available, a more direct algorithm for the uniform embedding of words. The desired aspect can be measured as a shift in the uniform L space, while the algorithm optimizes this characteristic number. In a certain situation, it is desirable to keep the clusters intact in H after they have been assigned to the uniform L space. Currently, the SD algorithm cannot guarantee such a property."}, {"heading": "B. Topic Drift over Time", "text": "There may be new topics while old topics fade away. Assuming that most topics are still the same and have a similar relationship to each other, it would be desirable to create a new SD card S \u2032 (p) similar to the old S \u2032 (p)."}, {"heading": "C. Application to Structured Data", "text": "It is also possible to apply the topic grids to the structured data on which any cluster algorithm can create cluster centers. Data points are then organized into these cluster centers, just as we use the theme to represent associated log entries."}], "references": [{"title": "On lines and planes of closest fit to systems of points in space", "author": ["K. Pearson"], "venue": "The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, vol. 2, no. 11, pp. 559\u2013572, 1901.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1901}, {"title": "Multidimensional scaling: I. theory and method", "author": ["W.S. Torgerson"], "venue": "Psychometrika, vol. 17, no. 4, pp. 401\u2013419, 1952.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1952}, {"title": "Stochastic neighbor embedding", "author": ["G.E. Hinton", "S.T. Roweis"], "venue": "Advances in Neural Information Processing Systems, 2002, pp. 833\u2013840.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["J.B. Tenenbaum", "V.D. Silva", "J.C. Langford"], "venue": "Science, vol. 290, no. 5500, pp. 2319\u20132323, 2000.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural computation, vol. 15, no. 6, pp. 1373\u20131396, 2003.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Extremely randomized trees", "author": ["P. Geurts", "D. Ernst", "L. Wehenkel"], "venue": "Machine Learning, vol. 63, no. 1, pp. 3\u201342, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2003}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G.E. Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. 2579- 2605, p. 85, 2008.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Communications of the ACM, vol. 18, no. 9, pp. 509\u2013517, 1975.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1975}, {"title": "Time bounds for selection", "author": ["M. Blum", "R.W. Floyd", "V. Pratt", "R.L. Rivest", "R.E. Tarjan"], "venue": "Journal of Computer and System Sciences, vol. 7, no. 4, pp. 448\u2013461, 1973.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1973}], "referenceMentions": [{"referenceID": 0, "context": "For example, the principal component analysis (PCA) [1] attempts to combine the dimensions of H to form the dimensions in L that best explain the variance in the data.", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "The multidimensional scaling (MDS) [2] M tries to preserve the distance between data points {p}, during the mapping from H to L, so that", "startOffset": 35, "endOffset": 38}, {"referenceID": 2, "context": "(1) The stochastic neighbor embedding (SNE) [3] type of algorithms further emphasize the local distances.", "startOffset": 44, "endOffset": 47}, {"referenceID": 3, "context": "Some popular choices include isomap [4], spectral embedding [5], and totally random trees embedding [6]).", "startOffset": 36, "endOffset": 39}, {"referenceID": 4, "context": "Some popular choices include isomap [4], spectral embedding [5], and totally random trees embedding [6]).", "startOffset": 60, "endOffset": 63}, {"referenceID": 5, "context": "Some popular choices include isomap [4], spectral embedding [5], and totally random trees embedding [6]).", "startOffset": 100, "endOffset": 103}, {"referenceID": 6, "context": "Topics are generated in H to summarize the activities, using techniques like the latent Dirichlet allocation (LDA) model [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 7, "context": "The topics are mapped from H to L via MDS in Figure 2 (a)(e) and t-SNE [8] in Figure 2 (c)(g).", "startOffset": 71, "endOffset": 74}, {"referenceID": 8, "context": "of the k-d tree[9] with some major differences", "startOffset": 15, "endOffset": 18}, {"referenceID": 9, "context": "When using the O(n) median of medians algorithm [10] in selecting the median at each depth level, the worst-case complexity is O(n logn).", "startOffset": 48, "endOffset": 52}], "year": 2016, "abstractText": "We propose the split-diffuse (SD) algorithm that takes the output of an existing dimension reduction algorithm, and distributes the data points uniformly across the visualization space. The result, called the topic grids, is a set of grids on various topics which are generated from the free-form text content of any domain of interest. The topic grids efficiently utilizes the visualization space to provide visual summaries for massive data. Topical analysis, comparison and interaction can be performed on the topic grids in a more perceivable way.", "creator": "LaTeX with hyperref package"}}}