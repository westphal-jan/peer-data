{"id": "1608.07639", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2016", "title": "Learning to generalize to new compositions in image understanding", "abstract": "Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, ~7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.", "histories": [["v1", "Sat, 27 Aug 2016 00:34:00 GMT  (2801kb,D)", "http://arxiv.org/abs/1608.07639v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["yuval atzmon", "jonathan berant", "vahid kezami", "amir globerson", "gal chechik"], "accepted": false, "id": "1608.07639"}, "pdf": {"name": "1608.07639.pdf", "metadata": {"source": "CRF", "title": "Learning to generalize to new compositions in image understanding", "authors": ["Yuval Atzmon", "Jonathan Berant", "Vahid Kezami", "Amir Globerson", "Gal Chechik"], "emails": ["yuval.atzmon@biu.ac.il"], "sections": [{"heading": "1 Introduction", "text": "Recently, deep neural networks have been successfully used for this task (MS-COCO, 2015). Although the results were both inspiring and impressive, following the analysis of the results it became clear that the current approaches suffer from two fundamental problems: First, generalization was bad for images describing scenarios that were not visible at the time of the training. Second, assessing the descriptions was difficult, however, as strong language models can generate reasonable descriptions that are missing in the e-ar Xiv: 160 8.07 639v 1 [cs.C V] 27 Aug 2sential components in the image. However, a quantitative assessment of these two problems is still wrong. In this paper, we propose to address these problems by focusing on structured representations for image descriptions."}, {"heading": "2 Generalizing to novel compositions", "text": "Our most important observation is that two types of generalizations should be separated that are of interest when creating image descriptions: the first, which generalizes to new images of the same class, is routinely evaluated, including the current data sharing of the MS-COCO challenge (Lin et al., 2014); and the second, which we focus on, is generalizing to new scenarios, similar to transfer or zeroshot learning (Fei-Fei et al., 2006), in which learning is expanded to semantically similar classes. Importantly, this generalization is the crux of learning in complex scenes, since both language and visual scenes are compositional, resulting in an exponentially large number of possible descriptions. Therefore, a central goal of learning to describe images would be to quantify the generalization to new combinations of known units and relations."}, {"heading": "3 A Structured Prediction Model", "text": "To jointly predict an SRO triplet, we train a structured prediction model based on a deep revolutionary network. First, an image is analyzed to generate candidate bouncing boxes (Erhan et al., 2014) with their labels (Szegedy et al., 2015). Similar to Xu et al. (2015), the classifier was trained on a large dataset without fine-tuning the current data. Specifically, for the structured model on the top of the deep network, we used structured SVM (SSVM) (Tsochantaridis et al., 2005) to minimize the hinge loss between the predicted and the ground truth SRO triplets. Specifically, our model learns a score function for (s, r, o) on SRO triplets, decomposed as: f (s, r, o, o) + wOfO variants see Application + RfR prediction for Bound Bound al (2014), Swunding Bound Bound (2014, Swr-Swo), Bound Bound (2015)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 The Data", "text": "We evaluated captions on the MS-COCO data (Lin et al., 2014), currently the standard benchmark for the evaluation of caption models (328K images, \u2264 5 textual descriptions per image). We analyzed MS-COCO descriptions in SRO triplets by first constructing dependency sparse trees for each description (Andor et al., 2016) and then using manually constructed patterns to extract triplets from each description. Finally, each word was contained. Removal of descriptions without SROs (due to noun phrases, rare prepositions or parsing errors) yielded 444K unique (image, SRO) pairs 1. Analysis of structured phrases and images naturally involves grounding entities to specific image locations. Datasets such as Visual-Genome et al, Krishna et al., 2016) and MS-COCO provide distinct boxes for generalized entries to form the location of many entries, here."}, {"heading": "4.2 Compared Methods", "text": "We compared the following methods and baselines: 1. SSVM / Conv Our model, described in paragraph 3.2. Show-Attend-and-Tell (SA & T). A state-of-the-art RNN attention model for captions generation (Xu et al., 2015). We retrained the decoder layers to predict SRO triplets with soft attention. Hyper parameters were adjusted to maximize the accuracy of a rating set, the learning rate in (0.1, 0.05, 10 \u2212 1, 10 \u2212 3) and the weight drop in (0, 10 \u2212 8, 10 \u2212 7,.,., 10 \u2212 2). Importantly, we also controlled the model capacity by adjusting the dimensions of the embedding (100, 200, 400,...) and the weight loss in (0, 10 \u2212 8, 10 \u2212 7,.,., 10 \u2212 2)."}, {"heading": "4.3 Evaluation procedure", "text": "We test all candidate pairs of Bounding Boxes (BB) on one image. For each BB pair, all candidate SRO triplets are sorted according to their results and compared with the group of soil truth triplets to calculate precision @ k for this image. Images can have more than one soil truth SRO, as they are associated with up to 5 descriptions. For captions, BLEU score is a common measurement. Here, SROaccuracy corresponds to BLEU-3, and a term accuracy corresponds to BLEU-1. We found that the calculation of BLEU between a description and its SRO is too noisy. Our rating metric does not treat semantic smearing, namely the case where an image can be described in multiple ways, all semantically adequate, but using different words and therefore counting as an error."}, {"heading": "4.4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 Compositional vs. within-class generalization", "text": "Figure 2 and Table 1 show an average precision @ k across the images, comparing SSVM and SA & T in terms of test and training performance; in the uppermost panel, both methods are trained and tested on the MSCOCO split; the SSVM / Conv model (blue) wins with a precision of p @ 1 = 10.6% and the SA & T model (green) achieves p @ 1 = 9.4%; the baseline test accuracy was p @ 1 = 0.028% for SC; the most common S, R and O in the dataset were man with and table, but the treble (man with table) did not appear in the data at all and yielded 0% MF accuracy; the effect is noticeable in the compositional split (lower panel), where the SSVM / Conv model transfers well to new combinations (compare training p @ 1 = 8.3% and test p @ 1 = 6% \u00b1 0.7%)."}, {"heading": "4.4.2 Model complexity", "text": "Therefore, we tested SA & T with different capacities and varied the number of parameters (Word Dimonsionality and LSTM hidden state Dimonsionality).As expected, the training error decreased with the number of parameters.Importantly, the test error decreased to a certain point and then started to risk due to overfit.For the MS-COCO split, the best test error was SA & T better than the SSVM model, but for the compositional split.In other words, a wide range of LSTM parameters still does not generalize well to the compositional split.Importantly, the number of examples in our experiments is well within the dataset size that SA & T originally used (Flickr8k, Flickr30k, COCO)."}, {"heading": "4.4.3 Comparing SSVM models", "text": "To get a better understanding of the signals useful for the SRO prediction, we compared several variants of the SSVM model, each of which uses different characteristics as potential inputs for the R node, for details on the potentials see Appendix A.1. SSVM R Subject + Object: The R Node Potential takes the object category (O) and the subject category (S), each represented as a sparse \"one-hot\" vector. 2. SSVM R Object: The R Node Potential only takes the object category (O), which is represented as a sparse \"one-hot\" vector. 4. SSVM Spatial: The potential inputs for the R node include only spatial outcomes. 5. SSVM R Subject Category: The inputs include both the spatial characteristics and the object category, which have a hot-vector effect for the spatial SR6."}, {"heading": "4.4.4 Manual evaluation", "text": "Since images can be described in innumerable ways, we sampled 100 random predictions of the SRO model manually to assess the actual model accuracy. For each SRO prediction, we answered two questions: (a) Does this SRO exist in the image (b) Is this an appropriate SRO description for the image. In 32% of the cases, the SSVM produced an SRO that exists in the image, and in 23% of the cases, it was a reasonable description of the image."}, {"heading": "5 Related Work", "text": "The automatic description of images has been developed by several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015) and has also been applied to parts of images (Johnson et al., 2015a; Krishna et al., 2016). Compositional aspects of language and images have recently been studied by (Andreas et al., 2015), which took on a visual QA task by splitting up questions into substructures and reusing modular networks. (Johnson et al., 2015b) combined subjects, objects and relationships in a graph structure for image restoration. (Kulkarni et al al, 2011) learned spatial relations to generalize descriptions based on a template (al et al., 2015b), combining scenes, objects and relationships in a graph structure for image restoration. (Kulkarni et al al, 2011)"}, {"heading": "6 Summary", "text": "This paper contains two main contributions: First, we examine the role of generalizing new combinations of known objects in vision-to-language problems and propose an experimental framework for measuring this compositional generalization; second, we note that existing, state-of-the-art captioning models are poorly generalizing new combinations compared to a structured predictive model; and, in future work, we plan to expand our approach to full captions and address deeper semantic structures, including modifiers, adjectives, and more."}, {"heading": "Appendix A: A structured-SVM model", "text": "Our model learns a score function f (s, r, o) on SRO triplets, broken down as: wSfS (s) + wOfO (o) + wRfR (r) + wSRfSR (s, r) + wROfRO (r, o), where wS, wO, wSR, wRO are scalar weights learned by the algorithm. We learned a sparse linear transformation matrix from the caption localization vocabulary (O, wR, wSRO, wSRO), based on empirical common probabilities on training data. fS (\"cow\") was learned to be a weighted combination of probabilities that the localizer classes {\"ox,\" bull, \"\" calf. \"Object node potential fO (o)."}, {"heading": "Appendix B: matching visual entities to caption terms", "text": "When creating the dataset, we selected the images in which the visual entities could be mapped to terms in the captions. Since the vocabulary of visual entity recognition (used by the localizer) differs from the vocabulary of captions, we estimated a mapping from the localization vocabulary to the captions terms using the method used by Zitnick et al. (2013). Specifically, we calculated PMI between the captions predicted by the localizer for the Bounding Boxes (BBLs) and the nouns in the SRO. (2) We took into account the top 5 matches for each S / O vocabulary and manually cropped outliers (for example, the term bed had a high MI with cat recognition). (3) We removed a data sample if the S / O captions did not match any of the BBLs. This PMI step led us to only having the 300 entries included in the transformations."}], "references": [{"title": "Deep compositional question answering with neural module networks. arXiv preprint arXiv:1511.02799", "author": ["Marcus Rohrbach", "Trevor Darrell", "Dan Klein"], "venue": null, "citeRegEx": "Andreas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Andreas et al\\.", "year": 2015}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["Chen", "Zitnick2014] Xinlei Chen", "Lawrence Zitnick"], "venue": "arXiv preprint arXiv:1411.5654", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["Lisa Anne Hendricks", "Sergio Guadarrama", "Marcus Rohrbach", "Subhashini Venugopalan", "Kate Saenko", "Trevor Darrell"], "venue": null, "citeRegEx": "Donahue et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Donahue et al\\.", "year": 2015}, {"title": "Scalable object detection using deep neural networks", "author": ["Erhan et al.2014] Dumitru Erhan", "Christian Szegedy", "Alexander Toshev", "Dragomir Anguelov"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Erhan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Erhan et al\\.", "year": 2014}, {"title": "From captions to visual concepts and back", "author": ["Fang et al.2015] Hao Fang", "Saurabh Gupta", "Forrest Iandola", "Rupesh K Srivastava", "Li Deng", "Piotr Doll\u00e1r", "Jianfeng Gao", "Xiaodong He", "Margaret Mitchell", "John C Platt"], "venue": "Proceedings of the IEEE Conference", "citeRegEx": "Fang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fang et al\\.", "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["Farhadi et al.2010] Ali Farhadi", "Mohsen Hejrati", "Mohammad Amin Sadeghi", "Peter Young", "Cyrus Rashtchian", "Julia Hockenmaier", "David Forsyth"], "venue": null, "citeRegEx": "Farhadi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Farhadi et al\\.", "year": 2010}, {"title": "One-shot learning of object categories", "author": ["Fei-Fei et al.2006] Li Fei-Fei", "Rob Fergus", "Pietro Perona"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions", "citeRegEx": "Fei.Fei et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Fei.Fei et al\\.", "year": 2006}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["Frome et al.2013] Andrea Frome", "Greg S Corrado", "Jon Shlens", "Samy Bengio", "Jeff Dean", "Tomas Mikolov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Frome et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Frome et al\\.", "year": 2013}, {"title": "2015a. Densecap: Fully convolutional localization networks for dense captioning", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1511.07571", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "2015b. Image retrieval using scene graphs", "author": ["Ranjay Krishna", "Michael Stark", "Li-Jia Li", "David A Shamma", "Michael S Bernstein", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Karpathy", "Fei-Fei2015] Andrej Karpathy", "Li FeiFei"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Karpathy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy et al\\.", "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models. arXiv preprint arXiv:1411.2539", "author": ["Kiros et al.2014] Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": null, "citeRegEx": "Kiros et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Visual genome: Connecting language and vision using crowdsourced dense image", "author": ["Yuke Zhu", "Oliver Groth", "Justin Johnson", "Kenji Hata", "Joshua Kravitz", "Stephanie Chen", "Yannis Kalantidis", "Li-Jia Li", "David A Shamma"], "venue": null, "citeRegEx": "Krishna et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Krishna et al\\.", "year": 2016}, {"title": "Baby talk: Understanding and generating image descriptions", "author": ["Visruth Premraj", "Sagnik Dhar", "Siming Li", "Yejin Choi", "Alexander C. Berg", "Tamara L. Berg"], "venue": "In Proceedings of the 24th CVPR", "citeRegEx": "Kulkarni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2011}, {"title": "Microsoft coco: Common objects in context", "author": ["Lin et al.2014] Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "Lawrence Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Lin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Visual relationship detection with language priors", "author": ["Lu et al.2016] Cewu Lu", "Ranjay Krishna", "Michael Bernstein", "Li Fei-Fei"], "venue": "In European Conference on Computer Vision", "citeRegEx": "Lu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2016}, {"title": "Explain images with multimodal recurrent neural networks. arXiv preprint arXiv:1410.1090", "author": ["Mao et al.2014] Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Alan L Yuille"], "venue": null, "citeRegEx": "Mao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mao et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": null, "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Large margin methods for structured and interdependent output variables", "author": ["Thorsten Joachims", "Thomas Hofmann", "Yasemin Altun"], "venue": "In Journal of Machine Learning Research,", "citeRegEx": "Tsochantaridis et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Tsochantaridis et al\\.", "year": 2005}, {"title": "Translating videos to natural language using deep recurrent neural networks. arXiv preprint arXiv:1412.4729", "author": ["Huijuan Xu", "Jeff Donahue", "Marcus Rohrbach", "Raymond Mooney", "Kate Saenko"], "venue": null, "citeRegEx": "Venugopalan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Latent embeddings for zero-shot classification", "author": ["Xian et al.2016] Yongqin Xian", "Zeynep Akata", "Gaurav Sharma", "Quynh Nguyen", "Matthias Hein", "Bernt Schiele"], "venue": "arXiv preprint arXiv:1603.08895", "citeRegEx": "Xian et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xian et al\\.", "year": 2016}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Yoshua Bengio"], "venue": "arXiv preprint arXiv:1502.03044", "citeRegEx": "Bengio.,? \\Q2015\\E", "shortCiteRegEx": "Bengio.", "year": 2015}, {"title": "Situation recognition: Visual semantic role labeling for image understanding", "author": ["Yatskar et al.2016] Mark Yatskar", "Luke Zettlemoyer", "Ali Farhadi"], "venue": "In Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Yatskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yatskar et al\\.", "year": 2016}, {"title": "Learning the visual interpretation of sentences", "author": ["Devi Parikh", "Lucy Vanderwende"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "Zitnick et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zitnick et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "As a first step, we use simple structured representations consisting of subject-relationobject (SRO) triplets (Farhadi et al., 2010).", "startOffset": 110, "endOffset": 132}, {"referenceID": 14, "context": "The first, generalizing to new images of the same class, is routinely being evaluated, including in the current data split of the MS-COCO challenge (Lin et al., 2014).", "startOffset": 148, "endOffset": 166}, {"referenceID": 6, "context": "The second type, which we focus on, is concerned with generalizing to new scenarios, akin to transfer or zeroshot learning (Fei-Fei et al., 2006), where learning is extended to semantically-similar classes.", "startOffset": 123, "endOffset": 145}, {"referenceID": 3, "context": "First, an image is analyzed to produce candidate bounding boxes (Erhan et al., 2014)", "startOffset": 64, "endOffset": 84}, {"referenceID": 17, "context": "with their labels (Szegedy et al., 2015).", "startOffset": 18, "endOffset": 40}, {"referenceID": 17, "context": "with their labels (Szegedy et al., 2015). Similar to Xu et al. (2015), the classifier was trained on a large dataset without fine-tuning on the current data.", "startOffset": 19, "endOffset": 70}, {"referenceID": 18, "context": "For the structured model on top of the deep network, we used structured SVM (SSVM) (Tsochantaridis et al., 2005), minimizing the hinge loss be-", "startOffset": 83, "endOffset": 112}, {"referenceID": 14, "context": "We evaluated image captioning on the MS-COCO data (Lin et al., 2014), currently the standard bench-", "startOffset": 50, "endOffset": 68}, {"referenceID": 12, "context": "Datasets like Visual-Genome (Krishna et al., 2016) and MS-COCO provide human-marked bounding boxes for many entities.", "startOffset": 28, "endOffset": 50}, {"referenceID": 3, "context": "larger datasets, we instead inferred bounding boxes using a pre-trained deep-network localizer (Erhan et al., 2014).", "startOffset": 95, "endOffset": 115}, {"referenceID": 24, "context": "To match the two vocabularies we followed the procedure of Zitnick et al. (2013), see Appendix B for details.", "startOffset": 59, "endOffset": 81}, {"referenceID": 16, "context": "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images", "startOffset": 15, "endOffset": 210}, {"referenceID": 11, "context": "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images", "startOffset": 15, "endOffset": 210}, {"referenceID": 2, "context": "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images", "startOffset": 15, "endOffset": 210}, {"referenceID": 20, "context": "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images", "startOffset": 15, "endOffset": 210}, {"referenceID": 19, "context": "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images", "startOffset": 15, "endOffset": 210}, {"referenceID": 4, "context": "several groups (Xu et al., 2015; Karpathy and FeiFei, 2015; Mao et al., 2014; Kiros et al., 2014; Donahue et al., 2015; Vinyals et al., 2015; Venugopalan et al., 2014; Chen and Zitnick, 2014; Fang et al., 2015), and was also applied to parts of images", "startOffset": 15, "endOffset": 210}, {"referenceID": 12, "context": "(Johnson et al., 2015a; Krishna et al., 2016).", "startOffset": 0, "endOffset": 45}, {"referenceID": 0, "context": "have been recently explored by (Andreas et al., 2015), who approached a visual QA task by breaking questions into substructures, and re-using modular networks.", "startOffset": 31, "endOffset": 53}, {"referenceID": 13, "context": "(Kulkarni et al., 2011) learned spatial relations for generating descriptions based on a template.", "startOffset": 0, "endOffset": 23}, {"referenceID": 24, "context": "(Zitnick et al., 2013) modelled synthetic scenes generated using CRF.", "startOffset": 0, "endOffset": 22}, {"referenceID": 23, "context": "The dataset of (Yatskar et al., 2016) has combinations of entities", "startOffset": 15, "endOffset": 37}, {"referenceID": 5, "context": "(Farhadi et al., 2010) developed ways to match sentences and images, through a space of meaning parametrized by subject-verbobject triplets which our structured model is closely related to.", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "Very recently, (Lu et al., 2016) trained a model that leverages language priors from semantic embeddings to predict subject-relation-object tuples.", "startOffset": 15, "endOffset": 32}, {"referenceID": 24, "context": "tity recognition (used by the localizer) differs from the vocabulary of captions, we estimated a mapping from the locaizer vocabulary to the caption terms following the procedure of Zitnick et al. (2013).", "startOffset": 182, "endOffset": 204}], "year": 2016, "abstractText": "Recurrent neural networks have recently been used for learning to describe images using natural language. However, it has been observed that these models generalize poorly to scenes that were not observed during training, possibly depending too strongly on the statistics of the text in the training data. Here we propose to describe images using short structured representations, aiming to capture the crux of a description. These structured representations allow us to tease-out and evaluate separately two types of generalization: standard generalization to new images with similar scenes, and generalization to new combinations of known entities. We compare two learning approaches on the MS-COCO dataset: a state-of-the-art recurrent network based on an LSTM (Show, Attend and Tell), and a simple structured prediction model on top of a deep network. We find that the structured model generalizes to new compositions substantially better than the LSTM, \u223c7 times the accuracy of predicting structured representations. By providing a concrete method to quantify generalization for unseen combinations, we argue that structured representations and compositional splits are a useful benchmark for image captioning, and advocate compositional models that capture linguistic and visual structure.", "creator": "LaTeX with hyperref package"}}}