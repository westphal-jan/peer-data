{"id": "1608.07685", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2016", "title": "KSR: A Semantic Representation of Knowledge Graph within a Novel Unsupervised Paradigm", "abstract": "Knowledge representation is a critical topic in AI, and currently embedding as a key branch of knowledge representation takes the numerical form of entities and relations to joint the statistical models. However, most embedding methods merely concentrate on the triple fitting and ignore the explicit semantic expression, leading to an uninterpretable representation form. Thus, traditional embedding methods do not only degrade the performance, but also restrict many potential applications. For this end, this paper proposes a semantic representation method for knowledge graph \\textbf{(KSR)}, which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Because both the aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments justify our model outperforms other state-of-the-art baselines in a substantial extent.", "histories": [["v1", "Sat, 27 Aug 2016 09:53:38 GMT  (242kb,D)", "http://arxiv.org/abs/1608.07685v1", null], ["v2", "Tue, 25 Oct 2016 02:48:01 GMT  (220kb,D)", "http://arxiv.org/abs/1608.07685v2", "submitting to AAAI.2017"], ["v3", "Tue, 13 Jun 2017 02:06:16 GMT  (146kb,D)", "http://arxiv.org/abs/1608.07685v3", "submitting to NIPS.2017"]], "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["han xiao", "minlie huang", "xiaoyan zhu"], "accepted": false, "id": "1608.07685"}, "pdf": {"name": "1608.07685.pdf", "metadata": {"source": "CRF", "title": "Knowledge Semantic Representation A Generative Model for Interpretable Knowledge Graph Embedding", "authors": ["Han Xiao", "Minlie Huang", "Xiaoyan Zhu"], "emails": ["bookman@vip.163.com;", "aihuang@tsinghua.edu.cn", "zxy-dcs@tsinghua.edu.cn"], "sections": [{"heading": "Introduction", "text": "In fact, most of them are able to survive on their own."}, {"heading": "Related Work", "text": "TransE (Bordes et al. 2013) is a groundbreaking work on the translation-based principle, which represents the possibility of triples and a smaller score, so is better. Next variants transform entities into different subspaces to play different roles. TransH (Wang et al. 2014b) uses the relationship-specific hyperplane to position the entities. TransR (Lin et al. 2015) uses the relationship-related matrix to transform the embedding space into different subspaces. TransH (Wang et al.) uses the relationship-related matrix to rotate the embedding space. Such research also includes TransG (Xiao et al, Huang, and Zhu 2016b), TransA (Xiao et al. 2015), TransD (Ji et al.) and TransM (Fan et al. 2014).Further research incorporates additional structural information into the embedding."}, {"heading": "Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Model Description", "text": "We use a hierarchical generation process on two levels to semantically represent the knowledge elements (Entities / Relations / Triples), while all categories (Uniples) are defined as follows: For each Triple (h, r, t), (First-Level), (First-Level), (Second-Level), (Second-Level), (Second-Level), (Second-Level), (Second-Level), (X-Level), (X-Level), (X-Level), (X-Level), (X-Level), (X-Level), (X-Level), (X-Level), (X-Level), (X-Level), (X, X-X, X-X, X-X-Level), (X-X, X-X-X-Level), (X-X-X, X-X-X-Level, X-X-X-Level, X-X-X-Level, X-X-Level, X-X-X-Level, X-X-Level (X-Level), X-Level, X-X-Level (X-Level, X-Level), X-Level (X-Level, X-Level, X-X-Level, X-Level, X-Level, X-X-Level, X-Level)"}, {"heading": "Objective & Training", "text": "To be better differentiated, we maximize the ratio of the probability of true tripling to the false one. Our goal is as follows: \u2022 (h, r, t) \u00b7 (h, r, t) \u00b7 (h, r, t) \u00b7 (h, r, t) \u00b7 (h, \"r, t.\" The specific formula for [h, \"r,\" t \"[h,\" r, \"t\" (4) is very similar to (Xiao, Huang and Zhu 2016a).In terms of efficiency, the temporal complexity of our training algorithm is O (nd), where n is the attribute number and d is the category number for each attribute."}, {"heading": "Clustering Perspective", "text": "With respect to the hybrid form of equations (1) - (3), our method represents at both the first and second levels the spirit of the hybrid model, which could be further analyzed from the perspective of clustering. The generative process of the second level bundles the knowledge elements (units / relationships / triples) according to some aspects related to knowledge. These aspects come from the first level, mathematically according to all likely terms associated with fi. Furthermore, the generative process of the first level adapts different knowledge spaces with the feed-back of the second level. Mathematically, the feed-back corresponds to [z1.. n, y1.. n, f | h, r, t]. Essentially, knowledge is grouped semantically into a form with multiple views, so that by modelling the multi-view cluster nature, KSR is semantically interpretable as a unit."}, {"heading": "Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Settings", "text": "We refer readers to the statistics of these data sets (Xiao, Huang and Zhu 2016a) and (Xie et al. 2016). The entity descriptions of FB15K are the same as DKRL (Xie et al. 2016), each of which constitutes only a small part of the corresponding wiki page. WN18 text information is the definitions we get from the order. Implementation. We have implemented TransE, TransH, TransR and ManifoldE for comparison, reproducing the polished results directly with the reported optimal parameters. Note that some results are reported for the same task directly from related literature. The optimal settings of KSR are learning factor \u03b1 = 0.0004, Margin-ig = 2.5 and Laplace hyperparameters = 0.04. For a fair comparison within the same quantity, we assume three quantities (Settings = 10 = Settings), most settings = 10 settings (settings = settings)."}, {"heading": "Entity Classification", "text": "motivations. In order to attest to our semantic performance, we perform the classification prediction of the entity. Since the entity type such as Human Language, Artist and Book Author is a semantically relevant task, this task could justify the semantic representation. Overall, this is a multi-level classification task with 25 / 50 / 75 classes, which means for each entity that the method should provide a series of types instead of a specific type. In classification training, we use the concatenation of the category distribution ([z1 | e], [z1 | e],..., [zn | e]) as an entity representation, where [zi | e] is a distribution implemented as a vector. Entity representation is the feature for the classifier. To compare fairly, our front-end classifier is identical to the logistic regression in a one-to-to-to-to-rest setting for the multi-level classification (the precision follows the rule)."}, {"heading": "Knowledge Graph Completion", "text": "Motivation. This task is a benchmark task, a.k.a \"Link Prediction,\" which concerns the ability to identify triples. Many NLP tasks could benefit from the link prediction, such as relationship extraction (Hoffmann et al. 2011).Evaluation protocol. The same protocol used in previous studies is adopted. Firstly, for each test triple (h, r, t), we replace the tail t (or head h) with each unit e in the knowledge diagram. Then, a probable value of this corrupt triple is calculated using the score function fr (h, t). By ranking these values in ascending order, we then get the rank of the original triple. The evaluation metrics are the average of the ranks as Mean Rank and the proportion of the tests triple whose rank is not greater than 10 (as HITS @ 10). This is referred to as the \"Raw\" setpoint setting. When we filter out the corrupt three-dimensions in training, the existing ones are filtered out."}, {"heading": "Semantic Analysis: Case Study", "text": "In fact, FB15K is more complex than this setting, so many smaller features and categories would be suppressed. Taking this setting into account is to facilitate research and presentation. First, we analyze the specific semantics of each individual feature. We use the entity descriptions to calculate the common probability of the word in the textual descriptions of an entity table."}, {"heading": "Semantic Analysis: Statistical Justification", "text": "We perform two statistical analyses in the same environment as the \"Case Study\" subsection. First, we randomly select 100 units and manually verify the correctness of semantic representations according to common knowledge. There are 68 units whose semantic representations are perfectly correct and also 19 units whose representations are incorrect for only one characteristic. There are only 13 units whose corresponding representations are incorrect for more than one characteristic. Thus, the result proves the strong semantic expressiveness of KSR. Figure 3: The heatmap of the correlations between knowledge characteristics in semantic analysis. Dark color corresponds to the high correlation and light color indicates the weakness. Second, if two characteristics (both with the category yes) occur in a semantic representation of a unit / relationship, this knowledge element (unit / relationship) contributes to the correlation between the two characteristics."}, {"heading": "Conclusion", "text": "In this thesis we propose the topic of knowledge semantics (KSA) and then develop the method of knowledge semantic representation (KSR) for it, which is a hierarchical generational process on two levels in order to explicitly present knowledge semantically. We also analyze our method from the perspective of clustering and identification. Experimental results justify the effectiveness and semantic expressiveness of our method."}], "references": [{"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Bollacker"], "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,", "citeRegEx": "Bollacker,? \\Q2008\\E", "shortCiteRegEx": "Bollacker", "year": 2008}, {"title": "Learning structured embeddings of knowledge bases", "author": ["Bordes"], "venue": "In Proceedings of the Twenty-fifth AAAI Conference on Artificial Intelligence", "citeRegEx": "Bordes,? \\Q2011\\E", "shortCiteRegEx": "Bordes", "year": 2011}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Bordes"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bordes,? \\Q2013\\E", "shortCiteRegEx": "Bordes", "year": 2013}, {"title": "T", "author": ["M. Fan", "Q. Zhou", "E. Chang", "Zheng"], "venue": "F.", "citeRegEx": "Fan et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantically smooth knowledge graph embedding", "author": ["Guo"], "venue": "In Proceedings of ACL", "citeRegEx": "Guo,? \\Q2015\\E", "shortCiteRegEx": "Guo", "year": 2015}, {"title": "Learning to represent knowledge graphs with gaussian embedding", "author": ["He"], "venue": "In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management,", "citeRegEx": "He,? \\Q2015\\E", "shortCiteRegEx": "He", "year": 2015}, {"title": "D", "author": ["R. Hoffmann", "C. Zhang", "X. Ling", "L. Zettlemoyer", "Weld"], "venue": "S.", "citeRegEx": "Hoffmann et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "G", "author": ["R. Jenatton", "N.L. Roux", "A. Bordes", "Obozinski"], "venue": "R.", "citeRegEx": "Jenatton et al. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin"], "venue": "In Proceedings of the TwentyNinth AAAI Conference on Artificial Intelligence", "citeRegEx": "Lin,? \\Q2015\\E", "shortCiteRegEx": "Lin", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Liu Lin", "Y. Sun 2015] Lin", "Z. Liu", "M. Sun"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "G", "author": ["Miller"], "venue": "A.", "citeRegEx": "Miller 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "and Chang", "author": ["A. Neelakantan"], "venue": "M.-W.", "citeRegEx": "Neelakantan and Chang 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Holographic embeddings of knowledge graphs. arXiv preprint arXiv:1510.04935", "author": ["Rosasco Nickel", "M. Poggio 2015] Nickel", "L. Rosasco", "T. Poggio"], "venue": null, "citeRegEx": "Nickel et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Tresp Nickel", "M. Kriegel 2011] Nickel", "V. Tresp", "H.-P. Kriegel"], "venue": "In Proceedings of the 28th international conference on machine learning (ICML-11),", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "C", "author": ["R. Socher", "D. Chen", "Manning"], "venue": "D.; and Ng, A.", "citeRegEx": "Socher et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Knowledge graph and text jointly embedding", "author": ["Wang"], "venue": "Citeseer", "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Wang,? \\Q2014\\E", "shortCiteRegEx": "Wang", "year": 2014}, {"title": "Knowledge base completion using embeddings and rules", "author": ["Wang Wang", "Q. Guo 2015] Wang", "B. Wang", "L. Guo"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence", "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "TransA: An adaptive approach for knowledge graph embedding", "author": ["Xiao"], "venue": "arXiv preprint arXiv:1509.05490", "citeRegEx": "Xiao,? \\Q2015\\E", "shortCiteRegEx": "Xiao", "year": 2015}, {"title": "2016a. From one point to a manifold: Knowledge graph embedding for precise link prediction", "author": ["Huang Xiao", "H. Zhu 2016a] Xiao", "M. Huang", "X. Zhu"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelligence", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Transg : A generative model for knowledge graph embedding", "author": ["Huang Xiao", "H. Zhu 2016b] Xiao", "M. Huang", "X. Zhu"], "venue": "In Proceedings of the 29th international conference on computational linguistics", "citeRegEx": "Xiao et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiao et al\\.", "year": 2016}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["Xie"], "venue": null, "citeRegEx": "Xie,? \\Q2016\\E", "shortCiteRegEx": "Xie", "year": 2016}, {"title": "Aligning knowledge and text embeddings by entity descriptions", "author": ["Zhong"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Zhong,? \\Q2015\\E", "shortCiteRegEx": "Zhong", "year": 2015}], "referenceMentions": [], "year": 2017, "abstractText": "Knowledge representation is a critical topic in AI, and currently embedding as a key branch of knowledge representation takes the numerical form of entities and relations to joint the statistical models. However, most embedding methods merely concentrate on the triple fitting and ignore the explicit semantic expression, leading to an uninterpretable representation form. Thus, traditional embedding methods do not only degrade the performance, but also restrict many potential applications. For this end, this paper proposes a semantic representation method for knowledge graph (KSR), which imposes a two-level hierarchical generative process that globally extracts many aspects and then locally assigns a specific category in each aspect for every triple. Because both the aspects and categories are semantics-relevant, the collection of categories in each aspect is treated as the semantic representation of this triple. Extensive experiments justify our model outperforms other state-of-the-art baselines in a substantial", "creator": "LaTeX with hyperref package"}}}