{"id": "1608.07719", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2016", "title": "Temperature-Based Deep Boltzmann Machines", "abstract": "Deep learning techniques have been paramount in the last years, mainly due to their outstanding results in a number of applications, that range from speech recognition to face-based user identification. Despite other techniques employed for such purposes, Deep Boltzmann Machines are among the most used ones, which are composed of layers of Restricted Boltzmann Machines (RBMs) stacked on top of each other. In this work, we evaluate the concept of temperature in DBMs, which play a key role in Boltzmann-related distributions, but it has never been considered in this context up to date. Therefore, the main contribution of this paper is to take into account this information and to evaluate its influence in DBMs considering the task of binary image reconstruction. We expect this work can foster future research considering the usage of different temperatures during learning in DBMs.", "histories": [["v1", "Sat, 27 Aug 2016 15:31:21 GMT  (1169kb)", "http://arxiv.org/abs/1608.07719v1", "Submitted to Neural Processing Letters"], ["v2", "Sun, 4 Sep 2016 00:55:52 GMT  (1169kb)", "http://arxiv.org/abs/1608.07719v2", "Submitted to Neural Processing Letters"]], "COMMENTS": "Submitted to Neural Processing Letters", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["leandro aparecido passos junior", "joao paulo papa"], "accepted": false, "id": "1608.07719"}, "pdf": {"name": "1608.07719.pdf", "metadata": {"source": "CRF", "title": "Temperature-Based Deep Boltzmann Machines", "authors": ["Leandro Aparecido Passos J\u00fanior", "Jo\u00e3o Paulo Papa"], "emails": ["leandropassosjr@gmail.com", "papa@fc.unesp.br"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.07 719v 1 [cs.L G] 27 Aug 2Keywords Deep Learning \u00b7 Deep Boltzmann Machines \u00b7 Machine Learning"}, {"heading": "1 Introduction", "text": "In this context, it should be noted that this is a project, which is a project, which is primarily a project, which is primarily a project, which is primarily a project."}, {"heading": "2 Deep Boltzmann Machines", "text": "In this section, we will briefly explain the theoretical background relating to RBMs and DBMs. [2] The learning phase is carried out using an unsupervised manner. [3] The architecture of a restricted Boltzmann machine includes a visible layer v with m units and a hidden layer h with n units. [4] In addition, a real evaluated matrix Wm \u00b7 n models of the weights between the visible and hidden neurons are created, in which the weight between the visible unit vi and the hidden unit hj.Let us assume that both v and h are regarded as binary evaluated units. [4] The energy function of a restricted Boltzmann machine is given by: E (v, h) = \u2212 m units."}, {"heading": "3 Methodology", "text": "In this section, we present the methodology used to evaluate the proposed approach, as well as the data sets and experimental setup.3.1 \u00b7 DatasetsWe propose the behavior of DBMs at different temperatures in the context of the reconstruction of binary images using three public datasets, as described below: - MNIST Dataset1: it consists of images of handwritten digits. The original version contains a training set of 60,000 images from the digits \"0\" - \"9,\" 1 http: / yann.lecun.com / exdb / mnist / as a test set of 10,000 images2. Due to the high computational load for selecting the DBM model, we have decided to use the original test set along with a reduced version of the training set3. - CalTech 101 Silhouettes Data Set4: it is based on the previous Caltech 101 dataset, and it includes silhouettes of the resolution of 28 x 28. We have used only the training set3."}, {"heading": "4 Experimental Results", "text": "This year we have the opportunity to put ourselves at the top of the list in the same way that we put ourselves at the top of the leaderboard."}, {"heading": "5 Conclusions and Future Works", "text": "Inspired by a very recent paper suggesting the temperature-based Restricted Boltzmann machines [13], we decided to evaluate the influence of temperature in learning with Deep Boltzmann machines, which aim at the task of reconstructing binary images. Our results confirm the hypothesis raised by Li et al. [13], according to which the lower the temperature, the more generalized the network is, in order to obtain more accurate results. We observed that the network pushes the weights down at lower temperatures to favor sparseness, as the probability of matching to hidden units at lower temperatures is greater. In future work, we aim to propose an adaptive temperature that can be increased / decreased linearly along the iterations in order to accelerate the convergence process."}, {"heading": "Acknowledgments", "text": "The authors would like to thank FAPESP # 2014 / 16250-9, Capes and CNPq # 306166 / 2014-3."}], "references": [{"title": "Black holes and entropy", "author": ["J.D. Bekenstein"], "venue": "Physical Review D 7(8), 2333", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1973}, {"title": "2015 ieee conference on beyond principal components: Deep boltzmann machines for face modeling", "author": ["C.N. Duong", "K. Luu", "K.G. Quach", "T.D. Bui"], "venue": "Computer Vision and Pattern Recognition, CVPR \u201915, pp. 4786\u20134794", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Origin of generalized entropies and generalized statistical mechanics for superstatistical multifractal systems", "author": ["B. Gadjiev", "T. Progulova"], "venue": "International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering, vol. 1641, pp. 595\u2013602", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Computer Vision \u2013 ECCV 2012: 12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012, Proceedings, Part V, chap", "author": ["H. Goh", "N. Thome", "M. Cord", "J.H. Lim"], "venue": "Unsupervised and Supervised Visual Codes with Restricted Boltzmann Machines, pp. 298\u2013311. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Maxwell\u2013boltzmann statistics and the metaphysics of modality", "author": ["B.L. Gordon"], "venue": "Synthese 133(3), 393\u2013417", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Science 313(5786), 504 \u2013 507", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Discovering binary codes for documents by learning deep generative models", "author": ["G. Hinton", "R. Salakhutdinov"], "venue": "Topics in Cognitive Science 3(1), 74\u201391", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation 14(8), 1771\u20131800", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2002}, {"title": "Neural networks: Tricks of the trade: Second edition", "author": ["G.E. Hinton"], "venue": "chap. A Practical Guide to Training Restricted Boltzmann Machines, pp. 599\u2013619. Springer Berlin Heidelberg, Berlin, Heidelberg", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation 18(7), 1527\u20131554", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning algorithms for the classification restricted boltzmann machine", "author": ["H. Larochelle", "M. Mandel", "R. Pascanu", "Y. Bengio"], "venue": "The Journal of Machine Learning Research 13(1), 643\u2013669", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G.E. Hinton"], "venue": "Nature 521, 436\u2013444", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Temperature based restricted boltzmann machines", "author": ["G. Li", "L. Deng", "Y. Xu", "C. Wen", "W. Wang", "J. Pei", "L. Shi"], "venue": "Scientific reports 6, 19,133", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Nonlinear kramers equation associated with nonextensive statistical mechanics", "author": ["G. Mendes", "M. Ribeiro", "R. Mendes", "E. Lenzi", "F. Nobre"], "venue": "Physical Review E 91(5), 052,106", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Exact maxwell\u2013boltzmann, bose\u2013einstein and fermi\u2013dirac statistics", "author": ["R.K. Niven"], "venue": "Physics Letters A 342(4), 286\u2013293", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "On the model selection of bernoulli restricted boltzmann machines through harmony search", "author": ["J.P. Papa", "G.H. Rosa", "K.A.P. Costa", "A.N. Marana", "W. Scheirer", "D.D. Cox"], "venue": "Proceedings of the Genetic and Evolutionary Computation Conference, GECCO \u201915, pp. 1449\u20131450. ACM, New York, USA", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Model selection for discriminative restricted boltzmann machines through meta-heuristic techniques", "author": ["J.P. Papa", "G.H. Rosa", "A.N. Marana", "W. Scheirer", "D.D. Cox"], "venue": "Journal of Computational Science 9, 14\u201318", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Fine-tuning deep belief networks using harmony search", "author": ["J.P. Papa", "W. Scheirer", "D.D. Cox"], "venue": "Applied Soft Computing", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Sparse feature learning for deep belief networks", "author": ["M. Ranzato", "Y. Boureau", "Y. Cun"], "venue": "J. Platt, D. Koller, Y. Singer, S. Roweis (eds.) Advances in Neural Information Processing Systems 20, pp. 1185\u20131192. Curran Associates, Inc.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "On measures of entropy and information", "author": ["A. RRNYI"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1961}, {"title": "An efficient learning procedure for deep boltzmann machines", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "Neural Computation 24(8), 1967\u20132006", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning in neural networks: An overview", "author": ["J. Schmidhuber"], "venue": "Neural Networks 61, 85\u2013117", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust thermal boundary conditions applicable to a wall along which temperature varies in lattice-gas cellular automata", "author": ["J.W. Shim", "R. Gatignol"], "venue": "Physical Review E 81(4), 046,703", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Statistical mechanics of self-gravitating systems: Mixing as a criterion for indistinguishability", "author": ["L.B. e Silva", "M. Lima", "L. Sodr\u00e9", "J. Perez"], "venue": "Physical Review D 90(12), 123,004", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "author": ["P. Smolensky"], "venue": "Tech. rep., DTIC Document", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1986}, {"title": "Learning structured output representation using deep conditional generative models", "author": ["K. Sohn", "H. Lee", "X. Yan"], "venue": "C. Cortes, N.D. Lawrence, D.D. Lee, M. Sugiyama, R. Garnett (eds.) Advances in Neural Information Processing Systems 28, pp. 3465\u2013 3473. Curran Associates, Inc.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "The Journal of Machine Learning Research 15(1), 1929\u20131958", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Training restricted boltzmann machines using approximations to the likelihood gradient", "author": ["T. Tieleman"], "venue": "Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pp. 1064\u20131071. ACM, New York, NY, USA", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning invariant features using subspace restricted boltzmann machine", "author": ["J.M. Tomczak", "A. Gonczarek"], "venue": "Neural Processing Letters pp. 1\u201310", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Individual Comparisons by Ranking Methods", "author": ["F. Wilcoxon"], "venue": "Biometrics Bulletin 1(6), 80\u201383", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1945}], "referenceMentions": [{"referenceID": 3, "context": "1 Introduction Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12].", "startOffset": 157, "endOffset": 165}, {"referenceID": 1, "context": "1 Introduction Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12].", "startOffset": 157, "endOffset": 165}, {"referenceID": 25, "context": "1 Introduction Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12].", "startOffset": 157, "endOffset": 165}, {"referenceID": 11, "context": "1 Introduction Deep learning techniques have attracted considerable attention in the last years due to their outstanding results in a number of applications [4,2,26], since such techniques possess an intrinsic ability to learn different information at each level of a hierarchy of layers [12].", "startOffset": 288, "endOffset": 292}, {"referenceID": 8, "context": "Restricted Boltzmann Machines (RBMs) [9], for instance, are among the most pursued techniques, even though they are not", "startOffset": 37, "endOffset": 40}, {"referenceID": 9, "context": "deep learning-oriented themselves, but by building blocks composed of stacked RBMs on top of each other one can obtain the so-called Deep Belief Networks (DBNs) [10] or the Deep Boltzmann Machines (DBMs) [21], which basically differ from each other by the way the inner layers interact among themselves.", "startOffset": 161, "endOffset": 165}, {"referenceID": 20, "context": "deep learning-oriented themselves, but by building blocks composed of stacked RBMs on top of each other one can obtain the so-called Deep Belief Networks (DBNs) [10] or the Deep Boltzmann Machines (DBMs) [21], which basically differ from each other by the way the inner layers interact among themselves.", "startOffset": 204, "endOffset": 208}, {"referenceID": 10, "context": "The Restricted Boltzmann Machine is a probabilistic model that uses a layer of hidden units to model the distribution over a set of inputs, thus compounding a generative stochastic neural network [11,22].", "startOffset": 196, "endOffset": 203}, {"referenceID": 21, "context": "The Restricted Boltzmann Machine is a probabilistic model that uses a layer of hidden units to model the distribution over a set of inputs, thus compounding a generative stochastic neural network [11,22].", "startOffset": 196, "endOffset": 203}, {"referenceID": 24, "context": "RBMs were firstly idealized under the name of \u201cHarmonium\u201d by Smolensky in 1986 [25], and some years later renamed to RBM by Hinton et.", "startOffset": 79, "endOffset": 83}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 6, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 15, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 16, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 17, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 28, "context": "Since then, the scientific community has been putting a lot of effort in order to improve the results in a number of application that somehow make use of RBM-based models [6, 7,16,17,18,29].", "startOffset": 171, "endOffset": 189}, {"referenceID": 12, "context": "[13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann.", "startOffset": 179, "endOffset": 183}, {"referenceID": 23, "context": "[13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann.", "startOffset": 185, "endOffset": 189}, {"referenceID": 2, "context": "[13] recently highlighted the importance of a crucial concept in Boltzmann-related distributions: their \u201ctemperature\u201d, which has a main role in the field of statistical mechanics [14], [24], [3], idealized by Wolfgang Boltzmann.", "startOffset": 191, "endOffset": 194}, {"referenceID": 4, "context": "In fact, a Maxwell-Boltzmann distribution [5,23,15] is a probability distribution of particles over various possible energy states without interacting with one another, expect for some very brief collisions, where they exchange energy.", "startOffset": 42, "endOffset": 51}, {"referenceID": 22, "context": "In fact, a Maxwell-Boltzmann distribution [5,23,15] is a probability distribution of particles over various possible energy states without interacting with one another, expect for some very brief collisions, where they exchange energy.", "startOffset": 42, "endOffset": 51}, {"referenceID": 14, "context": "In fact, a Maxwell-Boltzmann distribution [5,23,15] is a probability distribution of particles over various possible energy states without interacting with one another, expect for some very brief collisions, where they exchange energy.", "startOffset": 42, "endOffset": 51}, {"referenceID": 12, "context": "[13] demonstrated the temperature influences on the way RBMs fire neurons, as well as they showed its analogy to the state of particles in a physical system, where a lower temperature leads to a lower particle activity, but higher entropy [1], [20].", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[13] demonstrated the temperature influences on the way RBMs fire neurons, as well as they showed its analogy to the state of particles in a physical system, where a lower temperature leads to a lower particle activity, but higher entropy [1], [20].", "startOffset": 239, "endOffset": 242}, {"referenceID": 19, "context": "[13] demonstrated the temperature influences on the way RBMs fire neurons, as well as they showed its analogy to the state of particles in a physical system, where a lower temperature leads to a lower particle activity, but higher entropy [1], [20].", "startOffset": 244, "endOffset": 248}, {"referenceID": 7, "context": "Notice the terms P (h\u0303|\u1e7d) and \u1e7d can be obtained by means of the Contrastive Divergence [8] technique, which basically ends up performing Gibbs sampling using the training data as the visible units.", "startOffset": 87, "endOffset": 90}, {"referenceID": 20, "context": "Roughly speaking, using such procedure, the conditional probabilities given by Equations 15-17, and Contrastive Divergence, one can learn DBM parameters one layer at a time [21].", "startOffset": 173, "endOffset": 177}, {"referenceID": 12, "context": "[13] showed that a temperature parameter T controls the sharpness of the logistic-sigmoid function.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "05 [30], we conducted a cross-validation procedure with 20 runnings.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In order to provide a more precise experimental validation, we trained both DBMs and DBNs with two different algorithms: Contrastive Divergence (CD) [8] and Persistent Contrastive Divergence (PCD) [28].", "startOffset": 149, "endOffset": 152}, {"referenceID": 27, "context": "In order to provide a more precise experimental validation, we trained both DBMs and DBNs with two different algorithms: Contrastive Divergence (CD) [8] and Persistent Contrastive Divergence (PCD) [28].", "startOffset": 197, "endOffset": 201}, {"referenceID": 12, "context": "[13], i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19], sparsity in the neuron\u2019s activity favours the power of generalization of a network, which is somehow related to dropping neurons out in order to avoid overfitting [27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[19], sparsity in the neuron\u2019s activity favours the power of generalization of a network, which is somehow related to dropping neurons out in order to avoid overfitting [27].", "startOffset": 169, "endOffset": 173}, {"referenceID": 12, "context": "Inspired by a very recent work that proposed the Temperaturebased Restricted Boltzmann Machines [13], we decided to evaluate the influence of the temperature when learning with Deep Boltzmann Machines aiming at the task of binary image reconstruction.", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "[13], where the lower the temperature, the more generalized is the network.", "startOffset": 0, "endOffset": 4}], "year": 2017, "abstractText": "Deep learning techniques have been paramount in the last years, mainly due to their outstanding results in a number of applications, that range from speech recognition to face-based user identification. Despite other techniques employed for such purposes, Deep Boltzmann Machines are among the most used ones, which are composed of layers of Restricted Boltzmann Machines (RBMs) stacked on top of each other. In this work, we evaluate the concept of temperature in DBMs, which play a key role in Boltzmann-related distributions, but it has never been considered in this context up to date. Therefore, the main contribution of this paper is to take into account this information and to evaluate its influence in DBMs considering the task of binary image reconstruction. We expect this work can foster future research considering the usage of different temperatures during learning in DBMs.", "creator": "LaTeX with hyperref package"}}}