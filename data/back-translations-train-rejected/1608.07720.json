{"id": "1608.07720", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Aug-2016", "title": "A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features", "abstract": "Relation classification is associated with many potential applications in the artificial intelligence area. Recent approaches usually leverage neural networks based on structure features such as syntactic or dependency features to solve this problem. However, high-cost structure features make such approaches inconvenient to be directly used. In addition, structure features are probably domain-dependent. Therefore, this paper proposes a bi-directional long-short-term-memory recurrent-neural-network (Bi-LSTM-RNN) model based on low-cost sequence features to address relation classification. This model divides a sentence or text segment into five parts, namely two target entities and their three contexts. It learns the representations of entities and their contexts, and uses them to classify relations. We evaluate our model on two standard benchmark datasets in different domains, namely SemEval-2010 Task 8 and BioNLP-ST 2016 Task BB3. In the former dataset, our model achieves comparable performance compared with other models using sequence features. In the latter dataset, our model obtains the third best results compared with other models in the official evaluation. Moreover, we find that the context between two target entities plays the most important role in relation classification. Furthermore, statistic experiments show that the context between two target entities can be used as an approximate replacement of the shortest dependency path when dependency parsing is not used.", "histories": [["v1", "Sat, 27 Aug 2016 15:41:22 GMT  (27kb)", "http://arxiv.org/abs/1608.07720v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fei li", "meishan zhang", "guohong fu", "tao qian", "donghong ji"], "accepted": false, "id": "1608.07720"}, "pdf": {"name": "1608.07720.pdf", "metadata": {"source": "CRF", "title": "A Bi-LSTM-RNN Model for Relation Classification Using Low-Cost Sequence Features", "authors": ["Fei Li", "Meishan Zhang", "Guohong Fu", "Tao Qian", "Donghong Ji"], "emails": ["dhji}@whu.edu.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.07 720v 1 [cs.C L] 27 A"}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Related Work", "text": "Most of them are laborious and time-consuming. Kernel-based approaches do not require a great deal of effort in the field of feature engineering, but well-designed core functions, which are usually based on syntactic or dependency structures, are crucial for classifying relationships. Approaches based on deep neural networks are becoming new research focuses for classifying relationships, as they can achieve promising results with less manual intervention. RecursiveNNNs [10], [15] are initially used for this task to learn sentence representations along syntactic or dependent structures."}, {"heading": "3. Our Bi-LSTM-RNN Model", "text": "Our model has several characteristics: the classification of relationships is modelled on the basis of entity and context representations learned from LSTM RNNs; only low-cost sequence characteristics are used to avoid the problems of structural characteristics; characteristics are extracted from bidirectional RNs using simple pooling technologies; relationships between entities occurring in different sets can also be classified."}, {"heading": "3.1. Long Short Term Memory (LSTM)", "text": "LSTMs [12] aim to facilitate the formation of RNNs by solving the decreasing and exploding gradient problems in the deep or long structures. They can be defined as follows: In an input sequence x = {x1, x2,..., xn}, LSTMs associate each of them with an input gate (it), a forgisstor (f t), an output gate (ot), a cell candidate (c \u0442t), a cell state (ct), and a hidden state (ht). They determine what new information is present in the current cell state. f t determines which information from the previous cell state ct \u00b7 \u00b7 b \u00b7 b \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 (ft) \u00b7 b \u00b7 b \u00b7 b \u00b7 t (c \u00b7 t) (t), c \u00b7 t (t), c \u00b7 t (t), t (t), t (t)."}, {"heading": "3.2. Bi-LSTM-RNN", "text": "The framework of our Bi-LSTM RNN model is shown in Figure 1 = \"This sentence or text segment can be considered as a symbolic sequence s = {s1, s2,..., sn}. An LSTM unit takes the embedding of each token st as input and output of a hidden state h\" t calculated by Equation 1. Then we get a hidden state sequence h, \"h\" 2,..., h \"n,\" after the LSTM unit has finished its recursive calculation along all tokens from left to right. Here, we capture not only the information of the tokens st, but also that of its predecessors. To capture the information of its successors, a counterpart h \"t of h\" t is also generated by another LSTM unit in the opposite direction."}, {"heading": "3.3. Training", "text": "In view of a number of commented training examples, the training objective of our model is to minimize cross entropy loss, using an L2 regularization term given by L (\u03b8) = \u2212 \u2211 ilog pgi + \u03b22 \u0432 \u03b8 \u0432 22, (7). pgi indicates the probability of the gold ratio type of the i-th training example as specified in the model. \u03b2 is the regularization parameter. For the model, we use standard training frames, namely stochastic gradients acceptable using AdaGrad. Pgi calculates the derivatives from the standard backpropagation [32]. Further details are described in section 4."}, {"heading": "3.4. Features", "text": "This year it has come to the point where it is a purely reactionary project, which is a purely reactionary project, which is a reactionary project."}, {"heading": "4. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. SemEval-2010 Task 8", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Data and Evaluation Metrics", "text": "This dataset [2] defines 9 directed relation types between two target units and an undirected Other type when two target units have none of these relationships. We treat each directed relation type as two relation types, so there are a total of 19 relation types in our model. The dataset consists of 8,000 training and 2,717 test sets, and each set is commented on with a relation type.After previous work [2], [10] the official macro-averaged F1 score (F1) is used to evaluate the performance of various models."}, {"heading": "Parameter Settings", "text": "Since it is not possible to perform a complete search for all parameters, some of the values are selected empirically after previous work [9], [10], [11]. The initial AdaGrad learning rate \u03b1 is set as 0.01 and the L2 regularization parameter \u03b2 as 10 \u2212 8. The dimension of pre-trained word embeddings n (pre) is set as 200. The dimensions of other embeddings, namely n (ran), n (pos), n (wnh) and n (char), are set as 50. The dimensions of LSTM hidden state (n (lstm) and token representation (n (h) are set as 200. Weight matrices W, bias vectors b and the embedding of search tables Eran, Echar, Ewnh are randomly initialized in the English word templates (n (h))."}, {"heading": "Results", "text": "The experimental results of the test set are shown in Table 1. MVRNN [10], C-RNN [15] and DepNN [16] are based on recursive NNN, but DepNN also combines CNNs to capture characteristics of the shortest dependency paths and improve the result to 83.6%. FCM [19] achieves a similar result by 1. https: / / dumps.wikimedia.org / enwiki / break down sentences into factors, extract characteristics and combine them via sum pooling. CNN-based depLCNN [18] and RNN-based SDP-LSTM [11] classify relationships using the shortest dependency paths between two units and achieve similar results. After taking into account the relationship directness of characteristics and their combination through sum pooling."}, {"heading": "4.2. BioNLP-ST 2016 Task BB3", "text": "Although structural characteristics are useful for classifying relationships, they are probably domain-dependent. In addition, there are about 26% relationships between entities that occur in different sets, based on our statistics for BioNLP-ST 2016 Task BB3 [4]. Structural characteristics are not easy to use directly, as they are designed for use within a set. We are experimenting with this data set to prove that our model is still effective even when the above problems exist."}, {"heading": "Data and Evaluation Metrics", "text": "This task includes several sub-tasks and we focus on the sub-task relation classification. The sub-task takes into account a relation type, namely Lives In, which indicates that bacteria live in a habitat. The data set consists of 61, 34 and 51 documents for training, development and testing, respectively. There are 1080, 730, 1093 units and 327, 223, 340 relationships in the areas of training, development and testing. To evaluate our model, we use the official evaluation service2. The evaluation quantities are standard accuracy (P), recall (R) and F1 score (F1)."}, {"heading": "Parameter Settings", "text": "The parameters are based on the official development set of 34 documents. Dimensions of pre-trained word embedding (n (pre)) and random word embedding (n (ran)) are set to 200. Dimensions of other feature embedding, namely n (pos), n (wnh) and n (char), are set to 50. Dimensions of LSTM hidden state (n (lstm) and token representation (n (h)) are set to 200. Other parameter settings are similar to those in the previous task. Weight matrices W, bias vectors b and the embedding tables Eran, Echar, Epic, Ewnh are randomly initialized in the range (-0.01, 0.01). We use biomedical word embedding [38], which are trained during embedding trainings. http: / / bibliome.jouy.inra.fr / demo / BioNLST-embedding / Evaluations."}, {"heading": "Results", "text": "The experimental results of the test set are in Table 3. VERSE receives the state of the art F1 (55.8%) in the official evaluation. TurkuNLP and LIMSI achieve the best precision or retrieval, respectively. If our model takes into account the relationships between bacteria / habitat units that occur in the same set, it can obtain better F1 than LIMSI. If our model looks at the relationships between bacteria / habitat units that occur in two consecutive sets, F1 increases from 49.8% to 51.3%. If the set window continues to expand, F1 sinks. This may be because most bacteria / habitat units that comprise more than two sets have no live in relationships, the number of positive (15%) and negative (85%) examples of training the model become very unbalanced. Feature contributions are shown in Table 4. Our model receives 41.3% in F1 by using only pre-trained word attributes. Word-hyper-characteristics improve from 1.3% to 44.8% and the most helpful attributes are the 1.3%."}, {"heading": "5. Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Which context contributes the most?", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "6. Conclusion", "text": "We propose a Bi-LSTM RNN model based on low-cost sequence characteristics to classify relationships. Our motivation is that the relationship between two target units can be represented by the units and contexts surrounding them. We avoid the use of structural characteristics to adapt the model to other areas. Experimental results from two benchmark datasets demonstrate the effectiveness of our model and its performance approaches the performance of modern models. By evaluating the contributions of different contexts, we find that the middle context plays the most important role in classifying relationships. Furthermore, we find that the middle context can replace the shortest dependency path, for example, when parsing dependencies is not used."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Relation classification is associated with many potential applications in the artificial intelligence area. Recent approaches usually leverage neural networks based on structure features such as syntactic or dependency features to solve this problem. However, high-cost structure features make such approaches inconvenient to be directly used. In addition, structure features are probably domaindependent. Therefore, this paper proposes a bidirectional long-short-term-memory recurrent-neuralnetwork (Bi-LSTM-RNN) model based on low-cost sequence features to address relation classification. This model divides a sentence or text segment into five parts, namely two target entities and their three contexts. It learns the representations of entities and their contexts, and uses them to classify relations. We evaluate our model on two standard benchmark datasets in different domains, namely SemEval-2010 Task 8 and BioNLP-ST 2016 Task BB3. In the former dataset, our model achieves comparable performance compared with other models using sequence features. In the latter dataset, our model obtains the third best results compared with other models in the official evaluation. Moreover, we find that the context between two target entities plays the most important role in relation classification. Furthermore, statistic experiments show that the context between two target entities can be used as an approximate replacement of the shortest dependency path when dependency parsing is not used.", "creator": "LaTeX with hyperref package"}}}