{"id": "1608.07793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2016", "title": "Partially Observable Markov Decision Process for Recommender Systems", "abstract": "We report the \"Recurrent Deterioration\" (RD) phenomenon observed in online recommender systems. The RD phenomenon is reflected by the trend of performance degradation when the recommendation model is always trained based on users' feedbacks of the previous recommendations. There are several reasons for the recommender systems to encounter the RD phenomenon, including the lack of negative training data and the evolution of users' interests, etc. Motivated to tackle the problems causing the RD phenomenon, we propose the POMDP-Rec framework, which is a neural-optimized Partially Observable Markov Decision Process algorithm for recommender systems. We show that the POMDP-Rec framework effectively uses the accumulated historical data from real-world recommender systems and automatically achieves comparable results with those models fine-tuned exhaustively by domain exports on public datasets.", "histories": [["v1", "Sun, 28 Aug 2016 09:42:52 GMT  (44kb,D)", "http://arxiv.org/abs/1608.07793v1", null], ["v2", "Thu, 1 Sep 2016 15:41:02 GMT  (44kb,D)", "http://arxiv.org/abs/1608.07793v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.IR", "authors": ["zhongqi lu", "qiang yang"], "accepted": false, "id": "1608.07793"}, "pdf": {"name": "1608.07793.pdf", "metadata": {"source": "CRF", "title": "Partially Observable Markov Decision Process for Recommender Systems", "authors": ["Zhongqi Lu", "Qiang Yang"], "emails": ["zluab@cse.ust.hk", "qyang@cse.ust.hk"], "sections": [{"heading": "1 Introduction", "text": "This year it is more than ever before."}, {"heading": "2 Preliminaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The POMDP", "text": "A POMDP models a decision-making process in which system dynamics are assumed to be determined by an MDP, but the underlying state cannot be directly observed. It maintains a probability distribution over the set of possible states, based on a series of observations and observational probabilities, as well as the underlying MDP. POMDP can be a proper modelling of recommendation systems for two reasons. On the one hand, the Markovian assumption has proven valid for the sequential recommendations [24]. On the other hand, we do not expect that the actions of a user will be fully observed on all items of a recommendation system. Therefore, we will avoid defining the actions of users as states. Instead, we argue that the behavior of the observed user would depend on some hidden states with a probability. In the context of such modelling, the POMDP would be suitable for the sequential recommendations."}, {"heading": "2.2 Neural fitted Q-learning", "text": "In classical Q-Learning [26], the goal is to learn a Q-value function, which represents a mapping of the state and action space to an evaluation space. This Q-value function follows an important identity known as the Bellman Equation. This is based on an intuition: If the optimal Q? (s) value function is known for all possible actions a at the next step s? (s), then the optimal strategy is to choose the action a \"to maximize the expected value of r (s) + g? (s) \u2212 value of the sequence s? (s)."}, {"heading": "3 POMDP-Rec for Recommender Systems", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 The POMDP-Rec framework", "text": "Given the Markovian characteristics of recommendation systems [24], we will consider the POMDP model suitable to explain the nature of recommendation systems; the recommendation system would not be able to fully observe the actions of users towards the entire item set, since users usually give limited feedback [1]. Formally, the POMDP Rec framework is represented in this paper by an 8-fold (S, A, T, R, E, O, P, E), with \u2022 S being a series of hidden states. \u2022 A is a set of actions. \u2022 T (s) is a set of conditional transition probabilities for actions a causing the environmental transit from the state to the s. \u2022 R: S \u00d7 A \u2212 R is the reward function. \u2022 O is a set of observations of user behavior. \u2022 o (o | s, a) is a set of conditional observation probabilities that cause the probabilities to be transferred by others to the state of observation."}, {"heading": "3.2 Belief Approximation of POMDP-Rec", "text": "Although the POMDP approach provides a natural way to model the recommendation systems, their solution is not trivial [10]. In order to solve the proposed POMDP Rec framework among the recommendation system settings, we have developed an approximation by introducing the beliefs of the hidden states in POMDP, known as states of faith. To estimate the states of faith, we propose to refer to the low-dimensional factor model [17]. The approach could handle the low level of the user's observations because the states of faith are learned in a collaborative model. In the low-dimensional factor model, the actions of a user are modeled toward an item by combining the latent properties of the items and the estimates of the user's latent interests. Following the assessments of the latent properties of item V and the assessments of the user's latent interests, the latent interests of the user are assumed that the latencies are derived from the normal interests of some of the user and some corresponding distributors."}, {"heading": "3.3 Solving POMDP-Rec for Recommendations", "text": "We would like to solve POMDP-Rec by Q-Learning [27]. By entering the Q-function, we define a transition from states of belief < b, b \">, where b\" is the consecutive state of belief after b. \"The Q-value function is the solution to the Bellman optimality equation: Q (< b, b\" >) = r + \u03b3 b. \"Our approach is an instance of the Fitted-Q iteration family [6], which is successfully used in Atari games [18]. The POMDP-Rec framework consists of two important steps: (1) generating a PS training pattern and (2) training these patterns within a neural network. Furthermore, we found in experiments that the pattern would lead to better results by randomly shifting Q results."}, {"heading": "3.4 Properties of POMDP-Rec", "text": "The training data collected in recommendation systems have three main features. First, user feedback is usually unevenly distributed. For example, an online advertising receiver system can only see positive feedback, but no feedback can indicate that either the negative algorithm 1 POMDP solver for recommendation systems input: set of transitive observations D = (oij; a; o \"ij), i\" (1,.), j \"(1,.), j\" (1,.,.), number of iterations N; output: a neural network for estimating the Q value function QN; iter = 0;% iteration counter Init _ MLP () \u2192 Q0;% Rand init the neural network Estimate a transition function (.) by sampling D and calculating belief states. < < N. \""}, {"heading": "3.5 Implementation details", "text": "The raw data collected in a recommendation system can be collected in triples of the form (o, a, o \u2032), where o is the original observation, a is the recommendation to be made, and o \u2032 is the next observation. The successive observations o and o \u2032 are to be related to the same (user, item) pair. The sentence of these triples is called the example sentence D. Definition of rewards The definition of rewards is important for the Q function. By inheriting the notations in the previous sections, we introduce a definition of reward: r (b \u2032 ij, a) = 1 + exp (C) (D) (D) (D) (D) (D) (D) is the future interaction between user i and item j (D) and it serves as the basis of truth."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset and setup", "text": "We would like to test our proposed POMDP-Rec framework against the public data sets retrieved from the real-world recipient systems. Of the real-world recipient systems, the data is likely to be (a) sparse, (b) contain unequal distributions of numerical ratings, and (c) show the changing interests of users when the system is running for a relatively long time. We found that the MovieLens data set and the Yahoo Music data set are ideal as an offline data set to test the POMDP-Rec framework and the most modern methods, although the anonymization of these public data sets prevents us from further analyzing the faith states and Q function, etc. in the POMDP-Rec model. The 1M MovieLens data set [7] contains approximately 1 million reviews of 3,952 movies from 6,040 users with Unix timestamps. Each user has at least 20 ratings \u2212 and the average saving of the entire data set is about 4.2%."}, {"heading": "4.2 Performance comparison", "text": "To simulate the real recommendation scenario, we would like to split the training data and the test data by time. We use about 80% of the data from earlier times as training data and the remaining data for testing. As discussed in the previous section, the data is split and scattered into time windows, and the predictions of the POMDP-Rec are evaluated for each time window. We report on the average performance of the POMDP-Rec on the MovieLens data set in Table 1, and the Yahoo Music data set in Table 2.1http: / / movielens.org 2https: / / www.yahoo.com / music / To compare with the previous results, we adopt the official evaluation standards KDDCUP \u2032 11, Root Mean Square Error (RMSE). That is, RMSE = Promptions mi = 1, n j = 1 Iij = 1 Iij (Rij \u2212 R-ij) 2 / I, whwhwhy, are available for usage, IJ-Items and Items."}, {"heading": "4.3 Analysis of POMDP-Rec", "text": "In this section, we analyze the POMDP-Rec framework empirically based on the Yahoo Music dataset. Previous literature [8] showed that estimating the value function could be quite skewed, especially if there are large overestimates of future Q values. It seems that repeated iteration over the randomly mixed sample set could improve performance. In the experiments, we evaluate the POMDP-Rec framework after each iteration and show the trend of performance improvements in Figure 1. From Figure 1, we note that the statement of iteration 6 shows a significant improvement in the performance of the model. We would like to see its stability starting with iteration 6. In previous studies of Q-Learning [18], average reward and average maxQ were often assumed to be indicators of model stability. Therefore, we plot the average reward and average maxQ in terms of the number of training samples in Figure 2. The POMDP-Rec framework is presented as stable in the Yahoo Music recommendation 2 and the Yahoo Music is stable in the maxQ recommendation."}, {"heading": "5 Related Works", "text": "The sequential character of the recommendation process has been noted in the past [24]. Common practice is to model the sequential nature using the Markove Decision Process (MDP) [24, 23]. Taking up the Markovian idea to model the sequential recommendation a step further, we propose that the sequential recommendation should be considered as a partially observable MDP. Since in the settings of receiver systems the actions of the user cannot be fully observed, but each action we observe can be used to estimate the distributions of the user's interests collaboratively. As an intuitive sketch of a natural modeling of a receiver system, the receiver system observes the actions of the user at any time interval, unfolds its beliefs and makes recommendations. These processes are seamlessly modeled by our proposed POMDP-Rec models. From the perspective of solving the classy work associated with the RD phenomenon is also our modeling."}, {"heading": "6 Conclusion", "text": "We propose the POMDP-Rec framework, which was developed to model the recommendation process. POMDP-Rec framework takes care of the changing distribution of test samples through the transition between its states. Learning POMDP-Rec does not require a balance between positive and negative samples, and the Markov property of POMDP-Rec eliminates the effects of recurring training. With these nice features, the POMDP-Rec framework achieves good results in offline experiments on both the MovieLens 1M dataset and the Yahoo Music dataset. In the future, we would like to apply the POMDP-Rec framework to a real online advertising recommendation system, train it and evaluate it using practical metrics (e.g. CTR or revenue), and try to analyze the beliefs learned."}], "references": [{"title": "Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 17(6):734\u2013749", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "The netflix prize", "author": ["J. Bennett", "S. Lanning"], "venue": "KDD cup and workshop", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2007}, {"title": "Dynamic Programming: Deterministic and Stochastic Models", "author": ["D.P. Bertsekas"], "venue": "Prentice-Hall, Inc., Upper Saddle River, NJ, USA", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1987}, {"title": "et al", "author": ["P.-L. Chen", "C.-T. Tsai", "Y.-N. Chen", "K.-C. Chou", "C.-L. Li", "C.-H. Tsai", "K.-W. Wu", "Y.-C. Chou", "C.-Y. Li", "W.-S. Lin"], "venue": "A linear ensemble of individual and blended models for music rating prediction. In KDD Cup, pages 21\u201360", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "The yahoo! music dataset and kdd-cup\u201911", "author": ["G. Dror", "N. Koenigstein", "Y. Koren", "M. Weimer"], "venue": "KDD Cup, pages 8\u201318", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Tree-based batch mode reinforcement learning", "author": ["D. Ernst", "P. Geurts", "L. Wehenkel"], "venue": "Journal of Machine Learning Research, pages 503\u2013556", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "The movielens datasets: History and context", "author": ["F.M. Harper", "J.A. Konstan"], "venue": "ACM Transactions on Interactive Intelligent Systems (TiiS), 5(4):19", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Double q-learning", "author": ["H.V. Hasselt"], "venue": "Advances in Neural Information Processing Systems, pages 2613\u20132621", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2010}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Value-function approximations for partially observable markov decision processes", "author": ["M. Hauskrecht"], "venue": "Journal of Artificial Intelligence Research, pages 33\u201394", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "Probabilistic matrix factorization with nonrandom missing data", "author": ["J.M. Hernandez-lobato", "N. Houlsby", "Z. Ghahramani"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), pages 1512\u20131520", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Y. Koren"], "venue": "KDD, pages 447\u2013456. ACM", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2009}, {"title": "A contextual-bandit approach to personalized news article recommendation", "author": ["L. Li", "W. Chu", "J. Langford", "R.E. Schapire"], "venue": "WWW, pages 661\u2013670. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Collaborative prediction and ranking with non-random missing data", "author": ["B.M. Marlin", "R.S. Zemel"], "venue": "Proceedings of the third ACM conference on Recommender systems, pages 5\u201312. ACM", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Content-boosted collaborative filtering for improved recommendations", "author": ["P. Melville", "R.J. Mooney", "R. Nagarajan"], "venue": "AAAI/IAAI, pages 187\u2013192", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2002}, {"title": "Probabilistic matrix factorization", "author": ["A. Mnih", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, pages 1257\u20131264", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2007}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Human-level control through deep reinforcement learning. Nature, 518(7540):529\u2013533", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "One-class collaborative filtering", "author": ["R. Pan", "Y. Zhou", "B. Cao", "N.N. Liu", "R. Lukose", "M. Scholz", "Q. Yang"], "venue": "Eighth IEEE International Conference on Data Mining,ICDM\u201908., pages 502\u2013511. IEEE", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Factorization machines", "author": ["S. Rendle"], "venue": "Data Mining (ICDM), 2010 IEEE 10th International Conference on, pages 995\u20131000. IEEE", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Neural fitted q iteration\u2013first experiences with a data efficient neural reinforcement learning method", "author": ["M. Riedmiller"], "venue": "Machine Learning: ECML 2005, pages 317\u2013328. Springer", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "A hidden markov model for collaborative filtering", "author": ["N. Sahoo", "P.V. Singh", "T. Mukhopadhyay"], "venue": "MIS Quarterly, 36(4):1329\u20131356", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "An mdp-based recommender system", "author": ["G. Shani", "D. Heckerman", "R.I. Brafman"], "venue": "Journal of Machine Learning Research, 6:1265\u20131295", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "The optimal control of partially observable markov processes over a finite horizon", "author": ["R.D. Smallwood", "E.J. Sondik"], "venue": "Operations Research, 21(5):1071\u20131088", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1973}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 1. MIT press Cambridge", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1998}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, 8(3-4):279\u2013292", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1992}, {"title": "Hierarchical exploration for accelerating contextual bandits", "author": ["Y. Yue", "S.A. Hong", "C. Guestrin"], "venue": "The 29th International Conference on Machine Learning", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Exploration vs", "author": ["X. Zhao", "P.I. Frazier"], "venue": "exploitation in the information filtering problem. arXiv preprint arXiv:1407.8186", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "A fast parallel sgd for matrix factorization in shared memory systems", "author": ["Y. Zhuang", "W.-S. Chin", "Y.-C. Juan", "C.-J. Lin"], "venue": "Proceedings of the 7th ACM conference on Recommender systems, pages 249\u2013256. ACM", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 14, "context": "Some tweaks, such as sampling for negative samples [15, 11] and/or neighborhood regularization [16], could make the RD phenomenon less severe.", "startOffset": 51, "endOffset": 59}, {"referenceID": 10, "context": "Some tweaks, such as sampling for negative samples [15, 11] and/or neighborhood regularization [16], could make the RD phenomenon less severe.", "startOffset": 51, "endOffset": 59}, {"referenceID": 15, "context": "Some tweaks, such as sampling for negative samples [15, 11] and/or neighborhood regularization [16], could make the RD phenomenon less severe.", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "In the previous researches, modeling the sequential recommendation process based on the Markov Decision Process (MDP) was proposed [24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 24, "context": "The recommendation problem considered in this paper could be formulated by a POMDP [25].", "startOffset": 83, "endOffset": 87}, {"referenceID": 23, "context": "On one hand, the Markovian assumption has been proven to be valid for the sequential recommendations [24].", "startOffset": 101, "endOffset": 105}, {"referenceID": 25, "context": "In classical Q-learning [26], the goal is to learn a Q-value function, which is a mapping from the state and action space to an evaluation space.", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": "Recent researches have demonstrated the success of using neural fitted function to approximate the Q-values in real-world applications, such as playing Atari [18].", "startOffset": 158, "endOffset": 162}, {"referenceID": 21, "context": "As mentioned in [22], the above", "startOffset": 16, "endOffset": 20}, {"referenceID": 5, "context": "process of generating Q-values can be parameterized via an instance of the Fitted Q Iteration family of algorithms [6].", "startOffset": 115, "endOffset": 118}, {"referenceID": 23, "context": "Given the Markovian properties of recommender systems [24], we will find the POMDP modeling to be suitable to explain the nature of recommender systems.", "startOffset": 54, "endOffset": 58}, {"referenceID": 0, "context": "The recommender system would not be able to fully observe the users\u2019 actions towards the entire item set, because the users usually provide limited feedbacks [1].", "startOffset": 158, "endOffset": 161}, {"referenceID": 0, "context": "\u2022 \u03b3 \u2208 [0, 1] is the discount factor.", "startOffset": 6, "endOffset": 12}, {"referenceID": 9, "context": "Although the POMDP approach provides a natural way to model the recommender systems, solving it is non-trivial [10].", "startOffset": 111, "endOffset": 115}, {"referenceID": 16, "context": "To estimate the belief states, we propose to refer to the low-dimensional factor model [17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "Following [17], the estimations of items\u2019 latent features V and the estimations of user\u2019s latent interests U are assumed to be from some particular normal distributions with zero-mean and corresponding variances.", "startOffset": 10, "endOffset": 14}, {"referenceID": 2, "context": "An important consequence is that the belief states form a fully observable MDP, and thus it is a sufficient statistic for choosing optimal actions [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 8, "context": "Q-learning has been adopted in solving the POMDP problems in some recent works [9, 29].", "startOffset": 79, "endOffset": 86}, {"referenceID": 28, "context": "Q-learning has been adopted in solving the POMDP problems in some recent works [9, 29].", "startOffset": 79, "endOffset": 86}, {"referenceID": 26, "context": "We would like to solve the POMDP-Rec by Q-learning [27].", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "Our approach is an instance of the Fitted Q Iteration family [6], which has successful applications in playing Atari games [18].", "startOffset": 61, "endOffset": 64}, {"referenceID": 17, "context": "Our approach is an instance of the Fitted Q Iteration family [6], which has successful applications in playing Atari games [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 29, "context": "In order to make future recommendations, we generate the candidates by varying the parameters of a matrix factorization model [30], and the candidate with the highest Q value is adopted for the future recommendation.", "startOffset": 126, "endOffset": 130}, {"referenceID": 19, "context": "feedback or the users miss the recommendations [20].", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": "The popular Collaborative Filtering (CF) approaches for recommender systems aim to capture the users\u2019 up-to-date interests, thus the historical data are usually used with a time decay factor [12] and lots of data could be \u201cexpired\u201d.", "startOffset": 191, "endOffset": 195}, {"referenceID": 6, "context": "The 1M MovieLens dataset [7] contains about 1 million ratings of 3,952 movies by 6,040 users with Unix timestamps.", "startOffset": 25, "endOffset": 28}, {"referenceID": 4, "context": "The Yahoo Music dataset [5] comprises 262, 810, 175 ratings of 624, 961 music items by 1, 000, 990 users collected during the year 1999\u2212 2010.", "startOffset": 24, "endOffset": 27}, {"referenceID": 11, "context": "\u2022 timeSVD++ [12].", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "This method is reported to achieve excellent performance on the Netflix dataset [2] by considering the time changing behaviors throughout the life span of the data.", "startOffset": 80, "endOffset": 83}, {"referenceID": 20, "context": "\u2022 Factorization Machine (FM) [21].", "startOffset": 29, "endOffset": 33}, {"referenceID": 29, "context": "We use the libMF implementation [30].", "startOffset": 32, "endOffset": 36}, {"referenceID": 3, "context": "1 [4], by the official KDDCUP\u203211 competition report [5].", "startOffset": 2, "endOffset": 5}, {"referenceID": 4, "context": "1 [4], by the official KDDCUP\u203211 competition report [5].", "startOffset": 52, "endOffset": 55}, {"referenceID": 7, "context": "Previous literatures [8] showed that the estimation of the value function could be rather biased, especially when there are large over-estimations of future Q-values.", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "In the previous researches on Q-learning [18], the average reward and the average maxQ are often adopted as indicators of the model stability.", "startOffset": 41, "endOffset": 45}, {"referenceID": 23, "context": "The sequential nature of the recommendation process was noticed in the past [24].", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "The common practice is to model the sequential nature via the Markove Decision Process (MDP) [24, 23].", "startOffset": 93, "endOffset": 101}, {"referenceID": 22, "context": "The common practice is to model the sequential nature via the Markove Decision Process (MDP) [24, 23].", "startOffset": 93, "endOffset": 101}, {"referenceID": 14, "context": "On one hand, the researchers proposed to augment the training data by generating negative samples [15, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 10, "context": "On one hand, the researchers proposed to augment the training data by generating negative samples [15, 11].", "startOffset": 98, "endOffset": 106}, {"referenceID": 12, "context": "On the other hand, the reinforcement learning approaches have been proposed, such as the contextual bandit models [13, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 27, "context": "On the other hand, the reinforcement learning approaches have been proposed, such as the contextual bandit models [13, 28].", "startOffset": 114, "endOffset": 122}, {"referenceID": 17, "context": "In terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9].", "startOffset": 112, "endOffset": 127}, {"referenceID": 18, "context": "In terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9].", "startOffset": 112, "endOffset": 127}, {"referenceID": 13, "context": "In terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9].", "startOffset": 112, "endOffset": 127}, {"referenceID": 8, "context": "In terms of methodology, this work is inspired by the recent advances of deep reinforcement learning researches [18, 19, 14, 9].", "startOffset": 112, "endOffset": 127}], "year": 2017, "abstractText": "We report the \u2018Recurrent Deterioration\u2019 (RD) phenomenon observed in online recommender systems. The RD phenomenon is reflected by the trend of performance degradation when the recommendation model is always trained based on users\u2019 feedbacks of the previous recommendations. There are several reasons for the recommender systems to encounter the RD phenomenon, including the lack of negative training data and the evolution of users\u2019 interests, etc. Motivated to tackle the problems causing the RD phenomenon, we propose the POMDP-Rec framework, which is a neural-optimized Partially Observable Markov Decision Process algorithm for recommender systems. We show that the POMDP-Rec framework effectively uses the accumulated historical data from real-world recommender systems and automatically achieves comparable results with those models fine-tuned exhaustively by domain exports on public datasets.", "creator": "LaTeX with hyperref package"}}}