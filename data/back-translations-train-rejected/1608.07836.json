{"id": "1608.07836", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Aug-2016", "title": "What to do about non-standard (or non-canonical) language in NLP", "abstract": "Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., socio-demographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language.", "histories": [["v1", "Sun, 28 Aug 2016 17:51:41 GMT  (241kb,D)", "http://arxiv.org/abs/1608.07836v1", "KONVENS 2016"]], "COMMENTS": "KONVENS 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barbara plank"], "accepted": false, "id": "1608.07836"}, "pdf": {"name": "1608.07836.pdf", "metadata": {"source": "CRF", "title": "What to do about non-standard (or non-canonical) language in NLP", "authors": ["Barbara Plank"], "emails": ["b.plank@rug.nl"], "sections": [{"heading": null, "text": "The reason for this problem is obvious: NLP models are trained using examples from a limited number of canonical varieties that are considered standard, most notably the English Newswire. However, there are many dimensions, such as socio-demography, language, genre, sentence type, etc., where texts can deviate from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how best to go beyond current practice to train homogeneous data from a single domain and language.In this paper, I address the notion of canonicity and how it shapes our community's approach to language. I advocate using what I call random data, i.e. non-obvious data that is hitherto neglected, hidden in plain text, or raw data that needs to be refined."}, {"heading": "1 Introduction", "text": "It's not as if it's about a way, in which it's about a way, in which people in the different countries of the world behave in different ways. (...) It's not as if it's about a way, in which people in the world behave in a way, in which they do it. (...) It's not as if it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way, in which it's about a way and in which it's about a way it's about a way it's about, in which it's about a way it's about a way it's about a way and in which it's about a way it's about a way and in which it's about a way it's about a way it's about a way and in which it's about a way it's about a way it's about a way it's about a way and a way it's about a way it's about a way it's about a way and a way it's about a way it's about a way and a way it's about a way it's about a way and a way it's about a way it's about a way it's about a way and a way it's about a way it's about and a way it's about a way it's about a way it's about a way and a way it's about a way it's about a way and a way it's about a way it's about and a way it's about a way it's about a way it's about a way it's about a way and a way it's about a way it's about a way and a way it's about a way it's about a way it's about a way and a way it's about a way it's about a way and a way it's about a way"}, {"heading": "2 What to do about non-standard data", "text": "There are generally three main approaches to collecting non-standard data."}, {"heading": "2.1 Annotate more data", "text": "Commenting on more data is a first and intuitive solution. However, it is naive for several reasons. Domain (whatever that means) and language (whatever that means) are two factors of text variation. Let's take the cross-product between the two. We will never be able to create annotated data that includes all possible combinations. This is the problem of data sparseness in education, illustrated in Figure 1. The figure shows only a tiny subset of 4This is related to the problem of overexposure in ethics, for example (Hovy and Spruit, 2016). The languages of the world and a tiny fraction of the potential domains out there. The problem is that most of the data available there are not labeled. Annotation takes time. At the same time requires ways of communication change, so that what we are commenting today could be very far from what we need to process tomorrow. We cannot simply \"find our way out\" (iron stone, 2013), not to trim it."}, {"heading": "2.2 Bring training and test data closer to each other", "text": "A less well-known but similar approach is to artificially corrupt the training data in order to resemble the expected goal (van der Plas et al., 2009). However, normalization implies \"norm,\" and as Eisenstein (2013) notes: Whose norm do we target? (e.g. work versus work). Furthermore, he notes that it is surprisingly difficult to find a precise idea of the normalization task. Corrupting training data is a less explored endeavor. However, this second strategy depends on the assumption that one knows what to expect. What we need are models that provide nonsensical predictions about unexpected input factors, i.e. models that include invariant representations. For example, our models should be able to learn similar representations, such as this level versus the concept of love: *."}, {"heading": "2.3 Domain adaptation", "text": "The approaches range from feature augmentation, shared representation learning, instance weighting to approaches that exploit the representation induced by the general background corpora. To get an overview, see (Plank, 2011; Weiss et al., 2016). However, what all these approaches have in common is an unrealistic assumption: they know the target domain. Again, much of the work is based on knowledge of the target language and requires some in-domain data, typically parallel data. However, one extreme case of adaptation is lingual learning, whose goal is similar: adaptation models trained on some source languages to adapt to languages where few or no resources exist. Again, much of the work requires knowledge of the target language and requires some in-domain data, typically parallel data. However, most work focuses on a limited number of languages that aim to transfer from multiple sources to many target languages (Agic 2016)."}, {"heading": "3 Fortuitous data", "text": "What we need are models that engage in unexpected interventions and from which we can turn away, from a variety of sources. In order to build such models, I argue that the key to this is to be able to go to invisible places, it is necessary to reconstruct and reconstruct the data. (...) It is only a matter of time before it has to be reconstructed. (...) It is a matter of time until it has to be reconstructed. (...) It is a matter of time until it has to be reconstructed, until it has to be reconstructed. (...) It is a matter of time until it has to be reconstructed. \""}, {"heading": "4 But what\u2019s in a domain?", "text": "As already mentioned (Plank, 2011), there is no common ground on what constitutes a domain. Blitzer et al. (2006) attribute domain differences mostly to differences in vocabulary, Biber (1988) examines differences between corpora from a sociolinguistic perspective, but McClosky (2010) looks at them in a broader perspective: \"By domain we mean the style, genre, and medium of a document.\" Terms like genre, register, text type, domain, and style are often used differently in different communities (Lee, 2002) or interchangeably. While there is no definition of domain, work on domain adaptation is plentiful, but mainly focused on the assumption of a dichotomy: source versus target, with little interest in how they differ. In fact, there is surprisingly little work on how texts are vague and the consequences for NLP."}, {"heading": "5 The variety space", "text": "I have shown here a hypothetical example of this kind. Our datasets are spaces of this high-dimensional space, depending on the task in which we are moving. Points in space are the data instances, and regions form domains, etc. A dataset D is a sample from the variety space, due to various factors such as age, gender, personality, etc. However, a domain is a variety that forms a region in this complicated network of similarities, with some members being more prototypical than other languages. However, we do not have access to the number of latent factors or their types. This vision is inspired by the idea of a prototypical theory in Cognitive Science and Wittgenstein's graded notion of categories. Figure 2 shows a hypothetical example of this variety spatially. Our datasets are spaces of this high-dimensional space."}, {"heading": "6 Conclusions", "text": "Current NLP models still suffer dramatically when applied to non-canonical data, where canonicity is a relative notion; in our field, newswire was and remains the de facto standard, the canonical data to which we normally apply our models. Although newswire has advanced the field in many ways, it has also introduced almost imperceptible distortions; what we need is to be aware of such distortions, to collect enough distorted data and model diversity. I argue that if we adopt the diversity of this heterogeneous data by combining it with appropriate algorithms, in addition to incorporating text covariants / latents, we will not only produce more robust models, but also enable adaptive language technologies that are able to address natural language variations."}, {"heading": "Acknowledgments", "text": "I would like to thank the organizers for inviting me to the keynote at KONVENS 2016 as well as He'ctor Mart\u0131 \"nez Alonso, Dirk Hovy, Anders Johannsen, Zeljko Agic\" and Gertjan van Noord for valuable discussions and feedback on earlier drafts of this paper."}], "references": [{"title": "Multilingual projection for parsing truly low-resource languages", "author": ["\u017deljko Agi\u0107", "Anders Johannsen", "Barbara Plank", "Hctor Martnez Alonso", "Natalie Schluter", "Anders S\u00f8gaard."], "venue": "TACL, 4:301\u2013312.", "citeRegEx": "Agi\u0107 et al\\.,? 2016", "shortCiteRegEx": "Agi\u0107 et al\\.", "year": 2016}, {"title": "Truth is a lie: Crowd truth and the seven myths of human annotation", "author": ["Lora Aroyo", "Chris Welty"], "venue": null, "citeRegEx": "Aroyo and Welty,? \\Q2015\\E", "shortCiteRegEx": "Aroyo and Welty", "year": 2015}, {"title": "Using reading behavior to predict grammatical functions", "author": ["Maria Barrett", "Anders S\u00f8gaard."], "venue": "Workshop on Cognitive Aspects of Computational Language Learning.", "citeRegEx": "Barrett and S\u00f8gaard.,? 2015", "shortCiteRegEx": "Barrett and S\u00f8gaard.", "year": 2015}, {"title": "Variation across speech and writing", "author": ["Douglas Biber."], "venue": "Cambridge University Press.", "citeRegEx": "Biber.,? 1988", "shortCiteRegEx": "Biber.", "year": 1988}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proceedings of the 2006 conference on empirical methods in natural language processing, pages 120\u2013128. Association for Computa-", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "What to do about bad language on the internet", "author": ["Jacob Eisenstein."], "venue": "NAACL, Stroudsburg, Pennsylvania.", "citeRegEx": "Eisenstein.,? 2013", "shortCiteRegEx": "Eisenstein.", "year": 2013}, {"title": "From news to comment: Resources and benchmarks for parsing the language of web", "author": ["Jennifer Foster", "Ozlem Cetinoglu", "Joachim Wagner", "Joseph Le Roux", "Joakim Nivre", "Deirdre Hogan", "Josef VanGenabith"], "venue": "IJCNLP", "citeRegEx": "Foster et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2011}, {"title": "Multilingual language processing from bytes", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya."], "venue": "arXiv.", "citeRegEx": "Gillick et al\\.,? 2015", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Prototypes: Between plato and wittgenstein", "author": ["Talmy Giv\u00f3n."], "venue": "Noun classes and categorization, pages 77\u2013102.", "citeRegEx": "Giv\u00f3n.,? 1986", "shortCiteRegEx": "Giv\u00f3n.", "year": 1986}, {"title": "Multilingual dependency parsing evaluation: a large-scale analysis of word order properties using artificial data", "author": ["Kristina Gulordava", "Paola Merlo."], "venue": "Transactions of the Association for Computational Linguistics, 4:343\u2013356.", "citeRegEx": "Gulordava and Merlo.,? 2016", "shortCiteRegEx": "Gulordava and Merlo.", "year": 2016}, {"title": "Lexical normalization for social media text", "author": ["Bo Han", "Paul Cook", "Timothy Baldwin."], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST), 4(1):5.", "citeRegEx": "Han et al\\.,? 2013", "shortCiteRegEx": "Han et al\\.", "year": 2013}, {"title": "Tagging performance correlates with author age", "author": ["Dirk Hovy", "Anders S\u00f8gaard."], "venue": "ACL.", "citeRegEx": "Hovy and S\u00f8gaard.,? 2015", "shortCiteRegEx": "Hovy and S\u00f8gaard.", "year": 2015}, {"title": "The social impact of natural language processing", "author": ["Dirk Hovy", "Shannon L. Spruit."], "venue": "LREC.", "citeRegEx": "Hovy and Spruit.,? 2016", "shortCiteRegEx": "Hovy and Spruit.", "year": 2016}, {"title": "When POS datasets don\u2019t add up: Combatting sample bias", "author": ["Dirk Hovy", "Barbara Plank", "Anders S\u00f8gaard."], "venue": "LREC.", "citeRegEx": "Hovy et al\\.,? 2014", "shortCiteRegEx": "Hovy et al\\.", "year": 2014}, {"title": "Mining for unambiguous instances to adapt part-of-speech taggers to new domains", "author": ["Dirk Hovy", "Barbara Plank", "H\u00e9ctor Mart\u0131nez Alonso", "Anders S\u00f8gaard."], "venue": "NAACL.", "citeRegEx": "Hovy et al\\.,? 2015", "shortCiteRegEx": "Hovy et al\\.", "year": 2015}, {"title": "Improving sentence compression by learning to predict gaze", "author": ["Sigrid Klerke", "Yoav Goldberg", "Anders S\u00f8gaard."], "venue": "NAACL.", "citeRegEx": "Klerke et al\\.,? 2016", "shortCiteRegEx": "Klerke et al\\.", "year": 2016}, {"title": "Genres, registers, text types, domains and styles: clarifying the concepts and navigating a path through the bnc jungle", "author": ["David Lee."], "venue": "Language and Computers, 42(1):247\u2013292.", "citeRegEx": "Lee.,? 2002", "shortCiteRegEx": "Lee.", "year": 2002}, {"title": "Any domain parsing: automatic domain adaptation for natural language parsing", "author": ["David McClosky."], "venue": "Ph.D. thesis, Brown University.", "citeRegEx": "McClosky.,? 2010", "shortCiteRegEx": "McClosky.", "year": 2010}, {"title": "A universal part-of-speech tagset", "author": ["Slav Petrov", "Dipanjan Das", "Ryan McDonald."], "venue": "LREC.", "citeRegEx": "Petrov et al\\.,? 2012", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Adapting taggers to twitter using not-so-distant supervision", "author": ["Barbara Plank", "Dirk Hovy", "Ryan McDonald", "Anders S\u00f8gaard."], "venue": "COLING.", "citeRegEx": "Plank et al\\.,? 2014a", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Learning part-of-speech taggers with inter-annotator agreement loss", "author": ["Barbara Plank", "Dirk Hovy", "Anders S\u00f8gaard."], "venue": "EACL.", "citeRegEx": "Plank et al\\.,? 2014b", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Non-canonical language is not", "author": ["Barbara Plank", "H\u00e9ctor Mart\u0131\u0301nez Alonso", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2015}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "ACL.", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Domain adaptation for parsing", "author": ["Barbara Plank."], "venue": "Ph.D. thesis, University of Groningen.", "citeRegEx": "Plank.,? 2011", "shortCiteRegEx": "Plank.", "year": 2011}, {"title": "What i\u2019ve learned about annotating informal text (and why you shouldnt take my word for it)", "author": ["Nathan Schneider."], "venue": "The 9th Linguistic Annotation Workshop held in conjuncion with NAACL 2015.", "citeRegEx": "Schneider.,? 2015", "shortCiteRegEx": "Schneider.", "year": 2015}, {"title": "A gold standard dependency corpus for english", "author": ["Natalia Silveira", "Timothy Dozat", "Marie-Catherine De Marneffe", "Samuel R Bowman", "Miriam Connor", "John Bauer", "Christopher D Manning."], "venue": "LREC.", "citeRegEx": "Silveira et al\\.,? 2014", "shortCiteRegEx": "Silveira et al\\.", "year": 2014}, {"title": "Profiting from mark-up: Hyper-text annotations for guided parsing", "author": ["Valentin I Spitkovsky", "Daniel Jurafsky", "Hiyan Alshawi."], "venue": "ACL.", "citeRegEx": "Spitkovsky et al\\.,? 2010", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2010}, {"title": "Token and type constraints for cross-lingual part-of-speech tagging", "author": ["Oscar T\u00e4ckstr\u00f6m", "Dipanjan Das", "Slav Petrov", "Ryan McDonald", "Joakim Nivre."], "venue": "Transactions of the Association for Computational Linguistics, 1:1\u201312.", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? 2013", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Domain adaptation with artificial data for semantic parsing of speech", "author": ["Lonneke van der Plas", "James Henderson", "Paola Merlo."], "venue": "NAACL.", "citeRegEx": "Plas et al\\.,? 2009", "shortCiteRegEx": "Plas et al\\.", "year": 2009}, {"title": "A survey of transfer learning", "author": ["Karl Weiss", "Taghi M Khoshgoftaar", "DingDing Wang."], "venue": "Journal of Big Data, 3(1):1\u201340.", "citeRegEx": "Weiss et al\\.,? 2016", "shortCiteRegEx": "Weiss et al\\.", "year": 2016}, {"title": "Different flavors of gum: Evaluating genre and sentence type effects on multilayer corpus annotation quality", "author": ["Amir Zeldes", "Dan Simonson."], "venue": "Proceedings of the 10th Linguistic Annotation Workshop held in conjunction with ACL 2016 (LAW-X", "citeRegEx": "Zeldes and Simonson.,? 2016", "shortCiteRegEx": "Zeldes and Simonson.", "year": 2016}, {"title": "Adapting a part-of-speech tagset to nonstandard text: The case of stts", "author": ["Heike Zinsmeister", "Ulrich Heid", "Kathrin Beck."], "venue": "LREC.", "citeRegEx": "Zinsmeister et al\\.,? 2014", "shortCiteRegEx": "Zinsmeister et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "guage\u2019 (Eisenstein, 2013).", "startOffset": 7, "endOffset": 25}, {"referenceID": 21, "context": "cessing and annotating are two aspects, and the difficulty in one, say processing, does not necessarily propagate the same way to annotation (Plank et al., 2015).", "startOffset": 141, "endOffset": 161}, {"referenceID": 12, "context": ", (Hovy and Spruit, 2016).", "startOffset": 2, "endOffset": 25}, {"referenceID": 5, "context": "We cannot just \u201cannotate our way out\u201d (Eisenstein, 2013).", "startOffset": 38, "endOffset": 56}, {"referenceID": 31, "context": "Moreover, it might not be trivial to find the right annotators; annotation schemes might need adaptation as well (Zinsmeister et al., 2014) and tradeoffs for doing so need to be defined (Schneider, 2015).", "startOffset": 113, "endOffset": 139}, {"referenceID": 24, "context": ", 2014) and tradeoffs for doing so need to be defined (Schneider, 2015).", "startOffset": 54, "endOffset": 71}, {"referenceID": 10, "context": "Han et al. (2013). A less known but similar approach is to artificially corrupt the training data to make it more similar to the expected target do-", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "However, normalization implies \u201cnorm\u201d, and as Eisenstein (2013) remarks: whose norm are we targeting? (e.", "startOffset": 46, "endOffset": 64}, {"referenceID": 23, "context": "For an overview, see (Plank, 2011; Weiss et al., 2016).", "startOffset": 21, "endOffset": 54}, {"referenceID": 29, "context": "For an overview, see (Plank, 2011; Weiss et al., 2016).", "startOffset": 21, "endOffset": 54}, {"referenceID": 0, "context": "However, most work has focused on a restricted set of languages, only recently approaches emerged that aim to transfer from multiple sources to many target languages (Agi\u0107 et al., 2016).", "startOffset": 166, "endOffset": 185}, {"referenceID": 19, "context": "Concrete examples include hyperlinks that can be used to build more robust named entity and part-of-speech taggers (Plank et al., 2014a), or HTML markup for parsing (Spitkovsky et al.", "startOffset": 115, "endOffset": 136}, {"referenceID": 26, "context": ", 2014a), or HTML markup for parsing (Spitkovsky et al., 2010).", "startOffset": 37, "endOffset": 62}, {"referenceID": 14, "context": "Similarly, Wiktionary can be used to mine large pools of data for unambiguous instances (Hovy et al., 2015), or can guide constrained inference like in typeconstrained POS tagging (T\u00e4ckstr\u00f6m et al.", "startOffset": 88, "endOffset": 107}, {"referenceID": 27, "context": ", 2015), or can guide constrained inference like in typeconstrained POS tagging (T\u00e4ckstr\u00f6m et al., 2013; Plank et al., 2014b).", "startOffset": 80, "endOffset": 125}, {"referenceID": 20, "context": ", 2015), or can guide constrained inference like in typeconstrained POS tagging (T\u00e4ckstr\u00f6m et al., 2013; Plank et al., 2014b).", "startOffset": 80, "endOffset": 125}, {"referenceID": 20, "context": ", (Plank et al., 2014b; Aroyo et al., 2015).", "startOffset": 2, "endOffset": 43}, {"referenceID": 2, "context": ", (Barrett and S\u00f8gaard, 2015; Klerke et al., 2016).", "startOffset": 2, "endOffset": 50}, {"referenceID": 15, "context": ", (Barrett and S\u00f8gaard, 2015; Klerke et al., 2016).", "startOffset": 2, "endOffset": 50}, {"referenceID": 23, "context": "As already noted earlier (Plank, 2011), there is no common ground on what constitutes a domain.", "startOffset": 25, "endOffset": 38}, {"referenceID": 16, "context": "\u201d Terms such as genre, register, text type, domain, style are often used differently in different communities (Lee, 2002), or interchangeably.", "startOffset": 110, "endOffset": 121}, {"referenceID": 3, "context": "Blitzer et al. (2006) attribute domain differences mostly to differences in vocabulary, Biber (1988) explores differences between corpora from a sociolinguistics perspective.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "(2006) attribute domain differences mostly to differences in vocabulary, Biber (1988) explores differences between corpora from a sociolinguistics perspective.", "startOffset": 73, "endOffset": 86}, {"referenceID": 3, "context": "(2006) attribute domain differences mostly to differences in vocabulary, Biber (1988) explores differences between corpora from a sociolinguistics perspective. McClosky (2010) considers it in a broader view: \u201cBy domain, we mean the style, genre, and medium of a document.", "startOffset": 73, "endOffset": 176}, {"referenceID": 9, "context": "\u201d A very recent paper examines word order properties and their impact on parsing taking a control experiment approach (Gulordava and Merlo, 2016).", "startOffset": 118, "endOffset": 145}, {"referenceID": 11, "context": "On another angle, it has been shown that tagging accuracy correlates with demographic factors such as age (Hovy and S\u00f8gaard, 2015).", "startOffset": 106, "endOffset": 130}, {"referenceID": 27, "context": "For example, focusing on annotation difficulty, Zeldes and Simonson (2016) remark \u201cthat domain adaptation may be folding in sentence type effects\u201d, motivated by earlier findings by Silveira et al.", "startOffset": 48, "endOffset": 75}, {"referenceID": 23, "context": "For example, focusing on annotation difficulty, Zeldes and Simonson (2016) remark \u201cthat domain adaptation may be folding in sentence type effects\u201d, motivated by earlier findings by Silveira et al. (2014) who remark that \u201c[t]he most striking difference between the two types of data [Web and newswire] has to do with imperatives, which occur two orders of magnitude more often in the EWT [English Web Treebank].", "startOffset": 181, "endOffset": 204}, {"referenceID": 22, "context": "We use TNT,6 an HMMbased tagger, and BILTY, a bidirectional LSTM tagger (Plank et al., 2016).", "startOffset": 72, "endOffset": 92}, {"referenceID": 18, "context": "Both taggers are trained on the WSJ training portion converted to Universal POS tags (Petrov et al., 2012).", "startOffset": 85, "endOffset": 106}, {"referenceID": 11, "context": "consider parts of the Web Treebank (emails and answers), two Twitter datasets (FOSTER and GIMPEL/OCT27, Twitter sample 1 and 2 respectively), review data from two different age groups (Hovy and S\u00f8gaard, 2015), above 45 and below 35 years, and data from the CoNLL-X dataset from other Indogermanic languages.", "startOffset": 184, "endOffset": 208}, {"referenceID": 18, "context": "Table 2: Tagging accuracy on various test set varieties (domains, languages and age groups; Tw=Twitter), using coarse POS (Petrov et al., 2012).", "startOffset": 122, "endOffset": 143}, {"referenceID": 7, "context": "8Subtoken representations are used train a single tagger for multiple languages (Gillick et al., 2015).", "startOffset": 80, "endOffset": 102}, {"referenceID": 13, "context": "We see that performance varies greatly on different samples of Twitter data, as also reported earlier (Hovy et al., 2014).", "startOffset": 102, "endOffset": 121}, {"referenceID": 8, "context": "(Giv\u00f3n, 1986).", "startOffset": 0, "endOffset": 13}], "year": 2016, "abstractText": "Real world data differs radically from the benchmark corpora we use in natural language processing (NLP). As soon as we apply our technologies to the real world, performance drops. The reason for this problem is obvious: NLP models are trained on samples from a limited set of canonical varieties that are considered standard, most prominently English newswire. However, there are many dimensions, e.g., sociodemographics, language, genre, sentence type, etc. on which texts can differ from the standard. The solution is not obvious: we cannot control for all factors, and it is not clear how to best go beyond the current practice of training on homogeneous data from a single domain and language. In this paper, I review the notion of canonicity, and how it shapes our community\u2019s approach to language. I argue for leveraging what I call fortuitous data, i.e., nonobvious data that is hitherto neglected, hidden in plain sight, or raw data that needs to be refined. If we embrace the variety of this heterogeneous data by combining it with proper algorithms, we will not only produce more robust models, but will also enable adaptive language technology capable of addressing natural language variation.", "creator": "TeX"}}}