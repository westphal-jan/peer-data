{"id": "1608.08176", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2016", "title": "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based Software Engineering)", "abstract": "Topic Modeling finds human-readable structures in large sets of unstructured SE data. A widely used topic modeler is Latent Dirichlet Allocation. When run on SE data, LDA suffers from \"order effects\" i.e. different topics be generated if the training data was shuffled into a different order. Such order effects introduce a systematic error for any study that uses topics to make conclusions. This paper introduces LDADE, a Search-Based SE tool that tunes LDA's parameters using DE (Differential Evolution). LDADE has been tested on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of SE papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (the traditional VEM method, or using Gibbs sampling). In all tests, the pattern was the same: LDADE's tunings dramatically reduces topic instability. The implications of this study for other software analytics tasks is now an open and pressing issue. In how many domains can search-based SE dramatically improve software analytics?", "histories": [["v1", "Mon, 29 Aug 2016 18:45:00 GMT  (3893kb,D)", "http://arxiv.org/abs/1608.08176v1", "10 pages + 2 page references. Submitted to ICSE 2017"], ["v2", "Wed, 8 Feb 2017 01:19:06 GMT  (4153kb,D)", "http://arxiv.org/abs/1608.08176v2", "12 pages + 3 page references. Submitted to IST Journal"]], "COMMENTS": "10 pages + 2 page references. Submitted to ICSE 2017", "reviews": [], "SUBJECTS": "cs.SE cs.AI cs.CL cs.IR", "authors": ["amritanshu agrawal", "wei fu", "tim menzies"], "accepted": false, "id": "1608.08176"}, "pdf": {"name": "1608.08176.pdf", "metadata": {"source": "CRF", "title": "What is Wrong with Topic Modeling? (and How to Fix it Using Search-based SE)", "authors": ["Amritanshu Agrawal", "Wei Fu", "Tim Menzies"], "emails": ["aagrawa8@ncsu.edu", "wfu@ncsu.edu", "tim.menzies@gmail.com"], "sections": [{"heading": null, "text": "When working on SE data, LDA suffers from \"order effects,\" i.e. different topics are generated if the training data is not moved in a different order. Such order effects introduce a systematic error for each study that uses topics to draw conclusions. This paper introduces LDADE, a search-based SE tool that adjusts the parameters of LDA using DE (Differential Evolution). LDADE was tested using data from a programmer information exchange site (stack overflow), titles and abstract text from thousands of SE papers and software bug reports from NASA. Results were collected through various implementations of LDA (Python + Scikit Learn), Scala + Spark across different platforms (Linux, Macintosh), and for different types of LDAs (the traditional VEM method, or the use of Gibbs sampling). In all tests, the themes were reduced to the same themes: Tuning-LDA-reduced the Learn-reduced themes, Learn-reduced the themes from different platforms, Learn-reduced the themes to different themes."}, {"heading": "II. RELATED WORK", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. About Tuning: Important and Ignored", "text": "When we tune a Data Miner, we actually change how a learner applies his heuristics. This means that tuned Data Miners use different heuristics, which means that they ignore different possible models, which means that they return different models; i.e., how we learn changes what we learn. However, tuning issues are poorly handled in the software analysis literature. Wei et al. [14] examined hundreds of current SE papers in the field of software error prediction from static code attributes. They found that most SE authors do not take steps to explore tunings (rare exception: [59]). For example, Elish et al. [11] compared support vector machines with other data miners for the purpose of error prediction. This paper tested different off-the-shelf data miners in the same way without adjusting the parameters of each learner. Nevertheless, Weal showed (different) that (one) set of data miners is suitable for (one)."}, {"heading": "B. About Topic Modeling", "text": "Latent Dirichlet Allocation (LDA) is a generative statistical model that enables observation sets to be explained by unobserved groups that explain why some parts of the data are similar; it learns the different distributions (the amount of topics, the associated word probabilities, the topic of each word, and the specific mix of topics of each document); what makes theme modeling interesting is that these algorithms scale to very large text corpses. In this essay, for example, we apply the total stack overflow and two other large text corpses in SE.Initially, \u03b2 can be randomly determined as follows: Each word in a document was generated by first randomly selecting a topic (from the topic distribution of the document) and then randomly selecting a word (from the word distribution of the topic). Successive iterations of the algorithm count the implications of a previous sample, which in turn are gradually updated."}, {"heading": "C. About Order Effects", "text": "This paper uses tuning to fix \"order effects\" in theme modeling. Langley [18] defines such effects as follows:"}, {"heading": "A learner L exhibits an order effect on a training set", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "T if there exist two or more orders of T for which", "text": "L produces different knowledge structures. Many learners show order effects: for example, certain incremental cluster algorithms generate different clusters depending on the order in which they examine the data. [18] Therefore, some algorithms examine the space of possible models across numerous random areas of the data (e.g. random forests [7]). The description offered above in Sections II-B shows (a) how theme modeling might be susceptible to order effects and (b) how such order effects could be tamed: \u2022 In the above description k, \u03b1, \u03b2 are randomly initialized and then updated through an incremental resampling process. Such incremental updates tend to order effects. \u2022 One technique for reducing the effect of different data orders is to initialize k, \u03b1, \u03b2 to a large value. The trick of using this technique is that different data sets require different initializations; i.e., the tuning process must be repeated for each new set of data."}, {"heading": "D. WorkArounds for LDA Instability Problem", "text": "To understand how other researchers explored LDA instability, in April 2016 we searched scholar.google.com for the connection between \"lda\" and \"topics\" or \"stable\" or \"cohrence.\" Since 2012, there have been 189 such papers, 57 of which are due to the results of software engineering. \u2022 28 mentions of instability in LDA. \u2022 Of these 28, despite mentioning stability problems, used LDA parameters. \u2022 The other papers used a combination of manual adjustment or subordinate exploration based on \"technical judgments\" (i.e. some settings guided by the researchers)."}, {"heading": "III. METHODS", "text": "This section describes our assessment methods for measuring instability as well as the optimization methods to reduce this instability."}, {"heading": "A. Data Sets", "text": "In order to answer our research questions and make our results reproducible, we use three open source datasets summarized in Table III and described below. PITS is a text mining dataset generated from the reports of the NASA software project and the Problem Tracking System (PITS) [38], [41]. This text discusses errors and changes found in major reports and review patches. Such problems are used to manage quality assurance to support communication between developers. Topic modeling in PITS can be used to identify the top topics that can identify each severity separately. The dataset can be downloaded from the PROMISE repository [40]. Note that this data comes from six different NASA projects, which we refer to as PitsA, PitsB, etc.Stackoverflow is the flagship website of the Stack Exchange, which contains questions and answers to a wide range of topics."}, {"heading": "B. Similarity Scoring", "text": "In order to evaluate the coherence of the themes in LDA, we do not employ a direct approach by asking people about topics, and we use an indirect approach to evaluate the coherence of the themes in LDA, which is why it is important for us to focus on the topics that matter most to us."}, {"heading": "C. Tuning Topic Modeling with LDADE", "text": "LDADE is a combination of topic modeling (with LDA) and an optimizer (with Differential Evolution, or DE) that adjusts the parameters of the LDA to optimize (i.e. maximize) it. We choose to use DE after a literature search based on search methods. Literature mentions many optimizers: simulated annealing [12], [46], [46], [46], [algorithm 1 pseudo code for untuned LDA with default parameters such as DE (differentiated evolution), [57], [44], [46], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44], [44]."}, {"heading": "IV. EXPERIMENTATION", "text": "In this section, all the results from the smaller datasets (pits and citemaps) come from a Scikit Learn-based Python implementation running on a single 4-core computer. All the results from the larger data (stack overflow) come from a Mllib-based Scale implementation running on a Spark system with 45 nodes (8 cores per node). Note that we mostly shift the stack overflow results to the \u00a7 V threats, where we will show that (a) there is instability in theme modeling across multiple platforms and implementation languages; (b) tuning can improve stability for different platforms and implementations for LDA."}, {"heading": "A. RQ1: Are the default settings of LDA correct?", "text": "This first research question examines the core premise of this article that changes in the order of training data dramatically affect the topics learned about LDA. Note that if this is not true, there would be no added value in this paper. Figure 6 shows n vs < n for out-of-tune LDA. Note that stability most collapses after n = 5 words. This means that any report on LDA topics using more than five words per topic is changed just by changing the order of input. This is a significant result, since the defaults in the LDA papers [34], [52] must report the top 10 words per topic. As shown in Figure 7a, it would be rare for such a 10-word topic to be found over multiple rounds."}, {"heading": "B. RQ2: Does tuning improve the stability scores of LDA?", "text": "Figure 7a and Figure 7b show the stability improvement generated by tuning. Tuning never has a negative effect (reduces stability) and often has a large positive effect - especially after 5 terms overlap. The biggest improvement was the PitsD dataset, which overlaps 100% with up to 8 terms (i.e. always found in all runs). Overall, after reports on topics with up to 7 words, in most cases (66%), we recommend finding these topics in models created using different input orders. Accordingly, our response to RQ2: Result 2 Based on Figure 7, urgently tuning for future LDA studies. Figure 7a: After Tuning: uses parameters learned from DE. Figure 7b: Delta = After - Before. Figure 7: RQ1, RQ2 stability results over ten repeated runs. In these figures, larger numbers are better. Figure 8: Data sets vs. Parameters (Figure) (data sets) (variation)."}, {"heading": "C. RQ3: Do different data sets need different configurations to make LDA stable?", "text": "Figures 8, 9 and 10 show the results of the tuning. In each set of vertical bars, the mean values generated over 10 tunings are shown. Furthermore, the interquartile range (IQR) of these tunings is shown (the IQR is the 75th to 25th percentile value and is a non-parametric measure of the variation around the mean). Note that in Figure 8, IQR = 0 for PitsB data set, in which the tuning values always converge to the same final value. These numbers show how Tuning selects the different parameter ranges. Some of the above numbers are far from the default values; e.g. Garousi et al. [17] recommend the use of k = 67 topics, but in our data sets, the best results were obtained with k \u2264 24. Clearly: Result 3 Do not use tunings suggested by other researchers from other data sets, but always tune to all new data."}, {"heading": "D. RQ4: Is tuning easy?", "text": "In the DE literature, it is recommended to use a population size np that is ten times greater than the number of parameters to be optimized [57]. For example, the DE literature recommends using k, \u03b1, \u03b2 np = 30 when voting. Figure 11 examines np = 30 compared to the np = 10 we use in Algorithm 2 (as well as some other variants of DE's F and CR parameters). This figure shows results that are relevant for citemap, and for space reasons, results are displayed in relation to other datasets in https: / / goo.gl / HQNASF. After checking the results from all datasets, we can say that there is not much improvement when using other F, CR, and population sizes. Therefore, our other experiments used F = 0.7, CR = 0.3, and np = 10. Also: Result 4 Determining stable parameters for topic models is easier than standard optimization tasks."}, {"heading": "E. RQ5: What is the runtime cost of tuning?", "text": "Search-based SE methods can be very slow. Harman et al. once took 15 years of CPU time to find and verify the tunings required for software clone detectors [66]. Sayyad et al. routinely used 106 ratings (or more) of their models to extract products from severely restricted product lines [56]. Therefore, before recommending any search-based method, it is wise to consider the runtime costs of this recommendation. To understand our temporal results, remember that untuned, tuned LDA uses algorithm 1 or algorithm 2. Based on the psuedocode shown above, we assume that tunings will be three times slower than not tuned. This is definitely more than no use of LDA, but given modern cloud computing environments, this may not be a tedious increase."}, {"heading": "F. RQ6: Should data miners be used \u201coff-the-shelf\u201d with their default tunings?", "text": "Figures 8, 9 and 10 show that the range of the \"best\" tunings is very data set specific. Consequently, with a new data set, the default tunings often fall far from the usable range. Figures 12 and 13 show that tuning is definitely slower than usual, but the total cost is not prohibitive. Result 6 If the goal is to let people browse through the topics learned, we cannot recommend the use of \"standard\" LDA. Note that this conclusion applies, as mentioned above, to uncontrolled cluster applications. LDA may be sufficient for other types of monitored classification applications."}, {"heading": "V. THREATS TO VALIDITY", "text": "The usual threats to validity from software analysis apply to this paper: \u2022 Our conclusions assume that the goal of subject modeling is to produce stable topics that humans search through and reflect on. As mentioned in the introduction, there is a class of work that does not. \u2022 The conclusions of this paper are based on a limited number of data sets, and it is possible that other data invalidate our conclusions. As with all analytical work, any researcher can make his conclusions and materials public and encourage other researchers to repeat / refute / improve their conclusions. For example, the footnotes of this paper contain all Urls in which other researchers can download our materials and examine the conclusions of this paper."}, {"heading": "VI. CONCLUSION AND FUTURE WORK", "text": "Based on the above, we offer a specific and general conclusion. We most specifically recommend \u2022 Any study that shows that the topics learned from the LDA lead to different conclusions. However, stability can be greatly increased after tuning. \u2022 Contrary to the advice of Stacy et al. [34], LDA topics should not be reported as the top ten words. Due to regulatory effects, such a report can be highly incorrect. Our results show that up to eight words can be reliably reported, but only after tuning to stability using tools such as LDADE. \u2022 Do not download pre-set LDA topics from someone else because, as shown here, the best LDA tunings vary from software to data set. Our experience is that this recommendation does not represent onerous demand, as tuning contributes less than a factor of ten to the total times of the DyDA search."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The work is partially funded by NSF Awards # 1506586."}], "references": [{"title": "Mining search topics from a code search engine usage log", "author": ["S.K. Bajracharya", "C.V. Lopes"], "venue": "In MSR,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "What are developers talking about? an analysis of topics and trends in stack overflow", "author": ["A. Barua", "S.W. Thomas", "A.E. Hassan"], "venue": "Empirical Software Engineering,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "MOSS\u201d multiobjective scatter search applied to nonlinear multiple criteria optimization", "author": ["R.P. Beausoleil"], "venue": "European Journal of Operational Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "Random search for hyper-parameter optimization", "author": ["J. Bergstra", "Y. Bengio"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Nltk: the natural language toolkit", "author": ["S. Bird"], "venue": "In Proceedings of the COLING/ACL on Interactive presentation sessions,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "In the Journal of machine Learning research,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "Topic-based software defect explanation", "author": ["T.-H. Chen", "W. Shang", "M. Nagappan", "A.E. Hassan", "S.W. Thomas"], "venue": "Journal of Systems and Software,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Explaining software defects using topic models", "author": ["T.-H. Chen", "S.W. Thomas", "M. Nagappan", "A.E. Hassan"], "venue": "In Mining Software Repositories (MSR),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Tuning pid controller with multiobjective differential evolution", "author": ["I. Chiha", "J. Ghabi", "N. Liouane"], "venue": "In Communications Control and Signal Processing (ISCCSP),", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Predicting defect-prone software modules using support vector machines", "author": ["K.O. Elish", "M.O. Elish"], "venue": "Journal of Systems and Software,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2008}, {"title": "Converging on the optimal attainment of requirements", "author": ["M.S. Feather", "T. Menzies"], "venue": "In Requirements Engineering,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "The Text Mining Handbook", "author": ["J R.-S. Feldman"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Tuning for software analytics: Is it really necessary", "author": ["W. Fu", "T. Menzies", "X. Shen"], "venue": "Information and Software Technology,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Automated classification of software change messages by semi-supervised latent dirichlet allocation", "author": ["Y. Fu", "M. Yan", "X. Zhang", "L. Xu", "D. Yang", "J.D. Kymer"], "venue": "Information and Software Technology,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Analysis of user comments: an approach for software requirements evolution", "author": ["L.V. Galvis Carre\u00f1o", "K. Winbladh"], "venue": "In Proceedings of the 2013 International Conference on Software Engineering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Citations, research topics and active countries in software engineering: A bibliometrics study", "author": ["V. Garousi", "M.V. M\u00e4ntyl\u00e4"], "venue": "Computer Science Review,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Models of incremental concept formation", "author": ["J.H. Gennari", "P. Langley", "D. Fisher"], "venue": "Artificial Intelligence,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1989}, {"title": "The general employee scheduling problem. an integration of ms and ai", "author": ["F. Glover", "C. McMillan"], "venue": "Computers & operations research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1986}, {"title": "On the complexity of the satisfiability problem", "author": ["A.T. Goldberg"], "venue": "PhD thesis,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1979}, {"title": "Using heuristics to estimate an appropriate number of latent topics in source code analysis", "author": ["S. Grant", "J.R. Cordy", "D.B. Skillicorn"], "venue": "Science of Computer Programming,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National academy of Sciences,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "How do users like this feature? a fine grained sentiment analysis of app reviews", "author": ["E. Guzman", "W. Maalej"], "venue": "In Requirements Engineering Conference (RE),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Relating requirements to implementation via topic analysis: Do topics extracted from requirements make sense to managers and developers", "author": ["A. Hindle", "C. Bird", "T. Zimmermann", "N. Nagappan"], "venue": "In Software Maintenance (ICSM),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2012}, {"title": "Automated topic naming to support cross-project analysis of software maintenance activities", "author": ["A. Hindle", "N.A. Ernst", "M.W. Godfrey", "J. Mylopoulos"], "venue": "In Proceedings of the 8th Working Conference on Mining Software Repositories,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Latent dirichlet allocation: stability and applications to studies of user-generated content", "author": ["S. Koltcov", "O. Koltsova", "S. Nikolenko"], "venue": "In Proceedings of the 2014 ACM conference on Web science,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Gale: Geometric active learning for search-based software engineering", "author": ["J. Krall", "T. Menzies", "M. Davies"], "venue": "Software Engineering, IEEE Transactions on,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Machine reading tea leaves: Automatically evaluating topic coherence and topic model quality", "author": ["J.H. Lau", "D. Newman", "T. Baldwin"], "venue": "In EACL,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Topic modeling of nasa space system problem reports: research in practice", "author": ["L. Layman", "A.P. Nikora", "J. Meek", "T. Menzies"], "venue": "In Proceedings of the 13th International Workshop on Mining Software Repositories,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2016}, {"title": "Predicting effectiveness of irbased bug localization techniques", "author": ["T.-D.B. Le", "F. Thung", "D. Lo"], "venue": "In 2014 IEEE 25th International Symposium on Software Reliability Engineering,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "An exploratory analysis of mobile development issues using stack overflow", "author": ["M. Linares-V\u00e1squez", "B. Dit", "D. Poshyvanyk"], "venue": "In Proceedings of the 10th Working Conference on Mining Software Repositories,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Improving trace accuracy through data-driven configuration and composition of tracing features", "author": ["S. Lohar", "S. Amornborvornwong", "A. Zisman", "J. Cleland-Huang"], "venue": "In Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2013}, {"title": "Bug localization using latent dirichlet allocation", "author": ["S.K. Lukins", "N.A. Kraft", "L.H. Etzkorn"], "venue": "Information and Software Technology,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2010}, {"title": "The app sampling problem for app store mining", "author": ["W. Martin", "M. Harman", "Y. Jia", "F. Sarro", "Y. Zhang"], "venue": "In Mining Software Repositories (MSR),", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Mining business topics in source code using latent dirichlet allocation", "author": ["G. Maskeri", "S. Sarkar", "K. Heafield"], "venue": "In Proceedings of the 1st India software engineering conference,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Trends in topics at SE conferences (1993-2013)", "author": ["G. Mathew", "A. Agrawal", "T. Menzies"], "venue": "Submitted to ICSE\u201917. Available from tiny.cc/trendsSE,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2017}, {"title": "Improving iv&v techniques through the analysis of project anomalies: Text mining pits issue reports-final report", "author": ["T. Menzies"], "venue": null, "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2008}, {"title": "The business case for automated software engineering", "author": ["T. Menzies", "O. Elrawas", "J. Hihn", "M. Feather", "R. Madachy", "B. Boehm"], "venue": "In Proceedings of the twenty-second IEEE/ACM international conference on Automated software engineering,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2007}, {"title": "The Promise Repository of Empirical Software Engineering Data", "author": ["T. Menzies", "R. Krishna", "D. Pryor"], "venue": "http://openscience.us/repo,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Automated severity assessment of software defect reports", "author": ["T. Menzies", "A. Marcus"], "venue": "In Software Maintenance,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2008}, {"title": "The magical number seven, plus or minus two: Some limits on our capacity for processing information", "author": ["G.A. Miller"], "venue": "Psychological review,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 1956}, {"title": "Expectation-propagation for the generative aspect model", "author": ["T. Minka", "J. Lafferty"], "venue": "In Proceedings of the Eighteenth conference on Uncertainty in artificial intelligence,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2002}, {"title": "Sspmo: A scatter tabu search procedure for non-linear multiobjective optimization", "author": ["J. Molina", "M. Laguna", "R. Mart\u0131", "R. Caballero"], "venue": "INFORMS Journal on Computing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2007}, {"title": "Structured versus unstructured data: The balance of power continues to shift. IDC (Industry Development and Models) Mar", "author": ["A. Nadkarni", "N. Yezhkova"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2014}, {"title": "Abyss: Adapting scatter search to multiobjective optimization", "author": ["A.J. Nebro", "F. Luna", "E. Alba", "B. Dorronsoro", "J.J. Durillo", "A. Beham"], "venue": "Evolutionary Computation, IEEE Transactions on,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2008}, {"title": "Topic modelling for qualitative studies", "author": ["S.I. Nikolenko", "S. Koltcov", "O. Koltsova"], "venue": "Journal of Information Science, page 0165551515617393,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2015}, {"title": "On the equivalence of information retrieval methods for automated traceability link recovery", "author": ["R. Oliveto", "M. Gethers", "D. Poshyvanyk", "A. De Lucia"], "venue": "In Program Comprehension (ICPC),", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "Differential evolution methods for unsupervised image classification", "author": ["M.G. Omran", "A.P. Engelbrecht", "A. Salman"], "venue": "In Evolutionary Computation,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2005}, {"title": "An analysis of the coherence of descriptors in topic modeling", "author": ["D. O\u2019Callaghan", "D. Greene", "J. Carthy", "P. Cunningham"], "venue": "Expert Systems with Applications,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Particle swarm-simulated annealing fusion algorithm and its application in function optimization", "author": ["H. Pan", "M. Zheng", "X. Han"], "venue": "In Computer Science and Software Engineering,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2008}, {"title": "How to effectively use topic models for software engineering tasks? an approach based on genetic algorithms", "author": ["A. Panichella", "B. Dit", "R. Oliveto", "M. Di Penta", "D. Poshyvanyk", "A. De Lucia"], "venue": "In Proceedings of the 2013 International Conference on Software Engineering,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2011}, {"title": "Retrieval from software libraries for bug localization: a comparative study of generic and composite text models", "author": ["S. Rao", "A. Kak"], "venue": "In Proceedings of the 8th Working Conference on Mining Software Repositories,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2011}, {"title": "Scalable product line configuration: A straw to break the camel\u2019s back", "author": ["A.S. Sayyad", "J. Ingram", "T. Menzies", "H. Ammar"], "venue": "In Automated Software Engineering (ASE),", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2013}, {"title": "Differential evolution\u2013a simple and efficient heuristic for global optimization over continuous spaces", "author": ["R. Storn", "K. Price"], "venue": "Journal of global optimization,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 1997}, {"title": "Msr4sm: Using topic models to effectively mining software repositories for software maintenance tasks", "author": ["X. Sun", "B. Li", "H. Leung", "Y. Li"], "venue": "Information and Software Technology,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "Automated Parameter Optimization of Classification techniques for Defect Prediction Models", "author": ["C. Tantithamthavorn", "S. McIntosh", "A.E. Hassan", "K. Matsumoto"], "venue": "In Proc. of the International Conference on Software Engineering (ICSE),", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2016}, {"title": "Mining software repositories using topic models", "author": ["S.W. Thomas"], "venue": "In Proceedings of the 33rd International Conference on Software Engineering,", "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2011}, {"title": "Studying software evolution using topic models", "author": ["S.W. Thomas", "B. Adams", "A.E. Hassan", "D. Blostein"], "venue": "Science of Computer Programming,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2014}, {"title": "Static test case prioritization using topic models", "author": ["S.W. Thomas", "H. Hemmati", "A.E. Hassan", "D. Blostein"], "venue": "Empirical Software Engineering,", "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2014}, {"title": "Using latent dirichlet allocation for automatic categorization of software", "author": ["K. Tian", "M. Revelle", "D. Poshyvanyk"], "venue": "In Mining Software Repositories,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2009}, {"title": "A historical dataset of software engineering conferences", "author": ["B. Vasilescu", "A. Serebrenik", "T. Mens"], "venue": "In Proceedings of the 10th Working Conference on Mining Software Repositories,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2013}, {"title": "A comparative study of differential evolution, particle swarm optimization, and evolutionary algorithms on numerical benchmark problems", "author": ["J. Vesterstr\u00f8m", "R. Thomsen"], "venue": "In Evolutionary Computation,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2004}, {"title": "Searching for better configurations: a rigorous approach to clone evaluation", "author": ["T. Wang", "M. Harman", "Y. Jia", "J. Krinke"], "venue": "In Proceedings  of the 2013 9th Joint Meeting on Foundations of Software Engineering,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2013}, {"title": "Lda-based document models for ad-hoc retrieval", "author": ["X. Wei", "W.B. Croft"], "venue": "In Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2006}, {"title": "Improving the Usability of Topic Models", "author": ["Y. Yang"], "venue": "PhD thesis, NORTHWESTERN UNIVERSITY,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2015}, {"title": "A heuristic approach to determine an appropriate number of topics in topic modeling", "author": ["W. Zhao", "J.J. Chen", "R. Perkins", "Z. Liu", "W. Ge", "Y. Ding", "W. Zou"], "venue": "BMC bioinformatics,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2015}, {"title": "Active learning for multi-objective optimization", "author": ["M. Zuluaga", "G. Sergent", "A. Krause", "M. P\u00fcschel"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2013}], "referenceMentions": [{"referenceID": 42, "context": "As shown in Figure 1, most of the planet\u2019s 1600 Exabytes of data does not appear in structured sources (databases, etc) [45].", "startOffset": 120, "endOffset": 124}, {"referenceID": 5, "context": "One of the common techniques for finding related topics within unstructured text (an area called topic modeling) is Latent Dirichlet Allocation (LDA) [6].", "startOffset": 150, "endOffset": 153}, {"referenceID": 6, "context": "buggy or not [8]).", "startOffset": 13, "endOffset": 16}, {"referenceID": 42, "context": "From [45].", "startOffset": 5, "endOffset": 9}, {"referenceID": 53, "context": "To fix this problem, we proposes LDADE: a combination of LDA and a search-based optimizer (differential evolution, or DE) [57]) that automatically tunes LDA\u2019s < k, \u03b1, \u03b2 > parameters.", "startOffset": 122, "endOffset": 126}, {"referenceID": 51, "context": "[55] 2011 112 WCRE Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "[48] 2010 108 MSR Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2] 2014 96 ESE Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 49, "context": "[52] 2013 75 ICSE Y Y Y Uses GA to tune parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] 2013 61 ICSE Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] 2011 45 MSR Y Y N They validated the topic labelling techniques using multiple experiments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] 2014 44 RE Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 56, "context": "[60] 2011 44 ICSE Y Y N Open issue to choose optimal parameters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] 2012 35 MSR Y Y N Choosing the optimal number of topics is difficult.", "startOffset": 0, "endOffset": 3}, {"referenceID": 57, "context": "[61] 2014 35 SCP Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 58, "context": "[62] 2014 31 ESE Y Y N Choosing right set of parameters is a difficult task.", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] 2009 29 MSR Y Y N Explored Configurations without any explanation and accepted to the fact their results were better because of the corpus they used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "[33] 2013 27 ESEC/FSE Y Y Y Explored Configurations using LDA-GA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "[32] 2013 20 MSR Y Y N In Future, they planned to use LDA-GA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] 2014 15 WebSci Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] 2012 13 ICSM Y Y N Explored Configurations without any explanation (Just with no.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] 2013 13 SCP Y Y N Their work focused on optimizing LDA\u2019s topic count parameter.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] 2015 6 IST Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] 2016 5 CS Review Y Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "[31] 2014 5 ISSRE N Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 44, "context": "[47] 2015 3 JIS Y Y N They improvised LDA into ISLDA which gave stability across different runs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "[58] 2015 2 IST Y Y Y Explored Configurations using LDA-GA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "[8] 2016 0 JSS N Y N Explored Configurations without any explanation.", "startOffset": 0, "endOffset": 3}, {"referenceID": 34, "context": "In a companion paper submitted to ICSE\u201917 [37] we apply LDADE to 9291 SE papers to find trends in topics between 1993 to 2013.", "startOffset": 42, "endOffset": 46}, {"referenceID": 12, "context": "Previously [14], we showed that (a) tuning software defect predictors can lead to large improvements of the performance of those predictors; and that (b) those tunings are different in different data sets; so (c) it is important to apply automatic tuning as part of the analysis of any new data set.", "startOffset": 11, "endOffset": 15}, {"referenceID": 3, "context": "The impact of tuning are well understood in the theoretical machine learning literature [4].", "startOffset": 88, "endOffset": 91}, {"referenceID": 12, "context": "[14] surveyed hundreds of recent SE papers in the area of software defect prediction from static code attributes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 55, "context": "They found that most SE authors do not take steps to explore tunings (rare exception: [59]).", "startOffset": 86, "endOffset": 90}, {"referenceID": 9, "context": "[11] compared support vector machines to other data miners for the purposes of defect prediction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 45, "context": "Figure 3: Example (from [48]) of generating topics from Stackoverflow.", "startOffset": 24, "endOffset": 28}, {"referenceID": 16, "context": "Langley [18] defines such effects as follows:", "startOffset": 8, "endOffset": 12}, {"referenceID": 16, "context": "certain incremental clustering algorithms generate different clusters, depending on the order with which they explore the data [18].", "startOffset": 127, "endOffset": 131}, {"referenceID": 21, "context": "In the literature, researchers [23], [35], [36] manually accessed the topics and then used for further experiments.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "In the literature, researchers [23], [35], [36] manually accessed the topics and then used for further experiments.", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "In the literature, researchers [23], [35], [36] manually accessed the topics and then used for further experiments.", "startOffset": 43, "endOffset": 47}, {"referenceID": 26, "context": "Some made use of Amazon Mechanical Turk to create goldstandard coherence judgements [28].", "startOffset": 84, "endOffset": 88}, {"referenceID": 64, "context": "For example, SCLDA [68] can handle different kinds of knowledge such as word correlation, document correlation, document label and so on.", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "Some researchers [33], [52], [58] used Genetic Algorithm (GA) to tune the parameters\u2013 but for different purposes than LDADE.", "startOffset": 17, "endOffset": 21}, {"referenceID": 49, "context": "Some researchers [33], [52], [58] used Genetic Algorithm (GA) to tune the parameters\u2013 but for different purposes than LDADE.", "startOffset": 23, "endOffset": 27}, {"referenceID": 54, "context": "Some researchers [33], [52], [58] used Genetic Algorithm (GA) to tune the parameters\u2013 but for different purposes than LDADE.", "startOffset": 29, "endOffset": 33}, {"referenceID": 49, "context": "[52] used GAs to increase the precision of predictions made by a classifier; i.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "experiment with one parameter: cluster size) [16], [63] achieved higher stability by just increasing the number of cluster size.", "startOffset": 45, "endOffset": 49}, {"referenceID": 59, "context": "experiment with one parameter: cluster size) [16], [63] achieved higher stability by just increasing the number of cluster size.", "startOffset": 51, "endOffset": 55}, {"referenceID": 35, "context": "PITS is a text mining data set generated from NASA software project and issue tracking system (PITS) reports [38], [41].", "startOffset": 109, "endOffset": 113}, {"referenceID": 38, "context": "PITS is a text mining data set generated from NASA software project and issue tracking system (PITS) reports [38], [41].", "startOffset": 115, "endOffset": 119}, {"referenceID": 37, "context": "The dataset can be downloaded from the PROMISE repository [40].", "startOffset": 58, "endOffset": 62}, {"referenceID": 60, "context": "[64].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "For this study, all datasets were preprocessed using the usual text mining filters [13]:", "startOffset": 83, "endOffset": 87}, {"referenceID": 4, "context": "\u2022 Stop words removal using NLTK toolkit3 [5] : ignore very common short words such as \u201cand\u201d or \u201cthe\u201d.", "startOffset": 41, "endOffset": 44}, {"referenceID": 50, "context": "\u2022 PITS and Citemap is small enough to process on a single (four core) desktop machine using Scikit-Learn [53] and Python.", "startOffset": 105, "endOffset": 109}, {"referenceID": 26, "context": "To evaluate topics coherence in LDA, there is a direct approach, by asking people about topics, and an indirect approach by evaluating pointwise mutual information (PMI) [28], [50] between the topic words.", "startOffset": 170, "endOffset": 174}, {"referenceID": 47, "context": "To evaluate topics coherence in LDA, there is a direct approach, by asking people about topics, and an indirect approach by evaluating pointwise mutual information (PMI) [28], [50] between the topic words.", "startOffset": 176, "endOffset": 180}, {"referenceID": 24, "context": "The usual trend is that as the value of perplexity drops, the number of topics should grow [26].", "startOffset": 91, "endOffset": 95}, {"referenceID": 24, "context": "Researchers caution that the value of perplexity doesn\u2019t remain constant with different topic size and with dictionary sizes [26], [69].", "startOffset": 125, "endOffset": 129}, {"referenceID": 65, "context": "Researchers caution that the value of perplexity doesn\u2019t remain constant with different topic size and with dictionary sizes [26], [69].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "Another approach researchers have used is Jaccard Similarity [16], [50].", "startOffset": 61, "endOffset": 65}, {"referenceID": 47, "context": "Another approach researchers have used is Jaccard Similarity [16], [50].", "startOffset": 67, "endOffset": 71}, {"referenceID": 39, "context": "For that purpose, we will study the case of n \u2264 9 (we use 9 as our maximum size since the cognitive science literature tells us that 7\u00b12 is a useful upper size for artifacts to be browsed by humans [42]).", "startOffset": 198, "endOffset": 202}, {"referenceID": 10, "context": "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, \u03b1, \u03b2 (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score\u2190 \u2205 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data\u2190 shuffle(Data) 6: Topics.", "startOffset": 61, "endOffset": 65}, {"referenceID": 36, "context": "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, \u03b1, \u03b2 (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score\u2190 \u2205 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data\u2190 shuffle(Data) 6: Topics.", "startOffset": 67, "endOffset": 71}, {"referenceID": 18, "context": "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, \u03b1, \u03b2 (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score\u2190 \u2205 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data\u2190 shuffle(Data) 6: Topics.", "startOffset": 100, "endOffset": 104}, {"referenceID": 53, "context": "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, \u03b1, \u03b2 (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score\u2190 \u2205 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data\u2190 shuffle(Data) 6: Topics.", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, \u03b1, \u03b2 (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score\u2190 \u2205 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data\u2190 shuffle(Data) 6: Topics.", "startOffset": 202, "endOffset": 205}, {"referenceID": 17, "context": "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, \u03b1, \u03b2 (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score\u2190 \u2205 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data\u2190 shuffle(Data) 6: Topics.", "startOffset": 207, "endOffset": 211}, {"referenceID": 41, "context": "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, \u03b1, \u03b2 (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score\u2190 \u2205 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data\u2190 shuffle(Data) 6: Topics.", "startOffset": 213, "endOffset": 217}, {"referenceID": 43, "context": "The literature mentions many optimizers: simulated annealing [12], [39]; various genetic algorithms [20] augmented by techniques such as DE (differential evolution [57]), tabu search and scatter search [3], [19], [44], [46]; Algorithm 1 Pseudocode for untuned LDA with Default Parameters Input: Data, n, k, \u03b1, \u03b2 (Defaults) Output: Raw Score 1: function LDASCORE( n, Data) 2: Score\u2190 \u2205 3: for j = 0 to 10 do 4: for i = 0 to 10 do 5: data\u2190 shuffle(Data) 6: Topics.", "startOffset": 219, "endOffset": 223}, {"referenceID": 48, "context": "particle swarm optimization [51]; numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods [27], [70].", "startOffset": 28, "endOffset": 32}, {"referenceID": 25, "context": "particle swarm optimization [51]; numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods [27], [70].", "startOffset": 176, "endOffset": 180}, {"referenceID": 66, "context": "particle swarm optimization [51]; numerous decomposition approaches that use heuristics to decompose the total space into small problems, then apply a response surface methods [27], [70].", "startOffset": 182, "endOffset": 186}, {"referenceID": 63, "context": "Firstly, it has proved useful in prior SE tuning studies [67].", "startOffset": 57, "endOffset": 61}, {"referenceID": 40, "context": "\u2022 VEM is the deterministic variational EM method that computes \u03b1, \u03b2 via expectation maximization [43].", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "\u2022 Gibbs sampling [22], [67] is a Markov Chain Monte Carlo algorithm, which is an approximate stochastic process for computing and updating \u03b1, \u03b2.", "startOffset": 17, "endOffset": 21}, {"referenceID": 63, "context": "\u2022 Gibbs sampling [22], [67] is a Markov Chain Monte Carlo algorithm, which is an approximate stochastic process for computing and updating \u03b1, \u03b2.", "startOffset": 23, "endOffset": 27}, {"referenceID": 27, "context": "Topic modeling researchers in SE have argued that Gibbs leads to stabler models [29], [30] (a claim which we test, below).", "startOffset": 86, "endOffset": 90}, {"referenceID": 8, "context": "k 10 [10,100] Number of topics or cluster size", "startOffset": 5, "endOffset": 13}, {"referenceID": 0, "context": "\u03b1 None [0,1] Prior of document topic distribution.", "startOffset": 7, "endOffset": 12}, {"referenceID": 0, "context": "\u03b2 None [0,1] Prior of topic word distribution.", "startOffset": 7, "endOffset": 12}, {"referenceID": 0, "context": "add(Population[i],ldascore(Population[i],n,Data) 6: end for 7: for i = 0 to iter do 8: NewGeneration\u2190 \u2205 9: for j = 0 to np\u2212 1 do 10: Si \u2190Extrapolate(Population[j],Population,cr,f,np) 11: if ldascore(Si) \u2265 Cur Gen[j][1] then 12: NewGeneration.", "startOffset": 215, "endOffset": 218}, {"referenceID": 61, "context": "Hence, Vesterstrom and Thomsen [65] found DE to be competitive with particle swarm optimization and other GAs.", "startOffset": 31, "endOffset": 35}, {"referenceID": 8, "context": "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.", "startOffset": 4, "endOffset": 8}, {"referenceID": 12, "context": "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.", "startOffset": 10, "endOffset": 14}, {"referenceID": 30, "context": "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.", "startOffset": 16, "endOffset": 20}, {"referenceID": 46, "context": "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.", "startOffset": 22, "endOffset": 26}, {"referenceID": 49, "context": "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.", "startOffset": 28, "endOffset": 32}, {"referenceID": 54, "context": "see [10], [14], [33], [49], [52], [58] ) but this is the first time they have been applied to tune LDA to increase stability.", "startOffset": 34, "endOffset": 38}, {"referenceID": 31, "context": "This is a significant result since the standard advice in the LDA papers [34], [52] is to report the top 10 words per topic.", "startOffset": 73, "endOffset": 77}, {"referenceID": 49, "context": "This is a significant result since the standard advice in the LDA papers [34], [52] is to report the top 10 words per topic.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "[17] recommend using k = 67 topics yet in our data sets, best results were seen using k \u2264 24.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "The DE literature recommends using a population size np that is ten times larger than the number of parameters being optimized [57].", "startOffset": 127, "endOffset": 131}, {"referenceID": 62, "context": "once needed 15 years of CPU time to find and verify the tunings required for software clone detectors [66].", "startOffset": 102, "endOffset": 106}, {"referenceID": 52, "context": "routinely used 10 evaluations (or more) of their models in order to extract products from highly constrained product lines [56].", "startOffset": 123, "endOffset": 127}, {"referenceID": 6, "context": "Rather, they use LDA as part of some internal process where the topics are intermediaries used to generate some other goal [8].", "startOffset": 123, "endOffset": 126}, {"referenceID": 31, "context": "[34], LDA topics should not be reported as the top ten words.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "In other work, we have shown that tuning significantly helps defect prediction [14] (a result that has also been reported by other researchers [52]).", "startOffset": 79, "endOffset": 83}, {"referenceID": 49, "context": "In other work, we have shown that tuning significantly helps defect prediction [14] (a result that has also been reported by other researchers [52]).", "startOffset": 143, "endOffset": 147}], "year": 2016, "abstractText": "Topic Modeling finds human-readable structures in large sets of unstructured SE data. A widely used topic modeler is Latent Dirichlet Allocation. When run on SE data, LDA suffers from \u201corder effects\u201d i.e. different topics be generated if the training data was shuffled into a different order. Such order effects introduce a systematic error for any study that uses topics to make conclusions. This paper introduces LDADE, a Search-Based SE tool that tunes LDA\u2019s parameters using DE (Differential Evolution). LDADE has been tested on data from a programmer information exchange site (Stackoverflow), title and abstract text of thousands of SE papers, and software defect reports from NASA. Results were collected across different implementations of LDA (Python+Scikit-Learn, Scala+Spark); across different platforms (Linux, Macintosh) and for different kinds of LDAs (the traditional VEM method, or using Gibbs sampling). In all tests, the pattern was the same: LDADE\u2019s tunings dramatically reduces topic instability. The implications of this study for other software analytics tasks is now an open and pressing issue. In how many domains can search-based SE dramatically improve software analytics? Keywords\u2014Topic modeling, Stability, LDA, tuning, differential evolution.", "creator": "LaTeX with hyperref package"}}}