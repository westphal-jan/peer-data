{"id": "1608.08188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Aug-2016", "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "abstract": "Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd.", "histories": [["v1", "Mon, 29 Aug 2016 19:24:25 GMT  (5041kb,D)", "http://arxiv.org/abs/1608.08188v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CV cs.HC", "authors": ["danna gurari", "kristen grauman"], "accepted": false, "id": "1608.08188"}, "pdf": {"name": "1608.08188.pdf", "metadata": {"source": "CRF", "title": "Visual Question: Predicting If a Crowd Will Agree on the Answer", "authors": ["Danna Gurari", "Kristen Grauman"], "emails": [], "sections": [{"heading": null, "text": "This year, the time has come for us to be able to try to find a solution that is capable, that we are able to find a solution."}, {"heading": "II. RELATED WORK", "text": "This year it is so far that it will only take a few days until it will be so far again, until it is so far again."}, {"heading": "III. PAPER OVERVIEW", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "VI. CAPTURING ANSWER DIVERSITY WITH LESS EFFORT", "text": "The question we have to ask ourselves is whether we will be able to reduce the number of answers to the question we have asked ourselves. (...) The answer to the question we have asked ourselves is that we are getting a handle on the number of answers to the question we have asked ourselves. (...) The answer to the question we have asked ourselves is that the number of answers to the question we have asked ourselves is very high. (...) Our system automatically decides which visual questions we are asking in order to allocate the \"extra\" answers to the question \"around which we are focusing. (...) The answer to the question after the answer to the question we have asked ourselves. (...) The answer to the question we have asked ourselves is the answer to the question we have asked ourselves. (...)"}, {"heading": "VII. CONCLUSIONS", "text": "To motivate the practical implications of this problem, we analyzed nearly half a million visual questions and showed that there is almost a 50 / 50 split between visual questions that lead to agreement or disagreement. We observed that disagreement in the set arose from different types of answers (yes / no, counting, others) for many different reasons. Next, we proposed a system that automatically predicts whether a visual question leads to a single or multiple answers from a crowd. Our method surpasses a strong existing VQA system that is limited to estimating system uncertainty, not crowd disagreement. Finally, we demonstrated how we can use the prediction system to accelerate collection of different answers from a crowd by typically at least 20% from today's status quo of fixed redundancy allocation."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank the Office of Naval Research (ONR YIP N00014-12-1-0754) and the National Science Foundation (IIS-1065390) for their financial support. We thank Dinesh Jayaraman, Yu-Chuan Su, Suyog Jain and Chao-Yeh Chen for their support in the experiments."}], "references": [{"title": "Vizwiz: Nearly realtime answers to visual questions", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White", "T. Yeh"], "venue": "ACM symposium on User interface software and technology (UIST), 2010, pp. 333\u2013342.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), 2016, pp. 1545\u20141554.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2015, pp. 2425\u20132433.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2014, pp. 1682\u20131690.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual madlibs: Fill in the blank image generation and question answering", "author": ["L. Yu", "E. Park", "A.C. Berg", "T.L. Berg"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2015, pp. 2461\u20132469.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Crowdsourcing subjective fashion advice using VizWiz: Challenges and opportunities", "author": ["M.A. Burton", "E. Brady", "R. Brewer", "C. Neylan", "J.P. Bigham", "A. Hurst"], "venue": "ACM SIGACCESS conference on Computers and accessibility (ASSETS), 2012, pp. 135\u2013142.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "The multidimensional wisdom of crowds", "author": ["P. Welinder", "S. Branson", "S. Belongie", "P. Perona"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2010, pp. 2424\u20132432.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Image specificity", "author": ["M. Jas", "D. Parikh"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 2727\u2013 2736.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiview triplet embedding: Learning attributes in multiple maps", "author": ["E. Amid", "A. Ukkonen"], "venue": "International Conference on Machine Learning (ICML), 2015, pp. 1472\u20131480.  10", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "SQUARE: A benchmark for research on computing crowd consensus", "author": ["A. Sheshadri", "M. Lease"], "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Predicting sufficient annotation strength for interactive foreground segmentation", "author": ["S.D. Jain", "K. Grauman"], "venue": "IEEE International Conference on Computer Vision (ICCV), 2013, pp. 1313\u20131320.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Combining crowd and expert labels using decision theoretic active learning", "author": ["A.T. Nguyen", "B.C. Wallace", "M. Lease"], "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Pull the plug? predicting if computers or humans should segment images", "author": ["D. Gurari", "S.D. Jain", "M. Betke", "K. Grauman"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 382\u2013391.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Active learning literature survey", "author": ["B. Settles"], "venue": "University of Wisconsin, Madison, Tech. Rep., 2010.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Tropel: Crowdsourcing detectors with minimal training", "author": ["G. Patterson", "G.V. Horn", "S. Belongie", "P. Perona", "J. Hays"], "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Cost-sensitive active visual category learning", "author": ["S. Vijayanarasimhan", "K. Grauman"], "venue": "International Journal of Computer Vision (IJCV), vol. 91, no. 1, 2011, pp. 24\u201344.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "To re(label), or not to re(label)", "author": ["C.H. Lin", "Mausam", "D.S. Weld"], "venue": "AAAI Conference on Human Computation and Crowdsourcing (HCOMP), 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Get another label? Improving data quality and data mining using multiple, noisy labelers", "author": ["V.S. Sheng", "F. Provost", "P.G. Ipeirotis"], "venue": "International Conference on Knowledge Discovery and Data Mining (KDD), 2008, pp. 614\u2013622.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Answering visual questions with conversational crowd assistants", "author": ["W.S. Lasecki", "P. Thiha", "Y. Zhong", "E. Brady", "J.P. Bigham"], "venue": "ACM SIGACCESS Conference on Computers and Accessibility (ASSETS), 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft COCO: Common objects in context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollar", "C.L. Zitnick"], "venue": "IEEE European Conference on Computer Vision (ECCV), 2014, pp. 740\u2013 755.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "IEEE European Conference on Computer Vision (ECCV), 2015, pp. 1\u20139.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Salient object subitizing", "author": ["J. Zhang", "S. Ma", "M. Sameki", "S. Sclaroff", "M. Betke", "Z. Lin", "X. Shen", "B. Price", "R. Mech"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015, pp. 4045\u2013 4054.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2001}, {"title": "Deeper lstm and normalized cnn visual question answering model", "author": ["J. Lu", "X. Lin", "D. Batra", "D. Parikh"], "venue": "https://github.com/VT-vision-lab/ VQA LSTM CNN, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "What would be possible if a person had an oracle that could immediately provide the answer to any question about the visual world? Sight-impaired users could quickly and reliably figure out the denomination of their currency and so whether they spent the appropriate amount for a product [1].", "startOffset": 288, "endOffset": 291}, {"referenceID": 1, "context": "an image or video [2], [3], [4].", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "an image or video [2], [3], [4].", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "an image or video [2], [3], [4].", "startOffset": 28, "endOffset": 31}, {"referenceID": 2, "context": "Today\u2019s status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question [3], [1], [5].", "startOffset": 175, "endOffset": 178}, {"referenceID": 0, "context": "Today\u2019s status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question [3], [1], [5].", "startOffset": 180, "endOffset": 183}, {"referenceID": 4, "context": "Today\u2019s status quo is to assume a fixed number of human responses per visual question and so a fixed cost, delay, and potential diversity of answers for every visual question [3], [1], [5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 0, "context": "We show in our experiments that our system saves 19 40-hour work weeks and $1800 to answer 121,512 visual questions, compared to today\u2019s status quo approach [1].", "startOffset": 157, "endOffset": 160}, {"referenceID": 2, "context": "fields as diverse as computer vision [3], computational linguistics [2], and machine learning [4] rely on large datasets to improve their VQA algorithms.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "fields as diverse as computer vision [3], computational linguistics [2], and machine learning [4] rely on large datasets to improve their VQA algorithms.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "fields as diverse as computer vision [3], computational linguistics [2], and machine learning [4] rely on large datasets to improve their VQA algorithms.", "startOffset": 94, "endOffset": 97}, {"referenceID": 2, "context": "Current methods to create these datasets assume a fixed number of human answers per visual question [3], [5], thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "Current methods to create these datasets assume a fixed number of human answers per visual question [3], [5], thereby either compromising on quality by not collecting all plausible answers or cost by collecting additional answers when they are redundant.", "startOffset": 105, "endOffset": 108}, {"referenceID": 1, "context": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4].", "startOffset": 241, "endOffset": 244}, {"referenceID": 2, "context": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4].", "startOffset": 246, "endOffset": 249}, {"referenceID": 0, "context": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4].", "startOffset": 251, "endOffset": 254}, {"referenceID": 3, "context": "Visual Question Answering Services: Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images [2], [3], [1], [4].", "startOffset": 256, "endOffset": 259}, {"referenceID": 0, "context": "For example, crowd-powered systems aim to supply a prespecified, fixed number of answers per visual question [1] and automated systems return a single answer for every visual", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "question [2], [3], [4].", "startOffset": 9, "endOffset": 12}, {"referenceID": 2, "context": "question [2], [3], [4].", "startOffset": 14, "endOffset": 17}, {"referenceID": 3, "context": "question [2], [3], [4].", "startOffset": 19, "endOffset": 22}, {"referenceID": 2, "context": "advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer [3].", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "of workers [1], [6].", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "of workers [1], [6].", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Other systems ensure a fixed number of answers are collected per visual question [3], [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "Other systems ensure a fixed number of answers are collected per visual question [3], [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 6, "context": "For example, researchers have suggested ways to resolve crowd disagreement due to task difficulty [7] and ambiguity/specificity [8], [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "For example, researchers have suggested ways to resolve crowd disagreement due to task difficulty [7] and ambiguity/specificity [8], [9].", "startOffset": 128, "endOffset": 131}, {"referenceID": 8, "context": "For example, researchers have suggested ways to resolve crowd disagreement due to task difficulty [7] and ambiguity/specificity [8], [9].", "startOffset": 133, "endOffset": 136}, {"referenceID": 9, "context": "Some methods demonstrate which workers to trust most when aggregating multiple responses into a final, single response [10], [7].", "startOffset": 119, "endOffset": 123}, {"referenceID": 6, "context": "Some methods demonstrate which workers to trust most when aggregating multiple responses into a final, single response [10], [7].", "startOffset": 125, "endOffset": 128}, {"referenceID": 8, "context": "Other methods leverage context to automatically disambiguate which of multiple outcomes is the desired outcome [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 10, "context": "For example, one method distributes a budget between three different levels of human effort when deciding how to segment images [11].", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "Another method spends a budget between less costly crowd workers and more costly expert efforts to improve outcomes for biomedical citation screening [12].", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "Another method predicts when to employ algorithms versus crowd workers to segment images [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 13, "context": "Minimizing Human Labeling: Our aim to actively decide how to allocate human effort to improve results is also somewhat related to active learning [14].", "startOffset": 146, "endOffset": 150}, {"referenceID": 14, "context": "Some methods iteratively supplement a training dataset with the most informative images for training a classifier [15], [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "Some methods iteratively supplement a training dataset with the most informative images for training a classifier [15], [16].", "startOffset": 120, "endOffset": 124}, {"referenceID": 16, "context": "Other methods solicit redundant labels to prevent incorrect/noisy labels from teaching prediction models to make mistakes [17], [18].", "startOffset": 122, "endOffset": 126}, {"referenceID": 17, "context": "Other methods solicit redundant labels to prevent incorrect/noisy labels from teaching prediction models to make mistakes [17], [18].", "startOffset": 128, "endOffset": 132}, {"referenceID": 18, "context": "Continuous Dialogue with the Crowd: Two services - Be My Eyes [19] and Chorus:View [20] - offer users a continuous", "startOffset": 83, "endOffset": 87}, {"referenceID": 18, "context": "Our work offers an alternative by demonstrating how a crowdsourcing service might instead solicit multiple answers for a one time back-and-forth rather than enacting a more costly, continuous communication channel with a single voice, whether from a single person [19] or the consensus of a crowd [20].", "startOffset": 297, "endOffset": 301}, {"referenceID": 2, "context": "VQA Datasets: We conduct our analysis on a total of 459,861 visual questions and 4,598,610 answers coming from today\u2019s largest freely-available VQA benchmark [3].", "startOffset": 158, "endOffset": 161}, {"referenceID": 19, "context": ", 369,861) of VQAs are about real images that show 91 types of objects that would be \u201ceasily recognizable by a 4 year old\u201d in their natural context [21].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "images [3].", "startOffset": 7, "endOffset": 10}, {"referenceID": 2, "context": "Towards this aim, visual questions were collected by asking three Amazon Mechanical Turk (AMT) crowd workers to look at a given image and generate a text-based question about it that would \u201cstump a smart robot\u201d [3].", "startOffset": 211, "endOffset": 214}, {"referenceID": 2, "context": "image with associated question and asking him/her to respond with \u201ca brief phrase and not a complete sentence\u201d [3].", "startOffset": 111, "endOffset": 114}, {"referenceID": 2, "context": ", \u201ca\u201d, \u201can\u201d, \u201cthe\u201d), as was done in prior work [3].", "startOffset": 47, "endOffset": 50}, {"referenceID": 20, "context": ", m = 1 person) [22] as well as more conservative answer validation schemes (i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 2, "context": ", m = 3 people) [3].", "startOffset": 16, "endOffset": 19}, {"referenceID": 21, "context": "salient object subitizing [23] (SOS) method, which produces five probabilities that indicate whether an image contains 0, 1, 2, 3, or 4+ salient objects.", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "We leverage a random forest classification model [24] to predict an answer (dis)agreement label for a given visual question.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "Deep Learning System: We next adapt a VQA deep learning architecture [25] to learn the predictive combination of visual and textual features.", "startOffset": 69, "endOffset": 73}, {"referenceID": 24, "context": "fully connected layer of the Convolutional Neural Network (CNN), VGG16 [26].", "startOffset": 71, "endOffset": 75}, {"referenceID": 2, "context": "We capitalize on today\u2019s largest visual question answering dataset [3] to evaluate our prediction system, which includes 369,861 visual questions about real images.", "startOffset": 67, "endOffset": 70}, {"referenceID": 23, "context": "Therefore, we employ as a baseline a related VQA algorithm [25], [3]", "startOffset": 59, "endOffset": 63}, {"referenceID": 2, "context": "Therefore, we employ as a baseline a related VQA algorithm [25], [3]", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Both our proposed classification systems outperform the VQA Algorithm [3] baseline; e.", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature [3], [22].", "startOffset": 157, "endOffset": 160}, {"referenceID": 20, "context": "Our overall finding that most of the predictive power stems from language-based features parallels feature analysis findings in the automated VQA literature [3], [22].", "startOffset": 162, "endOffset": 166}, {"referenceID": 2, "context": "Today\u2019s status quo is to either uniformly collect N answers for every visual question [3] or collect multiple answers where the number is determined by external crowdsourcing conditions [1].", "startOffset": 86, "endOffset": 89}, {"referenceID": 0, "context": "Today\u2019s status quo is to either uniformly collect N answers for every visual question [3] or collect multiple answers where the number is determined by external crowdsourcing conditions [1].", "startOffset": 186, "endOffset": 189}, {"referenceID": 0, "context": "8) answers for each question\u201d [1].", "startOffset": 30, "endOffset": 33}, {"referenceID": 2, "context": "VQA Algorithm [3]: As in the previous section, we leverage the output", "startOffset": 14, "endOffset": 17}, {"referenceID": 23, "context": "confidence score from the publicly-shared model [25]", "startOffset": 48, "endOffset": 52}, {"referenceID": 0, "context": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].", "startOffset": 88, "endOffset": 91}, {"referenceID": 5, "context": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].", "startOffset": 140, "endOffset": 143}, {"referenceID": 4, "context": "This predictor illustrates the best a user can achieve today with crowd-powered systems [1], [6] or with current dataset collection methods [3], [5].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Figure 6b also illustrates the advantage of our system over a related VQA algorithm [3] for our novel application of costsensitive answer collection from a crowd.", "startOffset": 84, "endOffset": 87}], "year": 2016, "abstractText": "Visual question answering (VQA) systems are emerging from a desire to empower users to ask any natural language question about visual content and receive a valid answer in response. However, close examination of the VQA problem reveals an unavoidable, entangled problem that multiple humans may or may not always agree on a single answer to a visual question. We train a model to automatically predict from a visual question whether a crowd would agree on a single answer. We then propose how to exploit this system in a novel application to efficiently allocate human effort to collect answers to visual questions. Specifically, we propose a crowdsourcing system that automatically solicits fewer human responses when answer agreement is expected and more human responses when answer disagreement is expected. Our system improves upon existing crowdsourcing systems, typically eliminating at least 20% of human effort with no loss to the information collected from the crowd.", "creator": "LaTeX with hyperref package"}}}