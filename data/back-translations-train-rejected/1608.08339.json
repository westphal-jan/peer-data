{"id": "1608.08339", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2016", "title": "American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition and signer-independence", "abstract": "In this thesis, we study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL, and recognizing it is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work, we propose several types of recognition approaches, and explore the signer variation problem. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 8% letter error rates. The signer-independent setting is much more challenging, but with neural network adaptation we achieve up to 17% letter error rates.", "histories": [["v1", "Tue, 30 Aug 2016 06:12:22 GMT  (4909kb,D)", "http://arxiv.org/abs/1608.08339v1", "PhD Thesis"]], "COMMENTS": "PhD Thesis", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["taehwan kim"], "accepted": false, "id": "1608.08339"}, "pdf": {"name": "1608.08339.pdf", "metadata": {"source": "CRF", "title": "American Sign Language fingerspelling recognition from video: Methods for unrestricted recognition and signer-independence", "authors": ["Taehwan Kim", "Greg Shakhnarovich", "Yisong Yue", "Vassilis Athitsos", "Iain Matthews", "Hao Tang", "Sarah Taylor", "Raquel Urtasun", "Heejin Choi", "Andrew Cotter", "Jonathan Keane", "Jian Peng", "Karthik Sridharan", "Zhiyong Wang", "Payman Yadollahpour", "Jian Yao"], "emails": [], "sections": [{"heading": null, "text": "American Sign Language fingerororororingrecognition from video: Methods for unlimited recognition and signer-IndependencebyTaehwan KimA thesis submitted in partial fulfilling the requirements for the degree of Philosophy in Computer Scienceat the Toyota Technological Institute at ChicagoChicago, Illionois August 2016 Thesis Committee: Vassilis AthitsosKaren Livescu (Thesis Advisor) Greg ShakhnarovichYisong Yuear Xiv: 160 8.08 339v 1 [cs.C L] 30 Aug 201 62American Sign Language fingerororororororingrecognition from video: Methods for full recognition and sign independence ofTaehwan Kim"}, {"heading": "Abstract", "text": "In this paper, we investigate the problem of finger-spelled character recognition in American Sign Language (ASL). Fingerkeeping comprises a significant but relatively studied part of ASL, and its recognition is difficult for a number of reasons: it involves fast, small movements that are often strongly coarticulated; it exhibits significant discrepancies between signatories; and a lack of continuously collected fingerspelling data has been identified. In this paper, we propose several types of recognition approaches and investigate the problem of signature variation. Our most powerful models are segmental (semi-Markov) conditional random fields that use deep neural network-based features. In the signature-dependent setting, our recognition devices achieve letter error rates of up to 8%. The signature-independent setting is much more difficult, but with neural network adaptation we reach up to 17% letter error rate."}, {"heading": "Acknowledgments", "text": "First and foremost, I would like to thank my PhD student Karen Livescu for her insightful guidance, endless patience and kindness throughout my PhD thesis, which would not have been possible without her support and encouragement. I also thank my PhD students Vassilis Athitsos, Greg Shakhnarovich and Yisong Yue, who guided and shaped this work through their valuable feedback and help. Many others have contributed directly and indirectly to the work described in this work. I have been fortunate to work with and thank my co-authors: Iain Matthews, Hao Tang, Sarah Taylor, Raquel Urtasun and Weiran Wang. I am grateful to Adam Bohlander and Chrissy Novak for their kindness and help in all administrative matters. I thank David McAllester for his help and guidance of this great school. I thank my fellow students whose inspiration I have infinite."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 15", "text": "1.1 Challenges................................................. 16 Posts 1.2 Posts....................................... 18 Posts 1.3."}, {"heading": "2 Recognition methods 21", "text": "2.1 Recognition.......................................... 222.1.1 Tandem Model........................ 22 2.1.2 Segment Correction CRF.............................................................. 282.2 DNN Adaptation......................................"}, {"heading": "3 Experimental Results 31", "text": "3.1 Data and comments..............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Conclusion 47", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Examples of hypotheses produced by models 57", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B Example images of phonological feature values 59", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "C Analysis: decomposition of errors 61", "text": ""}, {"heading": "D Word lists recorded and used in experiments 63", "text": ""}, {"heading": "List of Tables", "text": "2.1. Definition and possible values for phonological characteristics. 3.2. Letter error rates (%) for different settings of the SCRF and DNN training for the signatory-adapted cases..........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "List of Figures", "text": "The answer to this question is: \"I have no idea.\" The answer is: \"I have no idea.\" The answer is: \"I have no idea.\" \"I have no idea.\" \"I have no idea.\" \"I have no idea.\" \"\" I have no idea. \"\" \"I have no idea.\". \"\" \"I have no idea.\". \"\" \"I have no idea.\" \"\". \"\". \"\" \"\" \"..\" \"\" \"..\" \"\" \"\" \"..\" \"\" \"\" \"..\" \"\" \"\" \"..\" \"\" \"\" \".\" \"\" \"\" \"..\" \"\" \".\" \"\" \"\" \"\" \"\".. \"\" \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\". \"\". \"\". \"\" \"\". \"\". \"\" \".\" \"\" \".\" \".\" \"\". \"\". \"\" \".\" \".\" \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\". \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\" \".\" \"\" \"\" \"\". \"\" \".\" \"\" \".\" \"\". \"\" \"\". \"\" \"\" \"\". \"\" \"\". \"\" \".\". \"\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\". \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\". \"\" \""}, {"heading": "Introduction", "text": "In the US, there are approximately 350,000 to 500,000 people working in primary language (ASL). Automatic sign recognition is a burgeoning technology that has the potential to improve the ability of the deaf and deaf to take full advantage of modern information technology. For example, online sign language blogs and news agencies are now almost completely unindexed and untraceable because they make little comment. While there has been extensive research into automatic speech recognition and analysis in recent decades, sign language blogs and news agencies have been much less developed."}, {"heading": "1.1 Challenges", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "1.2 Contributions", "text": "Most previous work on finger letters and hand shape has focused on limited conditions, such as careful articulation, isolated characters, limited (20-100 words) vocabulary [32, 55, 72]. In this project, we are looking at unrestricted finger letter strings. This is a more natural environment, as finger letters are commonly used for names and other \"new\" terms that do not appear in a closed vocabulary. The work presented here uses the largest video data we know to contain unrestricted, connected finger letters, consisting of four signatures, each marking 600 word marks for a total of 350k image expressions. We are developing discriminatory segment models that allow us to introduce more flexible features of finger letter segments. The use of such segment functions is useful for gesture modeling, where it is natural to look at the track of some measurements or statistics of an entire segment."}, {"heading": "1.3 Related work", "text": "In fact, it is such that most of them will be able to survive themselves without a process in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, and in which it comes to a process, in which it comes to a process, and in which it"}, {"heading": "Recognition methods", "text": "Our task is to use as input a video (a sequence of images) that corresponds to a finger-letter word, as in Figure 2-1, and predict the signed letters. This is a sequence prediction task analogous to the connected telephone or word recognition, but there are some interesting sign language properties of the data area. A striking aspect of finger-letter sequences, as in Figure 2-1, is the large amount of movement and the lack of a longer \"steady state\" for each letter. Typically, each letter is represented by a brief \"culmination of articulation\" of one or more frames, where the movement of the hand is minimal and the hand shape comes closest to the target hand shape of the letter. This peak is surrounded by a longer period of movement between the current letter and the previous / next letter."}, {"heading": "2.1 Recognizers", "text": "When developing recognition systems, we consider several considerations: firstly, although the data set is large by standards of research on sign language, it is still quite small compared to typical language datasets, which means that large models with many context-dependent units cannot be trained with our data (as our initial experiments confirmed), so we limit our attention here to \"mono-letter\" models, i.e. models where each unit is a context-independent letter. Secondly, we consider the use of articulatory (phonological and phonetic) characteristic units, as there is evidence from speech recognition research that they can be useful in low-data settings [54, 81, 13]. Secondly, we want our models to be able to capture rich sign-language-specific information, such as the dynamic aspects of finger letters discussed above; this suggests the segmental models that we are looking at below. Finally, we want our models to be able to easily adapt our array of differentiators to each other, so that our networks can be formed independently of each other."}, {"heading": "2.1.1 Tandem model", "text": "The first ones we are looking at are based on the popular tandem approach of speech recognition [27]. In tandem-based speech recognition, Neural Networks (NN) are trained to classify telephones, and their outputs (telephone posteriors) are post-processed and used as observation objects in a standard HMM-based recognition system. Post-processing can include the protocols of the posteriors (or simply the linear outputs of the NNN instead of the posteriors), analyzing principal components and / or appending acoustic features to the NN outputs."}, {"heading": "2.1.2 Rescoring segmental CRF", "text": "In fact, most of them are able to orient themselves in a different direction than in a different direction, namely the direction in which they are moving."}, {"heading": "Language model feature", "text": "The language model attribute is a smoothed Bigram probability of the pair of letters corresponding to an edge: (,,) = (,)."}, {"heading": "Baseline consistency feature", "text": "To take advantage of the presence of a high-quality baseline, we use a baseline introduced by [DN103]. This function is constructed using the 1-best output hypothesis from an HMM-based baseline detector. Characteristic value is 1 when a segment contains exactly one letter, and the label matches it:, \""}, {"heading": "Peak detection features", "text": "The finger spelling of a sequence of letters results in a corresponding sequence of \"peaks\" of articulation. Intuitively, these are frames in which the hand reaches the target hand shape for a particular letter. The peak frame and the frames surrounding it for each letter tend to be characterized by very little movement, since the transition to the current letter is finished while the transition to the next letter has not yet begun, whereas the transition frames between letter pins have more movement. To use this information and encourage each predicted letter segment to have a single peak, we define letter-specific \"peak recognition features\" as follows: We first calculate approximate derivatives of the visual descriptors consisting of the 2-standard for the difference between the descriptors in each pair of consecutive images, smoothed by an average over 5-frame window. We expect there to be a single local minimum in this approximate segment function (if there is only a peak)."}, {"heading": "2.1.3 First-pass segmental CRF", "text": "One of the disadvantages of a rescoring approach is that the quality of the final results depends on both the quality of the baseline grids and the fit between the segments of the baseline grids and those of the second pass model. Therefore, we are also considering a first-pass segment model that uses features similar to the rescoring model. In particular, we are using a first-pass SCRF inspired by the phonetic detector from Tang et al. [84] We are using the same feature functions as in [84], namely average DNN outputs across each segment, samples of DNN outputs within the segment, limits of DNN outputs in each segment, duration and bias, all lexifying."}, {"heading": "2.2 DNN adaptation", "text": "In fact, the fact is that most of them are able to move to a different world in which they are able than to another world in which they are able to integrate."}, {"heading": "Experimental Results", "text": "We report on experiments with the finger-letter data of four native ASL signers. First, we describe data and annotations, some of the front-end details of hand segmentation and feature extraction, followed by experiments with the frame-level DNN classifiers (paragraph 3.3) and letter sequence detectors (paragraph 3.4)."}, {"heading": "3.1 Data and annotation", "text": "The data was recorded at 60 frames per second in a studio environment (one for signatories 1 and 2, the other for signatories 3 and 4). For the complete lists, see Table D.1 and D.2. Each word was spelled twice, resulting in 600 instances of words signed by each signatory.The lists contained English and foreign words, including proper names and ordinary English nouns. The total number of images in the data of four signatories is 350k frames. The recording settings, including the differences in the environment and the placement of the camera in the recording sessions, are in Figure 3-1. The signatories were seated at a chair, and the screen showing the word to be signed was marked in front of them. Each word was repeated twice, and the signatories were asked to press the green button if they signed the word correctly, otherwise the green button for the next button was not pressed."}, {"heading": "Hand localization and segmentation", "text": "For each signatory, we have developed a model for hand recognition, similar to the one used in [45, 55]. Using manually annotated hand regions, called Polygonal Regions of Interest (ROI) in 30 frames, we match a mixture of gaussies to the color of the hand pixels in L * a * b color space. We have also used the same 30 frames to build a single gaussian model. For each pixel in the image, pixel values in or near marked hand pixels are marked. Then, we rename each pixel as a hand or background based on a quota ratio."}, {"heading": "Handshape descriptors", "text": "For most experiments, we use histograms of oriented gradients (HOG [18]) as a visual descriptor (feature vector) for a given hand region2. First, we shrink the narrow delimitation box of the hand region to a canonical size of 128 x 128 pixels and then calculate HOG characteristics on a spatial pyramid of regions, 4 x 4, 8 x 8 and 16 x 16 grids with eight orientation fields per grid cell, resulting in 2688-dimensional descriptors. Pixels outside the handmask are ignored in this calculation. For HMM-based detection grids, these descriptors were projected to a maximum of 200 main dimensions to accelerate the calculation; the exact dimensionality in each experiment was matched to a development set. For DNN frame classifiers, we found that the finer grids did not improve significantly with increasing complexity, so we use 128-dimensionals."}, {"heading": "3.2 An initial experiment: tandem models and shallow", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.3 DNN frame classification performance", "text": "This year, we will be able to find a solution, we will be able to find a solution, we will be able to find a solution, we will be able to find a solution, we will be able to find a solution, \"he said."}, {"heading": "3.4 Letter recognition experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.4.1 Signer-dependent recognition", "text": "Our first continuous letter recognition experiments are signature-dependent; that is, we train and test on the same signature, for each signature, we use a 10-fold setup: in each fold, 80% of the data is used as a training set, 10% as a tuning parameter development error, and the remaining 10% as a final test set. We fine-tune the parameters in each fold, but in order to compare them with adaptation experiments, we use 8 out of 10 folds to calculate the final test results and report the average letter error rate (LER) on these 8 folds. For language models, we train letter bigramas in different sizes that contain both English words and names. We use HTK [1] to work on the basis of letter error detection (LER) to improve language models."}, {"heading": "3.4.2 Signer-independent recognition", "text": "In the signatory-independent environment, we want to recognize a new signatory's fingerprint sequence because a model has only been trained on data from other signatories. For each of the four test signatories, we train the models of the remaining three signatories and report on the performance of each test signatory and the average across the four test signatories. For direct comparison with the signatory-dependent experiments, the performance of each test signatory is itself an average over the 8 test folds for that signatory. As shown in the first row of Table 3.1, the signatory-independent performance of the three types of detectors is quite poor, with the rescoring SCRF slightly outperforming the Tandem HMM and the First Pass SCRF. Perhaps the poor performance is to be expected with such a small number of training signatories."}, {"heading": "3.4.3 Signer-adapted recognition", "text": "The rest of Table 3.1 (second and third row) gives the related letter recognition performance achieved with the three types of models using DNNs that were fine-tuned using different types of adaptation data (Ground Truth, GT, vs. Forced-Aligned, FA).For all models, the models are not retrained with the customized DNNs, but the detector's hyperparameters are tuned to 10% of the test signer's data.The tuned models are evaluated on an invisible 10% of the test signer's remaining data.Finally, we repeat this for eight options of tuning and testing kits that cover the 80% of the test signer's data that we do not use for the adaptation, and report the mean letter error rate across the test set.As shown in Table 3.1, customization allowed performance to jump to 30.3% with the forced alignment labels and up to 17.3% of the probability of error rate."}, {"heading": "3.5 Extensions and analysis", "text": "Next, we will analyze our results and consider potential enhancements to improve models.3.5.1 Analysis: Could we do better if we train fully on fit data? In this section, we will consider alternatives to our chosen fit setting - customizing DNNs using sequence models (HMMs / SCRFs) that are trained only on sign-independent data. We will fix the model on a first-pass SCRF and the fit data on 20% of the test signer's data that are labeled with Ground Truth Peak. In this setting, we will consider two alternative ways to use the fit data: (1) use the fit data of the test signer to train both the DNNs and the sequence model from scratch, ignoring the signature-independent training sets; and (2) train the DNNs from scratch to use the fit data from scratch better than the fit data (SCF trains the DNF on the fine-independent training signer's results) using our Rims.1"}, {"heading": "3.5.2 Analysis: Letter vs. feature DNNs", "text": "Next, we compare the character-DNN classifiers and the phonological feature DNN classifiers in the context of the initial SCRF recognition. We also consider an alternative subletter feature set, in particular a set of phonetic features introduced by Keane [42], the character values of which are listed in Table 3.3. We use the first-pass SCRF with either letter-only classifiers, only phonetic feature classifiers, characters + phonological feature classifiers and characters + phonetic feature classifiers. We do not consider the case of phonological features alone because they are non-discriminatory for some characters. Figure 3-6 shows the character-recognition results for the sign-dependent and sign-customized settings. We note that the use of character classifiers alone exceeds the other options in the speaker-dependent setting and achieves a letter-recognition result for the sign-dependent and sign-customized settings."}, {"heading": "3.5.3 Analysis: DNNs vs. CNNs", "text": "We also conduct experiments with Convolutionary Neural Networks (CNNs) as frame classifiers. In all previous experiments, we have used HoG features as image descriptors. However, the use of raw pixel CNNs without handmade image descriptors has recently become popular and has shown improved performance for image recognition tasks (e.g. [48, 77]). To see the effectiveness of CNNs for our frame classifiers, we compare the performance of letters and phonological characteristics between DNNs and CNNs. We use a signal dependent setting and the same 8-fold setting. The inputs of CNNs are grayscale 64 x 64 x pixels. is the number of input images used in the input window as in our DNNs. For the structure of CNNs, we use 32 kernels of 3 x 3 filters with stripe de 1."}, {"heading": "3.5.4 Improving performance in the force-aligned adaptation case", "text": "Next, we will try to improve the performance of the adaptation in the absence of ground truth (manually commented) peak labels. If we only use the character sequence for the adaptation data, we will use the signature-independent Tandem Recognizer to obtain force-oriented FrameLabels. Then, we will adapt the DNNs using force-oriented adaptation data (fine-tune), as we did in the FA case before. Then, we will match the adaptation data of the test signer with the adapted Recognizer. Finally, we will readjust the DNNs to the reoriented data again. During this experiment, we will not change the recognition value, but only update the DNNs. We will use First-Pass SCRF with 42-letter classifiers for these experiments. With this iterative realignment approach, we can improve the detection error rate in the FA case by about 1.3%, as shown in Table 3.4."}, {"heading": "3.5.5 Improving performance with segmental cascades", "text": "Finally, we will consider whether we can improve the performance of our best models, the first-pass SCRFs, by rescording their results in a second run with more powerful features. We will follow the discriminatory Segment Cascades (DSC) approach of [84], which uses a simpler first-pass SCRF for generating grids, and a second SCRF with more mathematically sophisticated features for rescoring. For these experiments, we will start with the most successful first-pass SCRF in the above experiments, which uses the letter DNNs and is adjusted with 20% of the data of the test signatory. For the second run SCRF, we will use the first-pass score as a feature and add two more complex features: a segmental DNN that takes an entire hypothetical segment as input and produces posterior probabilities for all letter classes."}, {"heading": "3.5.6 Analysis: decomposition of errors", "text": "We calculate the number of substitution errors (S), deletion errors 43 (D), and insertion errors (I) to assign hypotheses that match the letter sequence of truth to models. Results are presented in Table 3.6, which shows that the first-pass SCRF model largely improves the deletion and substitution errors compared to other models. Figure 3-8 shows such an example; Tandem HMM has many deletions, and Rescoring SCRF has a substitution, which is corrected with First-Pass SCRF Model. For more details with additional analysis of signer-dependent and non-signer settings, see Appendix C.444546Chapter 4."}, {"heading": "Conclusion", "text": "This thesis addresses the problem of unrestricted character sequence recognition in ASL, where character sequences are not limited to a closed vocabulary. This problem arises due to the small amount of available training data and significant variation between signatures. Our main findings are: signature-independent fingerspelling recognition, where the test signer is not seen in training, is quite challenging, at least with the small data sets available to date, with letter error rates around 60%. On the other hand, the recognition of letter error rates below 10% is quite successful, and the customization allows us to bridge a large part of the gap between signing performance and signature performance."}, {"heading": "Appendix A", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Examples of hypotheses produced by", "text": "Figures A-1, A-2, 3-8 and A-3 illustrate the detection task and hypothesis of the models. In each of these figures we show the basic truth segments. The top images are displayed above each letter segment; the hand region segmentation masks are automatically determined using the probability model described in Section 3.1. We also show intermediate images that were achieved in the middle between the peaks and before the first and after the last peak. Among the basic truth segments are the segments achieved with the Tandem HMM, where SCRF was resorbed, and at the bottom the segments achieved with the First-Pass SCRF.5758."}, {"heading": "Appendix B", "text": "Sample Images of Phonological Characteristics5960"}, {"heading": "Appendix C", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Analysis: decomposition of errors", "text": "Once we have identified the hypothesis from the models, the optimal character string will match each of the recognized and recognized markup sequences using dynamic programming. With the optimal alignment, the number of replacement errors (S), deletion errors (D), and insertion errors (I) can be calculated that match the reference label sequence to the recognized hypothesis. Please note that the letter error rate is calculated as follows: letter error rate = 100% + + and we use the implementation of HTK [1] for this analysis. The results will be shown in Table C.3 with signature-dependent experiment, Table C.1 with signature-independent experiment, and Table C.2 with customization.With signature-independent setting (Table C.1), the 1st pass model appears to produce more deletions and less substitution and insertion. Note that the 1st pass model has lower error rates than the 3rd (1)."}, {"heading": "Appendix D", "text": "Word lists recorded and used in experiments636465"}], "references": [{"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "Proc. ICASSP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Hmm adaptation using vector taylor series for noisy speech recognition", "author": ["Alex Acero", "Li Deng", "Trausti T Kristjansson", "Jerry Zhang"], "venue": "In INTERSPEECH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "BoostMap: A method for efficient approximate similarity rankings", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "CVPR", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "and Quan Yuan", "author": ["V. Athitsos", "C. Neidle", "S. Sclaroff", "J. Nash", "A. Stefan", "A. Thangali", "H. Wang"], "venue": "Large lexicon project: American sign language video corpus and sign language indexing/retrieval algorithms. In Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "A theory of learning from different domains", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan"], "venue": "Machine learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira"], "venue": "In Proceedings of the 2006 conference on empirical methods in natural language processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "A linguistic feature vector for the visual interpretation of sign language", "author": ["R. Bowden", "D. Windridge", "T. Kadir", "A. Zisserman", "M. Brady"], "venue": "ECCV", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Native and foreign vocabulary in American Sign Language: A lexicon with multiple origins", "author": ["D. Brentari", "C. Padden"], "venue": "Foreign vocabulary in sign languages: A cross-linguistic investigation of word formation, pages 87\u2013119. Lawrence Erlbaum, Mahwah, NJ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "A Prosodic Model of Sign Language Phonology", "author": ["Diane Brentari"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1998}, {"title": "Upper body detection and tracking in extended signing sequences", "author": ["P. Buehler", "M. Everingham", "D.P. Huttenlocher", "A. Zisserman"], "venue": "International Journal of Computer Vision, 95(2):180\u2013197", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "An articulatory feature-based tandem approach and factored observation modeling", "author": ["Ozgur \u00c7etin", "Arthur Kantor", "Simon King", "Chris Bartels", "Mathew Magimai-doss", "Joe Frankel", "Karen Livescu"], "venue": "In ICASSP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "The Phonetics and Phonology of Handshape in American Sign Language", "author": ["D.A. Cheek"], "venue": "PhD thesis, University of Texas at Austin", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Sign language spotting based on semi-Markov conditional random field", "author": ["Seong-Sik Cho", "Hee-Deok Yang", "Seong-Whan Lee"], "venue": "In Workshop on the Applications of Computer Vision,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. Audio, Speech, and Language Processing", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "IEEE Transactions on,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "Proc. CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2005}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III"], "venue": "arXiv preprint arXiv:0907.1815,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Modelling and recognition of the linguistic components in American Sign Language", "author": ["L. Ding", "A.M. Mart\u00ednez"], "venue": "Image Vision Comput., 27(12)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2009}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "Proc. Interspeech", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Semisupervised domain adaptation with instance constraints", "author": ["Jeff Donahue", "Judy Hoffman", "Erik Rodner", "Kate Saenko", "Trevor Darrell"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "SignSpeak - understanding", "author": ["P. Dreuw", "J. Forster", "Y. Gweth", "D. Stein", "H. Ney", "G. Martinez", "J. Verges Llahi", "O. Crasborn", "E. Ormel", "W. Du", "T. Hoyoux", "J. Piater", "J.M. Moya Lazaro", "M. Wheatley"], "venue": "recognition, and translation of sign languages. In Proc. Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT)", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "M", "author": ["P. Dreuw", "D. Rybach", "T. Deselaers"], "venue": "Zahedi, , and H. Ney. Speech recognition techniques for a sign language recognition system. In Interspeech", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Speech recognition techniques for a sign language recognition system", "author": ["Philippe Dreuw", "David Rybach", "Thomas Deselaers", "Morteza Zahedi", "Hermann Ney"], "venue": "In Interspeech,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Activity recognition and abnormality detection with the switching hidden semi-Markov model", "author": ["Thi V Duong", "Hung H Bui", "Dinh Q Phung", "Svetha Venkatesh"], "venue": "In CVPR,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "Tandem acoustic modeling in large-vocabulary recognition", "author": ["Daniel P.W. Ellis", "Rita Singh", "Sunil Sivadas"], "venue": "In ICASSP,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "Transfer learning in sign language", "author": ["A. Farhadi", "D. Forsyth", "R. White"], "venue": "CVPR", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Exploiting depth discontinuities for vision-based fingerspelling recognition", "author": ["R.S. Feris", "M. Turk", "R. Raskar", "K. Tan", "G. Ohashi"], "venue": "IEEE Workshop on Real-Time Vision for Human-Computer Interaction", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving continuous sign language recognition: Speech recognition techniques and system design", "author": ["J. Forster", "O. Koller", "C. Oberd\u00f6rfer", "Y. Gweth", "H. Ney"], "venue": "Proc. SLPAT", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Maximum likelihood linear transformations for hmm-based speech recognition", "author": ["Mark JF Gales"], "venue": "Computer speech & language,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1998}, {"title": "Dynamic fingerspelling recognition using geometric and motion features", "author": ["P. Goh", "E.-J. Holden"], "venue": "ICIP", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2006}, {"title": "Domain adaptation for object recognition: An unsupervised approach", "author": ["Raghuraman Gopalan", "Ruonan Li", "Rama Chellappa"], "venue": "In 2011 international conference on computer vision,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "CSR-III text", "author": ["D. Graff", "R. Rosenfeld", "D. Paul"], "venue": "http://http://www.ldc. upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC95T6", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "In ICML,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Isolated sign language recognition using hidden Markov models", "author": ["K. Grobel", "M. Assan"], "venue": "International Conference on System Man and Cybernetics", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "Video-based recognition of fingerspelling in real-time", "author": ["K. Grobel", "H. Hienz"], "venue": "Workshops Bildverarbeitung f\u00fcr die Medizin", "citeRegEx": "37", "shortCiteRegEx": null, "year": 1996}, {"title": "Efficient segmental conditional random fields for phone recognition", "author": ["Yanzhang He", "Eric Fosler-Lussier"], "venue": "In Proceedings of the Annual Conference of the International Speech Communication Association (Interspeecha\u0302A\u0306Z\u030112),", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 1898}, {"title": "Learning American Sign Language (Levels I & II)", "author": ["T. Humphries", "C. Padden"], "venue": "Pearson Education, New York, second edition", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["Jing Jiang", "ChengXiang Zhai"], "venue": "In ACL,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "Machine recognition of Auslan signs using powergloves: towards large lexicon integration of sign language", "author": ["M.W. Kadous"], "venue": "Workshop on the Integration of Gesture in Language and Speech", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1996}, {"title": "Towards an articulatory model of handshape: What fingerspelling tells us about the phonetics and phonology of handshape in American Sign Language", "author": ["Jonathan Keane"], "venue": "PhD thesis, University of Chicago,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Coarticulation in ASL fingerspelling", "author": ["Jonathan Keane", "Diane Brentari", "Jason Riggle"], "venue": "NELS,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "Hand pose estimation and hand shape classification using multi-layered randomized decision forests", "author": ["Cem Keskin", "Furkan K\u0131ra\u00e7", "Yunus Emre Kara", "Lale Akarun"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2012}, {"title": "American Sign Language fingerspelling recognition with phonological feature-based tandem models", "author": ["T. Kim", "K. Livescu", "G. Shakhnarovich"], "venue": "IEEE Workshop on Spoken Language Technology", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2012}, {"title": "Fingerspelling recognition with semi-markov conditional random fields", "author": ["Taehwan Kim", "Gregory Shakhnarovich", "Karen Livescu"], "venue": "In IEEE International Conference on Computer Vision,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2013}, {"title": "Signer-independent fingerspelling recognition with deep neural network adaptation", "author": ["Taehwan Kim", "Weiran Wang", "Hao Tang", "Karen Livescu"], "venue": "In Proc. ICASSP,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2012}, {"title": "Speaker adaptation based on map estimation of hmm parameters", "author": ["C-H Lee", "J-L Gauvain"], "venue": "Acoustics, Speech, and Signal Processing, 1993. ICASSP-93., 1993 IEEE International Conference on, volume 2, pages 558\u2013561. IEEE", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1993}, {"title": "Maximum likelihood linear regression for speaker adaptation of continuous density hidden markov models", "author": ["Christopher J Leggetter", "Philip C Woodland"], "venue": "Computer Speech & Language,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 1995}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "Proc. Interspeech", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2010}, {"title": "Feature learning based on sae\u2013pca network for human gesture recognition", "author": ["Shao-Zi Li", "Bin Yu", "Wei Wu", "Song-Zhi Su", "Rong-Rong Ji"], "venue": "in rgbd images. Neurocomputing,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2015}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H. Liao"], "venue": "Proc. ICASSP", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2013}, {"title": "Articulatory feature-based methods for acoustic and audio-visual speech recognition: Summary from the 2006 jhu summer workshop", "author": ["Karen Livescu", "Omer Cetin", "Mark Hasegawa-Johnson", "Simon King", "Christopher Bartels", "Nash Borges", "Amir Kantor", "Pyare Lal", "Lisa Yung", "Ari Bezman"], "venue": "In Acoustics, Speech and Signal Processing,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2007}, {"title": "Automatic recognition of fingerspelled words in British Sign Language", "author": ["S. Liwicki", "M. Everingham"], "venue": "2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D.G. Lowe"], "venue": "International Journal of Computer Vision, 60(2):91\u2013110", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2004}, {"title": "Collecting a motion-capture corpus of American Sign Language for data-driven generation research", "author": ["P. Lu", "M. Huenerfauth"], "venue": "NAACL-HLT Workshop on Speech and Language Processing for Assistive Technologies", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "Cuny american sign language motion-capture corpus: first release", "author": ["Pengfei Lu", "Matt Huenerfauth"], "venue": "In Proceedings of the 5th Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon, The 8th International Conference on Language Resources and Evaluation (LREC", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "Purdue ASL database for the recognition of American sign language", "author": ["A.M. Martinez", "R.B. Wilbur", "R. Shay", "A.C. Kak"], "venue": "Proceedings of IEEE International Conference on Multimodal Interfaces (ICMI)", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2002}, {"title": "How many people use ASL in the united states? Why estimates need updating", "author": ["R.E. Mitchell", "T.A. Young", "B. Bachleda", "M.A. Karchmer"], "venue": "Sign Language Studies, 6(3)", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2006}, {"title": "Automated extraction of signs from continuous sign language sentences using iterated conditional modes", "author": ["S. Nayak", "S. Sarkar", "B. Loeding"], "venue": "CVPR", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2009}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "Proc. Eurospeech", "citeRegEx": "62", "shortCiteRegEx": null, "year": 1995}, {"title": "Efficient model-based 3D tracking of hand articulations using Kinect", "author": ["I. Oikonomidis", "N. Kyriazis", "A.A. Argyros"], "venue": "BMVC", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Recognition of finger spelling of American Sign Language with artificial neural network using position/orientation sensors and data glove", "author": ["C. Oz", "M.C. Leu"], "venue": "2nd international conference on Advances in Neural Networks", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2005}, {"title": "How the alphabet came to be used in a sign language", "author": ["Carol Padden", "Darline Clark Gunsauls"], "venue": "Sign Language Studies, page 4:10i\u0308\u00a3\u00a133,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2004}, {"title": "Tools for the analysis of benchmark speech recognition tests", "author": ["D.S. Pallet", "W.M. Fisher", "J.G. Fiscus"], "venue": "ICASSP", "citeRegEx": "66", "shortCiteRegEx": null, "year": 1990}, {"title": "Automatic and efficient long term arm and hand tracking for continuous sign language TV broadcasts", "author": ["T. Pfister", "J. Charles", "M. Everingham", "A. Zisserman"], "venue": "BMVC", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2012}, {"title": "Advances in phoneticsbased sub-unit modeling for transcription alignment and sign language recognition", "author": ["V. Pitsikalis", "S. Theodorakis", "C. Vogler", "P. Maragos"], "venue": "IEEE CVPR Workshop on Gesture Recognition", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz", "Jan Silovsky", "Georg Stemmer", "Karel Vesely"], "venue": "In IEEE 2011 Workshop on Automatic Speech Recognition and Understanding. IEEE Signal Processing Society,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2011}, {"title": "Spelling it out: Real-time asl fingerspelling recognition", "author": ["Nicolas Pugeault", "Richard Bowden"], "venue": "In Computer Vision Workshops (ICCV Workshops),", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2011}, {"title": "Fingerspelling recognition through classification of letterto-letter transitions", "author": ["S. Ricco", "C. Tomasi"], "venue": "ACCV", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2009}, {"title": "Affine-invariant modeling of shape-appearance images applied on sign language handshape classification", "author": ["A. Roussos", "S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "ICIP", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2010}, {"title": "Adapting visual category models to new domains", "author": ["Kate Saenko", "Brian Kulis", "Mario Fritz", "Trevor Darrell"], "venue": "In European conference on computer vision,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2010}, {"title": "Semi-Markov conditional random fields for information extraction", "author": ["Sunita Sarawagi", "William W. Cohen"], "venue": "In NIPS,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2004}, {"title": "Human action segmentation and recognition using discriminative semi-Markov models", "author": ["Qinfeng Shi", "Li Cheng", "Li Wang", "Alex Smola"], "venue": "International Journal of Computer Vision,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2011}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15:1929\u20131958", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time American Sign Language recognition using desk and wearable computer based video", "author": ["T. Starner", "J. Weaver", "A. Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(12)", "citeRegEx": "79", "shortCiteRegEx": null, "year": 1998}, {"title": "SRILM at sixteen: update and outlook", "author": ["A. Stolcke", "J. Zheng", "W. Wang", "V. Abrash"], "venue": "ASRU", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrating multilingual articulatory features into speech recognition", "author": ["Sebastian St\u00fcker", "Florian Metze", "Tanja Schultz", "Alex Waibel"], "venue": "In INTERSPEECH,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2003}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "Proc. SLT", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time articulated hand pose estimation using semi-supervised transductive regression forests", "author": ["Danhang Tang", "Tsz-Ho Yu", "Tae-Kyun Kim"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2013}, {"title": "Discriminative segmental cascades for feature-rich phone recognition", "author": ["H. Tang", "W. Wang", "K. Gimpel", "K. Livescu"], "venue": "Proc. ASRU", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of training approaches for discriminative segmental models", "author": ["Hao Tang", "Kevin Gimpel", "Karen Livescu"], "venue": "In INTERSPEECH,", "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2014}, {"title": "Exploiting phonological constraints for handshape inference in ASL video", "author": ["A. Thangali", "J.P. Nash", "S. Sclaroff", "C. Neidle"], "venue": "CVPR", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2011}, {"title": "Model-level data-driven sub-units for signs in videos of continuous sign language", "author": ["S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "ICASSP", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2010}, {"title": "Interarticulator co-ordination in deaf signers with Parkinson\u2019s disease", "author": ["M. E Tyrone", "J. Kegl", "H. Poizner"], "venue": "Neuropsychologia, 37(11)", "citeRegEx": "89", "shortCiteRegEx": null, "year": 1999}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["Paul Viola", "Michael J. Jones"], "venue": "In CVPR,", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2001}, {"title": "Parallel hidden Markov models for American Sign Language recognition", "author": ["C. Vogler", "D. Metaxas"], "venue": "ICCV", "citeRegEx": "91", "shortCiteRegEx": null, "year": 1999}, {"title": "Toward scalability in ASL recognition: Breaking down signs into phonemes", "author": ["C. Vogler", "D. Metaxas"], "venue": "Gesture Workshop", "citeRegEx": "92", "shortCiteRegEx": null, "year": 1999}, {"title": "A framework for recognizing the simultaneous aspects of American Sign Language", "author": ["C. Vogler", "D. Metaxas"], "venue": "Computer Vision and Image Understanding, 81:358\u2013384", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2001}, {"title": "Handshapes and movements: Multiple-channel ASL recognition", "author": ["C. Vogler", "D. Metaxas"], "venue": "Gesture Workshop", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2003}, {"title": "Real-time hand-tracking with a color glove", "author": ["R. Wang", "J. Popovic"], "venue": "SIGGRAPH", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2009}, {"title": "The Phonetics of Fingerspelling", "author": ["S. Wilcox"], "venue": "John Benjamins Publishing Company", "citeRegEx": "96", "shortCiteRegEx": null, "year": 1992}, {"title": "Detecting coarticulation in sign language using conditional random fields", "author": ["R. Yang", "S. Sarkar"], "venue": "ICPR", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptation of contextdependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "Proc. SLT", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2012}, {"title": "Geometric features for improving continuous appearance-based sign language recognition", "author": ["M. Zahedi", "P. Dreuw", "D. Rybach", "T. Deselaers", "H. Ney"], "venue": "BMVC", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2006}, {"title": "Sign language recognition using a combination of new vision based features", "author": ["M.M. Zaki", "S.I. Shaheen"], "venue": "Pattern Recognition Letters, 32(4)", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2011}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "Proc. ICASSP", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2013}, {"title": "Histogram of 3d facets: a depth descriptor for human action and hand gesture recognition", "author": ["Chenyang Zhang", "Yingli Tian"], "venue": "Computer Vision and Image Understanding,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2015}, {"title": "Segmental CRF approach to large vocabulary continuous speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "ASRU", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification and recognition with direct segment models", "author": ["Geoffrey Zweig"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2012}], "referenceMentions": [{"referenceID": 56, "context": "In the US, there are about 350,000\u2013500,000 people for whom American Sign Language (ASL) is the primary language [60].", "startOffset": 112, "endOffset": 116}, {"referenceID": 21, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 94, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 6, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 51, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 82, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 86, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 41, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 42, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 26, "context": "Research on this problem has included both speech-inspired approaches and computer vision-based techniques, using either/both video and depth sensor input [25, 100, 9, 55, 88, 92, 45, 46, 30].", "startOffset": 155, "endOffset": 191}, {"referenceID": 7, "context": "In fact, the fingerspelling handshapes account for about 72% of ASL handshapes [10], making research on fingerspelling applicable to ASL in general.", "startOffset": 79, "endOffset": 83}, {"referenceID": 61, "context": "Fingerspelling is a constrained but important part of ASL, accounting for up to 35% of ASL [65].", "startOffset": 91, "endOffset": 95}, {"referenceID": 41, "context": "ASL fingerspelling uses a single hand and involves relatively small and quick motions of the hand and fingers, as opposed to the typically larger arm motions involved in other 1This thesis includes material previously published in several papers [45, 46, 47].", "startOffset": 246, "endOffset": 258}, {"referenceID": 42, "context": "ASL fingerspelling uses a single hand and involves relatively small and quick motions of the hand and fingers, as opposed to the typically larger arm motions involved in other 1This thesis includes material previously published in several papers [45, 46, 47].", "startOffset": 246, "endOffset": 258}, {"referenceID": 43, "context": "ASL fingerspelling uses a single hand and involves relatively small and quick motions of the hand and fingers, as opposed to the typically larger arm motions involved in other 1This thesis includes material previously published in several papers [45, 46, 47].", "startOffset": 246, "endOffset": 258}, {"referenceID": 35, "context": "Reproduced from [39].", "startOffset": 16, "endOffset": 20}, {"referenceID": 8, "context": "Sign language handshape has its own phonology that has been studied but does not yet enjoy a broadly agreed-upon understanding [11].", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "Recent linguistic work on sign language phonology has developed approaches based on articulatory features, which are closely related to motions of parts of the hand [20].", "startOffset": 165, "endOffset": 169}, {"referenceID": 20, "context": "Sign language recognition research has focused more on the larger motions of sign and on interactions between multiple body parts, and less so on the details of handshape [24, 97].", "startOffset": 171, "endOffset": 179}, {"referenceID": 91, "context": "Sign language recognition research has focused more on the larger motions of sign and on interactions between multiple body parts, and less so on the details of handshape [24, 97].", "startOffset": 171, "endOffset": 179}, {"referenceID": 78, "context": "At the same time, computer vision research has studied pose estimation and tracking of hands [83], but usually not in the context of a grammar that constrains the motion.", "startOffset": 93, "endOffset": 97}, {"referenceID": 8, "context": "Therefore, we propose to use linguistic features suggested by linguistics research on ASL [11, 42].", "startOffset": 90, "endOffset": 98}, {"referenceID": 38, "context": "Therefore, we propose to use linguistic features suggested by linguistics research on ASL [11, 42].", "startOffset": 90, "endOffset": 98}, {"referenceID": 28, "context": "Most previous work on fingerspelling and handshape has focused on restricted conditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies [32, 55, 72].", "startOffset": 172, "endOffset": 184}, {"referenceID": 51, "context": "Most previous work on fingerspelling and handshape has focused on restricted conditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies [32, 55, 72].", "startOffset": 172, "endOffset": 184}, {"referenceID": 67, "context": "Most previous work on fingerspelling and handshape has focused on restricted conditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies [32, 55, 72].", "startOffset": 172, "endOffset": 184}, {"referenceID": 51, "context": "Signer-dependent applications [55, 46] have achieved letter error rates (Levenshtein distances between hypothesized and true letter sequences, as a proportion of the number of true letters) of 10% or less.", "startOffset": 30, "endOffset": 38}, {"referenceID": 42, "context": "Signer-dependent applications [55, 46] have achieved letter error rates (Levenshtein distances between hypothesized and true letter sequences, as a proportion of the number of true letters) of 10% or less.", "startOffset": 30, "endOffset": 38}, {"referenceID": 37, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 33, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 60, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 89, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 25, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 66, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 40, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 96, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 48, "context": "Even though some prior work has used additional input modalities, such as specialized gloves and depth sensors [41, 37, 64, 95, 29, 70, 44, 102, 52], video is more practical in many settings, and for online or archival recordings is the only choice; we restrict the remaining discussion to video.", "startOffset": 111, "endOffset": 148}, {"referenceID": 74, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 85, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 32, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 20, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [79, 91, 36, 24], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 91, "context": "In addition, there have been some efforts involving conditional models [97] and more complex (non-linear-chain) graphical models [86, 94].", "startOffset": 71, "endOffset": 75}, {"referenceID": 81, "context": "In addition, there have been some efforts involving conditional models [97] and more complex (non-linear-chain) graphical models [86, 94].", "startOffset": 129, "endOffset": 137}, {"referenceID": 88, "context": "In addition, there have been some efforts involving conditional models [97] and more complex (non-linear-chain) graphical models [86, 94].", "startOffset": 129, "endOffset": 137}, {"referenceID": 55, "context": "Video corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6].", "startOffset": 92, "endOffset": 107}, {"referenceID": 20, "context": "Video corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6].", "startOffset": 92, "endOffset": 107}, {"referenceID": 19, "context": "Video corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6].", "startOffset": 92, "endOffset": 107}, {"referenceID": 3, "context": "Video corpora have been constructed for the task of recognition of sign language from video [59, 24, 23, 6].", "startOffset": 92, "endOffset": 107}, {"referenceID": 83, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 23, "endOffset": 35}, {"referenceID": 90, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 23, "endOffset": 35}, {"referenceID": 11, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 23, "endOffset": 35}, {"referenceID": 59, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 77, "endOffset": 85}, {"referenceID": 66, "context": "Motion capture systems [89, 96, 14] and, more recently, Microsoft\u2019s KinectTM [63, 70] have also been used to collect sign language data.", "startOffset": 77, "endOffset": 85}, {"referenceID": 53, "context": "One exception is [57], which includes motion-capture glove data; however, this database is aimed at learning models for ASL generation and does not include the type of naturalistic, coarticulated data that we would like to study.", "startOffset": 17, "endOffset": 21}, {"referenceID": 54, "context": "Also [58] collected motion capture ASL data consisting of both sign and fingerspelling, but small scale for fingerspelling.", "startOffset": 5, "endOffset": 9}, {"referenceID": 6, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 135, "endOffset": 151}, {"referenceID": 91, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 135, "endOffset": 151}, {"referenceID": 93, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 135, "endOffset": 151}, {"referenceID": 94, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 135, "endOffset": 151}, {"referenceID": 24, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 200, "endOffset": 216}, {"referenceID": 20, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 200, "endOffset": 216}, {"referenceID": 57, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 200, "endOffset": 216}, {"referenceID": 16, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 200, "endOffset": 216}, {"referenceID": 9, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 234, "endOffset": 242}, {"referenceID": 63, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [9, 97, 99, 100], sometimes combined with appearance descriptors [28, 24, 61, 20] and color models [12, 67].", "startOffset": 234, "endOffset": 242}, {"referenceID": 14, "context": "In this work, we are aiming at relatively small motions that are difficult to track a priori, and therefore begin with general image appearance features based on histograms of oriented gradients (HOG features) [18].", "startOffset": 210, "endOffset": 214}, {"referenceID": 6, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 16, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 81, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 88, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 87, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 86, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 85, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 64, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 82, "context": ", [9, 20, 86, 94, 93, 92, 91, 68, 88].", "startOffset": 2, "endOffset": 37}, {"referenceID": 2, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 108, "endOffset": 119}, {"referenceID": 68, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 108, "endOffset": 119}, {"referenceID": 66, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 108, "endOffset": 119}, {"referenceID": 28, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 160, "endOffset": 172}, {"referenceID": 51, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 160, "endOffset": 172}, {"referenceID": 67, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [5, 73, 70] and fingerspelling sequence recognition [32, 55, 72].", "startOffset": 160, "endOffset": 172}, {"referenceID": 97, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 80, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 79, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 98, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 34, "context": "For speech recognition, segmental conditional random fields (SCRFs) and their variants have been applied fairly widely [103, 85, 84, 105, 104, 38].", "startOffset": 119, "endOffset": 146}, {"referenceID": 70, "context": "In natural language processing, semi-Markov CRFs have been used for named entity recognition [75], where the labeling is binary.", "startOffset": 93, "endOffset": 97}, {"referenceID": 71, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [76, 26] with a small set of possible activities to choose from, including work on spotting of specific (non-fingerspelled) signs in sign language video [15].", "startOffset": 120, "endOffset": 128}, {"referenceID": 22, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [76, 26] with a small set of possible activities to choose from, including work on spotting of specific (non-fingerspelled) signs in sign language video [15].", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [76, 26] with a small set of possible activities to choose from, including work on spotting of specific (non-fingerspelled) signs in sign language video [15].", "startOffset": 273, "endOffset": 277}, {"referenceID": 97, "context": "In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [103, 105].", "startOffset": 245, "endOffset": 255}, {"referenceID": 79, "context": "We compare this approach to an efficient first-pass segmental model [84].", "startOffset": 68, "endOffset": 72}, {"referenceID": 4, "context": "For the challenging signer-independent setting in which the signers in the training and test data are different, we may consider domain adaptation [7].", "startOffset": 147, "endOffset": 150}, {"referenceID": 15, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 49, "endOffset": 60}, {"referenceID": 5, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 49, "endOffset": 60}, {"referenceID": 36, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 49, "endOffset": 60}, {"referenceID": 69, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 78, "endOffset": 90}, {"referenceID": 18, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 78, "endOffset": 90}, {"referenceID": 29, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 78, "endOffset": 90}, {"referenceID": 1, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 114, "endOffset": 125}, {"referenceID": 27, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 114, "endOffset": 125}, {"referenceID": 46, "context": "Applications include natural language processing [19, 8, 40], computer vision [74, 22, 33] and speech recognition [4, 31, 50].", "startOffset": 114, "endOffset": 125}, {"referenceID": 46, "context": "The classic techniques include maximum likelihood linear regression [50] and maximum a posteriori [49].", "startOffset": 68, "endOffset": 72}, {"referenceID": 45, "context": "The classic techniques include maximum likelihood linear regression [50] and maximum a posteriori [49].", "startOffset": 98, "endOffset": 102}, {"referenceID": 49, "context": "In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21].", "startOffset": 176, "endOffset": 191}, {"referenceID": 0, "context": "In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21].", "startOffset": 176, "endOffset": 191}, {"referenceID": 77, "context": "In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21].", "startOffset": 176, "endOffset": 191}, {"referenceID": 17, "context": "In recent years, however, speech recognition systems are typically based on deep neural networks (DNNs), and speaker adaptation approaches specific to DNNs have been developed [53, 3, 82, 21].", "startOffset": 176, "endOffset": 191}, {"referenceID": 50, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [54, 81, 13].", "startOffset": 183, "endOffset": 195}, {"referenceID": 76, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [54, 81, 13].", "startOffset": 183, "endOffset": 195}, {"referenceID": 10, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [54, 81, 13].", "startOffset": 183, "endOffset": 195}, {"referenceID": 23, "context": "1 Tandem model The first recognizer we consider is based on the popular tandem approach to speech recognition [27].", "startOffset": 110, "endOffset": 114}, {"referenceID": 8, "context": "1: Definition and possible values for phonological features based on [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 8, "context": "For detailed descriptions, see [11].", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "[13], who used articulatory feature NN classifiers rather than phone classifiers.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "We use a phonological feature set developed by Brentari [11], who proposed seven features for ASL handshape.", "startOffset": 56, "endOffset": 60}, {"referenceID": 30, "context": "For this work, the language model is trained using ARPA CSR-III text, which includes English words and names [34].", "startOffset": 109, "endOffset": 113}, {"referenceID": 70, "context": "SCRFs [75, 103] are conditional loglinear models with feature functions that can be based on variable-length segments of input frames, allowing for great flexibility in defining feature functions.", "startOffset": 6, "endOffset": 15}, {"referenceID": 97, "context": "SCRFs [75, 103] are conditional loglinear models with feature functions that can be based on variable-length segments of input frames, allowing for great flexibility in defining feature functions.", "startOffset": 6, "endOffset": 15}, {"referenceID": 70, "context": "Semi-Markov CRFs [75], also referred to as segmental CRFs [103] or SCRFs, provide this ability.", "startOffset": 17, "endOffset": 21}, {"referenceID": 97, "context": "Semi-Markov CRFs [75], also referred to as segmental CRFs [103] or SCRFs, provide this ability.", "startOffset": 58, "endOffset": 63}, {"referenceID": 97, "context": "Baseline consistency feature To take advantage of the existence of a high-quality baseline, we use a baseline feature like the one introduced by [103].", "startOffset": 145, "endOffset": 150}, {"referenceID": 8, "context": "1, we use the linguistic handshape feature set developed by Brentari [11], who proposed seven features to describe handshape in ASL.", "startOffset": 69, "endOffset": 73}, {"referenceID": 79, "context": "[84].", "startOffset": 0, "endOffset": 4}, {"referenceID": 79, "context": "We use the same feature functions as in [84], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias, all lexicalized.", "startOffset": 40, "endOffset": 44}, {"referenceID": 49, "context": ", [53, 3, 82, 21]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 0, "context": ", [53, 3, 82, 21]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 77, "context": ", [53, 3, 82, 21]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 17, "context": ", [53, 3, 82, 21]).", "startOffset": 2, "endOffset": 17}, {"referenceID": 58, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [62, 98, 51].", "startOffset": 96, "endOffset": 108}, {"referenceID": 92, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [62, 98, 51].", "startOffset": 96, "endOffset": 108}, {"referenceID": 47, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [62, 98, 51].", "startOffset": 96, "endOffset": 108}, {"referenceID": 39, "context": "For more details about data and annotation steps, please refer to [43].", "startOffset": 66, "endOffset": 70}, {"referenceID": 42, "context": "1This section includes material previously published in [46].", "startOffset": 56, "endOffset": 60}, {"referenceID": 41, "context": "Hand localization and segmentation For every signer, we trained a model for hand detection similar to that used in [45, 55].", "startOffset": 115, "endOffset": 123}, {"referenceID": 51, "context": "Hand localization and segmentation For every signer, we trained a model for hand detection similar to that used in [45, 55].", "startOffset": 115, "endOffset": 123}, {"referenceID": 84, "context": "We suppress pixels that fall within regions detected as faces by the ViolaJones face detector [90], since these tend to be false positives.", "startOffset": 94, "endOffset": 98}, {"referenceID": 14, "context": "Handshape descriptors For most experiments, we use histograms of oriented gradients (HOG [18]) as the visual descriptor (feature vector) for a given hand region2.", "startOffset": 89, "endOffset": 93}, {"referenceID": 52, "context": "SIFT features is a commonly used type of image appearance features, and we extract SIFT features from each image, based on local histograms of oriented image gradients [56].", "startOffset": 168, "endOffset": 172}, {"referenceID": 41, "context": "3This section includes material previously published in [45].", "startOffset": 56, "endOffset": 60}, {"referenceID": 75, "context": "We implement the HMMs with HTK [1] and language models with SRILM [80].", "startOffset": 66, "endOffset": 70}, {"referenceID": 30, "context": "We train smoothed backoff bigram letter language models using lexicons of various sizes, consisting of the most frequent words in the ARPA CSR-III text, which includes English words and names [34].", "startOffset": 192, "endOffset": 196}, {"referenceID": 95, "context": "The DNNs have three hidden layers, each with 3000 ReLUs [101].", "startOffset": 56, "endOffset": 61}, {"referenceID": 73, "context": "Network learning is done with cross-entropy training with a weight decay penalty of 10\u22125, via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [78] at a rate of 0.", "startOffset": 190, "endOffset": 194}, {"referenceID": 43, "context": "4This section includes material previously published in [47].", "startOffset": 56, "endOffset": 60}, {"referenceID": 62, "context": "An asterisk (\u2018*\u2019) indicates statistically significant improvement over the corresponding baseline (\u2018BL\u2019) using the same training labels for the HMMs, according to a MAPSSWE test [66] at p < 0.", "startOffset": 178, "endOffset": 182}, {"referenceID": 75, "context": "We use HTK [1] to implement the tandem HMM-based recognizers and SRILM [80] to train the language models.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": ", [17]), we also conduct an initial experiment with them.", "startOffset": 2, "endOffset": 6}, {"referenceID": 65, "context": "We use Kaldi toolkit [69] for implementation.", "startOffset": 21, "endOffset": 25}, {"referenceID": 38, "context": "We also consider an alternative subletter feature set, in particular a set of phonetic features introduced by Keane [42], whose feature values are listed in Table 3.", "startOffset": 116, "endOffset": 120}, {"referenceID": 44, "context": ", [48, 77]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 72, "context": ", [48, 77]).", "startOffset": 2, "endOffset": 10}, {"referenceID": 38, "context": "3: Phonetic features [42].", "startOffset": 21, "endOffset": 25}, {"referenceID": 79, "context": "We follow the discriminative segmental cascades (DSC) approach of [84], where a simpler first-pass SCRF is used for lattice generation and a second SCRF, with more computationally demanding features, is used for rescoring.", "startOffset": 66, "endOffset": 70}, {"referenceID": 62, "context": "For the statistical significance test, we use MAPSSWE test [66].", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "We compare: letter only, phonetic features only, letter + phonological feature [11] and letter + phonetic feature [42].", "startOffset": 79, "endOffset": 83}, {"referenceID": 38, "context": "We compare: letter only, phonetic features only, letter + phonological feature [11] and letter + phonetic feature [42].", "startOffset": 114, "endOffset": 118}, {"referenceID": 31, "context": ", [35] for speech recognition.", "startOffset": 2, "endOffset": 6}], "year": 2016, "abstractText": "In this thesis, we study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL, and recognizing it is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work, we propose several types of recognition approaches, and explore the signer variation problem. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 8% letter error rates. The signer-independent setting is much more challenging, but with neural network adaptation we achieve up to 17% letter error rates. Thesis Supervisor: Karen Livescu Title: Assistant Professor", "creator": "LaTeX with hyperref package"}}}