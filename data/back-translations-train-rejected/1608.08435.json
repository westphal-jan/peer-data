{"id": "1608.08435", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2016", "title": "Multi-Label Classification Method Based on Extreme Learning Machines", "abstract": "In this paper, an Extreme Learning Machine (ELM) based technique for Multi-label classification problems is proposed and discussed. In multi-label classification, each of the input data samples belongs to one or more than one class labels. The traditional binary and multi-class classification problems are the subset of the multi-label problem with the number of labels corresponding to each sample limited to one. The proposed ELM based multi-label classification technique is evaluated with six different benchmark multi-label datasets from different domains such as multimedia, text and biology. A detailed comparison of the results is made by comparing the proposed method with the results from nine state of the arts techniques for five different evaluation metrics. The nine methods are chosen from different categories of multi-label methods. The comparative results shows that the proposed Extreme Learning Machine based multi-label classification technique is a better alternative than the existing state of the art methods for multi-label problems.", "histories": [["v1", "Tue, 30 Aug 2016 13:08:06 GMT  (667kb)", "http://arxiv.org/abs/1608.08435v1", "6 pages, 7 figures, 7 tables, ICARCV"]], "COMMENTS": "6 pages, 7 figures, 7 tables, ICARCV", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["rajasekar venkatesan", "meng joo er"], "accepted": false, "id": "1608.08435"}, "pdf": {"name": "1608.08435.pdf", "metadata": {"source": "CRF", "title": "Multi-Label Classification Method Based on Extreme Learning Machines", "authors": ["Rajasekar Venkatesan", "Meng Joo Er"], "emails": ["RAJA0046@e.ntu.edu.sg", "EMJER@ntu.edu.sg"], "sections": [{"heading": null, "text": "In fact, it is the case that most of them are able to determine themselves as they have behaved, and that they are able to determine themselves, to determine how they want to act. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) It is as if they were able to determine themselves. (...) \"(...)\" (...) \"() ((...)\" () ((()) ((()) ((()) (()) (()) () () () () () () () () () () (()) (() () () () (()) (() () () () (()) (() () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () (() () () (() () (() () (() () () ((() () (() (() () ((() () (() (() ((() () (() (() ((() (() ((() (() () (((() ((() (() ((() (() ((() ((((() (())) ((((()) (((()) ((((((())) (((((((((()))))) ((((((((((((((("}, {"heading": "A. Algorithm Adaptation Methods", "text": "The AA methods are those that can adapt, extend and adapt an existing machine learning algorithm to meet the needs of solving multi-label problems [14]. Therefore, the AA methods can be subcategorized based on the basis of the existing algorithm on which the multi-label variant is developed. Multi-label variants are developed based on boosting, kNN, decision trees, neural networks and SVM [14 and references within]."}, {"heading": "B. Problem Transformation Methods", "text": "Some of the early PT methods use simple transformation techniques such as instance elimination, label elimination, label decomposition, etc. More advanced transformation methods such as copy transformation, label powerset, pruned problem transformation methods are developed later on. More popular and novel transformation methods such as the Binary Relevance (BR) method and classifier chain (CC) method have been developed in recent years. These methods dissect multi-label classification into a series of single-label classification problems, with each individual label problem focusing on one label in the multi-label case. Label powerset methods such as HOMER combine the multiple labels, creating new labels that make it a single-label problem. Paired methods use multiple classifiers that cover all possible label pairs. To combine the output of the classifiers, either the method based on CLL or WR is applied."}, {"heading": "C. Ensemble Methods", "text": "Some of the well-known ensemble methods are Random k label sets (RAkEL), Ensemble of Classifier Chains (ECC), Random forest based predictive clustering trees (RF-PCT), Random Decision Tree (RDT), etc. Based on the machine learning algorithm used, the multi-label methods have been grouped, as shown in Figure 2 [14]. The brief review shows that ELM-based techniques have not yet been used to implement the multi-label classification problem. This paper proposes an AA method multi-label classifier based on ELM.III. PROPOSED ALGORITHMThis section provides a brief overview of the ELM technique and the proposed algorithm used for multi-label classification."}, {"heading": "B. Processing of Inputs", "text": "The multi-label output corresponding to the input sequence is generally provided as \"0\" and \"1\" for each label, with \"1\" corresponding to the labels to which the input patterns belong. In multi-label classification, each of the input patterns can belong to one or more labels, and these inputs are converted from unipolar to bipolar representation."}, {"heading": "C. ELM Training", "text": "The basic batch learning equation given in (1) can be summarized as follows: H\u03b2 = Y, where H is a N x N'matrix, \u03b2 is a N '* L matrix, and Y is a N x L output matrix, where each line provides the output of the corresponding input sample and each column corresponds to each of the labels. In the training phase, the output weight \u03b2 from the training input and output data is evaluated as \u03b2 = H + Y, where H + is the generalized inversion of the hidden output matrix of Moore-Penrose and Y is the bipolar N x L matrix."}, {"heading": "D. ELM Testing", "text": "In the test phase, the test output is evaluated on the basis of the \u03b2 values found in the equation Y = H\u03b2 during the training phase. In the one-label classification, the class name to which the input sample belongs can be identified by determining the column with the maximum value in Y. In case of problems with multi-label classification, however, the input samples can belong to one or more labels and therefore cannot be directly identified by identifying the column with the maximum value. In multi-label classification, the N x L values of Y are passed on as arguments for the bipolar step function. A threshold of \"0\" is applied to the resulting values. The column set of the resulting matrix with values 1 indicates the multi-label affiliation of the corresponding input. IV. MULTI-LABEL CLASSIFICATION METRICS"}, {"heading": "A. Dataset Metrics", "text": "The degree of multi-label characteristics of each data set varies. Some data sets may have a large part of the input data set as multi-label characteristics, while some other data sets have only a few multi-label characteristics. The degree of multi-label characteristics can be quantified using two metrics: label cardinality (Lc) and label density (Ld). Let the data set be specified as {xi, Yi}, i = 1... N with L number of labels. Then, label cardinality is defined as the average number of labels of the input samples in the data set. Label density means the average number of labels of the input samples divided by the number of labels. Label-N cardinalityLabel 11 (2) NiiLYN density Label11 (3) Label cardinality means the average number of labels of the existing data sets, the number of the training datasets are independent of the number of the existing data sets."}, {"heading": "B. Evaluation Metrics", "text": "eiD rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the"}, {"heading": "A. Hamming Loss", "text": "The hammer loss indicates the percentage of false labels in the total number of labels. The lower the hammer loss, the better the performance of the method used. For an ideal classifier, the hammer loss is 0. The hammer loss can be calculated by (4)."}, {"heading": "B. Accuracy", "text": "The accuracy of the multi-label classifier is defined as the ratio of the predicted correct labels to the total number of labels in this case. The general accuracy is the average across all instances. The accuracy can be evaluated using (5)."}, {"heading": "C. Precision", "text": "Precision is the ratio of the predicted correct labels to the total number of actual labels averaged over all instances. In other words, it is the ratio of the true positives to the sum of the true positives and false positives averaged over all instances. The expression for precision is given in (6)."}, {"heading": "D. Recall", "text": "Recall is the ratio of the predicted correct labels to the total number of predicted labels, averaged over all cases. In other words, it is the ratio of the true positives to the sum of the true positives and false negatives averaged over all cases. The expression for recall is given in (7)."}, {"heading": "E. F1-measure", "text": "In fact, it is the case that most people who are able, are able, are able to move, are able to move and are not able, are not able, are able to move."}, {"heading": "ACKNOWLEDGMENT", "text": "The first author thanks Nanyang Technological University, Singapore, for the NTU Research Student Scholarship."}], "references": [{"title": "A Preliminary approach to the multi-label classification problem of Portuguese juridical documents", "author": ["T Gonclaves", "P Quaresma"], "venue": "EPIA 2003, LNCS (LNAI),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T Joachims"], "venue": "Nedellec C, Rouveirol C (Ed.), ECML, LNCS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1998}, {"title": "Evaluation of Two Systems on Multiclass Multi-label document classification", "author": ["X Luo", "A.N. Zincir Heywood"], "venue": "ISMIS 2005, LNCS (LNAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Experiments with multi-label text classifier on the Reuters collection", "author": ["D Tikk", "G Biro"], "venue": "Proc. Of the International Conference on Computational Cybernetics (ICCC 2003),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "Multi-label informed latent semantic indexing", "author": ["K Yu", "S Yu", "V Tresp"], "venue": "Proceedings of the 28th annual international ACM SIGIR conference on Research and Development in information retrieval,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}, {"title": "Significance level based multiple tree classification", "author": ["A Karalic", "V Pirnat"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1991}, {"title": "A kernel method for multi-labelled classification", "author": ["A Elisseeff", "J Weston"], "venue": "Neural Information Processing Systems, NIPS,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "A k-nearest neighbour based algorithm for multi-label classification", "author": ["M.L. Zhang", "Z.H. Zhou"], "venue": "Proceedings of the 1st IEEE international Conference on Granular Computing, Beijing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2005}, {"title": "Multi-label semantic scene classification", "author": ["M Boutell", "X Shen", "J Luo", "C Brouwn"], "venue": "Technical Report,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2003}, {"title": "Multi-label machine learning and its application to semantic scene classification. Storage and Retrieval Methods and Applications for Multimedia", "author": ["X Shen", "M Boutell", "J Luo", "C Brown"], "venue": "Yeung MM, Lienhart RW, Li CS (Ed.), Proceedings of the SPIE,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "Efficient Approximation Algorithms for Multi-label Map Labelling", "author": ["B. Zhu", "C.K. Poon"], "venue": "Aggarwal AK, Pandu Rangan C (Ed.) ISSAC", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1999}, {"title": "Kernel methods for multi-labelled classification and categorical regression problems", "author": ["A Elisseeff", "J Weston"], "venue": "Technical Report, BIOwulf Technologies,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "An extensive experimental comparison of methods for multi-label learning", "author": ["Gjorgji Madjarov", "Dragi Kocev", "Dejan Gjorgjevikj", "Saso Dzeroski"], "venue": "Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "A Tutorial on Multi-label Classification Techniques", "author": ["Andre CPLF de Carvalho", "Alex A Freitas"], "venue": "Foundations of Computational Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2009}, {"title": "Multi-label Classification: An Overview", "author": ["Grigorios Tsoumakas", "Ioannis Katakis"], "venue": "International Journal of Data Warehousing and Mining,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2007}, {"title": "ML-kNN: A lazy learning approach to multi-label learning", "author": ["Min L. Zhang", "Zhi H. Zhou"], "venue": "Pattern Recognition,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 1, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 2, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 3, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 4, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 100, "endOffset": 105}, {"referenceID": 5, "context": "Initially, the application of multilabel classification is primarily focused on text-categorization [1-5] and medical diagnosis [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 6, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 97, "endOffset": 102}, {"referenceID": 7, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 97, "endOffset": 102}, {"referenceID": 8, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 125, "endOffset": 132}, {"referenceID": 9, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 125, "endOffset": 132}, {"referenceID": 10, "context": "The application of multi-label classification has extended to other areas such as bioinformatics [8-9], scene classification [10-11], map labelling [12] etc.", "startOffset": 148, "endOffset": 152}, {"referenceID": 11, "context": "Thus, binary classification, multi-class classification and ordinal regression problems can be seen as special cases of multi-label problems where the number of labels assigned to each instance is equal to 1 [13].", "startOffset": 208, "endOffset": 212}, {"referenceID": 12, "context": "in their paper [14] categorizes these techniques into three major categories.", "startOffset": 15, "endOffset": 19}, {"referenceID": 12, "context": "[14] and from the comparison of results it can be seen that there exists no single method that performs uniformly well on a wide range of datasets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15], the term \u201cclassification\u201d can be formally defined as, \u201cGiven a set of training examples composed of pairs {xi, yi}, find a function f(x) that maps each attribute vector xi to its associated class yi, i = 1,2,3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "\u201d Single label classification involves associating a single label \u2018l\u2019 from a set of disjoint labels \u2018L\u2019 to each of the input data sequence [16].", "startOffset": 139, "endOffset": 143}, {"referenceID": 15, "context": "But the generality of multi-label classification makes it more difficult to be implemented and trained than the others [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "The paper [14] discuss in detail the state of the arts multi-label classification methods and categorizes the existing methods into three groups.", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "Adapted from [14], the overview of existing methods can be summarized as shown in figure 1.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "The initial overview of the multi-label classification methods is presented in [16], which classifies the existing methods then into two categories.", "startOffset": 79, "endOffset": 83}, {"referenceID": 12, "context": "In recent years, more multi-label classifiers have been developed and the most recent overview of multi-label classifiers introduces the third category of Ensemble (EN) methods [14].", "startOffset": 177, "endOffset": 181}, {"referenceID": 12, "context": "The AA methods are those that can adapt, extend and customize an existing machine learning algorithm to meet the needs of solving multi-label problems [14].", "startOffset": 151, "endOffset": 155}, {"referenceID": 12, "context": "Based on the machine learning algorithm used, the multi-label methods have been grouped as shown in figure 2 [14].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Two datasets having same label cardinality, but different label density can differ largely in their properties and may cause different behavior to the training algorithm [17].", "startOffset": 170, "endOffset": 174}, {"referenceID": 12, "context": "The results of the state of the art methods are obtained from [14].", "startOffset": 62, "endOffset": 66}], "year": 2014, "abstractText": "In this paper, an Extreme Learning Machine (ELM) based technique for Multi-label classification problems is proposed and discussed. In multi-label classification, each of the input data samples belongs to one or more than one class labels. The traditional binary and multi-class classification problems are the subset of the multi-label problem with the number of labels corresponding to each sample limited to one. The proposed ELM based multi-label classification technique is evaluated with six different benchmark multi-label datasets from different domains such as multimedia, text and biology. A detailed comparison of the results is made by comparing the proposed method with the results from nine state of the arts techniques for five different evaluation metrics. The nine methods are chosen from different categories of multi-label methods. The comparative results shows that the proposed Extreme Learning Machine based multi-label classification technique is a better alternative than the existing state of the art methods for multi-label problems. Keywords\u2014Machine Learning, Extreme Learning Machines, Multi-label Learning, Classification.", "creator": "Microsoft\u00ae Word 2013"}}}