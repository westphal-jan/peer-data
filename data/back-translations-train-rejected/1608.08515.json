{"id": "1608.08515", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2016", "title": "Language Detection For Short Text Messages In Social Media", "abstract": "With the constant growth of the World Wide Web and the number of documents in different languages accordingly, the need for reliable language detection tools has increased as well. Platforms such as Twitter with predominantly short texts are becoming important information resources, which additionally imposes the need for short texts language detection algorithms. In this paper, we show how incorporating personalized user-specific information into the language detection algorithm leads to an important improvement of detection results. To choose the best algorithm for language detection for short text messages, we investigate several machine learning approaches. These approaches include the use of the well-known classifiers such as SVM and logistic regression, a dictionary based approach, and a probabilistic model based on modified Kneser-Ney smoothing. Furthermore, the extension of the probabilistic model to include additional user-specific information such as evidence accumulation per user and user interface language is explored, with the goal of improving the classification performance. The proposed approaches are evaluated on randomly collected Twitter data containing Latin as well as non-Latin alphabet languages and the quality of the obtained results is compared, followed by the selection of the best performing algorithm. This algorithm is then evaluated against two already existing general language detection tools: Chromium Compact Language Detector 2 (CLD2) and langid, where our method significantly outperforms the results achieved by both of the mentioned methods. Additionally, a preview of benefits and possible applications of having a reliable language detection algorithm is given.", "histories": [["v1", "Tue, 30 Aug 2016 15:43:52 GMT  (83kb,D)", "http://arxiv.org/abs/1608.08515v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["ivana balazevic", "mikio braun", "klaus-robert m\\\"uller"], "accepted": false, "id": "1608.08515"}, "pdf": {"name": "1608.08515.pdf", "metadata": {"source": "CRF", "title": "Language Detection For Short Text Messages In Social Media", "authors": ["Ivana Bala\u017eevi\u0107", "Mikio Braun", "Klaus-Robert M\u00fcller"], "emails": [], "sections": [{"heading": null, "text": "With the steady growth of the World Wide Web and the number of documents in different languages, the need for reliable speech recognition tools has increased accordingly. Platforms such as Twitter with predominantly short texts are becoming important sources of information, which in addition imposes the need for speech recognition algorithms for short texts. In this article, we will show how the inclusion of personalised user-specific information in the speech recognition algorithm leads to an important improvement in recognition results. In order to choose the best algorithm for speech recognition for short text messages, we will examine several machine learning approaches. These approaches include the use of well-known classifiers such as SVM and logistic regression, a dictionary-based approach and a probability model based on modified Kneser-Ney smoothing. In addition, the extension of the probability model to include additional user-specific information such as the accumulation of evidence per user and user interface language, with the aim of improving classification performance."}, {"heading": "1 Introduction", "text": "It is often the first step in a document processing pipeline. In addition, it is considered a critical step in the pre-processing of application processes that require language-specific modeling, such as search engines, where different tokenizer methods can be used depending on the language detected. Another common example of the use of language recognition methods is a previous step towards machine translation, as the language of the text to be translated is not always specified. Therefore, a reliable speech recognition tool has been studied even since the 1960s, short texts appearing on social media still require special treatment due to their specific language type, i.e. acronyms, abbreviations, spelling errors, emoticons, new words, etc., despite the fact that language recognition has long been a known problem, an appropriate solution for short text classification has yet to be found."}, {"heading": "1.1 Datasets", "text": "The data set used for this task consisted of.json files in which the Tweet objects are stored. However, each Tweet object contains different types of relevant information about its nature, such as the unique identifier of the tweet itself, the text it contains, information about its author, the time and place at which it was created, etc. However, only portions of this information are considered relevant to the classification task. The files used contain tweets collected via the Twitter API in April 2012, with a total of around 22,000 tweets in 16 different languages being collected randomly at different times over two days. Languages appearing in fewer than 3 tweets are discarded and, due to insufficient domain knowledge, are combined into one language in Indonesian and Malay. Language distribution within the data set is shown in Fig. 1, that when collecting the data we comply with Twitter's terms of use."}, {"heading": "2 Methods", "text": "The general task of speech recognition is to predict the language l in which the text will be written for a given text t. A typical naive approach to solving the speech recognition task would be to show the text to a specific language expert, who would then decide in which language the text will be written. However, this would require many different language experts for each of the languages. However, this solution becomes even more problematic if the text database is not static but changes over time, where scalability becomes a problem. Therefore, a machine learning approach is required. In the approach of machine learning to solving the speech recognition problem, we get a certain amount of data (a set of texts in different languages) and the labels (languages to which these texts belong).The labels were previously assigned to the data by some form of the annotation process. Although the need for human language experts still exists in the annotation step, the algorithm does the rest of the work. After the labels and not only the object in which the object is monitored, the object in which the learning object is the object is the object, each object is monitored."}, {"heading": "2.1 Preprocessing", "text": "The pre-processing task is usually the first part of a machine-learning document processing pipeline that precedes the extraction of characteristics from the data. In this work, pre-processing included editing the tweet texts and assigning the corresponding language names. Text editing generally consists of cleaning the texts, removing any information deemed unnecessary for the task, and transforming all texts into the same, mutually comparable form. In the first step of pre-processing, all links and expressions that appeal to a specific user (the @ username form) are removed from each tweet text, using simple regular expressions because they are considered irrelevant to the distinction between languages. In addition, all emoticons are removed because they retain the same form in all languages. Subsequently, the text is converted to lowercase letters, all multiple spaces are truncated, and all punctuation characters are removed. In addition, all emoticons are removed because they retain the same form in all languages."}, {"heading": "2.2 Feature Extraction", "text": "In this paper, two different types of characteristics are extracted from the tweet texts, depending on the classifier used: characters n-grams and words bag-of-words features. Characters n-grams can be described as all strings of length n in the given text. On the other hand, the words bag-of-words features are defined as a disordered collection of words in the text. Whenever possible, the character n-gram feature model is chosen over the word bag-of-words model, which is justified by the specific type of language used in the data set. Namely, the character n-grams model is more resistant to misspellings, abbreviations, acronyms and word derivations than the word bag-of-words model, as it does not strictly force the division of texts by white spaces. For SVM and logistic regression classification, n-grams are selected as suitable characteristics."}, {"heading": "2.3 Classification", "text": "SVM Support vector machines (SVMs) are supervised learning models, used most for classification and regression problems. (In the following paragraphs, the SVM classification is described for the sake of simplicity, since multi-class classification is only an extension of this model [25]. Multi-class support in this paper is handled according to the one-on-one scheme. SVM classification focuses on trying to maximize margin, i.e. the removal of the data points of both classes from the decision boundary based on structural risk minimization [26]. One way to achieve this is to solve the dual optimization problem [27]. The dual optimization problem is defined as: max."}, {"heading": "2.4 Including Additional Information", "text": "The most important contribution and novelty of this paper compared to previous work in the field of speech recognition is the extension of the probabilistic model to include additional personalized user information. Due to the fact that the output of the probabilistic model takes the form of a probability distribution across different languages, additional information can be added to the model in order to improve the predictions. Two types of information are examined here and added to the original model and their impact on predictions is assessed, which include prior information on a particular user's use of language and information on the Language UI. First, a prior frequency distribution for the choice of a particular language is defined for each user. This distribution is chosen in such a way that it is initially uniform, with an experimentally determined value, as described later in the results section. For each new text in the test data, the chosen language should no longer be determined by looking only at the probability distribution of the classifier as before, but rather at the product of this distribution with the previous user-specific distribution."}, {"heading": "3 Results", "text": "In this first part of this section, we give a brief introduction to the accuracy measurements with which the performance of the classifiers is compared. In order to gain a better insight into the performance of the different methods, their prediction results are compared in the second part of this section. In addition, the next subsection assesses how adding additional information to the classification model will impact solely on the basis of character n-grams. Subsequently, the best prediction method is selected and these results are compared with the CLD2 and Langid results described in the last subsection."}, {"heading": "3.1 Accuracy Measures", "text": "A good classifier is defined as one where the number of true positives (TP) and true negatives (TN) is high, while at the same time keeping the number of false positives (FP) and false negatives (FN) low. To summarize these results, two different measures of accuracy have been used to evaluate the performance of the classifiers: micro- and macro-averaged F1 score. To understand the F1 score, precision and retrieval must first be explained. Precision is defined as: Precision = TPTP + FP and it measures the ability of the classifier not to assign a sample to the class to which it does not belong. Recall is defined as: Recall = TPTP + FN and it measures the classifier's ability to find all samples belonging to that class (both assigned and the non-assigned languages)."}, {"heading": "3.2 Performance Comparison of Different Methods", "text": "In order to assess the quality of SVM and logistic regression forecasts, a 5-fold cross-validation procedure is performed on both classification methods and the presented results are in the form of mean micro and macro averages. In addition, SVM and logistic regression are combined, as it is assumed that this will increase classification accuracy, as indicated in [33] and [34]. In ensemble learning, the confidence values for selecting a particular language are in fact the probabilities gained from both classification methods. In fact, if the predicted language for a particular text is different from the classifiers, the label is removed from the classifier, which produces a higher consolidation score, which implements a higher consolidation implementation of SVM learning learning scores."}, {"heading": "3.5 Getting More Out of the Data", "text": "In this section, attention is drawn to the potential applications of having a reliable algorithm for recognizing short text languages (78.04% followed by 96% TZ). Although speech recognition is an interesting and challenging task in itself, many more conclusions and appealing statistics can be drawn after they have already been applied to real-world data. Therefore, the links between the predicted languages and the UI language and the location will be examined. Figure 5 shows the relationship between the predicted language and the UI language. It is not so surprising to note that users belonging to many different nationalities (provided that the nationalities normally correspond to the UI languages of the users) are tweeting in English or that many users have English as their UI language, regardless of which language they are tweeting in. On the other hand, it is interesting to see that users who frequently tweet in Spanish are set up Spanish and Portuguese as their UI languages. This illustrates the geographical and UI similarity of these languages, which are Ulex languages."}, {"heading": "4 Conclusions", "text": "This paper examines various algorithmic approaches to speech recognition for short texts in social media. The first approach involves the use of well-known classifiers such as SVM and logistic regression, and the combination of the two. The second approach is based on a probabilistic model with modified Kneser-Ney smoothing, extending it to include additional, user-specific information; the last approach is a simple dictionary-based method; when comparing the classification performance of all algorithms, the probabilistic model outperforms the other methods; the dictionary method achieves by far the worst results, as short tweets full of spelling errors, abbreviations, and acronyms do not match most of the words present in the Aspell dictionaries; the other two methods are trained directly on Twitter data, giving them a significant advantage over the dictionary method; once additional information about users is inserted into the probability-programmed model, the accuracy of the language, and the accuracy of the fact that the accuracy of the CLP is increased with prior to the fact that the use of the language and the CLP."}, {"heading": "Acknowledgments", "text": "This work was supported by the Brain Korea 21 Plus Program, the National Research Foundation of Korea, which is funded by the Ministry of Education and the Federal Ministry of Education and Research (BMBF) under the funding programs 01IS14013A-E and 01GQ1115. Correspondence to the KRM."}], "references": [{"title": "Microblog language identification: Overcoming the limitations of short, unedited and idiomatic text", "author": ["Simon Carter", "Wouter Weerkamp", "Manos Tsagkias"], "venue": "Lang. Resour. Eval.,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Making sense of twitter search", "author": ["Gene Golovchinsky", "Miles Efron"], "venue": "In Proc. CHI2010 Workshop on Microblogging: What and How Can We Learn From It? April", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "Earthquake shakes twitter users: Real-time event detection by social sensors", "author": ["Takeshi Sakaki", "Makoto Okazaki", "Yutaka Matsuo"], "venue": "In Proceedings of the 19th International 18  Conference on World Wide Web, WWW", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Microblogging during two natural hazards events: What twitter may contribute to situational awareness", "author": ["Sarah Vieweg", "Amanda L. Hughes", "Kate Starbird", "Leysia Palen"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Qualitative Media Analysis (Qualitative Research Methods)", "author": ["D.L. Altheide"], "venue": "Sage Pubn Inc,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1996}, {"title": "Twitter power: Tweets as electronic word of mouth", "author": ["Bernard J. Jansen", "Mimi Zhang", "Kate Sobel", "Abdur Chowdury"], "venue": "J. Am. Soc. Inf. Sci. Technol.,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Predicting elections with twitter: What 140 characters reveal about political sentiment", "author": ["A. Tumasjan", "T.O. Sprenger", "P.G. Sandner", "I.M. Welpe"], "venue": "In Proceedings of the Fourth International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Predicting imdb movie ratings using social media", "author": ["Andrei Oghina", "Mathias Breuss", "Manos Tsagkias", "Maarten de Rijke"], "venue": "In Proceedings of the 34th European Conference on Advances in Information Retrieval,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "N-gram-based text categorization", "author": ["William B. Cavnar", "John M. Trenkle"], "venue": "Proceedings of SDAIR-94, 3rd Annual Symposium on Document Analysis and Information Retrieval,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1994}, {"title": "Compact language detector", "author": ["Dick Sites"], "venue": "https://github.com/CLD2Owners/cld2,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Langid.py: An off-the-shelf language identification tool", "author": ["Marco Lui", "Timothy Baldwin"], "venue": "In Proceedings of the ACL 2012 System Demonstrations,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Statistical identification of language", "author": ["Ted Dunning"], "venue": "Technical report,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1994}, {"title": "Gauging similarity with n-grams: Language-independent categorization", "author": ["Marc Damashek"], "venue": "of text. Science,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1995}, {"title": "An information-theoretic measure for document similarity", "author": ["Javed A. Aslam", "Meredith Frost"], "venue": "In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Informaion Retrieval,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Language identification in web pages", "author": ["Bruno Martins", "M\u00e1rio J. Silva"], "venue": "In Proceedings of the 2005 ACM Symposium on Applied Computing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2005}, {"title": "Kernel based text categorization", "author": ["O Teytaud", "Radwan Jalam"], "venue": "In 12th International Joint Conference on Neural Networks (IJCNN),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Text classification using string kernels", "author": ["Huma Lodhi", "Craig Saunders", "John Shawe-Taylor", "Nello Cristianini", "Chris Watkins"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "Language identification based on string kernels", "author": ["Canasai Kruengkrai", "Prapass Srichaivattana", "Virach Sornlertlamvanich", "Hitoshi Isahara"], "venue": "Proceedings of the 5th International Symposium on Communications and Information Technologies", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2005}, {"title": "Automatic language identification using deep neural networks", "author": ["Ignacio Lopez-Moreno", "Javier Gonzalez-Dominguez", "Oldrich Plchot"], "venue": "In Proc. ICASSP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Language identification in short utterances using long short-term memory (lstm) recurrent neural networks", "author": ["Ruben Zazo", "Alicia Lozano-Diez", "Javier Gonzalez-Dominguez", "Doroteo T. Toledano", "Joaquin Gonzalez-Rodriguez"], "venue": "PLoS ONE, 11(1):1\u201317,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Language identification: The long and the short of the matter", "author": ["Timothy Baldwin", "Marco Lui"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2010}, {"title": "The twitter of babel: Mapping world languages through microblogging", "author": ["Delia Mocanu", "Andrea Baronchelli", "Bruno Gon\u00e7alves", "Nicola Perra", "Alessandro Vespignani"], "venue": "platforms. CoRR,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "An empirical study of smoothing techniques for language modeling", "author": ["Stanley F. Chen", "Joshua Goodman"], "venue": "In Proceedings of the 34th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1996}, {"title": "On the algorithmic implementation of multiclass kernel-based vector machines", "author": ["Koby Crammer", "Yoram Singer"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2002}, {"title": "The Nature of Statistical Learning Theory", "author": ["Vladimir N. Vapnik"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1995}, {"title": "An introduction to kernel-based learning algorithms", "author": ["K.-R. M\u00fcller", "S. Mika", "G. R\u00e4tsch", "S. Tsuda", "B Sch\u00f6lkopf"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2001}, {"title": "A practical guide to support vector classification", "author": ["Chih-Wei Hsu", "Chih-Chung Chang", "Chih-Jen Lin"], "venue": "Technical report,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Text categorization with suport vector machines: Learning with many relevant features", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the 10th European Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1998}, {"title": "On relevant dimensions in kernel feature spaces", "author": ["Mikio L. Braun", "Joachim M. Buhmann", "Klaus-Robert M\u00fcller"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2008}, {"title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "author": ["Christopher M. Bishop"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "The janusrtk switchboard/callhome 1997 evaluation system", "author": ["M. Finke", "J. Fritsch", "P. Geutner", "K. Ries", "T. Zeppenfeld", "A. Waibel"], "venue": "In Proceedings of LVCSR Hub 5-E Workshop,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1997}, {"title": "Popular ensemble methods: An empirical study", "author": ["David Opitz", "Richard Maclin"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 1999}, {"title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "author": ["John C. Platt"], "venue": "In Advances in Large Margin Classifiers,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1999}], "referenceMentions": [{"referenceID": 0, "context": "Platforms such as Twitter, where these short texts are used, are recently becoming important real-time information resources [1,2].", "startOffset": 125, "endOffset": 130}, {"referenceID": 1, "context": "Platforms such as Twitter, where these short texts are used, are recently becoming important real-time information resources [1,2].", "startOffset": 125, "endOffset": 130}, {"referenceID": 2, "context": "A wide range of applications is connected to their usage - event detection [3,4], media analysis [5], opinion mining [6,7], predicting movie ratings [8], etc.", "startOffset": 75, "endOffset": 80}, {"referenceID": 3, "context": "A wide range of applications is connected to their usage - event detection [3,4], media analysis [5], opinion mining [6,7], predicting movie ratings [8], etc.", "startOffset": 75, "endOffset": 80}, {"referenceID": 4, "context": "A wide range of applications is connected to their usage - event detection [3,4], media analysis [5], opinion mining [6,7], predicting movie ratings [8], etc.", "startOffset": 97, "endOffset": 100}, {"referenceID": 5, "context": "A wide range of applications is connected to their usage - event detection [3,4], media analysis [5], opinion mining [6,7], predicting movie ratings [8], etc.", "startOffset": 117, "endOffset": 122}, {"referenceID": 6, "context": "A wide range of applications is connected to their usage - event detection [3,4], media analysis [5], opinion mining [6,7], predicting movie ratings [8], etc.", "startOffset": 117, "endOffset": 122}, {"referenceID": 7, "context": "A wide range of applications is connected to their usage - event detection [3,4], media analysis [5], opinion mining [6,7], predicting movie ratings [8], etc.", "startOffset": 149, "endOffset": 152}, {"referenceID": 8, "context": "Some of the best known models include the one of Cavnar and Trenkle [9] popularized in the textcat tool, the Chromium Compact Language Detector 2 (CLD2) [10], originally extracted from the source code for Google Chromium\u2019s library by Michael McCandless and developed further by Dick Sites, and langid [11], an off-the-shelf language identification tool by Lui and Baldwin.", "startOffset": 68, "endOffset": 71}, {"referenceID": 9, "context": "Some of the best known models include the one of Cavnar and Trenkle [9] popularized in the textcat tool, the Chromium Compact Language Detector 2 (CLD2) [10], originally extracted from the source code for Google Chromium\u2019s library by Michael McCandless and developed further by Dick Sites, and langid [11], an off-the-shelf language identification tool by Lui and Baldwin.", "startOffset": 153, "endOffset": 157}, {"referenceID": 10, "context": "Some of the best known models include the one of Cavnar and Trenkle [9] popularized in the textcat tool, the Chromium Compact Language Detector 2 (CLD2) [10], originally extracted from the source code for Google Chromium\u2019s library by Michael McCandless and developed further by Dick Sites, and langid [11], an off-the-shelf language identification tool by Lui and Baldwin.", "startOffset": 301, "endOffset": 305}, {"referenceID": 8, "context": "The Cavnar and Trenkle method uses a per-language character frequency model and classifies documents via their relative \u201cout-of-place\u201d distance from each language (see [9] for more details).", "startOffset": 168, "endOffset": 171}, {"referenceID": 11, "context": "Variants on this method include Bayesian models for character sequence prediction [12], dot products of word frequency vectors [13], and information-theoretic measures of document similarity [14, 15].", "startOffset": 82, "endOffset": 86}, {"referenceID": 12, "context": "Variants on this method include Bayesian models for character sequence prediction [12], dot products of word frequency vectors [13], and information-theoretic measures of document similarity [14, 15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 13, "context": "Variants on this method include Bayesian models for character sequence prediction [12], dot products of word frequency vectors [13], and information-theoretic measures of document similarity [14, 15].", "startOffset": 191, "endOffset": 199}, {"referenceID": 14, "context": "Variants on this method include Bayesian models for character sequence prediction [12], dot products of word frequency vectors [13], and information-theoretic measures of document similarity [14, 15].", "startOffset": 191, "endOffset": 199}, {"referenceID": 10, "context": "trained on 97 languages over a naive Bayes classifier with a multinomial event model over a mixture of byte n-grams (1 \u2264 n \u2264 4) designed to be used off-the-shelf [11].", "startOffset": 162, "endOffset": 166}, {"referenceID": 15, "context": "Additionally, kernel methods such as support vector machines (SVMs) were recently successfully applied to the same task [16\u201318], which motivated testing their performance on the short texts dataset from this paper.", "startOffset": 120, "endOffset": 127}, {"referenceID": 16, "context": "Additionally, kernel methods such as support vector machines (SVMs) were recently successfully applied to the same task [16\u201318], which motivated testing their performance on the short texts dataset from this paper.", "startOffset": 120, "endOffset": 127}, {"referenceID": 17, "context": "Additionally, kernel methods such as support vector machines (SVMs) were recently successfully applied to the same task [16\u201318], which motivated testing their performance on the short texts dataset from this paper.", "startOffset": 120, "endOffset": 127}, {"referenceID": 18, "context": "Recently, approaches based on deep neural network architectures are becoming increasingly common, with very promising results in language detection on speech data [19, 20].", "startOffset": 163, "endOffset": 171}, {"referenceID": 19, "context": "Recently, approaches based on deep neural network architectures are becoming increasingly common, with very promising results in language detection on speech data [19, 20].", "startOffset": 163, "endOffset": 171}, {"referenceID": 20, "context": "An interesting research by Baldwin and Lui [21] explores the impact of document length on language detection, with the conclusion that the performance accuracy improves significantly with increasing the document length.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "Another example of short text language detection is done in [1] and it relies on the results of the before mentioned textcat tool.", "startOffset": 60, "endOffset": 63}, {"referenceID": 21, "context": "Even though the distribution of the number of tweets per language in the dataset is highly skewed, it corresponds quite well to the distribution of Twitter languages given in [22].", "startOffset": 175, "endOffset": 179}, {"referenceID": 22, "context": "For every combination of n values (the values of n = {2, 3, 4} are considered reasonable for short Twitter texts) and normalization types (tf-idf and length normalization are examined here), the micro- and macro-averaged F1-scores are computed using the scikit-learn software [23].", "startOffset": 276, "endOffset": 280}, {"referenceID": 23, "context": "Due to the use of the modified Kneser-Ney smoothing [24] and its recursive nature, character 1-4-grams are chosen as the appropriate features.", "startOffset": 52, "endOffset": 56}, {"referenceID": 24, "context": "In the next paragraphs, the SVM classification is described for the case of only two classes for simplicity, since multi-class classification is just an extension of that model [25].", "startOffset": 177, "endOffset": 181}, {"referenceID": 25, "context": "the distance of the data points of both classes from the decision boundary based on structural risk minimization [26].", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "One way to achieve this is by solving the dual optimization problem [27].", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": "After the training phase is completed, a new data point is classified according to the following expression [27]:", "startOffset": 108, "endOffset": 112}, {"referenceID": 27, "context": "Therefore, training an SVM with a linear kernel is faster than with any other kernel, particularly when using a dedicated library such as LibLinear [28].", "startOffset": 148, "endOffset": 152}, {"referenceID": 28, "context": "Finally, most of the text classification problems are linearly separable [29], so no other kernels except for the linear are needed.", "startOffset": 73, "endOffset": 77}, {"referenceID": 22, "context": "SVM is chosen as one of the models in this work because of its many advantages [23]: it is effective in high dimensional spaces, it is still effective even in cases where the number of dimensions exceeds the number of samples (usually the case with categorization of text documents), it uses only a subset of training points in the decision function, so it is also memory efficient, and it is unlikely to overfit, since the ratio of number of data points and effective dimensions is typically high [30], given that an appropriate regularization term is used.", "startOffset": 79, "endOffset": 83}, {"referenceID": 29, "context": "SVM is chosen as one of the models in this work because of its many advantages [23]: it is effective in high dimensional spaces, it is still effective even in cases where the number of dimensions exceeds the number of samples (usually the case with categorization of text documents), it uses only a subset of training points in the decision function, so it is also memory efficient, and it is unlikely to overfit, since the ratio of number of data points and effective dimensions is typically high [30], given that an appropriate regularization term is used.", "startOffset": 498, "endOffset": 502}, {"referenceID": 30, "context": "Logistic Regression Logistic regression, despite having the word \u201cregression\u201d as part of its name, is a linear model for classification rather than regression [31].", "startOffset": 159, "endOffset": 163}, {"referenceID": 22, "context": "It is a type of probabilistic statistical model, where the probabilities describing the possible assignments to different classes are modeled using the logistic function [23], which is defined as:", "startOffset": 170, "endOffset": 174}, {"referenceID": 23, "context": "The algorithm is based on modified Kneser-Ney smoothing, which is a slightly altered version of Kneser-Ney smoothing, proven to outperform the original version [24].", "startOffset": 160, "endOffset": 164}, {"referenceID": 31, "context": "The estimates for the optimal discount values D1, D2, and D3 are computed as a function of training data counts [32]: D1 = 1\u2212 2Y n2 n1 D2 = 2\u2212 3Y n3 n2 D3+ = 3\u2212 4Y n4 n3 where Y = n1 n1+2n2 and nx is the number of n-grams n appearing x times in the training data.", "startOffset": 112, "endOffset": 116}, {"referenceID": 22, "context": "The micro-averaged F1-score calculates the metrics globally by counting the total number of TPs, FPs, and FNs, while the macro-averaged F1-score calculates the metrics for each label and finds their unweighted mean, not taking label imbalance into account [23].", "startOffset": 256, "endOffset": 260}, {"referenceID": 32, "context": "Additionally, SVM and logistic regression are combined together to form an ensemble learning method, since it is assumed that this will increase the classification accuracy, as suggested in [33] and [34].", "startOffset": 190, "endOffset": 194}, {"referenceID": 33, "context": "However, since the scikit-learn implementation of the SVM classifier does not implicitly include probability scores, those are obtained with the use of Platt scaling [35] by setting the probability parameter to True.", "startOffset": 166, "endOffset": 170}], "year": 2016, "abstractText": "With the constant growth of the World Wide Web and the number of documents in different languages accordingly, the need for reliable language detection tools has increased as well. Platforms such as Twitter with predominantly short texts are becoming important information resources, which additionally imposes the need for short texts language detection algorithms. In this paper, we show how incorporating personalized user-specific information into the language detection algorithm leads to an important improvement of detection results. To choose the best algorithm for language detection for short text messages, we investigate several machine learning approaches. These approaches include the use of the well-known classifiers such as SVM and logistic regression, a dictionary based approach, and a probabilistic model based on modified Kneser-Ney smoothing. Furthermore, the extension of the probabilistic model to include additional user-specific information such as evidence accumulation per user and user interface language is explored, with the goal of improving the classification performance. The proposed approaches are evaluated on randomly collected Twitter data containing Latin as well as non-Latin alphabet languages and the quality of the obtained results is compared, followed by the selection of the best performing algorithm. This algorithm is then evaluated against two already existing general language detection tools: Chromium Compact Language Detector 2 (CLD2) and langid, where our method significantly outperforms the results achieved by both of the mentioned methods. Additionally, a preview of benefits and possible applications of having a reliable language detection algorithm is given. 1 ar X iv :1 60 8. 08 51 5v 1 [ cs .C L ] 3 0 A ug 2 01 6", "creator": "LaTeX with hyperref package"}}}