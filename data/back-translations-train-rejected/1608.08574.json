{"id": "1608.08574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2016", "title": "Applying Naive Bayes Classification to Google Play Apps Categorization", "abstract": "There are over one million apps on Google Play Store and over half a million publishers. Having such a huge number of apps and developers can pose a challenge to app users and new publishers on the store. Discovering apps can be challenging if apps are not correctly published in the right category, and, in turn, reduce earnings for app developers. Additionally, with over 41 categories on Google Play Store, deciding on the right category to publish an app can be challenging for developers due to the number of categories they have to choose from. Machine Learning has been very useful, especially in classification problems such sentiment analysis, document classification and spam detection. These strategies can also be applied to app categorization on Google Play Store to suggest appropriate categories for app publishers using details from their application.", "histories": [["v1", "Tue, 30 Aug 2016 17:46:55 GMT  (1029kb,D)", "http://arxiv.org/abs/1608.08574v1", "Experiment Results"]], "COMMENTS": "Experiment Results", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["babatunde olabenjo"], "accepted": false, "id": "1608.08574"}, "pdf": {"name": "1608.08574.pdf", "metadata": {"source": "CRF", "title": "Applying Na\u0131\u0308ve Bayes Classification to Google Play Apps Categorization", "authors": ["Babatunde Olabenjo"], "emails": [], "sections": [{"heading": null, "text": "In this project, we have built two variations of the Na\u00efve Bayes Classifier using open metadata from top developer apps on Google Play Store in others to classify new apps on the store. These classifiers are then compared with different rating methods and their results against each other. Results show that the Na\u00efve Bayes algorithm works well for our classification and potentially the app categorization for Android app publishers on Google Play Store can be automated. INTRODUCTION Machine Learning has been used in various areas to study and learn from patterns in data to make accurate predictions. The use of machine learning can be seen in our daily lives, especially in email providers for the detection of spam messages."}, {"heading": "A. Motivation", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "II. LITERATURE REVIEW", "text": "There have been numerous applications of machine learning in the industry; Amazon Store, IBM E-Commerce and others have used machine learning in product classification as well as product recommendations; ad placement and ad content design have been greatly improved by Google with machine learning; machine learning has also been widely used by Google for its image search in machine vision; although there are several machine learning algorithms for different tasks, classification problems have become the most prevalent in this space. As described by [10] Classification is an example of supervised learning, where a number of correctly identified observations are fed into the machine learning algorithm to train the system; the process allows the machine learning algorithm to correctly identify 1TheSunDaily: http: / / www.thesundaily.my / news / 871252new data provided, based on the knowledge gained from the training set, the machine learning algorithms for their structural learning are known as the unstructured learning process it is used in this category."}, {"heading": "A. Applying Na\u0131\u0308ve Bayes to Classification Problems", "text": "One of the earliest applications of this model to information extraction was applied by [15], where extracting specific facts from the text can also play a massive role in facilitating large amounts of data for users to understand. [16] A proposed strategy for improving strategies using Nai \ufffd ve Bayes for information extraction was applied by [15] The authors show that a well-designed method for improving the performance of a Bayes of Bayes learning system can improve Bayes learning systems. [17] Bayes compared it with other classification algorithms for medical datasets. Their results show that Na\u0131 ve Bayes Bayes works better than other algorithms in classifying medical datasets and can be applied. [17] Comparing Bayes with other classification algorithms for medical datasets."}, {"heading": "III. ALGORITHM DESCRIPTION", "text": "The classifier used in this research by Na\u0131ve Bayes is a simplified probabilistic classifier based on the Bayes theorem. Bayes theorem describes the probability of an event based on the conditions of the event. Bayes rule is mathematically defined as (1) P (A | B) = P (B | A) P (A) P (B) (1), where A and B are two events, so that P (A) is the previous probability, and P (B) the probabilities of A and B are independent of each other 2.P (A | B) is the subsequent probability described as the conditional probability of observing event A, since B is the probability of observing event B, since A is the truth. As discussed in previous sections, Na\u0131ve Bayes's classifier has been used in various applications such as document classification, spam detection of e-mails, and sensory analysis."}, {"heading": "A. Theoretical Background", "text": "From [26] we can see that, although Na\u00efve Bayes's probability estimates are sometimes of low quality, the classification decisions can yield good results. In the text classification described by [26], words are presented as tokens and placed in a particular class, and by using the Maximum a Posterior (MAP) we can generate the classifier as described by [26] as (2).cMAP = argmax c \u00b2 C (P (c | d)) = argmax c \u00b2 C (c) \u00b2 1 \u2264 nd P (tk | c) (2), where P (c | d) the conditional probability of the document cgiven dtk represents the tokens of the document, C represents the amount of classes used in the classification. P (c) represents the previous probability of class c and P (tk | c) the conditional probability of the document c.Here we estimate the probability multiplied by a certain class c \u00b2."}, {"heading": "B. Application in Google Play Store App Categorization", "text": "To build a classifier that classifies words for our data set as characteristics for a particular category, we would use two variations: the multinomial and the Bernoulli Na\u0131 \ufffd ve Bayes classifier. The Multinomial Na\u0131 \ufffd ve Bayes classifier is used when the number of occurrences of a word in our prediction seems to have a greater meaning for classifying apps into categories. If the word \"fun\" occurs several times in the category of games and occurs very rarely in the category of education, it turns out that the word \"fun\" is more important and less important for the category of education for the category of games. Alternatively, if the word \"fun\" does not occur in the category of business, it is assumed that \"fun\" cannot be used to classify an app in the category of business because it does not occur there."}, {"heading": "IV. DATASET OVERVIEW", "text": "The record used in this project is the metadata of 1,197.995 of 1,390,545 apps after bad data was filtered out. This record is a CSV file with apps that were extracted from the Google Play Store with GooglePlayStoreCrawler5 as of June 2015. These data contain the following attributes: http: / / nlp.stanford.edu / IR-book / html / htmledition / the-bernoullimodel-1.html5GooglePlayStoreCrawler: https: / github.com / MarcelloLins / GooglePlayAppsCrawlerAppName, Develktop Developer, DeveloperURL, DeveloperNormalizedDomain, Category, IsFree, Price, Reviewer, Score.Count / GooglePlayAppsCrawlerAppName, Developer, Developeration, Develktop URL, DeveloperationApps, Develktop Domain, Category, Free, Desktop, Applicator, Desktop, Desktop, Desktop, Application, Desktop, Desktop, Application, Desktop, Desktop, Desktop, Application, Desktop, Desktop, Application, Desktop, Desktop, Desktop, Application, Counter, Count, name, Cooretop, Crawler.top, Scoretop, Scoretop, Application, Scoretop, Application,.Name, Scoretop, Scoretop, Application, Desktop, Desktop"}, {"heading": "A. Feature Processing", "text": "To create a precise predictive model, we selected five attributes from all the applications; these are used to extract our characteristics for each category later. These attributes include: \u2022 AppName \u2022 ContentRating \u2022 IsFree \u2022 HaveInAppPurchases \u2022 DescriptionWith the bag-of-words model, we extracted tokens from our 10,369 apps after combining these attributes, and removed stopwords such as \"the, as, is, who, on,..\" by removing numbers, punctuation, and the placement of all words in lowercase. At this point, we have 113,463 characteristics extracted from 10,369 app details using Term Frequency-Inverse Document Frequency (TF-IDF) statistics. TF-IDF allows us to reduce the impact of frequently occurring tokens so that they do not affect the characteristics that occur in small amounts of Document Frequency (TF-IDF), which is used as formula for calculating T6."}, {"heading": "B. Further Processing", "text": "In this process, the TF-IDF method was reused to reduce the number of features from 113,463 to 14,571 by removing words that occur in more than 70% and words that occur in less than 0.05% of all 10,369 apps. In addition, apps were grouped into two filters, as shown below: 1) All apps: All apps contain all 10,369 apps with 14,571 optimal features after pre-processing. 2) Filtered apps: Filtered apps contain 8,366 apps after removing apps with little description (less than a single paragraph), ensuring that we remove apps that are not descriptive enough. A descriptive app consists of about four to six sentences that contain about 100 words 6. In addition, categories are poorly supported, i.e. categories with very few apps have been removed."}, {"heading": "C. Prior and Likelihood Formation", "text": "s ascMAP = argmax c-CP (app | c) P (c) = argmax c-CP (w0, w1, w2,..., wn | c) P (c). For example, the prediction for the category \"BUSINESS\" is calculated as follows: P (b) = Nb N = 158 10369 = 0.0156 paragraph Length: https: / / strainindex.wordpress.com / 2010 / 10 / 25 / plainparagraph-length / The probability using multinomial Na\u00efve Bayes can be calculated as follows: P (w | b) = count (w, b) count (b), w-W (W), W-W (W) | W-W"}, {"heading": "V. RESULTS", "text": "The algorithm was implemented using Python and the \"sklearn\" 7 library for machine learning. Sklearn is a Python machine learning library based on NumPy, SciPy and matplotlib 8, which allows us to perform data mining and data analysis such as clustering, regression analysis, classification and pre-processing of data."}, {"heading": "A. Evaluating the model", "text": "Other evaluation strategies included testing the model with the training and test data, using a confusion matrix to determine true positives / negatives and false positives / negatives, and showing a learning curve to see how the model increases the training and test categories and develops recursive functionality to see how the model works in eliminating features for each iteration.The following highlights how this classifier uses these methods with the AllCategories Apps and GroupedGameApps as described in the previous paragraph.AllCategories represents 41 categories out of 10,369 apps, and the GroupedGameApps represents 25 categories out of 10,369 applications when all games are grouped as a single category.1) Training and test data collection: The data was divided into training and test data sets ranging from approximately 80% to 20%."}, {"heading": "VI. DISCUSSION", "text": "The results show that Multinomial Na\u0131 \ufffd ve Bayes performs better than the Bernoulli Na\u0131 \ufffd ve Bayes algorithm; this is because the number of occurrences of a word plays a large role in our classification problem. In Bernoulli Na\u0131 \ufffd ve Bayes, the absence of a word is significant, and although the results are not as efficient as the Multinomial Na\u0131 \ufffd ve Bayes, it may play a role in classifying other problems, such as spam detection in emails. The overall result shows that the algorithm is weak in classifying games because the similarity in words used to describe most gaming applications in different categories is weak. It can also be observed that the classifier performs better than others in classifying gaming apps in completely different categories."}, {"heading": "VII. FUTURE WORK", "text": "Na\u0131 \ufffd ve Bayes assumes that the functions are independent, and this is probably not the case in many scenarios. There are still several aspects that need to be improved, and it is important to note that Na\u0131 \ufffd ve Bayes is relatively efficient in document classification as used in our app categorization, but this can be further improved by using more advanced algorithms."}, {"heading": "A. N-gram usage", "text": "For example, when using 2 grams, \"alarm clocks,\" \"angry birds,\" and \"music boxes\" can improve results because these phrases specifically describe a feature. Multiple n-grams can be combined like a one-off, 2-gram, or 3-gram to see how precision can be improved with phrases and individual words using natural language processing techniques."}, {"heading": "B. Other Machine Learning algorithms", "text": "Advanced classification algorithms such as Support Vector Machines (SVM), Hidden Markov Models (HMM), and Decision Trees can further improve the accuracy of categorization, and while computationally intensive, these algorithms can perform better than the Na\u0131 \ufffd ve Bayes classifier."}, {"heading": "VIII. SUMMARY", "text": "Although the error rates are higher than for the classification of apps in other categories, it turns out that most of the misclassifications that occur in the \"games\" category do not necessarily mean that these apps do not belong to the predicted category. For example, an app published in an \"arcade game\" category may also be suitable in an \"action game\" category depending on the type of game. Furthermore, the training data used is not fundamentally a complete proof of how apps are classified. Furthermore, it can be noted that multinomial Na\u00efve Bayes perform better in text classification than the Bernoulli Naive Bayes classifier, where the number of occurrences of a word is important. In general, proper data collection, optimization and a large training set can significantly improve the performance of a classification algorithm for machine learning."}], "references": [{"title": "Artificial Intelligence: A Modern Approach, 3rd edition", "author": ["S. Russell", "P. Norvig"], "venue": "Pearson Education Limited,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2009}, {"title": "On The Naive Bayes Model for Text Categorization", "author": ["S. Eyheramendy", "D.D. Lewis", "D. Madigan"], "venue": "Proceedings Artificial Intelligence & Statistics, 2003, pp. 3\u201310.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2003}, {"title": "Naive Bayes for Text Classification with Unbalanced Classes", "author": ["E. Frank", "R.R. Bouckaert"], "venue": "PKDD\u201906 Proceedings of the 10th European conference on Principle and Practice of Knowledge Discovery in Databases, vol. 4213, Berlin, Germany, 2006, pp. 503\u2013510.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Text Classification for Authorship Attribution Using Naive Bayes Classifier with Limited Training Data", "author": ["F. Howedi", "M. Mohd"], "venue": "Computer Engineering and Intelligent Systems, vol. 5, no. 14, pp. 48\u201356, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Is Na\u0131\u0308ve bayes a good classifier for document classification?", "author": ["S.L. Ting", "W.H. Ip", "A.H.C. Tsang"], "venue": "International Journal of Software Engineering and its Applications,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2011}, {"title": "Fast and Accurate Sentiment Classification Using an Enhanced Naive Bayes Model", "author": ["V. Narayanan", "I. Arora", "A. Bhatia"], "venue": "Intelligent Data Engineering and Automated Learning \u2013 IDEAL 2013. Springer Berlin Heidelberg, 2013, vol. 8206, pp. 194\u2013201.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Estimation of prediction error by using K-fold crossvalidation", "author": ["T. Fushiki"], "venue": "Statistics and Computing, vol. 21, no. 2, pp. 137\u2013146, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "RANDOM PERMUTATION TESTING IN MULTIPLE LINEAR REGRESSION", "author": ["M.-H. Huh", "M. Jhun"], "venue": "Communications in Statistics - Theory and Methods, vol. 30, no. 10, pp. 2023\u20132032, aug 2001.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2023}, {"title": "Recursive feature elimination with random forest for PTR-MS analysis of agroindustrial products", "author": ["P.M. Granitto", "C. Furlanello", "F. Biasioli", "F. Gasperi"], "venue": "Chemometrics and Intelligent Laboratory Systems, vol. 83, no. 2, pp. 83\u201390, 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Introduction to Machine Learning", "author": ["E. Alpaydin"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Can structural MRI aid in clinical classification? A machine learning study in two independent samples of patients with schizophrenia, bipolar disorder and healthy subjects", "author": ["H.G. Schnack", "M. Nieuwenhuis", "N.E. van Haren", "L. Abramovic", "T.W. Scheewe", "R.M. Brouwer", "H.E. Hulshoff Pol", "R.S. Kahn"], "venue": "NeuroImage, vol. 84, pp. 299\u2013306, jan 2014.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Using supervised learning to classify clothing brand styles", "author": ["C. David Kreyenhagen", "T.I. Aleshin", "J.E. Bouchard", "A.M.I. Wise", "R.K. Zalegowski"], "venue": "2014 Systems and Information Engineering Design Symposium (SIEDS). Charlottesville, VA, USA: IEEE, apr 2014, pp. 239\u2013243.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Sentiment Analysis on Unstructured Review", "author": ["R. Nithya", "D. Maheswari"], "venue": "2014 International Conference on Intelligent Computing Applications. Coimbatore, India: IEEE, mar 2014, pp. 367\u2013371.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic Indexing: An Experimental Inquiry", "author": ["M.E. Maron"], "venue": "Journal of the ACM, vol. 8, no. 3, pp. 404\u2013417, jul 1961.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1961}, {"title": "Machine learning for information extraction in informal domains", "author": ["D. Freitag"], "venue": "Machine learning, vol. 39, no. 2-3, pp. 169\u2013202, 2000.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2000}, {"title": "Naive Bayes Modeling with Proper Smoothing for Information Extraction", "author": ["Zhenmei Gu", "N. Cercone"], "venue": "2006 IEEE International Conference on Fuzzy Systems. Vancouver, BC, Canada: IEEE, 2006, pp. 393\u2013400.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Medical Data Classification with Naive Bayes Approach", "author": ["K. Al-Aidaroo", "A. Bakar", "Z. Othman"], "venue": "Information Technology Journal, vol. 11, no. 9, pp. 1166\u20131174, sep 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A lexicon pool augmented Naive Bayes Classifier for Nepali Text", "author": ["S.K. Thakur", "V.K. Singh"], "venue": "2014 Seventh International Conference on Contemporary Computing (IC3). Noida, India: IEEE, aug 2014, pp. 542\u2013546.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "An Effective Algorithm for Improving the Performance of Naive Bayes for Text Classification", "author": ["Guo Qiang"], "venue": "2010 Second International Conference on Computer Research and Development, no. 1. Kuala Lumpur, Malaysia: IEEE, 2010, pp. 699\u2013701.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Low Cost Portability for Statistical Machine Translation based on N-gram Frequency and TF-IDF", "author": ["M. Eck", "S. Vogel", "A. Waibel"], "venue": "IInternational Workshop on Spoken Language Translation, IWSLT 2005, Pittsburgh, PA, USA, 2005, pp. 61\u201367.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "An Approach to Spam Detection by Naive Bayes Ensemble Based on Decision Induction", "author": ["Z. Yang", "X. Nie", "W. Xu", "J. Guo"], "venue": "Sixth International Conference on Intelligent Systems Design and Applications, vol. 2. Jinan, China: IEEE, oct 2006, pp. 861\u2013866.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning Weighted Naive Bayes with Accurate Ranking", "author": ["H. Zhang", "Shengli Sheng"], "venue": "Fourth IEEE International Conference on Data Mining (ICDM\u201904). IEEE, 2004, pp. 567\u2013570.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Bug report, feature request, or simply praise? On automatically classifying app reviews", "author": ["W. Maalej", "H. Nabil"], "venue": "2015 IEEE 23rd International Requirements Engineering Conference (RE). Ottawa, ON: IEEE, aug 2015, pp. 116\u2013125.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Numeric rating of Apps on Google Play Store by sentiment analysis on user reviews", "author": ["M.R. Islam"], "venue": "2014 International Conference on Electrical Engineering and Information & Communication Technology. Dhaka, Bangladesh: IEEE, apr 2014, pp. 1\u20134.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparing naive Bayes, decision trees, and SVM with AUC and accuracy", "author": ["J. Huang", "J. Lu", "C. Ling"], "venue": "Third IEEE International Conference on Data Mining. IEEE Comput. Soc, 2003, pp. 553\u2013556.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "These learning systems as described by [1] are:", "startOffset": 39, "endOffset": 42}, {"referenceID": 0, "context": "This way, the computer learns from the mistakes it has previously made and from the reward it gets from achieving a particular goal [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "There is an increasing number of research using machine learning to classify text, sentiment analysis and documents with Na\u0131\u0308ve Bayes such as [2]\u2013[5].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "There is an increasing number of research using machine learning to classify text, sentiment analysis and documents with Na\u0131\u0308ve Bayes such as [2]\u2013[5].", "startOffset": 146, "endOffset": 149}, {"referenceID": 4, "context": "businesses [5].", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": "Since most machine learning classification algorithms are time-consuming and complicated, using Na\u0131\u0308ve Bayes classification provides a fast and simple way to classify data [6].", "startOffset": 172, "endOffset": 175}, {"referenceID": 6, "context": "These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].", "startOffset": 61, "endOffset": 64}, {"referenceID": 7, "context": "These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].", "startOffset": 207, "endOffset": 210}, {"referenceID": 8, "context": "These validation methods include the k-fold cross-validation [7], shuffle-split cross-validation used to generate a learning curve that determines the training and test scores for various training data size [8] and the recursive feature elimination for testing the number of features that produces the best results [9].", "startOffset": 315, "endOffset": 318}, {"referenceID": 9, "context": "As described by [10] classification is an example of supervised learning, where a", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "[11] using machine learning to classify patients with schizophrenia, bipolar disorder and healthy subjects with their structural MRI scans.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "There has also been similar research in product classification using SVM, for example [12] shows that SVM adds value to the classification of fashion brands in their research thereby making it easy for users to narrow down their searches when looking for a particular product.", "startOffset": 86, "endOffset": 90}, {"referenceID": 12, "context": "Other research such as sentiment analysis done by [13] used Na\u0131\u0308ve Bayes algorithm to classify the most identified features in an unstructured review and determine polarity distribution in terms of being positive, negative and neutral.", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "The Naive Bayes machine learning model [14] is a popular statistical learning system that has been successful in many applications where features are independent of each other.", "startOffset": 39, "endOffset": 43}, {"referenceID": 14, "context": "One of the earliest application of this model to Information Extraction was done by [15].", "startOffset": 84, "endOffset": 88}, {"referenceID": 15, "context": "[16] proposed a smoothing strategy using Na\u0131\u0308ve Bayes for Information Extraction.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[17] compared Na\u0131\u0308ve Bayes with other classification algorithms for a medical dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "An example of this is the modified Na\u0131\u0308ve Bayes algorithm proposed by [18] to improve the classification of Nepali texts.", "startOffset": 70, "endOffset": 74}, {"referenceID": 18, "context": "Another example of improving the Naive Bayes algorithm is the improved Naive Bayes probabilistic model-Multinomial Event Model for text classification by [19].", "startOffset": 154, "endOffset": 158}, {"referenceID": 18, "context": "independently even though multiple occurrences of the same word in a document are not necessarily independent [19] and Multinomial Na\u0131\u0308ve Bayes does not account for this occurrence.", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "Other weighting schemes in text classification involve the use of N-grams, which is a sequence of n-items from a given document or text and (Term Frequency-Inverse Document Frequency) TF-IDF, which shows how important a word is in a document or a given set of documents [20].", "startOffset": 270, "endOffset": 274}, {"referenceID": 20, "context": "[21] proposed a Na\u0131\u0308ve Bayes spam detection method based on decision trees, they also presented an improved method based on classifier error weight.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Their experimentation shows that the implementation is valid, but there are not many solutions for valid incremental decision tree induction algorithm as they described [21].", "startOffset": 169, "endOffset": 173}, {"referenceID": 21, "context": "Another use of Na\u0131\u0308ve Bayes is that proposed by [22] for ranking.", "startOffset": 48, "endOffset": 52}, {"referenceID": 21, "context": "Their results show that the weighted Na\u0131\u0308ve Bayes outperforms the standard Na\u0131\u0308ve Bayes, and both the weighted Na\u0131\u0308ve Bayes and the standard Na\u0131\u0308ve Bayes are better in performance than the decision tree algorithm [22].", "startOffset": 213, "endOffset": 217}, {"referenceID": 22, "context": "There has been a few research done on Google Play Store using sentiment analysis applied to customer reviews on mobile apps to determine their polarity such as [23], where app reviews were automatically classified and result compared with other classification methods.", "startOffset": 160, "endOffset": 164}, {"referenceID": 23, "context": "Additionally, [24] used sentiment analysis on customer reviews on the store.", "startOffset": 14, "endOffset": 18}, {"referenceID": 18, "context": "Although the authors did not use Na\u0131\u0308ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Na\u0131\u0308ve Bayes can also be applied in classifying customer reviews on Google Play Store.", "startOffset": 90, "endOffset": 94}, {"referenceID": 20, "context": "Although the authors did not use Na\u0131\u0308ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Na\u0131\u0308ve Bayes can also be applied in classifying customer reviews on Google Play Store.", "startOffset": 96, "endOffset": 100}, {"referenceID": 22, "context": "Although the authors did not use Na\u0131\u0308ve Bayes for analyzing the reviews, research such as [19], [21] and [23] show that machine learning using Na\u0131\u0308ve Bayes can also be applied in classifying customer reviews on Google Play Store.", "startOffset": 105, "endOffset": 109}, {"referenceID": 24, "context": "Since the classification of apps will be done on a regular computer, Na\u0131\u0308ve Bayes is most efficient in terms of CPU and memory consumption as described in [25].", "startOffset": 155, "endOffset": 159}, {"referenceID": 19, "context": "The formula used to calculate TF-IDF is given as (6) as described in [20].", "startOffset": 69, "endOffset": 73}], "year": 2016, "abstractText": "There are over one million apps on Google Play Store and over half a million publishers. Having such a huge number of apps and developers can pose a challenge to app users and new publishers on the store. Discovering apps can be challenging if apps are not correctly published in the right category, and, in turn, reduce earnings for app developers. Additionally, with over 41 categories on Google Play Store, deciding on the right category to publish an app can be challenging for developers due to the number of categories they have to choose from. Machine Learning has been very useful, especially in classification problems such sentiment analysis, document classification and spam detection. These strategies can also be applied to app categorization on Google Play Store to suggest appropriate categories for app publishers using details from their application. In this project, we built two variations of the Na\u0131\u0308ve Bayes classifier using open metadata from top developer apps on Google Play Store in other to classify new apps on the store. These classifiers are then evaluated using various evaluation methods and their results compared against each other. The results show that the Na\u0131\u0308ve Bayes algorithm performs well for our classification problem and can potentially automate app categorization for Android app publishers on Google Play Store.", "creator": "LaTeX with hyperref package"}}}