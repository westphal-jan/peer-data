{"id": "1608.08589", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Aug-2016", "title": "Game-Theoretic Modeling of Driver and Vehicle Interactions for Verification and Validation of Autonomous Vehicle Control Systems", "abstract": "Autonomous driving has been the subject of increased interest in recent years both in industry and in academia. Serious efforts are being pursued to address legal, technical and logistical problems and make autonomous cars a viable option for everyday transportation. One significant challenge is the time and effort required for the verification and validation of the decision and control algorithms employed in these vehicles to ensure a safe and comfortable driving experience. Hundreds of thousands of miles of driving tests are required to achieve a well calibrated control system that is capable of operating an autonomous vehicle in an uncertain traffic environment where multiple interactions between vehicles and drivers simultaneously occur. Traffic simulators where these interactions can be modeled and represented with reasonable fidelity can help decrease the time and effort necessary for the development of the autonomous driving control algorithms by providing a venue where acceptable initial control calibrations can be achieved quickly and safely before actual road tests. In this paper, we present a game theoretic traffic model that can be used to 1) test and compare various autonomous vehicle decision and control systems and 2) calibrate the parameters of an existing control system. We demonstrate two example case studies, where, in the first case, we test and quantitatively compare two autonomous vehicle control systems in terms of their safety and performance, and, in the second case, we optimize the parameters of an autonomous vehicle control system, utilizing the proposed traffic model and simulation environment.", "histories": [["v1", "Tue, 30 Aug 2016 18:25:35 GMT  (1968kb,D)", "http://arxiv.org/abs/1608.08589v1", "13 pages, 16 figures"]], "COMMENTS": "13 pages, 16 figures", "reviews": [], "SUBJECTS": "cs.AI cs.GT", "authors": ["nan li", "dave oyler", "mengxuan zhang", "yildiray yildiz", "ilya kolmanovsky", "anouck girard"], "accepted": false, "id": "1608.08589"}, "pdf": {"name": "1608.08589.pdf", "metadata": {"source": "CRF", "title": "Game-Theoretic Modeling of Driver and Vehicle Interactions for Verification and Validation of Autonomous Vehicle Control Systems", "authors": ["Nan Li", "Dave Oyler", "Mengxuan Zhang", "Yildiray Yildiz", "Ilya Kolmanovsky", "Anouck Girard"], "emails": [], "sections": [{"heading": null, "text": "In fact, most of them will be able to play by the rules they have set for the future."}, {"heading": "II. PROBLEM DEFINITION", "text": "The problem we are dealing with is the modeling of the behavior of drivers in a traffic scenario in which the cars are driven on a three-lane highway. Later, we show that such models can be used in simulators to evaluate the policy of autonomous vehicle control. Fig. 1 shows an example scenario with 6 cars. Note that this is not a limitation of the proposed method and that scenarios can be handled with more cars and lanes. Simulated cars are assumed to drive in the same direction and are driven by human drivers who comply with general traffic laws."}, {"heading": "A. Physical models", "text": "The discrete time equations of motion for each vehicle during forward movement and lane change are given by x (t + 1) = x (t) + vx (t) \u2206 t, vx (t + 1) = vx (t) + a (t) \u2206 t, (1) y (t + 1) = y (t) + vy (t) \u2206 t, where x, y are the vehicle length or lateral position vx and vy are the vehicle length or lateral speeds. We assume that all cars with either \u00b1 a1 [m / s2] or \u00b1 a2 [m / s2] accelerate and slow down. The nominal values, \u00b1 a1 [m / s2], reflect the acceleration / deceleration of a human vehicle with \u00b1 a1 [m / s2] each occurring at a specific lane change or at a constant lane change."}, {"heading": "B. Observation space", "text": "In real traffic flow, a driver cannot observe or process all information about all cars on the road. A person may be able to observe and use the information he receives from cars in a particular proximity to his own. In particular, a human driver can barely measure his exact distances to other cars. He / she can only estimate the distances and indicate them as \"near,\" \"far,\" etc. Therefore, we assign the following observation space to drivers: 1) The distance from our car to the car directly in front of him, called range, quantified as \"near\" (distance \u2264 dc [m]), \"nominal\" (distance \u2264 dc [m] < distance \u2264 df [m]) or \"far\" (distance > df [m]), the distance to the car in the front \"left lane,\" quantified as \"near,\" \"nominal\" or \"far,\" 3) The distance to the car in the front \"right lane.\""}, {"heading": "C. Action space", "text": "Drivers have 7 basic measures at their disposal: 1) \"Keep lane and speed,\" 2) \"Accelerate\" at speed = a1 [m / s2], provided speed does not exceed vmax [km / h], 3) \"Accelerate\" at speed = \u2212 a1 [m / s2], provided speed exceeds vmin [km / h], 4) \"Accelerate hard\" at speed = a2 [m / s2], provided speed does not exceed vmax [km / h], 5) \"Accelerate hard\" at speed = \u2212 a2 [m / s2], provided speed exceeds vmin [km / h], 6) Change lane to the left, provided there is a lane on the left, 7) Change lane to the right, provided there is a lane on the right."}, {"heading": "D. Reward function", "text": "A \"reward function\" is a mathematical representation of a driver's goals. The basic goals of drivers in real traffic are 1) not to have an accident, such as a car accident (safety), 2) to minimize the time it takes to reach the goal (performance), 3) to avoid a reasonable path out of the previous cars (comfort). These goals can be reflected in the reward function, the asR = w1c + w3h + w4e, (2) where wi, 2, 3, 4, are the weights for each term and c, h and e represent \"speed limitation,\" \"velocity\" and \"effort\" metrics. \""}, {"heading": "E. Constraints", "text": "The reward function (2) already reflects the penalty for breaches of distance restrictions, which can be regarded as soft control restrictions. In some combinations of states and actions that obviously result in breaches of these restrictions, we can also impose tough restrictions to avoid the occurrence of such combinations. In particular, we introduce the following tough restrictions that make certain actions impossible in certain situations: 1) If a car is in the left lane in a parallel position, the controlled car cannot change lanes to the left, 2) If a car is in the right lane in a parallel position, the controlled car cannot change lanes to the right, 3) If a car is in the left lane \"close\" and \"close,\" the controlled car cannot change lanes to the left, 4) If a car is in the right lane \"close\" and \"close,\" the controlled car cannot change lanes to the right."}, {"heading": "III. DRIVER INTERACTION MODEL", "text": "The driver-interaction model developed in this study enables the modelling of the interaction between driver and driver and autonomous vehicle through hierarchical decision-making and reinforcement learning. The model is a \"policy,\" i.e. a stochastic map of the driver's observation space to his action space (see Section II). In other words, this map assigns a probability distribution of possible actions for each observation. In the following, we explain how this model is generated."}, {"heading": "A. Hierarchical decision making", "text": "The interaction model developed is based on the idea that intelligent agents (such as drivers) have different levels of thinking. According to this observation, a Level 0 agent does not take into account the likely actions of other agents with whom he interacts, but behaves in a knee-jerk manner. For example, if a driver applies emergency braking when he observes an obstacle on the road, without taking into account how the following car would react to this sudden deceleration, this behavior is referred to as Level 0 behavior and the driver is referred to as a Level 0 driver. On the other hand, if this driver assumes that the next car is being driven by a Level 0 driver who would apply emergency braking in the event of an obstacle, which may not be sufficient to avoid a collision, and therefore decides to change lanes to avoid this collision, that driver is referred to as a Level 1 driver."}, {"heading": "B. Reinforcement learning to solve the Partially Observable Markov Decision Process", "text": "The problem dealt with in this paper is a multi-agent decision problem. We use a Reinforcement Learning (RL) algorithm to determine the guidelines for each agent based on the reward function defined in Section II-D. To achieve the maximum reward, the RL algorithm uses two steps, including 1) \"reward evaluation,\" in which the state action pairs are mapped based on the cumulative reward values they win, and 2) \"policy improvement,\" in which the probability of selecting actions that have higher reward values is increased. For more details on RL, see [31]. Conventional RL algorithms require the process to reach Markov for convergence guarantees. Note: Although the underlying dynamics of the highway problem is investigated, each agent (driver) can only observe a sub-space of the entire state (see Section II-POB) and therefore a partially observable POB problem must be solved."}, {"heading": "C. The role of hierarchical decision making in obtaining driver policies", "text": "The process of obtaining driver guidelines is referred to as \"training,\" in which the trained driver is a learner and the other vehicles and automation constitute the environment.During the training process, the environmental model is required to achieve state transitions as a result of driver actions, where the hierarchical decision-making approach plays a crucial rule: when training a level-k driver policy, a level-1 policy is assigned to all traffic except the trained driver.The process begins with the definition of a level-0 policy (see Section III-A), which is the lowest level where drivers do not consider interaction with other drivers and do not explicitly consider their eventual actions.Once a level-0 policy is established, the RL algorithm is applied by assigning level-0 guidelines to all vehicles, except the vehicle being trained. At the end of the training process, a level-1 policy of similar nature is achieved during training."}, {"heading": "IV. AUTONOMOUS DRIVING CONTROL APPROACHES", "text": "The proposed traffic model was used to build a simulator to test and evaluate the performance of autonomous driving control algorithms. As concrete examples, two approaches to autonomous driving are evaluated and compared based on Stackelberg strategies and decision trees, using a simulator in which traffic, unlike the controlled vehicle, consists of drivers modelled according to our game theory strategies. In this section, the control algorithms to be tested are briefly explained and in the next section, simulation assessments are provided. Stackelberg strategies and the decision tree strategies compared in this study were originally developed in [10], [15] and [16]. As these strategies were developed under assumptions that represent a simpler traffic environment, some necessary changes were made to enable the autonomous vehicle that will apply these strategies to operate in the traffic environment investigated in this study."}, {"heading": "A. Stackelberg policies", "text": "In order to generate a Stackelberg policy for the autonomous vehicle, we consider three vehicles as actors, and the rest of the vehicles are considered as the environment. The three actors are assigned roles as \"leader,\" \"first runner,\" and \"second runner,\" and they choose their actions one by one: the leader chooses his action first, followed by the first runner, and finally the second runner. Each actor evaluates his actions based on a utility function consisting of two parts. The first part, known as positive benefit, is defined as: Upos = {min (d4, dv) if there is a vehicle in front of him, dv, otherwise (10), where d4 represents the distance to the car directly in front of him, i.e. the forward movement and dv the maximum visibility. The second part of the utility vehicle is known as the negative benefit: Uneg = d \u2212 vrT \u2212 dmin, (11) where d \u2212 and vr is the distance to the road and the relative speed of the car directly behind the vehicle."}, {"heading": "B. Decision tree policies", "text": "In the Decision Tree approach for autonomous driving, the actions of the vehicle are determined by a route planner who evaluates a predetermined number of pre-selected action profiles by building a tree of potential action profiles and evaluating each branch according to a specified measurement parameter. In this essay, the decision tree consists of two levels, each level allowing the seven actions listed in Section II-C. Thus, the action profiles consist of two actions each, evaluating 49 profiles. The evaluation metric is based on the reward function (2), which is also used for training the level guidelines. Specifically, the total reward is calculated as the weighted sum of the rewards received from the two levels: Rtotal = wl1Rl1 + wl2Rl2, (13) where wl1, wl2 and R + are the weighting terms. The car applies the first shift action of the profile, which has the highest overall reward of all profiles, repeated in each step of this process and R +."}, {"heading": "C. Path planner triggering threshold", "text": "Both the Stackelberg and the Decision Tree are used as path planners and only triggered when necessary and advantageous. If the policy is not triggered, the vehicle follows a predefined driving pattern. Specifically, the driving algorithm can be explained using Fig. 4 as follows: 1) \"Accelerate\" when there are no other cars in region A, 2) The path planner is triggered when there are cars in region A but no cars in region B, 3) \"Safe Mode,\" which is the same as the Eleven 0 policy in this paper, is applied when there are cars in region B. It is clear that if there are no cars in region A, the autonomous vehicle can safely accelerate, while the autonomous vehicle, if there are cars in region B, should slow down to keep a reasonable distance. Activation logic is designed to increase safety, and 2) reduce costs by avoiding unnecessary measures."}, {"heading": "V. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Environment and set-up", "text": "We model the environment as follows: The width of a lane is 3.6 [m], and the safety zones around the cars that should not be violated are modeled as 6 [m] -2 [m] boxes. Cars always drive in the middle of a lane, unless they change lanes. Cars only accelerate or decelerate at \u00b1 a1 = \u00b1 2.5 [m / s2] or \u00b1 a2 = \u00b1 a2 = 5 [m / s2], and lane changes occur at a constant lateral speed, so that the total time to change lanes is tcl = 2 [s]. During lane changes, the longitudinal lane remains constant and as soon as a lane change begins, it is progressively brought to completion. Longitudinal direction is called x, and its origin is colored with the car to be trained or evaluated.To configure the simulation, the following values must be specified: 1) the number of cars, nc), c) the distance allowed, c) and (3) the distance allowed."}, {"heading": "B. Level-0 driver behavior", "text": "In this section, we present simulation results to illustrate the driving behaviour of a Level 0 car. In addition, we present the user interface of the simulator. In Fig. 6, the red car in the middle is the teaching / test vehicle, while the yellow cars represent the traffic environment. The red arrow in front of the red car indicates its direction of travel, and the size of the arrow indicates how fast the car is moving. The panel on the left is a speedometer, and the steering wheel on the right indicates the lateral movement of the car. The green field and the red field in the middle indicate the accelerator or brake pedal, and when one of them turns blue, this indicates that the pedal is pressed. The coordinate axis is fixed on the test car and the movements of the other cars can be tracked based on their relative distance from the red car. In Fig., the yellow car steps directly in front of the red car (\"wide\"). \"In this car, the accelerator pedal of Level b and the accelerator pedal of Level 6 is not stable\" c \"at the stage.\""}, {"heading": "C. Training process", "text": "This year it is more than ever before."}, {"heading": "E. A comparative quantitative evaluation of Stackelberg and decision tree policies", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "F. Optimal autonomous driving controller calibration", "text": "One of the possible applications of the proposed traffic modeling approach is the calibration of parameter values in the autonomous driving algorithms. We illustrate this with the Decision Tree Policy as an example. The optimization goal is defined by taking into account safety and performance: The goal is to maximize the following reward rate: Robj = p1 (\u2212 c) + p2 vmax \u2212 vmin \u2212 vmin, (16) where the weights p1 and p2 are determined by the user, c) is the condition violation rate defined in the figure. 11, v \u00b2 x is the average speed during the 200 [s] simulations, and vmin and vmax are the lower and upper limits of the test vehicle. This reward function is designed so that each of its terms is dimensioned."}, {"heading": "VI. SUMMARY", "text": "This paper presents a hierarchical approach to modelled interaction of driving behaviour in traffic, based on the theory of the brain game. It offers an approach to simulating interactive driving behaviour under the given traffic conditions. A traffic simulator has been developed using Level-k driving models. It can be used to test and verify algorithms of autonomous driving, as well as to discover challenging trajectories and scenarios that can facilitate the testing of future autonomous vehicles. To illustrate the use of the simulator, we have defined and tested two strategies for controlling autonomous vehicles in terms of safety and performance."}], "references": [{"title": "Autonomous driving in urban environments: approaches, lessons and challenges", "author": ["M. Campbell", "M. Egerstedt", "J.P. How", "R.M. Murray"], "venue": "Philosophical Transactions of the Royal Society of London A: Mathematical, Physical and Engineering Sciences, vol. 368, no. 1928, pp. 4649\u20134672, 2010.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1928}, {"title": "Autonomous vehicle technology: A guide for policymakers", "author": ["J.M. Anderson", "N. Kalra", "K.D. Stanley", "P. Sorensen", "C. Samaras", "O.A. Oluwatola"], "venue": "RAND Corporation, Santa Monica, CA, Research Report, 2016. Available at http:www.rand.orgpubsresearch reportsRR443- 2.html.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Australian innovation", "author": ["N. Kalra. (2016", "Feb. 01"], "venue": "[Weblog entry], THERANDBLOG, Available: http:www.rand.orgblog201602withdriverless-cars-how-safe-is-safe-enough.html [Apr 03, 2016].", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Formal verification of an autonomous vehicle system", "author": ["T. Wongpiromsarn", "R.M. Murray"], "venue": "Conference on Decision and Control, May, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Periodically controlled hybrid systems: Verifying a controller for an autonomous vehicle", "author": ["T. Wongpiromsarn", "S. Mitra", "R.M. Murray", "A. Lamperski"], "venue": "Proceedings of the HSCC: Hybrid Systems, Computation and Control Conference. California Institute of Technology, 2008.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Verified hybrid controllers for automated vehicles", "author": ["J. Lygeros", "D.N. Godbole", "S. Sastry"], "venue": "IEEE Transactions on Automatic Control, vol. 43, no. 4, pp. 522\u2013539, 1998.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Online verification of automated road vehicles using reachability analysis", "author": ["M. Althoff", "J.M. Dolan"], "venue": "IEEE Transactions on Robotics, vol. 30, no. 4, pp. 903\u2013918, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Automated driving: The role of forecasts and uncertainty-a control perspective", "author": ["A. Carvalho", "S. Lefevre", "G. Schildbach", "J. Kong", "F. Borrelli"], "venue": "European Journal of Control, vol. 24, pp. 14\u201332, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Team cornell\u2019s skynet: Robust perception and planning in an urban environment", "author": ["I. Miller", "M. Campbell", "D. Huttenlocher", "F.-R. Kline", "A. Nathan", "S. Lupashin", "J. Catlin", "B. Schimpf", "P. Moran", "N. Zych", "E. Garcia", "M. Kurdziel", "H. Fujishima"], "venue": "Journal of Field Robotics, vol. 25, no. 8, pp. 493\u2013527, 2008.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2008}, {"title": "A path planner for autonomous driving on highway human mimicry approach with binary decision diagrams", "author": ["L. Claussman", "A. Carvalho", "G. Schildbach"], "venue": "Proceedings of the European Control Conference, Linz, Austria, July 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic decision-making under uncertainty for autonomous driving using continuous pomdps", "author": ["S. Brechtel", "T. Gindele", "R. Dillmann"], "venue": "Proc. IEEE Intelligent Transportation Systems (ITSC), October 2014, pp. 392\u2013399.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Multipolicy decision-making for autonomous driving via changepoint-based behavior prediction", "author": ["E. Galceran", "A.G. Cunnigham", "R.M. Eustice", "E. Olson"], "venue": "Proceedings of Robotics: Science and Systems, Rome, Italy, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Predictive active steering control for autonomous vehicle systems", "author": ["P. Falcone", "F. Borrelli", "J. Asgari", "H.E. Tseng", "D. Hrovat"], "venue": "IEEE Transactions on Control Systems Technology, vol. 15, no. 3, pp. 566\u2013580, 2007.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Predictive control of an autonomous ground vehicle using linearization approach", "author": ["A. Carvalho", "Y. Gao", "A. Gray", "H.E. Tseng", "F. Borrelli"], "venue": "Proceedings of the 16th IEEE Annual Conference on Intelligent Transportation Systems, The Hague, The Netherlands, October 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Stackelberg game based model of highway driving", "author": ["J.H. Yoo", "R. Langari"], "venue": "Proc. ASME Dynamic Systems and Control Conference joint with JSME Motion and Vibration Conference, Fort Lauderdale, Florida, Oct. 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "A stackelberg game theoretic driver model for merging", "author": ["\u2014\u2014"], "venue": "Proc. ASME Dynamic Systems and Control Conference, Palo Alto, California, Oct. 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Lane keeping assistance with learning-based driver model and model predictive control", "author": ["S. Lefevre", "Y. Gao", "D. Vasquez", "E. Tseng", "R. Bajcsy", "F. Borrelli"], "venue": "Proceedings of the 12th international symposium on advanced vehicle control, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Autonomous car following: A learning-based approach", "author": ["S. Lefevre", "A. Carvalho", "F. Borrelli"], "venue": "IEEE Intelligent Vehicles Symposium, 2015, pp. 920\u2013926.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Safe semi-autonomous control with enhanced driver modeling", "author": ["R. Vasudevan", "V. Shia", "Y. Gao", "R. Cervera-Navarro", "R. Bajcsy", "F. Borrelli"], "venue": "Proc. Amer. Control Conf., 2012, pp. 2896\u20132903.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Semiautonomous vehicular control using driver modeling", "author": ["V. Shia", "Y. Gao", "R. Vasudevan", "K.D. Campbell", "T. Lin", "F. Borrelli", "R. Bajcsy"], "venue": "IEEE Transactions on Intelligent Transportation Systems, vol. 15, no. 6, pp. 2696\u20132709, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Toward an integrated model of driver behavior in cognitive architecture", "author": ["D. Salvucci", "E. Boer", "A. Liu."], "venue": "Transportation Research Record: Journal of the Transportation Research Board, vol. 1779, pp. 9\u201316, 2001.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Modelling lane changing and merging in microscopic traffic simulation", "author": ["P. Hidas"], "venue": "Transportation Research Part C: Emerging Technologies, vol. 10, no. 5, pp. 351\u2013371, 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Chapter: Game theoretic modeling of pilot behavior during mid-air encounters. in Decision making with multiple imperfect decision makers", "author": ["R. Lee", "D. Wolpert"], "venue": "Intelligent Systems Reference Library Series. Springer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Cyber-physical security: A game theory model of humans interacting over control systems", "author": ["S. Backhaus", "R. Bent", "J. Bono", "R. Lee", "B. Tracey", "D. Wolpert", "D. Xie", "Y. Yildiz"], "venue": "IEEE Transactions on Smart Grid, vol. 4, no. 4, pp. 2320\u20132327, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Game theory controller for hybrid electric vehicles", "author": ["C. Dextreit", "I.V. Kolmanovsky"], "venue": "IEEE Transactions on Control Systems Technology, vol. 22, no. 2, pp. 652\u2013663, 2014.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Predicting pilot behavior in medium-scale scenarios using game theory and reinforcement learning", "author": ["Y. Yildiz", "A. Agogino", "G. Brat"], "venue": "Journal of Guidance, Control, and Dynamics, vol. 37, no. 4, pp. 1335\u2013 1343, 2014.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "A game theoretical model of traffic with multiple interacting drivers for use in autonomous vehicle development", "author": ["D. Oyler", "Y. Yildiz", "A. Girard", "N.I. Li", "I. Kolmanovsky"], "venue": "Proc. Amer. Control Conf., Boston, MA, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical reasoning game theory based approach for evaluation and testing of autonomous vehicle control systems", "author": ["N. Li", "D. Oyler", "M. Zhang", "Y. Yildiz", "A. Girard", "I. Kolmanovsky"], "venue": "IEEE Conference on Decision and Control, Las Vegas, accepted for publication, 2016.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "On players models of other players: Theory and experimental evidence", "author": ["D. Stahl", "P. Wilson"], "venue": "Games and Economic Behavior, vol. 10, no. 1, p. 218254, 1995.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1995}, {"title": "Comparing models of strategic thinking in Van Huyck, Battalio, and Beil\u2019s coordination games", "author": ["M.A. Costa-Gomes", "V.P. Crawford", "N. Iriberri"], "venue": "Journal of the European Economic Association, vol. 7, no. 2-3, pp. 365\u2013376, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "Cambridge: MIT press,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1998}, {"title": "Reinforcement learning algorithm for partially observable markov decision problems", "author": ["T. Jaakkola", "P.S. Satinder", "I. Jordan."], "venue": "Advances in Neural Information Processing Systems 7: Proceedings of the 1994 Conference, 1994.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1994}, {"title": "Average reward reinforcement learning: Foundations, algorithms, and empirical results", "author": ["S. Mahadevan"], "venue": "Machine learning, vol. 22, no. 1-3, pp. 159\u2013195, 1996.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "ONE of the most significant challenges that must be addressed before autonomous cars can be deployed in mass production is the Verification and Validation (V&V) of their control systems in terms of safety and performance [1], [2].", "startOffset": 221, "endOffset": 224}, {"referenceID": 1, "context": "ONE of the most significant challenges that must be addressed before autonomous cars can be deployed in mass production is the Verification and Validation (V&V) of their control systems in terms of safety and performance [1], [2].", "startOffset": 226, "endOffset": 229}, {"referenceID": 2, "context": "It has been estimated that autonomous vehicles need to be driven 275 million miles without fatality to assure the same rate of reliability as existing human driven cars [3].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": ", see [4], [5], [6], [7]).", "startOffset": 6, "endOffset": 9}, {"referenceID": 4, "context": ", see [4], [5], [6], [7]).", "startOffset": 11, "endOffset": 14}, {"referenceID": 5, "context": ", see [4], [5], [6], [7]).", "startOffset": 16, "endOffset": 19}, {"referenceID": 6, "context": ", see [4], [5], [6], [7]).", "startOffset": 21, "endOffset": 24}, {"referenceID": 7, "context": "One common approach to the design of control systems for autonomous vehicles is to utilize a hierarchical control structure, wherein a higher level outer loop controller generates reference trajectories for the lower level inner loop controller which, in turn, determines the steering angle and acceleration/deceleration inputs required to follow the reference trajectory [8].", "startOffset": 372, "endOffset": 375}, {"referenceID": 8, "context": "Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers.", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers.", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers.", "startOffset": 162, "endOffset": 166}, {"referenceID": 11, "context": "Several control approaches have been proposed for autonomous vehicles including decision trees [9], [10], Partially Observable Markov Decision Processes (POMDPs) [11] and methods based on multi-policy decision making [12], that are mainly employed as outer loop controllers.", "startOffset": 217, "endOffset": 221}, {"referenceID": 12, "context": "For the inner loop, one of the most common approaches is based on Model Predictive Control (MPC) [13]-[14].", "startOffset": 97, "endOffset": 101}, {"referenceID": 13, "context": "For the inner loop, one of the most common approaches is based on Model Predictive Control (MPC) [13]-[14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Note that advanced driver behavioral models may also be used in the outer loop [15], [16], with the motivation that an autonomous vehicle should be able to drive at least as well as a human driver.", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "Note that advanced driver behavioral models may also be used in the outer loop [15], [16], with the motivation that an autonomous vehicle should be able to drive at least as well as a human driver.", "startOffset": 85, "endOffset": 89}, {"referenceID": 1, "context": "In fact, some experts have suggested that autonomous vehicles should be permitted on public roads only after it is proven that they are superior to human drivers [2].", "startOffset": 162, "endOffset": 165}, {"referenceID": 16, "context": "In [17] and [18], a Hidden Markov Model (HMM) based driver model is", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "In [17] and [18], a Hidden Markov Model (HMM) based driver model is", "startOffset": 12, "endOffset": 16}, {"referenceID": 18, "context": "In [19] and [20], k-means clustering is used to determine the driving mode and define an approach to predicting and overbounding future vehicle trajectories.", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "In [19] and [20], k-means clustering is used to determine the driving mode and define an approach to predicting and overbounding future vehicle trajectories.", "startOffset": 12, "endOffset": 16}, {"referenceID": 20, "context": "In [21], a \u201ccognitive architectures\u201d approach, which is \u201ca computational framework that incorporates builtin, well-tested parameters and constraints on cognitive and perceptual-motor processes,\u201d is utilized for driver modeling.", "startOffset": 3, "endOffset": 7}, {"referenceID": 21, "context": "In [22], lane change bear X iv :1 60 8.", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "The core ideas are synergistic with the framework of \u201csemi network-form games,\u201d [23], [24] and help us obtain the probable outcomes of a complex traffic scenario driven by multiple interactions.", "startOffset": 80, "endOffset": 84}, {"referenceID": 23, "context": "The core ideas are synergistic with the framework of \u201csemi network-form games,\u201d [23], [24] and help us obtain the probable outcomes of a complex traffic scenario driven by multiple interactions.", "startOffset": 86, "endOffset": 90}, {"referenceID": 14, "context": "Other game theoretic approaches, in particular, based on Stackelberg games, have been studied for application to vehicle highway driving problems in [15] and [16].", "startOffset": 149, "endOffset": 153}, {"referenceID": 15, "context": "Other game theoretic approaches, in particular, based on Stackelberg games, have been studied for application to vehicle highway driving problems in [15] and [16].", "startOffset": 158, "endOffset": 162}, {"referenceID": 24, "context": "The latter are considered in [25] for Hybrid Electric Vehicle (HEV) energy management where the driver and the powertrain are considered to be two players in a game.", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "Indeed, an implementation of the proposed approach for a 50 player game can be found in [26], and scenarios with up to 30 vehicles are handled in this paper.", "startOffset": 88, "endOffset": 92}, {"referenceID": 26, "context": "Preliminary results have appeared in conference papers [27], [28].", "startOffset": 55, "endOffset": 59}, {"referenceID": 27, "context": "Preliminary results have appeared in conference papers [27], [28].", "startOffset": 61, "endOffset": 65}, {"referenceID": 26, "context": "Differently from [27], [28], in this paper, a) we incorporate a more realistic action space including harder brakes and faster accelerations; b) we develop a more realistic traffic model with more representative distance constraint violation rates via improvements in the reinforcement learning procedure; and c) we demonstrate that optimal parameter values for an autonomous vehicle control algorithm can be obtained using a cost function based on safety and performance.", "startOffset": 17, "endOffset": 21}, {"referenceID": 27, "context": "Differently from [27], [28], in this paper, a) we incorporate a more realistic action space including harder brakes and faster accelerations; b) we develop a more realistic traffic model with more representative distance constraint violation rates via improvements in the reinforcement learning procedure; and c) we demonstrate that optimal parameter values for an autonomous vehicle control algorithm can be obtained using a cost function based on safety and performance.", "startOffset": 23, "endOffset": 27}, {"referenceID": 28, "context": "A detailed explanation of this hierarchical modeling method is given in [29] and [30].", "startOffset": 72, "endOffset": 76}, {"referenceID": 29, "context": "A detailed explanation of this hierarchical modeling method is given in [29] and [30].", "startOffset": 81, "endOffset": 85}, {"referenceID": 30, "context": "For more details on RL, see [31].", "startOffset": 28, "endOffset": 32}, {"referenceID": 31, "context": "\u201d In this work we employ the Jaakkola RL algorithm [32], which distinguishes itself from conventional approaches by guaranteeing to converge at least to a local maximum in terms of average rewards, when the problem is of POMDP type.", "startOffset": 51, "endOffset": 55}, {"referenceID": 31, "context": "See [32] for further details.", "startOffset": 4, "endOffset": 8}, {"referenceID": 32, "context": ", if this policy were executed for an infinite time [33].", "startOffset": 52, "endOffset": 56}, {"referenceID": 29, "context": "In experimental studies [30], it is shown that in human interactions, level-3 players are very rarely encountered and therefore in our results we trained policies up to and including level-2.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "The Stackelberg policies and the decision tree policies that are compared in this study were originally developed in [10], [15] and [16].", "startOffset": 117, "endOffset": 121}, {"referenceID": 14, "context": "The Stackelberg policies and the decision tree policies that are compared in this study were originally developed in [10], [15] and [16].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "The Stackelberg policies and the decision tree policies that are compared in this study were originally developed in [10], [15] and [16].", "startOffset": 132, "endOffset": 136}, {"referenceID": 14, "context": "It is noted that in [15] and [16] vehicle dynamics are different than the ones used in this paper.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "It is noted that in [15] and [16] vehicle dynamics are different than the ones used in this paper.", "startOffset": 29, "endOffset": 33}, {"referenceID": 9, "context": "In [10], it was assumed that the environment evolves deterministically over a planning horizon, independently of the controlled vehicle\u2019s actions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 14, "context": "This is also in agreement with the results in [15], [16] and [10].", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "This is also in agreement with the results in [15], [16] and [10].", "startOffset": 52, "endOffset": 56}, {"referenceID": 9, "context": "This is also in agreement with the results in [15], [16] and [10].", "startOffset": 61, "endOffset": 65}, {"referenceID": 29, "context": "These percentages of various levels are assumed based on an experimental study conducted in [30].", "startOffset": 92, "endOffset": 96}], "year": 2016, "abstractText": "Autonomous driving has been the subject of increased interest in recent years both in industry and in academia. Serious efforts are being pursued to address legal, technical and logistical problems and make autonomous cars a viable option for everyday transportation. One significant challenge is the time and effort required for the verification and validation of the decision and control algorithms employed in these vehicles to ensure a safe and comfortable driving experience. Hundreds of thousands of miles of driving tests are required to achieve a well calibrated control system that is capable of operating an autonomous vehicle in an uncertain traffic environment where multiple interactions between vehicles and drivers simultaneously occur. Traffic simulators where these interactions can be modeled and represented with reasonable fidelity can help decrease the time and effort necessary for the development of the autonomous driving control algorithms by providing a venue where acceptable initial control calibrations can be achieved quickly and safely before actual road tests. In this paper, we present a gametheoretic traffic model that can be used to 1) test and compare various autonomous vehicle decision and control systems and 2) calibrate the parameters of an existing control system. We demonstrate two example case studies, where, in the first case, we test and quantitatively compare two autonomous vehicle control systems in terms of their safety and performance, and, in the second case, we optimize the parameters of an autonomous vehicle control system, utilizing the proposed traffic model and simulation environment.", "creator": "LaTeX with hyperref package"}}}