{"id": "1608.08716", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Measuring Machine Intelligence Through Visual Question Answering", "abstract": "As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine's ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.", "histories": [["v1", "Wed, 31 Aug 2016 02:56:00 GMT  (2553kb,D)", "http://arxiv.org/abs/1608.08716v1", "AI Magazine, 2016"]], "COMMENTS": "AI Magazine, 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CV cs.LG", "authors": ["c lawrence zitnick", "aishwarya agrawal", "stanislaw antol", "margaret mitchell", "dhruv batra", "devi parikh"], "accepted": false, "id": "1608.08716"}, "pdf": {"name": "1608.08716.pdf", "metadata": {"source": "CRF", "title": "Measuring Machine Intelligence Through Visual Question Answering", "authors": ["C. Lawrence Zitnick", "Aishwarya Agrawal", "Virginia Tech", "Stanislaw Antol", "Margaret Mitchell", "Dhruv Batra", "Devi Parikh"], "emails": ["zitnick@fb.com", "aish@vt.edu", "santol@vt.edu", "memitc@microsoft.com", "dbatra@vt.edu", "parikh@vt.edu"], "sections": [{"heading": "1. Introduction", "text": "A phrase like \"Mary ran away quickly from the snarling bear\" conjures up both vivid visual and auditory interpretations. We imagine Mary running in the opposite direction to a wild bear, with the sound of the bear enough to frighten anyone. While interpreting a sentence like this is effortless for a human being, the construction of intelligent machines with the same deep understanding is anything but trivial. How can we determine whether a machine has reached the same deep understanding of our world as a human being? In our example sentence above, the understanding of a human being is rooted in multiple modalities. Even simple implications of the sentence, such as \"Mary is probably out,\" cannot be trivially deduced. How can we determine whether a machine has reached the same deep understanding of our world as a human being? In our example sentence, the understanding of a human being is rooted in multiple modalities."}, {"heading": "2. Image Captioning", "text": "These tasks have advantages, such as being easy to describe and being able to capture the public's imagination. [26] Unfortunately, tasks such as captioning have proved problematic because they actually involve the actual tests of intelligence. Especially, captioning evaluation may be as difficult as the captioning task itself [12, 30, 19, 22, 27]. Captions that are judged \"good\" by human observers are actually Containar Xiv: 160 8.08 716v 1 [cs.A] 3 ugsignificant variance, although they describe the same image. Many people would consider the longer, more detailed descriptions to be better, but the details described by the captions vary significantly, such as \"two hands,\" \"short black hair,\" \"\" etc."}, {"heading": "3. Visual Question Answering", "text": "The task must be easy to evaluate, but it is difficult to solve these two problems, which we propose to answer the task of the visual question (VQA) [1, 18, 24, 29, 4, 17]. VQA's task requires a machine to answer a natural language question over an image, as shown in Figure 3. Unlike the task of evaluating answers to questions, the evaluation of the answers is relatively simple. The simplest approach is to ask the questions with multiple choice answers, similar to standardized tests for students. As computers never tire of reading long lists of answers, we can even increase the length of the answer list."}, {"heading": "4. Abstract Scenes", "text": "The visual question to answer the task requires a variety of skills. The machine must be able to understand the image, the question and the reason for the answer. It may not be interesting for many researchers working with artificial intelligence to explore the tasks associated with perception and computer vision at a low level. Indeed, many of the questions may be impossible to solve given the current capabilities of modern computer vision algorithms. For example, the question \"How many mobile phones are in the picture?\" may not be answered if computer vision algorithms cannot accurately detect mobile phones. In fact, even for advanced computer algorithms, many objects are difficult to detect, especially small objects [23]. To allow multiple possibilities for exploring VQA, we insert abstract scenes into the dataset [2, 34, 35, 36]. Abstract scenes or cartoon images are created from groups of clip art, Figure 7. The scenes are created by human subjects using a graphical user interface that allows them to assign a variety of problems."}, {"heading": "5. Discussion", "text": "While answering visual questions seems to be a promising approach to measuring machine intelligence for multimodal tasks, it may turn out to have unforeseen shortcomings. We have studied several basic algorithms that perform poorly compared to human performance. As we explore the dataset, it is possible to find solutions that do not require \"real AI.\" However, we hope to continuously update the dataset with appropriate analysis to reflect the current progress of the field. As certain question or image types are too easy to answer, we can add new questions and images. Other modalities can also be explored, such as stories based on audio and text [13, 14, 33, 28]. Finally, we believe that designing a multimodal challenge is essential for accelerating and measuring the progress of AI."}], "references": [{"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2425\u20132433", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Zero-shot learning via visual abstraction", "author": ["S. Antol", "C.L. Zitnick", "D. Parikh"], "venue": "European Conference on Computer Vision, pages 401\u2013416. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning the semantics of words and pictures", "author": ["K. Barnard", "D. Forsyth"], "venue": "Computer Vision, 2001. ICCV 2001. Proceedings. Eighth IEEE International Conference on, volume 2, pages 408\u2013415. IEEE", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "et al", "author": ["J.P. Bigham", "C. Jayant", "H. Ji", "G. Little", "A. Miller", "R.C. Miller", "R. Miller", "A. Tatarowicz", "B. White", "S. White"], "venue": "Vizwiz: nearly real-time answers to visual questions. In Proceedings of the 23nd annual ACM symposium on User interface software and technology, pages 333\u2013342. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Microsoft coco captions: Data collection and evaluation server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Mind\u2019s eye: A recurrent visual representation for image caption generation", "author": ["X. Chen", "C. Lawrence Zitnick"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2422\u20132431", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Neil: Extracting visual knowledge from web data", "author": ["X. Chen", "A. Shrivastava", "A. Gupta"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1409\u2013 1416", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "AI: The tumultuous history of the search for artificial intelligence", "author": ["D. Crevier"], "venue": "Basic Books, Inc.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1993}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning everything about anything: Webly-supervised visual concept learning", "author": ["S.K. Divvala", "A. Farhadi", "C. Guestrin"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3270\u20133277", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2625\u20132634", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparing automatic evaluation measures for image description", "author": ["D. Elliott", "F. Keller"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics: Short Papers, volume 452, page 457", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Open question answering over curated and extracted knowledge bases", "author": ["A. Fader", "L. Zettlemoyer", "O. Etzioni"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1156\u20131165. ACM", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Paraphrasedriven learning for open question answering", "author": ["A. Fader", "L.S. Zettlemoyer", "O. Etzioni"], "venue": "ACL (1), pages 1608\u20131618. Citeseer", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "From captions to visual concepts and back. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1473\u20131482", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Every picture tells a story: Generating sentences from images", "author": ["A. Farhadi", "M. Hejrati", "M.A. Sadeghi", "P. Young", "C. Rashtchian", "J. Hockenmaier", "D. Forsyth"], "venue": "European Conference on Computer Vision, pages 15\u201329. Springer", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Are you talking to a machine? dataset and methods for multilingual image question", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "Advances in Neural Information Processing Systems, pages 2296\u20132304", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, 112(12):3618\u20133623", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Framing image description as a ranking task: Data", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "models and evaluation metrics. Journal of Artificial Intelligence Research, 47:853\u2013 899", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3128\u20133137", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "Transactions of the Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Babytalk: Understanding and generating simple image descriptions", "author": ["G. Kulkarni", "V. Premraj", "V. Ordonez", "S. Dhar", "S. Li", "Y. Choi", "A.C. Berg", "T.L. Berg"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(12):2891\u2013 2903", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision, pages 740\u2013755. Springer", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "Advances in Neural Information Processing Systems, pages 1682\u20131690", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Researchers announce advance in imagerecognition software", "author": ["J. Markoff"], "venue": "The New York Times", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Midge: Generating image descriptions from computer vision detections", "author": ["M. Mitchell", "X. Han", "J. Dodge", "A. Mensch", "A. Goyal", "A. Berg", "K. Yamaguchi", "T. Berg", "K. Stratos", "H. Daum\u00e9 III"], "venue": "Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 747\u2013756. Association for Computational Linguistics", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Mctest: A challenge dataset for the open-domain machine comprehension of text", "author": ["M. Richardson", "C.J. Burges", "E. Renshaw"], "venue": "EMNLP, volume 3, page 4", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint video and text parsing for understanding events and answering queries", "author": ["K. Tu", "M. Meng", "M.W. Lee", "T.E. Choe", "S.-C. Zhu"], "venue": "IEEE MultiMedia, 21(2):42\u201370", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u20134575", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning common sense through visual abstraction", "author": ["R. Vedantam", "X. Lin", "T. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 2542\u20132550", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "B", "author": ["J. Weston", "A. Bordes", "S. Chopra", "A.M. Rush"], "venue": "van Merri\u00ebnboer, A. Joulin, and T. Mikolov. Towards aicomplete question answering: A set of prerequisite toy tasks. arXiv preprint arXiv:1502.05698", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Bringing semantics into focus using visual abstraction", "author": ["C.L. Zitnick", "D. Parikh"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3009\u20133016", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning the visual interpretation of sentences", "author": ["C.L. Zitnick", "D. Parikh", "L. Vanderwende"], "venue": "Proceedings of the IEEE International Conference on Computer Vision, pages 1681\u20131688", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Adopting abstract images for semantic scene understanding", "author": ["C.L. Zitnick", "R. Vedantam", "D. Parikh"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 38(4):627\u2013 638", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 7, "context": "Marvin Minsky famously stated in 1966 [8] to one of his students,\u201cConnect a television camera to a computer and get the machine to describe what it sees.", "startOffset": 38, "endOffset": 41}, {"referenceID": 2, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 21, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 26, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 15, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 18, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 14, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 5, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 10, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 24, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 20, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 19, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 31, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 35, "endOffset": 81}, {"referenceID": 25, "context": "Are tasks such as image captioning [3, 22, 27, 16, 19, 15, 6, 11, 25, 21, 20, 32] promising candidates for testing artificial intelligence? These tasks have advantages, such as being easy to describe and being capable of capturing the imagination of the public [26].", "startOffset": 261, "endOffset": 265}, {"referenceID": 11, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 29, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 18, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 21, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 26, "context": "Most notably, the evaluation of image captions may be as difficult as the image captioning task itself [12, 30, 19, 22, 27].", "startOffset": 103, "endOffset": 123}, {"referenceID": 14, "context": "(left) An example image caption generated from [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 29, "context": "significant variance even though they describe the same image [30].", "startOffset": 62, "endOffset": 66}, {"referenceID": 14, "context": "In many cases it might be too easy! Consider an example success from a recent paper on image captioning [15], Figure 2.", "startOffset": 104, "endOffset": 108}, {"referenceID": 22, "context": "The results in Figure 2 were obtained by training on the Microsoft Common Objects in Context (MS COCO) dataset [23].", "startOffset": 111, "endOffset": 115}, {"referenceID": 4, "context": "This dataset contains five independent captions written by humans for over 120,000 images [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 8, "context": "This observation leads to a surprisingly simple algorithm for generating captions [9].", "startOffset": 82, "endOffset": 85}, {"referenceID": 29, "context": "From this set, select the caption with highest consensus [30], i.", "startOffset": 57, "endOffset": 61}, {"referenceID": 5, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 10, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 24, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 20, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 19, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 31, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 114, "endOffset": 137}, {"referenceID": 14, "context": "Despite its simplicity, this approach is competitive with more advance approaches using recurrent neural networks [6, 11, 25, 21, 20, 32] and other language models [15] which can achieve 27.", "startOffset": 164, "endOffset": 168}, {"referenceID": 0, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 17, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 23, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 28, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 3, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 16, "context": "To solve these two problems we propose the task of Visual Question Answering (VQA) [1, 18, 24, 29, 4, 17].", "startOffset": 83, "endOffset": 105}, {"referenceID": 22, "context": "In fact, even for state-of-the-art algorithms many objects are difficult to detect, especially small objects [23].", "startOffset": 109, "endOffset": 113}, {"referenceID": 1, "context": "To enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36].", "startOffset": 94, "endOffset": 109}, {"referenceID": 33, "context": "To enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36].", "startOffset": 94, "endOffset": 109}, {"referenceID": 34, "context": "To enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36].", "startOffset": 94, "endOffset": 109}, {"referenceID": 35, "context": "To enable multiple avenues for researching VQA, we introduce abstract scenes into the dataset [2, 34, 35, 36].", "startOffset": 94, "endOffset": 109}, {"referenceID": 34, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 1, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 6, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 9, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 30, "context": "The abstract scenes may be useful for a variety of other tasks as well, such as learning common sense knowledge [35, 2, 7, 10, 31].", "startOffset": 112, "endOffset": 130}, {"referenceID": 12, "context": "Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].", "startOffset": 75, "endOffset": 91}, {"referenceID": 13, "context": "Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].", "startOffset": 75, "endOffset": 91}, {"referenceID": 32, "context": "Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].", "startOffset": 75, "endOffset": 91}, {"referenceID": 27, "context": "Other modalities may also be explored such as audio and text-based stories [13, 14, 33, 28].", "startOffset": 75, "endOffset": 91}], "year": 2016, "abstractText": "As machines have become more intelligent, there has been a renewed interest in methods for measuring their intelligence. A common approach is to propose tasks for which a human excels, but one which machines find difficult. However, an ideal task should also be easy to evaluate and not be easily gameable. We begin with a case study exploring the recently popular task of image captioning and its limitations as a task for measuring machine intelligence. An alternative and more promising task is Visual Question Answering that tests a machine\u2019s ability to reason about language and vision. We describe a dataset unprecedented in size created for the task that contains over 760,000 human generated questions about images. Using around 10 million human generated answers, machines may be easily evaluated.", "creator": "LaTeX with hyperref package"}}}