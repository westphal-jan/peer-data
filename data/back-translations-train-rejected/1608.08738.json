{"id": "1608.08738", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "A Dictionary-based Approach to Racism Detection in Dutch Social Media", "abstract": "We present a dictionary-based approach to racism detection in Dutch social media comments, which were retrieved from two public Belgian social media sites likely to attract racist reactions. These comments were labeled as racist or non-racist by multiple annotators. For our approach, three discourse dictionaries were created: first, we created a dictionary by retrieving possibly racist and more neutral terms from the training data, and then augmenting these with more general words to remove some bias. A second dictionary was created through automatic expansion using a \\texttt{word2vec} model trained on a large corpus of general Dutch text. Finally, a third dictionary was created by manually filtering out incorrect expansions. We trained multiple Support Vector Machines, using the distribution of words over the different categories in the dictionaries as features. The best-performing model used the manually cleaned dictionary and obtained an F-score of 0.46 for the racist class on a test set consisting of unseen Dutch comments, retrieved from the same sites used for the training set. The automated expansion of the dictionary only slightly boosted the model's performance, and this increase in performance was not statistically significant. The fact that the coverage of the expanded dictionaries did increase indicates that the words that were automatically added did occur in the corpus, but were not able to meaningfully impact performance. The dictionaries, code, and the procedure for requesting the corpus are available at:", "histories": [["v1", "Wed, 31 Aug 2016 06:28:28 GMT  (24kb)", "http://arxiv.org/abs/1608.08738v1", "7 pages, presented at the first workshop on Text Analytics for Cybersecurity and Online Safety (TA-COS), collocated with LREC 2016"]], "COMMENTS": "7 pages, presented at the first workshop on Text Analytics for Cybersecurity and Online Safety (TA-COS), collocated with LREC 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["st\\'ephan tulkens", "lisa hilte", "elise lodewyckx", "ben verhoeven", "walter daelemans"], "accepted": false, "id": "1608.08738"}, "pdf": {"name": "1608.08738.pdf", "metadata": {"source": "CRF", "title": "A Dictionary-based Approach to Racism Detection in Dutch Social Media", "authors": ["St\u00e9phan Tulkens", "Lisa Hilte", "Elise Lodewyckx", "Ben Verhoeven", "Walter Daelemans"], "emails": ["walter.daelemans}@uantwerpen.be,", "elise.lodewyckx@student.uantwerpen.be"], "sections": [{"heading": null, "text": "ar Xiv: 160 8.08 738v 1 [cs.C L] 31 Aug 201 6Keywords: racism, word2vec, dictionary-based approaches, computer-based styleometry"}, {"heading": "1. Introduction", "text": "Furthermore, there is no clear definition of what exactly is a racist statement; what is racist for a person is most likely not universally considered racist. Although there are mechanisms to report racist acts, victims often neglect this because they believe that reporting the situation will not solve anything, according to Unia, the Belgian Inter-Federal Centre for Equal Opportunities. [1] However, the extent of this problem is currently unknown. Therefore, the aim of our system is twofold: it can be used to shed light on how many racist remarks are not reported online, and furthermore, the automated detection of racism could provide interesting insights into the linguistic mechanisms used in racist discourses. In this study, we try to automatically detect racist language in Dutch social media by using a methodological approach."}, {"heading": "2. Related Research", "text": "The classification of racist insults presents us with the problem of giving an adequate definition of racism. (More so1http: / / www.diversiteit.bethan) In other areas, the way in which a statement is an act of racism is very personal and does not simply fit a simple definition. Belgian anti-racist law prohibits discrimination, violence and crime based on physical qualities (such as skin color), nationality or ethnicity, but it does not mention textual insults based on these qualifications. (2) For this reason, this definition is not sufficient for our purposes, as it includes the racist statements that would be found on social media; few statements that people might perceive as racist are actually punishable by the law, other than statements that explicitly promote the use of violence. Therefore, we use a common sense definition of racist language, including all negative expressions and insults relating to ethnicity, religion and culture."}, {"heading": "3. Datasets and Annotations", "text": "In this section, we describe our data collection, our annotation guidelines (3.1) and the results of our comments (3.2 and 3.3). For our current research, we have compiled a corpus of social media comments retrieved from Facebook pages that are likely to elicit racist reactions in their comments. We specifically targeted two sites: the page of a prominent Belgian anti-Islamic organization and the page of a Belgian right-wing organization. In both cases, the Facebook pages were officially tolerated by the organizations and, predictably, served as a communication platform for organizing political gatherings. Although both sides, the former more than the latter, explicitly profess not to be racist, the comments they attracted were still highly critical of foreigners and, predictably, Muslims. This is also why we have removed comments from these sites, and not the posts themselves. While the narrow focus of the pages introduces prejudice into our data, as the opinions of the people visiting these sites do not reflect the general population's racial views."}, {"heading": "3.1 Annotation Style", "text": "We commented on the retrieved comments using three different labels: \"racist,\" \"non-racist\" and \"invalid.\" The \"racist\" label describes comments that contain negative statements or insults about a person's ethnicity, nationality, religion or culture. This definition also includes statements that, for example, equate an ethnic group with an extremist group, as well as extreme generalizations. The following examples are comments that were classified as racist: 1. Het zijn precies de vreemden die de haat of het racisme opwekken bij de autochtonen. It is the foreigners who elicit hatred and racism from the locals."}, {"heading": "3.2 Training Data", "text": "To collect the training data, we used Pattern3 (De Smedt and Daelemans, 2012) to scrape off the 100 most recent posts from both sides, and then extracted any comments that responded to those comments, resulting in 5759 extracted comments: 4880 from the first page and 8793http: / / www.clips.uantwerpen.be / patternfrom the second page; the second page attracted much less comments on each post, possibly because the page was posted more frequently; in addition, the organization behind the first page was prominently featured in the news at the time of the extraction, which might explain the gap in the frequency of comments between the two sides; the corpus was commented on by two commenters who were both students of similar age and background; and when A and B couldn't agree on a label, a third commentator, C, was used as a commentator to keep the annotations as gold standard labels."}, {"heading": "3.3 Test data", "text": "The reason for this is that the two candidates are the two candidates who prevailed over the other two candidates in the first round. \"They prevailed in the second round,\" he said. \"They prevailed in the second and third rounds.\" \"They prevailed in the second and third rounds.\" \"They prevailed in the third and fourth rounds.\" \"They prevailed in the third and fourth rounds.\" \"They prevailed in the third and fourth rounds.\" They prevailed in the second and fourth rounds. \"\" They prevailed in the third and fourth rounds. \"\" They prevailed in the second and fourth rounds. \"\" They prevailed in the third and fourth rounds. \"\" They prevailed in the third and fourth rounds. \"\" They prevailed in the third and fourth round. \"\" They prevailed in the fourth round. \"\" They prevailed in the third and fourth round. \"\" They prevailed in the fourth round. \"\" They prevailed in the third and fourth round. \"\""}, {"heading": "4. Experimental Setup", "text": "In this section, we will describe our experimental setup. First, we will discuss our dictionary-based approach and describe both the LIWC dictionary we use and the creation of dictionaries related to racist discourses (Section 4.1). Next, we will describe the pre-processing of the data (Section 4.2)."}, {"heading": "4.1 Dictionaries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 LIWC", "text": "In our classification task, we will use the LIWC dictionaries for Dutch4 (Zijlstra et al., 2004). We assume that some of LIWC's word categories may be useful for recognizing (implicit) racist discourses, since some of these categories are associated with markers of racist discourse reported in previous research (see section 2), including pronouns, words of negative emotions, references to others, certainty, religion, and curse words. 4A comprehensive overview of all categories in the Dutch version of LIWC can be found in Zijlstra et al. (2004, pp. 277-278)."}, {"heading": "4.1.2 Discourse Dictionaries", "text": "In addition to the Dutch LIWC data, we have created a dictionary that contains words that refer specifically to the racist discourse. We expect a dictionary-based approach where words are grouped into categories to work well in this case, because many of the racist terms used in our corpus are neologisms and hapaxes, such as halalhoer (halal prostitutes). Alternatively, existing terms are often reused in a ridiculous way, such as using the word mossel (mussel) to refer to Muslims. The dictionary was created as follows: terms referring to racist discourses were manually extracted from the training data. These terms were then divided into different categories, in which most categories have both a neutral and a negative subcategory. The negative subcategory contains explicit insults, while the neutral subcategory contains words that are normally used in a neutral way, such as marginalized (black)."}, {"heading": "4.1.3 Automated Dictionary Expansion", "text": "In order to extend the coverage of the categories in our dictionary, we performed dictionary extensions to both neutral and negative categories with word2vec (Mikolov et al., 2013). word2vec is a collection of models that are able to capture semantic similarities between words based on the sentence contexts in which these words occur, by projecting words into an n-dimensional space and giving words with similar contexts similar places in that space. Therefore, words that are closer to each other than cosmic distance are more similar. As we observed considerable semantic variations in the insults in our corpus, we expect dictionary extensions with word2vec to lead to the extraction of previously unknown insults, as we assume that similar insults are used in similar contexts. In parallel, we know that many words belonging to certain semantic categories, such as diseases and insults, are used as insults."}, {"heading": "4.2 Preprocessing and Featurization", "text": "For pre-processing, the text was first symbolized with the Dutch tokenizer of Pattern (De Smedt and Daelemans, 2012) and then scaled down and split into spaces, resulting in dictionary lists suitable for lexical processing. Our dictionary-based approach, such as LIWC, generates a ndimensional vector of normalized and scaled numbers, where n is the number of dictionary categories. We obtain these numbers by dividing the frequency of words in each specific category by the total number of words in the comment. Since all characteristics are already normalized and scaled, there was no need for further scaling. Moreover, because the number of characteristics is so small, we did not make an explicit feature selection."}, {"heading": "5. Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Performance on the Training Set", "text": "We estimated the optimal values for the SVM parameters by conducting an in-depth search through the parameter space, resulting in the selection of an RBF kernel with a C value of 1 and a gamma number of 0. For the SVM and other experiments, we used the implementation of Scikit-Learn (Pedregosa et al., 2011). By cross-validating the training data, all dictionary-based approaches with lexical categories associated with racist discourses were rated significantly better than models that used only the general word categories of LIWC. Since current research concerns the binary classification of racist statements, we report only results for the positive class, i.e. the racist class. If only LIWC categories were used as characteristics, an F score of 0.34 (dev. 0.07) was achieved for the racist class. If we use the original discourse dicariate, we achieved an F score of 0.50 (d. dev. 0.05)."}, {"heading": "5.2 Testing the Effect of Expansion", "text": "As you can see, the performance of the different models on the train track was comparable, regardless of their expansion. This is due to the creation of procedures for the dictionary, because the words in the original dictionaries were retrieved directly from the training data, the extended and cleaned versions might not be able to demonstrate their generalization, since most of the racist words are contained in the original dictionaries, as well as the extended dictionaries. This artifact might disappear in the test categories retrieved from the same two sites, but the invisible words will not be included in the original dictionaries that are present in the extended dictionaries but are present in the extended versions."}, {"heading": "6. Conclusions and Future Work", "text": "These comments were automatically retrieved from public social media sites with an anti-Islamic orientation. The definition of racism that we have used to comment on the comments also includes religious and cultural racism, a phenomenon that is described in various studies (Orru, 2015; Bonilla-Silva, 2002; Razavi et al., 2010). We use a Vector Machine support to classify comments as racist or not based on the prevalence of words related to racist discourses. To evaluate the performance, we have used our own comments as the gold standard. The most powerful model received an F score of 0.46 for the racist class on the test set, which is an acceptable decrease in terms of cross-validation experiments on the training data."}, {"heading": "7. Acknowledgments", "text": "We are very grateful to Leona Erens and Franc ois Deleu from Unia for cooperating with us and providing us with the necessary data. We thank the three anonymous reviewers for their helpful comments and advice."}, {"heading": "8. Supplementary Materials", "text": "Supplement materials are available at https: / / github.com / clips / hades."}, {"heading": "9. Bibliographical References", "text": "Critical Sociology, 28 (1-2): 41-64. Cohen, J. (1968). Weighted Kappa: Nominal scale agreement provision for scaled disadvantage or partial credit. Psychological bulletin, 70 (4): 213. De Smedt, T. and Daelemans, W. (2012). Pattern for Python. The Journal of Machine Learning Research, 13 (1): 2063-2067. Greevy, E. and Smeaton, S. (2004a). Text categorization of racist texts using a support machine. 7 es Journe es internationales d'Analyse statistique des Donne es Textuelles.Greevy, E. and Smeaton, A. F. (2004b)."}], "references": [{"title": "The linguistics of color blind racism: How to talk nasty about blacks without sounding \u201cracist", "author": ["E. Bonilla-Silva"], "venue": "Critical Sociology, 28(1-2):41\u201364.", "citeRegEx": "Bonilla.Silva,? 2002", "shortCiteRegEx": "Bonilla.Silva", "year": 2002}, {"title": "Weighted Kappa: Nominal scale agreement provision for scaled disagreement or partial credit", "author": ["J. Cohen"], "venue": "Psychological bulletin, 70(4):213.", "citeRegEx": "Cohen,? 1968", "shortCiteRegEx": "Cohen", "year": 1968}, {"title": "Pattern for Python", "author": ["T. De Smedt", "W. Daelemans"], "venue": "The Journal of Machine Learning Research, 13(1):2063\u20132067.", "citeRegEx": "Smedt and Daelemans,? 2012", "shortCiteRegEx": "Smedt and Daelemans", "year": 2012}, {"title": "Text categorization of racist texts using a support vector machine", "author": ["E. Greevy", "S. Smeaton"], "venue": "7 es Journ\u00e9es internationales d\u2019Analyse statistique des Donn\u00e9es Textuelles.", "citeRegEx": "Greevy and Smeaton,? 2004a", "shortCiteRegEx": "Greevy and Smeaton", "year": 2004}, {"title": "Classifying racist texts using a support vector machine", "author": ["E. Greevy", "A.F. Smeaton"], "venue": "Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, pages 468\u2013469. ACM.", "citeRegEx": "Greevy and Smeaton,? 2004b", "shortCiteRegEx": "Greevy and Smeaton", "year": 2004}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Computer-intensive methods for testing hypotheses: an introduction", "author": ["E. Noreen"], "venue": null, "citeRegEx": "Noreen,? \\Q1989\\E", "shortCiteRegEx": "Noreen", "year": 1989}, {"title": "Racist discourse on social networks: A discourse analysis of Facebook posts in Italy", "author": ["P. Orr\u00f9"], "venue": "Rhesis, 5(1):113\u2013133.", "citeRegEx": "Orr\u00f9,? 2015", "shortCiteRegEx": "Orr\u00f9", "year": 2015}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Jour-", "citeRegEx": "Pedregosa et al\\.,? 2011", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Linguistic inquiry and word count: LIWC 2001", "author": ["J.W. Pennebaker", "M.E. Francis", "R.J. Booth"], "venue": "Mahway: Lawrence Erlbaum Associates, 71:2001.", "citeRegEx": "Pennebaker et al\\.,? 2001", "shortCiteRegEx": "Pennebaker et al\\.", "year": 2001}, {"title": "Social prejudice as a resource of power: Towards the functional ambivalence of stereotypes", "author": ["U. Quasthoff"], "venue": "Wodak, R.(\u00e9d.), Language, Power and Ideology. Amsterdam: Benjamins, pages 181\u2013196.", "citeRegEx": "Quasthoff,? 1989", "shortCiteRegEx": "Quasthoff", "year": 1989}, {"title": "Offensive language detection using multi-level classification", "author": ["A.H. Razavi", "D. Inkpen", "S. Uritsky", "S. Matwin"], "venue": "Advances in Artificial Intelligence, pages 16\u201327. Springer.", "citeRegEx": "Razavi et al\\.,? 2010", "shortCiteRegEx": "Razavi et al\\.", "year": 2010}, {"title": "Discourse and discrimination: Rhetorics of racism and antisemitism", "author": ["M. Reisigl", "R. Wodak"], "venue": "Routledge.", "citeRegEx": "Reisigl and Wodak,? 2005", "shortCiteRegEx": "Reisigl and Wodak", "year": 2005}, {"title": "Evaluating unsupervised Dutch word embeddings as a linguistic resource", "author": ["S. Tulkens", "C. Emmery", "W. Daelemans"], "venue": "Proceedings of the 10th International Conference on Language Resources and Evaluation (LREC). European Language Resources Association", "citeRegEx": "Tulkens et al\\.,? 2016", "shortCiteRegEx": "Tulkens et al\\.", "year": 2016}, {"title": "Discourse and racism", "author": ["T.A. Van Dijk"], "venue": "The Blackwell companion to racial and ethnic studies, pages 145\u2013 159.", "citeRegEx": "Dijk,? 2002", "shortCiteRegEx": "Dijk", "year": 2002}, {"title": "De Nederlandse versie van de \u2018linguistic inquiry and word count\u2019(LIWC)", "author": ["H. Zijlstra", "T. Van Meerveld", "H. Van Middendorp", "J.W. Pennebaker", "R. Geenen"], "venue": "Gedrag & gezondheid, 32:271\u2013281.", "citeRegEx": "Zijlstra et al\\.,? 2004", "shortCiteRegEx": "Zijlstra et al\\.", "year": 2004}], "referenceMentions": [{"referenceID": 12, "context": "Furthermore, the use of stereotypes and prejudiced statements (Reisigl and Wodak, 2005; Quasthoff, 1989), as well as a heightened occurrence of truth claims (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a), are reported as typical characteristics of racist discourse .", "startOffset": 62, "endOffset": 104}, {"referenceID": 10, "context": "Furthermore, the use of stereotypes and prejudiced statements (Reisigl and Wodak, 2005; Quasthoff, 1989), as well as a heightened occurrence of truth claims (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a), are reported as typical characteristics of racist discourse .", "startOffset": 62, "endOffset": 104}, {"referenceID": 4, "context": "Furthermore, the use of stereotypes and prejudiced statements (Reisigl and Wodak, 2005; Quasthoff, 1989), as well as a heightened occurrence of truth claims (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a), are reported as typical characteristics of racist discourse .", "startOffset": 157, "endOffset": 211}, {"referenceID": 3, "context": "Furthermore, the use of stereotypes and prejudiced statements (Reisigl and Wodak, 2005; Quasthoff, 1989), as well as a heightened occurrence of truth claims (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a), are reported as typical characteristics of racist discourse .", "startOffset": 157, "endOffset": 211}, {"referenceID": 4, "context": "In this, we follow Orr\u00f9 (2015), Bonilla-Silva (2002) and Razavi et al.", "startOffset": 19, "endOffset": 31}, {"referenceID": 0, "context": "In this, we follow Orr\u00f9 (2015), Bonilla-Silva (2002) and Razavi et al.", "startOffset": 32, "endOffset": 53}, {"referenceID": 0, "context": "In this, we follow Orr\u00f9 (2015), Bonilla-Silva (2002) and Razavi et al. (2010), who show that racism is no longer strictly limited to physical or ethnic qualities, but can also include social and cultural aspects.", "startOffset": 32, "endOffset": 78}, {"referenceID": 0, "context": "In this, we follow Orr\u00f9 (2015), Bonilla-Silva (2002) and Razavi et al. (2010), who show that racism is no longer strictly limited to physical or ethnic qualities, but can also include social and cultural aspects. Additionally, several authors report linguistic markers of racist discourse; Van Dijk (2002) reports that the number of available topics is greatly restricted when talking about foreigners.", "startOffset": 32, "endOffset": 306}, {"referenceID": 0, "context": "In this, we follow Orr\u00f9 (2015), Bonilla-Silva (2002) and Razavi et al. (2010), who show that racism is no longer strictly limited to physical or ethnic qualities, but can also include social and cultural aspects. Additionally, several authors report linguistic markers of racist discourse; Van Dijk (2002) reports that the number of available topics is greatly restricted when talking about foreigners. Orr\u00f9 (2015), who performed a qualitative study of posts from Italian social media sites, shows that these chosen topics are typically related to migration, crime and economy.", "startOffset": 32, "endOffset": 415}, {"referenceID": 4, "context": "\u201cwhite civilization\u201d (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 21, "endOffset": 75}, {"referenceID": 3, "context": "\u201cwhite civilization\u201d (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 21, "endOffset": 75}, {"referenceID": 7, "context": "Stylistically, racist discourse is characterized by a higher rate of certain word classes, like imperatives and adjectives and a higher noun-adjective ratio (Orr\u00f9, 2015; Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 157, "endOffset": 223}, {"referenceID": 4, "context": "Stylistically, racist discourse is characterized by a higher rate of certain word classes, like imperatives and adjectives and a higher noun-adjective ratio (Orr\u00f9, 2015; Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 157, "endOffset": 223}, {"referenceID": 3, "context": "Stylistically, racist discourse is characterized by a higher rate of certain word classes, like imperatives and adjectives and a higher noun-adjective ratio (Orr\u00f9, 2015; Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 157, "endOffset": 223}, {"referenceID": 4, "context": "Greevy and Smeaton also report a more frequent use of modals and adverbs, which they link to the higher frequency of truth claims in racist utterances (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 151, "endOffset": 205}, {"referenceID": 3, "context": "Greevy and Smeaton also report a more frequent use of modals and adverbs, which they link to the higher frequency of truth claims in racist utterances (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 151, "endOffset": 205}, {"referenceID": 3, "context": "Stylistically, racist discourse is characterized by a higher rate of certain word classes, like imperatives and adjectives and a higher noun-adjective ratio (Orr\u00f9, 2015; Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a). Greevy and Smeaton also report a more frequent use of modals and adverbs, which they link to the higher frequency of truth claims in racist utterances (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a). In several studies, pronoun use is reported as an important feature in the detection of racist language. While Orr\u00f9 (2015) reports a high frequency of (especially first person plural) pronouns in racist data, Van Dijk (2002) reports a more general finding: the importance of us and them constructions in racist discourse.", "startOffset": 170, "endOffset": 555}, {"referenceID": 3, "context": "Stylistically, racist discourse is characterized by a higher rate of certain word classes, like imperatives and adjectives and a higher noun-adjective ratio (Orr\u00f9, 2015; Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a). Greevy and Smeaton also report a more frequent use of modals and adverbs, which they link to the higher frequency of truth claims in racist utterances (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a). In several studies, pronoun use is reported as an important feature in the detection of racist language. While Orr\u00f9 (2015) reports a high frequency of (especially first person plural) pronouns in racist data, Van Dijk (2002) reports a more general finding: the importance of us and them constructions in racist discourse.", "startOffset": 170, "endOffset": 657}, {"referenceID": 9, "context": "Our dictionary-based approach is inspired by methods used in previous research, like LIWC (Linguistic Inquiry and Word Count) (Pennebaker et al., 2001).", "startOffset": 126, "endOffset": 151}, {"referenceID": 3, "context": "The relevant features in this study therefore differ from those reported in other studies, as different words are used to insult different groups of people (Greevy and Smeaton, 2004a).", "startOffset": 156, "endOffset": 183}, {"referenceID": 4, "context": "Finally, some other successful quantitative approaches to racism detection that have been used in earlier studies are a bag of words (BoW) approach as well as the analysis of part-of-speech (PoS) tags (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 201, "endOffset": 255}, {"referenceID": 3, "context": "Finally, some other successful quantitative approaches to racism detection that have been used in earlier studies are a bag of words (BoW) approach as well as the analysis of part-of-speech (PoS) tags (Greevy and Smeaton, 2004b; Greevy and Smeaton, 2004a).", "startOffset": 201, "endOffset": 255}, {"referenceID": 1, "context": "We calculated inter-annotator agreement using the Kappa score (\u03ba) (Cohen, 1968).", "startOffset": 66, "endOffset": 79}, {"referenceID": 15, "context": "1 LIWC In our classification task, we will use the LIWC dictionaries for Dutch4 (Zijlstra et al., 2004).", "startOffset": 80, "endOffset": 103}, {"referenceID": 5, "context": "3 Automated Dictionary Expansion To broaden the coverage of the categories in our dictionary, we performed dictionary expansion on both the neutral and the negative categories using word2vec (Mikolov et al., 2013).", "startOffset": 191, "endOffset": 213}, {"referenceID": 13, "context": "For expansion we used the best-performing model from Tulkens et al. (2016), which is based on a corpus of 3.", "startOffset": 53, "endOffset": 75}, {"referenceID": 8, "context": "For the SVM and other experiments, we used the implementation from Scikit-Learn (Pedregosa et al., 2011).", "startOffset": 80, "endOffset": 104}, {"referenceID": 6, "context": "To obtain additional evidence, we computed the statistical significance of performance differences between the models based on the dictionaries and unigram baseline model using approximate randomization testing (ART) (Noreen, 1989).", "startOffset": 217, "endOffset": 231}, {"referenceID": 3, "context": "In comparison to previous research, our approach leads to worse results than those of Greevy and Smeaton (2004a), who report a precision score of 0.", "startOffset": 86, "endOffset": 113}, {"referenceID": 7, "context": "The definition of racism we used to annotate the comments therefore includes religious and cultural racism as well, a phenomenon reported on in different studies (Orr\u00f9, 2015; Bonilla-Silva, 2002; Razavi et al., 2010).", "startOffset": 162, "endOffset": 216}, {"referenceID": 0, "context": "The definition of racism we used to annotate the comments therefore includes religious and cultural racism as well, a phenomenon reported on in different studies (Orr\u00f9, 2015; Bonilla-Silva, 2002; Razavi et al., 2010).", "startOffset": 162, "endOffset": 216}, {"referenceID": 11, "context": "The definition of racism we used to annotate the comments therefore includes religious and cultural racism as well, a phenomenon reported on in different studies (Orr\u00f9, 2015; Bonilla-Silva, 2002; Razavi et al., 2010).", "startOffset": 162, "endOffset": 216}], "year": 2016, "abstractText": "We present a dictionary-based approach to racism detection in Dutch social media comments, which were retrieved from two public Belgian social media sites likely to attract racist reactions. These comments were labeled as racist or non-racist by multiple annotators. For our approach, three discourse dictionaries were created: first, we created a dictionary by retrieving possibly racist and more neutral terms from the training data, and then augmenting these with more general words to remove some bias. A second dictionary was created through automatic expansion using a word2vec model trained on a large corpus of general Dutch text. Finally, a third dictionary was created by manually filtering out incorrect expansions. We trained multiple Support Vector Machines, using the distribution of words over the different categories in the dictionaries as features. The best-performing model used the manually cleaned dictionary and obtained an F-score of 0.46 for the racist class on a test set consisting of unseen Dutch comments, retrieved from the same sites used for the training set. The automated expansion of the dictionary only slightly boosted the model\u2019s performance, and this increase in performance was not statistically significant. The fact that the coverage of the expanded dictionaries did increase indicates that the words that were automatically added did occur in the corpus, but were not able to meaningfully impact performance. The dictionaries, code, and the procedure for requesting the corpus are available at: https://github.com/clips/hades.", "creator": "LaTeX with hyperref package"}}}