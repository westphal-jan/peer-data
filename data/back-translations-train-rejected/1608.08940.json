{"id": "1608.08940", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Hash2Vec, Feature Hashing for Word Embeddings", "abstract": "In this paper we propose the application of feature hashing to create word embeddings for natural language processing. Feature hashing has been used successfully to create document vectors in related tasks like document classification. In this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data. The results show that this algorithm, that does not need training, is able to capture the semantic meaning of words. We compare the results against GloVe showing that they are similar. As far as we know this is the first application of feature hashing to the word embeddings problem and the results indicate this is a scalable technique with practical results for NLP applications.", "histories": [["v1", "Wed, 31 Aug 2016 17:01:09 GMT  (596kb,D)", "http://arxiv.org/abs/1608.08940v1", "ASAI 2016, 45JAIIO"]], "COMMENTS": "ASAI 2016, 45JAIIO", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["luis argerich", "joaqu\\'in torr\\'e zaffaroni", "mat\\'ias j cano"], "accepted": false, "id": "1608.08940"}, "pdf": {"name": "1608.08940.pdf", "metadata": {"source": "CRF", "title": "Hash2Vec: Feature Hashing for Word Embeddings", "authors": ["Luis Argerich", "Matias J. Cano", "Joaquin Torre Zaffaroni"], "emails": ["lrargerich@gmail.com"], "sections": [{"heading": null, "text": "Keywords: feature hashing, natural language processing, text embedding"}, {"heading": "1 Introduction", "text": "In the eeisrcehnn eeisrVnlrhe\u00fccn nvo the eeisrmtlrne\u00fceaeVrlhsrtee\u00fccnlhsrtee\u00fccnh ni rde eeisrcehnlrrtee\u00fcgcnlrVnlrrrrrrrrrcehnlcnos \"lehros\" lehros \"eeirrrrrf\u00fc\" eeirlrrrrrrrrrrrcehnos \"lehros\" lehros \"lehrros\" lehros \"lehrrros\" eos \"eeisrrrrrrrrrf\u00fc\" eeirlrrrrrrrrf\u00fc ide eeirlcrrlrrrrrrrrrrrrrrrteeeeeeeeeeeeeeeeecehnos \"lcos\" lehros \"lehrD,\" lehrD \"lehnos\" lehnos \"lehnos\" lehrD, \"lehnr\" lehnos \"eos\" lehnr \"eos\" D, \"lehnr\" eos \"lehrrrrrrrrrrrrrcehnos\" eos \"lehnr\" eos \"eos\" lehrrrrrrrrrrrcehrcehnos."}, {"heading": "2 Algorithm", "text": "The main formula for the algorithm can be found in (1) n (k) n (k) n (k) n (k) n (k) j (c) n (w) n (c) n (k)) n (k) i (1) w (k) i (k) i (k) n (k) e (k) e (k) e (c) e (c) e (c) e (c) e (k) e (k) e (k) e (k) e (k) e (k) e (k) e (k) e (c) e (c) e (c) e (c) e (c) e) e (c) e (c) e (c) e (c) e (c) e) e (c) e (c) e (e) c) (e) (e) (e) (e) (e) (e) (c) e) (e) (e) (e) (c) e) (e) (c) e) (e) (c) e) (c) e) (c) e) n (k) n (k) e) (k) e) (k) e) (k) e) (k) e) (k) e) (k) (k) e) (k) e) (k) e) (k) (k) e) (k) (k) e) (c) e) (c) e) (c) (c) e) (c) (c) e) (e) (e) (e) (e) (c) (e) (e) (e) (c) (e) (e) (c) (e) (e) (e) (c) (e) (e) (c) (c) (e) (c) (e) (c) (e) (c) (c)."}, {"heading": "2.1 Variations on the algorithm", "text": "In order to improve the quality of the embedding, we decided to try out some variations of the basic idea and perform various pre-processing tasks before running Hash2Vec. When running the model with huge corpora, there are words that occur frequently but have no meaning, as they serve only a syntactical purpose. To prevent these words from being a source of noise for Hash2Vec, we edit the corpus to remove them. We used two criteria to filter the words: calculating a certain percentile, avoiding the words above, or using a dashed list of words that need to be removed. Both methods resulted in an improvement in similarity testing. We applied the algorithm proposed on [4] to attach phrases, which is very important, because otherwise \"New York,\" \"\" \"San Francisco,\" etc. would not exist as a single brand. 4 We also achieved more modest results by using the text less homogeneous than if each sentence is shifted according to the original sentence distribution in a single context."}, {"heading": "2.2 Applications to speed up other embedding algorithms", "text": "The algorithm we propose constructs a low-dimensional approximation of the word co-occurence matrix. The embedding can then be used directly or be the starting point for a matrix factorization algorithm like the one in GloVe. In the case of word2vec [11] we have shown that the SGNS model learns an explicit matrix factorization (EMF) and that very similar results can be obtained by applying the EMF to the original matrix. The reduced form of the matrix that Hash2Vec generates can be used instead of the original to obtain very similar results again without the memory imprint of an O (n2) matrix."}, {"heading": "2.3 Streaming applications and parallelization", "text": "As embedding is refined during word processing, the algorithm is handy for streaming applications. The model can also be adapted to language changes, such as the use of new words, so embedding can be updated continuously while always being usable, which GloVe cannot do. By its very nature, Hash2Vec is easy to parallelise, as embedding of different parts of text can be updated in parallel and then simply added together to construct the final vectors.This associative property makes it trivial that the algorithm can be ported to a model like MapReduce. The algorithm could be run on Apache Spark, where each node processes a text fragment and a single final reduction operation can output the final embedding. Even more, if no regulation function is applied to the final vectors, embedding that can be trained in different contexts by adding the individual vectors of each word in both companies."}, {"heading": "3 Results", "text": "For benchmarking, we have compared our results with two commonly used datasets: the first is wordsim353 [12] and the second is from Amazon's Mechanical Turk, as used in [13]. Both datasets contain pairs of words with similarity values for each representing human-assigned similarity judgments. We then calculate the Spearman correlation between our results and the datasets to obtain a measure of the quality of the embeddings. The measure of similarity we use is cosmic similarity, as used in [3]. Both diagrams in Figure 1 show that Hash2Vec approximates the full vectors as the vector dimension increases. A very good approximation can be achieved with dimensions in the order of hundreds, orders of magnitude smaller than full vectors. As expected, the co-overcurrence matrix seems to be a theoretical limit to the performance of Hash2Vec."}, {"heading": "3.1 Comparison with GloVe", "text": "We trained GloVe with the same corpus with k = 15 and n = 600 in both cases. Table 1 shows the 5 most similar words returned by GloVe and Hash2Vec for some queries.6 We observe that the results of GloVe and Hash2Vec are very similar. Both models capture the semantic meaning of words and in some cases Hash2Vec seems to surpass Glove."}, {"heading": "3.2 Embeddings visualization", "text": "To understand how the algorithm behaves and to observe the resulting vector space further, we applied t-SNE [14], a widely used dimensionality reduction algorithm. After training Hash2Vec on a 110 million word corpus (k = 5 and n = 600), we applied t-SNE. Since the algorithm groups similar words, we should be able to see it in the diagram. Since t-SNE has about O (n2) in memory, we used only the 15,000 most common words. Zipf's law gives some guarantees to retain only the most common words. In Figure 2, we show an interesting sector extracted from the reduced matrix Cartesian diagram."}, {"heading": "3.3 The myth behind word analogies and Hash2Vec for word analogies", "text": "Word analogies have been used to show the expressiveness of a word that embeds a particular word. [2] GloVe and Word2Vec produce embeddings in which there is a linear relationship between analog words. [3] The most popular example is the analogy \"Prince is to princess like king is to.,\" in which the model was supposed to show that the model \"Prince-princess \u2261 king-queen.\" In [3] the authors claim that models that capture this linearity have a superior understanding of others. [16] On the other hand, the word analogy task is only a derivation of similarity. If you ask the model \"x is to y like z is to w,\" we actually maximize the dot product of w (x + y \u2212 z), which is the same as maximizing wx + wy \u2212 wz, the word analogy task is actually a question of the model for words that are x and similar to Worc, but different from wx-V2d or Wordings."}, {"heading": "4 Conclusions", "text": "In this paper, we have described in detail a very simple algorithm that is capable of constructing word embedding in linear time. It requires no training and requires minimal memory. We have shown that the results of the algorithm in word similarity and analogy tasks are comparable to GloVe [3], which is one of the most advanced algorithms for word embedding. The algorithm can also be used to approximate the word equality matrix to train models such as GloVe or EMF with minimal memory usage.The results show that feature hashing is a very powerful technique when applied to text, as it can reduce the dimensionality of word vectors from millions to just hundreds, while covering the semantics of each individual token in the vocabulary.8 Hash2Vec can be used as a very fast method of using word embedding with minimal code for flow or then incorporating it into dynamic applications."}], "references": [{"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "W.T. Yih", "G. Zweig"], "venue": "In HLT-NAACL, 746\u2013751", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global Vectors for Word Representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,Vol. 14, 1532\u20131543", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Hash kernels", "author": ["Q. Shi", "J. Petterson", "G. Dror", "J. Langford", "A.L. Strehl", "A.J. Smola", "S.V.N. Vishwanathan"], "venue": "In International Conference on Artificial Intelligence and Statistics, 496\u2013503", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative Email-Spam Filtering with the Hashing Trick", "author": ["J. Attenberg", "K. Weinberger", "A. Dasgupta", "A. Smola", "M. Zinkevich"], "venue": "In Proceedings of the Sixth Conference on Email and Anti-Spam", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Feature hashing for large scale multitask learning", "author": ["K. Weinberger", "A. Dasgupta", "J. Langford", "A. Smola", "J. Attenberg"], "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning, 1113\u20131120", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Extensions of Lipschitz mappings into a Hilbert space", "author": ["W.B. Johnson", "J. Lindenstrauss"], "venue": "Contemporary mathematics, 26,1, 189\u2013206", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1984}, {"title": "Database-friendly random projections: Johnson-Lindenstrauss with binary coins", "author": ["D. Achlioptas"], "venue": "Journal of computer and System Sciences, 66, 4, 671\u2013687", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2003}, {"title": "Improving word representations via global context and multiple word prototypes", "author": ["E.H. Huang", "R. Socher", "C.D. Manning", "A.Y. Ng"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long PapersVolume 1, 873\u2013882", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Word embedding revisited: A new representation learning and explicit matrix factorization perspective", "author": ["Y. Li", "L. Xu", "F. Tian", "L. Jiang", "X. Zhong", "E. Chen"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence, 3650\u20133656", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "E. Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web, 406\u2013414", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2001}, {"title": "A word at a time: computing word relatedness using temporal semantic analysis", "author": ["K. Radinsky", "E. Agichtein", "E. Gabrilovich", "S. Markovitch"], "venue": "In Proceedings of the 20th international conference on World wide web, 337\u2013346", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Visualizing data using t-SNE", "author": ["L. Van der Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Another stemmer", "author": ["D.P. Chris"], "venue": "In ACM SIGIR Forum, 24, 3, 56\u201361", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1990}, {"title": "Linguistic Regularities in Sparse and Explicit Word Representations", "author": ["O. Levy", "Y. Goldberg", "I. Ramat-Gan"], "venue": "In CoNLL, 171\u2013180", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "In recent works, several interesting properties of the resulting vector spaces were found [2].", "startOffset": 90, "endOffset": 93}, {"referenceID": 1, "context": "There are many other applications of word embeddings [3] which make the topic a very active area of research in NLP.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "As a result, modern models (GloVe [3], Word2Vec [4] learn to represent words with a fixed reduced dimensionality.", "startOffset": 34, "endOffset": 37}, {"referenceID": 2, "context": "A simple technique for dimensionality reduction is Feature Hashing [5], also known as the hashing trick.", "startOffset": 67, "endOffset": 70}, {"referenceID": 2, "context": "Feature hashing has been used successfully to reduce the dimensionality of the BOW model for texts [5].", "startOffset": 99, "endOffset": 102}, {"referenceID": 3, "context": "[6] used feature hashing to classify mail as spam or ham.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "To mitigate the effect of collisions in the resulting vectors [7] proposes the use of a second hash function \u03be that determines the sign of a feature.", "startOffset": 62, "endOffset": 65}, {"referenceID": 5, "context": "This is explained using the JohnsonLindenstrauss lemma [8] [9] and showing that feature hashing is a particular case of a J-L projection where the projection matrix has exactly one +1 or \u22121 in each row.", "startOffset": 55, "endOffset": 58}, {"referenceID": 6, "context": "This is explained using the JohnsonLindenstrauss lemma [8] [9] and showing that feature hashing is a particular case of a J-L projection where the projection matrix has exactly one +1 or \u22121 in each row.", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "It is a variation of the feature hashing equation shown in [5] with the addition of the second hashing function proposed by [7], as well as the domain-specific part which is the weight function for each co-occurrence.", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "It is a variation of the feature hashing equation shown in [5] with the addition of the second hashing function proposed by [7], as well as the domain-specific part which is the weight function for each co-occurrence.", "startOffset": 124, "endOffset": 127}, {"referenceID": 2, "context": "Ck represents all the contexts of the k-th word and h is a hashing function as [5] shows.", "startOffset": 79, "endOffset": 82}, {"referenceID": 4, "context": "\u03be is the additional hashing function such that \u03be : String \u2192 {\u22121, 1} proposed by [7].", "startOffset": 80, "endOffset": 83}, {"referenceID": 7, "context": "[10] explore this idea to construct different representations for the same word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "In the case of word2vec, [11] have shown that the SGNS model learns an explicit matrix factorization (EMF) and that very similar results can be obtained by using applying the EMF on the original matrix.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "The first is wordsim353 [12] and the second is from Amazon\u2019s Mechanical Turk, as used in [13].", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "The first is wordsim353 [12] and the second is from Amazon\u2019s Mechanical Turk, as used in [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 1, "context": "The similarity measure we used is cosine similarity, like the one used in [3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 11, "context": "To understand how the algorithm behaves and to further observe the resulting vector space, we applied t-SNE [14], a widely used dimensionality reduction algorithm.", "startOffset": 108, "endOffset": 112}, {"referenceID": 0, "context": "Word analogies have been used to show the expressiveness power of a word embedding model [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 1, "context": "In [3] the authors claim that models that capture this linearity have a superior understanding to others.", "startOffset": 3, "endOffset": 6}, {"referenceID": 13, "context": "In contrast, [16] show that the word analogy task is just a derivation of similarity.", "startOffset": 13, "endOffset": 17}, {"referenceID": 1, "context": "We showed the results of the algorithm to be comparable to GloVe [3] in the word similarity and analogy tasks, which is one of the state of the art algorithms for word embeddings.", "startOffset": 65, "endOffset": 68}], "year": 2016, "abstractText": "In this paper we propose the application of feature hashing to create word embeddings for natural language processing. Feature hashing has been used successfully to create document vectors in related tasks like document classification. In this work we show that feature hashing can be applied to obtain word embeddings in linear time with the size of the data. The results show that this algorithm, that does not need training, is able to capture the semantic meaning of words. We compare the results against GloVe showing that they are similar. As far as we know this is the first application of feature hashing to the word embeddings problem and the results indicate this is a scalable technique with practical results for NLP applications.", "creator": "LaTeX with hyperref package"}}}