{"id": "1608.08953", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Dynamic Allocation of Crowd Contributions for Sentiment Analysis during the 2016 U.S. Presidential Election", "abstract": "Opinions about the 2016 U.S. Presidential Candidates have been expressed in millions of tweets that are challenging to analyze automatically. Crowdsourcing the analysis of political tweets effectively is also difficult, due to large inter-rater disagreements when sarcasm is involved. Each tweet is typically analyzed by a fixed number of workers and majority voting. We here propose a crowdsourcing framework that instead uses a dynamic allocation of the number of workers. We explore two dynamic-allocation methods: (1) The number of workers queried to label a tweet is computed offline based on the predicted difficulty of discerning the sentiment of a particular tweet. (2) The number of crowd workers is determined online, during an iterative crowd sourcing process, based on inter-rater agreements between labels.We applied our approach to 1,000 twitter messages about the four U.S. presidential candidates Clinton, Cruz, Sanders, and Trump, collected during February 2016. We implemented the two proposed methods using decision trees that allocate more crowd efforts to tweets predicted to be sarcastic. We show that our framework outperforms the traditional static allocation scheme. It collects opinion labels from the crowd at a much lower cost while maintaining labeling accuracy.", "histories": [["v1", "Wed, 31 Aug 2016 17:20:09 GMT  (406kb,D)", "http://arxiv.org/abs/1608.08953v1", "10 pages, 3 figures"], ["v2", "Thu, 9 Feb 2017 18:05:46 GMT  (250kb,D)", "http://arxiv.org/abs/1608.08953v2", "10 pages, 3 figures"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.HC cs.CL cs.SI", "authors": ["mehrnoosh sameki", "mattia gentil", "kate k mays", "lei guo", "margrit betke"], "accepted": false, "id": "1608.08953"}, "pdf": {"name": "1608.08953.pdf", "metadata": {"source": "CRF", "title": "Dynamic Allocation of Crowd Contributions for Sentiment Analysis during the 2016 U.S. Presidential Election", "authors": ["Mehrnoosh Sameki", "Mattia Gentil", "Kate K. Mays", "Lei Guo", "Margrit Betke"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most people are able to survive themselves and themselves, and they are able to survive themselves, \"he said in an interview with The New York Times,\" I don't think we feel able to change the world. \""}, {"heading": "2 Method", "text": "Our approach consists of three main components: First, we have developed a method for recognizing sarcasm in tweets (Section 2.1); this first step was important because sarcasm is one of the most confusing and misleading linguistic traits to classify even a human commentator, especially when an individual tweet is analyzed out of context (Section 2.2); then we constructed a decision tree that assigns a fixed number of crowd workers to each tweet based on the presidential candidate mentioned in the tweet and other text characteristics, especially his sarcasm (Section 2.2); and in designing such a tree, we were motivated by the following insight: Tweets that are expected to be analyzed clearly and directly would require fewer commentators than tweets that are sarcastic and complicated. To mark the tree, we estimated how tedious it would be for crowd-working to be understood correctly offline."}, {"heading": "2.1 Sarcasm Detection", "text": "Our first step was to predict whether a particular tweet was sarcastic or not. We used a Bayesian approach to estimate the likelihood of sarcasm based on training data from domain experts. Our training data includes the term \"sarcasm present\" or \"sarcasm not present\" for 800 tweets about the four presidential candidates Clinton, Cruz, Sanders and Trump. We looked for general characteristics that are usually indications of the presence of sarcasm in a sentence (Gonza \u00d3lezIba \u00d3n, Muresan and Juniper 2011; Davidov, Tsur and Rappoport 2010) and grouped them into 7 categories: 1. Quotes: People often copy a candidate's words to make fun of them. 2. Question marks, exclamations or dropouts. 3. All capital letters: Twitterers sometimes emphasize sarcasm by using words or entire sentences with allcapital letters.4. Emoticons such as \": (\" 5. Words that express laughter or other categories we express with each other \")."}, {"heading": "2.2 Decision Tree", "text": "The decision tree we have designed is a tweet about a number of crowdsourcing workers who are asked to flag the tweet. To gain insight into the characteristics of a tweet that could cause a crowd worker to hold their own in the sensation classification and justify additional crowd work, we obtained gold standard data and conducted a formative crowd sourcing tweet that clearly identifies the candidates of the tweet. Expert labels We used 1,000 tweets about the four presidential candidates Clinton, Cruz, Sanders and Trump. For these tweets, we had gold standard labels about two categories provided by experts in political communication. The first category was whether each of the four candidates was mentioned in the tweet. The second category described whether the tweet was generally \"positive,\" neutral \"or\" negative \"about each candidate mentioned in the tweet. If more than one candidate was mentioned in a tweet, the sentiment toward each candidate was labeled in the tweet. The second category described whether the tweet was\" positive, \"neutral\" or \"negative\" about each candidate mentioned in the tweet. If more than one candidate was mentioned in a tweet, the sentiment toward each candidate was labeled in the tweet."}, {"heading": "2.3 Dynamic Worker Assignment", "text": "Here we propose an online scheme for determining the number of crowdworkers to be interviewed for each tweet. This approach cannot be calculated in advance for the crowdsourcing experiment, but is an iterative method based on the results of the crowdwork.Our idea is to request a small number of workers to provide the mood analysis for each tweet in a first round of crowdsourcing, and then perform one or more rounds of crowdsourcing for the tweets that the workers disagree with. In this way, the difficulty of the tweet is directly observed as a measure of disagreement in the first round of crowdsourcing, and we do not run the risk of wasting effort on tweets that are trivially classified.In order to evaluate our approach, we have developed two approaches of our idea, which include two rounds of crowdsourcing: Dynamic Decision Tree 1 (DT1) The first dynamic tree assigns two workers to the \"very simple\" and \"easy\" second round of crowdsourcing \"if we can observe a crowd class of difficulty 2."}, {"heading": "2.4 Equivalency of Traditional Static versus Proposed Dynamic Worker Allocation", "text": "Previous work has shown that the probability p that a crowdworker will correctly perform a task t according to a gold standard label can be described as a function p (t, w) of the task difficulty and skill of the worker (Ho and Vaughan 2012). For convenience, we disregard dependence on the worker. In a generic task, we can calculate the probability PM that the gold standard will be successfully achieved by majority voting for a set of crowdsourcing baseline schemes as a function of p. For example, the probability PM that the traditional crowdsourcing scheme 3 will deliver the correct results is the probability that at least 2 out of 3 will perform the task correctly \u2212 with isPM = 3 P (iworkers are correct) = 3 (3 i) pi) pi (3) = p2 p2 (p2) = p2 (p2 (1 \u2212 p) the probability that a dynamic model of 5 \u2212 we will correctly complete the crowdsourcing crowdsourcing scheme using the traditional crowdsourcing work."}, {"heading": "3 Experimental Methodology", "text": "We chose these candidates because they were the two leading candidates in the polls at the time of collecting data from each major U.S. political party (Republican and Democrat), and the data was collected using the Crimson Hexagon ForSight Social Media Analytics Platform (http: / / www.crimsonhexagon.com / platform), which was characterized by two domain experts with a background in political communication in a two-phase process. In the first phase, the experts independently determined the sentiment toward each candidate mentioned in each tweet, and in the second phase, they came to a consensus on the tweets they initially rejected."}, {"heading": "4 Results", "text": "Our experiments showed that the clues we used to detect sarcasm were very diverse and were used differently depending on the subject of the tweet. We found that smileys were not used at all, while the most significant element for detecting sarcasm was the presence of phrases such as \"lol,\" \"hahaha\" for example in the following tweet:"}, {"heading": "RT @rickygervais If Trump was a teacher he\u2019d be fired for publicly saying the things he says. Luckily he isn\u2019t a teacher. Just the next president. Hahaha.", "text": "The presence of sarcasm was actually a factor that increased the difficulty of the tweet classification: in our dataset we found a candidate we expected, sarcasm tweets had a 71.2% match percentage. This metric increased to 78.3% in the handling of non-sarcastic tweets. It turned out that the presence of sarcasm is not as ubiquitous as we expected, as only 73 out of 800 reviews were considered sarcastic by the respective candidates, and a surprising 68.5% of them concern Donald Trump (see Table 1). The last row of the table shows that even after weighing the sarcasm presence over the number of tweets mentioned by each candidate, Donald Trump still leads to 12% of his tweets that are sarcastic. In terms of the mood normally associated with sarcasm, the last column of the table proves that sarcasm is usually associated with a negative feeling toward a candidate."}, {"heading": "5 Discussion and Conclusions", "text": "As crowdsourcing becomes more and more theoretical analysis and simulation studies, here we present a concrete problem concerning the classification of two messages. Example applications are real-time sentiment analysis to provide quick indications of changes in public opinion or the collection of sufficiently large training data for machine learning methods for big data analysis (Wang et al. 2012). Studies such as ours on how to balance the goals of efficiency and accuracy in crowdsourcing are therefore particularly timely. Few papers have examined dynamic approaches to crowdsourcing based on iterative rounds of crowdsourcing and the number of workforce assignments based on content and annotations. (Bragg et al. 2014; Ho and Vaughan 2012; Kolobov, Mausam and Weld 2013) Links have been made to active and reactive learning (Yan et al. 2011; Lin, Mausam and Weld 2015)."}], "references": [{"title": "Parallel task routing for crowdsourcing", "author": ["Bragg"], "venue": "In Second AAAI Conference on Human Computation and Crowdsourcing (HCOMP", "citeRegEx": "Bragg,? \\Q2014\\E", "shortCiteRegEx": "Bragg", "year": 2014}, {"title": "Semi-supervised recognition of sarcastic sentences in twitter and amazon", "author": ["Tsur Davidov", "D. Rappoport 2010] Davidov", "O. Tsur", "A. Rappoport"], "venue": "In Proceedings of the Fourteenth Conference on Computational Natural Language", "citeRegEx": "Davidov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Davidov et al\\.", "year": 2010}, {"title": "Identifying sarcasm in twitter: A closer look", "author": ["Muresan Gonz\u00e1lez-Ib\u00e1\u00f1ez", "S. Muresan", "N. Wacholder"], "venue": "In Proceedings of the 49th Annual Meeting of the Association", "citeRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Gonz\u00e1lez.Ib\u00e1\u00f1ez et al\\.", "year": 2011}, {"title": "Big social data", "author": ["Guo"], "venue": null, "citeRegEx": "Guo,? \\Q2016\\E", "shortCiteRegEx": "Guo", "year": 2016}, {"title": "Online task assignment in crowdsourcing markets", "author": ["Ho", "Vaughan 2012] Ho", "C.-J", "J.W. Vaughan"], "venue": "Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence,", "citeRegEx": "Ho et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Ho et al\\.", "year": 2012}, {"title": "Data quality from crowdsourcing: A study of annotation selection criteria", "author": ["Melville Hsueh", "P.-Y. Sindhwani 2009] Hsueh", "P. Melville", "V. Sindhwani"], "venue": "In Proceedings of the NAACL HLT 2009 Workshop on Active Learning for Natural Language", "citeRegEx": "Hsueh et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hsueh et al\\.", "year": 2009}, {"title": "Efficient crowdsourcing for multiclass labeling", "author": ["Oh Karger", "D.R. Shah 2013] Karger", "S. Oh", "D. Shah"], "venue": "In Proceedings of the ACM SIGMETRICS/International Conference on Measurement and Modeling of Computer Systems,", "citeRegEx": "Karger et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Karger et al\\.", "year": 2013}, {"title": "Joint crowdsourcing of multiple tasks", "author": ["Mausam Kolobov", "A. Weld 2013] Kolobov", "Mausam", "D.S. Weld"], "venue": "In 1st AAAI Conference on Human Computation and Crowdsourcing (HCOMP", "citeRegEx": "Kolobov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Kolobov et al\\.", "year": 2013}, {"title": "Reactive learning: Actively trading off larger noisier training sets against smaller cleaner ones", "author": ["Mausam Lin", "C.H. Weld 2015] Lin", "M. Mausam", "D.S. Weld"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "POMDP-basedworker pool selection for crowdsourcing", "author": ["Goel Rajpal", "S. Mausam 2015] Rajpal", "K. Goel", "M. Mausam"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "Rajpal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rajpal et al\\.", "year": 2015}, {"title": "Analyzing media messages: Using quantitative content analysis in research", "author": ["Lacy Riffe", "D. Fico 2014] Riffe", "S. Lacy", "F. Fico"], "venue": null, "citeRegEx": "Riffe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Riffe et al\\.", "year": 2014}, {"title": "Efficient budget allocation with accuracy guarantees for crowdsourcing classification tasks", "author": ["Tran-Thanh"], "venue": "In Proceedings of the 2013 International Conference on Autonomous Agents and Multi-", "citeRegEx": "Tran.Thanh,? \\Q2013\\E", "shortCiteRegEx": "Tran.Thanh", "year": 2013}, {"title": "Predicting elections with Twitter: What 140 characters reveal about political sentiment", "author": ["Tumasjan"], "venue": "In Fourth International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "Tumasjan,? \\Q2010\\E", "shortCiteRegEx": "Tumasjan", "year": 2010}, {"title": "A system for realtime twitter sentiment analysis of 2012 U.S. presidential election cycle", "author": ["Wang"], "venue": "In Proceedings of the 50th Annual", "citeRegEx": "Wang,? \\Q2012\\E", "shortCiteRegEx": "Wang", "year": 2012}, {"title": "Active learning from crowds", "author": ["Yan"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "Yan,? \\Q2011\\E", "shortCiteRegEx": "Yan", "year": 2011}, {"title": "Affective news: The automated coding of sentiment in political texts. Political Communication 29(2):205\u2013231", "author": ["Young", "L. Soroka 2012] Young", "S. Soroka"], "venue": null, "citeRegEx": "Young et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Young et al\\.", "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "Opinions about the 2016 U.S. Presidential Candidates have been expressed in millions of tweets that are challenging to analyze automatically. Crowdsourcing the analysis of political tweets effectively is also difficult, due to large inter-rater disagreements when sarcasm is involved. Each tweet is typically analyzed by a fixed number of workers and majority voting. We here propose a crowdsourcing framework that instead uses a dynamic allocation of the number of workers. We explore two dynamic-allocation methods: (1) The number of workers queried to label a tweet is computed offline based on the predicted difficulty of discerning the sentiment of a particular tweet. (2) The number of crowd workers is determined online, during an iterative crowd sourcing process, based on inter-rater agreements between labels. We applied our approach to 1,000 twitter messages about the four U.S. presidential candidates Clinton, Cruz, Sanders, and Trump, collected during February 2016. We implemented the two proposed methods using decision trees that allocate more crowd efforts to tweets predicted to be sarcastic. We show that our framework outperforms the traditional static allocation scheme. It collects opinion labels from the crowd at a much lower cost while maintaining labeling accuracy.", "creator": "LaTeX with hyperref package"}}}