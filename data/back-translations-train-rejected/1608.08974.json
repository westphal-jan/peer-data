{"id": "1608.08974", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "31-Aug-2016", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models", "abstract": "Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques -- guided backpropagation and occlusion -- to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps.", "histories": [["v1", "Wed, 31 Aug 2016 18:11:29 GMT  (485kb,D)", "https://arxiv.org/abs/1608.08974v1", null], ["v2", "Fri, 9 Sep 2016 19:51:06 GMT  (486kb,D)", "http://arxiv.org/abs/1608.08974v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG", "authors": ["yash goyal", "akrit mohapatra", "devi parikh", "dhruv batra"], "accepted": false, "id": "1608.08974"}, "pdf": {"name": "1608.08974.pdf", "metadata": {"source": "CRF", "title": "Towards Transparent AI Systems: Interpreting Visual Question Answering Models", "authors": ["Yash Goyal", "Akrit Mohapatra", "Devi Parikh", "Dhruv Batra", "Virginia Tech"], "emails": ["dbatra}@vt.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to determine for themselves what they want and what they want."}, {"heading": "2 Related Work", "text": "Many gradient-based methods (Zeiler and Fergus, 2014; Simonyan et al., 2014; Springenberg et al., 2015) have been proposed in the field of computer vision in recent years to visualize deep revolutionary neural networks, but most of them have focused on the task of image classification on iconic images where the main object occupies most of the image. Our work differs in two ways - 1) we also calculate gradients for the input question and 2) we use guided backpropagation (Springenberg et al., 2015) for the task of VQA, where the model can view different regions in the same image for different questions. According to our literature review, we are the first to examine this problem for VQA. Our occlusion experiment is inspired by (Zeiler and Fergus, 2014), which masks small regions in the image with a gray spot and observes the output of an image classification model."}, {"heading": "3 Approach", "text": "At a high level, we consider a VQA model as a learned function a = fw (i, q), which takes an input image i and a question about the image q, by which parameters w are parameterized and produces an answer a. To measure the importance of the components of i and q (i.e. pixels and words), we consider the best linear approximation to f around each test point (itest, qtest): f (i, q) 'f (itest, qtest) + [i \u2212 itest, q \u2212 qtest] f (itest, qtest) (1) Intuitively, the two key parameters we have to calculate are f (itest, qtest) / \u2202 f (itest, qtest) / \u0445 f (itest, qtest) + [i \u2212 itest] q, i.e. the partial derivatives of the function w.r.t. Each of the inputs (image and question)."}, {"heading": "3.1 Guided Backpropagation", "text": "Guided Reverse Propagation (Springenberg et al., 2015) is a gradient-based visualization technology used to visualize neuron activation at various levels in CNNs. It has been shown to perform better than its counterparts such as deconvolution (Zeiler and Fergus, 2014), especially for visualizing higher order levels. Intuitively, it is a modified version of reverse propagation that limits negative gradients from backward toward the input layer, leading to sharper image visualizations. Specifically, Guided BP is identical to classical BP, except in the way that backward propagation is calculated in Reflected Linear Units (ReLUs). Let hl input in layer l and hl + 1 denote the output. Remember that a ReLU is defined as hl + 1 = relief (hl)."}, {"heading": "3.2 Discrete Derivatives", "text": "In this method, we systematically mask subsets of the input, propagate the masked input by the VQA model, and calculate the change in the probability of the answer predicted by the unmasked original input. As there are two inputs in the model, we focus on one input at a time, with the other input remaining unchanged (imitation of partial derivatives). To calculate the importance of a question word, we mask this word by deleting it from the question and feed the question with the original image as an input into the model. The importance of the question word is calculated as a change in the probability of the originally predicted answer. We follow the same procedure on the images to calculate the importance of image regions. We divide the image into a grid of size 16 x 16, each close a cell with a gray field as an input, feed the disturbed image with the entire question into the model, and calculate the reduction in the probability of the originally predicted answer."}, {"heading": "4 Results", "text": "While picture / question / importance maps on individual input factors provide crucial insights into the inner workings of a model (see Fig. 2), what do the aggregated statistics of these maps tell us about the model?"}, {"heading": "4.1 Analyzing Image Importance", "text": "(Das et al., 2016) recently collected human attention comments for (question, image) pairs from the VQA dataset (Antol et al., 2015). Faced with a blurred image and a question, people were asked to deblur the regions in the image that were helpful in answering the question. We evaluate the quality of the image meaning maps obtained from the two methods (guided backpropagation and occlusion) by comparing them with 2a gray patches of intensities (R, G, B) = (123.68, 116.779, 103.939), meaning RGB pixel values across a large image dataset ImageNet (Deng et al., 2009) on which CNN is located. 3Question meaning maps: https: / / mlp.ece.ece.vt. edu / the human attention consists of: https: / mec.vect.image / https /.human attention /.vect.vis /.the human attention."}, {"heading": "4.2 Analyzing Question Importance", "text": "Since there is no human attention dataset for questions, we instead analyze the meaning maps for questions based on their POS tags. Our hypothesis is that wh words and nouns should be most important for predicting a \"reasonable\" model. We record the likelihood that a word in a question is most important because it has a specific POS tag. To get reliable statistics, we selected 15 most common POS tags from the VQA validation dataset and grouped similar tags into a category, e.g. WDT, WP, WRB as wh words. The histogram is in Fig. 3. In fact, wh words are most important followed by adjectives and nouns. Adjectives and nouns rank high because many questions tend to ask for properties of objects or objects themselves. This finding suggests that the language model is part of the VQA model and is able to focus on appropriate words without preexposing them."}, {"heading": "5 Conclusion", "text": "Although we focus on only one VQA model in this work, the methods are generalizable to all other end-to-end VQA models, and the occlusion method can even be applied to any (non-end-to-end) VQA model that considers it a black box. We believe that these methods and results can be helpful in interpreting current VQA models and in developing the next generation of VQA models. Recognition. This work has been partially supported by the following: National Science Foundation CAREER Awards to DB and DP, Army Research Office YIP Awards to DB and DP, ICTAS Junior Faculty Awards to DB and DP, Army Research Lab Grant W911NF-15-2-0080 to DP and DB, Office of Naval Research Grant N00014-14-1 0606 to DB, DB Junior Faculty Awards to DB and DP Allen Research."}], "references": [{"title": "Cloudcv: Large-scale distributed computer vision as a cloud service", "author": ["Clint Solomon Mathialagan", "Yash Goyal", "Neelima Chavali", "Prakriti Banik", "Akrit Mohapatra", "Ahmed Osman", "Dhruv Batra"], "venue": "In Mobile Cloud Vi-", "citeRegEx": "Agrawal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Agrawal et al\\.", "year": 2015}, {"title": "VQA: Visual Question Answering", "author": ["Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C. Lawrence Zitnick", "Devi Parikh"], "venue": "In ICCV", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "How to explain individual classification decisions", "author": ["Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert M\u00fcller"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Baehrens et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baehrens et al\\.", "year": 2010}, {"title": "Large-scale simple question answering with memory networks. CoRR, abs/1506.02075", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Sequence learning with hidden units in spiking neural networks", "author": ["Brea et al.2011] Johanni Brea", "Walter Senn", "JeanPascal Pfister"], "venue": "In NIPS", "citeRegEx": "Brea et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Brea et al\\.", "year": 2011}, {"title": "Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions", "author": ["Das et al.2016] Abhishek Das", "Harsh Agrawal", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "venue": "In EMNLP", "citeRegEx": "Das et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Das et al\\.", "year": 2016}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Li Deng", "Dong Yu", "George E Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": null, "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "What has my classifier learned? Visualizing the classification rules of bag-of-feature model by support region detection", "author": ["Liu", "Wang2012] L. Liu", "L. Wang"], "venue": "In CVPR", "citeRegEx": "Liu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2012}, {"title": "Deeper LSTM and normalized CNN Visual Question Answering model", "author": ["Lu et al.2015] Jiasen Lu", "Xiao Lin", "Dhruv Batra", "Devi Parikh"], "venue": null, "citeRegEx": "Lu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2015}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["Marcus Rohrbach", "Mario Fritz"], "venue": "In ICCV", "citeRegEx": "Malinowski et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malinowski et al\\.", "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["Ren et al.2015] Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In NIPS", "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier", "author": ["Sameer Singh", "Carlos Guestrin"], "venue": "In Knowledge Discovery and Data Mining (KDD)", "citeRegEx": "Ribeiro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ribeiro et al\\.", "year": 2016}, {"title": "Mastering the game of Go with deep neu", "author": ["Silver et al.2016] David Silver", "Aja Huang", "Chris J Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": null, "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "author": ["Andrea Vedaldi", "Andrew Zisserman"], "venue": "In ICLR Workshop Track", "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "In ICLR Workshop Track", "citeRegEx": "Springenberg et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Springenberg et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Oriol Vinyals", "Quoc Le"], "venue": "In NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott E. Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In CVPR", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Towards AIComplete Question Answering: A Set of Prerequisite", "author": ["Weston et al.2015] Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "Toy Tasks. CoRR,", "citeRegEx": "Weston et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Visualizing and Understanding Convolutional Networks", "author": ["Zeiler", "Fergus2014] Matthew D. Zeiler", "Rob Fergus"], "venue": "In ECCV", "citeRegEx": "Zeiler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeiler et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Fueled by a combination of massive datasets and advances in deep neural networks (DNNs), the community has made remarkable progress in the past few years on a variety of \u2018low-level\u2019 AI tasks such as image classification (Szegedy et al., 2015) machine translation (Brea et al.", "startOffset": 220, "endOffset": 242}, {"referenceID": 4, "context": ", 2015) machine translation (Brea et al., 2011; Sutskever et al., 2014) and speech recognition (Hinton et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 15, "context": ", 2015) machine translation (Brea et al., 2011; Sutskever et al., 2014) and speech recognition (Hinton et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 6, "context": ", 2014) and speech recognition (Hinton et al., 2012).", "startOffset": 31, "endOffset": 52}, {"referenceID": 12, "context": "to play Go (Silver et al., 2016), answering reading comprehension questions by understanding short stories (Bordes et al.", "startOffset": 11, "endOffset": 32}, {"referenceID": 3, "context": ", 2016), answering reading comprehension questions by understanding short stories (Bordes et al., 2015; Weston et al., 2015), and", "startOffset": 82, "endOffset": 124}, {"referenceID": 17, "context": ", 2016), answering reading comprehension questions by understanding short stories (Bordes et al., 2015; Weston et al., 2015), and", "startOffset": 82, "endOffset": 124}, {"referenceID": 1, "context": "even answering questions about images (Antol et al., 2015; Ren et al., 2015; Malinowski et al., 2015).", "startOffset": 38, "endOffset": 101}, {"referenceID": 10, "context": "even answering questions about images (Antol et al., 2015; Ren et al., 2015; Malinowski et al., 2015).", "startOffset": 38, "endOffset": 101}, {"referenceID": 9, "context": "even answering questions about images (Antol et al., 2015; Ren et al., 2015; Malinowski et al., 2015).", "startOffset": 38, "endOffset": 101}, {"referenceID": 8, "context": "Specifically, we try to interpret a recent state-of-art VQA model (Lu et al., 2015) trained on recently released VQA (Antol et al.", "startOffset": 66, "endOffset": 83}, {"referenceID": 1, "context": ", 2015) trained on recently released VQA (Antol et al., 2015) dataset.", "startOffset": 41, "endOffset": 61}, {"referenceID": 14, "context": "1) uses guided backpropagation (Springenberg et al., 2015) to analyze important words in the question and important regions in the image.", "startOffset": 31, "endOffset": 58}, {"referenceID": 5, "context": "4, we present qualitative and quantitative analyses of these image/question \u2018importance maps\u2019 \u2013 question importance maps are analyzed using their Part-of-Speech (POS) tags; image importance maps are compared to \u2018human attention maps\u2019 or maps showing where humans look for answering a question about the image (Das et al., 2016).", "startOffset": 309, "endOffset": 327}, {"referenceID": 0, "context": "org/vqa/ (Agrawal et al., 2015) explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.", "startOffset": 9, "endOffset": 31}, {"referenceID": 13, "context": "Many gradient based methods (Zeiler and Fergus, 2014; Simonyan et al., 2014; Springenberg et al., 2015) have been proposed in recent years in the field of computer vision to visualize deep convolutional neural networks.", "startOffset": 28, "endOffset": 103}, {"referenceID": 14, "context": "Many gradient based methods (Zeiler and Fergus, 2014; Simonyan et al., 2014; Springenberg et al., 2015) have been proposed in recent years in the field of computer vision to visualize deep convolutional neural networks.", "startOffset": 28, "endOffset": 103}, {"referenceID": 14, "context": "the input question, and 2) we use guided backpropagation (Springenberg et al., 2015) for the task of VQA, where the model can look at different regions in the same image for different questions.", "startOffset": 57, "endOffset": 84}, {"referenceID": 11, "context": "A few recent works (Ribeiro et al., 2016; Baehrens et al., 2010; Liu and Wang, 2012) have begun to study the task of providing interpretable posthoc explanations for classifier predictions.", "startOffset": 19, "endOffset": 84}, {"referenceID": 2, "context": "A few recent works (Ribeiro et al., 2016; Baehrens et al., 2010; Liu and Wang, 2012) have begun to study the task of providing interpretable posthoc explanations for classifier predictions.", "startOffset": 19, "endOffset": 84}, {"referenceID": 14, "context": "Guided backpropagation (Springenberg et al., 2015) is a gradient-based visualization technique used to visualize activations of neurons in different layers in CNNs.", "startOffset": 23, "endOffset": 50}, {"referenceID": 14, "context": "For more details, please refer to (Springenberg et al., 2015).", "startOffset": 34, "endOffset": 61}, {"referenceID": 5, "context": "(Das et al., 2016) recently collected human attention annotations for (question, image) pairs from VQA dataset (Antol et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 1, "context": ", 2016) recently collected human attention annotations for (question, image) pairs from VQA dataset (Antol et al., 2015).", "startOffset": 100, "endOffset": 120}, {"referenceID": 1, "context": "The human attention dataset contains annotations for 1374 (question, image) pairs from VQA (Antol et al., 2015) validation set.", "startOffset": 91, "endOffset": 111}, {"referenceID": 5, "context": "Following the evaluation protocol in (Das et al., 2016), we take the absolute value of the importance maps and compute their mean rank-correlation with the human attention maps.", "startOffset": 37, "endOffset": 55}], "year": 2016, "abstractText": "Deep neural networks have shown striking progress and obtained state-of-the-art results in many AI research fields in the recent years. However, it is often unsatisfying to not know why they predict what they do. In this paper, we address the problem of interpreting Visual Question Answering (VQA) models. Specifically, we are interested in finding what part of the input (pixels in images or words in questions) the VQA model focuses on while answering the question. To tackle this problem, we use two visualization techniques \u2013 guided backpropagation and occlusion \u2013 to find important words in the question and important regions in the image. We then present qualitative and quantitative analyses of these importance maps. We found that even without explicit attention mechanisms, VQA models may sometimes be implicitly attending to relevant regions in the image, and often to appropriate words in the question.", "creator": "LaTeX with hyperref package"}}}