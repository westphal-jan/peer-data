{"id": "1609.00085", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "A Novel Progressive Learning Technique for Multi-class Classification", "abstract": "In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for real-world applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior.", "histories": [["v1", "Thu, 1 Sep 2016 01:50:18 GMT  (1032kb)", "http://arxiv.org/abs/1609.00085v1", "23 pages, 13 tables, 11 figures"], ["v2", "Sun, 22 Jan 2017 09:52:06 GMT  (1342kb)", "http://arxiv.org/abs/1609.00085v2", "23 pages, 13 tables, 11 figures"]], "COMMENTS": "23 pages, 13 tables, 11 figures", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["rajasekar venkatesan", "meng joo er"], "accepted": false, "id": "1609.00085"}, "pdf": {"name": "1609.00085.pdf", "metadata": {"source": "CRF", "title": "A Novel Progressive Learning Technique for Multi-class Classification", "authors": ["Rajasekar Venkatesan", "Meng Joo Er"], "emails": ["RAJA0046@e.ntu.edu.sg", "EMJER@ntu.edu.sg"], "sections": [{"heading": null, "text": "A comparative study shows that the technique developed is superior. Keywords - classification, machine learning, multi-level sequential learning, progressive learning. When a new class (which does not originate from what has been learned so far) is encountered, the structure of neural networks is automatically reconfigured by enabling new neurons and linkages, and the parameters are calculated in such a way that what has been learned is preserved. This technique is suitable for real-world applications, where the number of classes is often unknown and online learning from real-time data is required.The consistency and complexity of the progressive learning technique are analysed, and several standard data sets are used to assess the performance of the technology developed.A comparative study shows that the technology developed is superior. Keywords - classification, machine learning, multi-level, sequential learning, progressive learning."}, {"heading": "1. INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2.1 Extreme Learning Machines", "text": "In fact, the fact is that it is a question of a way and a way in which it is about a way in which it is about the question, to what extent it is about a way in which it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what extent it is about the question, to what it is about the question, to what extent it is about the question, to what extent it is about the question, to what it is about the question, to what it is about the question is about the question to what is about the question, to what is about the question to what is about the question, to what extent it is about the question is about the question, to what is about the question to what is about the question is about the question, to what is about the question to what is about the question to what is about the question, to what is about the question, to what is about the question is about the question, to what extent it is about the question to what is about the question, to what is about the question, to what is about the question, to what extent it is about the question to what is about the question, to what extent it is about the question, to what extent it is about the question, to what is about the question is about the question is about the question, to what is about the question, to what is about the question, to what is about the question, to what is to what is about the question, to what is to what is"}, {"heading": "2.2 Online Sequential \u2013 Extreme Learning Machine", "text": "This year, the number of unemployed in the US is many times higher than in other countries, where the number of unemployed is many times higher, many times higher than in Germany."}, {"heading": "3. PROGRESSIVE LEARNING TECHNIQUE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Learning like children", "text": "Peter Jarvis has described in detail in his book [42] the nature of the human learning process. In contrast to the training and testing cycle of the traditional machine learning algorithm, human learning is a continuous process, with no end to the learning / training phase. Whenever the human brain stumbles over a new phenomenon, learning is resumed [42]. The central feature of human learning is that learning new phenomena does not affect the knowledge learned. New knowledge is lean and is added to existing knowledge.Although there are several online and sequential learning methods, information about the number of classes is fixed during initialization, limiting the possibility of learning newer classes during learning. Existing machine learning algorithms cannot resume learning when a completely new class / data class is encountered after initialization. For applications such as cognitive robotics, real learning, etc., the system is robust and dynamic to learn new classes."}, {"heading": "3.2 Proposed Algorithm", "text": "This year it is more than ever before in the history of the city."}, {"heading": "4 EXPERIMENTATION", "text": "The proposed progressive learning algorithms exhibit \"dynamic\" learning of a new class of data. Current multi-classification algorithms do not adapt when they meet a new class, and therefore the accuracy decreases when they are introduced with one or more new classes. The proposed algorithm reconstructs itself to adapt to new classifications and retains the acquired knowledge as much as possible. The proposed progressive learning algorithm is tested with several real and standard data sets. Standard data sets are generally uniformly distributed, but in order to effectively test the performance of progressive learning, it should be presented with conditions in which new classes are introduced in a non-uniform manner. Standard data sets cannot be used directly to test the progressive learning algorithms efficiently."}, {"heading": "5 RESULTS AND DISCUSSIONS", "text": "The functionality of the technique, consistency and complexity are the three key characteristics that need to be tested for each new technique. Functional tests are used to validate the functionality of the proposed algorithm and lead to its expected behavior. The functionality of the technique is tested using iris, waveform and balance scale datasets. The operational functionality of the concept of progressive learning in the proposed algorithm is tested in the functionality test. Consistency is another key feature that is indispensable for each new technique. The proposed algorithm should provide consistent results for several studies with minimal variance. As it is an ELM-based algorithm, the consistency of the proposed method across several studies of the same dataset and also the consistency over 10-fold cross validation is tested. Complexity analysis is essential for a new technique. The number of operations performed and calculations involved in the proposed method are evaluated at the end of the proposed new method and the new performance of the existing method is also compared with each other by the algorithm's introduction."}, {"heading": "5.1 Functionality", "text": "The proposed technique is experimented with iris, waveform and equilibrium scale to verify the basic intended functionality of the technique. Iris dataset consists of three classes evenly distributed over the 150 instances. To facilitate the testing of progressive learning, the dataset is redistributed so that the first 50 samples consist of only two classes (Sentosa, VersionColor) and the third class (Virgo) is introduced only after the 51st sample. This type of redistribution emulates the real time scenario of the occurrence of a new class. The ability of the proposed algorithm to adapt and learn the new class can be verified by this test. Distribution details of the used dataset are given in TABLE. The progressive learning algorithm is tested on the run with the specified iris dataset."}, {"heading": "5.2 Consistency", "text": "The proposed technique is checked for consistency in its results. Consistency is a central virtue that any technique should have. The learning method that provides contradictory results is not reliable for practical applications. Therefore, since the proposed technique is an ELM-based technique, the input weights and hidden bias values are randomly initialized. Multiple versions of the same data set and the same specification result in different outcomes. Cross validation is the most common method for evaluating the consistency of a given technique. The proposed algorithm is run multiple times with each of the data sets to determine consistency across multiple executions. The consistency results of repeated multiple executions of the three data sets are tested in Table 5. Cross validation is the most common method for evaluating the consistency of a given technique. The proposed algorithm is weighed with each data set for 5-cross validation (5-cfv) and 10-LE consistency is tested for each of the proposed technique."}, {"heading": "5.3 Computational Reduction", "text": "The number of people in work who are able to do their jobs is at an all-time high."}, {"heading": "5.4 Introduction of New Class at Different Time Instants", "text": "In order to analyze the reaction, three different test cases are tested and performance measured; the new class is introduced at three different times: 1. Very early during the training, 2. Mid-training, 3. Towards the end of the training; the test accuracy curve in each of the test cases is plotted and the results evaluated and compared; the timing of the introduction of a new class for each of the test cases is shown in tabular form and is shown in Table 9. The performance of the proposed network for each of the test cases is shown in Figure 6. Figure 6 shows that the network is able to learn the new class regardless of the time a new class is introduced into the system; and the final accuracy of the stationary condition tests is the same in all test cases."}, {"heading": "5.5 Multiple New Classes", "text": "Learning several new classes through the proposed algorithm is tested using the character recognition dataset. Several combinations of tests are performed, such as 1. Sequential introduction of 2 new classes (4 classes) 2. Sequential introduction of 3 new classes (5 classes) 3. Simultaneous introduction of 2 new classes along with a new class sequentially (5 classes) The performance of the proposed algorithm on each of the test cases is monitored."}, {"heading": "5.5.1 Sequential Introduction of 2 new classes", "text": "Character dataset with 4 classes (A, B, C and D) is used to test the sequential introduction of two new classes into the proposed algorithm; the dataset is redistributed to meet the test requirements for progressive learning; the specifications of the dataset are described in TABLE 10.Initially, the network is trained sequentially with only two classes A and B up to 800 samples; a new class \"C\" is introduced into the training data in the 801st sample and a fourth class \"D\" is introduced as the 1601st sample; the proposed algorithm identifies both the new classes and recalibrates each time and continues learning, resulting in two sudden increases in the learning curve of the network; the first increase occurring in the 801st sample corresponds to learning in the class \"C\" and the second increase occurring in the 1601st sample corresponds to learning in the class \"D. The learning curve graph is shown in Figure 7."}, {"heading": "5.5.2 Sequential introduction of 3 new classes", "text": "The character dataset with 5 classes (A, B, C, D and E) is used for the sequential introduction of 3 new classes. Initially, the network is trained to recognize only two classes. Three new classes (C, D and E) are introduced successively after the first training of two classes. Data set specifications are shown in Table 11.Each of the new classes is introduced sequentially at a later stage and the algorithm adapts to each new class and also maintains test accuracy at the same level.The test accuracy curve is shown in Figure 8.to verify that the learning of each new class is independent of previously learned classes, the general test accuracy is broken down into individual test accuracy of each class and is shown in Fig. 9. It is evident that the test accuracy of each of the classes remains above 90%. Even if a new class is introduced, a new learning curve is created that contributes to the overall accuracy of each class."}, {"heading": "5.5.3 Simultaneous introduction of new classes", "text": "To ensure that the proposed algorithm works effectively when several classes are introduced simultaneously (introduced in the same block), character sets with specifications are used, as shown in TABLE 13. Here, the two classes C and D are introduced jointly and the new class E is introduced at a later date. Test accuracy is shown in Fig. 10.The first increase observed in the sample of 800 in the test accuracy curve corresponds to the introduction of two new classes (characters C and D). The algorithm identifies both the new classes and recalibrates to facilitate the completion of several classes. The second increase in the curve corresponds to the introduction of the third class (character E). To show that the previous knowledge is retained and new knowledge is added along with the existing one, the test accuracy is split for each of the five alphabets and in Fig. 11.It can be seen that two new learning curves corresponding to each new class C and D are introduced in the 800th sample. Both newly introduced classes, the additional classes A are pre-learned at the same time, with several classes pre-learned."}, {"heading": "6. CONCLUSIONS", "text": "This paper develops a novel learning method of progressive learning for the division into several classes. Progressive learning allows the network to dynamically learn several new classes on the run. New classes can be learned both sequentially and simultaneously. Therefore, this technique is very suitable for applications where the number of classes to be learned is unknown. Progressive learning allows the network to recalibrate and adapt when it encounters a new data class. The proposed progressive learning method will work effectively in applications such as cognitive robotics, where the system is trained on the basis of real-time based data."}, {"heading": "ACKNOWLEDGMENT", "text": "The first author thanks Nanyang Technological University, Singapore, for the NTU Research StudentScholarship."}], "references": [{"title": "Learning Representations by Back-propagating Errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Nature, vol. 323, no. 9, pp. 533-536", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1986}, {"title": "Training Feedforward Networks with the Marquardt Algorithm", "author": ["M.T. Hagan", "M.B. Menhaj"], "venue": "IEEE Trans. on Neural Networks, vol. 5, no. 6, pp. 989-993", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Neural Network Learning without Backpropagation", "author": ["B.M. Wilamowski", "H. Yu"], "venue": "IEEE Trans. on Neural Networks, vol. 21. No. 11, pp. 1793-1803", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Orthogonal Least Squares Learning Algorithm for Radial Basis Function Networks", "author": ["S. Chen", "C. Cowan", "P. Grant"], "venue": "IEEE Trans. on Neural Networks, vol. 2, no. 2, pp. 302-309", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1991}, {"title": "A Fast Nonlinear Model Identification Method", "author": ["K. Li", "J.X. Peng", "G.W. Irwin"], "venue": "IEEE Trans. on Automatic Control, vol. 50, no. 8, pp. 1211- 1216", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "Evolutionary Algorithms for Neural Network Design and Training", "author": ["J. Branke"], "venue": "Proc. Of first Nordic workshop on genetic algorithms and its applications", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1995}, {"title": "A Review of Evolutionary Artificial Neural Networks", "author": ["X. Yao"], "venue": "International Journal of Intelligent Systems, vol. 8, no. 4, pp. 539-567", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1993}, {"title": "An Improved On-Line Sequential Learning Algorithm for Extreme Learning Machine", "author": ["B. Li", "J. Wang", "Y. Li", "Y. Song"], "venue": "Advances in Neural Networks \u2013 ISNN 2007, vol. 4491, pp. 1087-1093", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2007}, {"title": "Extreme Learning Machine: Theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, vol. 70, pp. 489-501", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning Internal Representations by Error Propagation", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "No. ICS-8506, California University San Diego La Jolla Inst. for Cognitive Science", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1985}, {"title": "Smooth Function Approximation Using Neural Networks", "author": ["S. Ferrari", "R.F. Stengel"], "venue": "IEEE Transactions on Neural Networks, vol. 16, pp. 24-38", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "Classification Ability of Single Hidden Layer Feedforward Neural Networks", "author": ["G.B. Huang", "Y.Q. Chen", "H.A. Babri"], "venue": "IEEE Transactions on Neural Networks, vol. 11, pp. 799-801", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Extreme Learning Machines: A Survey", "author": ["G.B. Huang", "D.H. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics, vol. 2, pp. 107-122", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Generalized Single-hidden Layer Feedforward Networks for Regression Problems", "author": ["N. Wang", "M.J. Er", "M. Han"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 6, pp. 1161-1176", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A Fast and Accurate Online Sequential Learning Algorithm for Feedforward Networks", "author": ["N.Y. Liang", "G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 17, pp. 1411-1423", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Proceedings of International Joint Conference on Neural Networks, vol. 2, pp. 985-990", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Universal Approximation Using Incremental Constructive Feedforward Networks with Random Hidden Nodes", "author": ["G.B. Huang", "L. Chen", "C.K. Siew"], "venue": "IEEE Transactions on Neural Networks, vol. 17, pp. 879-892", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Convex Incremental Extreme Learning Machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing, vol. 70, no. 16, pp. 3056-3062", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2007}, {"title": "Enhanced Random Search based Incremental Extreme Learning Machine", "author": ["G.B. Huang", "L. Chen"], "venue": "Neurocomputing, vol. 71, no. 16, pp. 3460-3468", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "A Study on Effectiveness of Extreme Learning Machine", "author": ["Y. Wang", "F. Cao", "Y. Yuan"], "venue": "Neurocomputing, vol. 74, pp. 2483-2490", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Extreme Learning Machine for Regression and Multiclass Classification", "author": ["G.B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Trans. on Systems, Man and Cybernetics, Part B: Cybernetics, vol. 42, no. 2, pp. 513-529", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Evolutionary Extreme Learning Machine", "author": ["Q.Y. Zhu", "A.K. Qin", "P.N. Suganthan", "G.B. Huang"], "venue": "Pattern Recognition, vol. 38, pp. 1759- 1763", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Fully Complex Extreme Learning Machine", "author": ["M.B. Li", "G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neurocomputing, vol.68, pp.306- 314", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Parsimonious Extreme Learning Machine Using Recursive Orthogonal Least Squares", "author": ["N. Wang", "M.J. Er", "M. Han"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 25, no. 10, pp. 1828-1841", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "An Efficient Sequential Learning Algorithm for Growing and Pruning RBF (GAP-RBF) Networks", "author": ["G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: vol. 34, pp. 2284-2292", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2004}, {"title": "A Generalized Growing and Pruning RBF (GGAP-RBF) Neural Network for Function Approximation", "author": ["G.B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "IEEE Transactions on Neural Networks, vol. 16, pp. 57-67", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Hybrid Recursive Least Squares Algorithm for Online Sequential Identification Using Data Chunks", "author": ["N. Wang", "J.C. Sun", "M.J. Er", "Y.C. Liu"], "venue": "Neurocomputing, vol. 174, pp. 651-660", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Constructive Multi-output Extreme Learning Machine with Application to Large Tanker Motion Dynamics Identification", "author": ["N. Wang", "M. Han", "N. Dong", "M.J. Er"], "venue": "Neurocomputing, vol. 128, pp. 59-72", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Online Sequential Fuzzy Extreme Learning Machine for Function Approximation and Classification Problems", "author": ["H.J. Rong", "G.B. Huang", "N. Sundararajan", "P. Saratchandran"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B: vol. 39, pp. 1067-1072", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "On-Line Sequential Extreme Learning Machine", "author": ["G.B. Huang", "N.Y. Liang", "H.J. Rong", "P. Saratchandran", "N. Sundararajan"], "venue": "Computational Intelligence, vol. 2005, pp. 232-237", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Trends in Extreme Learning Machines: A Review", "author": ["G. Huang", "G.B. Huang", "S. Song", "K. You"], "venue": "Neural Networks, vol. 61, pp. 32-48", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "A Hybrid Automatic System for the Diagnosis of Lung Cancer Based on Genetic Algorithm and Fuzzy Extreme Learning Machines", "author": ["M.R. Daliri"], "venue": "Journal of Medical Systems, vol. 36, no. 2, pp. 1001-1005", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Fuzzy Extreme Learning Machine for Classification", "author": ["W.B. Zhang", "H.B. Ji"], "venue": "Electronics Letters, vol. 49, no. 7, pp. 448-449", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "A New Automatic Target Recognition System based on Wavelet Extreme Learning Machine", "author": ["E. Avci", "R. Coteli"], "venue": "Expert Systems with Applications, vol. 39, no. 16, pp. 12340-12348", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Application of Extreme Learning Machine for Series Compensated Transmission Line Protection", "author": ["V. Malathi", "N.S. Marimuthu", "S. Baskar", "K. Ramar"], "venue": "Engineering Applications of Artificial Intelligence, vol. 24, no. 5, pp. 880-887", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2011}, {"title": "Parameter-insensitive Kernel in Extreme Learning for Non-linear Support Vector Regression", "author": ["B. Freney", "M. Verleysen"], "venue": "Neurocomputing, vol. 74, no. 16, pp. 2526-2531", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Weighted Extreme learning Machine for Imbalance Learning", "author": ["W.W. Zong", "G.B. Huang", "Y.Q. Chen"], "venue": "Neurocomputing, vol. 101, pp. 229- 242", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "A New Robust Training Algorithm for a Class of Single Hidden Layer feedforward neural networks", "author": ["Z.H. Man", "K. Lee", "D.H. Wang", "Z.W. Cao", "C.Y. Miao"], "venue": "Neurocomputing, vol. 74, no. 16, pp. 2491-2501", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Voting based Extreme Learning Machine", "author": ["J.W. Cao", "Z.P. Lin", "G.B. Huang", "N. Liu"], "venue": "Information Sciences, vol 185, no. 1, pp. 66-77", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2012}, {"title": "Prediction of Protein-protein Interactions from Amino Acid Sequences with Ensemble Extreme Learning Machines and Principal Component Analysis", "author": ["Z.H. You", "Y.K. Lei", "L. Zhu", "J.F. Xia", "B. Wang"], "venue": "BMC Bioinformatics, 14", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Dynamic Ensemble Extreme Learning Machine based on Sample Entropy", "author": ["J.H. Zhai", "H.Y. Xu", "X.Z. Wang"], "venue": "Soft Computing, vol. 16, no. 9, pp. 1493-1502", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "A Class Incremental Extreme learning machine for activity recognition", "author": ["Z. Zhao", "Z. Chen", "Y. Chen", "S. Wang", "H. Wang"], "venue": "Cognitive Computation, vol. 6 no. 3, pp. 423-431", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION HE study on feedforward neural network (FNN) has gained prominence since the advent of back propagation (BP) algorithm [1].", "startOffset": 132, "endOffset": 135}, {"referenceID": 1, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 2, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 3, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 4, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 5, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 6, "context": "Several improved and optimized variants of the BP algorithm have then been developed and analyzed [2-7].", "startOffset": 98, "endOffset": 103}, {"referenceID": 7, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 8, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 9, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 10, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 11, "context": "In the past two decades, single hidden layer feedforward neural networks (SLFNs) has gained significant importance due to its widespread applications in recognition, classification and function approximation area [8-12].", "startOffset": 213, "endOffset": 219}, {"referenceID": 12, "context": "Several learning techniques have been proposed since then for effective training of the SLFN [13-14].", "startOffset": 93, "endOffset": 100}, {"referenceID": 13, "context": "Several learning techniques have been proposed since then for effective training of the SLFN [13-14].", "startOffset": 93, "endOffset": 100}, {"referenceID": 14, "context": "The learning techniques can be grouped under two basic categories; Batch learning and Sequential learning [15].", "startOffset": 106, "endOffset": 110}, {"referenceID": 15, "context": "One of the relatively new batch learning scheme called Extreme Learning Machines (ELM) is proposed by Huang et al in 2004 [16].", "startOffset": 122, "endOffset": 126}, {"referenceID": 16, "context": "The special nature of ELM is that the input weights and the hidden node biases can be chosen at random [17].", "startOffset": 103, "endOffset": 107}, {"referenceID": 16, "context": "A key feature of ELM is that it maintains the universal approximation capability of SLFN [17-19].", "startOffset": 89, "endOffset": 96}, {"referenceID": 17, "context": "A key feature of ELM is that it maintains the universal approximation capability of SLFN [17-19].", "startOffset": 89, "endOffset": 96}, {"referenceID": 18, "context": "A key feature of ELM is that it maintains the universal approximation capability of SLFN [17-19].", "startOffset": 89, "endOffset": 96}, {"referenceID": 19, "context": "It has gained much attention to it and several research works are further made in it due to its special nature of random input weight initialization and its unique advantage of extreme learning speed [20].", "startOffset": 200, "endOffset": 204}, {"referenceID": 8, "context": "The advantages of ELM over other traditional feedforward neural network are analyzed in the literature [9,21].", "startOffset": 103, "endOffset": 109}, {"referenceID": 20, "context": "The advantages of ELM over other traditional feedforward neural network are analyzed in the literature [9,21].", "startOffset": 103, "endOffset": 109}, {"referenceID": 21, "context": "Many new variants and developments are made to the ELM and significant results are achieved in the approximation, classification and regression areas [22-24].", "startOffset": 150, "endOffset": 157}, {"referenceID": 22, "context": "Many new variants and developments are made to the ELM and significant results are achieved in the approximation, classification and regression areas [22-24].", "startOffset": 150, "endOffset": 157}, {"referenceID": 23, "context": "Many new variants and developments are made to the ELM and significant results are achieved in the approximation, classification and regression areas [22-24].", "startOffset": 150, "endOffset": 157}, {"referenceID": 7, "context": "In many cases, sequential learning algorithms are preferred over batch learning algorithms as they do not require retraining whenever a new data sample is received [8].", "startOffset": 164, "endOffset": 167}, {"referenceID": 14, "context": "Online-sequential learning method that combines ELM and Recursive Least Square (RLS) algorithm is later developed and is called Online-Sequential extreme learning machine (OS-ELM) [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 7, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 8, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 16, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 21, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 22, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 28, "context": "Several variants of ELM and OS-ELM were developed and proposed in the literature [8,9,17,22,23,29].", "startOffset": 81, "endOffset": 98}, {"referenceID": 15, "context": "[16] is given below.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The mathematical framework and the training process are extensively described in the literature [9].", "startOffset": 96, "endOffset": 99}, {"referenceID": 8, "context": "Lemma 1: [9] Given a standard SLFN with N hidden nodes and activation function g: R \u2192 R which is infinitely differentiable in any interval, for N arbitrary distinct samples (xi,ti), where xi \u03b5 R and ti \u03b5 R, for any wi and bi randomly chosen from any intervals of R and R, respectively, according to any continuous probability distribution, then with probability one, the hidden layer output matrix H of the SLFN is invertible and ||H\u03b2 \u2013 T|| = 0.", "startOffset": 9, "endOffset": 12}, {"referenceID": 8, "context": "Lemma 2: [9] Given any small positive value \u03b5 > 0 and activation function g: R \u2192 R which is infinitely differentiable in any interval, there exists P \u2264 N such that for N arbitrary distinct samples (xi,ti), where xi \u03b5 R and ti \u03b5 R, for any wi and bi randomly chosen from any intervals of R and R, respectively, according to any continuous probability distribution, then with probability one, ||HNxP \u03b2PXm \u2013 TNXm|| < \u03b5.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "2 Online Sequential \u2013 Extreme Learning Machine Based on the batch learning method of the ELM, sequential modification is performed and Online Sequential-ELM (OS-ELM) is proposed in literature [15].", "startOffset": 192, "endOffset": 196}, {"referenceID": 14, "context": "As stated in [15], this solution gives the least square solution to H\u03b2 = T.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 8, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 12, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 14, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 29, "context": "several papers [8,9,13,15,30].", "startOffset": 15, "endOffset": 29}, {"referenceID": 30, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 57, "endOffset": 61}, {"referenceID": 31, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 159, "endOffset": 166}, {"referenceID": 32, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 159, "endOffset": 166}, {"referenceID": 33, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 159, "endOffset": 166}, {"referenceID": 34, "context": "The standard variants of activation function used in ELM [31] and other special mapping functions and their variants are discussed in detail in the literature [32-35].", "startOffset": 159, "endOffset": 166}, {"referenceID": 35, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 46, "endOffset": 50}, {"referenceID": 36, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 76, "endOffset": 80}, {"referenceID": 37, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 101, "endOffset": 105}, {"referenceID": 18, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 123, "endOffset": 127}, {"referenceID": 38, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 142, "endOffset": 149}, {"referenceID": 39, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 142, "endOffset": 149}, {"referenceID": 40, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 142, "endOffset": 149}, {"referenceID": 30, "context": "The other variants of ELM includes ELM Kernel [36], ELM for imbalanced data [37], ELM for noisy data [38], Incremental ELM [19], ELM ensemble [39-41] and many other variants are summarized in [31] 3.", "startOffset": 192, "endOffset": 196}, {"referenceID": 41, "context": "The method of increasing the dimension of the matrix, the weight update and matrix recalibration methods of the proposed algorithm are significantly different from the class-incremental extreme learning machine [43].", "startOffset": 211, "endOffset": 215}], "year": 2016, "abstractText": "In this paper, a progressive learning technique for multi-class classification is proposed. This newly developed learning technique is independent of the number of class constraints and it can learn new classes while still retaining the knowledge of previous classes. Whenever a new class (non-native to the knowledge learnt thus far) is encountered, the neural network structure gets remodeled automatically by facilitating new neurons and interconnections, and the parameters are calculated in such a way that it retains the knowledge learnt thus far. This technique is suitable for realworld applications where the number of classes is often unknown and online learning from real-time data is required. The consistency and the complexity of the progressive learning technique are analyzed. Several standard datasets are used to evaluate the performance of the developed technique. A comparative study shows that the developed technique is superior. Key Words\u2014Classification, machine learning, multi-class, sequential learning, progressive learning.", "creator": "Microsoft\u00ae Word 2013"}}}