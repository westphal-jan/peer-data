{"id": "1609.00222", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Sep-2016", "title": "Ternary Neural Networks for Resource-Efficient AI Applications", "abstract": "The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limit their deployability on ubiquitous computing devices such as smart phones or wearables. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach. Using only ternary weights and ternary neurons, with a step activation function of two-thresholds, the student ternary network learns to mimic the behaviour of its teacher network. We propose a novel, layer-wise greedy methodology for training TNNs. During training, a ternary neural network inherently prunes the smaller weights by setting them to zero. This makes them even more compact thus more resource-friendly. We devise a purpose-built hardware design for TNNs and implement it on FPGA. The benchmark results with our purpose-built hardware running TNNs reveal that, with only 1.24 microjoules per image, we can achieve 97.76% accuracy with 5.37 microsecond latency and with a rate of 255K images per second on MNIST.", "histories": [["v1", "Thu, 1 Sep 2016 13:08:47 GMT  (81kb,D)", "http://arxiv.org/abs/1609.00222v1", null], ["v2", "Sun, 26 Feb 2017 09:44:34 GMT  (84kb,D)", "http://arxiv.org/abs/1609.00222v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.NE", "authors": ["hande alemdar", "vincent leroy", "adrien prost-boucle", "fr\\'ed\\'eric p\\'etrot"], "accepted": false, "id": "1609.00222"}, "pdf": {"name": "1609.00222.pdf", "metadata": {"source": "CRF", "title": "Ternary Neural Networks for Resource-Efficient AI Applications", "authors": ["Hande Alemdar", "Nicholas Caldwell", "Vincent Leroy", "Adrien Prost-Boucle", "Fr\u00e9d\u00e9ric P\u00e9trot"], "emails": ["name.surname@imag.fr"], "sections": [{"heading": "1 Introduction", "text": "Deep Neural Networks (DNNs) have achieved state-of-the-art results on a wide range of AI tasks, including computer vision [1], speech recognition [2] and natural language processing [3]. As DNNs become more complex, their number of layers, number of weights and computing costs is increasing. While DNNs are usually trained on powerful servers supported by GPUs, they can be used for classification tasks on a variety of hardware. Our goal in this paper is to train DNNs that are able to classify a high throughput of low-power devices. In recent years, two main directions of research have been explored to reduce the cost of DNNs classifications. The first preserves the gliding point precision of DNNs, but drastically increases the economy and weights required for the operation of Xiv: 160 9.00 222v 1 [cs.L] 1compression."}, {"heading": "2 Training Ternary Neural Networks", "text": "We use a teacher-student approach to the training of TNNs. First, we train the real teacher network with stochastically firing ternary neurons. Then, we let the student network learn how to imitate the behavior of the teacher using a layer-by-layer greedy algorithm. Both the teacher and the student network have the same architecture. The weights of the student network are the ternarized version of the teacher networks. The student network uses a step function with two thresholds as an activation function. In Table 1, we provide our notation and descriptions. Generally, we describe the discrete values with a bold font. Real parameters are denoted by normal font. We use [.] to denote a matrix or a vector. In the following subsections, we describe the details of the two levels."}, {"heading": "2.1 The Teacher Network", "text": "The teacher network is trained as a real neural network and has stochastically firing ternary neurons with output values of \u2212 1, 0 or 1. To achieve a ternary output for teacher neurons nti, we add a stochastic trigger step after the hyperbolic tangent, Tanh, activation function, as shown in Table 1. Although we use Tanh to obtain the range (\u2212 1, 1) prior to ternarization, any nonlinear function such as hard tanh or soft sign that has the same range applies. We do not impose any restrictions on the weights of the teacher network and do not ternarize it at this stage. The advantage of this approach is that we can use any technique that already exists for efficient NN training, such as batch normalization [10], dropouts [9], etc. The teacher network can have any architecture with any number of neurons and be trained with any of the usual training algorithms."}, {"heading": "2.2 The Student Network", "text": "The goal of the student network is to predict the performance of the teacher network. Since we use the same architecture for both networks, there is a 1: 1 correspondence between the neurons of both. Each student neuron nsi learns to imitate the behavior of the corresponding teacher neuron nti individually and independently of the other neurons. To achieve this, a student neuron uses the corresponding teacher neuron as a guideline to determine its own ternary weights, using two thresholds tloi and thii on the weights of the teacher neuron. This step is called weight tternarization. To have a ternary neuron output, we have a step activation function of two thresholds bilo and bihi. The output tternarization step determines this. Figure 1 shows the tternarization procedure for a sample neuron output, we have a step activation function of two thresholds bilo and bihi. The output tternarization step determines this."}, {"heading": "2.2.1 Output Ternarization", "text": "The student network uses a two-stage activation function to have a ternary output as described in Table 1. To achieve this, we calculate three different output distributions of the transfer function for the student neuron using the ternary output value of the teacher neuron for a ternary neuron i for a given set of ternary weights W. To achieve this, we use y \u2212 to designate the settlo thi 050100150200 W-1 -0.5 0 0.5 1 0500100015002000 tanh (WTx + b) -1 0 1 01234 # 10 4 nt1000 W2000 y!; y0; y + 4 # 4 nsof transfer function outputs of the student neuron for which the output value of the teacher neuron \u2212 0.5 1 0500100015002000 tanh (WTx + b) -1 0 and y + is similarly defined for teacher neuron output values 0 and 1."}, {"heading": "2.2.2 Weight Ternarization", "text": "While we evaluate the number of possible ternarizations and the order of the weight classes and negative thresholds thi and thi that we calculate, we will reduce the number of possible ternarization schemes (we have the number of possible ternarization schemes that we select to reduce the number of ternarization steps). (We have the number of possible ternarization layers that we select to reduce the number of ternarizations. (We have the optimal thresholds for the weight values determined by evaluating the quality of the ternarization with a score function.) For a given neuron with positive weights and n negative weights, the total number of possible ternarization schemes is np, as we respect the original character and order of weights. (We have the optimal thresholds for the quality of the ternarization with a score function.) We have a total number of possible ternarization schemes that we respect the original weights (we respect the positive and negative weights)."}, {"heading": "3 Hardware", "text": "We developed and implemented a hardware architecture on the FPGA that takes advantage of the fact that input values and neuron weights are limited to ternary values {\u2212 1, 0, + 1}. These values are represented on 2 bits, usually using two complementary encodings. Figure 2 illustrates the implementation of a three-layered NN. The design is divided into several interconnected blocks, which form a pipeline that corresponds to the sequence of NN processing steps. We assume that a particular NN configuration is used for a large number of classification operations, so we can use embedded memory blocks for runtime storage for the sake of area and energy efficiency, the neuron weights and output stararization thresholds blo and bhi. The calculated part of each neuron is an instance of a small component containing a ternary multiplier (two logic blocks per neural) and a few new gates (one each)."}, {"heading": "4 Experiments", "text": "We conduct our experiments on the MNIST database of handwritten digits [12], a well-researched database for benchmarking methods on real data. MNIST has a training set with 60K examples and a test set with 10K examples of 28x28 grayscale images. We use the last 10K samples of the training set as a validation set for early stopping and model selection.We experiment with both multilayer perceptrons (MLP) in a permutation invariant manner and with Convolutionary Neural Networks (CNN).For the MLPs, we experiment with different architectures in terms of depth and neuron number. We use 250, 500, 750 and 1000 neurons per layer for 2, 3 and 4-layer networks. For the CNNs, we use a LENET-like architecture with 10 and 15 filters in the first and second continuous layer, followed by two completely interconnected neurons per layer for 2, 3 and 4-layer networks."}, {"heading": "4.1 Ternarization Performance", "text": "We measure this by using the difference in accuracy between the teacher network and the student network. Table 2 shows this difference between the teacher network and student networks in education and test rates for three different, exhaustive search thresholds. As we always use the original output of the teacher network as a reference, errors in the network are not amplified. On the contrary, deeper networks allow the student network to correct some of the errors in the upper layers, mitigating the errors. Also, we perform a retraining step before we ternarize a shift as it slightly improves performance. Neurarization performance generally decreases with lower thresholds, but the decrease is marginal."}, {"heading": "4.2 Classification Performance", "text": "The classification in the MNIST dataset is included in Table 3. We also compare the performance of ternary NNs with other solutions included in the literature, which we discuss in more detail in Section 5. We have also trained the performance of binary NNs using the code provided by the authors. To allow a fair comparison, we use the results to their algorithm required by our appropriate hardware. Note that the reported results match the real performance of the NNNNs [6] with the neurons in three layers. We use the results in relation to the closest architecture in our experiments."}, {"heading": "4.3 Hardware Performance", "text": "The performance of our hardware solution in terms of latency, throughput and energy efficiency is shown in Table 4. We know that TrueNorth can operate at the two extremes of power consumption and accuracy, consuming 0.268 \u00b5J on a low-accuracy (92.7%) network, and consuming up to 108 \u00b5J on a committee of 64 networks that reaches 99.4%. Our hardware cannot operate at these two extremes, but in the mid-range, we exceed TrueNorth in terms of both energy efficiency - accuracy compromise and speed. TrueNorth consumes 4 \u00b5J at an accuracy of 95% at a throughput of 1000 frames / s and a latency of 1 ms. Our TNN hardware, which consumes 3.63 \u00b5J, achieves an accuracy of 98.14% at a rate of 255 102 frames / s and a latency of 8.09 \u00b5s. In addition, if our FPGA design were designed as an ASIC, it could consume even less performance in the order of 15%."}, {"heading": "5 Discussion and Related Work", "text": "This year, it has reached the point where there is only one question: \"What kind of country is this?\" he asked."}, {"heading": "6 Conclusion", "text": "In this study, we have proposed TNNs for resource-efficient deep learning applications, and our TNNs have demonstrated that they exceed the referenced resource-efficient DNNs in terms of accuracy. Combined with our TNNs, our hardware also offers significant throughput and latency improvements, and where both optimal classification accuracy and energy efficiency are required, we outperform previous work. We present a pipeline-based FPGA architecture that takes advantage of the assumption that the same NN configuration will be reused for many operations, utilizing embedded memory and performing parallel execution on input streams. The resulting throughput of ternary DNNs exhibits 255 times the rate of TrueNorth, while showing lower power consumption per classification. We suggest a teacher-student approach to train TNNNs with weights limited to {1, 0, 1}."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel rahman Mohamed", "Geoffrey Hinton"], "venue": "In ICASSP,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "In ICLR,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Bitwise neural networks", "author": ["Minje Kim", "Paris Smaragdis"], "venue": "In International Conference on Machine Learning (ICML) Workshop on Resource-Efficient Machine Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1", "author": ["Matthieu Courbariaux", "Itay Hubara", "Daniel Soudry", "Ran El-Yaniv", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1602.02830,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Backpropagation for energy-efficient neuromorphic computing", "author": ["Steve K Esser", "Rathinakumar Appuswamy", "Paul Merolla", "John V Arthur", "Dharmendra S Modha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1929}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1998}, {"title": "Training binary multilayer neural networks for image classification using expectation backpropagation", "author": ["Zhiyong Cheng", "Daniel Soudry", "Zexi Mao", "Zhenzhong Lan"], "venue": "arXiv preprint arXiv:1503.03562,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Measuring the Gap Between FPGAs and ASICs", "author": ["Ian Kuon", "Jonathan Rose"], "venue": "IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "In ICLR,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "A million spiking-neuron integrated circuit with a scalable communication network and interface", "author": ["Paul A Merolla", "John V Arthur", "Rodrigo Alvarez-Icaza", "Andrew S Cassidy", "Jun Sawada", "Filipp Akopyan", "Bryan L Jackson", "Nabil Imam", "Chen Guo", "Yutaka Nakamura", "Bernard Brezzo", "Ivan Vo", "Steven K. Esser", "Rathinakumar Appuswamy", "Brian Taba", "Arnon Amir", "Myron D. Flickner", "William P. Risk", "Rajit Manohar", "Dharmendra S. Modha"], "venue": "Science, 345(6197):668\u2013673,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks (DNNs) have achieved state-of-the-art results on a wide range of AI tasks including computer vision [1], speech recognition [2] and natural language processing [3].", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "Deep neural networks (DNNs) have achieved state-of-the-art results on a wide range of AI tasks including computer vision [1], speech recognition [2] and natural language processing [3].", "startOffset": 145, "endOffset": 148}, {"referenceID": 2, "context": "Deep neural networks (DNNs) have achieved state-of-the-art results on a wide range of AI tasks including computer vision [1], speech recognition [2] and natural language processing [3].", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "compression [4, 5].", "startOffset": 12, "endOffset": 18}, {"referenceID": 4, "context": "compression [4, 5].", "startOffset": 12, "endOffset": 18}, {"referenceID": 5, "context": "The second direction completely eliminates the need for floating-point operations using weights discretization [6, 7, 8], with extreme cases such as binary neural networks eliminating the need for multiplications.", "startOffset": 111, "endOffset": 120}, {"referenceID": 6, "context": "The second direction completely eliminates the need for floating-point operations using weights discretization [6, 7, 8], with extreme cases such as binary neural networks eliminating the need for multiplications.", "startOffset": 111, "endOffset": 120}, {"referenceID": 7, "context": "The second direction completely eliminates the need for floating-point operations using weights discretization [6, 7, 8], with extreme cases such as binary neural networks eliminating the need for multiplications.", "startOffset": 111, "endOffset": 120}, {"referenceID": 8, "context": "The teacher network is trained with stochastic firing using back-propagation, and can benefit from all techniques that exist in the literature such as dropout [9], batch normalization [10], and convolutions.", "startOffset": 159, "endOffset": 162}, {"referenceID": 9, "context": "The teacher network is trained with stochastic firing using back-propagation, and can benefit from all techniques that exist in the literature such as dropout [9], batch normalization [10], and convolutions.", "startOffset": 184, "endOffset": 188}, {"referenceID": 9, "context": "The benefit of this approach is that we can use any technique that already exists for efficient NN training, such as batch normalization [10], dropout [9], etc.", "startOffset": 137, "endOffset": 141}, {"referenceID": 8, "context": "The benefit of this approach is that we can use any technique that already exists for efficient NN training, such as batch normalization [10], dropout [9], etc.", "startOffset": 151, "endOffset": 154}, {"referenceID": 10, "context": "We perform our experiments on the MNIST database of handwritten digits [12], a well-studied database for benchmarking methods on real-world data.", "startOffset": 71, "endOffset": 75}, {"referenceID": 5, "context": "We also cite the reported performance of Bitwise NNs [6] with 1024 neurons in 3 layers.", "startOffset": 53, "endOffset": 56}, {"referenceID": 11, "context": "For EBP, we use the results provided in [13] and map the results to the closest architecture in our experiments.", "startOffset": 40, "endOffset": 44}, {"referenceID": 6, "context": "11 Binarized NN [7] 6.", "startOffset": 16, "endOffset": 19}, {"referenceID": 5, "context": "74 Bitwise NN [6] 1.", "startOffset": 14, "endOffset": 17}, {"referenceID": 11, "context": "36 EBP [13] 4.", "startOffset": 7, "endOffset": 11}, {"referenceID": 7, "context": "08 TrueNorth [8] 7.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "For TrueNorth [8], we only cite the relevant accuracy results in Table 3.", "startOffset": 14, "endOffset": 17}, {"referenceID": 12, "context": "Moreover, if our FPGA design was built as an ASIC, it could use even less power by an order of magnitude [15].", "startOffset": 105, "endOffset": 109}, {"referenceID": 13, "context": "[16] propose the BinaryConnect (BC) method for binarizing only the weights, leaving the inputs and the activations as real-values.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "In the back-propagation phase, they use a quantization mechanism so that the multiplication operations are converted to bit-shift operations [17].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": "More recently, the same authors extend their idea to the activations of the neurons also [7].", "startOffset": 89, "endOffset": 92}, {"referenceID": 15, "context": "[18] propose Expectation Backpropagation (EBP), an algorithm for learning the weights of a binary network using a variational Bayes technique.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "Kim and Smaragdis propose Bitwise NN [6] which is a completely binary approach, where all the inputs, weights, and the outputs are binary.", "startOffset": 37, "endOffset": 40}, {"referenceID": 16, "context": "Recently, IBM announced an energy efficient TrueNorth chip, designed for spiking neural network architectures [19].", "startOffset": 110, "endOffset": 114}, {"referenceID": 7, "context": "[8] propose an algorithm for training networks that are compatible with IBM TrueNorth chip.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 51, "endOffset": 54}, {"referenceID": 15, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 94, "endOffset": 98}, {"referenceID": 5, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 192, "endOffset": 198}, {"referenceID": 7, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 251, "endOffset": 254}, {"referenceID": 0, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 255, "endOffset": 261}, {"referenceID": 0, "context": "BC [16] R {\u22121, 0, 1} R R {\u22121, 0, 1} R Binarized NN [7] R {\u22121, 1} {\u22121, 1} R {\u22121, 1} {\u22121, 1} EBP[18] R R R R {\u22121, 0, 1} {\u22121, 1} Bitwise NN [6] (\u22121, 1) (\u22121, 1) (\u22121, 1) {\u22121, 1} {\u22121, 0, 1} {\u22121, 1} [0, 1] (\u22121, 1) (\u22121, 1) {0, 1} {\u22121, 0, 1} {\u22121, 1} TrueNorth [8] [0, 1] [\u22121, 1] [0, 1] {0, 1} {\u22121, 0, 1} {0, 1} Ternary NN {0, 1} R {\u22121, 0, 1} {0, 1} {\u22121, 0, 1} {\u22121, 0, 1}", "startOffset": 270, "endOffset": 276}], "year": 2017, "abstractText": "The computation and storage requirements for Deep Neural Networks (DNNs) are usually high. This issue limit their deployability on ubiquitous computing devices such as smart phones or wearables. In this paper, we propose ternary neural networks (TNNs) in order to make deep learning more resource-efficient. We train these TNNs using a teacher-student approach. Using only ternary weights and ternary neurons, with a step activation function of two-thresholds, the student ternary network learns to mimic the behaviour of its teacher network. We propose a novel, layer-wise greedy methodology for training TNNs. During training, a ternary neural network inherently prunes the smaller weights by setting them to zero. This makes them even more compact thus more resource-friendly. We devise a purpose-built hardware design for TNNs and implement it on FPGA. The benchmark results with our purpose-built hardware running TNNs reveal that, with only 1.24\u03bcJ per image, we can achieve 97.76% accuracy with 5.37\u03bcs latency and with a rate of 255K images per second on MNIST.", "creator": "LaTeX with hyperref package"}}}