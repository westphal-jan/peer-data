{"id": "1609.00464", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2016", "title": "The Semantic Knowledge Graph: A compact, auto-generated model for real-time traversal and ranking of any relationship within a domain", "abstract": "This paper describes a new kind of knowledge representation and mining system which we are calling the Semantic Knowledge Graph. At its heart, the Semantic Knowledge Graph leverages an inverted index, along with a complementary uninverted index, to represent nodes (terms) and edges (the documents within intersecting postings lists for multiple terms/nodes). This provides a layer of indirection between each pair of nodes and their corresponding edge, enabling edges to materialize dynamically from underlying corpus statistics. As a result, any combination of nodes can have edges to any other nodes materialize and be scored to reveal latent relationships between the nodes. This provides numerous benefits: the knowledge graph can be built automatically from a real-world corpus of data, new nodes - along with their combined edges - can be instantly materialized from any arbitrary combination of preexisting nodes (using set operations), and a full model of the semantic relationships between all entities within a domain can be represented and dynamically traversed using a highly compact representation of the graph. Such a system has widespread applications in areas as diverse as knowledge modeling and reasoning, natural language processing, anomaly detection, data cleansing, semantic search, analytics, data classification, root cause analysis, and recommendations systems. The main contribution of this paper is the introduction of a novel system - the Semantic Knowledge Graph - which is able to dynamically discover and score interesting relationships between any arbitrary combination of entities (words, phrases, or extracted concepts) through dynamically materializing nodes and edges from a compact graphical representation built automatically from a corpus of data representative of a knowledge domain.", "histories": [["v1", "Fri, 2 Sep 2016 04:26:54 GMT  (1232kb,D)", "http://arxiv.org/abs/1609.00464v1", "Accepted to be published in 2016 IEEE 3rd International Conference on Data Science and Advanced Analytics"], ["v2", "Mon, 5 Sep 2016 15:06:45 GMT  (1229kb,D)", "http://arxiv.org/abs/1609.00464v2", "Accepted for publication in 2016 IEEE 3rd International Conference on Data Science and Advanced Analytics"]], "COMMENTS": "Accepted to be published in 2016 IEEE 3rd International Conference on Data Science and Advanced Analytics", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["trey grainger", "khalifeh aljadda", "mohammed korayem", "ries smith"], "accepted": false, "id": "1609.00464"}, "pdf": {"name": "1609.00464.pdf", "metadata": {"source": "CRF", "title": "The Semantic Knowledge Graph: A compact, auto-generated model for real-time traversal and ranking of any relationship within a domain", "authors": ["Trey Grainger", "Khalifeh AlJadda", "Mohammed Korayem", "Andries Smith"], "emails": ["andries.smith}@careerbuilder.com"], "sections": [{"heading": null, "text": "In fact, most of them will be able to move to another world, where they will be able to move to another world, where they will be able to move to another world, where they will be able to move to where they are."}, {"heading": "II. RELATED WORK", "text": "Ontologies are used in various areas to build vocabulary that is shared and used by domain experts. Ontologies can be divided into three different categories: formal ontologies that have axioms and definitions in logic, and terminological ontologies that allow a common understanding of the structure of information, and the ability to analyze domain knowledge. Ontologies can be divided into three different categories."}, {"heading": "III. METHODOLOGY", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Problem Description", "text": "In fact, most of them are able to move to another world, in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "B. Model Structure", "text": "(Consider an undirected graph G = (V, E), where V and E-V \u00b7 V denote the sets of nodes or edges, respectively. We define the following: \u2022 D = {d1, d2,..., dm} is a set of documents that represent a corpus that the semantic knowledge graph will use to extract and evaluate semantic relationships. \u2022 X = {x1, x2,..., xk} is a set of elements stored in D. \u2022 T = {t1, t2,... tn} where ti is an element that assigns an entity type to an element, e.g. a keyword, title, location, company, person, etc. Considering the previous notations, the set of node V can be defined in our graph as an element where ti is an element that assigns an entity type to an element, e.g. a keyword, title, company, person, etc."}, {"heading": "C. Materialization of Nodes and Edges", "text": "The core of the SKG model is the idea that there is a layer of indirection between two nodes vi and vj and the edge eij that connects them. Specifically, this means that the edge eij between nodes vi and vj is materialized whenever | f (eij) | > 0.To traverse the graph from the source node vi to the target node vj, our system needs a search index that links node vi to a set of documents, as well as a separate search index that can be derived from these documents to the node vj or other node. We refer to this first index as our Terms-Docs inverted index, and to the second index as our Docs-Terms unlinked, both of which are validated in Figure 2."}, {"heading": "D. Scoring Semantic Relationships", "text": "The semantic knowledge graph (SKG) is able to capture the strength of the semantic relationship between the entities at the edge. For example, if we do not know how semantic the keyword java is, we can use the SKG to capture the relationship between these two terms. In this case, the semantic relationship between item xi and item xj is materialized to capture the source node vi (that of xi) and the goal of the documents that contain xj."}, {"heading": "E. Discovering Semantic Relationships", "text": "In addition, this model enables the materialization of nodes and the extraction of relationships using these materialized nodes. To discover associated elements with a specific tag tk to an element xi with tag tj, we begin by querying the inverted index for the element xi, which we assign as node vi, which corresponds to the document set Dvi. We query the docs terms for the tag tk and save the retrieved documents as Dtk = {d | x-D, x: tk}. We define Vvi, tk = {vj | xj-D, d-Dtk-Dvi}, where vjis is a node that stores an element xj, and we define Vvi, tk as the set of nodes that store elements with potential relationship to xi of type tk (see Figure 3 (a). Finally, we apply Vj-Vvi, tk, these relationships (relatj relationships to), to the relationships between the latvi and (v)."}, {"heading": "IV. SYSTEM IMPLEMENTATION", "text": "In fact, we are able to find ourselves in a situation in which we are able to outdo each other, in which we outdo each other, in which we outdo each other, in which we outdo each other, in which we outdo each other, in which we outdo each other, in which we outdo each other, in which we outdo each other, in which we outdo each other, in which we outdo each other, in which we outdo each other."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": "While the SKG is a generally applicable model for all areas that can be represented by documents with overlapping references on the same units, we focused our tests on use cases within the job search area, using records provided to us by CareerBuilder, one of the largest job boards in the world. For our experiments, we used two sets of data: 1) a collection of 3 million job ads and 2) a collection of 1 million job seekers \"CVs containing a total of 3 million sections of the job history (representing a jobseeker's previous jobs). While these two sets of data could be combined into a single chart, we only had to use a single set of data at the same time, and thus maintain each set of data in a separate SKG for subsequent experiments. All of our experiments used the SKG implementation described in the System Implementation section, which was opened together with the publication of this paper. In terms of performance, the SKG was able to collect tens of millions of data sets in a few seconds, running through the thousands of relationships in most of years."}, {"heading": "A. Data Cleansing", "text": "Most records contain some dirty data, especially when free-text content is involved. While we have previously described how the SKG can detect relationships within a corpus of documents, it is just as good in ranking the relationships provided by the user. Use Case: As an example, we list a list of relationships broken down by search engine query protocols described in [19, 20]. The idea behind this is that users who perform similar search results often search for related terms and phrases. For example, someone looking for registered nurses will search ER, hospital and so on. Someone looking for Java will often search for software developers, and so on."}, {"heading": "B. Predictive Analytics:", "text": "The question that arises is whether the people who are able to survive themselves, to survive themselves, or whether they are able to survive themselves, to survive themselves. The question that arises is whether they are able to survive themselves, is whether they are able to survive themselves, is whether they are able to survive themselves, is whether they are able to survive themselves, or whether they are able to survive themselves. The question is whether they are able to survive themselves or not."}, {"heading": "C. Document Summarization", "text": "Another interesting application of the SRG is the identification of the most important topics within a document. In any case, some words are used in relation to the topic of the document, while others are unimportant."}, {"heading": "VI. FUTURE WORK", "text": "This year it is more than ever before."}, {"heading": "VII. CONCLUSION", "text": "This system enables the automatic creation of a graph that encodes statistical relationships between all keywords, phrases and entities represented within free text and semi-structured documents, allowing these relationships to be traversed and evaluated by the strength of the relationship within a specific range. In contrast to conventional graph databases, which perform either the most in-depth search or the broadest search for all nodes, because the Semantic Knowledge Graph materializes edges between nodes and phrases or structured data (titles, categories, data, numbers, etc.) or structured data (titles, data, numbers, etc.), unlike traditional graph databases, which perform either the most in-depth search or the broadest search for all nodes, the Semantic Knowledge Graph materializes the edges between nodes and assigns their weighting to the fly, based on the number of overlapping documents or the connections of the nodes within a corus of documents."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank CareerBuilder for supporting this research and development. In particular, the authors would like to thank Daniel Crouch, David Bernal, Lamar Payson and Jacob Maggio, who also contributed their ideas and code reviews during the development of the semantic knowledge graph, as well as Colin Field, Matt McNair, Eric Presley and Abdel Tefridj for their support in the development and open sourcing of the referenced implementation."}], "references": [{"title": "Scalable knowledge harvesting with high precision and high recall", "author": ["Ndapandula Nakashole", "Martin Theobald", "Gerhard Weikum"], "venue": "In Proceedings of WSDM", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Algorithm and tool for automated ontology merging and alignment", "author": ["Natalya Fridman Noy", "Mark A Musen"], "venue": "In Proceedings of the AAAI 2000.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Development of an automated ontology generator for analyzing customer concerns", "author": ["Meghan Daly", "Fletcher Grow", "Mackenzie Peterson", "Jeremy Rhodes", "Robert L Nagel"], "venue": "In Systems and Information Engineering Design Symposium (SIEDS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In Proceedings of KDD", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Crowdsourced query augmentation through semantic discovery of domain-specific jargon", "author": ["Khalifeh AlJadda", "Mohammed Korayem", "Trey Grainger", "Chris Russell"], "venue": "In IEEE Big Data", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Pgmhd: A scalable probabilistic graphical model for massive hierarchical data problems", "author": ["K. AlJadda", "M. Korayem", "C. Ortiz", "T. Grainger", "J.A. Miller", "W.S. York"], "venue": "In Big Data (Big Data),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Using tf-idf to determine word relevance in document queries", "author": ["Juan Ramos"], "venue": "In Proceedings of the first instructional conference on machine learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2003}], "referenceMentions": [{"referenceID": 0, "context": "Other approaches (DeepDive [8], Nell2RDF [9], and PROSPERA [10]) crawl the web and use machine learning and natural language processing to build web-scale knowledge graphs.", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "While ontology learning systems are typically able to automate much of the ontology building (and sometimes maintenance) process, this comes at the expense of a loss of accuracy due to the replacement of human experts with more error-prone algorithms [11\u201315].", "startOffset": 251, "endOffset": 258}, {"referenceID": 2, "context": "While ontology learning systems are typically able to automate much of the ontology building (and sometimes maintenance) process, this comes at the expense of a loss of accuracy due to the replacement of human experts with more error-prone algorithms [11\u201315].", "startOffset": 251, "endOffset": 258}, {"referenceID": 3, "context": "While ontology learning systems are typically able to automate much of the ontology building (and sometimes maintenance) process, this comes at the expense of a loss of accuracy due to the replacement of human experts with more error-prone algorithms [11\u201315].", "startOffset": 251, "endOffset": 258}, {"referenceID": 4, "context": "Use Case: As an example use case, we leveraged the SKG to clean a list of relationships mined from search engine query logs using a similar methodology to that described in [19, 20].", "startOffset": 173, "endOffset": 181}, {"referenceID": 5, "context": "Use Case: As an example use case, we leveraged the SKG to clean a list of relationships mined from search engine query logs using a similar methodology to that described in [19, 20].", "startOffset": 173, "endOffset": 181}, {"referenceID": 6, "context": "In other scenarios where no category for the document is known a priori, it is possible to instead leverage other statistics from the terms-docs inverted index, such as tf-idf scoring of each term within the document, to find the set of most globally interesting terms within the document [22].", "startOffset": 289, "endOffset": 293}], "year": 2016, "abstractText": "This paper describes a new kind of knowledge representation and mining system which we are calling the Semantic Knowledge Graph. At its heart, the Semantic Knowledge Graph leverages an inverted index, along with a complementary uninverted index, to represent nodes (terms) and edges (the documents within intersecting postings lists for multiple terms/nodes). This provides a layer of indirection between each pair of nodes and their corresponding edge, enabling edges to materialize dynamically from underlying corpus statistics. As a result, any combination of nodes can have edges to any other nodes materialize and be scored to reveal latent relationships between the nodes. This provides numerous benefits: the knowledge graph can be built automatically from a real-world corpus of data, new nodes along with their combined edges can be instantly materialized from any arbitrary combination of preexisting nodes (using set operations), and a full model of the semantic relationships between all entities within a domain can be represented and dynamically traversed using a highly compact representation of the graph. Such a system has widespread applications in areas as diverse as knowledge modeling and reasoning, natural language processing, anomaly detection, data cleansing, semantic search, analytics, data classification, root cause analysis, and recommendations systems. The main contribution of this paper is the introduction of a novel system the Semantic Knowledge Graph which is able to dynamically discover and score interesting relationships between any arbitrary combination of entities (words, phrases, or extracted concepts) through dynamically materializing nodes and edges from a compact graphical representation built automatically from a corpus of data representative of a knowledge domain. The source code for our Semantic Knowledge Graph implementation is being published along with this paper to facilitate further research and extensions of this work.", "creator": "LaTeX with hyperref package"}}}