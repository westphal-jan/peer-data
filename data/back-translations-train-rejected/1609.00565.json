{"id": "1609.00565", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "2-Sep-2016", "title": "Skipping Word: A Character-Sequential Representation based Framework for Question Answering", "abstract": "Recent works using artificial neural networks based on word distributed representation greatly boost the performance of various natural language learning tasks, especially question answering. Though, they also carry along with some attendant problems, such as corpus selection for embedding learning, dictionary transformation for different learning tasks, etc. In this paper, we propose to straightforwardly model sentences by means of character sequences, and then utilize convolutional neural networks to integrate character embedding learning together with point-wise answer selection training. Compared with deep models pre-trained on word embedding (WE) strategy, our character-sequential representation (CSR) based method shows a much simpler procedure and more stable performance across different benchmarks. Extensive experiments on two benchmark answer selection datasets exhibit the competitive performance compared with the state-of-the-art methods.", "histories": [["v1", "Fri, 2 Sep 2016 11:57:46 GMT  (160kb,D)", "http://arxiv.org/abs/1609.00565v1", "to be accepted as CIKM2016 short paper"]], "COMMENTS": "to be accepted as CIKM2016 short paper", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lingxun meng", "yan li", "mengyi liu", "peng shu"], "accepted": false, "id": "1609.00565"}, "pdf": {"name": "1609.00565.pdf", "metadata": {"source": "CRF", "title": "Skipping Word: A Character-Sequential Representation based Framework for Question Answering", "authors": ["Lingxun Meng", "Yan Li", "Mengyi Liu", "Peng Shu"], "emails": ["menglingxun@sogou-", "mengyi.liu}@vipl.ict.ac.cn", "shupeng203672@sogou-", "permissions@acm.org."], "sections": [{"heading": null, "text": "Tags Semantic Matching; Deep Learning; Convolutional Neural Networks; Word Embedings;"}, {"heading": "1. INTRODUCTION", "text": "Inspired by the achievements of the Convolutionary Networks (a.k.a, ConvNets) in the field of computer vision, more and more ConvNets researchers are re-ranking [14, 11], e.t.c. Almost all of them pointed out that ConvNets could successfully capture the local syntactic structure to increase performance. In addition to the appropriate use of ConvNets, the researchers also owed their success to the recent distributed representation of words, such as word2vec [9]. No doubt, perfect word embedding (WE) could greatly increase performance, but there are so many English word embedding systems online, trained with varying permissions to make digital or hard copies of all or parts of this work for personal or classroom use, provided that copies are not distributed for profit or commercial advantage and that copies are included on the first page."}, {"heading": "2. SEMANTIC MATCH LEARNING", "text": "Most previous work formulated sentences, phrases, or texts using semantic parsing methods, which require experts to create grammars and knowledge data schemes [1]. Lately, much work has been done using ConvNets to build text based on word representation [11]. Our method follows this constitution, but replaces the word reference table with a more basic character reference table, and the reference table, revolutionary filters, and normalization parameters are shared between question and answer pairs (see fig. 1 for a better understanding).ar Xiv: 160 9.00 565v 1 [cs.C L] 2S ep2 016"}, {"heading": "2.1 Character-Sequential Representation", "text": "The sentence in this work is represented as a sequence of characters: [c0,..., c | s | \u2212 1], where ci is taken from the character set C. Each character is represented by its corresponding index in the character set C, namely {0, \u00b7 \u00b7 \u00b7, | C | \u2212 1}. To learn the character representation, we design the first level of our model as an overview table, i.e., the character embedding matrix W-Rd \u00b7 | C |, where each column represents a character that is c-dimensional. From this level, our sentence is represented as follows: S = [c0 c1 \u00b7 \u00b7 c | s | \u2212 1]. (1) In contrast to earlier embedding methods, we construct sentences directly from characters, skipping the transition step of modeling word comprehension (i.e., the character set instead of the character set)."}, {"heading": "2.2 Convolution and Pooling", "text": "The folding layer consists of a filter bank F-Rn-c-w, together with the filter biases b-Rn, where n and w refer to the number and width of the filters, and c denotes the channels of the data from the bottom layer. Specifically, for the first folding layer c corresponds to the embedding dimension d, which means uniting over the letters to learn the pattern. In our construction, the entire folding operation is one-dimensional. Formally, the output of the folding with the filter bank F over the set S is calculated as follows: T = [ti, j] = F-type S + b = [F-Ti-Sj-w + 1: j + bi] (2), in which i the number of the filter bank F, Fi-R (d-w) is the vectorization of each filter card, Sj-w-1: j vectorization of the matrix covering embedding from the index."}, {"heading": "2.3 Batch Normalization", "text": "Batch normalization (BN) [2] was originally proposed to reduce the changes in the distribution of the individual layers entered during training, which was called internal covariance shift. As an important component of deep networks, it allows us to use much higher learning rates and to be less careful with initial data preprocessing and weight initialization. In addition, it can also act as regularization, in some cases without the use of dropout.Specifically, for the specified output T after the folding process, we will normalize each ti along the filter axis as a sequence: t-i = ti-E [ti] Var [ti], (3) where the expectation and variance are calculated over the entire training dataset. A pair of parameters: i and \u03b2i (with the same dimension with ti) are further used to scale and shift the normalized value as a sequence: BN (T) = [targets i \u00b7 t-i + i]."}, {"heading": "3. EXPERIMENTS", "text": "We evaluate the proposed method on the basis of two benchmarks for the problem of solution selection, namely TrecQA [12] and WikiQA [13]."}, {"heading": "3.1 Datasets", "text": "TrecQA1 was collected from the QA track data of the Text Retrieval Conference (TREC), and the correctness of the chosen answer was guaranteed by manual judgment for parts of the dataset. WikiQA2 is another open domain question and answer data set collected from real-world queries made by the Bing search engine without hu-1http: / / cs.stanford.edu / people / mengqiu / data / qg-emnlp07-data.tgz 2http: / / research.microsoft.com / en-us / downloads / 4495da01-db8c-4041-a7f6-7984f6a905 / man editorial revision. Table 1 summarizes the statistics for the two datasets."}, {"heading": "3.2 Experimental Setup", "text": "We set the maximum word length of Questions and Answers to 30 / 80 for the TrecQA record or 50 / 50 for the WikiQA record. We also set the maximum character length of Questions and Answers to 192 / 386 for the TrecQA record or 125 / 386 for the WikiQA record. All words cannot be found in the spoken word lookup table, which is supplemented by an additional index called UNK IND.In the following [16] our character set C also consists of 71 characters, including 26 lowercase English letters, 10 digits, 33 other characters, the new line character, the padding symbol and the unknown symbol. The characters are as follows: abcdefghijklmnopqrstuvwxyz0123456789,;!?: '/\\ delimiter \"026B30D @ # $% 4\" & * + - = < > (] {]}"}, {"heading": "4. RESULTS AND DISCUSSIONS", "text": "The configuration parameters of both Word Embedding (WE) and Character Sequential Representation (CSR) based networks are listed in Table 2 (we carefully implement WE-based network architecture according to [11]). In addition, the similarity of the weight matrix (sim (x, y) = xTMy, where M is the similarity matrix) between the question pairs mentioned in [11] is learned to perform a better matching. Simple word overlap and weighted word overlap are used in both countries. We use the Caffe [3] open deep learning frames to complete our implementation."}, {"heading": "5. CONCLUSION", "text": "To solve this problem, we propose an End-to-End Character-Sequential Representation (CSR) based ConvNets for the selection of answers, which simply models sentences by strings and then combines the embedding of characters with point-by-point response ranking training. Extensive experiments provide further evidence that the construction directly from character to sentence works just as well or even better than the hierarchical formula of character word tendency. We hope that this work can bring some insights to the research and industrial community that corpus-free and skipped word representation from character directly to sentence could play a role as an end-to-end strategy in different languages, with its benefits of simplification and the concise body."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "We would like to thank all members of the Wireless Sponsored Search Engine Advertising Team (ADWR) for the detailed discussion and implementation. The authors also thank Dingding Qian and Kai Chen for the proposal for submission of the paper. This paper is partially supported by the Natural Science Foundation of Zhejiang Province, PR China under contract number LQ15F020005."}, {"heading": "7. REFERENCES", "text": "[1] J. Berant, A. Chou, R. Frostig, and P. Liang.Semantic parsing on freebase from question-answer pairs. In EMNLP, 2013. [2] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, 2015. [3] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell. Caffe: Convolutional architecture for fast feature embedding. CoRR, 2014. [4] N. Kalchbrenner, E. Grefenstette, and P. Blunsom. A revolutionary neural network for modeling sentences. In ACL, pp. 655-665, 2014. [5] Y. Y. Kim."}], "references": [{"title": "Semantic parsing on freebase from question-answer pairs", "author": ["J. Berant", "A. Chou", "R. Frostig", "P. Liang"], "venue": "In EMNLP,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "A convolutional neural network for modelling sentences", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": "In ACL,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Character-aware neural language models", "author": ["Y. Kim", "Y. Jernite", "D. Sontag", "A.M. Rush"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Distributed representations of sentences and documents", "author": ["Q.V. Le", "T. Mikolov"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Neural variational inference for text processing", "author": ["Y. Miao", "L. Yu", "P. Blunsom"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Learning to rank short text pairs with convolutional deep neural networks", "author": ["A. Severyn", "A. Moschitti"], "venue": "In Proceedings of SIGIR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "What is the jeopardy model? a quasi-synchronous grammar for qa", "author": ["M. Wang", "N.A. Smith", "T. Mitamura"], "venue": "In EMNLP-CoNLL,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Wikiqa: A challenge dataset for open-domain question answering", "author": ["Y. Yang", "W.-t. Yih", "C. Meek"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Question answering using enhanced lexical semantic models", "author": ["W.-t. Yih", "M.-W. Chang", "C. Meek", "A. Pastusiak"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Deep learning for answer sentence selection", "author": ["L. Yu", "K.M. Hermann", "P. Blunsom", "S. Pulman"], "venue": "NIPS Deep Learning and Representation Learning Workshop,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Character-level convolutional networks for text classification", "author": ["X. Zhang", "J. Zhao", "Y. LeCun"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}], "referenceMentions": [{"referenceID": 4, "context": ", text classification [5], short text pair re-ranking [14, 11], e.", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": ", text classification [5], short text pair re-ranking [14, 11], e.", "startOffset": 54, "endOffset": 62}, {"referenceID": 10, "context": ", text classification [5], short text pair re-ranking [14, 11], e.", "startOffset": 54, "endOffset": 62}, {"referenceID": 8, "context": "In addition to the appropriate use of ConvNets, researchers also owed their success to recent distributed representation of words, like word2vec [9].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "Recently, some researchers start to use character representation to constitute different natural language models based on the fact that character embedding model not only alleviates the parametric burden significantly, but also brings more ability of handling morphologically rich language and out-of-vocabulary words [6].", "startOffset": 318, "endOffset": 321}, {"referenceID": 15, "context": "Besides, character-level representation has also been successfully investigated for text classification [16], where one-hot representation is feed into Deep ConvNets.", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "The majority of previous works formulated sentence, phrase or text using semantic parsing methods, which require experts to handcraft grammars and knowledge base schema [1].", "startOffset": 169, "endOffset": 172}, {"referenceID": 10, "context": "Recently, lots of works spring up by using ConvNets to constitute text based on word representation [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 3, "context": "The previous works [4] pointed out that using wide type of convolution was able to better and more frequently reach boundaries of sentences than the narrow type.", "startOffset": 19, "endOffset": 22}, {"referenceID": 3, "context": "Recently, k-max pooling strategy [4] was proposed to extract k large activation values rather than the topmost one, and this could construct a deeper architecture with several convolutional layers.", "startOffset": 33, "endOffset": 36}, {"referenceID": 1, "context": "Batch Normalization (BN) [2] was originally proposed to reduce the changes in distribution of each layers input during training, which was called internal covariance shift .", "startOffset": 25, "endOffset": 28}, {"referenceID": 11, "context": ", TrecQA [12] and WikiQA [13].", "startOffset": 9, "endOffset": 13}, {"referenceID": 12, "context": ", TrecQA [12] and WikiQA [13].", "startOffset": 25, "endOffset": 29}, {"referenceID": 15, "context": "Following [16], our character set C also consists of 71 characters, including 26 english lower letters, 10 digits, 33 other characters, the new line character, the padding symbol and the unknown symbol.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "The configuration parameters of both word embedding (WE) and character-sequential representation (CSR) based networks are listed in Table 2 (we carefully implemented WE based network architecture according to [11]).", "startOffset": 209, "endOffset": 213}, {"referenceID": 10, "context": "Additionally, similarity weight matrix (sim(x, y) = xMy, where M is the similarity matrix) between question answer pairs is learnt along with WE models training, that is mentioned in [11] to perform a better matching.", "startOffset": 183, "endOffset": 187}, {"referenceID": 2, "context": "We use the open deep learning framework Caffe [3] to complete our implementation.", "startOffset": 46, "endOffset": 49}, {"referenceID": 8, "context": "We use three sets of word embedding for WE based models: (i) word2vec [9] is trained on part of Google News dataset, containing 100 billion words; (ii) GloVe-twitter [10] is trained on 2 billion tweets, containing 27 billion tokens; (iii) GloVe-6B is trained on Wikipedia data and the http://trec.", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "We use three sets of word embedding for WE based models: (i) word2vec [9] is trained on part of Google News dataset, containing 100 billion words; (ii) GloVe-twitter [10] is trained on 2 billion tweets, containing 27 billion tokens; (iii) GloVe-6B is trained on Wikipedia data and the http://trec.", "startOffset": 166, "endOffset": 170}, {"referenceID": 11, "context": "Jeopardy[12] .", "startOffset": 8, "endOffset": 12}, {"referenceID": 6, "context": "6852 PV[7] .", "startOffset": 7, "endOffset": 10}, {"referenceID": 13, "context": "6058 LCLR[14] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "7700 Bigram[15] .", "startOffset": 11, "endOffset": 15}, {"referenceID": 14, "context": "6086 Bigram[15] .", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "7846 LCLR[14] .", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "6190 NASM [8] .", "startOffset": 10, "endOffset": 13}, {"referenceID": 12, "context": "8117 CNN[13] .", "startOffset": 8, "endOffset": 12}], "year": 2016, "abstractText": "Recent works using artificial neural networks based on word distributed representation greatly boost the performance of various natural language learning tasks, especially question answering. Though, they also carry along with some attendant problems, such as corpus selection for embedding learning, dictionary transformation for different learning tasks, etc. In this paper, we propose to straightforwardly model sentences by means of character sequences, and then utilize convolutional neural networks to integrate character embedding learning together with point-wise answer selection training. Compared with deep models pre-trained on word embedding (WE) strategy, our character-sequential representation (CSR) based method shows a much simpler procedure and more stable performance across different benchmarks. Extensive experiments on two benchmark answer selection datasets exhibit the competitive performance compared with the state-of-the-art methods.", "creator": "LaTeX with hyperref package"}}}