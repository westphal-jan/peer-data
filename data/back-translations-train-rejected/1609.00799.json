{"id": "1609.00799", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Sep-2016", "title": "Lexical-Morphological Modeling for Legal Text Analysis", "abstract": "In the context of the Competition on Legal Information Extraction/Entailment (COLIEE), we propose a method comprising the necessary steps for finding relevant documents to a legal question and deciding on textual entailment evidence to provide a correct answer. The proposed method is based on the combination of several lexical and morphological characteristics, to build a language model and a set of features for Machine Learning algorithms. We provide a detailed study on the proposed method performance and failure cases, indicating that it is competitive with state-of-the-art approaches on Legal Information Retrieval and Question Answering, while not needing extensive training data nor depending on expert produced knowledge. The proposed method achieved significant results in the competition, indicating a substantial level of adequacy for the tasks addressed.", "histories": [["v1", "Sat, 3 Sep 2016 07:24:08 GMT  (143kb,D)", "http://arxiv.org/abs/1609.00799v1", "16 pages, 5 figures, Lecture notes in computer science: New Frontiers in Artificial Intelligence, 2016/03"]], "COMMENTS": "16 pages, 5 figures, Lecture notes in computer science: New Frontiers in Artificial Intelligence, 2016/03", "reviews": [], "SUBJECTS": "cs.IR cs.CL", "authors": ["danilo s carvalho", "minh-tien nguyen", "tran xuan chien", "minh le nguyen"], "accepted": false, "id": "1609.00799"}, "pdf": {"name": "1609.00799.pdf", "metadata": {"source": "CRF", "title": "Lexical-Morphological Modeling for Legal Text Analysis", "authors": ["Danilo S. Carvalho", "Minh-Tien Nguyen", "Chien-Xuan Tran", "Minh-Le Nguyen"], "emails": ["nguyenml}@jaist.ac.jp"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "2 Related Works", "text": "Liu, Chen and Ho [3] presented the Three-Phase Prediction (TPP) method for retrieving the relevant statutes in Taiwanese criminal law, since general language queries. The method was a hierarchical ranking approach for legal corporations, using a combination of several information response techniques, as well as machine learning and feature selection ones. The results were evaluated in terms of recall, resulting in a new system for solving RTE problems. Inkpen et al. showed one of the first successful models for RTE with SVMs [5]. Later, Castillo proposed a new system for solving RTE with SVMs [6], which included training data RTE-3, annotated data for RTE-5, and the development of RTE-5. 32 features were used and the training model achieved the best F measures in two respects."}, {"heading": "3 Legal Question Answering", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "4 Proposed Approach", "text": "In order to be able to perform both relevance analysis and textual tailment recognition independently in phase one and two and jointly in phase three, IR methods and classification methods were developed separately. First, both the right corpus and training data are analyzed and combined into representation models. Afterwards, the models are used to evaluate articles or classify answers according to the task at hand. The representation model used for relevance analysis is a mixed n-gram collection and the model used for word processing are feature vectors for machine learning. Figure 2 shows the overall overview of the proposed method."}, {"heading": "4.1 Relevance Analysis", "text": "A detailed analysis of the Civil Code and training data has shown that lexical and syntactical overlaps can vary to a high degree between questions and articles, and also between articles that relate to the same topic. However, certain morphological characteristics, such as the terms that exhibit a higher consistency between the topics, are. Therefore, the representation model used was called a mixed model with which to deal in terms of the way in which the terms are lemmatized. Therefore, R2NC was called (Ranking Related N-gram Collections). A summarized view of the process is shown in Figure 3. Steps to construct the model are detailed as follows: 1. Collect the entire content for each article, including section title; 2. Checking references between articles and annotates accordingly; 3. Tokenize and POS day."}, {"heading": "4.2 Textual Entailment", "text": "The first level, however, describes whether the evidence supports or rejects the hypothesis (YES / NO, respectively). However, due to the time constraints of the competition, only the first level is examined. Therefore, semantic relationships such as negation and antonym cannot be taken into account in the TE evaluation. To detect a TE relationship on a pair (Q, a), a similar approach [4] can be used in which an answer to Q can be found if the similarity is greater than a certain threshold. However, high-level inferences (see section 3) and the identification of the threshold will make these methods more difficult to apply. We therefore propose to use the classification of the TE relationship with two advantages."}, {"heading": "5 Experiments and Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "The data set was obtained from the published data for COLIEE Shared Task 10, consisting of a text file with the Japanese Civil Code and a series of XML files containing training and test data for phases one to three. The training set for the three tasks consists of 267 pairs (question, relevant articles), divided only into phases one and two, respectively, of information gathering and textual adjustment methods. Each experiment included: i) data analysis, ii) model and parameter adjustments, and iii) test runs. 10 webdocs.cs.ualberta.ca / m"}, {"heading": "5.2 Parameter adjustment", "text": "For R2NC, the parameters Iq, Iart, confidence threshing (here abbreviated to ct), reference threshing (rt) and also k, the maximum n-gram size, were empirically adjusted to the training data using the following simple procedure: - Starting from Iq = 0.8, ct = 0.5, rt = 0.5 and k = 1, 1. A single parameter is increased or decreased by step r = 0.1 until the F measure cannot be increased for a leave-one-out test. 2. Repeat (1) starting from the last value obtained with r = 0.01. 3. Repeat (1) and (2) for all parameters. For k, the step was set to r = 1. Iq and Iart. The parameters Iq + Iart respect the constraint Iq = 1. The parameters are changed in a specific order: 1. Confidence thesh, 2. k, 3. Reference thesh, 4. Iq."}, {"heading": "5.3 Baselines", "text": "As for the second edition of COLIEE, there is still no definitive baseline for the competition data set. However, common baseline techniques and related work results can be used 11 weka.wikispaces.com to evaluate the performance of each task. In phase one, a relationship between R2NC and TPP can be drawn [3]. For the TE task, the following basic principles were used for comparison: - SVMs: uses Support Vector Machines (SVMs) 12 [19] with Weka. The parameters are C = 1, \u03b3 = 0, Kernel Type = radial base function (RBF). - AdaBoost SVMs: uses SVMs as weak learners instead of DecisionStump."}, {"heading": "5.4 Evaluation Method", "text": "Given the limited training data available, the performance of the model in both tasks of the training data set was evaluated by three measures: Accuracy (P), callback (R), and F measure (F) as in Equation (3), (4), and (5). In phase two, Accuracy (A) is measured as in Equation (6). P = CrRt (3) R = Cr Rl (4) F = 2 (P \u0445 R) P + R (5) A = CqQ (6), where Cr counts correctly retrieved items for all queries, Rt counts correctly retrieved items for all queries, Rl counts the relevant items for all queries, Cq counts correctly confirmed queries as true or false, and Q counts all queries."}, {"heading": "5.5 Pre-competition Results", "text": "The results suggest that R2NC is likely to compete with state-of-the-art approaches to relevance analysis in legal documents such as TPP [3]. However, the proposed method is much simpler compared to TPP and works with significantly less training data: 266 documents for R2NC versus 1518 documents for TPP. The design of R2NC also makes it difficult for the model to be overtrained beyond parameter adjustment, as no training data is counted 12 https: / / www.csie.ntu.edu.tw / \u0445 cjlin / libsvm / more than one shot, as opposed to convergence based. Experiments were repeated using traditional TF-IDF scoring instead of R2NC formula, which yields 0.51 F measurements."}, {"heading": "5.6 Feature Evaluation", "text": "The results show that all effective characteristics contribute to the method. Note that both Damerau-Levenshtein and Euclidean are distance characteristics, whereas the longest common substring (lcs) is a statistical characteristic. Results show that there is not much word overlap between a question and relevant articles in legal texts. An interesting aspect is that the Word2Vec similarity has a large positive impact on the model, which supports the conclusion on similarity set forth in Section 5.5."}, {"heading": "5.7 Competition Results", "text": "The methodology presented in this paper achieved significant results in COLIEE, coming second in phase one (IR) and third in phase three (combined IR + TE), and was not well placed in phase two (TE).The relevant competition results are presented in Table 6 as announced in JURISIN 2015."}, {"heading": "5.8 Post-competition Analysis and Improvements", "text": "The post-competition analysis pointed out potential sources of classification problems in Phase 2 (TE) and also gave indications of improvements in both tasks. For R2NC, the lack of an implicit semantic mapping was an important factor compared to the highest-ranking approach. To compensate for this, a term dictionary was added as a new source of information to expand the question-n-gram models described in Section 4.1. By using linguistic observations, it was possible to create basic entries in the dictionary (non-specialist knowledge), improving the Phase 1 F measure for common data (Table 3) from 0.54 to 0.55. In Phase 2, overadjustment of training data was considered to be the main factor reducing classification performance. Our system achieved over 61% accuracy (Table 4) when it ran on the common data, but only 37.88% reported the results of the competition. The results of Phase 3 show that the accuracy of the classification improved with the more accurate assumption of the overfit information is consistent with that of the classification."}, {"heading": "5.9 Error Analysis and Discussion", "text": "An investigation was carried out on the ranking created with R2NC in phase one (see section 4.1), which found that relevant articles ranked 3rd and below contained keywords that did not appear in the corresponding question in the corpus, reinforcing the view that the questions are strongly directed, albeit at a conceptual level. Relevant articles ranked lower than 15th place (about 20%) require a relatively high degree of abstraction to obtain an interpretation that could be associated with the corresponding question. Table 7 shows an example of a complex relevance relationship. Table 8 shows a case where our system provides correct results (ID H18-2-4). In this example, there are several common words from which this approach can correctly assess the TE relationship, e.g. reimbursement. In addition, several words can be derived from the questions using the similarity of Word2Vec, e.g. person, manager, costs or expenses."}, {"heading": "6 Conclusion", "text": "This paper examines the difficult question of building a QA system in the legal field. In addition, we propose a model that comprises three stages: retrieval of legal information, retrieval of legal texts and replies to legal texts. In the first stage, a mixed n-gram model from morphological analysis is used to decide whether the questions can be answered positively or negatively by the retrieved articles. Finally, correct answers are provided to users in the final phase. Contributions to this work in the IR and TE task are: 1) a simple but effective language model for legal texts."}, {"heading": "Acknowledgements", "text": "This work is supported in part by the support of the NII Research Cooperation and the JAIST Research Grant."}], "references": [{"title": "Berring: \u201cThe heart of legal information: The crumbling infrastructure of legal research", "author": ["C. Robert"], "venue": "Legal information and the development of American law. St. Paul, MN: Thomson/West,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "The LKIF Core ontology of basic legal concepts", "author": ["Rinke Hoekstra", "Joost Breuker", "Marcello Di Bello", "Alexander Boer"], "venue": "Proc. of the Workshop on Legal Ontologies and Artificial Intelligence Techniques (LOAIT", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Predicting associated statutes for legal problems", "author": ["Yi-Hung Liu", "Yen-Liang Chen", "Wu-Liang Ho"], "venue": "Information Processing & Management", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Refining the judgment threshold to improve recognizing textual entailment using similarity.", "author": ["Quang-Thuy Ha", "Thi-Oanh Ha", "Thi-Dung Nguyen", "Thuy-Linh Nguyen Thi"], "venue": "Computational Collective Intelligence. Technologies and Applications", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Machine Learning Experiments for Textual Entailment", "author": ["Diana Inkpen", "Darren Kipp", "Vivi Nastase"], "venue": "Proceedings of the Second Challenge Workshop Recognising Textual Entailment : 17-20,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "An approach to Recognizing Textual Entailment and TE Search Task using SVM", "author": ["Julio Javier Castillo"], "venue": "Procesamiento del Lenguaje Natural,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Recognizing Textual Entailment in Vietnamese Text: An Experiment Study.", "author": ["Minh-Tien Nguyen", "Quang-Thuy Ha", "Thi-Dung Nguyen", "Tri-Thanh Nguyen", "Le-Minh Nguyen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Using Machine Translation for Recognizing Textual Entailment in Vietnamese Language.", "author": ["Quang Nhat Minh Pham", "Le Minh Nguyen", "Akira Shimazu"], "venue": "RIVF: 1-6,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "The third PASCAL recognising textual entailment challenge", "author": ["Danilo Giampiccolo", "Bernardo Magnini", "Ido Dagan", "Bill Dolan"], "venue": "Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing. Association for Computational Linguistics:", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Answering Legal Questions by Mining Reference Information", "author": ["Oanh Thi Tran", "Bach Xuan Ngo", "Minh Le Nguyen", "Akira Shimazu"], "venue": "New Frontiers in Artificial Intelligence. Springer International Publishing:", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Answering Yes/No Questions in Legal Bar Exams", "author": ["Mi-Young Kim", "Ying Xu", "Randy Goebel", "Ken Satoh"], "venue": "New Frontiers in Artificial Intelligence. Springer International Publishing:", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "RRE Task: The Task of Recognition of Requisite Part and Effectuation Part in Law Sentences.", "author": ["Bach Xuan Ngo", "Minh Le Nguyen", "Akira Shimazu"], "venue": "J. IJCPOL", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Reference Resolution in Legal Texts.", "author": ["Oanh Thi Tran", "Bach Xuan Ngo", "Minh Le Nguyen", "Akira Shimazu"], "venue": "In Proc. of ICAIL: 101-110,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Recognizing textual entailment: Rational, evaluation and approaches - Erratum", "author": ["Ido Dagan", "Bill Dolan", "Bernardo Magnini", "Dan Roth"], "venue": "Natural Language Engineering", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Intrinsic and Extrinsic Approaches to Recognizing Textual Entailment", "author": ["Rui Wang"], "venue": "Saarland University,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Ciprian Chelba", "Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "In Proceedings of NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2013}, {"title": "A decision-theoretic generalization of online learning and an application to boosting", "author": ["Yoav Freund", "Robert E. Schapire"], "venue": "Journal of computer and system sciences", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1997}, {"title": "Vapnik: \u201cSupport-Vector Networks", "author": ["Corinna Cortes", "Vladimir"], "venue": "Machine Learning", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1995}], "referenceMentions": [{"referenceID": 0, "context": "This growth is not accompanied by a matching increase in information analysis capabilities, which points to a severe under-utilization of available resources and to potential for information quality issues [1].", "startOffset": 206, "endOffset": 209}, {"referenceID": 1, "context": "Common legal ontologies are among the efforts to facilitate automatic legal reasoning, but have not seen strong development in the past years [2].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Liu, Chen and Ho [3] presented the three-phase prediction (TPP) method for retrieval of relevant statutes in Taiwan\u2019s criminal law, given general language queries.", "startOffset": 17, "endOffset": 20}, {"referenceID": 4, "context": "showed one of the first successful models for RTE using SVMs [5].", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "Later, Castillo proposed a new system for solving RTE using SVMs [6], in which training data includes RTE-3, annotated data set from RTE-4, and the development set of RTE-5.", "startOffset": 65, "endOffset": 68}, {"referenceID": 6, "context": "[7] conducted a study of RTE on a Vietnamese version of RTE-3 [8] translated from Giampiccolo et.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[7] conducted a study of RTE on a Vietnamese version of RTE-3 [8] translated from Giampiccolo et.", "startOffset": 62, "endOffset": 65}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "addressed legal text QA by using inference [10].", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "proposed a hybrid method containing simple rules and unsupervised learning using deep linguistic features to address RTE in civil law [11].", "startOffset": 134, "endOffset": 138}, {"referenceID": 6, "context": "This work uses all features in [7], as they apply to the same purpose.", "startOffset": 31, "endOffset": 34}, {"referenceID": 5, "context": "Our approach differs from [6] in using Word2Vec[17] similarity instead of WordNet.", "startOffset": 26, "endOffset": 29}, {"referenceID": 16, "context": "Our approach differs from [6] in using Word2Vec[17] similarity instead of WordNet.", "startOffset": 47, "endOffset": 51}, {"referenceID": 11, "context": ", requisite and effectuation [12].", "startOffset": 29, "endOffset": 33}, {"referenceID": 12, "context": "Another aspect is that law documents are written in a high abstraction level [13]; therefore, they often require collection and linking of multiple concept references to enable understanding and answering of a question.", "startOffset": 77, "endOffset": 81}, {"referenceID": 8, "context": ", an}), if Q is answered by ai (1 6 i 6 n), then ai entails Q [9], [14].", "startOffset": 62, "endOffset": 65}, {"referenceID": 13, "context": ", an}), if Q is answered by ai (1 6 i 6 n), then ai entails Q [9], [14].", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "To detect a TE relation on a pair (Q, a), a similarity-based approach [4] can be used, in which a can answer Q if the similarity is greater than a certain threshold.", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "[7], so all the features in that work were used.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "For the classification, the Weka toolset 8 implementation of AdaBoost [18] was used, with classifier = DecisionStump.", "startOffset": 70, "endOffset": 74}, {"referenceID": 2, "context": "For phase one, a relationship can be drawn between R2NC and TPP [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 18, "context": "For the TE task, the following baselines were used for comparison: \u2013 SVMs: uses Support Vector Machines (SVMs) [19] with Weka.", "startOffset": 111, "endOffset": 115}, {"referenceID": 2, "context": "The results indicate that R2NC is expected to be competitive with state-ofthe-art approaches to relevance analysis in legal documents, such as TPP [3].", "startOffset": 147, "endOffset": 150}, {"referenceID": 5, "context": ", news articles [6,7] due to the characteristics of law dataset, as shown in Section 3.", "startOffset": 16, "endOffset": 21}, {"referenceID": 6, "context": ", news articles [6,7] due to the characteristics of law dataset, as shown in Section 3.", "startOffset": 16, "endOffset": 21}], "year": 2016, "abstractText": "In the context of the Competition on Legal Information Extraction/Entailment (COLIEE), we propose a method comprising the necessary steps for finding relevant documents to a legal question and deciding on textual entailment evidence to provide a correct answer. The proposed method is based on the combination of several lexical and morphological characteristics, to build a language model and a set of features for Machine Learning algorithms. We provide a detailed study on the proposed method performance and failure cases, indicating that it is competitive with state-of-the-art approaches on Legal Information Retrieval and Question Answering, while not needing extensive training data nor depending on expert produced knowledge. The proposed method achieved significant results in the competition, indicating a substantial level of adequacy for the tasks addressed.", "creator": "LaTeX with hyperref package"}}}