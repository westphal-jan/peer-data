{"id": "1609.01188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2016", "title": "Bi-Text Alignment of Movie Subtitles for Spoken English-Arabic Statistical Machine Translation", "abstract": "We describe efforts towards getting better resources for English-Arabic machine translation of spoken text. In particular, we look at movie subtitles as a unique, rich resource, as subtitles in one language often get translated into other languages. Movie subtitles are not new as a resource and have been explored in previous research; however, here we create a much larger bi-text (the biggest to date), and we further generate better quality alignment for it. Given the subtitles for the same movie in different languages, a key problem is how to align them at the fragment level. Typically, this is done using length-based alignment, but for movie subtitles, there is also time information. Here we exploit this information to develop an original algorithm that outperforms the current best subtitle alignment tool, subalign. The evaluation results show that adding our bi-text to the IWSLT training bi-text yields an improvement of over two BLEU points absolute.", "histories": [["v1", "Mon, 5 Sep 2016 14:55:31 GMT  (20kb)", "http://arxiv.org/abs/1609.01188v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["fahad al-obaidli", "stephen cox", "preslav nakov"], "accepted": false, "id": "1609.01188"}, "pdf": {"name": "1609.01188.pdf", "metadata": {"source": "CRF", "title": "Bi-Text Alignment of Movie Subtitles for Spoken English-Arabic Statistical Machine Translation", "authors": ["Fahad Al-Obaidli", "Stephen Cox", "Preslav Nakov"], "emails": ["faalobaidli@qf.org.qa", "pnakov@qf.org.qa", "s.j.cox@uea.ac.uk"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.01 188v 1 [cs.C L] 5S ep2 01Keywords: machine translation, bi-text alignment, movie subtitles."}, {"heading": "1 Introduction", "text": "In fact, it is the case that most people who are in a position to move to another world, to move to another world in which they can no longer move, in which they can no longer move, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they can no longer live, in which they cannot live, in which they can no longer live, in which they can no longer live, in which they can not live, in which they do not want to want to live, in which they do not want to want to want to want to live, in which they do not want to want to want to want to want to want to live, in which they do not want to want to want to want to want to want to live, in which they do not want to want to want to want to want to want to live, in which they do not want to want to want to want to want to want to want to live, in which they do not want to want to want to want to want to want to want to live."}, {"heading": "2 Related Work", "text": "There is a body of literature on the construction of multilingual parallel corpora for reasons discussed earlier. The initial results were satisfactory. \"Tiedemann has made a lot of efforts in this direction and made significant contributions to the alignment of film subtitles [25,26,27]. Initially, he collected bi-texts for 59 languages in his OpenSubtitles2013 corpus, which were derived from 308,000 subtitles of approximately 18,900 films downloaded from OpenSubtitles.org, one of the free online databases of movie subtitles. However, initially, he also required the use of bi-text alignment from Movie Subtitles 3For Alignment, Tiedemann, started with the traditional approach of long sentence alignment [5] with sentence boundaries marked at an earlier stage. This is based on the idea that sentences are linguistically driven elements, and therefore it would be more appropriate to edit them with linguistic elements rather than just subtitles appearing on the screen."}, {"heading": "3 Method", "text": "We consider film subtitles to be a unique source of bio-texts in an attempt to align as many translations of films as possible to improve performance from English to Arabic SMT. Compared to other translations, English to Arabic translation is particularly a rare translation direction and often yields significantly lower results compared to the opposite direction (Arabic to English). First, we collected pairs of English to Arabic subtitles from more than 29,000 movies / TV shows, a collection that is larger than all existing bilingual subtitle records. We then designed a pipeline of heuristic processing to eliminate the inherent noise associated with the source of subtitles to achieve good quality alignment. We used the time information provided in the subtitle files to develop a \"time-overlap\" based alignment method. Note that temporal information overlaps with the source of subtitles, and it has been proven in previous work to apply other approaches such as overlapping traditional English."}, {"heading": "3.1 Data Gathering", "text": "We identified target subtitles from OpenSubtitles based on their daily generated exports, which include references to their entire database of about 3M subtitles (as of May 20, 2015), as well as some useful information. We looked for Arabic and English subtitle files that are provided in the most common subrip format for consistency. It is common for multiple subtitle versions of the same movie to exist in the same language. In this case, we look for movies with matching movie release names to form a subtitle pair for each movie. Otherwise, the versions are randomly selected. Later, pairs with matching movie release names yielded a significantly higher alignment ratio than randomly selected ones."}, {"heading": "3.2 Preprocessing", "text": "We then downloaded the subtitles courtesy of OpenSubtitles administrators, subjected the files to a series of quality checks such as language, format, encoding identifiers, and cleaned them further to eliminate unnecessary text such as HTML formatting labels, divided dialogue segments between multiple speakers (starting with leading hyphens) into multiple segments, and divided the time allocated among them according to their new character length. We excluded corrupt and misidentified subtitle files from our collection, and ended up with 29,000 full pairs of Arabic-English subtitle files of unique movies / TV series episodes."}, {"heading": "3.3 Bi-text Alignment", "text": "Each segment of one of the language pairs is compared with the segments of the other, and the segment time information is used to detect overlap. If an overlap is identified, a predefined overlap threshold is applied to decide whether the segment pair is counted as a valid translation or whether it is excluded from the final corpus.The ratio is a modified version of the Jaccard index and is defined as: ratio = intersection + 1union + 1, where intersection is the period in which both segments intersect over the runtime of the movie, while unification is the period of time the two segments cover together, i.e. the period from the beginning of the earliest segment to the end of the most recent one. [6] Fahad Al-Obaidli \u2020, Stephen Cox and Preslav Nakov \u2020 Here is a pseudocode of our approach:"}, {"heading": "1. Loop through documents A and B and look for a minimal overlap", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2. If no overlap is found proceed", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3. If overlap is found calculate the overlap ratio", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4. If the ratio is more than or equal to the threshold then", "text": "4.1 Matching Segment Pair4.2 Note the index of the matching segment in document B4.3 Break open the inner top to go to the next segment in document A and compare it with the next segment in document B from the last listed index."}, {"heading": "5. Else, go to 1.", "text": "We define and distinguish between the overlaps here as (i) partial overlaps in which the initial time value of a segment is within the other segment (from the other document) and its final value, which is beyond that interval, and the complete overlaps in which the initial time value of a segment is within the other segment and within which the overlap threshold is reached. This framework allows us to decide which side of the pair to iterate next. Furthermore, the overlap is only considered to be an overlap between two overlapping segments when a predefined minimum overlap tail tip is reached."}, {"heading": "4 Experiments and Evaluation", "text": "In the following, we first describe the basic system and the evaluation structure, and then present our subtitle alignment and machine translation experiments."}, {"heading": "4.1 Baseline System", "text": "We built a phrase-based SMT model [14] as implemented in the Moses toolkit [13] to train an SMT system that translates from English to Arabic. We trained all system components (translation, reorganization, and language models) on the IWSLT '13 data, 2 which included a training text of 150K sentences [3]. Following [10] we normalized the Arabic training, development, and test data using MADA [23], automatically specifying all false instances of alef, ta marbuta, and alef maqsura. We further segmented the Arabic words using the Stanford word segmentator [17]. For English, we converted all words into lowercase letters. We built our phrase tables using the standard Moses pipeline with a maximum phrase length of 7 and Kneser-Ney smoothing."}, {"heading": "4.2 Evaluation Setup", "text": "One of the problems when translating into Arabic is the frequent errors in the written Arabic text, such as incorrect characters, punctuation and other orthographic features of the language. In the case of 29K pairs of different films, it would be unrealistic to attempt to model them. Furthermore, the Arabic references provided by IWSLT in both the tuning and the test kits are no exception to these random errors, which poses a problem for evaluating the results of the systems when they are expected to produce proper Arabic. In fact, it is shown to affect the IWSLT baseline with the NIST v13 evaluation tool by more than two BLEU points [24]. Therefore, we normalize both the tuning and the test references with the QCRI Arabic Normalizer v3.0.4, which uses the morphological analysis of MADA to output an enriched form of Arabic; it also standardizes the digits to make them all appear in Arabic and converts them all into the English translation after the characters have been synchronized."}, {"heading": "4.3 Experiments", "text": "We produced two datasets with an overlap threshold of 0.65, one with 1-1 alignments only (i.e., we only allow the alignment of an English segment to an Arabic segment) and the other with 1-M alignments 5 (i.e., we allow the alignment of an English segment to a sequence of one or more consecutive Arabic segments).We choose the threshold of 0.65 approximately as the midpoint between the strictest conditions, 1.0, which is not promising for the open nature of the data, or the less strict condition, which creates the possibility of two adjacent segments suitable for alignment with the same segment on the other side of the pair, and hence generate duplications. Nevertheless, this is only a case of presenting noise on the data contaminated with partial alignments, i.e., pairs with translations of the other when the threshold is lowered."}, {"heading": "5 Conclusion and Future Work", "text": "We have presented our efforts to get better resources for machine translation of spoken text into English and Arabic. Evaluation results have shown that our data set, in combination with our alignment algorithm, has improved over two BLEU points absolutely over a strong spoken SMT system. In future work, we would like to include more sources of information in the alignment process, such as cognates, translation dictionaries, punctuation, numbers, data, etc. Adding some language-specific information might also be interesting, such as linguistic knowledge. We also plan to include our tool for recognizing ads that are typically placed at the beginning or end of the file. Another interesting research direction is selecting the best subtitle pair if multiple versions of subtitles for the same movie for the source or target text for the language version 11 subtitles."}], "references": [{"title": "The AMARA corpus: Building parallel language resources for the educational domain", "author": ["Ahmed Abdelali", "Francisco Guzm\u00e1n", "Hassan Sajjad", "Stephan Vogel"], "venue": "In Proceedings of the Ninth International Conference on Language Resources and Evaluation,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "The mathematics of statistical machine translation: parameter estimation", "author": ["Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer"], "venue": "Computational Linguistics,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1993}, {"title": "Report on the 10th IWSLT evaluation campaign", "author": ["Mauro Cettolo", "Jan Niehues", "Sebastian St\u00fcker", "Luisa Bentivogli", "Marcello Frederico"], "venue": "In Proceedings of the International Workshop on Spoken Language Translation, IWSLT", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Stabilizing minimum error rate training", "author": ["George Foster", "Roland Kuhn"], "venue": "In Proceedings of the Fourth Workshop on Statistical Machine Translation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "A program for aligning sentences in bilingual corpora", "author": ["William A Gale", "Kenneth W Church"], "venue": "Computational linguistics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Analyzing optimization for statistical machine translation: MERT learns verbosity, PRO learns length", "author": ["Francisco Guzm\u00e1n", "Preslav Nakov", "Stephan Vogel"], "venue": "In Proceedings of the Nineteenth Conference on Computational Natural Language Learning,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "KenLM: Faster and smaller language model queries", "author": ["Kenneth Heafield"], "venue": "In Proceedings of the 6th Workshop on Statistical Machine Translation,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Tuning as ranking", "author": ["Mark Hopkins", "Jonathan May"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "AMARA: A sustainable, global solution for accessibility, powered by communities of volunteers", "author": ["Dean Jansen", "Aleli Alcala", "Francisco Guzm\u00e1n"], "venue": "In Universal Access in Human-Computer Interaction. Design for All and Accessibility Practice,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Orthographic and morphological processing for English-Arabic statistical machine translation", "author": ["Ahmed El Kholy", "Nizar Habash"], "venue": "Machine Translation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In Proceedings of the Tenth Machine Translation Summit, MT Summit\u2019", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2005}, {"title": "Edinburgh system description for the 2005 IWSLT speech translation evaluation", "author": ["Philipp Koehn", "Amittai Axelrod", "Alexandra Birch Mayne", "Chris Callison-Burch", "Miles Osborne", "David Talbot"], "venue": "In Proceedings of the International Workshop on Spoken Language", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2005}, {"title": "Open source toolkit for statistical machine translation", "author": ["Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst. Moses"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL: Interactive Poster and Demonstration Sessions,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Building Parallel Corpora from Movies", "author": ["Caroline Lavecchia", "Kamel Sm\u00e4\u0131li", "David Langlois"], "venue": "In Proceedings of the 4th International Workshop on Natural Language Processing and Cognitive Science, NLPCS \u201907,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Multilingual aligned corpora from movie subtitles", "author": ["Mathieu Mangeot", "Emmanuel Giguet"], "venue": "Report in Laboratoire d\u2019Informatique, Syste\u0300mes, Traitement de l\u2019Information et de la Connaissance,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "Word segmentation of informal Arabic with domain adaptation", "author": ["Will Monroe", "Spence Green", "Christopher D. Manning"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Improving English-Spanish Statistical Machine Translation: Experiments in Domain Adaptation, Sentence Paraphrasing, Tokenization, and Recasing", "author": ["Preslav Nakov"], "venue": "In Proceedings of the Third Workshop on Statistical Machine Translation,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2008}, {"title": "Parameter optimization for statistical machine translation: It pays to learn from hard examples", "author": ["Preslav Nakov", "Fahad Al Obaidli", "Francisco Guzman", "Stephan Vogel"], "venue": "In Proceedings of the International Conference Recent Advances in Natural Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Optimizing for sentencelevel BLEU+1 yields short translations", "author": ["Preslav Nakov", "Francisco Guzm\u00e1n", "Stephan Vogel"], "venue": "In Proceedings of the 24th International Conference on Computational Linguistics,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "A tale about PRO and monsters", "author": ["Preslav Nakov", "Francisco Guzm\u00e1n", "Stephan Vogel"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Improved Statistical Machine Translation for Resource-poor Languages Using Related Resource-rich Languages", "author": ["Preslav Nakov", "Hwee Tou Ng"], "venue": "In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 3,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2009}, {"title": "Arabic morphological tagging, diacritization, and lemmatization using lexeme models and feature ranking", "author": ["Ryan Roth", "Owen Rambow", "Nizar Habash", "Mona Diab", "Cynthia Rudin"], "venue": "In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "QCRI at IWSLT 2013: Experiments in Arabic-English and English-Arabic spoken language translation", "author": ["Hassan Sajjad", "Francisco Guzm\u00e1n", "Preslav Nakov", "Ahmed Abdelali", "Kenton Murray", "Fahad Al Obaidli", "Stephan Vogel"], "venue": "In Proceedings of the 10th International Workshop on Spoken Language Translation,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Building a multilingual parallel subtitle corpus", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the Computational Linguistics in the Netherlands,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Improved sentence alignment for movie subtitles", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the Conference on Recent Advances in Natural Language Processing,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2007}, {"title": "Synchronizing translated movie subtitles", "author": ["J\u00f6rg Tiedemann"], "venue": "Proceedings of the Sixth International Conference on Language Resources and Evaluation,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "Parallel data, tools and interfaces in OPUS", "author": ["J\u00f6rg Tiedemann"], "venue": "In Proceedings of the Eighth International Conference on Language Resources and Evaluation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "The automatic translation of film subtitles. A machine translation success story", "author": ["Martin Volk"], "venue": "Journal for Language Technology and Computational Linguistics (JLCL),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Evaluating MT with translations or translators: what is the difference", "author": ["Martin Volk", "Soren Harder"], "venue": "In Proceedings of the Machine Translation Summit XI,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "Source Language Adaptation for Resource-Poor Machine Translation", "author": ["Pidong Wang", "Preslav Nakov", "Hwee Tou Ng"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2012}, {"title": "Source Language Adaptation Approaches for Resource-Poor Machine Translation", "author": ["Pidong Wang", "Preslav Nakov", "Hwee Tou Ng"], "venue": "Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Constructing parallel corpus from movie subtitles", "author": ["Han Xiao", "Xiaojie Wang"], "venue": "In Computer Processing of Oriental Languages. Language Technology for the Knowledge-based Economy,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}], "referenceMentions": [{"referenceID": 10, "context": "Existing parallel corpora are mostly derived from specialized domains such as administrative, technical and legislation documents [11].", "startOffset": 130, "endOffset": 134}, {"referenceID": 14, "context": "They are usually created to fit pirated copies of copyright-protected movies shared by organized groups, so their use in research could be deemed to be a positive side effect of the Internet movie-piracy scene [16].", "startOffset": 210, "endOffset": 214}, {"referenceID": 31, "context": "There is a body of literature about building multilingual parallel corpora from movie subtitles [33,15].", "startOffset": 96, "endOffset": 103}, {"referenceID": 13, "context": "There is a body of literature about building multilingual parallel corpora from movie subtitles [33,15].", "startOffset": 96, "endOffset": 103}, {"referenceID": 23, "context": "Tiedemann has put a lot of efforts in this direction and has made substantial contributions for aligning movie subtitles [25,26,27].", "startOffset": 121, "endOffset": 131}, {"referenceID": 24, "context": "Tiedemann has put a lot of efforts in this direction and has made substantial contributions for aligning movie subtitles [25,26,27].", "startOffset": 121, "endOffset": 131}, {"referenceID": 25, "context": "Tiedemann has put a lot of efforts in this direction and has made substantial contributions for aligning movie subtitles [25,26,27].", "startOffset": 121, "endOffset": 131}, {"referenceID": 4, "context": "For alignment, Tiedemann, started with the traditional approach of lengthbased sentence alignment [5] using sentence boundaries tagged in an earlier stage.", "startOffset": 98, "endOffset": 101}, {"referenceID": 28, "context": "Volk and Harder [30] built their Swedish-Danish corpus by first manually translating Swedish subtitles of English TV programmes with the help of trained translators using specified guidelines and, in the process, adding appropriate time information for the translations.", "startOffset": 16, "endOffset": 20}, {"referenceID": 24, "context": "This \u201ccommercial\u201d setup allows them to avoid the complex alignment approaches found in other studies [26,27].", "startOffset": 101, "endOffset": 108}, {"referenceID": 25, "context": "This \u201ccommercial\u201d setup allows them to avoid the complex alignment approaches found in other studies [26,27].", "startOffset": 101, "endOffset": 108}, {"referenceID": 27, "context": "Volk and Harder assumed that they could match most of the subtitles except where the Danish translator has changed the time codes sufficiently to fail the strict overlap condition [29].", "startOffset": 180, "endOffset": 184}, {"referenceID": 23, "context": "They state that their alignment approach is pragmatic and requires minimal human inspection, even though it is not sensible beyond this setup, using \u201cgenesis\u201d files (Swedish subtitles) as a reference, as demonstrated earlier by Tiedemann [25], where, in reality, many translations require accurate handling.", "startOffset": 238, "endOffset": 242}, {"referenceID": 8, "context": "It is also worth mentioning the AMARA project [9], a unique, open, scalable and flexible online collaborative platforms which takes advantage of the power of crowdsourcing and encourages volunteer translation and editing of subtitles of educational videos.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "[1] used the AMARA content to generate a parallel corpus out of the translations in order to improve an SMT system for the educational domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "Note that time overlap information is language-independent and it has been proven in previous work to outperform other traditional approaches such as length-based approaches, which rely on sentence boundaries to match translation segments [25].", "startOffset": 239, "endOffset": 243}, {"referenceID": 26, "context": "Moreover, our method outperformed the results yielded by the best previously available subtitle alignment tool [28] by 0.", "startOffset": 111, "endOffset": 115}, {"referenceID": 23, "context": "We decided to base our bi-text alignment approach on time information given its efficiency with subtitle documents, as found in previous work [25].", "startOffset": 142, "endOffset": 146}, {"referenceID": 12, "context": "We built a phrase-based SMT model [14], as implemented in the Moses toolkit [13], to train an SMT system translating from English to Arabic.", "startOffset": 76, "endOffset": 80}, {"referenceID": 2, "context": "We trained all components of the system (translation, reordering, and language models) on the IWSLT\u201913 data, which includes a training bi-text of 150K sentences [3].", "startOffset": 161, "endOffset": 164}, {"referenceID": 9, "context": "Following [10], we normalized the Arabic training, development and test data using MADA [23], fixing automatically all wrong instances of alef, ta marbuta and alef maqsura.", "startOffset": 10, "endOffset": 14}, {"referenceID": 21, "context": "Following [10], we normalized the Arabic training, development and test data using MADA [23], fixing automatically all wrong instances of alef, ta marbuta and alef maqsura.", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "We further segmented the Arabic words using the Stanford word segmenter [17].", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": "We first word-aligned the training bi-text using IBM model 4 [2] on the English-Arabic and on the ArabicEnglish directions; then, we consolidated the two alignments using the grow-diagfinal-and symmetrization heuristics.", "startOffset": 61, "endOffset": 64}, {"referenceID": 11, "context": "We also built a lexicalized reordering model [12]: msd-bidirectional-fe.", "startOffset": 45, "endOffset": 49}, {"referenceID": 6, "context": "We further built a 5-gram language model (LM) on the Arabic text with KneserNey smoothing using KenLM [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 7, "context": "We tuned the models using pairwise ranking optimization (PRO) [8] with the length correction of [20] with 1000-best lists, using the IWSLT\u201913 tuning dataset.", "startOffset": 62, "endOffset": 65}, {"referenceID": 18, "context": "We tuned the models using pairwise ranking optimization (PRO) [8] with the length correction of [20] with 1000-best lists, using the IWSLT\u201913 tuning dataset.", "startOffset": 96, "endOffset": 100}, {"referenceID": 12, "context": "We tested on the IWSLT\u201913 test dataset, which we decoded using the Moses [13] decoder to produce Arabic translations, which we then desegmented.", "startOffset": 73, "endOffset": 77}, {"referenceID": 3, "context": "In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested in [4].", "startOffset": 166, "endOffset": 169}, {"referenceID": 5, "context": "org 3 For a broader discussion see also [6,21].", "startOffset": 40, "endOffset": 46}, {"referenceID": 19, "context": "org 3 For a broader discussion see also [6,21].", "startOffset": 40, "endOffset": 46}, {"referenceID": 22, "context": "In fact, it is shown to affect the IWSLT baseline system score by more than two BLEU points with the NIST v13 scoring tool [24].", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "We apply the same normalization to each system\u2019s output after detokenization as a final post-processing step [19].", "startOffset": 109, "endOffset": 113}, {"referenceID": 26, "context": "This tool includes more complex features such as auto-synchronization and matching segments at the sentence level [28].", "startOffset": 114, "endOffset": 118}, {"referenceID": 16, "context": "We used the movies bi-text in two different ways: (a) we built a separate phrase table, which then we interpolated with the phrase table of the baseline system [18,22,31,32], and (b) we just concatenated our bi-text with the IWSLT\u201913 training bi-text in the baseline and trained on the concatenation.", "startOffset": 160, "endOffset": 173}, {"referenceID": 20, "context": "We used the movies bi-text in two different ways: (a) we built a separate phrase table, which then we interpolated with the phrase table of the baseline system [18,22,31,32], and (b) we just concatenated our bi-text with the IWSLT\u201913 training bi-text in the baseline and trained on the concatenation.", "startOffset": 160, "endOffset": 173}, {"referenceID": 29, "context": "We used the movies bi-text in two different ways: (a) we built a separate phrase table, which then we interpolated with the phrase table of the baseline system [18,22,31,32], and (b) we just concatenated our bi-text with the IWSLT\u201913 training bi-text in the baseline and trained on the concatenation.", "startOffset": 160, "endOffset": 173}, {"referenceID": 30, "context": "We used the movies bi-text in two different ways: (a) we built a separate phrase table, which then we interpolated with the phrase table of the baseline system [18,22,31,32], and (b) we just concatenated our bi-text with the IWSLT\u201913 training bi-text in the baseline and trained on the concatenation.", "startOffset": 160, "endOffset": 173}], "year": 2016, "abstractText": "We describe efforts towards getting better resources for EnglishArabic machine translation of spoken text. In particular, we look at movie subtitles as a unique, rich resource, as subtitles in one language often get translated into other languages. Movie subtitles are not new as a resource and have been explored in previous research; however, here we create a much larger bi-text (the biggest to date), and we further generate better quality alignment for it. Given the subtitles for the same movie in different languages, a key problem is how to align them at the fragment level. Typically, this is done using length-based alignment, but for movie subtitles, there is also time information. Here we exploit this information to develop an original algorithm that outperforms the current best subtitle alignment tool, subalign. The evaluation results show that adding our bi-text to the IWSLT training bi-text yields an improvement of over two BLEU points absolute.", "creator": "LaTeX with hyperref package"}}}