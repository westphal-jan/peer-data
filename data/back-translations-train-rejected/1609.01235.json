{"id": "1609.01235", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Sep-2016", "title": "PMI Matrix Approximations with Applications to Neural Language Modeling", "abstract": "The negative sampling (NEG) objective function, used in word2vec, is a simplification of the Noise Contrastive Estimation (NCE) method. NEG was found to be highly effective in learning continuous word representations. However, unlike NCE, it was considered inapplicable for the purpose of learning the parameters of a language model. In this study, we refute this assertion by providing a principled derivation for NEG-based language modeling, founded on a novel analysis of a low-dimensional approximation of the matrix of pointwise mutual information between the contexts and the predicted words. The obtained language modeling is closely related to NCE language models but is based on a simplified objective function. We thus provide a unified formulation for two main language processing tasks, namely word embedding and language modeling, based on the NEG objective function. Experimental results on two popular language modeling benchmarks show comparable perplexity results, with a small advantage to NEG over NCE.", "histories": [["v1", "Mon, 5 Sep 2016 17:47:49 GMT  (16kb)", "http://arxiv.org/abs/1609.01235v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["oren melamud", "ido dagan", "jacob goldberger"], "accepted": false, "id": "1609.01235"}, "pdf": {"name": "1609.01235.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["melamuo@cs.biu.ac.il", "dagan@cs.biu.ac.il", "goldbej@eng.biu.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.01 235v 1 [cs.C L] 5S ep"}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Euclidean Embedding of a Joint Distribution", "text": "In this section we extend the d-dimensional one-bed method with negative Sampling (SGNS) word-embedding algorithm (mi mi) to a general constellation of discrete common distribution embedding algorithms. We also extend the algorithm analysis of Levy and Goldberg [15] and supply an explicit print-out for the quality of the PMI matrix x x-x x x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-x-"}, {"heading": "3 Language modeling based on PMI Approximation", "text": "In this section, we apply the above-described embedding algorithm to the common distribution of a word w and its left-sided context c. We use the link between optimal embedding and PMImatrix factorization to construct an approximation of the conditional distribution p (w | c). As a result, we obtain an efficient algorithm for learning a language model. The common distribution of a word string fulfills the chain rule: log p (w1,..., wn) = 1log p (wi | ci) (15), so ci = (w1, wi \u2212 1) is the left-sided context ofwi. We use a simple search table for word representation and an LSTM recurrent neural network to obtain a left-sided context representation. We train the word and the left-sided context embedding to maximize the goal (13): S = [w, c + k, c)."}, {"heading": "4 Connection to Noise Contrastive Estimation", "text": "The linguistic algorithms that have most to do with our approach are those based on the Noise Contrastive Estimation (NCE) principle (NCE) =. There are several applications of NCE for speech modelling that are mainly different from each other and then explain the difference between the standard implementation of NCE and our approach, which can be seen as a simplified variant of the NCE. NCE transforms the parameter learning problem into a binary classification problem. Let's calculate the probability of a word w in the face of a c context and let pn (w) a \"noise\" word distribution (e.g. an unigrammed distribution). The NCE approach assumes that the word w is used explicitly from a mixed distribution 1k (w)."}, {"heading": "5 Experiments", "text": "In fact, most people are able to decide for themselves what they want and what they don't want."}, {"heading": "6 Conclusions", "text": "In this paper, we first derived an information-theoretical basis for the relationship between negative sample and PMI approximation. This analysis extends previous analyses by Levy and Goldberg [15] to the case of low-dimensional PMI matrix approximation. We have shown that the simplified negative sampling function (NEG), popular in the context of learning word representations, can also be used for learning parametric language models (NEGLMs) as long as the correct procedure is followed during the test period. Thus, we provided a unified approach based on PMI approximation for both word embedding and speech modeling. In analyzing the relationship between our proposed NEGLMs and NCE language models, we have shown that NEGLMs, although very closely related, have the advantage of a simpler objective function, while NCE LMs require some heuristic measures to achieve such stability, can provide an additional NEGLM embodiment based on two objective evaluations."}], "references": [{"title": "When and why are log-linear models selfnormalizing", "author": ["J. Andreas", "D. Klein"], "venue": "In NAACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Tailoring continuous word representations for dependency parsing", "author": ["M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "Association for Computational Linguistics (ACL)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Quick training of probabilistic neural nets by importance sampling", "author": ["Y. Bengio", "J. Senecal"], "venue": "In AISTATS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2003}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["C. Chelba", "T. Mikolov", "M. Schuster", "Q. Ge", "T. Brants", "P. Koehn", "T. Robinson"], "venue": "arXiv preprint arXiv:1312.3005", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Recurrent neural network language model training with noise contrastive estimation for speech recognition", "author": ["X. Chen", "X. Liu", "M. Gales", "P.C. Woodland"], "venue": "ICASSP", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Elements of information theory", "author": ["Thomas M Cover", "Joy A Thomas"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Notes on noise contrastive estimation and negative sampling", "author": ["C. Dyer"], "venue": "arXiv preprint arXiv:1410.8251", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckart", "G. Young"], "venue": "Psychometrika, 1:211\u2013218", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1936}, {"title": "Noise-contrastive estimation of unnormalized statistical models", "author": ["M.U. Gutmann", "A. Hyvarinen"], "venue": "with applications to natural image statistics. Journal of Machine Learning Research, 13:307\u2013361", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Blackout: Speeding up recurrent neural network language models with very large vocabularies", "author": ["S. Ji", "S. Vishwanathan", "N. Satish", "A. Nadathur", "J. Michael", "P. Dubey"], "venue": "ICLR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring the limits of language modeling", "author": ["R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu"], "venue": "arXiv preprint arXiv:1602.02410", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "arXiv preprint arXiv:1412.6980", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Skipthought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler"], "venue": "Proceedings of NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2000}, {"title": "Neural word embedding as implicit matrix factorization", "author": ["O. Levy", "Y. Goldberg"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "context2vec: Learning generic context embedding with bidirectional LSTM", "author": ["O. Melamud", "J. Goldberger", "I. Dagan"], "venue": "Proceedings of CONLL", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of context types and dimensionality in learning word embeddings", "author": ["O. Melamud", "D. McClosky", "S. Patwardhan", "M. Bansal"], "venue": "Proceedings of NAACL", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "A scalable hierarchical distributed language model", "author": ["A. Minh", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["A. Mnih", "Y.W. Teh"], "venue": "ICML", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Lexicon infused phrase embeddings for named entity resolution", "author": ["A. Passos", "V. Kumar", "A. McCallum"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["A. Vaswani", "Y. Zhao", "V. Fossum", "D. Chiang"], "venue": "EMNLP", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "Scaling recurrent neural network language models", "author": ["W. Williams", "N. Prasad", "D. Mrva", "T. Ash", "T. Robinson"], "venue": "ICASSP", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "arXiv preprint arXiv:1409.2329", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple", "author": ["B. Zoph", "A. Vaswani", "J. May", "K. Knight"], "venue": "fast noise-contrastive estimation for large RNN vocabularies. In NAACL", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 2, "context": "Several approaches have been proposed to cope with this scaling issue, including importance sampling [3], hierarchical softmax [19] and Noise Contrastive Estimation (NCE) [9].", "startOffset": 101, "endOffset": 104}, {"referenceID": 18, "context": "Several approaches have been proposed to cope with this scaling issue, including importance sampling [3], hierarchical softmax [19] and Noise Contrastive Estimation (NCE) [9].", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "Several approaches have been proposed to cope with this scaling issue, including importance sampling [3], hierarchical softmax [19] and Noise Contrastive Estimation (NCE) [9].", "startOffset": 171, "endOffset": 174}, {"referenceID": 19, "context": "NCE has been applied to train neural LMs with large vocabularies [20] and was also recently successfully used to train LSTM-RNN LMs (see e.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "[23] [5] [26]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "[23] [5] [26]).", "startOffset": 5, "endOffset": 8}, {"referenceID": 24, "context": "[23] [5] [26]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 17, "context": "In particular, the skip-gram embedding model with the negative sampling (NEG) objective function [18] as implemented in the word2vec toolkit, has become one of the most popular models today.", "startOffset": 97, "endOffset": 101}, {"referenceID": 20, "context": "Indeed, recent studies have obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity resolution and dependency parsing [21, 2, 17].", "startOffset": 198, "endOffset": 209}, {"referenceID": 1, "context": "Indeed, recent studies have obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity resolution and dependency parsing [21, 2, 17].", "startOffset": 198, "endOffset": 209}, {"referenceID": 16, "context": "Indeed, recent studies have obtained state-of-the-art results by using skip-gram embeddings on a variety of natural language processing tasks, such as named entity resolution and dependency parsing [21, 2, 17].", "startOffset": 198, "endOffset": 209}, {"referenceID": 12, "context": "The same embedding approach can be used for sentence representation [13] and context representation [16].", "startOffset": 68, "endOffset": 72}, {"referenceID": 15, "context": "The same embedding approach can be used for sentence representation [13] and context representation [16].", "startOffset": 100, "endOffset": 104}, {"referenceID": 14, "context": "Recently, Levy and Goldberg [15] offered some motivation for skip-gram\u2019s NEG objective function, showing that by maximizing this function the skip-gram algorithm implicitly factorizes a word-context pointwise mutual information (PMI) matrix.", "startOffset": 28, "endOffset": 32}, {"referenceID": 6, "context": "Dyer [7] argued that although NEG and NCE are superficially similar, NCE is a general parameter estimation technique that is asymptotically unbiased, while NEG is most useful for learning word representations, but not as a general-purpose estimator.", "startOffset": 5, "endOffset": 8}, {"referenceID": 14, "context": "We present a derivation of a NEG-based language modeling algorithm that is founded on an extension of the observation of Levy and Goldberg [15], which showed the relation between the skip-gram algorithm and PMI matrix factorization.", "startOffset": 139, "endOffset": 143}, {"referenceID": 21, "context": "It has a simplified objective function formulation which allows it to avoid heuristic components and initialization procedures that are used in various implementation of NCE language models [23] [5] [26].", "startOffset": 190, "endOffset": 194}, {"referenceID": 4, "context": "It has a simplified objective function formulation which allows it to avoid heuristic components and initialization procedures that are used in various implementation of NCE language models [23] [5] [26].", "startOffset": 195, "endOffset": 198}, {"referenceID": 24, "context": "It has a simplified objective function formulation which allows it to avoid heuristic components and initialization procedures that are used in various implementation of NCE language models [23] [5] [26].", "startOffset": 199, "endOffset": 203}, {"referenceID": 17, "context": "In this section we extend the skip-gram with negative sampling (SGNS) word embedding algorithm [18] to a general setup of embedding a discrete joint distribution.", "startOffset": 95, "endOffset": 99}, {"referenceID": 14, "context": "We also extend the algorithm analysis of Levy and Goldberg [15] and provide an explicit expression for the quality of the PMI matrix approximation obtained by the embedding algorithm.", "startOffset": 59, "endOffset": 63}, {"referenceID": 5, "context": "and finally using the definition of conditional KL divergence [6] we obtain:", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "Previous works have suggested other criteria for matrix factorization such as least-squares [8] and KL-divergence between the original matrix and the low-rank matrix approximation [14].", "startOffset": 92, "endOffset": 95}, {"referenceID": 13, "context": "Previous works have suggested other criteria for matrix factorization such as least-squares [8] and KL-divergence between the original matrix and the low-rank matrix approximation [14].", "startOffset": 180, "endOffset": 184}, {"referenceID": 14, "context": "Levy and Goldberg [15] showed that SGNS\u2019s objective achieves its maximal value when for each word-pair x, y the inner product of the embedding vectors ~x \u00b7 ~y = pmi(x, y).", "startOffset": 18, "endOffset": 22}, {"referenceID": 14, "context": "The result in [15], However, tells us nothing about the lower dimensional case where the embedding algorithm is actually interesting since at that case the PMI matrix factorisation is forced to compress the joint distribution and thereby learn a meaningful embedding.", "startOffset": 14, "endOffset": 18}, {"referenceID": 17, "context": "The SGNS word embedding algorithm [18] aims to represent each word x and each context word y as d-dimensional vectors ~x and ~y such that words that are \u201csimilar\" to each other will have similar vector representations.", "startOffset": 34, "endOffset": 38}, {"referenceID": 17, "context": "Following [18] we can sample negative instances from a smooth unigram distribution p(w) such that 0 \u2264 \u03b1 \u2264 1.", "startOffset": 10, "endOffset": 14}, {"referenceID": 8, "context": "The language modeling algorithms that are most related to our approach are those that are based on the Noise Contrastive Estimation (NCE) principle [9].", "startOffset": 148, "endOffset": 151}, {"referenceID": 24, "context": "There are several applications of NCE to language modeling that mainly differ from each other by the neural network architecture used to produce a parametric representation for the left-side context of the predicted word [26] [23] [5].", "startOffset": 221, "endOffset": 225}, {"referenceID": 21, "context": "There are several applications of NCE to language modeling that mainly differ from each other by the neural network architecture used to produce a parametric representation for the left-side context of the predicted word [26] [23] [5].", "startOffset": 226, "endOffset": 230}, {"referenceID": 4, "context": "There are several applications of NCE to language modeling that mainly differ from each other by the neural network architecture used to produce a parametric representation for the left-side context of the predicted word [26] [23] [5].", "startOffset": 231, "endOffset": 234}, {"referenceID": 19, "context": "However, it was found empirically [20] that setting Zc = 1 didn\u2019t hurt the performance (see also theoretical analysis in [1]).", "startOffset": 34, "endOffset": 38}, {"referenceID": 0, "context": "However, it was found empirically [20] that setting Zc = 1 didn\u2019t hurt the performance (see also theoretical analysis in [1]).", "startOffset": 121, "endOffset": 124}, {"referenceID": 4, "context": "[5] reported that setting log(Zc) = 9 gave them the best results.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18] suggested the negative sampling (NEG) training procedure, which is a simplified version of the objective function optimized by NCE.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "As mentioned above, it was found empirically [20] that setting Zc = 1 in the NCE objective function didn\u2019t hurt the performance.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "We use the heuristics that worked well in [23][26], initializing NCE\u2019s bias term from Equation 21 to bw = \u2212 log |V |, where V is the word vocabulary, and using Zc = 1.", "startOffset": 42, "endOffset": 46}, {"referenceID": 24, "context": "We use the heuristics that worked well in [23][26], initializing NCE\u2019s bias term from Equation 21 to bw = \u2212 log |V |, where V is the word vocabulary, and using Zc = 1.", "startOffset": 46, "endOffset": 50}, {"referenceID": 23, "context": "To build and train all compared models in this setting, we followed [25], which achieved excellent results on this dataset.", "startOffset": 68, "endOffset": 72}, {"referenceID": 24, "context": "We set the negative sampling parameter to k = 100 following [26], which showed highly competitive performance with NCE language models trained with this number of samples.", "startOffset": 60, "endOffset": 64}, {"referenceID": 3, "context": "As the second dataset, we used the much larger WMT 1B-word benchmark, 4 introduced by [4].", "startOffset": 86, "endOffset": 89}, {"referenceID": 22, "context": "However, we follow [24][10] and trim the vocabulary further to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "startOffset": 19, "endOffset": 23}, {"referenceID": 9, "context": "However, we follow [24][10] and trim the vocabulary further to the top 64K most frequent words in order to successfully fit a neural model to this data using reasonably modest compute resources.", "startOffset": 23, "endOffset": 27}, {"referenceID": 23, "context": "tgz [25] use larger models with more units.", "startOffset": 4, "endOffset": 8}, {"referenceID": 10, "context": "We follow [11], which found a 10% dropout rate to be sufficient for relatively small models fitted to this large training corpus.", "startOffset": 10, "endOffset": 14}, {"referenceID": 11, "context": "We train our model for just one epoch using the Adam optimizer [12] with default parameters, which we found to converge more quickly and effectively than SGD.", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "This analysis extends previous analysis of Levy and Goldberg [15] to the case of lower dimensional PMI matrix approximation.", "startOffset": 61, "endOffset": 65}], "year": 2016, "abstractText": "The negative sampling (NEG) objective function, used in word2vec, is a simplification of the Noise Contrastive Estimation (NCE) method. NEG was found to be highly effective in learning continuous word representations. However, unlike NCE, it was considered inapplicable for the purpose of learning the parameters of a language model. In this study, we refute this assertion by providing a principled derivation for NEG-based language modeling, founded on a novel analysis of a low-dimensional approximation of the matrix of pointwise mutual information between the contexts and the predicted words. The obtained language modeling is closely related to NCE language models but is based on a simplified objective function. We thus provide a unified formulation for two main language processing tasks, namely word embedding and language modeling, based on the NEG objective function. Experimental results on two popular language modeling benchmarks show comparable perplexity results, with a small advantage to NEG over NCE.", "creator": "LaTeX with hyperref package"}}}