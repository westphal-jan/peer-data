{"id": "1609.01459", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2016", "title": "Deviant Learning Algorithm: Learning Sparse Mismatch Representations through Time and Space", "abstract": "Predictive coding (PDC) has recently attracted attention in the neuroscience and computing community as a candidate unifying paradigm for neuronal studies and artificial neural network implementations particularly targeted at unsupervised learning systems. The Mismatch Negativity (MMN) has also recently been studied in relation to PC and found to be a useful ingredient in neural predictive coding systems. Backed by the behavior of living organisms, such networks are particularly useful in forming spatio-temporal transitions and invariant representations of the input world. However, most neural systems still do not account for large number of synapses even though this has been shown by a few machine learning researchers as an effective and very important component of any neural system if such a system is to behave properly. Our major point here is that PDC systems with the MMN effect in addition to a large number of synapses can greatly improve any neural learning system's performance and ability to make decisions in the machine world. In this paper, we propose a novel bio-mimetic computational intelligence algorithm -- the Deviant Learning Algorithm, inspired by these key ideas and functional properties of recent brain-cognitive discoveries and theories. We also show by numerical experiments guided by theoretical insights, how our invented bio-mimetic algorithm can achieve competitive predictions with even with very small problem specific data.", "histories": [["v1", "Tue, 6 Sep 2016 09:35:14 GMT  (270kb)", "http://arxiv.org/abs/1609.01459v1", "Working Paper"], ["v2", "Wed, 7 Sep 2016 09:20:09 GMT  (269kb)", "http://arxiv.org/abs/1609.01459v2", "Working Paper"], ["v3", "Tue, 6 Dec 2016 18:53:24 GMT  (207kb)", "http://arxiv.org/abs/1609.01459v3", "Working Paper"], ["v4", "Wed, 7 Dec 2016 20:55:33 GMT  (207kb)", "http://arxiv.org/abs/1609.01459v4", "Working Paper"], ["v5", "Sun, 1 Jan 2017 11:26:06 GMT  (207kb)", "http://arxiv.org/abs/1609.01459v5", "Working Paper"], ["v6", "Tue, 3 Jan 2017 02:49:15 GMT  (207kb)", "http://arxiv.org/abs/1609.01459v6", "Working Paper"]], "COMMENTS": "Working Paper", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["emmanuel ndidi osegi", "vincent ike anireh"], "accepted": false, "id": "1609.01459"}, "pdf": {"name": "1609.01459.pdf", "metadata": {"source": "CRF", "title": "Deviant Learning Algorithm: Learning Sparse Mismatch Representations through Time and Space", "authors": [], "emails": ["viaemeka@ust.edu.ng", "nd.osegi@sure-gp.com"], "sections": [{"heading": null, "text": "Mismatch Negativity (MMN) has also recently been studied in relation to PCs and is considered a useful component of neural predictive coding systems. Supported by the behaviour of living organisms, such networks are particularly useful in the formation of spatio-temporal transitions and invariant representations of the input world. However, most neural systems still do not consider a large number of synapses, although this has been demonstrated by a few machine learning scientists as an effective and very important component of any neural system if such a system is to behave properly. Our main point here is that PDC systems with the MMN effect can have a large number of synapses in addition to a large number of synapses, which greatly improve the prediction of neural learning systems and the ability to make decisions in the machine world."}, {"heading": "1. Introduction", "text": "Brain-like learning in machine-oriented systems is known to have significant benefits in the real world and in synthetic data problems. However, the most appropriate brain-cognitive approach to the problem of data learning is still a primary problem that concerns researchers worldwide. While some algorithms offer a promising alternative to a wide range of machine learning algorithms, two algorithms stand out as important candidates for future ML systems."}, {"heading": "2. Related Works", "text": "Over the years, brain-like learning in artificial neural networks (ANNs) has been described by many artificial intelligence (AI) researchers, and various models proposed as the search for a better machine intelligence that comes close to human thinking are continuing worldwide. More importantly, several AI approaches have recently aroused the interest of the artificial intelligence community, some of which include the slow feature learning in (Wiskott and Sejnowski, 2002), the deep generative models in (Hinton, 2007) and the recurring models in (Schmidhuber et al, 1992; Schmidhuber et al 2012 and Graves et al, 2013), historical compression models with an emphasis on unexpected inputs during learning (Schmidhuber, 1992), the spatio-temporal hierarchical representations in (Hawkins et al, 2010), including the concept of sparsely distributed representations as in (Ahmad et al, Neal and Hawkins models in 2013), the neurological models in general modules are all similar to the ones in terms of Ahmad et al, Neuro-Hawkins models in 2013, and Hawkins frequent models in 2015."}, {"heading": "3. Machine Brain-like Effects and Novel Theories", "text": "Brain-like theories, as discussed extensively in (Hawkins and Blakeslee, 2007) as a form of memory prediction framework, in (Hinton et al, 2006) as deep generative connectionist learning, and as recurring learning networks with long short-term memory in (Hochreiter and Schmidhuber, 1997; Gers et al, 1999), have profound implications for the solution approaches used in today's AI software systems. An interesting feature of some of these theories is the ability of machine life algorithms to remember and forget - an analogy to birth-death theory. We firmly believe in this behavior, as artificial life machines attempt to respond to its world by forming new memories and erasing past memories. This recreational process has strong genetic roots and lies outside the framework of this paper to describe every detail. In this section, we will discuss some very important theoretical principles behind our deviating learning algorithms, inducing some new ideologies."}, {"heading": "3.1 MisMatch Negataivity (MMN) Effect", "text": "The mismatch negativity effect (MMN), first discovered in the context of auditory stimulation (Naatanen et al, 1978), is a possible neurobiological plausible attempt to statistically model the behavior of neuronal tissue in the real world. It has also been studied in the context of visual modalities (see Pazo-Alvarez et al, 2003). In particular, the MMN effect attempts to validate the differential response in the brain. Some important MMN theories can be found in (Liedler et al, 2013b). However, as discussed in (Liedler et al, 2013a), MMN theories are still controversial and not completely conclusive to this day. Some of the theories identified include (Liedler et al, 2013a): the Change Detection Hypothesis (CDH), Adaptation Hypothesis (AH), Model Adjustment Hypothesis (MAH), Novelty Detection Hypothesis (NH) and the Prediction Hypothesis (NH)."}, {"heading": "3.2 Reticular Formation and Synaptic Response", "text": "Reactions to sensory impulses have been defined in the past by the reticular units that serve as a kind of detector for the threshold of familiarity with possible hypothalamic gating capabilities (Kimball, 1964), and this functionality is also somewhat similar to the notion of permanence introduced in (Hawkins et al, 2010). Permanence has a depolarizing effect on the synapses, leading to a state of \"potential synapse,\" i.e. synapses with a high probability of being connected. Thus, when the synapse connects, the neuron or cell ignites."}, {"heading": "3.3 Incremental Learning and the Growing of Synapses", "text": "In this situation, it becomes necessary to grow more synapses in order to improve the observations learned in the neural ecosystem. In the DLA, this is achieved with the help of the parameter called learning content. As an organism grows with age, the learning content should increase accordingly, leading to the complete destruction of earlier primitive states in order to learn more complex tasks."}, {"heading": "4. Deviant Learning Algorithm (DLA)", "text": "The Deviant Learning Algorithm (DLA) is approached by a systematic mathematical method. CPU-like DLA is divided into two core phases as shown in Figure 1 - the prediction phase and the follow-up phase. During the predictions, invariant representations are learned through a long generative list of well-defined standards. For the purposes of this study, the standards are assumed to be integral representations of the input chain. Appendix 1 contains the pseudo-code for the DLA."}, {"heading": "4.1 Pre-Prediction Phase", "text": "Here we perform first- and second-order mismatch operations, assuming that the inputs are one-dimensional (1-D) matrices or vectors.These operations are described in the following subparagraphs: 4.1.1 Mismatch: Suppose a permanent threshold range becomes 1: oo.Then, for each input deviation, we get a first-order inmatch (level-1) using a Real Absolute Deviation (RAD) as: otherwisenllnikIkextdevlgidev0,,...,,2,1,1 | | 1) 1 () 1 () ((1).where,) (lgI.is a long list of generic integers limited by an explicit threshold, extl and iI.is the source input for each copy at a given time, t"}, {"heading": "4.1.2 Overlap", "text": "The deviant overlap is summed up as) 1 (devk, as common patterns of ones (binary 1s) - overlaid into a virtual memory say oS, conditioned on) 1 (devk and is defined as follows: otherwisekSSdevoo jtjt0111) 1 (), (), (2) from which we get: 1) 1 () (devtkoo SS (3).and)) max (max () () (tt oo SS (4)"}, {"heading": "4.1.3 Winner Integer Selection:", "text": "To identify the integers (s) responsible for the observation (s),) (to S depends on a threshold, say, 1hT defined as a recurring operation in the positive sense, i.e. this time we only choose the integers for which) (to S is greater or equal to 1hT. Occasional observers are described by the process of forbidding and by the recurring operation: 01) (1hoaoaoTSzCC nn (5)"}, {"heading": "4.1.4 Learning:", "text": "We perform learning using a Hebbian Hopfield updating style based on the maximum value of) (to S and a time limit. We also promote diversity by adding some random noise to the permanent updating process, defined as: otherTSttS hoiomo0) (2lim, 111maxmax * (6).where limt is the specified time limit of 2hT, a threshold * m, an optional randomized hyperbolic tangent activation function adapted to (Anireh and Osegi, 2016).To ensure the updating process during learning, a Monte Carlo run on this routine is performed twice. We may choose to stop learning by setting 01, but we believe that learning is a continuous process and an end is not in sight unless the system is dead."}, {"heading": "4.1.5 Inference:", "text": "This process is described as follows: ndevcgsdevaviknIIk) 2 () () 2 (| (7).whereicg I) (= winning integers from (5) andsI = a new input sequence at the next time step Note that the average incongruence at level-2 is defined as: n devdev kk av) 2 () 2 (((8).from which we get: n devr kk) 2 ((9).where rk is a rating factor that can be used to evaluate the inference performance over time. Using (7) and (5), a predictive interpolation is performed as: 2) (.0,..., 2.1), min (), () 2 (sr rdevaoaorIP pandotherwisennikkkC iCP (10), where T is defined as an ideal position for the entire storage operation (1)."}, {"heading": "4.2 Post-Prediction Phase", "text": "In the phase following the prediction, we perform a third and final inmatch operation (level-3) based on a new permanent threshold, we say 2 such as: otherwisejjjSIk hcurrent tsdev010,,, 1 |), (| limlim 222) 3 (((12) We also calculate a line-by-line overlap such as:) (1) 3 (2) (devk kS t (13) Finally, we extract the possible match from memory by a conditional selection procedure using a prefix search (Graves et al, 2006): otherwiseTjiTSijSPhkpithpostr p3,3) () (, (, (14) 3hT, where lo is the length of novel deviations. In practice, this is typically a reverse for the copy in the previous time step."}, {"heading": "5. Experiments and Results", "text": "The first two datasets are specifically a classification problem; these are the IRIS dataset, which is a dataset for plant species (Bache and Lichman, 2013) to categorize the iris plant into three different classes, the HEART dataset (Blake et al, 1998) to categorize a heart disease as either the absence or presence of heart disease, and the Word Similarity dataset (Finkelstein, 2001) to estimate similarity values. We begin by defining some initial experimental parameters for the HTM-CLA and the DLA. For the HTMCLA, we build a hierarchy of cells using a Monte Carlo simulation limited by their corresponding durability values. This modification is less computationally complex than the DASS combinator icspropy technique (Ahmad and Hawkins, 2015)."}, {"heading": "6. Discussions", "text": "The results show a competitive performance for both the HTM-CLA and the DLA based on the given parameters - see Appendix 2. While the DLA outperformed the HTM-CLA in the IRIS dataset test, the HTM-CLA outperformed the DLA in terms of the HEART and Word-Similarity datasets. This can be attributed to variations in the parameters and the extremely sparse representation of the HTM-CLA. 7. Conclusion and future workA novel algorithm for cortical learning based on the MN effect has been developed. Our model can learn, classify and predict context in the long term. It is possible to implement forgetting and remembering by dynamically adapting the learning scope, which will be investigated in a future version of this paper."}, {"heading": "Acknowledgement", "text": "Source codes for the HTM-CLA and DLA are provided on the central Matlab website: www.matlabcentral.com Appendix 1: DLA PseudocodeInitialize Time Counter, Deadlines (nCounter), Memory and Durability Threshold old1CounterGenerate a long list of Integers (standards), nniI lg,... 1,) (FOR each exemplEvaluate 1 st order Mismatch:) 1 (devk / / Pre-prediction PhaseCompute Deviant-OverlapSelect Winner IntegersStore Winner IntegersUpdate permanence threshold / / LearningEvaluate 2 ndorder Mismatch:) 2 (devkPerform a predictive interpolation in time, tIf) 2 (devk is less than () 2 (devk) Extract the causal Integers / / Integers Maximally responsible Applization Mismatch:) 2 (IntegersStore Devigersthreshold Integersthreshold Integerner Integertime Integerform Integerform in time, tIf Winderform Winderform Winderform Winderform Winderform WinderStore / WinderStore WinderPhasa:) 2 (WinderStore Maximally responsible Applization Mismatch:)."}], "references": [{"title": "Properties of sparse distributed representations and their application to hierarchical temporal memory", "author": ["S. Ahmad", "J. Hawkins"], "venue": "arXiv preprint arXiv:1503.07469", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "A Modified Activation Function with Improved Run-Times For Neural Networks", "author": ["V.I. Anireh", "E.N. Osegi"], "venue": "arXiv preprint arXiv:1607.01691", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Canonical microcircuits for predictive", "author": ["A.M. Bastos", "W.M. Usrey", "R.A. Adams", "G.R. Mangun", "P. Fries", "K.J. Friston"], "venue": "coding. Neuron,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Frequency-based error backpropagation in a cortical network", "author": ["R. Bogacz", "M.W. Brown", "C. Giraud-Carrier"], "venue": "In Neural Networks,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2000}, {"title": "UCI} Repository of machine learning databases", "author": ["C. Blake", "E. Keogh", "C.J. Merz"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Continuous online sequence learning with an unsupervised neural network model", "author": ["Y. Cui", "C. Surpur", "S. Ahmad", "J. Hawkins"], "venue": "arXiv preprint arXiv:1512.05463", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["L. Finkelstein", "E. Gabrilovich", "Y. Matias", "E. Rivlin", "Z. Solan", "G. Wolfman", "Ruppin", "April"], "venue": "In Proceedings of the 10th international conference on World Wide Web (pp. 406-414)", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2001}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "Schmidhuber", "June"], "venue": "In Proceedings of the 23rd international conference on Machine learning (pp. 369-376)", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2006}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. Graves", "A.R. Mohamed", "Hinton", "May"], "venue": "IEEE international conference on acoustics, speech and signal processing (pp. 6645-6649)", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Why neurons have thousands of synapses, a theory of sequence memory in neocortex", "author": ["J. Hawkins", "S. Ahmad"], "venue": "Frontiers in neural circuits,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Learning multiple layers of representation", "author": ["G.E. Hinton"], "venue": "Trends in cognitive sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Hierarchical temporal memory including HTM cortical learning algorithms", "author": ["J. Hawkins", "S. Ahmad", "D. Dubinsky"], "venue": "Techical report,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Modelling trial-bytrial changes in the mismatch negativity", "author": ["F. Lieder", "J. Daunizeau", "M.I. Garrido", "K.J. Friston", "K.E. Stephan"], "venue": "PLoS Comput Biol,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "A neurocomputational model of the mismatch negativity", "author": ["F. Lieder", "K.E. Stephan", "J. Daunizeau", "M.I. Garrido", "K.J. Friston"], "venue": "PLoS Comput Biol,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Early selective-attention effect on evoked potential reinterpreted", "author": ["R. N\u00e4\u00e4t\u00e4nen", "A.W. Gaillard", "S. M\u00e4ntysalo"], "venue": "Acta psychologica,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1978}, {"title": "MMN in the visual modality: a review", "author": ["P. Pazo-Alvarez", "F. Cadaveira", "E. Amenedo"], "venue": "Biological psychology,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2003}, {"title": "A fixed size storage O (n3) time complexity learning algorithm for fully recurrent continually running networks", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 1992}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1992}, {"title": "Recurrent Neural Networks", "author": ["J. Schmidhuber", "A. Graves", "F. Gomez", "S. Hochreiter"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Slow feature analysis: Unsupervised learning of invariances", "author": ["L. Wiskott", "T.J. Sejnowski"], "venue": "Neural computation,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}], "referenceMentions": [], "year": 2016, "abstractText": "Predictive coding (PDC) has recently attracted attention in the neuroscience and computing community as a candidate unifying paradigm for neuronal studies and artificial neural network implementations particularly targeted at unsupervised learning systems. The Mismatch Negativity (MMN) has also recently been studied in relation to PC and found to be a useful ingredient in neural predictive coding systems. Backed by the behavior of living organisms, such networks are particularly useful in forming spatio-temporal transitions and invariant representations of the input world. However, most neural systems still do not account for large number of synapses even though this has been shown by a few machine learning researchers as an effective and very important component of any neural system if such a system is to behave properly. Our major point here is that PDC systems with the MMN effect in addition to a large number of synapses can greatly improve any neural learning system's performance and ability to make decisions in the machine world. In this paper, we propose a novel bio-mimetic computational intelligence algorithm \u2013 the Deviant Learning Algorithm, inspired by these key ideas and functional properties of recent brain-cognitive discoveries and theories. We also show by numerical experiments guided by theoretical insights, how our invented bio-mimetic algorithm can achieve competitive predictions with even with very small problem specific data.", "creator": "Microsoft\u00ae Office Word 2007"}}}