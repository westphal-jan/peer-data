{"id": "1609.01462", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2016", "title": "Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks", "abstract": "Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent estimation as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input.", "histories": [["v1", "Tue, 6 Sep 2016 09:45:51 GMT  (181kb,D)", "http://arxiv.org/abs/1609.01462v1", "Accepted at SIGDIAL 2016"]], "COMMENTS": "Accepted at SIGDIAL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["bing liu", "ian lane"], "accepted": false, "id": "1609.01462"}, "pdf": {"name": "1609.01462.pdf", "metadata": {"source": "CRF", "title": "Joint Online Spoken Language Understanding and Language Modeling with Recurrent Neural Networks", "authors": ["Bing Liu", "Ian Lane"], "emails": ["liubing@cmu.edu", "lane@cmu.edu"], "sections": [{"heading": "1 Introduction", "text": "As a critical component in spoken dialogue systems, spoken language comprehension (SLU), the system interprets the semantic meanings conveyed by speech signals. Important components in SLU systems include identifying the speaker's intention and extracting semantic components from the query of natural language, two tasks that are often referred to as intentions. Letter of Intent can be classified as a semantic expression, and slot filling can be treated as a sequence naming task. These two tasks are usually handled separately, so that a number of standard classifiers can be applied, such as support for vector machines (SVM) (Haffner et al., 2003) and the Convolutionary Neural Networks (CNNs) (Xu and Sarikaya, 2013)."}, {"heading": "2 Background", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Intent Detection", "text": "For an utterance with a word sequence w = (w1, w2,..., wT), the goal of intent recognition is to assign an intent class c from a predefined finite group of intent classes so that words or ngrams are mapped to a high-dimensional vector space and then combined component-wise by summing or intersection before being sent to the classifier. More structured neural network approaches for intent classification include the use of recursive neural networks (RecNN et al., 2014), recursive neural networks (Ravuri and Stolcke, 2015), better neural network approaches for expression classification (Kim Nvolutional, 2008 Comstructional Networks), and basic neural networks (RecNN et al., 2014)."}, {"heading": "2.2 Slot Filling", "text": "An important task in language understanding (SLU) is to extract semantic components by searching input text to enter values for predefined slots in a semantic frame (Mesnil et al., 2015), which is often referred to as slot filling. The slot filling task can also be seen as assigning a suitable semantic designation for each word in the given input text. In the following example from ATIS (Hemphill et al., 1990), the popular In / Out / Begin (IOB) annotation method assigns the words w = (w1, w2,..., wT), and tomorrow is the departure date. Other words in the example that have no semantic meaning are assigned to \"O.\" In an expression that consists of a sequence of words w = (w1, w2,..., wT), the target of the slot label is a sequence of semantic labels (slot = 2007, slot = 2), the slot is filled (in each case)."}, {"heading": "2.3 RNN Language Model", "text": "A language model assigns a probability following a probability distribution to a sequence of words w = (w1, w2,..., wT). In language modelling, w0 and wT + 1 are added to the word sequence symbolising the beginning and end of the sentence. By means of the chain rule, the probability of a word sequence can be factored in as: P (w) = T + 1% t = 1 P (wt | w0, w1,..., wt \u2212 1) (3) RNN-based language models (Mikolov et al., 2011) and the variant (Sundermeyer et al., 2012), which uses long-term memory (LSTM) (Hochreiter and Schmidhuber, 1997), have shown superior performance compared to conventional n-gram-based models. In this paper, we use an LSTM cell as a basic RNN unit for its greater ability to capture long-term dependencies in word sequences."}, {"heading": "2.4 RNN for Intent Detection and Slot Filling", "text": "As illustrated in Figure 2 (b), the RNN intent detection model uses the last RNN output to predict the intimacy class. This last RNN output can be considered to represent or embed the entire utterance. Alternatively, the embedding of the utterance can be obtained by taking the mean of the RNN output over the sequence. This embedding of the utterance is then used as input into the multinomial logistic regression to predict the intimacy classe.The RNN slot fill model takes the word as input and the corresponding slot label as output in each time step. The posterior probability for each slot label is calculated using the Softmax function over the RNN output. Dependencies of the slot labels can be modeled by inserting the output label from the previous time step into the current state of the model (while) the label can be inserted into the previous state (during)."}, {"heading": "3 Method", "text": "In this section, we describe the common SLU-LM model in detail. Figure 3 provides an overview of the proposed architecture."}, {"heading": "3.1 Model", "text": "Let w = (w0, w1, w2,..., wT + 1) represent the input string, where w0 and wT + 1 denote the beginning of the sentence (< bos >) and the end of the sentence (< eos >). Let c = (c0, c1, c2,..., cT) denote the sequence of intentions at each step of the sentence for the intention intention. Let s = (s0, s1, s2,..., sT) denote the intention sequence of the intention, where s0 is a filled-up intention associated with the sentence beginning < bos >. Referring to the common SLU-LM model shown in Figure 3, for the intention model, rather than predicting the intention, let the intention be used only after considering the entire statement as in the independent education intention model (Figure 2 (b)."}, {"heading": "3.2 Next Step Prediction", "text": "According to the model architecture in Figure 3, input to the system in time step t is the word in the t index of meaningfulness, and the output is the instruction class, the slot label, and the next word prediction. The RNN state ht encodes the information of all previously seen words, intuitions, and slot markups. The neural network model calculates the output in the following step sequence: ht = LSTM (ht \u2212 1, [wt, ct \u2212 1, st \u2212 1]) (7) P (ct | w \u2264 t, c < t, s < t) = IntentDist (ht) (8) P (st | w \u2264 t, c < t, s < t) = SlotLabelDist (ht) (9) P (wt + 1 | w \u2264 t, c \u2264 t, s \u2212 t is the output of lordons (ht, ct, st \u2212)."}, {"heading": "3.3 Training", "text": "The objective function also includes an L2 regularization term R (\u03b8) on the weights and biases of the three MLPs. This balances the search for the parameters that maximize the intention below the objective function: max. intention T = 0 [\u03b1c logP (c \u0445 | w \u2264 t, c < t, s < t) + \u03b1s logP (s \u0445 t, c < t, s < t) + \u03b1c logP (gew \u2264 t, c < t, s < t) + \u03b1s logP (s \u0445 t, c < t, s < t) + \u03b1w logP (gew \u2264 t, c \u2264 t;.) \u2212 \u03bbR (\u03b8) (11), where c \u0445 is the true intention class and s \u0445 t is the true intention class and the true slot label at the time."}, {"heading": "3.4 Inference", "text": "In the online deduction, we simply follow the greedy path of our conditional model without looking. At each step, the model emits the best intentional class and the best slot label on all previously issued symbols: c \u0442t = argmax ctP (ct | w \u2264 t, c, c, < t, s, s, < t) (12) s = argmax stP (st | w \u2264 t, c, c, < t, s, < t) (13) Many applications can benefit from this greedy inference approach compared to search-based inference methods, especially those running on embedded platforms without GPUs and with limited computing capacity. Alternatively, a left-to-right beam search can be performed (Sutskever et al., 2014; Chan et al., 2015) by maintaining a number of the best partial hypotheses at each step. Efficient beam search methods for the common conditional work remains to be maintained in our future model."}, {"heading": "3.5 Model Variations", "text": "In addition to the common RNN model described above (Figure 3), we also examine several common model variants for a fine-grained study of various factors influencing the common SLU-LM model performance. The designs of these model variants are shown in Figure 4.Figure 4 (a) demonstrating the design of a basic common SLU-LM model. At each step t, predictions of the intention class, slot and next word result in a common representation of the LSTM cell output quantity, and there are no conditional dependencies from previous intention classes and slot label output quantities. The single hidden layer MLP for each task results in additional discriminatory force for different tasks that take common representation as input. We use this model as a basic connection model. The models in Figure 4 (b) to 4 (d) extend the basic common model c by introducing conditional dependencies on the results of the intensity class."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "We used the Airline Travel Information Systems (ATIS) dataset (Hemphill et al., 1990) in our experiment. The ATIS dataset contains audio recordings of people making flight bookings, and it is widely used in language comprehension research. We followed the same ATIS corpus1 setup used in (Mesnil et al., 2015; Xu and Sarikaya, 2013; Tur et al., 2010). The training set contains 4978 statements from ATIS-2 and ATIS-3 corpora, and the test set contains 893 statements from ATIS-3 NOV93 and DEC94 datasets. We evaluated system performance when filling slots (127 different slot labels) using F1 score and performance on1We thank Gokhan Tur and Puyang Xu for sharing the ATIS datasets (NOV93 and DEC94 datasets)."}, {"heading": "4.2 Training Procedure", "text": "We used LSTM cell as the basic RNN unit according to the LSTM design in (Zaremba et al., 2014). The standard forge gate bias was set to 1. We used single-layer, unidirectional LSTM in the proposed joint online SLU-LM model. Deeper models by stacking the LSTM layers are to be investigated in future work. Text embeddings of size 300 were randomly initialized and finely tuned during the model training. We performed mini-batch training (batch size 16) using the Adam optimization method according to the proposed parameter setup in (Kingma and Ba, 2014). The maximum standard for gradient clipping was set to 5. During the model training, we applied dropout (drop-out rate 0.5) to the non-recurring connections (Zaremba et al., 2014) of RNN and the hidden layers of MLPs."}, {"heading": "4.3 Results and Discussions", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.3.1 Results with True Text Input", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu"}, {"heading": "4.3.2 Results in ASR Settings", "text": "To further evaluate the robustness of the proposed common SLU-LM model, we experimented with noisy voice input and performed SLU based on rescored ASR outputs. Model performance is evaluated based on ASR word error rate (WER), intentional classification error and slot filling F1 score. As shown in Table 2, the model with shared training RNN LM rescoring outperforms models with 5-gram LM rescoring and independent training RNN LM rescoring on all three evaluation variables. Using rescored ASR outputs (12.59% WHO) as input into the common training SLU model, the intentional classification error increased by 2.87%, and the slot filling F1 score decreased by 7.77% compared to the setup using real text input. Performance deterioration is expected as we show a more challenging and realistic setup with louder voice input from our common ASR model, with these results reflected in the table below."}, {"heading": "5 Conclusion", "text": "In this paper, we propose a conditional RNN model that allows us to collaboratively perform speech comprehension and speech modeling online. We show that the common training model achieves advantageous performance in intention detection and speech modeling by continuously modeling intent variations and slot markup dependencies along with the appearance of new words, slightly degrading intent detection errors compared to independent training models. In the ATIS benchmarking dataset, our common model results in 11.8% relative reduction in LM perplexity and 22.3% relative reduction in intent detection errors when using true text as input. The common model also shows consistent performance gains over independent training models in a more sophisticated and realistic setup using noisy voice input. Code to reproduce our experiments is available at: http: / / voic.sv.ced.u / software.html"}], "references": [{"title": "The third chime speech separation and recognition challenge: Dataset, task and baselines", "author": ["Jon Barker", "Ricard Marxer", "Emmanuel Vincent", "Shinji Watanabe."], "venue": "2015 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU 2015).", "citeRegEx": "Barker et al\\.,? 2015", "shortCiteRegEx": "Barker et al\\.", "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Samy Bengio", "Oriol Vinyals", "Navdeep Jaitly", "Noam Shazeer."], "venue": "Advances in Neural Information Processing Systems, pages 1171\u20131179.", "citeRegEx": "Bengio et al\\.,? 2015", "shortCiteRegEx": "Bengio et al\\.", "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1508.01211.", "citeRegEx": "Chan et al\\.,? 2015", "shortCiteRegEx": "Chan et al\\.", "year": 2015}, {"title": "A unified architecture for natural language processing: Deep neural networks with multitask learning", "author": ["Ronan Collobert", "Jason Weston."], "venue": "Proceedings of the 25th international conference on Machine learning, pages 160\u2013167. ACM.", "citeRegEx": "Collobert and Weston.,? 2008", "shortCiteRegEx": "Collobert and Weston.", "year": 2008}, {"title": "Joint semantic utterance classification and slot filling with recursive neural networks", "author": ["Daniel Guo", "Gokhan Tur", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 554\u2013559. IEEE.", "citeRegEx": "Guo et al\\.,? 2014", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Optimizing svms for complex call classification", "author": ["Patrick Haffner", "Gokhan Tur", "Jerry H Wright."], "venue": "Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP\u201903). 2003 IEEE International Conference on, volume 1, pages I\u2013632.", "citeRegEx": "Haffner et al\\.,? 2003", "shortCiteRegEx": "Haffner et al\\.", "year": 2003}, {"title": "The atis spoken language systems pilot corpus", "author": ["Charles T Hemphill", "John J Godfrey", "George R Doddington."], "venue": "Proceedings of the DARPA speech and natural language workshop, pages 96\u2013 101.", "citeRegEx": "Hemphill et al\\.,? 1990", "shortCiteRegEx": "Hemphill et al\\.", "year": 1990}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "An online sequence-to-sequence model using partial conditioning", "author": ["Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals", "Ilya Sutskeyver", "Samy Bengio."], "venue": "arXiv preprint arXiv:1511.04868.", "citeRegEx": "Jaitly et al\\.,? 2015", "shortCiteRegEx": "Jaitly et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Recurrent neural network structured output prediction for spoken language understanding", "author": ["Bing Liu", "Ian Lane."], "venue": "Proc. NIPS Workshop on Machine Learning for Spoken Language Understanding and Interactions.", "citeRegEx": "Liu and Lane.,? 2015", "shortCiteRegEx": "Liu and Lane.", "year": 2015}, {"title": "Maximum entropy markov models for information extraction and segmentation", "author": ["Andrew McCallum", "Dayne Freitag", "Fernando CN Pereira."], "venue": "ICML, volume 17, pages 591\u2013598.", "citeRegEx": "McCallum et al\\.,? 2000", "shortCiteRegEx": "McCallum et al\\.", "year": 2000}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2015}, {"title": "Extensions of recurrent neural network language model", "author": ["Tom\u00e1\u0161 Mikolov", "Stefan Kombrink", "Luk\u00e1\u0161 Burget", "Jan Honza \u010cernock\u1ef3", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Confer-", "citeRegEx": "Mikolov et al\\.,? 2011", "shortCiteRegEx": "Mikolov et al\\.", "year": 2011}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["Vassil Panayotov", "Guoguo Chen", "Daniel Povey", "Sanjeev Khudanpur."], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, pages 5206\u2013", "citeRegEx": "Panayotov et al\\.,? 2015", "shortCiteRegEx": "Panayotov et al\\.", "year": 2015}, {"title": "Recurrent neural network and lstm models for lexical utterance classification", "author": ["Suman Ravuri", "Andreas Stolcke."], "venue": "Sixteenth Annual Conference of the International Speech Communication Association.", "citeRegEx": "Ravuri and Stolcke.,? 2015", "shortCiteRegEx": "Ravuri and Stolcke.", "year": 2015}, {"title": "Generative and discriminative algorithms for spoken language understanding", "author": ["Christian Raymond", "Giuseppe Riccardi."], "venue": "INTERSPEECH, pages 1605\u20131608.", "citeRegEx": "Raymond and Riccardi.,? 2007", "shortCiteRegEx": "Raymond and Riccardi.", "year": 2007}, {"title": "Lstm neural networks for language modeling", "author": ["Martin Sundermeyer", "Ralf Schl\u00fcter", "Hermann Ney."], "venue": "INTERSPEECH, pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "What is left to be understood in atis? In Spoken Language Technology Workshop (SLT), 2010 IEEE, pages 19\u201324", "author": ["Gokhan Tur", "Dilek Hakkani-Tur", "Larry Heck."], "venue": "IEEE.", "citeRegEx": "Tur et al\\.,? 2010", "shortCiteRegEx": "Tur et al\\.", "year": 2010}, {"title": "Convolutional neural network based triangular crf for joint intent detection and slot filling", "author": ["Puyang Xu", "Ruhi Sarikaya."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 78\u201383. IEEE.", "citeRegEx": "Xu and Sarikaya.,? 2013", "shortCiteRegEx": "Xu and Sarikaya.", "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi."], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 189\u2013194. IEEE.", "citeRegEx": "Yao et al\\.,? 2014", "shortCiteRegEx": "Yao et al\\.", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["Wojciech Zaremba", "Ilya Sutskever", "Oriol Vinyals."], "venue": "arXiv preprint arXiv:1409.2329.", "citeRegEx": "Zaremba et al\\.,? 2014", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "For intent detection, a number of standard classifiers can be applied, such as support vector machines (SVMs) (Haffner et al., 2003) and convolutional neural networks (CNNs) (Xu and Sarikaya, 2013).", "startOffset": 110, "endOffset": 132}, {"referenceID": 21, "context": ", 2003) and convolutional neural networks (CNNs) (Xu and Sarikaya, 2013).", "startOffset": 49, "endOffset": 72}, {"referenceID": 12, "context": "For slot filling, popular approaches include using sequence models such as maximum entropy Markov models (MEMMs) (McCallum et al., 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al.", "startOffset": 113, "endOffset": 136}, {"referenceID": 17, "context": ", 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al.", "startOffset": 42, "endOffset": 70}, {"referenceID": 22, "context": ", 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al., 2014; Mesnil et al., 2015).", "startOffset": 109, "endOffset": 148}, {"referenceID": 13, "context": ", 2000), conditional random fields (CRFs) (Raymond and Riccardi, 2007), and recurrent neural networks (RNNs) (Yao et al., 2014; Mesnil et al., 2015).", "startOffset": 109, "endOffset": 148}, {"referenceID": 8, "context": "Moreover, it can be used as the RNN decoder in an end-to-end trainable sequence-to-sequence speech recognition model (Jaitly et al., 2015).", "startOffset": 117, "endOffset": 138}, {"referenceID": 4, "context": "More structured neural network approaches for utterance classification include using recursive neural network (RecNN) (Guo et al., 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014).", "startOffset": 118, "endOffset": 136}, {"referenceID": 16, "context": ", 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014).", "startOffset": 34, "endOffset": 60}, {"referenceID": 3, "context": ", 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014).", "startOffset": 102, "endOffset": 141}, {"referenceID": 9, "context": ", 2014), recurrent neural network (Ravuri and Stolcke, 2015), and convolutional neural network models (Collobert and Weston, 2008; Kim, 2014).", "startOffset": 102, "endOffset": 141}, {"referenceID": 13, "context": "A major task in spoken language understanding (SLU) is to extract semantic constituents by searching input text to fill in values for predefined slots in a semantic frame (Mesnil et al., 2015), which is often referred to as slot filling.", "startOffset": 171, "endOffset": 192}, {"referenceID": 6, "context": "In the below example from ATIS (Hemphill et al., 1990) corpus following the popular in/out/begin (IOB) annotation method, Seattle and San Diego are the from and to locations respectively according to the slot labels, and tomorrow is the departure date.", "startOffset": 31, "endOffset": 54}, {"referenceID": 17, "context": "Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al.", "startOffset": 52, "endOffset": 80}, {"referenceID": 22, "context": "Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al., 2014; Mesnil et al., 2015; Liu and Lane, 2015) are among the most popular methods for sequence labeling tasks.", "startOffset": 96, "endOffset": 155}, {"referenceID": 13, "context": "Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al., 2014; Mesnil et al., 2015; Liu and Lane, 2015) are among the most popular methods for sequence labeling tasks.", "startOffset": 96, "endOffset": 155}, {"referenceID": 11, "context": "Sequence models including conditional random fields (Raymond and Riccardi, 2007) and RNN models (Yao et al., 2014; Mesnil et al., 2015; Liu and Lane, 2015) are among the most popular methods for sequence labeling tasks.", "startOffset": 96, "endOffset": 155}, {"referenceID": 14, "context": "RNN-based language models (Mikolov et al., 2011), and the variant (Sundermeyer et al.", "startOffset": 26, "endOffset": 48}, {"referenceID": 18, "context": ", 2011), and the variant (Sundermeyer et al., 2012)", "startOffset": 25, "endOffset": 51}, {"referenceID": 7, "context": "using long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) have shown superior performance comparing to traditional n-gram based models.", "startOffset": 36, "endOffset": 70}, {"referenceID": 1, "context": "To bridge the gap between training and inference, scheduled sampling method (Bengio et al., 2015) can be applied.", "startOffset": 76, "endOffset": 97}, {"referenceID": 11, "context": "Instead of only using previous true label, using sample from previous predicted label distribution in model training makes the model more robust by forcing it to learn to handle its own prediction mistakes (Liu and Lane, 2015).", "startOffset": 206, "endOffset": 226}, {"referenceID": 19, "context": "Alternatively, one can do left-to-right beam search (Sutskever et al., 2014; Chan et al., 2015) by maintaining a set of \u03b2 best partial hypotheses at each step.", "startOffset": 52, "endOffset": 95}, {"referenceID": 2, "context": "Alternatively, one can do left-to-right beam search (Sutskever et al., 2014; Chan et al., 2015) by maintaining a set of \u03b2 best partial hypotheses at each step.", "startOffset": 52, "endOffset": 95}, {"referenceID": 6, "context": "We used the Airline Travel Information Systems (ATIS) dataset (Hemphill et al., 1990) in our experiment.", "startOffset": 62, "endOffset": 85}, {"referenceID": 13, "context": "We followed the same ATIS corpus1 setup used in (Mesnil et al., 2015; Xu and Sarikaya, 2013; Tur et al., 2010).", "startOffset": 48, "endOffset": 110}, {"referenceID": 21, "context": "We followed the same ATIS corpus1 setup used in (Mesnil et al., 2015; Xu and Sarikaya, 2013; Tur et al., 2010).", "startOffset": 48, "endOffset": 110}, {"referenceID": 20, "context": "We followed the same ATIS corpus1 setup used in (Mesnil et al., 2015; Xu and Sarikaya, 2013; Tur et al., 2010).", "startOffset": 48, "endOffset": 110}, {"referenceID": 0, "context": "To provide a more challenging and realistic evaluation, we used the simulated noisy utterances that were generated by artificially mixing clean speech data with noisy backgrounds following the simulation methods described in the third CHiME Speech Separation and Recognition Challenge (Barker et al., 2015).", "startOffset": 285, "endOffset": 306}, {"referenceID": 23, "context": "We used LSTM cell as the basic RNN unit, following the LSTM design in (Zaremba et al., 2014).", "startOffset": 70, "endOffset": 92}, {"referenceID": 10, "context": "We conducted mini-batch training (with batch size 16) using Adam optimization method following the suggested parameter setup in (Kingma and Ba, 2014).", "startOffset": 128, "endOffset": 149}, {"referenceID": 23, "context": "5) to the non-recurrent connections (Zaremba et al., 2014) of RNN and the hidden layers of MLPs, and applied L2 regularization (\u03bb = 10\u22124) on the parameters of MLPs.", "startOffset": 36, "endOffset": 58}, {"referenceID": 15, "context": "For the evaluation in ASR settings, we used the acoustic model trained on LibriSpeech dataset (Panayotov et al., 2015), and the language model trained on ATIS training corpus.", "startOffset": 94, "endOffset": 118}, {"referenceID": 4, "context": "1 RecNN (Guo et al., 2014) 4.", "startOffset": 8, "endOffset": 26}, {"referenceID": 4, "context": "22 2 RecNN+Viterbi (Guo et al., 2014) 4.", "startOffset": 19, "endOffset": 37}, {"referenceID": 4, "context": "Related joint models: RecNN: Joint intent detection and slot filling model using recursive neural network (Guo et al., 2014).", "startOffset": 106, "endOffset": 124}, {"referenceID": 4, "context": "RecNN+Viterbi: Joint intent detection and slot filling model using recursive neural network with Viterbi sequence optimization for slot filling (Guo et al., 2014).", "startOffset": 144, "endOffset": 162}], "year": 2016, "abstractText": "Speaker intent detection and semantic slot filling are two critical tasks in spoken language understanding (SLU) for dialogue systems. In this paper, we describe a recurrent neural network (RNN) model that jointly performs intent detection, slot filling, and language modeling. The neural network model keeps updating the intent prediction as word in the transcribed utterance arrives and uses it as contextual features in the joint model. Evaluation of the language model and online SLU model is made on the ATIS benchmarking data set. On language modeling task, our joint model achieves 11.8% relative reduction on perplexity comparing to the independent training language model. On SLU tasks, our joint model outperforms the independent task training model by 22.3% on intent detection error rate, with slight degradation on slot filling F1 score. The joint model also shows advantageous performance in the realistic ASR settings with noisy speech input.", "creator": "TeX"}}}