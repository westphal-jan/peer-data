{"id": "1609.01508", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Sep-2016", "title": "Low-rank Bandits with Latent Mixtures", "abstract": "We study the task of maximizing rewards from recommending items (actions) to users sequentially interacting with a recommender system. Users are modeled as latent mixtures of C many representative user classes, where each class specifies a mean reward profile across actions. Both the user features (mixture distribution over classes) and the item features (mean reward vector per class) are unknown a priori. The user identity is the only contextual information available to the learner while interacting. This induces a low-rank structure on the matrix of expected rewards r a,b from recommending item a to user b. The problem reduces to the well-known linear bandit when either user or item-side features are perfectly known. In the setting where each user, with its stochastically sampled taste profile, interacts only for a small number of sessions, we develop a bandit algorithm for the two-sided uncertainty. It combines the Robust Tensor Power Method of Anandkumar et al. (2014b) with the OFUL linear bandit algorithm of Abbasi-Yadkori et al. (2011). We provide the first rigorous regret analysis of this combination, showing that its regret after T user interactions is $\\tilde O(C\\sqrt{BT})$, with B the number of users. An ingredient towards this result is a novel robustness property of OFUL, of independent interest.", "histories": [["v1", "Tue, 6 Sep 2016 12:01:30 GMT  (74kb)", "http://arxiv.org/abs/1609.01508v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["aditya gopalan", "odalric-ambrym maillard", "mohammadi zaki"], "accepted": false, "id": "1609.01508"}, "pdf": {"name": "1609.01508.pdf", "metadata": {"source": "CRF", "title": "Low-rank Bandits with Latent Mixtures", "authors": ["Aditya Gopalan"], "emails": ["ADITYA@ECE.IISC.ERNET.IN", "ODALRICAMBRYM.MAILLARD@INRIA.FR", "ZAKI@ECE.IISC.ERNET.IN"], "sections": [{"heading": null, "text": "In fact, it is the case that most of them will be able to abide by the rules they have imposed on themselves, and that they will be able to understand the rules they have imposed on themselves. (...) In fact, it is the case that they are able to understand the rules they have given themselves. (...) In fact, it is the case that they are able to understand the rules they have imposed on themselves. (...) In fact, it is the case that they are able to break the rules. \"(...)"}], "references": [{"title": "Improved Algorithms for Linear Stochastic Bandits", "author": ["Yasin Abbasi-Yadkori", "David Pal", "Csaba Szepesvari"], "venue": "In Proc. NIPS,", "citeRegEx": "Abbasi.Yadkori et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Abbasi.Yadkori et al\\.", "year": 2011}, {"title": "A method of moments for mixture models and hidden markov models", "author": ["Animashree Anandkumar", "Daniel Hsu", "Sham M Kakade"], "venue": "arXiv preprint arXiv:1203.0683,", "citeRegEx": "Anandkumar et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2012}, {"title": "A tensor approach to learning mixed membership community models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M Kakade"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Tensor decompositions for learning latent variable models", "author": ["Animashree Anandkumar", "Rong Ge", "Daniel Hsu", "Sham M. Kakade", "Matus Telgarsky"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Anandkumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Anandkumar et al\\.", "year": 2014}, {"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["P. Auer", "N. Cesa-Bianchi", "P. Fischer"], "venue": "Machine Learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "A latent source model for online collaborative filtering", "author": ["Guy Bresler", "George H Chen", "Devavrat Shah"], "venue": "In Proc. NIPS", "citeRegEx": "Bresler et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bresler et al\\.", "year": 2014}, {"title": "Stochastic Linear Optimization under Bandit Feedback", "author": ["Varsha Dani", "Thomas P. Hayes", "Sham M. Kakade"], "venue": "In Proc. COLT,", "citeRegEx": "Dani et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Dani et al\\.", "year": 2008}, {"title": "Sequential transfer in multi-armed", "author": ["Mohammad Gheshlaghi Azar", "Alessandro Lazaric", "Emma Brunskill"], "venue": null, "citeRegEx": "Azar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Azar et al\\.", "year": 2014}, {"title": "bandit with finite set of models", "author": ["Thomas Hofmann", "Jan Puzicha"], "venue": "In Proc. NIPS,", "citeRegEx": "Hofmann and Puzicha.,? \\Q2013\\E", "shortCiteRegEx": "Hofmann and Puzicha.", "year": 2013}, {"title": "Using mixture models for collaborative filtering", "author": ["Jon Kleinberg", "Mark Sandler"], "venue": "Proc. ACM Symposium on Theory Of computing (STOC),", "citeRegEx": "Kleinberg and Sandler.,? \\Q2013\\E", "shortCiteRegEx": "Kleinberg and Sandler.", "year": 2013}, {"title": "Efficient learning by implicit exploration", "author": ["Tom\u00e1\u0161 Koc\u00e1k", "Gergely Neu", "Michal Valko", "R\u00e9mi Munos"], "venue": "Theory Of Computing (STOC),", "citeRegEx": "Koc\u00e1k et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Koc\u00e1k et al\\.", "year": 2004}, {"title": "bandit problems with side observations", "author": ["Alessandro Lazaric", "Emma Brunskill"], "venue": "In Proc. NIPS,", "citeRegEx": "Lazaric and Brunskill,? \\Q2014\\E", "shortCiteRegEx": "Lazaric and Brunskill", "year": 2014}, {"title": "Odalric-Ambrym Maillard and Shie Mannor", "author": ["2014. J\u00e9r\u00e9mie Mary", "Romaric Gaudel", "Preux Philippe"], "venue": "Proc. NIPS, pages 2220\u20132228,", "citeRegEx": "Mary et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mary et al\\.", "year": 2013}, {"title": "The final term above can be bounded using Anandkumar et al. (2012, Lemma E.4) \u2013 a version of Theorem", "author": ["Stewart"], "venue": null, "citeRegEx": "Stewart,? \\Q1990\\E", "shortCiteRegEx": "Stewart", "year": 1990}, {"title": "Unregularized Least squares In our setting where we consider finitely many arms, one way wonder whether it is possible to remove the regularization parameter \u03bb", "author": ["F. Appendix"], "venue": "Following Rusmevichientong and Tsitsiklis", "citeRegEx": "Appendix,? \\Q2010\\E", "shortCiteRegEx": "Appendix", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "It combines the Robust Tensor Power Method of Anandkumar et al. (2014b) with the OFUL linear bandit algorithm of Abbasi-Yadkori et al.", "startOffset": 46, "endOffset": 72}, {"referenceID": 0, "context": "(2014b) with the OFUL linear bandit algorithm of Abbasi-Yadkori et al. (2011). We provide the first rigorous regret analysis of this combination, showing that its regret after T user interactions is \u00d5(C \u221a BT ), with B the number of users.", "startOffset": 49, "endOffset": 78}, {"referenceID": 0, "context": "We develop a novel bandit algorithm (Algorithm 3) that combines (a) the Optimization in the Face of Uncertainty Linear bandit OFUL algorithm (Abbasi-Yadkori et al., 2011) for bandits with known action features, and (b) a variant of the Robust Tensor Power (RTP) algorithm (Anandkumar et al.", "startOffset": 141, "endOffset": 170}, {"referenceID": 0, "context": ", 2009; Anandkumar et al., 2014a,b); the task of simultaneously optimizing net utility in a bandit fashion in complex expression models like these has received little or no analytical treatment. Our work takes a step towards filling this void. An especially challenging aspect of online learning in recommender systems is the relatively meager number of available interactions with a same user, which is offset to an extent by the assumption that users can only have a limited number of taste profiles (classes). Indeed, if one can identify the class to which a certain user belongs and aggregate information from all other users in that class, then one can recommend to the user the best item for the class. In practice, classes are latent and not necessarily known in advance, and several works (Gentile et al., 2014; Lazaric et al., 2013; Maillard and Mannor, 2014) study the restricted situation when each user always belongs to one specific class (i.e., when all mixture distributions have support size 1). We go two steps further, since in many situations (a) users cannot be assumed to belong to one class only, such as when a user account is shared by several individuals (e.g. a smart-TV), and (b) the duration of a user-session, that is the number of consecutive recommendations to the same individual connected to a user-account, cannot assumed to be long1. The key challenges that this work addresses are (1) the lack of knowledge of \u201cfeatures\u201d on both the user-side and item-side in a linear bandit problem (in this case, both the user mixture weights and the item class reward profiles) and (2) provable regret minimization with very few i.e. O(1) interactions with every user b having a specific taste profile, as opposed to a large number of interactions such as in transfer learning (Lazaric et al., 2013). Contributions and overview of results. We consider a setting when users are assumed to come from arbitrary mixtures across classes (they are not assumed to fall perfectly in one class as was the assumption in works by Gentile et al. (2014); Maillard and Mannor (2014)).", "startOffset": 8, "endOffset": 2064}, {"referenceID": 0, "context": ", 2009; Anandkumar et al., 2014a,b); the task of simultaneously optimizing net utility in a bandit fashion in complex expression models like these has received little or no analytical treatment. Our work takes a step towards filling this void. An especially challenging aspect of online learning in recommender systems is the relatively meager number of available interactions with a same user, which is offset to an extent by the assumption that users can only have a limited number of taste profiles (classes). Indeed, if one can identify the class to which a certain user belongs and aggregate information from all other users in that class, then one can recommend to the user the best item for the class. In practice, classes are latent and not necessarily known in advance, and several works (Gentile et al., 2014; Lazaric et al., 2013; Maillard and Mannor, 2014) study the restricted situation when each user always belongs to one specific class (i.e., when all mixture distributions have support size 1). We go two steps further, since in many situations (a) users cannot be assumed to belong to one class only, such as when a user account is shared by several individuals (e.g. a smart-TV), and (b) the duration of a user-session, that is the number of consecutive recommendations to the same individual connected to a user-account, cannot assumed to be long1. The key challenges that this work addresses are (1) the lack of knowledge of \u201cfeatures\u201d on both the user-side and item-side in a linear bandit problem (in this case, both the user mixture weights and the item class reward profiles) and (2) provable regret minimization with very few i.e. O(1) interactions with every user b having a specific taste profile, as opposed to a large number of interactions such as in transfer learning (Lazaric et al., 2013). Contributions and overview of results. We consider a setting when users are assumed to come from arbitrary mixtures across classes (they are not assumed to fall perfectly in one class as was the assumption in works by Gentile et al. (2014); Maillard and Mannor (2014)).", "startOffset": 8, "endOffset": 2092}, {"referenceID": 4, "context": "UCB (Auer et al., 2002)) per user, scales as O(B \u221a TA/B) = O( \u221a ABT ) after T rounds2 , which is considerably suboptimal in the practical case with a very large number of items but very few representative user classes (C \u226a A).", "startOffset": 4, "endOffset": 23}, {"referenceID": 10, "context": "An alternative is the implicit exploration method due to Koc\u00e1k et al. (2014).", "startOffset": 57, "endOffset": 77}, {"referenceID": 1, "context": "Our next result makes use of the following proposition from Anandkumar et al. (2014b, Theorem 5.1), restated here for completeness. Proposition 1 (Theorem 5.1 of Anandkumar et al. (2014b)) Let T\u0302 = T + E \u2208 RC\u00d7C\u00d7C , where T is a symmetric tensor with orthogonal decomposition T = \u2211C c=1 \u03bbc\u03c6 \u22973 c , where each \u03bbc > 0, {\u03c6c}c\u2208[C] is an orthonormal basis, and E is a symmetric tensor with operator norm ||E|| 6 \u03b5.", "startOffset": 60, "endOffset": 188}, {"referenceID": 1, "context": "See Anandkumar et al. (2014b) for more details on notation and results.", "startOffset": 4, "endOffset": 30}, {"referenceID": 0, "context": "Algorithm 2 OFUL (Optimism in Face of Uncertainty for Linear bandits) (Abbasi-Yadkori et al., 2011) Require: Arms\u2019 features \u016a , regularization parameter \u03bb, norm parameter R\u0398 for all times t > 1 do 1.", "startOffset": 70, "endOffset": 99}, {"referenceID": 0, "context": "Theorem 2 (OFUL regret (Abbasi-Yadkori et al., 2011)) Assume that ||v||2 6 R\u0398, and that for all a \u2208 A, ||\u016ba||2 6 RX , |\u3008\u016ba,v\u3009| 6 1.", "startOffset": 23, "endOffset": 52}, {"referenceID": 0, "context": "In the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori.", "startOffset": 32, "endOffset": 119}, {"referenceID": 6, "context": "In the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori.", "startOffset": 32, "endOffset": 119}, {"referenceID": 9, "context": ", no exploration mini-sessions, and (c) An implementation of the Alternating Least Squares estimator (Tak\u00e1cs and Tikk, 2012; Mary et al., 2014) for the matrix U along with OFUL per-user. The proposed algorithm, with the theoretically suggested exploration \u00d5(n\u22121/2), is observed to exploit the latent structure considerably better than simple UCB, and is not too far from the unrealistic OFUL strategy which enjoys the luxury of latent class information. It is also competitive with performing Alternating Least Squares, which does not come with analytically sound performance guarantees in the bandit learning setting. Also, the large additive constants in the theoretical bounds for Algorithm 3 do not manifest here. Related work. The popular low-rank matrix completion problem studies the recovery U and V given a small number of entries sampled at random from UV T with both U and V being tall matrices, see for instance Jain et al. (2013) and citations therein.", "startOffset": 125, "endOffset": 943}, {"referenceID": 0, "context": "In the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori. In contrast, the problem of low regret recommendation across users with latent mixtures does not afford us the luxury of knowing either U or V , and so they must be learnt \u201con the fly\u201d. Another related work in the context of bandit type schemes for latent mixture model recommender systems is that of Bresler et al. (2014), in which, under the very specific uniform mixture model for all users, they exhibit strategies with good regret.", "startOffset": 33, "endOffset": 578}, {"referenceID": 0, "context": "In the linear bandit literature (Abbasi-Yadkori et al., 2011; Rusmevichientong and Tsitsiklis, 2010; Dani et al., 2008), the key constraining assumption is that either user side (V ) or item side (U ) features are precisely and completely known a priori. In contrast, the problem of low regret recommendation across users with latent mixtures does not afford us the luxury of knowing either U or V , and so they must be learnt \u201con the fly\u201d. Another related work in the context of bandit type schemes for latent mixture model recommender systems is that of Bresler et al. (2014), in which, under the very specific uniform mixture model for all users, they exhibit strategies with good regret. Nguyen et al. (2014) consider an alternating minimization type scheme in linear bandit models with two-sided uncertainty (an alternative model involving latent \u201cfactors\u201d).", "startOffset": 33, "endOffset": 713}, {"referenceID": 4, "context": ", 2014b, 2012) essentially with a standard UCB (Auer et al., 2002), but however works in the setting of a large number interactions with a same user, without assuming access to \u201cuser ids\u201d.", "startOffset": 47, "endOffset": 66}, {"referenceID": 1, "context": "(2013): The method combines the RTP method (Anandkumar et al., 2014b, 2012) essentially with a standard UCB (Auer et al., 2002), but however works in the setting of a large number interactions with a same user, without assuming access to \u201cuser ids\u201d. As a result, the regret bound in this setting scales linearly with the number of rounds. Our result in this paper shows that with additional access to just user identifiers, we can reduce the regret rate to be sublinear in time. The RTP method has been used as a processing step to the EM algorithm in crowdsourcing (Zhang et al., 2014), but only convergence properties are considered, which is not enough to provide regret guarantees. On the theoretical side, our contribution generalizes the setting of clustered bandits (Maillard and Mannor, 2014; Gentile et al., 2014) in which a hard clustering model is assumed (one user is assigned to one class, or equivalently mixture distributions can only have support size 1). In particular, Maillard and Mannor (2014) specifically highlight the benefit of a collaborative gain across users against using a vanilla UCB for each user.", "startOffset": 44, "endOffset": 1014}, {"referenceID": 1, "context": "(2013): The method combines the RTP method (Anandkumar et al., 2014b, 2012) essentially with a standard UCB (Auer et al., 2002), but however works in the setting of a large number interactions with a same user, without assuming access to \u201cuser ids\u201d. As a result, the regret bound in this setting scales linearly with the number of rounds. Our result in this paper shows that with additional access to just user identifiers, we can reduce the regret rate to be sublinear in time. The RTP method has been used as a processing step to the EM algorithm in crowdsourcing (Zhang et al., 2014), but only convergence properties are considered, which is not enough to provide regret guarantees. On the theoretical side, our contribution generalizes the setting of clustered bandits (Maillard and Mannor, 2014; Gentile et al., 2014) in which a hard clustering model is assumed (one user is assigned to one class, or equivalently mixture distributions can only have support size 1). In particular, Maillard and Mannor (2014) specifically highlight the benefit of a collaborative gain across users against using a vanilla UCB for each user. However their setting is less general than assuming a soft clustering of users (one user corresponds to a mixture of classes) across various \u201crepresentative\u201d taste profiles as we study here. The Alternating Least-Squares (ALS) method (Tak\u00e1cs and Tikk, 2012; Mary et al., 2014) has been shown to yield promising experimental results in similar settings where both U and V are unknown. However, no theoretical guarantees are known for this algorithm that may converge to a local optimum in general. The work of Valko et al. (2014) studies stochastic bandits with a linear model over a low-rank (graph Laplacian) structure.", "startOffset": 44, "endOffset": 1658}, {"referenceID": 4, "context": "Proof The proof closely follows that of Gheshlaghi Azar et al. (2013). First, note that by property of the rank 1 decomposition ((Anandkumar et al.", "startOffset": 51, "endOffset": 70}, {"referenceID": 7, "context": "We use the result of Lemma 5 from Gheshlaghi Azar et al. (2013) to control (e) and (f).", "startOffset": 45, "endOffset": 64}, {"referenceID": 1, "context": "3) are controlled by the perturbation method from Anandkumar et al. (2014b), under the condition that en = \u2016T \u2212 T\u0302\u2016 6 C1 \u03bbmin C (where C1 is a universal constant).", "startOffset": 50, "endOffset": 76}, {"referenceID": 7, "context": "In order to make the condition explicit in our setting, we use the fact that by Lemma 6 from Gheshlaghi Azar et al. (2013), if e n 6 12 min{\u0393, \u03c3min} then", "startOffset": 104, "endOffset": 123}, {"referenceID": 0, "context": "The argument from here can be continued in the same way as in Abbasi-Yadkori et al. (2011) to yield", "startOffset": 62, "endOffset": 91}, {"referenceID": 1, "context": "The final term above can be bounded using Anandkumar et al. (2012, Lemma E.4) \u2013 a version of Theorem 2.5 in Stewart et al. (1990). Assuming (u\u22c4)J is invertible, and \u2225(u\u22c4)\u22121 J ((un)J \u2212 (u\u22c4)J ) \u2225\u2225 2 < 1, then (un)J is invertible, and a resulting bound on the norm of its inverse lets us write \u2225(u\u22c4)\u22121 J \u2225\u2225 2 + \u2225(u\u22c4n)\u22121 J \u2212 (u\u22c4)\u22121 J \u2225\u2225 2 6 \u2225(u\u22c4)\u22121 J \u2225\u2225 2 + \u2016(un)J \u2212 (u)J\u20162 \u2225(u\u22c4)\u22121 J \u2225\u22252 2 1\u2212 \u2225(u\u22c4)\u22121 J ((un)J \u2212 (u\u22c4)J ) \u2225\u2225 2 .", "startOffset": 42, "endOffset": 130}], "year": 2016, "abstractText": "We study the task of maximizing rewards from recommending items (actions) to users sequentially interacting with a recommender system. Users are modeled as latent mixtures of C many representative user classes, where each class specifies a mean reward profile across actions. Both the user features (mixture distribution over classes) and the item features (mean reward vector per class) are unknown a priori. The user identity is the only contextual information available to the learner while interacting. This induces a low-rank structure on the matrix of expected rewards ra,b from recommending item a to user b. The problem reduces to the well-known linear bandit when either useror item-side features are perfectly known. In the setting where each user, with its stochastically sampled taste profile, interacts only for a small number of sessions, we develop a bandit algorithm for the two-sided uncertainty. It combines the Robust Tensor Power Method of Anandkumar et al. (2014b) with the OFUL linear bandit algorithm of Abbasi-Yadkori et al. (2011). We provide the first rigorous regret analysis of this combination, showing that its regret after T user interactions is \u00d5(C \u221a BT ), with B the number of users. An ingredient towards this result is a novel robustness property of OFUL, of independent interest.", "creator": "LaTeX with hyperref package"}}}