{"id": "1609.01819", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Semantic Video Trailers", "abstract": "Query-based video summarization is the task of creating a brief visual trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. In this paper, we propose an unsupervised label propagation approach for this task. Our approach effectively captures the multimodal semantics of queries and videos using state-of-the-art deep neural networks and creates a summary that is both semantically coherent and visually attractive. We describe the theoretical framework of our graph-based approach and empirically evaluate its effectiveness in creating relevant and attractive trailers. Finally, we showcase example video trailers generated by our system.", "histories": [["v1", "Wed, 7 Sep 2016 03:35:54 GMT  (864kb,D)", "http://arxiv.org/abs/1609.01819v1", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["harrie oosterhuis", "sujith ravi", "michael bendersky"], "accepted": false, "id": "1609.01819"}, "pdf": {"name": "1609.01819.pdf", "metadata": {"source": "META", "title": "Semantic Video Trailers", "authors": ["Harrie Oosterhuis", "Sujith Ravi", "Michael Bendersky"], "emails": ["HARRIE.OOSTERHUIS@STUDENT.UVA.NL", "SRAVI@GOOGLE.COM", "BEMIKE@GOOGLE.COM"], "sections": [{"heading": "1. Introduction", "text": "This year, it will only take one year for an agreement to be reached."}, {"heading": "2. Related Work", "text": "The task of summarizing a video can be understood as creating a textual description, storyboard, graphic representation, or video processing that adequately captures the content of a video (Money & Agius, 2008).In this study, we deal with the task of constructing a video collection that is accomplished by drawing on the video and skipping all the insignificant portions.Thus, all the content in the resulting skims is from the video and is played in the same chronological order.The main difference from this previous work is that our summaries are used in a query-based way to calculate the meaning of a video fragment. Some use only visual features, such as the model only adds a fragment when it is visually distinct from previously added fragments (Zhao & Xing, 2014; Almeida et al)."}, {"heading": "3. Method", "text": "In this section, we propose two models for a semantic video summary, the first of which uses only semantic information from the video, while the second integrates both semantic and visual information.Both models take a query q and a video V as input; the query is output by a user and the video is classified as relevant by a video-call algorithm. Each input video is initially divided into a second segment, which are then used to create the trailer summary. Working with these segments makes the final summary more comprehensible, since one second is enough time for the viewer to perceive a clip contained in the video. In addition, it makes the systems more scalable, since computer-costly operations only have to be performed every second instead of once for each frame in the full video.Both systems evaluate all segments of a single video based on the segment content and the query of the user. The summary is then created by taking the top 20 segments and sequencing them chronologically in the entire video in the order of the sequence."}, {"heading": "3.1. Query Representation", "text": "All our models are based on the intuition that segments that capture the same semantic content as the query should be included. Therefore, the model estimates how similar the content of the query and the segment are, and classifies them accordingly. The first step in estimating the similarity is to process the q query and map it to a universal representation of entities eq-Eq (and their corresponding confidence values weq) extracted from a knowledge base such as Wikipedia."}, {"heading": "3.2. Direct Matching", "text": "Given the entities Eq in the query, a simple approach is to use an image processing model to recognize the given entities in the frame, such as a deep learning architecture for image concept recognition (Szegedy et al., 2015; He et al., 2015). Then, the query segment matching is merely a reliability of the concept recognition model in detecting the query entities in the segment. However, this direct matching approach has several major drawbacks. First, the number of concepts that a state-of-the-art detection model can detect is limited to 22,000 by the largest publicly available corpus (Russakovsky et al., 2015), an extremely small subset of entities that a query can express. In addition, the processing of the data set of the query video pairs collected for our experiments in Section 4.2, which contains over 34,000 pairs, is limited."}, {"heading": "3.3. Semantic Matching", "text": "As in the previous method, we first apply the Inception Model (Szegeq et al., 2015) - a state-of-the-art, deep neural network architecture designed to recognize a large number of concepts in images - to each frame Fi in the segment. The model outputs a series of entity concepts EFi with confidence as to how secure the system is that each concept ef-EFi in the segment FI is. But instead of replacing each concept e with its precalculated entity mappings EFi and Eq, we calculate a dense semantic embed representation for both the query q and a given video frame menu Fi using its entity mappings Fi.In other words, we replace each concept e with its pre-calculated semantic embedding vector Se. Then, a semantic representation of the segment Fi on SFi = 1 | EFi-Ef entity of the sequence q.Similarly, we represent the quekery vector weighted by quekery."}, {"heading": "3.4. Graph-Based Matching", "text": "In fact, it is as if it is a matter of a way in which one sees oneself in a position to put oneself in the center of attention. (...) In fact, it is a matter of a way in which one puts oneself in the center of attention. (...) It is as if one is able to put oneself in the center of attention. (...) It is as if one were able to put oneself in the center of attention. (...) It is as if one were able to put oneself in the center. (...) It is as if one were able to put oneself in the center. (...) It is as if one were able to put oneself in the center of interest. (...) It is as if one were to put oneself in the center of interest. (...) It is as if one were to put oneself in the center. (...) It is as if one were to put oneself in the center of interest. (...)"}, {"heading": "4. Experiments", "text": "In this section we explain our experiments to evaluate the performance of our models. Section 4.1 presents two basics for comparison, then we discuss the data used for the evaluation and our experimental setup in Section 4.2 and Section 4.3 respectively."}, {"heading": "4.1. Baselines", "text": "In order to properly examine the performance of the models presented in Section 3, we are introducing the uniform base model for comparison. Similar to the models, the uniform baseline also uses segments of one second, but instead of assessing their relevance, the method selects segments according to a uniform distribution. As a result, each segment is equally likely to appear in the generated summary. Since the uniform scan covers all parts of the video equally, the summary is expected to cover all parts of the video. However, since the video does not take into account the content of the video or the query, it is expected to fail with videos that spend a disproportionate amount of time on some topics or contain cover material that has nothing to do with the query. Both are unlikely if a strong on-demand model was used or if it was a short video. In addition, we are introducing a second base model of twenty seconds, which is typically based on the first 20 seconds."}, {"heading": "4.2. Dataset", "text": "Since our proposed system uses a query and a matching video, we use YouTube to collect these pairs of query videos. As YouTube receives millions of user queries every day and has a wide variety of content, we consider it a good addition to test the effectiveness of our system. We sampled 1800 of the most frequently asked queries, and for each query, twenty matching videos were consistently sampled from the hundred search results. Subsequently, the summary system was applied to the resulting 34,725 videos, bearing in mind that some videos are associated with multiple queries. Scanning videos was limited to videos with a runtime of more than ten minutes. This ensures that the summary is not a trivial task. In addition, video query pairs that overlap with extracted units were also discarded. We decided to discard these videos to test the robustness of our system, as this limitation makes the direct match approach (section 3.2) impossible."}, {"heading": "4.3. Experimental Setup", "text": "The quality of a summary is difficult to assess objectively, so we used the Amazon Turk platform to conduct a crowdsourcing experiment with three evaluators per task. We compared models and baselines based on the crowdsourced ratings of generated summaries. However, the task of assessing a single summary proved very difficult for most people, instead we found that asking for preferences between summaries is a more understandable task. Accordingly, the task consisted of a single question: \"Someone is looking for a video about [query] which of the two following 20 second videos is best to show?\" followed by two consecutive summary trailers: one generated by one model, and another by a random order. For this reason, a judgment was collected for combining an Eachquery video pair, model, and baseline, giving us a total of 508 ratings."}, {"heading": "5. Results", "text": "In this section, we present the results of our experiment described in Section 4, provide some sample summaries, and evaluate our proposed summary method."}, {"heading": "5.1. Experimental results", "text": "The results of our crowdsourcing experiment are presented in Table 1. A clear preference of both models for the first twenty second baseline is visible. As they are statistically significant (p < 0.01), we conclude that our two models produce better summaries than this baseline. However, compared to the single baseline, the graph-based approach provides more favorable summaries compared to the semantic-only model. However, the overall preferences% for the two models are not that high compared to the single baseline. There could be several reasons for this, for example, the task is not easy for people who are unfamiliar with the video summary. In addition, the videos may not be suitable for a summary; to further investigate these assessments, they were divided into video category and length. Table 1 shows the preferences for videos in the gaming and animation category, which we compare with the gaming and animation category, are not uniform (29% of the videos) and all the others. These categories were selected because they are likely to be more prevalent on YouTube and less suitable for a summary."}, {"heading": "5.2. Example summaries", "text": "In order to study the effects of using different models, we need to consider the extent to which we are able to analyze the results of the study."}, {"heading": "6. Conclusion", "text": "We introduced a query-based video summary system that effectively combines semantic interpretations and visual signals from the video to construct summary trailers. Despite the difficulties in evaluating this complex task, we show that the new approach outperforms other baselines in terms of the quality of the summary as assessed by human evaluators. We also show several examples that demonstrate that the approach of combining embedding with frame annotations allows robust semantic recognition of relevant segments. In addition, our proposed graph-based model is able to detect portions of the video that are both relevant to the query and visually prominent in the video. Future research could expand this approach by applying the graph-based model to multiple related videos to find latent topics based on their visual similarity or to create multiple summary views per video that focus on a different topic. Finally, the use of a more na\u00efve-based summary model appears to be a promising dynamic direction for the video summary."}], "references": [{"title": "Online video summarization on compressed domain", "author": ["Almeida", "Jurandy", "Leite", "Neucimar J", "Torres", "Ricardo da S"], "venue": "Journal of Visual Communication and Image Representation,", "citeRegEx": "Almeida et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Almeida et al\\.", "year": 2013}, {"title": "Label propagation and quadratic criterion", "author": ["Bengio", "Yoshua", "Delalleau", "Olivier", "Le Roux", "Nicolas"], "venue": "Semi-Supervised Learning,", "citeRegEx": "Bengio et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2006}, {"title": "Automatic video classification: A survey of the literature. Systems, Man, and Cybernetics, Part C: Applications and Reviews", "author": ["Brezeale", "Darin", "Cook", "Diane J"], "venue": "IEEE Transactions on,", "citeRegEx": "Brezeale et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Brezeale et al\\.", "year": 2008}, {"title": "Summarisation of short-term and long-term videos using texture and colour", "author": ["Carvajal", "Johanna", "McCool", "Chris", "Sanderson", "Conrad"], "venue": "In Applications of Computer Vision (WACV),", "citeRegEx": "Carvajal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Carvajal et al\\.", "year": 2014}, {"title": "Summarization through submodularity and dispersion", "author": ["Dasgupta", "Anirban", "Kumar", "Ravi", "Sujith"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Dasgupta et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dasgupta et al\\.", "year": 2013}, {"title": "Opinosis: A graph-based approach to abstractive summarization of highly redundant opinions", "author": ["Ganesan", "Kavita", "Zhai", "ChengXiang", "Han", "Jiawei"], "venue": "In Proceedings of the 23rd International Conference on Computational Linguistics (COLING),", "citeRegEx": "Ganesan et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ganesan et al\\.", "year": 2010}, {"title": "Diverse sequential subset selection for supervised video summarization", "author": ["Gong", "Boqing", "Chao", "Wei-Lun", "Grauman", "Kristen", "Sha", "Fei"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "He et al\\.,? \\Q1916\\E", "shortCiteRegEx": "He et al\\.", "year": 1916}, {"title": "Large-scale video summarization using web-image priors", "author": ["Khosla", "Aditya", "Hamid", "Raffay", "Lin", "Chih-Jen", "Sundaresan", "Neel"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Khosla et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Khosla et al\\.", "year": 2013}, {"title": "Multi-task deep visual-semantic embedding for video thumbnail selection", "author": ["Liu", "Wu", "Mei", "Tao", "Zhang", "Yongdong", "Che", "Cherry", "Luo", "Jiebo"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Story-driven summarization for egocentric video", "author": ["Lu", "Zheng", "Grauman", "Kristen"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "Lu et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lu et al\\.", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov", "Tomas", "Sutskever", "Ilya", "Chen", "Kai", "Corrado", "Greg", "Dean", "Jeffrey"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Video summarisation: A conceptual framework and survey of the state of the art", "author": ["Money", "Arthur G", "Agius", "Harry"], "venue": "Journal of Visual Communication and Image Representation,", "citeRegEx": "Money et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Money et al\\.", "year": 2008}, {"title": "A survey of text summarization techniques", "author": ["Nenkova", "Ani", "McKeown", "Kathleen"], "venue": "Mining Text Data,", "citeRegEx": "Nenkova et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nenkova et al\\.", "year": 2012}, {"title": "Ontology reasoning scheme for constructing meaningful sports video summarisation", "author": ["Ouyang", "Jian-quan", "Liu", "Renren"], "venue": "Image Processing, IET,", "citeRegEx": "Ouyang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ouyang et al\\.", "year": 2013}, {"title": "Trecvid 2015 \u2013 an overview of the goals, tasks, data, evaluation mechanisms and metrics", "author": ["Over", "Paul", "Awad", "George", "Michel", "Martial", "Fiscus", "Jonathan", "Kraaij", "Wessel", "Smeaton", "Alan F", "Quenot", "Georges", "Ordelman", "Roeland"], "venue": "Proceedings of TRECVID", "citeRegEx": "Over et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Over et al\\.", "year": 2015}, {"title": "Category-specific video summarization", "author": ["Potapov", "Danila", "Douze", "Matthijs", "Harchaoui", "Zaid", "Schmid", "Cordelia"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "Potapov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Potapov et al\\.", "year": 2015}, {"title": "Learning temporal embeddings for complex video analysis", "author": ["Ramanathan", "Vignesh", "Tang", "Kevin", "Mori", "Greg", "FeiFei", "Li"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision, pp", "citeRegEx": "Ramanathan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ramanathan et al\\.", "year": 2015}, {"title": "Large scale distributed semi-supervised learning using streaming approximation", "author": ["Ravi", "Sujith", "Diao", "Qiming"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS),", "citeRegEx": "Ravi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ravi et al\\.", "year": 2016}, {"title": "Concept-based video retrieval", "author": ["Snoek", "Cees GM", "Worring", "Marcel"], "venue": "Foundations and Trends in Information Retrieval,", "citeRegEx": "Snoek et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2008}, {"title": "Going deeper with convolutions", "author": ["Szegedy", "Christian", "Liu", "Wei", "Jia", "Yangqing", "Sermanet", "Pierre", "Reed", "Scott", "Anguelov", "Dragomir", "Erhan", "Dumitru", "Vanhoucke", "Vincent", "Rabinovich", "Andrew"], "venue": "In CVPR 2015,", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Query-focused opinion summarization for usergenerated content", "author": ["Wang", "Lu", "Raghavan", "Hema", "Cardie", "Claire", "Castelli", "Vittorio"], "venue": "In Proceedings of the 25th International Conference on Computational Linguistics (COLING),", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Event driven web video summarization by tag localization and keyshot identification", "author": ["Wang", "Meng", "Hong", "Richang", "Li", "Guangda", "Zha", "ZhengJun", "Yan", "Shuicheng", "Chua", "Tat-Seng"], "venue": "Multimedia, IEEE Transactions on,", "citeRegEx": "Wang et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2012}, {"title": "Quasi real-time summarization for consumer videos", "author": ["Zhao", "Bin", "Xing", "Eric"], "venue": "https://www.youtube.com/yt/press/statistics.html,", "citeRegEx": "Zhao et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2008}], "referenceMentions": [{"referenceID": 15, "context": "Previous research, indeed, has dedicated a great deal of attention to video retrieval (Over et al., 2015), a task that is much harder than document retrieval due to the seman-", "startOffset": 86, "endOffset": 105}, {"referenceID": 0, "context": "the model only adds a fragment if it is visually distinct from already added fragments (Zhao & Xing, 2014; Almeida et al., 2013).", "startOffset": 87, "endOffset": 128}, {"referenceID": 3, "context": "Others cluster all the frames in the video based on their visual similarity (Carvajal et al., 2014), and subsequently compose a summary by including a single fragment from each cluster.", "startOffset": 76, "endOffset": 99}, {"referenceID": 6, "context": "Conversely, (Gong et al., 2014) propose a supervised system that learns from human created summaries.", "startOffset": 12, "endOffset": 31}, {"referenceID": 8, "context": "Moreover, if no such videos are available, the model can be trained on web images of the same category (Khosla et al., 2013).", "startOffset": 103, "endOffset": 124}, {"referenceID": 22, "context": "For instance, recognizing events summaries can better address user issued event queries (Wang et al., 2012).", "startOffset": 88, "endOffset": 107}, {"referenceID": 17, "context": "These can embody a temporal aspect as the embedding of a frame can also based on the preceding and following frames (Ramanathan et al., 2015).", "startOffset": 116, "endOffset": 141}, {"referenceID": 9, "context": "content (Liu et al., 2015).", "startOffset": 8, "endOffset": 26}, {"referenceID": 4, "context": "However, all these methods have primarily focused on summarizing text documents or user generated written content (Dasgupta et al., 2013; Wang et al., 2014).", "startOffset": 114, "endOffset": 156}, {"referenceID": 21, "context": "However, all these methods have primarily focused on summarizing text documents or user generated written content (Dasgupta et al., 2013; Wang et al., 2014).", "startOffset": 114, "endOffset": 156}, {"referenceID": 5, "context": "Graph-based methods have also been used in the past for summarization (Ganesan et al., 2010), but in a very different context.", "startOffset": 70, "endOffset": 92}, {"referenceID": 20, "context": "a deep learning architecture for concept detection in images (Szegedy et al., 2015; He et al., 2015).", "startOffset": 61, "endOffset": 100}, {"referenceID": 20, "context": "As in the previous method, we first apply the Inception model (Szegedy et al., 2015) \u2013 state-of-the-art deep neural network architecture, that is trained to detect a large number of concepts in images \u2013 on each frame Fi in the segment.", "startOffset": 62, "endOffset": 84}, {"referenceID": 11, "context": "using the recent approach from Mikolov et al. (2013), and trained on a large corpus of text documents from Wikipedia.", "startOffset": 31, "endOffset": 53}, {"referenceID": 1, "context": "The framework is typically used for semisupervised learning scenarios over graph structures (Bengio et al., 2006; Ravi & Diao, 2016; Wendt et al., 2016).", "startOffset": 92, "endOffset": 152}], "year": 2016, "abstractText": "Query-based video summarization is the task of creating a brief visual trailer, which captures the parts of the video (or a collection of videos) that are most relevant to the user-issued query. In this paper, we propose an unsupervised label propagation approach for this task. Our approach effectively captures the multimodal semantics of queries and videos using state-of-the-art deep neural networks and creates a summary that is both semantically coherent and visually attractive. We describe the theoretical framework of our graph-based approach and empirically evaluate its effectiveness in creating relevant and attractive trailers. Finally, we showcase example video trailers generated by our system.", "creator": "LaTeX with hyperref package"}}}