{"id": "1609.01885", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "DAiSEE: Towards User Engagement Recognition in the Wild", "abstract": "Extracting and understanding affective states of subjects through analysis of face images/videos is of high consequence to advance the levels of interaction in human-computer interfaces. This paper aims to highlight vision-related tasks focused on understanding \"reactions\" of subjects to presented content which has not been largely studied by the vision community in comparison to other emotions. To facilitate future study in this field, we present an effort in collecting DAiSEE, a free to use large-scale dataset using crowd annotation, that not only simulates a real world setting for e-learning environments, but also captures the interpretability issues of such affective states by human annotators. In addition to the dataset, we present benchmark results based on standard baseline methods and vote aggregation strategies, thus providing a springboard for further research.", "histories": [["v1", "Wed, 7 Sep 2016 08:50:11 GMT  (8337kb,D)", "https://arxiv.org/abs/1609.01885v1", "21 pages, 12 figures"], ["v2", "Wed, 16 Nov 2016 15:24:34 GMT  (5726kb,D)", "http://arxiv.org/abs/1609.01885v2", "9 pages, 9 figures"]], "COMMENTS": "21 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["abhay gupta", "vineeth balasubramanian"], "accepted": false, "id": "1609.01885"}, "pdf": {"name": "1609.01885.pdf", "metadata": {"source": "CRF", "title": "DAiSEE: Towards User Engagement Recognition in the Wild", "authors": ["Abhay Gupta", "Vineeth N Balasubramanian"], "emails": ["abhgup@microsoft.com", "vineethnb@iith.ac.in"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to abide by the rules they have established with regard to their country, and they will be able to leave it."}, {"heading": "2. Background and Related Work", "text": "As already mentioned, until recently, most efforts have focused on the seven essential expressions (neutrality, happiness, sadness, anger, disgust, fear, surprise) and the associated facial treatments, as in [38]. Despite recent efforts to go beyond these basic expressions, most user applications have proven to be models of effectiveness states in terms of dimensions such as valence and arousal [14] [17] [33] very limited work in terms of the perception of affective states associated with the engagement of the user, which have direct relevance to various applications as described above. Hernandez et al. [18] were the first attempts to grasp the problem of acknowledged engagement of the user. They modelled the problem of engaging a TV viewer as a binary classification task, using several geometric features for classification."}, {"heading": "3. The DAiSEE Dataset", "text": "We now present the DAiSEE dataset, which contains video recordings of 95 subjects in an e-learning environment labeled for engagement, frustration, confusion, and boredom through crowdsourcing. Typically, the dataset captures \"wild\" settings in the real world and is made publicly available along with the crowd's individual comments to facilitate open research. We will now discuss the data collection process, the subsequent annotation process, and the vote aggregation strategy for creating the dataset."}, {"heading": "3.1. Data Collection", "text": "We use a Full HD web camera (1920x1080, 30 fps, focal length 3.6 mm, field of view 78 \u0445) mounted on a computer and focused on the students watching videos on the computer screen. To simulate the e-learning environment, a tailor-made application was developed that presented a subject with 2 different videos (a total of 20 minutes in length), an educational and recreational backdrop to capture both serious and relaxed settings that allow natural variations in the user's usage levels. In order to model unrestricted settings, subjects had the opportunity to scroll through the videos. There are 95 subjects in the dataset belonging to the age group of 18-30 year olds, who are all currently enrolled students. The race of the subjects is Asian, with a male-female ratio of 37: 31. In total, 30 hours of recordings were recorded. The resulting datasets have 8000 video snippets in different locations and 2 different lighting levels."}, {"heading": "3.2. Data Annotation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 Class Labels", "text": "Motivated by recent work in intelligent tutoring systems [27], our data set consists of labels of four affective states related to user engagement, namely engagement, frustration, confusion, and boredom. Recent work [5] has shown that the six essential expressions: anger, disgust, fear, joy, sadness, and surprise [10] are unreliable in prolonged learning situations, as these basic expressions tend to change quickly. Each of the affective states is defined in four levels: (1) very low (2) low (3) high, and (4) very high (similar [37]). We pursued this class identification strategy in particular to avoid the \"neutral\" state, since early experiments have shown that crowd annotators often preferred additional female subjects to be captured in the future, and are attached to the data sets in order to select \"neutral\" as a state when they are not certain that the above-mentioned level 37 is required, as well as a lower-level reference."}, {"heading": "3.2.2 Annotation Process", "text": "Subtle affective states, such as user engagement, can be difficult to identify and vary at the discretion of the viewer. Therefore, we rely on the \"wisdom of the crowd\" to decide on our annotations in this dataset. The large amount of data and the easy availability of annotations on crowdsourcing platforms have led many other newer computer vision datasets, such as [9, 23, 34], to tap into the quantity for annotations. Although the annotations may be non-experts, it has been found that repeated labeling of examples by multiple annotators produces high-quality labels [20,29,36]. In this work, we used CrowdFlower, a popular site for crowdsourcing tasks, for which annotations are similar to [4]. CrowdFlower provides advanced quality control mechanisms, word targeting and detailed reports on the annotations achieved. Other features of these are the ones we use for CrowdFlower, the four questions we use to answer."}, {"heading": "3.3. Vote Aggregation", "text": "Once we have received the votes for all the video snippets, we use the Dawid Skene strategy [8], widely used as the gold standard for voice aggregation methods to aggregate annotator labels to obtain the only Ground Truth Label for each video. Dawid Skene is an unattended inference algorithm that provides the maximum likelihood estimate of the observer's error rates using the EM algorithm. The algorithm includes the following main steps: 1. Using the labels given by multiple annotators, we estimate the most likely \"correct\" label for each video snippet.2. Based on the estimated correct response for each object, we calculate the error rates for each annotator. 3. Taking into account the error rates for each annotator, we calculate the most likely \"correct\" label for each object. 4. Repeat the steps 2 and 3 until the error quota pre-determined number of any error criteria is met."}, {"heading": "4. Benchmark Results", "text": "The videos in DAiSEE are recorded with an image that extends up to the user's bust, allowing interested researchers to examine the relevance of non-facial cues such as upper body postures to the user's engagement (for example, a relaxed posture may indicate a higher degree of boredom, while an upright posture may indicate a high level of engagement).In this work, however, we focus on obtaining benchmark results based on the face extracted from each video image in the dataset. We extract features from the cropped faces and use standard classifiers to obtain our baseline results."}, {"heading": "4.1. Feature Extraction", "text": "We first perform face detection and alignment using the publicly available OpenFace [2] (which internally uses the DLIB2 tool. Once the faces are aligned, we crop the faces in the images of the videos. We then extract the following features from the face region of each snippet: 2DLIB: http: / / dlib.net3D HOG descriptors: We extract a dense 3D HOG [22] from the truncated video and process the features for a sack-of-words-typical representation. We apply k-means clustering to the descriptors with k = 256 clusters [35]. This results in a 256-dimensional frequency histogram of the facial features. We then normalize the histogram of the features. 3D LBP descriptors: We extract the LBP TOP [40] characteristics for each video. The characteristics for all orientations are concatenated for the representation of a video layer based on the Vace-7 method."}, {"heading": "4.2. Classifiers", "text": "After all characteristics have been determined, we use three classifiers: k-NN, SVM and Random Forests for classification. For SVM, we conducted preliminary studies with four different cores: RBF, Linear, Poly-3 and Poly-4. We found that RBF cores performed best among them. For k-NN, we varied k from 1 to 100 and found that at k = 49, we achieved the best mean performance for the various trait extraction strategies. For random forests, we used a forest size of 150 with a minimum spacing of 2, where we empirically observed the highest average accuracy."}, {"heading": "4.3. Performance Metrics", "text": "In order to ensure a fair comparison in the event of an imbalance of labels, we use average class accuracy as a benchmark of performance in this work (accuracy is measured individually for each class and its average is presented), with the average being calculated via the folds of the cross-validation."}, {"heading": "4.4. Results, Analysis and Discussion", "text": "Figure 6 shows the baseline results of our studies. Some sample results are shown in Figure 7. We analyze the performance of our benchmark results on DaiSEE, including the behavior of the dataset in terms of changes in lighting and location settings. Figure 6 shows that k-NN and random forests consistently outperform SVMs in our studies. In terms of feature extraction methods, 3D LBP performs better than 3D HOG and VGG-Face, albeit marginally. We believe this is because we use a sack-of-words representation for 3D HOG and VGG-Face that does not necessarily capture the characteristics of the video. In summary, we find that unattended learning using deep learning architectures tailored to this specific problem is an important direction of work that could help improve performance taking into account the recent successes of deeper learning processes."}, {"heading": "4.5. Confusion Matrix", "text": "To better understand the results, we present the confusion matrix for one of the result configurations. Table 1 provides us with the confusion matrix for interacting with 3DLBP as a feature extraction method and random forests as a classifier. This setting was chosen because it provided the best results for interaction, showing that the very high state (Label 4) provides the best results for class accuracy, which shows that it is easiest to generate very high interaction."}, {"heading": "4.6. A 2-class approach to understanding engagement", "text": "We examined the data set using a modified performance metric based on a two-tier approach to understanding engagement. If the predicted label is 2, but the true label is 1, then we consider that instance to be half correct while calculating accuracy. However, if the label is 3 or 4, we do not consider that instance to be correctly classified. Likewise, if the predicted label is 4, but the true label is 3, we consider that instance to be half correct in calculating accuracy. This approach takes into account the subtlety of the labels of the data set, as it is often very difficult to distinguish between high and very high engagement, and vice versa, low and very low engagement. The results of this approach are shown in Figure 8."}, {"heading": "4.7. Applicability to Real-world User Engagement", "text": "DetectionTo test the usefulness of DAiSEE for real-world user interest detection, we ran trained models on the dataset on a new stream of data from a web camera. Figure 9 shows the results of this study. In this experiment, an Intel Xeon E5 with two 8-core 1.2GHz CPUs with 64GB RAM is used, and we are able to process up to 100 frames / min in real-time video streams. Feature extraction and model validation are done with a single core, while facial recognition and alignment use both cores."}, {"heading": "5. Conclusions and Future Work", "text": "(This work introduces DAiSEE, a crowdsourced dataset that aims to provide the community with a dataset to capture user engagement in the wild. The proposed dataset has rich information including 4 different affective states, namely boredom, confusion, engagement and frustration across different location and illumination conditions and videos presented during the capture.Our future work plan includes a comprehensive study of deep learning architectures on this dataset. In addition, while we support the Dawid-Skene vote aggregation strategy, it is possible that newer vote aggregation methods can incorporate annotator statistics that may provide a consistent ground truth, and we leave this for further research. Towards this end, we offer annotator statistics from CrowdFlower along with the DAiSEE dataset.References [1] Affdex. http: / / www.affectiva.com / solutions / affectivdex /, [Online; accessed 13-March2016]."}], "references": [{"title": "Openface: an open source facial behavior analysis toolkit", "author": ["T. Baltru", "P. Robinson", "L.-P. Morency"], "venue": "In 2016  IEEE Winter Conference on Applications of Computer Vision (WACV),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Design a personalized elearning system based on item response theory and artificial neural network approach", "author": ["A. Baylari", "G.A. Montazer"], "venue": "Expert Systems with Applications,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2009}, {"title": "Crowdsourcing scientific progress: how crowdflower\u2019s hordes help harvard researchers study", "author": ["A. Burke"], "venue": "tb. Forbes. October,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2011}, {"title": "Affect detection: An interdisciplinary review of models, methods, and their applications", "author": ["R.A. Calvo", "S. D\u2019Mello"], "venue": "Affective Computing, IEEE Transactions on,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2010}, {"title": "Applying data mining techniques to e-learning problems", "author": ["F. Castro", "A. Vellido", "\u00c0. Nebot", "F. Mugica"], "venue": "In Evolution of teaching and learning paradigms in intelligent environment,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Moocs interrupted. http://www. openculture.com/2013/04/10_reasons_you_ didnt_complete_a_mooc.html, [Online: accessed 11-March-2016", "author": ["O. Culture"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Maximum likelihood estimation of observer error-rates using the em algorithm", "author": ["A.P. Dawid", "A.M. Skene"], "venue": "Applied statistics,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1979}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei- Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Universal and cultural differences in facial expression of emotion", "author": ["P. Eckman"], "venue": "In Nebraska symposium on motivation,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1972}, {"title": "The pascal visual object classes (voc) chal-  Figure 8. Average 2-class accuracy (classwise) for engagement for all classifiers, k-NN, SVM and Random Forests and 3 feature extraction methods, namely, 3D-HOG, 3D-LBP and VGG Figure 9. Labels predicted by using 3D-LBP with Random Forests classifier. E=Engagement; F=Frustration; C=Confusion; B=Boredom lenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "International journal of computer vision,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "Emotion representation, analysis and synthesis in continuous space: A survey", "author": ["H. Gunes", "B. Schuller", "M. Pantic", "R. Cowie"], "venue": "In Automatic Face & Gesture Recognition and Workshops (FG", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Demographic estimation from face images: Human vs. machine performance", "author": ["H. Han", "C. Otto", "X. Liu", "A.K. Jain"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks", "author": ["L. He", "D. Jiang", "L. Yang", "E. Pei", "P. Wu", "H. Sahli"], "venue": "In Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Measuring the engagement level of tv viewers", "author": ["J. Hernandez", "Z. Liu", "G. Hulten", "D. DeBarr", "K. Krum", "Z. Zhang"], "venue": "In Automatic Face and Gesture Recognition (FG),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Constructing a personalized e-learning system based on genetic algorithm and case-based reasoning approach", "author": ["M.-J. Huang", "H.-S. Huang", "M.-Y. Chen"], "venue": "Expert Systems with Applications,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2007}, {"title": "Repeated labeling using multiple noisy labelers", "author": ["P.G. Ipeirotis", "F. Provost", "V.S. Sheng", "J. Wang"], "venue": "Data Mining 4328  and Knowledge Discovery,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Initial trends in enrolment and completion of massive open online courses", "author": ["K. Jordan"], "venue": "The International Review of Research in Open and Distributed Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "A spatio-temporal descriptor based on 3d-gradients", "author": ["A. Klaser", "M. Marsza\u0142ek", "C. Schmid"], "venue": "In BMVC 2008-19th British Machine Vision Conference,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2008}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In Computer Vision\u2013ECCV", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression", "author": ["P. Lucey", "J.F. Cohn", "T. Kanade", "J. Saragih", "Z. Ambadar", "I. Matthews"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2010}, {"title": "Deep face recognition", "author": ["O.M. Parkhi", "A. Vedaldi", "A. Zisserman"], "venue": "Proceedings of the British Machine Vision,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Eye-tracking adaptable e-learning and content authoring support", "author": ["M. Pivec", "C. Trummer", "J. Pripfl"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Enriching the Student Model in an Intelligent Tutoring System", "author": ["R. Rajendran"], "venue": "PhD thesis, The IITB-Monash Research Academy,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Decoding children\u2019s social behavior", "author": ["J. Rehg", "G. Abowd", "A. Rozga", "M. Romero", "M. Clements", "S. Sclaroff", "I. Essa", "O. Ousley", "Y. Li", "C. Kim"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2013}, {"title": "Get another label? improving data quality and data mining using multiple, noisy labelers", "author": ["V.S. Sheng", "F. Provost", "P.G. Ipeirotis"], "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2008}, {"title": "Affective computing: A review", "author": ["J. Tao", "T. Tan"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Recognizing action units for facial expression analysis", "author": ["Y.-l. Tian", "T. Kanade", "J.F. Cohn"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2001}, {"title": "Avec 2014: 3d dimensional affect and depression recognition challenge", "author": ["M. Valstar", "B. Schuller", "K. Smith", "T. Almaev", "F. Eyben", "J. Krajewski", "R. Cowie", "M. Pantic"], "venue": "In Proceedings of the 4th International Workshop on Audio/Visual Emotion Challenge,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection", "author": ["G. Van Horn", "S. Branson", "R. Farrell", "S. Haber", "J. Barry", "P. Ipeirotis", "P. Perona", "S. Belongie"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Vlfeat: An open and portable library of computer vision algorithms", "author": ["A. Vedaldi", "B. Fulkerson"], "venue": "In Proceedings of the 18th ACM international conference on Multimedia,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2010}, {"title": "Online crowdsourcing: rating annotators and obtaining cost-effective labels", "author": ["P. Welinder", "P. Perona"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2010}, {"title": "The faces of engagement: Automatic recognition of student engagementfrom facial expressions", "author": ["J. Whitehill", "Z. Serpell", "Y.-C. Lin", "A. Foster", "J.R. Movellan"], "venue": "Affective Computing, IEEE Transactions on,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Boosting coded dynamic features for facial action units and facial expression recognition", "author": ["P. Yang", "Q. Liu", "D.N. Metaxas"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2007}, {"title": "A survey of affect recognition methods: Audio, visual, and spontaneous expressions", "author": ["Z. Zeng", "M. Pantic", "G.I. Roisman", "T.S. Huang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Dynamic texture recognition using local binary patterns with an application to facial expressions", "author": ["G. Zhao", "M. Pietikainen"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2007}, {"title": "Learning from the wisdom of crowds by minimax entropy", "author": ["D. Zhou", "S. Basu", "Y. Mao", "J.C. Platt"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "The ImageNet [9] and PASCAL VOC [13] challenges spearheaded the object recognition revolution, while the recent availability of datasets such as Microsoft COCO [23] is fueling the development of newer methods such as in vision-to-language and language-to-vision efforts.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "The ImageNet [9] and PASCAL VOC [13] challenges spearheaded the object recognition revolution, while the recent availability of datasets such as Microsoft COCO [23] is fueling the development of newer methods such as in vision-to-language and language-to-vision efforts.", "startOffset": 32, "endOffset": 36}, {"referenceID": 18, "context": "The ImageNet [9] and PASCAL VOC [13] challenges spearheaded the object recognition revolution, while the recent availability of datasets such as Microsoft COCO [23] is fueling the development of newer methods such as in vision-to-language and language-to-vision efforts.", "startOffset": 160, "endOffset": 164}, {"referenceID": 31, "context": "While there have been recent research efforts to develop methods for recognition of user engagement [37] [18], their datasets are not publicly available.", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "While there have been recent research efforts to develop methods for recognition of user engagement [37] [18], their datasets are not publicly available.", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "Understanding affective (emotional) states of a user, an important subarea of computer vision, has for a long time focused on datasets pertaining to the seven basic expressions: neutral, happiness, sadness, anger, disgust, surprise and contempt [24].", "startOffset": 245, "endOffset": 249}, {"referenceID": 27, "context": "While recent efforts in the last few years have expanded datasets in this domain to cover emotions in terms of a dimensional representation [33] [28], the vast subtleties in emotions and affective states necessitate the development of datasets for specific objectives.", "startOffset": 140, "endOffset": 144}, {"referenceID": 23, "context": "While recent efforts in the last few years have expanded datasets in this domain to cover emotions in terms of a dimensional representation [33] [28], the vast subtleties in emotions and affective states necessitate the development of datasets for specific objectives.", "startOffset": 145, "endOffset": 149}, {"referenceID": 27, "context": "Recent trends, including [33] [28], corroborate this approach to help progress towards tangible outcomes.", "startOffset": 25, "endOffset": 29}, {"referenceID": 23, "context": "Recent trends, including [33] [28], corroborate this approach to help progress towards tangible outcomes.", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "Surprisingly, MOOCs have a dropout rate of 91-93% [21], with the completion rate for the first assignment being around 45%.", "startOffset": 50, "endOffset": 54}, {"referenceID": 5, "context": "An online survey [7] lists the top ten reasons for dropouts from such platforms; poor course design was one of the reasons, which included components such as lack of proper student feedback, \u201clecture fatigue\u201d in courses that had only video lectures, lack of proper course introductions and student frustration.", "startOffset": 17, "endOffset": 20}, {"referenceID": 33, "context": "Determining the affective state of a user using computer vision and machine learning methods has been studied for over two decades now (please see [39] [31] for surveys).", "startOffset": 147, "endOffset": 151}, {"referenceID": 25, "context": "Determining the affective state of a user using computer vision and machine learning methods has been studied for over two decades now (please see [39] [31] for surveys).", "startOffset": 152, "endOffset": 156}, {"referenceID": 32, "context": "As mentioned earlier, until recently, most efforts focused on the seven essential expressions (neutral, happiness, sadness, anger, disgust, fear, surprise) and the facial action units connected with them, as in [38] [32].", "startOffset": 211, "endOffset": 215}, {"referenceID": 26, "context": "As mentioned earlier, until recently, most efforts focused on the seven essential expressions (neutral, happiness, sadness, anger, disgust, fear, surprise) and the facial action units connected with them, as in [38] [32].", "startOffset": 216, "endOffset": 220}, {"referenceID": 10, "context": "fective states in terms of dimensions such as valence and arousal [14] [17] [33] [28], very limited work has been carried out in perceiving affective states related to user engagement, which has direct relevance on various applications as described before.", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "fective states in terms of dimensions such as valence and arousal [14] [17] [33] [28], very limited work has been carried out in perceiving affective states related to user engagement, which has direct relevance on various applications as described before.", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "fective states in terms of dimensions such as valence and arousal [14] [17] [33] [28], very limited work has been carried out in perceiving affective states related to user engagement, which has direct relevance on various applications as described before.", "startOffset": 76, "endOffset": 80}, {"referenceID": 23, "context": "fective states in terms of dimensions such as valence and arousal [14] [17] [33] [28], very limited work has been carried out in perceiving affective states related to user engagement, which has direct relevance on various applications as described before.", "startOffset": 81, "endOffset": 85}, {"referenceID": 13, "context": "[18] were the first to attempt the problem of recognized user engagement.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[37] attempted to automatically understand engagement in learning environments.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "While machine learning methods have been used to personalize educational modules, diversify assessment methods and make personalized recommendations based on learner preferences and browsing patterns [6] [19] [3] [26], limited efforts have been attempted in understanding user engagement in the e-learning environment.", "startOffset": 200, "endOffset": 203}, {"referenceID": 14, "context": "While machine learning methods have been used to personalize educational modules, diversify assessment methods and make personalized recommendations based on learner preferences and browsing patterns [6] [19] [3] [26], limited efforts have been attempted in understanding user engagement in the e-learning environment.", "startOffset": 204, "endOffset": 208}, {"referenceID": 1, "context": "While machine learning methods have been used to personalize educational modules, diversify assessment methods and make personalized recommendations based on learner preferences and browsing patterns [6] [19] [3] [26], limited efforts have been attempted in understanding user engagement in the e-learning environment.", "startOffset": 209, "endOffset": 212}, {"referenceID": 21, "context": "While machine learning methods have been used to personalize educational modules, diversify assessment methods and make personalized recommendations based on learner preferences and browsing patterns [6] [19] [3] [26], limited efforts have been attempted in understanding user engagement in the e-learning environment.", "startOffset": 213, "endOffset": 217}, {"referenceID": 22, "context": "Motivated by recent work in intelligent tutoring systems [27], our dataset consists of labelings of four affective states related to user engagement, viz.", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "Recent work [5] has shown that the six essential expressions: anger, disgust, fear, joy, sadness and surprise [10] are not reliable in prolonged learning situations, since these basic expressions are prone to rapid changes.", "startOffset": 12, "endOffset": 15}, {"referenceID": 8, "context": "Recent work [5] has shown that the six essential expressions: anger, disgust, fear, joy, sadness and surprise [10] are not reliable in prolonged learning situations, since these basic expressions are prone to rapid changes.", "startOffset": 110, "endOffset": 114}, {"referenceID": 31, "context": "Each of the affective states is defined in four levels: (1) very low (2) low (3) high and (4) very high (similar to [37]).", "startOffset": 116, "endOffset": 120}, {"referenceID": 31, "context": "We also note that as in [37], the above 4-level annotation can be trivially changed to 2levels (high and low) as may be required for a given application setting.", "startOffset": 24, "endOffset": 28}, {"referenceID": 7, "context": "The large amounts of data and easy availability of annotators on crowdsourcing platforms has resulted in many other newer computer vision datasets such as [9, 23, 34] to tap into the crowd for annotations.", "startOffset": 155, "endOffset": 166}, {"referenceID": 18, "context": "The large amounts of data and easy availability of annotators on crowdsourcing platforms has resulted in many other newer computer vision datasets such as [9, 23, 34] to tap into the crowd for annotations.", "startOffset": 155, "endOffset": 166}, {"referenceID": 28, "context": "The large amounts of data and easy availability of annotators on crowdsourcing platforms has resulted in many other newer computer vision datasets such as [9, 23, 34] to tap into the crowd for annotations.", "startOffset": 155, "endOffset": 166}, {"referenceID": 15, "context": "Although the annotators can be non-experts, it has been shown that repeated labeling of examples by multiple annotators produces high-quality labels [20,29,36].", "startOffset": 149, "endOffset": 159}, {"referenceID": 24, "context": "Although the annotators can be non-experts, it has been shown that repeated labeling of examples by multiple annotators produces high-quality labels [20,29,36].", "startOffset": 149, "endOffset": 159}, {"referenceID": 30, "context": "Although the annotators can be non-experts, it has been shown that repeated labeling of examples by multiple annotators produces high-quality labels [20,29,36].", "startOffset": 149, "endOffset": 159}, {"referenceID": 2, "context": "In this work, we used CrowdFlower, a popular site for crowdsourced tasks, for the annotations, similar to [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 35, "context": "For each video snippet, we get votes from 10 different annotators (which is comparable to other standard crowdsourced datasets such as [41] [15]).", "startOffset": 135, "endOffset": 139}, {"referenceID": 11, "context": "For each video snippet, we get votes from 10 different annotators (which is comparable to other standard crowdsourced datasets such as [41] [15]).", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "After obtaining the votes for all video snippets, we use the Dawid-Skene [8] strategy, which is widely used as a gold standard for vote aggregation methods, to aggregate annotator labels to obtain the single ground truth label for each video.", "startOffset": 73, "endOffset": 76}, {"referenceID": 0, "context": "We first perform face detection and alignment using the publicly available OpenFace [2] (which internally uses DLIB2 tool.", "startOffset": 84, "endOffset": 87}, {"referenceID": 17, "context": "3D HOG Descriptors: We extract dense 3D HOG [22] from the cropped video and then process the features for a bag-of-words type representation.", "startOffset": 44, "endOffset": 48}, {"referenceID": 29, "context": "We apply k-means clustering on the descriptors with k = 256 clusters [35].", "startOffset": 69, "endOffset": 73}, {"referenceID": 34, "context": "3D LBP Descriptors: We extract the LBP-TOP [40] features for every video.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "Deep Face Descriptors: We use VGG-Face [25], a Convolutional Neural Network-based feature extraction method, for these descriptors.", "startOffset": 39, "endOffset": 43}, {"referenceID": 20, "context": "We extract the features from the fc7 layer to get the most generic features as the fc8 representations are tailored for the dataset proposed in [25].", "startOffset": 144, "endOffset": 148}], "year": 2016, "abstractText": "Recognizing user engagement plays an important role in several contemporary vision applications including advertising, healthcare, and e-learning. However, the lack of any publicly available dataset to address this problem severely limits the development of methodologies that can help make progress to address this challenge. In this paper, we introduce DAiSEE, a free-to-use large dataset comprising of 8000 video snippets captured from 95 users for recognizing user engagement in the wild. Baseline results using standard feature extraction and classification methods show the difficulty of working with this dataset. We believe that DAiSEE would provide the research community with challenges in feature extraction, context-based inference as well as the development of suitable machine learning methods that can consider annotator statistics in the training process itself, thus providing a springboard for further research.", "creator": "LaTeX with hyperref package"}}}