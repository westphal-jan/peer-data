{"id": "1609.01926", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "A modular architecture for transparent computation in Recurrent Neural Networks", "abstract": "Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Goedelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.", "histories": [["v1", "Wed, 7 Sep 2016 10:44:28 GMT  (879kb,AD)", "http://arxiv.org/abs/1609.01926v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.CL cs.FL cs.SC", "authors": ["giovanni sirio carmantini", "peter beim graben", "mathieu desroches", "serafim rodrigues"], "accepted": false, "id": "1609.01926"}, "pdf": {"name": "1609.01926.pdf", "metadata": {"source": "CRF", "title": "A modular architecture for transparent computation in Recurrent Neural Networks", "authors": ["Giovanni S. Carmantinia", "Peter beim Graben", "Mathieu Desroches", "Serafim Rodrigues"], "emails": ["giovanni.carmantini@gmail.com"], "sections": [{"heading": null, "text": "Computing is traditionally studied in terms of automata, formal languages and algorithms; however, the relationship between neural dynamics and symbolic representations and operations in traditional eliminative connectionism is still unclear. Therefore, we propose a unique perspective on this central issue, which we would like to refer to as transparent networking, by proposing representations of how symbolic computation can be implemented in neural substrates.In this study, we first present a new model of dynamics in a symbolic space, the multi-faceted shift, which shows that it supports the real-time simulation of a number of automates.We then show that the delization of multi-layered shifts defines nonlinear dynamic automates.Finally, we present an imaging between nonlinear dynamic automatesand recursive artificial neural networks."}, {"heading": "1. Introduction", "text": "In fact, the fact is that most of them will be able to put themselves in a situation where they are able to plunge themselves into a crisis, in which they are able, in which they are able, in which they are able to stabilise."}, {"heading": "2. Methods", "text": "This section outlines our general method of mapping a series of mathematical models on R-ANNs. In Figure 1, we summarize the complete mapping process that accompanies its representation. Our construction is a two-step process. We first define a multi-layered shift (a generalization of the shift map introduced in Moore, 1990) that mimics a mathematical model, and then encode its dynamics on the unit square using Go delization, which gives us a two-dimensional affin-linear map on the unit square, i.e. an NDA. In a second step, the NDA is mapped to a first-order R-ANN equipped with an architecture that captures the three key components of the NDA: i) a state that encodes the symbolic data of the model of calculation, ii) a set of affin theories that encode its operations on data."}, {"heading": "2.1. Elements of Symbolic Computation", "text": "A symbol is supposed to be a distinguishing element of a finite set of A, which we call the alphabet. Symbols can be linked together, i.e. for a, b, A, ab \u2261 (a, b) and A2. A sequence of symbols w, an is designated as a word of length n, which is designated n = | w |. The set of words of all possible lengths w of finite length | w | \u2265 0 is designated as A * (for | w | = 0, w = denotes the \"empty word\")."}, {"heading": "2.1.1. From Generalized to Versatile Shifts", "text": "The Theory of Symbolic Dynamics (Lind and Marcus, 1995) is a tool for studying dynamic systems based on discrediting time and space to interpret the motion sequences in a vector space as discrete sequences of infinite strings. Importantly, its theoretical apparatus can also be used to shift the dotted sequence of strings into a vector space. We begin with the redefinition of a representation of strings, the dotted sequence. According to Moore (1990, 1991), a dotted sequence of strings in an alphabet is a two-sided sequence of symbols. \"d \u2212 2 d0 d1 d2.\" This manuscript version is provided under the CC-BY-ND 4.0 license."}, {"heading": "2.1.2. Simulation of Various Automata by Versatile Shifts", "text": "It is now discussed how a series of automata can be defined as VSs in real time by defining the corresponding sequence representations of machine configurations and configurations. (Thus, only an empty transition phase is possible.) (There is only a limited transition period available from the CC-BY-NC-NC 4.0 license. (Formal publication was introduced by McCulloch and Pitts in 1943 and is widely used to describe systems in many fields of application ranging from computer science to biology, to name but a few.) At each step of computing an FSM in one of the finite set of states, and it can change its state as a result of incoming input."}, {"heading": "2.2. Introducing Nonlinear Dynamical Automata", "text": "We will now discuss how VSs, and thus the models of symbolic computation that they can simulate, can gradually be mapped on affine-linear systems on a vector space, resulting in nonlinear dynamic automata."}, {"heading": "2.2.1. Go\u0308del Encodings and the Symbol Plane", "text": "A Go-del encoding (or Go-delization, see Go-del, 1931) allows a sequence to be uniquely assigned a real number, so that the space of unilateral infinite sequences to the real interval [0, 1].4 For completeness, Go-delization is discussed alongside its graphic representation, which in Figure 3.Let AN be the space of unilateral infinite sequences via an alphabet A containing | A | = g symbols and s = d1d2... A sequence in this space, where dk is the k-th symbol in s. Let us also say that A \u2192 N can be a one-to-one function that associates each symbol in alphabet A with a natural number. Then, a Go-delization is an illustration from AN to [0, 1] a sequence of B symbols in s."}, {"heading": "2.2.2. Versatile Shifts as Affine-Linear Transformations", "text": "The push operator is defined in such a way that it appends the contents of a word \"A\" to the beginning of \"A,\" while the push operator is defined in such a way that it removes the first p symbols in \"s. We will now show that a sequence resulting from the application of pop and pop operations is equivalent to the application of an affin-linear transformation to the original printed sequence. We will then show that VSs can be mapped to a dotted sequence in\" s. \"\u03b2\" in order to shift and shift the unilateral operations to their unilateral constituencies. Let s = d2d3. be a unilateral infinite sequence on an alphabetical sequence A. Applying a popped operation p = dp + 1dp."}, {"heading": "2.2.3. Nonlinear Dynamical Automata", "text": "A nonlinear automaton (NDA) is a triple MNDA = (X, \u03b2, \u03a6), where P is a rectangular partition of the unit square X = [0, 1] 2 that isP = {Di, j, 1 \u2264 i \u2264 m, 1 \u2264 j, 1 \u2264 j, n, 2, (20), so that each cell is called Di, j = Ii, Jj, j = X. The pair (X, 1] is a temporally discrete dynamic system with phase space X and the flow space X."}, {"heading": "2.3. Solution Map between NDA and R-ANNs", "text": "The design of the map between the NDA and a first order R-ANN follows a conceptually natural and simple solution that attempts to mimic the affinelinear dynamics (given by Equation 21) of the NDA on the square of the subdivided unit (see Carmantini et al., 2015 for preliminary work in this direction).Let \u03c1 (\u00b7) denote the proposed map. The aim is to map the orbits of the NDA (i.e. \u03a6i, j (x, y)) on the orbits of the R-ANN, which are referred to as \u0432 i, j (x, y).The role of the map is to encode both affin-linear dynamics within each partition zellc \u00a9 2016.001. this manuscript version is provided under license CC-BY-NC-ND 4.0. Formal publication available at http ps: / dx.doi.org / 10.1016 / j.neunet.2016.001. (Tue, j) and to activate the emulation of the cell architecture we are not activated within this cell architecture."}, {"heading": "2.3.1. Network Architecture and Neural Dynamics", "text": "The simulation of an NDA orbit within the R-ANNs is divided into the MCL, BSL and LTL. Since \u03a6i, j (x) is a two-dimensional decoupled discrete card, it proposes only two neural units in a read-out layer, which is a role of the MCL. We refer to the two MCL units as cx and cy. At each c \u00a9 2016. This manuscript version is provided under the CC-BY-NC-ND 4.0 license. Formal publication available at https: / / dx.doi.org / 10.1016 / j.neunet.2016.001.Compilation step, the MCL stores the coding of the current machine configuration, which is then passed on to the BSL and LTL units. Subsequently, two sets of BSL units (bx and by) function as a switching system, determining in which cell, the current machine configuration, the TL and the respective application ELL are transferred."}, {"heading": "2.3.2. Machine Configuration Layer", "text": "The MCL encodes the state of the simulated NDA and thus the data of the simulated automaton (\u03b2 = \u03b2) and acts simultaneously as a read neural layer. Atc \u00a9 2016. This manuscript version is available under the CC-BY-NC-ND 4.0 license. Formal release available at https: / / dx.doi.org / 10.1016 / j.neunet.2016.001.simultaneously mediates the transmission of the current Go \ufffd del encoding of the emulated machine configuration to the BSL and LTL units at each calculation step. Since the Go \ufffd del encoding of a dotted sequence representing a machine configuration is performed from two values (see section 2.2.1), this implies that the MCL needs only two neural units (cx and cy) to encode the current configuration (cx x x x x x x x) x (x x x x x x x x x x), as a consequence the initialization of the R-ANNs in this layer is performed, where x (x) x (x x x x x x x x x x x x) is equal (x x x x x x x x x x x x x)."}, {"heading": "2.3.3. Branch Selection Layer", "text": "The BSL acts as a control unit that allows the sequential mapping of the orbits of the NDA unit. (http: / / www.BSL unit) The BSL unit (http: / / www.BSL unit) would convert the switching rules (x, y) and the dynamic switching between the LTL units (x, y) into x units in each case. The BSL units ensure that (ti, jx, j y) become active only when (cx, cy), j y) the switching rules i, j and j units become active, which then operate on a coded machine configuration. (Specifically, the BSL units ensure that (ti, jx, t, j y) become active when (cx, cy).Di, j = Ii \u00d7 Yy, with Ii units, where i + 1) is the i-th unit on the x axis and Yy."}, {"heading": "2.3.4. Linear Transformation Layer", "text": "The LTL embodies the set of affin-linear transformations of the NDA from which the network is built, and thus the set of symbolic operations defined by the transition table of the simulated automaton, which gives the LTL the functional ability to generate an updated encoded machine configuration from the current one, i.e., the affin-linear transformation of an NDA, \u03a6i, j (x, y) = (\u03bbi, jx + a i, j x, \u03bb i, j y, y y y) within a cell Di, j is simulated by the LTL unit (ti, jx, t i, j y y), inducing the following figure: (ti, jx, t i, j y y) = i, j 3 (x, y) = oractive units (\u00b7, implemented units of the LTL unit, \u00b7, y x)."}, {"heading": "2.4. Neuronal Observation Models", "text": "In order to compare connectionist simulation results with experimental evidence from neurophysiology or psychology, it is necessary to map the high-dimensional neuronal activation space (PCA) into a much lower-dimensional observation space spanned by p-N observables. A standard method for such a projection is PCA (Elman, 1991). If PCA is limited to the first main axis, the resulting scalar variable could be conceived as a measure of total activity in the neural network (as in Graben et al., 2008). Other important scalar observation points discussed in the literature are Smolensky's Harmony (Smolensky, 1986) H = ij uiwijujwith u = (ui) as the network activation vector and W = (wij) its synaptic weight matrix or Amari's mean network activity (Amari, 1974)."}, {"heading": "3. Results", "text": "The implementation of the R-ANN discussed in the previous sections simulates an NDA in real time and thus simulates its associated machine in real time. Formally, it can be shown that the NDA simulation (and thus the machine simulation) is achieved by a combination of synaptic and neural computation between three neural types (MCL, BSL and LTL) and with a total number of neural units that are equal to the sound units (and thus the machine simulation) of the R-ANN (33), where the number of subsequences that can appear in the left and right domains of VS dependence and the R-ANN are constructed, which is a total of 2 MCL units."}, {"heading": "3.1. Example 1: Finite-State Locomotive Pattern Generator", "text": "In the USA, in the EU, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in Europe, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA, in the USA"}, {"heading": "3.2. Example 2: Interactive Automata Networks", "text": "This year, it has reached the point where it will be able to put itself at the top of the list."}, {"heading": "4. Discussion and Outlook", "text": "In this study, we developed a constructive, transparent and parsimonious mapping of symbolic algorithms to neural networks. We first introduced a novel displacement map that extends generalized displacement and allows real-time simulation of a range of symbolic models of computation. We then demonstrated how VSs can be represented in a vectorial space by go-delization, which expands the generalized and linear systemsc. This manuscript version is provided under CC-BY-NC 4.0 license. Formal release available at https: / dx.doi.org / 10.1016 / j.neunet.2016.09.001.o This manuscript version is available under the CC-BY-NC-ND 4.0 license. Formal release at https: / / dx.doi.org / 10.1016 / jneunet.2016.09.001.o"}, {"heading": "Acknowledgements", "text": "This research is supported by a Heisenberg Fellowship (GR 3711 / 1- 2) from the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) to the PbG."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as to transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the G\u00f6delization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: i) the \u2217Corresponding author Email address: giovanni.carmantini@gmail.com (Giovanni S. Carmantini) Preprint submitted to Neural Networks September 8, 2016 ar X iv :1 60 9. 01 92 6v 1 [ cs .N E ] 7 S ep 2 01 6 design of a Central Pattern Generator from a finite-state locomotive controller, and ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments.", "creator": "LaTeX with hyperref package"}}}