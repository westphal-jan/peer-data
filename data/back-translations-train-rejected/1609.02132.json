{"id": "1609.02132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "UberNet: Training a `Universal' Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory", "abstract": "In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a `swiss knife' for vision tasks; we call this architecture an UberNet to indicate its overarching nature.", "histories": [["v1", "Wed, 7 Sep 2016 19:35:30 GMT  (7244kb,D)", "http://arxiv.org/abs/1609.02132v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["iasonas kokkinos"], "accepted": false, "id": "1609.02132"}, "pdf": {"name": "1609.02132.pdf", "metadata": {"source": "CRF", "title": "UberNet : Training a \u2018Universal\u2019 Convolutional Neural Network for Low-, Mid-, and High-Level Vision using Diverse Datasets and Limited Memory", "authors": ["Iasonas Kokkinos"], "emails": ["iasonas.kokkinos@ecp.fr"], "sections": [{"heading": null, "text": "We address two technical challenges that arise when we expand the range of tasks accomplished by a single CNN: (i) the formation of a deep architecture, taking into account various training sets, and (ii) the training of many (potentially unlimited) tasks with a limited storage budget. The right solution to these two problems allows us to train accurate predictors for a variety of tasks without compromising accuracy. Through these advancements, we train a CNN in an end-to-end manner that simultaneously addresses (a) boundary recognition (b) normal estimation (c) excellent estimation (d) semantic segmentation (e) human subsegmentation (e) semantic boundary detection (f) semantic boundary detection, (g) generation of regional suggestions and object detection. We achieve competitive performance while accomplishing all of these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this system can be found at cvecp.fr / /."}, {"heading": "1. Introduction", "text": "There are a variety of tasks involved, such as boundary capture, semantic segmentation, surface estimation, object classification, image classification, to name but a few. While Convolutionary Neural Networks (CNNs) have been the method of choice for text recognition for more than two decades, they have only recently demonstrated success in handling most, if not all, tasks relating to a single static image, we can list indicative successes of CNNs in superresolution [24], colorization, boundary detection [6, 28, 49, 107], symmetry detection, which relates to a single static image. [91], the descriptors of interest in image description, 110], Surfacear Xiv: 160 9.02 132v 1 [cs.C V] 7S ep2 01normal estimate [2, 25, 103], depth estimation."}, {"heading": "2. UberNet architecture", "text": "In fact, most of them are able to survive themselves if they don't put themselves in a position to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "3. Multi-Task Training using Diverse Datasets", "text": "Having described the loss of tasks for which we have no basic truth, we are now able to move in a way that covers both the VGG-based and the VGG-based knowledge in order to capture the variety of available data that we need during the training. [20, 77] As described in the introduction, the biggest challenge we face is the variety of tasks we want to cover. [20, 78] However, this may not be possible for arbitrary tasks, e.g. if we adjust the loss function to the information we have in a sample."}, {"heading": "4. Memory-Bound Multi-Task Training", "text": "To address these issues, we are building on the recent advances in storage efficiency that characterized the presentation of our extension to Multi-Task Learning. The basic implementation of the back-propagation algorithm maintains all the interlayer activations calculated during the forward pass. As shown in Figure 3, each layer then combines its stored gradients with the backward propagated gradients coming from the layer (s) above, the gradients for their own parameters, and then the gradients on the layer (s) below. While this strategy is achieved by reusing the activation signals, it is challenging as it is popular."}, {"heading": "5. Experiments", "text": "Our experimental evaluation has two objectives: firstly, to show that the generic UberNet architecture introduced in paragraph 2 successfully accomplishes a wide range of tasks. To investigate this, we primarily compare results obtained through methods based on the VGG network [93] - recent work on detection [22] and semantic segmentation [14], for example, has shown improvements through the use of deeper ResNets, but we consider the choice of the network to be in a sense orthogonal to the goal of this section. Secondly, to investigate how the inclusion of more tasks affects performance in the individual tasks. To eliminate erroneous sources of variation, we use a common initialization for all single and multi-task networks, which is achieved by pre-training a network for common semantic segmentation and object identification, as described in detail in paragraph 5,1."}, {"heading": "5.1. Experimental settings", "text": "In fact, it is a very successful campaign to put people's interests at the heart of society."}, {"heading": "5.2. Experimental Evaluation", "text": "This year, it has come to the point where it only takes one year to get to the next round."}, {"heading": "5.3. Effect of task weights", "text": "The performance of our network in terms of the multitude of tasks it addresses depends on the weights associated with the losses of various tasks in Eq.3, while others are neglected. If the weight of a task is much greater, it can be expected that this distorts the internal representation of the network in favor of the task in question, while neglecting others. Motivated by the empirical results in the previous paragraphs, we have investigated the effects of changing the weight attributed to the normal estimate task in Eq.3, in the case of solving multiple rather than individual tasks. In Table 12, we report how performance changes when we increase the weight of the normal estimate task (previous experiments relied on the \"Q = 1\" option). We realize that, at least for our specific experimental settings, there is \"no free lunch,\" and the performance measures of the various tasks act like communicating vessels. The evaluation may be affected by our optimization decisions, e.g. by larger batch sizes that may contribute more or better to a polymer 14, which may contribute to a polymer and a polymer."}, {"heading": "6. Conclusions and Future Work", "text": "In this work, we have introduced two techniques that allow us to train a CNN that tackles a wide range of computer vision problems in a unified architecture. We have shown that one can effectively scale to many and diverse tasks, since memory complexity is independent of the number of tasks and incoherently commented data sets can be combined during training. There are certain simple directions for future work: (i) take into account other tasks such as symmetry, human landmarks, texture segmentation, or any of the tasks stated in the introduction (ii) use of deeper architectures, such as ResNets [40] (iii) combining dense labeling results with structured predictions [11, 14, 61, 113]. Research in these directions is ongoing, but, more importantly, we consider this work as a first step toward tackling multiple tasks together by taking advantage of this synergy between them 99 99 - this was a recurring theme we have in this network vision, for example, for the 53, and that computer vision is fully available for this one."}, {"heading": "7. Acknowledgements", "text": "This work was supported by the EU projects FP7-RECONFIG, FP7-MOBOT and H2020-ISUPPORT donated by NVIDIA. I thank George Papandreou for showing how to implement low-memory backpropagation, Pierre-Andre \"Savalle for showing me how to handle prototype files, Ross Girshick for making the FasterRCNN system publicly available, and Nikos Paragios for creating the environment in which this work took place."}], "references": [{"title": "Contour detection and hierarchical image segmentation", "author": ["P. Arbelaez", "M. Maire", "C. Fowlkes", "J. Malik"], "venue": "PAMI,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Marr revisited: 2d-3d alignment via surface normal prediction", "author": ["A. Bansal", "B. Russell", "A. Gupta"], "venue": "Proc. CVPR,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent human pose estimation", "author": ["V. Belagiannis", "A. Zisserman"], "venue": "CoRR, abs/1605.02914,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Material recognition in the wild with the materials in context database", "author": ["S. Bell", "P. Upchurch", "N. Snavely", "K. Bala"], "venue": "Proc. CVPR,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks", "author": ["S. Bell", "C.L. Zitnick", "K. Bala", "R. Girshick"], "venue": "Proc. CVPR,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Deepedge: A multi-scale bifurcated deep network for top-down contour detection", "author": ["G. Bertasius", "J. Shi", "L. Torresani"], "venue": "Proc. CVPR,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "High-for-low and low-for-high: Efficient boundary detection from deep object features and its applications to high-level vision", "author": ["G. Bertasius", "J. Shi", "L. Torresani"], "venue": "Proc. ICCV,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Integrated perception with recurrent multi-task neural networks", "author": ["H. Bilen", "A. Vedaldi"], "venue": "Proc. NIPS,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. Bottou", "Y. Bengio", "Y. LeCun"], "venue": "Proc. CVPR,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1997}, {"title": "Describing people: A poselet-based approach to attribute classification", "author": ["L.D. Bourdev", "S. Maji", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Fast, exact and multiscale inference for semantic image segmentation with deep gaussian crfs", "author": ["S. Chandra", "I. Kokkinos"], "venue": "Proc. ECCV,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with task-specific edge detection using cnns and a discriminatively trained domain transform", "author": ["L. Chen", "J.T. Barron", "G. Papandreou", "K. Murphy", "A.L. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "Proc. ICLR,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "author": ["L. Chen", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A.L. Yuille"], "venue": "CoRR, abs/1606.00915,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Attention to scale: Scale-aware semantic image segmentation", "author": ["L. Chen", "Y. Yang", "J. Wang", "W. Xu", "A.L. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Training deep nets with sublinear memory cost", "author": ["T. Chen", "B. Xu", "C. Zhang", "C. Guestrin"], "venue": "CoRR, abs/1604.06174,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Detect what you can: Detecting and representing objects using holistic models and body parts", "author": ["X. Chen", "R. Mottaghi", "X. Liu", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Global contrast-based salient region detection", "author": ["M.-M. Cheng", "N.J. Mitra", "X. Huang", "P.H.S. Torr", "S.-M. Hu"], "venue": "PAMI,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep filter banks for texture recognition, description, and segmentation", "author": ["M. Cimpoi", "S. Maji", "I. Kokkinos", "A. Vedaldi"], "venue": "IJCV,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "Proc. ICCV,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Instance-aware semantic segmentation via multi-task network cascades", "author": ["J. Dai", "K. He", "J. Sun"], "venue": "Proc. CVPR,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "R-FCN: object detection via region-based fully convolutional networks", "author": ["J. Dai", "Y. Li", "K. He", "J. Sun"], "venue": "Proc. NIPS,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast edge detection using structured forests", "author": ["P. Doll\u00e1r", "C.L. Zitnick"], "venue": "PAMI,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Image super-resolution using deep convolutional networks", "author": ["C. Dong", "C.C. Loy", "K. He", "X. Tang"], "venue": "PAMI,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Predicting depth, surface normals and semantic labels with a common multiscale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "Proc. ICCV,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.M.A. Eslami", "L.J.V. Gool", "C.K.I. Williams", "J.M. Winn", "A. Zisserman"], "venue": "IJCV,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "PAMI,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "N\u02c6 4-fields: Neural network nearest neighbor fields for image transforms", "author": ["Y. Ganin", "V. Lempitsky"], "venue": "Proc. ACCV,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Laplacian reconstruction and refinement for semantic segmentation", "author": ["G. Ghiasi", "C.C. Fowlkes"], "venue": "Proc. ECCV,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Attend refine repeat: Active box proposal generation via in-out localization", "author": ["S. Gidaris", "N. Komodakis"], "venue": "CoRR, abs/1606.04446,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast R-CNN", "author": ["R.B. Girshick"], "venue": "Proc. ICCV,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Contextual action recognition with r*cnn", "author": ["G. Gkioxari", "R.B. Girshick", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Memory-efficient backpropagation through time", "author": ["A. Gruslys", "R. Munos", "I. Danihelka", "M. Lanctot", "A. Graves"], "venue": "CoRR, abs/1606.03401,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Matchnet: Unifying feature and metric learning for patch-based matching", "author": ["X. Han", "T. Leung", "Y. Jia", "R. Sukthankar", "A.C. Berg"], "venue": "Proc. CVPR,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic contours from inverse detectors", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "L. Bourdev", "S. Maji", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Simultaneous detection and segmentation", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "Proc. ECCV,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Hypercolumns for object segmentation and finegrained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "Proc. CVPR,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning dense convolutional embeddings for semantic segmentation", "author": ["A.W. Harley", "K.G. Derpanis", "I. Kokkinos"], "venue": "CoRR, abs/1511.04377,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Pixel-wise deep learning for contour detection", "author": ["J.-J. Hwang", "T.-L. Liu"], "venue": "Proc. ICLR,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepercut: A deeper, stronger, and faster multi-person pose estimation model", "author": ["E. Insafutdinov", "L. Pishchulin", "B. Andres", "M. Andriluka", "B. Schiele"], "venue": "CoRR, abs/1605.03170,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": "Proc. ICML,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R.B. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the ACM,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Salient region detection by UFO: uniqueness, focusness and objectness", "author": ["P. Jiang", "H. Ling", "J. Yu", "J. Peng"], "venue": "Proc. ICCV,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2013}, {"title": "Locally scale-invariant convolutional neural networks", "author": ["A. Kanazawa", "A. Sharma", "D.W. Jacobs"], "venue": "CoRR, abs/1412.5104,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Integrated segmentation and recognition of hand-printed numerals", "author": ["J.D. Keeler", "D.E. Rumelhart", "W.K. Leow"], "venue": "Proc. NIPS,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 1990}, {"title": "Visual boundary prediction: A deep neural prediction network and quality dissection", "author": ["J.J. Kivinen", "C.K.I. Williams", "N. Heess"], "venue": "AISTATS,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2014}, {"title": "Pushing the boundaries of boundary detection using deep learning", "author": ["I. Kokkinos"], "venue": "ICLR,", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}, {"title": "An expectation maximization approach to the synergy between image segmentation and object categorization", "author": ["I. Kokkinos", "P. Maragos"], "venue": "Proc. ICCV, volume I, pages 617\u2013624,", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2005}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "author": ["P. Kr\u00e4henb\u00fchl", "V. Koltun"], "venue": "NIPS,", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS,", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2013}, {"title": "Obj-cut", "author": ["M.P. Kumar", "P. Torr", "A. Zisserman"], "venue": "Proc. CVPR,", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2005}, {"title": "Discriminatively trained dense surface normal estimation", "author": ["L. Ladicky", "B. Zeisl", "M. Pollefeys"], "venue": "Proc. ECCV,", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning representations for automatic colorization", "author": ["G. Larsson", "M. Maire", "G. Shakhnarovich"], "venue": "CoRR, abs/1603.06668,", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proc. IEEE,", "citeRegEx": "56", "shortCiteRegEx": null, "year": 1998}, {"title": "Visual saliency based on multiscale deep features", "author": ["G. Li", "Y. Yu"], "venue": "Proc. CVPR,", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep contrast learning for salient object detection", "author": ["G. Li", "Y. Yu"], "venue": "Proc. CVPR,", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2016}, {"title": "The secrets of salient object segmentation", "author": ["Y. Li", "X. Hou", "C. Koch", "J.M. Rehg", "A.L. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic object parsing with graph LSTM", "author": ["X. Liang", "X. Shen", "J. Feng", "L. Lin", "S. Yan"], "venue": "Proc. CVPR,", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient piecewise training of deep structured models for semantic segmentation", "author": ["G. Lin", "C. Shen", "I.D. Reid", "A. van den Hengel"], "venue": null, "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2016}, {"title": "Microsoft COCO: common objects in context", "author": ["T. Lin", "M. Maire", "S.J. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "Proc. ECCV,", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional neural fields for depth estimation from a single image", "author": ["F. Liu", "C. Shen", "G. Lin"], "venue": "Proc. CVPR,", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning depth from single monocular images using deep convolutional neural fields", "author": ["F. Liu", "C. Shen", "G. Lin", "I.D. Reid"], "venue": "CoRR, abs/1502.07411,", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2015}, {"title": "SSD: single shot multibox detector", "author": ["W. Liu", "D. Anguelov", "D. Erhan", "C. Szegedy", "S.E. Reed"], "venue": "CoRR, abs/1512.02325,", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2015}, {"title": "Parsenet: Looking wider to see better", "author": ["W. Liu", "A. Rabinovich", "A.C. Berg"], "venue": "CoRR, abs/1506.04579,", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. CVPR,", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2015}, {"title": "Object detection and segmentation from joint embedding of parts and pixels", "author": ["M. Maire", "S.X. Yu", "P. Perona"], "venue": "ICCV,", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}, {"title": "A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics", "author": ["D. Martin", "C. Fowlkes", "D. Tal", "J. Malik"], "venue": "Proc. ICCV,", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2001}, {"title": "Cross-stitch networks for multi-task learning", "author": ["I. Misra", "A. Shrivastava", "A. Gupta", "M. Hebert"], "venue": "Proc. CVPR,", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2016}, {"title": "The role of context for object detection and semantic segmentation in the wild", "author": ["R. Mottaghi", "X. Chen", "X. Liu", "N.-G. Cho", "S.-W. Lee", "S. Fidler", "R. Urtasun", "A. Yuille"], "venue": "Proc. CVPR,", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2014}, {"title": "Neuronal Architectures for Pattern Theoretic Problems", "author": ["D. Mumford"], "venue": "Large Scale Theories of the Cortex. MIT Press,", "citeRegEx": "72", "shortCiteRegEx": null, "year": 1994}, {"title": "Direct intrinsics: Learning albedo-shading decomposition by convolutional regression", "author": ["T. Narihira", "M. Maire", "S.X. Yu"], "venue": "Proc. ICCV,", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2015}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["P.K. Nathan Silberman", "Derek Hoiem", "R. Fergus"], "venue": "In ECCV,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2012}, {"title": "Stacked hourglass networks for human pose estimation", "author": ["A. Newell", "K. Yang", "J. Deng"], "venue": "CoRR, abs/1603.06937,", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "Proc. ICCV,", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2015}, {"title": "Is object localization for free? - weakly-supervised learning with convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "Proc. CVPR,", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2015}, {"title": "Weakly- and semi-supervised learning of a DCNN for semantic image segmentation", "author": ["G. Papandreou", "L. Chen", "K. Murphy", "A.L. Yuille"], "venue": "Proc. ICCV,", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2015}, {"title": "Modeling local and global deformations in deep learning: Epitomic convolution, multiple instance learning, and sliding window detection", "author": ["G. Papandreou", "I. Kokkinos", "P. Savalle"], "venue": "Proc. CVPR,", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2015}, {"title": "Saliency filters: Contrast based filtering for salient region detection", "author": ["F. Perazzi", "P. Kr\u00e4henb\u00fchl", "Y. Pritch", "A. Hornung"], "venue": "Proc. CVPR,", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2012}, {"title": "Flowing convnets for human pose estimation in videos", "author": ["T. Pfister", "J. Charles", "A. Zisserman"], "venue": "Proc. ICCV,", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to segment object candidates", "author": ["P.H.O. Pinheiro", "R. Collobert", "P. Doll\u00e1r"], "venue": "Proc. NIPS,", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2015}, {"title": "Saliency detection via cellular automata", "author": ["Y. Qin", "H. Lu", "Y. Xu", "H. Wang"], "venue": "Proc. CVPR,", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2015}, {"title": "Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition", "author": ["R. Ranjan", "V.M. Patel", "R. Chellappa"], "venue": "CoRR, abs/1603.01249,", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster R- CNN: towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R.B. Girshick", "J. Sun"], "venue": "Proc. NIPS,", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2015}, {"title": "Discriminatively trained sparse code gradients for contour detection", "author": ["X. Ren", "L. Bo"], "venue": "Proc. NIPS,", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2012}, {"title": "U-net: Convolutional networks for biomedical image segmentation", "author": ["O. Ronneberger", "P. Fischer", "T. Brox"], "venue": "Proc. MICCAI,", "citeRegEx": "87", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei- Fei"], "venue": "IJCV,", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2015}, {"title": "Overfeat: Integrated recognition, localization and detection using convolutional networks", "author": ["P. Sermanet", "D. Eigen", "X. Zhang", "M. Mathieu", "R. Fergus", "Y. LeCun"], "venue": "Proc. ICLR,", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2014}, {"title": "Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection", "author": ["W. Shen", "X. Wang", "Y. Wang", "X. Bai", "Z. Zhang"], "venue": "Proc. CVPR,", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2015}, {"title": "Object skeleton extraction in natural images by fusing scale-associated deep side outputs", "author": ["W. Shen", "K. Zhao", "Y. Jiang", "Y. Wang", "Z. Zhang", "X. Bai"], "venue": "Proc. CVPR,", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative learning of deep convolutional feature point descriptors", "author": ["E. Simo-Serra", "E. Trulls", "L. Ferraz", "I. Kokkinos", "P. Fua", "F. Moreno-Noguer"], "venue": "Proc. ICCV,", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "Proc. ICLR,", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2015}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proc. CVPR,", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "venue": "CoRR, abs/1312.6199,", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2013}, {"title": "Deeppose: Human pose estimation via deep neural networks", "author": ["A. Toshev", "C. Szegedy"], "venue": "Proc. CVPR,", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning-based symmetry detection in natural images", "author": ["S. Tsogkas", "I. Kokkinos"], "venue": "Proc. ECCV,", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning for semantic part segmentation with high-level guidance", "author": ["S. Tsogkas", "I. Kokkinos", "G. Papandreou", "A. Vedaldi"], "venue": "CoRR, abs/1505.02438,", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2015}, {"title": "Image Parsing: Unifying Segmentation, Detection, and Recognition", "author": ["Z.W. Tu", "X. Chen", "A. Yuille", "S.C. Zhu"], "venue": "Proc. ICCV,", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2003}, {"title": "Situational object boundary detection", "author": ["J.R.R. Uijlings", "V. Ferrari"], "venue": "Proc. CVPR,", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2015}, {"title": "PISA: pixelwise image saliency by aggregating complementary appearance contrast measures with edgepreserving coherence", "author": ["K. Wang", "L. Lin", "J. Lu", "C. Li", "K. Shi"], "venue": "IEEE Trans. Im. Proc.,", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep networks for saliency detection via local estimation and global search", "author": ["L. Wang", "H. Lu", "X. Ruan", "M. Yang"], "venue": "Proc. CVPR,", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2015}, {"title": "Designing deep networks for surface normal estimation", "author": ["X. Wang", "D.F. Fouhey", "A. Gupta"], "venue": "Proc. CVPR,", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional pose machines", "author": ["S. Wei", "V. Ramakrishna", "T. Kanade", "Y. Sheikh"], "venue": "Proc. CVPR,", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2016}, {"title": "Scale-space filtering", "author": ["A.P. Witkin"], "venue": "IJCAI,", "citeRegEx": "105", "shortCiteRegEx": null, "year": 1983}, {"title": "Zoom better to see clearer: Human part segmentation with auto zoom net", "author": ["F. Xia", "P. Wang", "L. Chen", "A.L. Yuille"], "venue": "Proc. ECCV,", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2016}, {"title": "Holistically-nested edge detection", "author": ["S. Xie", "Z. Tu"], "venue": "Proc. ICCV,", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2015}, {"title": "Noisy label recovery for shadow detection in unfamiliar domains", "author": ["T.F. Yago Vicente", "M. Hoai", "D. Samaras"], "venue": "Proc. CVPR,", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2016}, {"title": "Lift: Learned invariant feature transform", "author": ["K.M. Yi", "E. Trulls", "V. Lepetit", "P. Fua"], "venue": "Proc. ECCV,", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to compare image patches via convolutional neural networks", "author": ["S. Zagoruyko", "N. Komodakis"], "venue": "Proc. CVPR,", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2015}, {"title": "Facial landmark detection by deep multi-task learning", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "Proc. ECCV,", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2014}, {"title": "Saliency detection by multi-context deep learning", "author": ["R. Zhao", "W. Ouyang", "H. Li", "X. Wang"], "venue": "Proc. CVPR,", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2015}, {"title": "Conditional random fields as recurrent neural networks", "author": ["S. Zheng", "S. Jayasumana", "B. Romera-Paredes", "V. Vineet", "Z. Su", "D. Du", "C. Huang", "P. Torr"], "venue": "Proc. ICCV,", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 55, "context": "While Convolutional Neural Networks (CNNs) have been the method of choice for text recognition for more than two decades [56], they have only been recently shown to successful at handling effectively most, if not all, vision tasks.", "startOffset": 121, "endOffset": 125}, {"referenceID": 23, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 128, "endOffset": 132}, {"referenceID": 54, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 172, "endOffset": 188}, {"referenceID": 27, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 172, "endOffset": 188}, {"referenceID": 48, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 172, "endOffset": 188}, {"referenceID": 106, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 172, "endOffset": 188}, {"referenceID": 90, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 209, "endOffset": 213}, {"referenceID": 108, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 240, "endOffset": 245}, {"referenceID": 34, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 265, "endOffset": 278}, {"referenceID": 91, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 265, "endOffset": 278}, {"referenceID": 109, "context": "While only considering works that apply to a single, static image we can indicatively list successes of CNNs in superresolution [24], colorization [55], boundary detection [6, 28, 49, 107], symmetry detection [91], interest point detection [109], image descriptors [35, 92, 110], surface Input Boundaries", "startOffset": 265, "endOffset": 278}, {"referenceID": 87, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 9, "endOffset": 13}, {"referenceID": 25, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 21, "endOffset": 25}, {"referenceID": 25, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 33, "endOffset": 37}, {"referenceID": 25, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 45, "endOffset": 49}, {"referenceID": 61, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 58, "endOffset": 62}, {"referenceID": 73, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 67, "endOffset": 71}, {"referenceID": 17, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 80, "endOffset": 84}, {"referenceID": 68, "context": "Imagenet [88] VOC\u201907 [26] VOC\u201910 [26] VOC\u201912 [26] MS-COCO [62] NYU [74] MSRA10K [18] BSD [69]", "startOffset": 89, "endOffset": 93}, {"referenceID": 35, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 76, "endOffset": 84}, {"referenceID": 70, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 76, "endOffset": 84}, {"referenceID": 35, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 141, "endOffset": 149}, {"referenceID": 70, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 141, "endOffset": 149}, {"referenceID": 16, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 189, "endOffset": 193}, {"referenceID": 9, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 231, "endOffset": 235}, {"referenceID": 70, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 344, "endOffset": 348}, {"referenceID": 90, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 395, "endOffset": 399}, {"referenceID": 96, "context": "Detection Partial Yes Yes Yes Yes No No No Semantic Segmentation No Partial [36, 71] Partial Yes Yes\u2217 No No Instance Segmentation No Partial [36, 71] Partial Yes No No No Human parts No No [17] No No No No No Human landmarks No No [10] No Yes No No No Surface Normals No No No No No Yes No No Saliency No No No No No No Yes No Boundaries No No [71] No No No No Yes Symmetry No No No No Partial, [91] No No [97]", "startOffset": 406, "endOffset": 410}, {"referenceID": 9, "context": "[10, 17, 71], but as the number of task grows it becomes impossible to use one dataset for all.", "startOffset": 0, "endOffset": 12}, {"referenceID": 16, "context": "[10, 17, 71], but as the number of task grows it becomes impossible to use one dataset for all.", "startOffset": 0, "endOffset": 12}, {"referenceID": 70, "context": "[10, 17, 71], but as the number of task grows it becomes impossible to use one dataset for all.", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 18, "endOffset": 30}, {"referenceID": 24, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 18, "endOffset": 30}, {"referenceID": 102, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 18, "endOffset": 30}, {"referenceID": 24, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 49, "endOffset": 61}, {"referenceID": 62, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 49, "endOffset": 61}, {"referenceID": 63, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 49, "endOffset": 61}, {"referenceID": 72, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 93, "endOffset": 97}, {"referenceID": 107, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 115, "endOffset": 120}, {"referenceID": 18, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 145, "endOffset": 149}, {"referenceID": 3, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 175, "endOffset": 178}, {"referenceID": 57, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 200, "endOffset": 209}, {"referenceID": 111, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 200, "endOffset": 209}, {"referenceID": 13, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 233, "endOffset": 244}, {"referenceID": 26, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 233, "endOffset": 244}, {"referenceID": 66, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 233, "endOffset": 244}, {"referenceID": 29, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 273, "endOffset": 283}, {"referenceID": 81, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 273, "endOffset": 283}, {"referenceID": 84, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 273, "endOffset": 283}, {"referenceID": 20, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 307, "endOffset": 319}, {"referenceID": 36, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 307, "endOffset": 319}, {"referenceID": 81, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 307, "endOffset": 319}, {"referenceID": 2, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 14, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 41, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 74, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 80, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 95, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 97, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 103, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 110, "context": "normal estimation [2, 25, 103], depth estimation [25, 63, 64], intrinsic image decomposition [73] shadow detection [108], texture classification [19], material classification [4], saliency estimation [58, 112], semantic segmentation [14, 27,67], region proposal generation [30,82,85], instance segmentation [21, 37, 82], pose estimation, part segmentation, and landmark localization [3,15,42,75,81,96,98,104,111], as well as the large body of works around object detection and image classification e.", "startOffset": 383, "endOffset": 412}, {"referenceID": 21, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 30, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 31, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 39, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 51, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 84, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 88, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 92, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 93, "context": "[22,31,32,40,52,85,89,93,94].", "startOffset": 0, "endOffset": 28}, {"referenceID": 92, "context": "Most of these works rely on finetuning a common pretrained CNN, such as the VGG network [93] or others [40, 52], which indicates the broad potential of these CNNs.", "startOffset": 88, "endOffset": 92}, {"referenceID": 39, "context": "Most of these works rely on finetuning a common pretrained CNN, such as the VGG network [93] or others [40, 52], which indicates the broad potential of these CNNs.", "startOffset": 103, "endOffset": 111}, {"referenceID": 51, "context": "Most of these works rely on finetuning a common pretrained CNN, such as the VGG network [93] or others [40, 52], which indicates the broad potential of these CNNs.", "startOffset": 103, "endOffset": 111}, {"referenceID": 94, "context": "Apart from simplicity and efficiency, the problem can also be motivated by arguing that by training a network so as to accomplish multiple tasks one leaves smaller space for \u2018blindspots\u2019 [95], effectively providing a more complete specification of the network duties.", "startOffset": 187, "endOffset": 191}, {"referenceID": 8, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 46, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 49, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 52, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 67, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 71, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 98, "context": "the long-standing problem of combining segmentation and recognition [9, 47, 50, 53, 68, 72, 99]), so this work can be understood as a first step in this direction.", "startOffset": 68, "endOffset": 95}, {"referenceID": 88, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 3, "endOffset": 7}, {"referenceID": 24, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 75, "endOffset": 79}, {"referenceID": 32, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 195, "endOffset": 199}, {"referenceID": 69, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 284, "endOffset": 288}, {"referenceID": 7, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 399, "endOffset": 402}, {"referenceID": 83, "context": "In [89] a CNN is used for joint localization detection and classification, [25] propose a network that jointly solves surface normal estimation, depth estimation and semantic segmentation, while [33] train a system for joint detection, pose estimation and region proposal generation; [70] study the effects of sharing information across networks trained for complementary tasks, while more recently [8] propose the introduction of intertask connections that can improve performance through task synergy, while [84] propose an architecture encompassing a host of face-related tasks.", "startOffset": 510, "endOffset": 514}, {"referenceID": 25, "context": "object positions, landmarks in PASCAL VOC [26]) is often missing from the datasets used for low-level tasks (e.", "startOffset": 42, "endOffset": 46}, {"referenceID": 68, "context": "BSD [69]), and vice versa.", "startOffset": 4, "endOffset": 8}, {"referenceID": 73, "context": "If we consider for instance a network that is supposed to be predicting both human landmarks and surface normals, we have no dataset where an image comes with annotations for both tasks, but rather disjoint datasets (NYU [74], and PASCAL VOC [26], or any other pose estimation dataset for keypoints) providing every image with annotations for only one of the two.", "startOffset": 221, "endOffset": 225}, {"referenceID": 25, "context": "If we consider for instance a network that is supposed to be predicting both human landmarks and surface normals, we have no dataset where an image comes with annotations for both tasks, but rather disjoint datasets (NYU [74], and PASCAL VOC [26], or any other pose estimation dataset for keypoints) providing every image with annotations for only one of the two.", "startOffset": 242, "endOffset": 246}, {"referenceID": 15, "context": "Instead, we build on recent developments in learning with deep architectures [16, 34] which have shown that it is possible to efficiently train a deep CNN with a memory complexity that is sublinear in the number of layers.", "startOffset": 77, "endOffset": 85}, {"referenceID": 33, "context": "Instead, we build on recent developments in learning with deep architectures [16, 34] which have shown that it is possible to efficiently train a deep CNN with a memory complexity that is sublinear in the number of layers.", "startOffset": 77, "endOffset": 85}, {"referenceID": 28, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 25, "endOffset": 41}, {"referenceID": 74, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 25, "endOffset": 41}, {"referenceID": 75, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 25, "endOffset": 41}, {"referenceID": 86, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 25, "endOffset": 41}, {"referenceID": 13, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 69, "endOffset": 82}, {"referenceID": 50, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 69, "endOffset": 82}, {"referenceID": 112, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 69, "endOffset": 82}, {"referenceID": 11, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 117, "endOffset": 125}, {"referenceID": 38, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 117, "endOffset": 125}, {"referenceID": 10, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 203, "endOffset": 210}, {"referenceID": 60, "context": "U-net type architectures [29, 75, 76, 87], Dense CRF post-processing [14, 51, 113] or bilateral filtertype smoothing [12, 39], as well as more general structured prediction with CNN-based pairwise terms [11,61] - but we leave this for future work.", "startOffset": 203, "endOffset": 210}, {"referenceID": 55, "context": "The starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation.", "startOffset": 77, "endOffset": 90}, {"referenceID": 66, "context": "The starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation.", "startOffset": 77, "endOffset": 90}, {"referenceID": 78, "context": "The starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation.", "startOffset": 77, "endOffset": 90}, {"referenceID": 88, "context": "The starting point is that of using a standard \u2018fully\u2019 convolutional network [56,67,79,89], namely a CNN that provides a field of decision variables, rather than a single classificiation at its output; this can be used to accomplish any dense labelling or regression task, such as boundary detection, normal estimation, or semantic segmentation.", "startOffset": 77, "endOffset": 90}, {"referenceID": 37, "context": "Skip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output.", "startOffset": 83, "endOffset": 100}, {"referenceID": 48, "context": "Skip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output.", "startOffset": 83, "endOffset": 100}, {"referenceID": 88, "context": "Skip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output.", "startOffset": 83, "endOffset": 100}, {"referenceID": 106, "context": "Skip layers: one first deviation from the most standard architecture is that as in [38, 49, 89, 107] we use skip layers that combine the top-layer neurons with the activations of intermediate neurons to form the network output.", "startOffset": 83, "endOffset": 100}, {"referenceID": 106, "context": "Tasks such as boundary detection clearly profit from the smaller degree of spatial abstraction of lower-level neurons [107], while even for high-level tasks, such as semantic segmentation, it has been shown [38,67] that skip layers can improve performance.", "startOffset": 118, "endOffset": 123}, {"referenceID": 37, "context": "Tasks such as boundary detection clearly profit from the smaller degree of spatial abstraction of lower-level neurons [107], while even for high-level tasks, such as semantic segmentation, it has been shown [38,67] that skip layers can improve performance.", "startOffset": 207, "endOffset": 214}, {"referenceID": 66, "context": "Tasks such as boundary detection clearly profit from the smaller degree of spatial abstraction of lower-level neurons [107], while even for high-level tasks, such as semantic segmentation, it has been shown [38,67] that skip layers can improve performance.", "startOffset": 207, "endOffset": 214}, {"referenceID": 4, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 45, "endOffset": 52}, {"referenceID": 65, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 45, "endOffset": 52}, {"referenceID": 42, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 81, "endOffset": 85}, {"referenceID": 48, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 223, "endOffset": 232}, {"referenceID": 106, "context": "Skip-layer normalization: Modifying slightly [5, 66], we use batch normalization [43] prior to forming the inner product with intermediate layers; this alleviates the need for very low learning rates, which was the case in [49, 107].", "startOffset": 223, "endOffset": 232}, {"referenceID": 48, "context": "Cumulative task-specific operations: Scaling up to many tasks requires keeping the task-specific memory and computation budget low, and we therefore choose as in [49, 107] to process the outputs of the skip-pooling with task-specific layers that perform linear operations.", "startOffset": 162, "endOffset": 171}, {"referenceID": 106, "context": "Cumulative task-specific operations: Scaling up to many tasks requires keeping the task-specific memory and computation budget low, and we therefore choose as in [49, 107] to process the outputs of the skip-pooling with task-specific layers that perform linear operations.", "startOffset": 162, "endOffset": 171}, {"referenceID": 106, "context": "2 we observe that instead of simply adding the scores (sum-fusion), one can accelerate training by concatenating the score maps and learning a linear function that operates on top of the concatenated score maps - as originally done in [107].", "startOffset": 235, "endOffset": 240}, {"referenceID": 13, "context": "Atrous convolution: We also use convolution with holes (\u00e0 trous) [14,79] which allows us to control the spatial resolution of the output layer.", "startOffset": 65, "endOffset": 72}, {"referenceID": 78, "context": "Atrous convolution: We also use convolution with holes (\u00e0 trous) [14,79] which allows us to control the spatial resolution of the output layer.", "startOffset": 65, "endOffset": 72}, {"referenceID": 14, "context": "Multi-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-", "startOffset": 28, "endOffset": 41}, {"referenceID": 45, "context": "Multi-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-", "startOffset": 28, "endOffset": 41}, {"referenceID": 48, "context": "Multi-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-", "startOffset": 28, "endOffset": 41}, {"referenceID": 78, "context": "Multi-resolution CNN: as in [15,46,49,79], rather than processing an image at a single resolution, we form an im-", "startOffset": 28, "endOffset": 41}, {"referenceID": 14, "context": "Even though in [15] a max-fusion scheme is shown to yield higher accuracy than sum-fusion, in our understanding this is particular to the case of semantic segmentation, where a large score at any scale suffices to assign an object label to a pixel.", "startOffset": 15, "endOffset": 19}, {"referenceID": 104, "context": "This may not be the case for boundaries where the score should be determined by the accumulation of evidence from multiple scales [105] or for normals, where maximization over scales of the normal vector entries does not make any sense.", "startOffset": 130, "endOffset": 135}, {"referenceID": 14, "context": "We therefore use a concatenation of the scores followed by a linear operation, as in the case of fusing the skip-layers described above, and leave the exploration of scale-aware processing [15] for the future.", "startOffset": 189, "endOffset": 193}, {"referenceID": 31, "context": "In this pyramid the highest resolution image is set similar to [32] so that the smallest image dimension is 621 pixels and the largest dimension does not exceed 921 (the exact numbers are so that dimensions are of the form 32k + 1, as requested by [15, 49]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "In this pyramid the highest resolution image is set similar to [32] so that the smallest image dimension is 621 pixels and the largest dimension does not exceed 921 (the exact numbers are so that dimensions are of the form 32k + 1, as requested by [15, 49]).", "startOffset": 248, "endOffset": 256}, {"referenceID": 48, "context": "In this pyramid the highest resolution image is set similar to [32] so that the smallest image dimension is 621 pixels and the largest dimension does not exceed 921 (the exact numbers are so that dimensions are of the form 32k + 1, as requested by [15, 49]).", "startOffset": 248, "endOffset": 256}, {"referenceID": 14, "context": "As in [15, 49] we use loss layers both at the outputs of the individual scales and the final responses, amounting to a mild form of deep supervision network (DSN) training [107].", "startOffset": 6, "endOffset": 14}, {"referenceID": 48, "context": "As in [15, 49] we use loss layers both at the outputs of the individual scales and the final responses, amounting to a mild form of deep supervision network (DSN) training [107].", "startOffset": 6, "endOffset": 14}, {"referenceID": 106, "context": "As in [15, 49] we use loss layers both at the outputs of the individual scales and the final responses, amounting to a mild form of deep supervision network (DSN) training [107].", "startOffset": 172, "endOffset": 177}, {"referenceID": 84, "context": "One exception to the uniform architecture outlined above is for detection, where we follow the work of [85] and learn a convolutional region proposal network, followed by a fully-connected subnetwork that classifies the region proposals into one of 21 labels (20 classes and background).", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "Recent advances however [22,65] may make this exception unnecessary.", "startOffset": 24, "endOffset": 31}, {"referenceID": 64, "context": "Recent advances however [22,65] may make this exception unnecessary.", "startOffset": 24, "endOffset": 31}, {"referenceID": 12, "context": "For region labelling tasks (semantic segmentation, human parts, saliency) and object detection we use the softmax loss function, as is common in all recent works on semantic segmentation [13,67] and object detection [32].", "startOffset": 187, "endOffset": 194}, {"referenceID": 66, "context": "For region labelling tasks (semantic segmentation, human parts, saliency) and object detection we use the softmax loss function, as is common in all recent works on semantic segmentation [13,67] and object detection [32].", "startOffset": 187, "endOffset": 194}, {"referenceID": 31, "context": "For region labelling tasks (semantic segmentation, human parts, saliency) and object detection we use the softmax loss function, as is common in all recent works on semantic segmentation [13,67] and object detection [32].", "startOffset": 216, "endOffset": 220}, {"referenceID": 31, "context": "For regression tasks (normal estimation, bounding box regression) we use the smooth `1 loss [32].", "startOffset": 92, "endOffset": 96}, {"referenceID": 48, "context": "For tasks where we want to estimate thin structures (boundaries, semantic boundaries) we use the MIL-based loss function introduced in [49] in order to accommodate imprecision in the placement of the boundary annotations.", "startOffset": 135, "endOffset": 139}, {"referenceID": 106, "context": "For these two tasks we also have a class imbalance problem, with many more negatives than positives; we mitigate this by using a weighted cross-entropy loss, as in [107], where we attribute a weight of 0.", "startOffset": 164, "endOffset": 169}, {"referenceID": 19, "context": "Certain recent works such as [20, 77, 78] manage to impute missing data in an EM-type approach, by exploiting domain-specific knowledge - e.", "startOffset": 29, "endOffset": 41}, {"referenceID": 76, "context": "Certain recent works such as [20, 77, 78] manage to impute missing data in an EM-type approach, by exploiting domain-specific knowledge - e.", "startOffset": 29, "endOffset": 41}, {"referenceID": 77, "context": "Certain recent works such as [20, 77, 78] manage to impute missing data in an EM-type approach, by exploiting domain-specific knowledge - e.", "startOffset": 29, "endOffset": 41}, {"referenceID": 31, "context": "In particular, for detection tasks it is reported in [32] that a batchsize of two suffices, while for dense labelling tasks such as semantic segmentation a batchsize of 10, 20 or even 30 is often used [14,107].", "startOffset": 53, "endOffset": 57}, {"referenceID": 13, "context": "In particular, for detection tasks it is reported in [32] that a batchsize of two suffices, while for dense labelling tasks such as semantic segmentation a batchsize of 10, 20 or even 30 is often used [14,107].", "startOffset": 201, "endOffset": 209}, {"referenceID": 106, "context": "In particular, for detection tasks it is reported in [32] that a batchsize of two suffices, while for dense labelling tasks such as semantic segmentation a batchsize of 10, 20 or even 30 is often used [14,107].", "startOffset": 201, "endOffset": 209}, {"referenceID": 15, "context": "In order to handle these problems we build on recent advances in memory-efficient backpropagation for deep networks [16, 34] and adapt them to the task of multitask learning1.", "startOffset": 116, "endOffset": 124}, {"referenceID": 33, "context": "In order to handle these problems we build on recent advances in memory-efficient backpropagation for deep networks [16, 34] and adapt them to the task of multitask learning1.", "startOffset": 116, "endOffset": 124}, {"referenceID": 15, "context": "We start by describing the basic idea behind the algorithm of [16], paving the way for the presentation of our extension to multi-task learning.", "startOffset": 62, "endOffset": 66}, {"referenceID": 43, "context": "In the popular Caffe [44] library memory is also allocated for all of the gradient signals, since a priori these could feed into multiple layers for a DAG network.", "startOffset": 21, "endOffset": 25}, {"referenceID": 15, "context": "The memory-efficient alternative described in [16] is shown in Fig.", "startOffset": 46, "endOffset": 50}, {"referenceID": 92, "context": "In order to examine this we compare primarily to results obtained by methods that rely on the VGG network [93] - more recent works e.", "startOffset": 106, "endOffset": 110}, {"referenceID": 21, "context": "on detection [22] and semantic segmentation [14] have shown improvements through the use of deeper ResNets [40], but we consider the choice of network to be in a sense orthogonal to the goal of this section.", "startOffset": 13, "endOffset": 17}, {"referenceID": 13, "context": "on detection [22] and semantic segmentation [14] have shown improvements through the use of deeper ResNets [40], but we consider the choice of network to be in a sense orthogonal to the goal of this section.", "startOffset": 44, "endOffset": 48}, {"referenceID": 39, "context": "on detection [22] and semantic segmentation [14] have shown improvements through the use of deeper ResNets [40], but we consider the choice of network to be in a sense orthogonal to the goal of this section.", "startOffset": 107, "endOffset": 111}, {"referenceID": 31, "context": "9 and a minibatch size of 10 - with the exception of detection, where we use a minibatch size of 2, following [32].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "in [14,32].", "startOffset": 3, "endOffset": 10}, {"referenceID": 31, "context": "in [14,32].", "startOffset": 3, "endOffset": 10}, {"referenceID": 13, "context": "In particular we use the network of [14] for semantic segmentation (\u2018COCO-S\u2019) and the network of [32] for detection, (\u2018COCO-D\u2019).", "startOffset": 36, "endOffset": 40}, {"referenceID": 31, "context": "In particular we use the network of [14] for semantic segmentation (\u2018COCO-S\u2019) and the network of [32] for detection, (\u2018COCO-D\u2019).", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "We finetune this network for 10000 iterations on the VOC07++ set [32] stands for the union of PASCAL VOC 2007 trainval and PASCAL VOC 2012 trainval sets; we start with a learning rate of 0.", "startOffset": 65, "endOffset": 69}, {"referenceID": 31, "context": "Object Detection: We start by verifying in the \u2018Ours 1-Task\u2019 row that we can replicate the results of [32]; exceptionally for this experiment, rather than using the initialization described above, we start from the MS-COCO pretrained network of [32], finetune on the VOC 2007++ dataset, and test on the VOC 2007 test dataset.", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": "Object Detection: We start by verifying in the \u2018Ours 1-Task\u2019 row that we can replicate the results of [32]; exceptionally for this experiment, rather than using the initialization described above, we start from the MS-COCO pretrained network of [32], finetune on the VOC 2007++ dataset, and test on the VOC 2007 test dataset.", "startOffset": 245, "endOffset": 249}, {"referenceID": 13, "context": "The only differences are that we use a minimal image side of 621 rather than 600, a maximal side of 961 rather than 1000, so as to comply with the 32k + 1 restriction on dimensions of [14], and use convolution with holes followed by appropriately modified RPN and ROI-pooling layers to get effectively identical results as [32].", "startOffset": 184, "endOffset": 188}, {"referenceID": 31, "context": "The only differences are that we use a minimal image side of 621 rather than 600, a maximal side of 961 rather than 1000, so as to comply with the 32k + 1 restriction on dimensions of [14], and use convolution with holes followed by appropriately modified RPN and ROI-pooling layers to get effectively identical results as [32].", "startOffset": 323, "endOffset": 327}, {"referenceID": 31, "context": "However when increasing the number of tasks performance drops, but is still comparable to the strong baseline of [32].", "startOffset": 113, "endOffset": 117}, {"referenceID": 31, "context": "F-RCNN, [32] VOC 2007++ 73.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "2 F-RCNN, [32] MS-COCO + VOC 2007++ 78.", "startOffset": 10, "endOffset": 14}, {"referenceID": 13, "context": "review in [14] for a recent comparison), we only compare to the methods lying closest to our own, which in turns relies on the \u2018Deeplab-Large Field of View (FOV)\u2019 architecture of [13].", "startOffset": 10, "endOffset": 14}, {"referenceID": 12, "context": "review in [14] for a recent comparison), we only compare to the methods lying closest to our own, which in turns relies on the \u2018Deeplab-Large Field of View (FOV)\u2019 architecture of [13].", "startOffset": 179, "endOffset": 183}, {"referenceID": 48, "context": "We first observe that thanks to the use of multi-scale processing we get a similar improvement over the singlescale architecture as the one we had obtained in [49].", "startOffset": 159, "endOffset": 163}, {"referenceID": 13, "context": "in [14] with ResNets and Atrous Spatial Pyramid Pooling; but these advances are complementary and easy to include in our network\u2019s archi-", "startOffset": 3, "endOffset": 7}, {"referenceID": 77, "context": "Deeplab -COCO + CRF [78] 70.", "startOffset": 20, "endOffset": 24}, {"referenceID": 48, "context": "4 Deeplab Multi-Scale [49] 72.", "startOffset": 22, "endOffset": 26}, {"referenceID": 48, "context": "1 Deeplab Multi-Scale -CRF [49] 74.", "startOffset": 27, "endOffset": 31}, {"referenceID": 77, "context": "Still, even without using CRF post-processing, we fare comparably to a strong baseline, such as [78].", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 14, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 59, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 97, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 105, "context": "Recent work has shown that semantic part segmentation is one more task that can be solved by CNNs [14, 15, 60, 98, 106].", "startOffset": 98, "endOffset": 119}, {"referenceID": 105, "context": "Deeplab LargeFOV [106] 51.", "startOffset": 17, "endOffset": 22}, {"referenceID": 105, "context": "78 Deeplab LargeFOV-CRF [106] 52.", "startOffset": 24, "endOffset": 29}, {"referenceID": 14, "context": "95 Multi-scale averaging [15] 54.", "startOffset": 25, "endOffset": 29}, {"referenceID": 14, "context": "Attention [15] 55.", "startOffset": 10, "endOffset": 14}, {"referenceID": 105, "context": "Auto Zoom [106] 57.", "startOffset": 10, "endOffset": 15}, {"referenceID": 59, "context": "54 Graph-LSTM [60] 60.", "startOffset": 14, "endOffset": 18}, {"referenceID": 16, "context": "Table 7: Part segmentation - mean Intersction-Over-Union accuracy on the dataset of [17].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "We use the dataset introduced in [17] and train a network that is architecturally identical to the one used for semantic segmentation, but is now finetuned for the task of segmenting human parts.", "startOffset": 33, "endOffset": 37}, {"referenceID": 35, "context": "Semantic Boundary Detection: We evaluate our method on the Semantic Boundary Detection task defined in [36], where the goal is to find where instances of the 20 PASCAL classes have discontinuities.", "startOffset": 103, "endOffset": 107}, {"referenceID": 35, "context": "Semantic Contours [36] 20.", "startOffset": 18, "endOffset": 22}, {"referenceID": 99, "context": "0 Situational Boundary [100] 31.", "startOffset": 23, "endOffset": 28}, {"referenceID": 6, "context": "6 High-for-Low [7] 47.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "7 High-for-Low-CRF [7] 54.", "startOffset": 19, "endOffset": 22}, {"referenceID": 35, "context": "Table 8: Semantic Boundary Detection Results: we report mean Average Precision (AP) performance (%) and Mean Max F-Measure Score on the validation set of PASCAL VOC 2010, provided by [36].", "startOffset": 183, "endOffset": 187}, {"referenceID": 35, "context": "We compare to the original method of [36], the situational boundary detector of [100], and the High-for-Low method of [7].", "startOffset": 37, "endOffset": 41}, {"referenceID": 99, "context": "We compare to the original method of [36], the situational boundary detector of [100], and the High-for-Low method of [7].", "startOffset": 80, "endOffset": 85}, {"referenceID": 6, "context": "We compare to the original method of [36], the situational boundary detector of [100], and the High-for-Low method of [7].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "The authors of [7] go beyond the individual task of boundary detection and explore what gains can be obtained by providing as inputs to this task the results of a separate semantic segmentation system (\u2018High-for-LowCRF\u2019 row).", "startOffset": 15, "endOffset": 18}, {"referenceID": 70, "context": "Boundary Detection: We train our network on the union of the (dataset-augmented) BSD trainval set and boundary images from the VOC context dataset [71] and evaluate it on the test set of the Berkeley Segmentation Dataset (BSD) of [69] and.", "startOffset": 147, "endOffset": 151}, {"referenceID": 68, "context": "Boundary Detection: We train our network on the union of the (dataset-augmented) BSD trainval set and boundary images from the VOC context dataset [71] and evaluate it on the test set of the Berkeley Segmentation Dataset (BSD) of [69] and.", "startOffset": 230, "endOffset": 234}, {"referenceID": 0, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 84, "endOffset": 91}, {"referenceID": 22, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 84, "endOffset": 91}, {"referenceID": 5, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 27, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 40, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 47, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 48, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 89, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 106, "context": "We compare our method to some of the best-established methods of boundary detection [1, 23], as well as more recent, deep learning-based ones [6, 28, 41, 48, 49, 90, 107].", "startOffset": 142, "endOffset": 170}, {"referenceID": 0, "context": "gPb-owt-ucm [1] 0.", "startOffset": 12, "endOffset": 15}, {"referenceID": 22, "context": "696 SE-Var [23] 0.", "startOffset": 11, "endOffset": 15}, {"referenceID": 47, "context": "803 DeepNets [48] 0.", "startOffset": 13, "endOffset": 17}, {"referenceID": 27, "context": "758 N4-Fields [28] 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 5, "context": "784 DeepEdge [6] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 40, "context": "807 CSCNN [41] 0.", "startOffset": 10, "endOffset": 14}, {"referenceID": 89, "context": "798 DeepContour [90] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 106, "context": "797 HED-fusion [107] 0.", "startOffset": 15, "endOffset": 20}, {"referenceID": 106, "context": "811 HED-late merging [107] 0.", "startOffset": 21, "endOffset": 26}, {"referenceID": 48, "context": "840 Multi-Scale [49] 0.", "startOffset": 16, "endOffset": 20}, {"referenceID": 48, "context": "861 Multi-Scale +sPb [49] 0.", "startOffset": 21, "endOffset": 25}, {"referenceID": 48, "context": "866 Ours, training setup of [49] 0.", "startOffset": 28, "endOffset": 32}, {"referenceID": 68, "context": "Table 9: Boundary Detection results: we report the maximal F meaure obtained at the Optimal Dataset Scale, Optimal Image Scale, as well as the Average Precision on the test set of the BSD dataset [69].", "startOffset": 196, "endOffset": 200}, {"referenceID": 48, "context": "A first experiment has been to train our new network with the exact same experimental setup as the one we had used in [49] - including Graduated Deep Supervised Network training, a mix of 30600 images obtained by dataset augmentation from the BSD (300 images augmented by 3 scales, 2 horizontal flips, and 16 rotations) with 20206 images from VOC-context (10103 images with two horizontal flips).", "startOffset": 118, "endOffset": 122}, {"referenceID": 48, "context": "815, surpassing even the performance we would get in [49] by using spectral boundaries on top.", "startOffset": 53, "endOffset": 57}, {"referenceID": 48, "context": "001 layer-specific rate used in [49, 107]), and use the particular mix of data used to train the UberNet in the multi-task setup.", "startOffset": 32, "endOffset": 41}, {"referenceID": 106, "context": "001 layer-specific rate used in [49, 107]), and use the particular mix of data used to train the UberNet in the multi-task setup.", "startOffset": 32, "endOffset": 41}, {"referenceID": 106, "context": "9 this can substantially affect performance - but we still remain competitive to previous state-of-the-art works, such as [107].", "startOffset": 122, "endOffset": 127}, {"referenceID": 106, "context": "For the multi-task training case performance drops a bit more, but always stays at a reasonably good level when compared to standard strong baselines such as [107].", "startOffset": 158, "endOffset": 163}, {"referenceID": 100, "context": "Saliency Estimation: We train on the MSRA-10K dataset of [101] and evaluate on the PASCAL-S dataset of [59], where a subset of the Pascal VOC10 validation set is annotated.", "startOffset": 57, "endOffset": 62}, {"referenceID": 58, "context": "Saliency Estimation: We train on the MSRA-10K dataset of [101] and evaluate on the PASCAL-S dataset of [59], where a subset of the Pascal VOC10 validation set is annotated.", "startOffset": 103, "endOffset": 107}, {"referenceID": 57, "context": "in [58], we will explore the performance of our method on those datasets in the future.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 35, "endOffset": 47}, {"referenceID": 44, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 35, "endOffset": 47}, {"referenceID": 79, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 35, "endOffset": 47}, {"referenceID": 57, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 76, "endOffset": 94}, {"referenceID": 82, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 76, "endOffset": 94}, {"referenceID": 101, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 76, "endOffset": 94}, {"referenceID": 111, "context": "We compare to some classic methods [18, 45, 80] as well as more recent ones [58, 83, 102, 112] that typically rely on deep learning.", "startOffset": 76, "endOffset": 94}, {"referenceID": 57, "context": "We note that our method sets a new state-of-the-art for this dataset, and even for the multi-task training case, our method outperforms the previous state-ofthe-art which was the CRF-based variant of [58].", "startOffset": 200, "endOffset": 204}, {"referenceID": 79, "context": "SF [80] 0.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "493 GC [18] 0.", "startOffset": 7, "endOffset": 11}, {"referenceID": 44, "context": "539 DRFI [45] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 100, "context": "690 PISA [101] 0.", "startOffset": 9, "endOffset": 14}, {"referenceID": 82, "context": "660 BSCA [83] 0.", "startOffset": 9, "endOffset": 13}, {"referenceID": 101, "context": "666 LEGS [102] 0.", "startOffset": 9, "endOffset": 14}, {"referenceID": 111, "context": "752 MC [112] 0.", "startOffset": 7, "endOffset": 12}, {"referenceID": 56, "context": "740 MDF [57] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 57, "context": "FCN [58] 0.", "startOffset": 4, "endOffset": 8}, {"referenceID": 57, "context": "793 DCL [58] 0.", "startOffset": 8, "endOffset": 12}, {"referenceID": 57, "context": "815 DCL + CRF [58] 0.", "startOffset": 14, "endOffset": 18}, {"referenceID": 58, "context": "Table 10: Saliency estimation results: we report the Maximal F-measure (MF) on the PASCAL Saliency dataset of [59].", "startOffset": 110, "endOffset": 114}, {"referenceID": 73, "context": "We train on the training set if [74] where normals are estimated by [54] and extend it with 20K", "startOffset": 32, "endOffset": 36}, {"referenceID": 53, "context": "We train on the training set if [74] where normals are estimated by [54] and extend it with 20K", "startOffset": 68, "endOffset": 72}, {"referenceID": 73, "context": "images of normal ground truth estimated from the raw images in the training scenes of [74]; since the method of [54] is not publicly available, we use as a surrogate the method of [86].", "startOffset": 86, "endOffset": 90}, {"referenceID": 53, "context": "images of normal ground truth estimated from the raw images in the training scenes of [74]; since the method of [54] is not publicly available, we use as a surrogate the method of [86].", "startOffset": 112, "endOffset": 116}, {"referenceID": 85, "context": "images of normal ground truth estimated from the raw images in the training scenes of [74]; since the method of [54] is not publicly available, we use as a surrogate the method of [86].", "startOffset": 180, "endOffset": 184}, {"referenceID": 1, "context": "[2, 25] are using alternative normal estimation methods for the extended data, but we would not expect the differences because of this to be too large.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "[2, 25] are using alternative normal estimation methods for the extended data, but we would not expect the differences because of this to be too large.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "VGG-Cascade [25] 22.", "startOffset": 12, "endOffset": 16}, {"referenceID": 1, "context": "9 VGG-MLP [2] 19.", "startOffset": 10, "endOffset": 13}, {"referenceID": 102, "context": "8 VGG-Design [103] 26.", "startOffset": 13, "endOffset": 18}, {"referenceID": 102, "context": "2 VGG-fused [103] 27.", "startOffset": 12, "endOffset": 17}, {"referenceID": 53, "context": "Table 11: Normal Estimation on NYU-v2 using the ground truth of [54].", "startOffset": 64, "endOffset": 68}, {"referenceID": 102, "context": "Even though our multi-task network\u2019s performance is not too different from the plain CNN-based result of [103], it is clear that there we have a somewhat unique gap in performance, when compared to what we seen in the remaining tasks.", "startOffset": 105, "endOffset": 110}, {"referenceID": 1, "context": "It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2], a coarse-to-fine cascade in [25]).", "startOffset": 63, "endOffset": 66}, {"referenceID": 24, "context": "It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2], a coarse-to-fine cascade in [25]).", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2], a coarse-to-fine cascade in [25]).", "startOffset": 184, "endOffset": 187}, {"referenceID": 24, "context": "It is however interesting that both competing methods (VGG-MLP [2], VGGcascade [25]) address the task by using additional layers on top of the VGG network (a Multi-Layer Perceptron in [2], a coarse-to-fine cascade in [25]).", "startOffset": 217, "endOffset": 221}, {"referenceID": 13, "context": "larger batch sizes, or more iterations and a polynomial schedule as in [14] could help.", "startOffset": 71, "endOffset": 75}, {"referenceID": 39, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 252, "endOffset": 256}, {"referenceID": 10, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 328, "endOffset": 345}, {"referenceID": 13, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 328, "endOffset": 345}, {"referenceID": 60, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 328, "endOffset": 345}, {"referenceID": 112, "context": "There are certain straightforward directions for future work: (i) considering more tasks, such as symmetry, human landmarks, texture segmentation, or any other of the tasks indicated in the introduction (ii) using deeper architectures, such as ResNets [40] (iii) combining the dense labelling results with structured prediction [11, 14, 61, 113].", "startOffset": 328, "endOffset": 345}, {"referenceID": 8, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 46, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 49, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 52, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 67, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 71, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}, {"referenceID": 98, "context": "for integrating segmentation and recognition [9, 47, 50, 53, 68, 72, 99], and we believe that successfully addressing this it is imperative to have a single network that can succesfully handle all of the involved tasks.", "startOffset": 45, "endOffset": 72}], "year": 2016, "abstractText": "In this work we introduce a convolutional neural network (CNN) that jointly handles low-, mid-, and high-level vision tasks in a unified architecture that is trained end-to-end. Such a universal network can act like a \u2018swiss knife\u2019 for vision tasks; we call this architecture an UberNet to indicate its overarching nature. We address two main technical challenges that emerge when broadening up the range of tasks handled by a single CNN: (i) training a deep architecture while relying on diverse training sets and (ii) training many (potentially unlimited) tasks with a limited memory budget. Properly addressing these two problems allows us to train accurate predictors for a host of tasks, without compromising accuracy. Through these advances we train in an end-to-end manner a CNN that simultaneously addresses (a) boundary detection (b) normal estimation (c) saliency estimation (d) semantic segmentation (e) human part segmentation (f) semantic boundary detection, (g) region proposal generation and object detection. We obtain competitive performance while jointly addressing all of these tasks in 0.7 seconds per frame on a single GPU. A demonstration of this system can be found at cvn.ecp.fr/ubernet/.", "creator": "LaTeX with hyperref package"}}}