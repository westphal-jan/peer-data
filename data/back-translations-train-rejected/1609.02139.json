{"id": "1609.02139", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Random Shuffling and Resets for the Non-stationary Stochastic Bandit Problem", "abstract": "We consider a non-stationary formulation of the stochastic multi-armed bandit where the rewards are no longer assumed to be identically distributed. For the best-arm identification task, we introduce a version of Successive Elimination based on random shuffling of the $K$ arms. We prove that under a novel and mild assumption on the mean gap $\\Delta$, this simple but powerful modification achieves the same guarantees in term of sample complexity and cumulative regret than its original version, but in a much wider class of problems, as it is not anymore constrained to stationary distributions. We also show that the original {\\sc Successive Elimination} fails to have controlled regret in this more general scenario, thus showing the benefit of shuffling. We then remove our mild assumption and adapt the algorithm to the best-arm identification task with switching arms. We adapt the definition of the sample complexity for that case and prove that, against an optimal policy with $N-1$ switches of the optimal arm, this new algorithm achieves an expected sample complexity of $O(\\Delta^{-2}\\sqrt{NK\\delta^{-1} \\log(K \\delta^{-1})})$, where $\\delta$ is the probability of failure of the algorithm, and an expected cumulative regret of $O(\\Delta^{-1}{\\sqrt{NTK \\log (TK)}})$ after $T$ time steps.", "histories": [["v1", "Wed, 7 Sep 2016 13:31:21 GMT  (451kb,D)", "http://arxiv.org/abs/1609.02139v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["robin allesiardo", "rapha\\\"el f\\'eraud", "odalric-ambrym maillard"], "accepted": false, "id": "1609.02139"}, "pdf": {"name": "1609.02139.pdf", "metadata": {"source": "META", "title": "SER4", "authors": ["Robin Allesiardo", "Rapha\u00ebl F\u00e9raud", "Odalric-Ambrym Maillard"], "emails": [], "sections": [{"heading": null, "text": "\u221a NK\u03b4 \u2212 1 log (K\u03b4 \u2212 1)), where \u03b4 is the failure probability of the algorithm and an expected cumulative regret of O (\u0445 \u2212 1 \u221a NTK log (TK)) after T-time steps."}, {"heading": "1 Introduction", "text": "The theoretical framework of the multi-armed bandit problem formalizes the basic exploration / exploitation problem that appears in the decision-making problems before partial information. At a high level, a range of K weapons is available to a player. At each corner, she must select an arm and receive a reward that corresponds to the arm played, without knowing what the reward received would have been if she had played another arm. The player faces the dilemma of exploration, that is, she plays an arm whose average reward is usually measured to achieve a better estimate or exploitation, which plays what appears to be the best arm based on current intermediate estimates to maximize her cumulative reward. The accuracy of player policy on Horizon T is typically measured in terms of example complexity or regret. The number of games required to find an approximation of the best arms with a high probability."}, {"heading": "2 Setting", "text": "We are looking at a generalization of the complexity of the test, in which the opponent chooses a sequence of distributions before starting the game, rather than directly selecting a sequence of rewards, generalizing the sequence by arbitrarily selecting a reward yk (t). The reward ykt (t) corresponds to the draw of this reward from a distribution of the middle yk (t) and a variance of zero. Problem: Let [K] = 1,..., K is a sequence of K-arms. The reward ykt (t) that the player receives after playing the arm is drawn from a distribution of the middle yk (t). (0, 1] The instantaneous gap between arm k and k in due time is t: \"k\" (t) def = \"k (t) \u2212 k\" (t), the reward the player receives after playing the arm kt (t). (1) Let k (t) the optimal refinality of the given time. \""}, {"heading": "3 Best Arm Identification in Non-stationary Stochastic Bandits.", "text": "In this section we present the SUCCESSIVE ELIMINATION WITH RANDOMIZED ROUND-ROBIN algorithm (SER3, see algorithm 1), a randomized version of SUCCESSIVE ELIMINATION that addresses the problem of best arm identification when rewards are not stationary."}, {"heading": "3.1 A modified Successive Elimination algorithm", "text": "The elimination mechanism The elimination mechanism was introduced by SUCCESSIVE ELIMINATION [4]. Estimates of the rewards are generated by sequential scanning of the arms. If the lower limit is higher than one of the upper limits, the elimination mechanism begins. A lower limit of the reward of the best empirical arm is calculated and compared with an upper limit of the reward of all other arms. If the lower limit is higher than one of the upper limits, then the associated arm is eliminated and no longer taken into account by the algorithm. Processes of scanning and elimination are repeated until the elimination of all arms except one.Algorithm 1 SUCCESSIVE ELIMINATION WITH RANDOMIZED ROBIN (SER3) Sampling processes: Sampling processes are repeated until the elimination of all arms except one.Algorithm 1 SUCCESSIVE ELIMINATION WITH RANDOMIZED ROBIN (SER3)"}, {"heading": "End if", "text": "We are thus able to calculate confidence in empirical means from non-identical distributions."}, {"heading": "3.2 Analysis", "text": "Sample complexity is the number of observations required to find a -optimal arm with a high probability. All theoretical results are provided for = 0 and therefore accept only k * as the optimal arm. All evidence is provided in the appendix. Theorem 1: For K \u2265 2: In fact, the sample complexity of SER3 is coupled to the upper limit by: O (K \u0445 2: Log (K \u03b4), where we are able to guarantee the cumulative regret. In this case, if only one arm remains in the set, the player continues playing this last arm until the end of the game."}, {"heading": "4 Non-stationary Stochastic Multi-armed Bandit with Best Arm Switches", "text": "The optimal procedure is the sequence of the pairs (optimal switching time): \"We have the switching problems.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" \"We.\" \"\" \"\" We. \"\" \"\" We. \"\" \"We.\" \"\" We. \"\" \"We.\" \"\" We. \".\" \"We.\" \"We.\" \"We.\" \"We.\" \"We.\" \"\" We. \"\" We. \"\" We. \"\" We. \""}, {"heading": "5 Numerical Experiments", "text": "We compare our algorithms with the state of the art. For each problem, K = 20 and T = 107. The current gap between the optimal arm and the others is constant, \u2206 = 0.05, i.e. the mean of the optimal arm is indeed a problem with the least regret. Throughout all the experiments, the probabilities of failure of SUCCESSIVE ELIMINATION (SE), SER3 and SER4 are successfully questioned, while the constant explorations of all algorithms of the EXP3 family are set to \u03b3 = 0.05. The results are averaged over 50 runs. On problems 1 and 2, the deviations are small (in the order of 103) and therefore do not show up. On problem 3, deviations are applied as grey areas under the curtains. Problem 1: sinusoidal means. The index of the optimal arm is drawn before the game and does not change."}, {"heading": "6 Conclusion", "text": "We investigated the use of random shuffling in designing sequential elimination algorithms for bandits. We demonstrated that the use of random shuffling extends their scope to a new class of problems with identifying the best arms in non-stationary distributions without requiring new parameters, while using stationary distributions to achieve the same level of guarantees as SE. We introduced SER3 and extended it to the problem with the switch bandits in SER4 by adding the likelihood of restarting the task to identify the best arms. To our knowledge, we demonstrated the first upper limit for the best arms-based problem in arm switches based on the complexity of the sample. The upper limit on the cumulative regret of SER4 depends only on the number N \u2212 1 of arm switches, as opposed to the number of distribution changes M \u2212 1 in SUCW-M (can be \u2265 T in our order)."}, {"heading": "A Technical results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A.1 Proof of Theorem 1", "text": "The proof consists of three main steps. The first step clarifies the conditions that lead to the elimination of an arm out of line. The second step shows that the optimal arm is not eliminated with a high probability. Finally, the third step shows that a suboptimal arm is eliminated after at most a critical number of steps, which then make it possible to derive an upper limit from the sample. Step 1: Conditions for the elimination of an arm. Hoeffding's inequality results in the expectation regarding the distribution Dy. After target length and arm k we have: P (\"\u00b5\") k \u2212 E [\u00b5 \"k\"]."}, {"heading": "A.2 Proof of Corollary 1", "text": "The accumulated pseudo-regret of the algorithm relates to: R (T) = \u2211 k 6 = k \u043d. (18) Taking into account the expectation of the corresponding random variables kt with respect to the randomization of the round tube (designated by Ekt), the result is: E [R (T)] = E [p) [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K (p) = K (p) = K). (18) Taking into account the expectation of the random variables kt with respect to the randomization of the round tube (designated by Ekt), the result is: E [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K = K = K = K (p) = K (p) = K (p) = K). (18) Taking into account the expectation of the random variable kt with respect to the randomization of the round tube (designated by Ekt), the expectation of the randomization of the round tube results in: E [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K [p) = K = K = K = K = K (p) = K = K (p) = K = K) = K)."}, {"heading": "A.3 Proof of Theorem 2", "text": "To prove theorem 2, we consider the following quantities: \u2022 The expected number of times in which the estimators are reset: Nreset = T. \u2022 The sample complexity required to find the best arm between each reset is SSER3 = O (K \u0445 2 log (K \u03b4 \u2206)). \u2022 The time before a reset following a negative binomial distribution of the parameters r = 1 and p = 1 \u2212 \u0443. Their expectation is limited by 1 / 2. \u2022 The number of arm switches: N \u2212 1. The sample complexity of SER4 is the total number of time steps that are output to add an arm at the time between each reset and each reset. (26) Taking into account the expectation of randomization of the resets, we obtain an upper limit for the expected number of suboptimal repeats given by O (TK \u0445 2 log (K\u043c) + N\u0440). The first term is the expectation of the total number of time steps required after the algorithm."}, {"heading": "A.4 Proof of Corollary 3", "text": "The conversion of Korollar2 into an upper limit dependent on cumulative regret is straightforward by replacing the complexity of the sample in the proof of theory 2 with cumulative regret and using the upper limit of Korollar1.E [R (T)] = O (TK) (KT) + N). (27) Establishing the upper limit (KT) and adopting the upper limit of Korollar1.E [R (T)] = O (TK) = O (NTK log (KT) + N). (28) We also derive below a distribution-independent upper limit. We introduce some notations, Nreset is the number of resets, Reseti is the number of round-robin phases between the ith and the (i + 1) th resets, and Ln is the number of time intervals before a reset."}], "references": [{"title": "EXP3 with Drift Detection for the Switching Bandit Problem", "author": ["Allesiardo", "Robin", "F\u00e9raud", "Rapha\u00ebl"], "venue": "IEEE International Conference on Data Science", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Finite-time Analysis of the Multiarmed Bandit Problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Fischer", "Paul"], "venue": "Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2002}, {"title": "The Nonstochastic Multiarmed Bandit Problem", "author": ["Auer", "Peter", "Cesa-Bianchi", "Nicol\u00f2", "Freund", "Yoav", "Schapire", "Robert E"], "venue": "SIAM J. Comput.,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2002}, {"title": "Action Elimination and Stopping Conditions for the Multi-Armed Bandit and Reinforcement Learning Problems", "author": ["Even-Dar", "Eyal", "Mannor", "Shie", "Mansour", "Yishay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2006}, {"title": "Random Forest for the Contextual Bandit Problem. AISTATS", "author": ["F\u00e9raud", "Rapha\u00ebl", "Allesiardo", "Robin", "Urvoy", "Tanguy", "Cl\u00e9rot", "Fabrice"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "On Upper-Confidence Bound Policies for Non-stationary Bandit Problems. Pages 174\u2013188", "author": ["Garivier", "Aur\u00e9lien", "Moulines", "Eric"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Multi-armed Bandit, Dynamic Environments and Meta-Bandits", "author": ["C. Hartland", "N. Baskiotis", "S. Gelly", "O. Teytaud", "M. Sebag"], "venue": "Online Trading of Exploration and Exploitation Workshop,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2006}, {"title": "On the Complexity of Best Arm Identification in Multi-Armed Bandit Models", "author": ["Kaufmann", "Emilie", "Capp\u00e9", "Olivier", "Garivier", "Aur\u00e9lien. Jan"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Discounted UCB. In: 2nd PASCAL Challenges Workshop", "author": ["L. Kocsis", "C. Szepesv\u00e1ri"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2006}, {"title": "Explore no more: improved high-probability regret bounds for non-stochastic bandits", "author": ["Neu", "Gergely"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "One Practical Algorithm for Both Stochastic and Adversarial Bandits", "author": ["Seldin", "Yevgeny", "Slivkins", "Aleksandrs"], "venue": "In: 31th Intl. Conf. on Machine Learning (ICML)", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}], "referenceMentions": [{"referenceID": 1, "context": "Stochastic algorithms usually assume distributions to be constant over time like with the Thompson Sampling (TS) [13], UCB [2] or Successive Elimination (SE) [4].", "startOffset": 123, "endOffset": 126}, {"referenceID": 3, "context": "Stochastic algorithms usually assume distributions to be constant over time like with the Thompson Sampling (TS) [13], UCB [2] or Successive Elimination (SE) [4].", "startOffset": 158, "endOffset": 161}, {"referenceID": 2, "context": "The EXP3 algorithm [3, 11] achieves an optimal regret ofO( \u221a T ) against an oblivious opponent that chooses rewards before the beginning of the game, with respect to the best policy that pulls the same arm over the totality of the game.", "startOffset": 19, "endOffset": 26}, {"referenceID": 9, "context": "The EXP3 algorithm [3, 11] achieves an optimal regret ofO( \u221a T ) against an oblivious opponent that chooses rewards before the beginning of the game, with respect to the best policy that pulls the same arm over the totality of the game.", "startOffset": 19, "endOffset": 26}, {"referenceID": 2, "context": "S [3], a variant of EXP3, that forgets the past adding at each time step a proportion of the mean gain and achieves controlled regret with respect to policies that allow arm switches during the run.", "startOffset": 2, "endOffset": 5}, {"referenceID": 8, "context": "Discounted UCB [10] and sliding-window UCB [6] are adaptations of UCB to the switching bandit problem and achieve a regret bound of O( \u221a MT log T ), where M \u2212 1 is the number of distribution changes.", "startOffset": 15, "endOffset": 19}, {"referenceID": 5, "context": "Discounted UCB [10] and sliding-window UCB [6] are adaptations of UCB to the switching bandit problem and achieve a regret bound of O( \u221a MT log T ), where M \u2212 1 is the number of distribution changes.", "startOffset": 43, "endOffset": 46}, {"referenceID": 0, "context": "R [1] combines EXP3 with a switch detector based on Hoeffding inequality [8] in order to detect switches of best arm.", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "It is also worth citing META-EVE [7] that associates UCB with a mean change detector, reseting the algorithm when a change is detected.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "Several variants combining stochastic and adversarial rewards have been proposed by Seldin & Slivkins [12].", "startOffset": 102, "endOffset": 106}, {"referenceID": 10, "context": "They also proposed another variant called adversarial with gap [12] which assumes the existence of a round after which an arm persists to be the best.", "startOffset": 63, "endOffset": 67}, {"referenceID": 0, "context": "The reward ykt(t) \u2208 [0, 1] obtained by the player after playing the arm kt is drawn from a distribution of mean \u03bckt(t) \u2208 [0, 1].", "startOffset": 20, "endOffset": 26}, {"referenceID": 0, "context": "The reward ykt(t) \u2208 [0, 1] obtained by the player after playing the arm kt is drawn from a distribution of mean \u03bckt(t) \u2208 [0, 1].", "startOffset": 121, "endOffset": 127}, {"referenceID": 7, "context": "In the literature [9], the sample-complexity of an algorithm is the number of samples needed by this algorithm to find a policy achieving a specific level of performance with high probability.", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "Analysis in sample complexity are useful for building hierarchical models of contextual bandits in a greedy way [5], reducing the exploration space.", "startOffset": 112, "endOffset": 115}, {"referenceID": 3, "context": "The elimination mechanism The elimination mechanism was introduced by SUCCESSIVE ELIMINATION [4].", "startOffset": 93, "endOffset": 96}, {"referenceID": 10, "context": "It is quite similar to the assumption used by Seldin & Slivkins [12] to be able to achieve logarithmic expected regret on moderately contaminated rewards, i.", "startOffset": 64, "endOffset": 68}, {"referenceID": 10, "context": "Another analogy can be done with the adversarial with gap setting [12], \u03c4min representing the time needed for the optimal arm to accumulate enough rewards and to distinguish itself from the suboptimal arms.", "startOffset": 66, "endOffset": 70}, {"referenceID": 5, "context": "They do not contradict the lower bound for non-stationary bandit whose scaling is in \u03a9( \u221a T ) [6] as it is due to the cost of the constant exploration for the case where the best arm changes.", "startOffset": 94, "endOffset": 97}, {"referenceID": 5, "context": "[6] and assumes means to be stationary between switches.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "R [1] to allow mean rewards to change at every time-steps.", "startOffset": 2, "endOffset": 5}, {"referenceID": 0, "context": "In order to allow the algorithm to choose another arm when a switch occurs, at each turn, estimators of SER3 are reseted with a probability \u03c6 \u2208 [0, 1] and", "startOffset": 144, "endOffset": 150}, {"referenceID": 0, "context": "R (see Corrolary 1 in [1]) and in SW-UCB (see Theorem 1 in [6]), and is standard in this type of results.", "startOffset": 22, "endOffset": 25}, {"referenceID": 5, "context": "R (see Corrolary 1 in [1]) and in SW-UCB (see Theorem 1 in [6]), and is standard in this type of results.", "startOffset": 59, "endOffset": 62}], "year": 2016, "abstractText": "We consider a non-stationary formulation of the stochastic multi-armed bandit where the rewards are no longer assumed to be identically distributed. For the best-arm identification task, we introduce a version of SUCCESSIVE ELIMINATION based on random shuffling of the K arms. We prove that under a novel and mild assumption on the mean gap \u2206, this simple but powerful modification achieves the same guarantees in term of sample complexity and cumulative regret than its original version, but in a much wider class of problems, as it is not anymore constrained to stationary distributions. We also show that the original SUCCESSIVE ELIMINATION fails to have controlled regret in this more general scenario, thus showing the benefit of shuffling. We then remove our mild assumption and adapt the algorithm to the best-arm identification task with switching arms. We adapt the definition of the sample complexity for that case and prove that, against an optimal policy with N \u22121 switches of the optimal arm, this new algorithm achieves an expected sample complexity ofO(\u2206\u22122 \u221a NK\u03b4\u22121 log(K\u03b4\u22121)), where \u03b4 is the probability of failure of the algorithm, and an expected cumulative regret of O(\u2206\u22121 \u221a NTK log(TK)) after T time steps.", "creator": "LaTeX with hyperref package"}}}