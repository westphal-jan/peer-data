{"id": "1609.02226", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Sep-2016", "title": "Fitted Learning: Models with Awareness of their Limits", "abstract": "Though deep learning has pushed the boundaries of classification forward, in recent years hints of the limits of standard classification have begun to emerge. Problems such as fooling, adding new classes over time, and the need to retrain learning models only for small changes to the original problem all point to a potential shortcoming in the classic classification regime, where a comprehensive a priori knowledge of the possible classes or concepts is critical. Without such knowledge, classifiers misjudge the limits of their knowledge and overgeneralization therefore becomes a serious obstacle to consistent performance. In response to these challenges, this paper extends the classic regime by reframing classification instead with the assumption that concepts present in the training set are only a sample of the hypothetical final set of concepts. To bring learning models into this new paradigm, a novel elaboration of standard architectures called the competitive overcomplete output layer (COOL) neural network is introduced. Experiments demonstrate the effectiveness of COOL by applying it to fooling, separable concept learning, one-class neural networks, and standard classification benchmarks. The results suggest that, unlike conventional classifiers, the amount of generalization in COOL networks can be tuned to match the problem.", "histories": [["v1", "Wed, 7 Sep 2016 23:59:36 GMT  (822kb,D)", "https://arxiv.org/abs/1609.02226v1", null], ["v2", "Sun, 11 Sep 2016 06:34:56 GMT  (820kb,D)", "http://arxiv.org/abs/1609.02226v2", null], ["v3", "Wed, 4 Jan 2017 17:14:41 GMT  (820kb,D)", "http://arxiv.org/abs/1609.02226v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["navid kardan", "kenneth o stanley"], "accepted": false, "id": "1609.02226"}, "pdf": {"name": "1609.02226.pdf", "metadata": {"source": "CRF", "title": "Fitted Learning: Models with Awareness of their Limits", "authors": ["Navid Kardan", "Kenneth O. Stanley"], "emails": ["kardan@knights.ucf.edu,", "kstanley@cs.ucf.edu"], "sections": [{"heading": "Introduction", "text": "This year, more than ever before in the history of the city, where it is so far that it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, where it is a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, where it is a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a place, a, a place, a place, a place, a place, a place, a place, a place, a, a place, a place, a place, a place, a, a, a place, a place, a place, a, a place, a place, a place, a, a, a place, a place, a, a, a place, a, a place, a place, a, a place, a place, a"}, {"heading": "Related Work", "text": "The next conceptual framework for applied learning is probably the Open Set Recognition Problem (OSRP), which is highly discriminatory (Jain, Scheirer, and Boult, 2014) in the vision community, where unknown classes appear in the test phase and the learning model should be able to detect such unknowns. But, as a recognition task, each learning model is focused on a single class, while the goal of paired learning is to treat multiple classes at the same time. That is, a paired model should be able to capture the data distribution for each class."}, {"heading": "Approach", "text": "COOL de facto expands the traditional output level by assigning multiple output units to each class or concept. Furthermore, the compulsion to force this group of neurons to compete ensures its diversity by dividing the entrance space between output units of the same class. Consequently, these output units ultimately only match in regions located near the training instances. In other words, unlike traditional architectures, COOL captures the distribution of the training instances of each class simultaneously with learning to differentiate between instances of different classes. The section begins with a description of the base unit of COOL, the neuron aggregate, and the rest of the section then describes the proposed new architecture for the output layers."}, {"heading": "Neuron Aggregate", "text": "A neuron aggregate is a collection of units, the so-called \"member units,\" which aims to learn a single concept. In this sense, a neuron aggregate acts as a whole and is inspired by the hypothesis (as opposed to the \"granny cell\") that in biological neural networks more than one neuron is involved in the recognition of each concept (Gross, 2002; Gazzaniga, 2004). An internally competing aggregate is a neuron aggregate whose activation is mutually inhibited (e.g. by mutual inhibition) and modelled in COOL by a Softmax function applied to member units. Consequently, it is sufficient that a neuron aggregate is part of a Softmax layer to become an internally competing aggregate."}, {"heading": "The Competitive Overcomplete Output Layer (COOL)", "text": "It's not just the way in which it's about the question of whether it's about the way it's about, but also the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way it's about the way in which it's about the way in which it's about the way it's about the way it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way in which it's about the way it's about the way it's about which"}, {"heading": "Visualization Experiments", "text": "In this section, a low-dimensional artificial dataset helps visualize the behavior of COOL networks and compare it with traditional neural networks. These experiments suggest that COOL networks are able to successfully capture the underlying distribution of training instances while maintaining the generalizability of conventional neural networks. In other words, they construct an idea of the limits of their knowledge; they recognize instances that are not within their competence (i.e. far from the training points) and behave like a traditional learning model."}, {"heading": "Problem Definition", "text": "In fact, most of them will be able to orient themselves in a different direction from the one they are going in."}, {"heading": "Why the COOL Mechanism Works", "text": "This section explains how the two components of the COOL setup, i.e. overcompleteness and competition, can work together to counteract over-generalization. In particular, it turns out that the over-complete architecture brings together an exponential number of models to simultaneously train, while the competition preserves the diversity in the member units of a neuron aggregate trying to learn the same concept.The argument begins with an aggregate X with n member units, X = {x1, x2,..., xn}."}, {"heading": "Overcompleteness", "text": "The idea behind the supercomplete layer is that it creates a dynamic similar to the formation of an exponential number of models at once, reflecting the motivation behind the dropout (Srivastava et al., 2014). Suppose that the output of each member unit is scaled just before multiplication in the test phase, i.e. the output of the member unit xi-i-i-1,..., n} is transformed according to S (x) = max (\u03c9 \u00b7 x, 1), where \u03c9 is DOO.In this finding, the output of each member unit can be interpreted as a probability value. Furthermore, any subset of an aggregate, with the exception of the empty aggregate, could in principle replace the original aggregate as the decision maker. Consequently, training an aggregate is difficult because it is analogous to training 2n \u2212 1 neuron subaggregates with the same activation behavior."}, {"heading": "Competition", "text": "Remember that during the formation of a particular instance, there is only one active aggregate, i.e. the aggregate, whose member units are trained for non-zero values. As all member units are trained in the active aggregate instance to learn the same function, one might expect them to converge to the same weights for each neuron in the aggregate. Interestingly, the competition induced by Softmax prevents such an outcome.To elaborate, one must assume that X is the current active aggregate for the input instance, that there are different member units and in practice in gradient descent there is at least one non-zero gradient 4 among xi units. The Softmax function then implies that within a neighborhood N of A units of the input instance, the different member units and in practice in gradient descent."}, {"heading": "The Fooling Problem", "text": "The most important observation is that neural networks can be made to confidently issue the wrong class for cases far beyond their training distribution. In other words, the fool problem is a direct consequence of over-generalization, which is more pronounced in deep architectures. To clarify the fool problem, let's start with a simple example: The classification task is to determine whether a fruit is a watermelon or an apple based on the weight of the fruit in kilograms. A possible training group is then T = (0.25, apple), (7.3, watermelon). In this example, a discriminatory model, such as an SVM, will find a decision limit that can be translated into if weight > 2.75 then watermelon."}, {"heading": "Reasons Behind Fooling", "text": "Goodfellow, Shlens, and Szegedy (2015) postulate a linear explanation for the existence of adversarial examples, in which small disturbances of an existing training example can lead to significant changes in the output of a high-dimensional problem. They also extend this explanation in their appendix to the more general case of any narrative example, i.e. one that does not necessarily resemble a training example, as in Nguyen, Yosinski, and Clune (2015). This paper (and COOL) focuses on the latter case. Generally, we assume that training cases are selected from an unknown probability distribution. Consider the classification of any input example x by the neural network N; x is a foolhardy example when classified by the network N with high reliability, while it is not drawn from the probability distribution D. A key factor behind fooling is then that discriminatory classifiers tend to generalize."}, {"heading": "Generating Fooling Instances", "text": "Previous methods of generating images from Szegedy et al. (2013) and Nguyen, Yosinski and Clune (2015) directly look for images that deceive the network. In contrast, here we introduce a third option that does not require a solution to a limited optimization problem: A random input instance x is fed into a new trainable neural network g, which is called a fooling generator network (FGN), the output of which is passed to the actual model f. In other words, g (x) is the foolhardy input of properties into the network instead of x. Grade descent can form the network g in such a way that g (x) generates a good fooling image. During this process, x and parameters of f are fixed and only g are trained. More formally, for: < n \u2192 [0, 1] k is an imaging of input vectors to a true target vector space, < a task that is insolvable."}, {"heading": "MNIST Fooling Experiment", "text": "In this experiment, a COOL network and a conventional CNN are first trained on the MNIST dataset. Then, 20 studies attempt to generate foolhardy instances for each model. One attempt is to train an FGN that is activated with a random input to trick the model, and is considered successful if the model classifies the foolhardy image generated with more than 99% confidence. However, if such a foolhardy example is not found before 10,000 parameter updates of the FGN, then the attempt is a failure. Further details of this experiment can be found in the appendix. COOL preserves the generalization capability of CNNs for this task: The classification accuracy of COOL and conventional foolhardy foolhardy-like CNNs was 99.18% and 99.14%, respectively. At the same time, in 200 studies (20 for each digit) the FGN approach preserves the generalization capability of CNNs for this task: The FGN narration accuracy of 914 of similar foolharms and 914 respectively."}, {"heading": "Fitted Learning and Separable Concept Learning", "text": "So far, COOL has been offered as a mechanism to prevent over-generalization in neural networks. Consequently, COOL networks can prevent deception or at least make it relatively difficult for an adversary to do so. A natural question that arises from this is how to measure the extent of over-generalization in a model. In this section, such a measure is defined by assuming access to a finite collection of i.i.d. samples in a training set, T = {(xi, yi) ni = 1 \u0445 D (X, Y), where X is the entrance hall of the training set T, Y is the production room, and D is the data distribution over X \u00d7 Y. In canonical classification theory, we assume that Y is a finite set of objects of size c.Given an undetermined negative function Y that is defined as a hypothesis."}, {"heading": "Introducing the Generalized Classification Problem", "text": "In conventional classification, a learning model simply draws a line between different concepts. A disadvantage of this strategy, as already discussed, is inherent over-generalization. Generalized classification introduced here addresses this problem by asking for the right level of generalization. Moreover, in many real-world applications, extensive knowledge of the concepts is not available during training, or worse, some new concepts may emerge over time. Another aspect of conventional classification is its intuitive interpretation. It is a fair assumption that classifiers are more confident when dealing with familiar test substances, while their confidence is degraded in the midst of novel cases. However, we counter-intuitively assume that learning models base the confidence of the test instance solely on the distance from the decision limit and its similarity to the training substances."}, {"heading": "Generalized Classification and Fooling", "text": "The disabling of output units for points not in the vicinity of training data in the first experiments in the two-dimensional space of the two-circle domain suggested that COOL networks are able to provide strong hypotheses, and further experiments showed that they can significantly reduce deception maneuvers. It is worth highlighting the relationship between strong hypotheses and deceptive maneuvers. To deceive the learning model g, one should find an instance x-x-D (x-y) in such a way that g (x) does not carry great risks. However, if g is by definition a strong hypothesis with low extended risk, it tends to assign such an x-to-\u03b5. Therefore, the closer a learning model is to a strong hypothesis with low extended risk, the less likely it is to be deceived. Note that in this discussion we differentiate between contradictory examples drawn by D (x-y) but not correctly classified by example, which are shortened by example."}, {"heading": "Separable Concept Learning", "text": "Unfortunately, it is not easy to formulate the generalized classification problem based on an approximation of the extended risk using examples in the training set, since we have only partial knowledge of Y. However, it is possible to evaluate the performance of a strong hypothesis within the framework of classical statistical learning; however, this evaluation consists of two components: (1) model performance on instance x, if x-D (x-y), and (2) if x-D (x-y = \u03b5); we call the latter the inhibition capacity of the model because it must impair its tendency to activate at least one output class. While the first component can be evaluated by any classification metric, for the inhibition capacity we introduce the concept of separable learning (SCL), the advantage of this new metric over previous evaluation methods is that it is applied in open recognition and object tasks. (Scheirer et al, 2013) is that it is not based on any threshold of the inconsistency of the model."}, {"heading": "MNIST Separable Concept Learning Experiment", "text": "In order to evaluate the feasibility of SCL and consolidate the results of the Fool Section, a series of new experiments with the MNIST dataset are being conducted, which show that the COOL mechanism can dramatically improve the ability of neural networks to learn subsets of concepts separately. These results also support that COOL networks are capable of successfully preventing over-generalization in high-dimensional spaces."}, {"heading": "Experimental Setup", "text": "Five examples of each variant are then trained on different subsets of the MNIST. Specifically, we divide the training set T into five subsets, T1, T2, T3, T4 and T5, with T1 = {(x, y) as well as T | y, 0, 1, T2 = (x, y), T | y, 2, 3, etc. Then we train one CNN from each architecture on T1, one on T2, and so on. In other words, \u03c1 = {T1, T2, T3, T4, T5} in Equation 7. Further training details can be found in the appendix."}, {"heading": "Results", "text": "Figure 11 shows the average test / validation error (equation 7) of conventional CNN versus COOL CNN over several eras. Here, the COOL models significantly outperform conventional CNNs with an improvement in detection rate of about 13% (over ten runs). Figure 11 also shows the accuracy of the MLP architectures (over two runs) in the same task, where COOL alone achieves a performance improvement of about 25%. Interestingly, COOL MLP (without folding) achieves a competitive performance in this task compared to conventional CNN. Overall, these results suggest that COOL can significantly improve the inhibition capability of learning models, which in turn leads to a more accurate representation of the knowledge embedded in the dataset and the robustness of the concepts learned."}, {"heading": "One-class Neural Network", "text": "This section is based on the COOL architecture and follows a similar approach to the SVM single-class method and proposes single-class networks, another possibility created by COOL. This alternative single-class model enables the direct application of deep learning methods in the field of single-class recognition. The following experiments suggest that this approach is promising and may allow neural networks to penetrate into the problematic area of estimating the support of probability distributions."}, {"heading": "Approach", "text": "To grasp the underlying distribution of a set of blank data, Scho \ufffd lkopf et al. (2001) proposed the single-class method SVM, in which the origin is treated as the only member of the second class. The algorithm then tries to find a maximum margin between the mapped characteristics of the instances of the first class and the origin. To our knowledge, this version and its extensions are the only standard methods for grasping an idea of data distribution in high-dimensional spaces (density estimation techniques are usually not effective in high-dimensional spaces due to the curse of dimensionality (Scott, 2008) and are applied, among other things, to the detection of anomalies. A COOL network can similarly construct single-class neural networks by including one (or more) instances that do not lie within a single class (Scott, 2008). In other words: According to the same notation introduced in the previous section, the members of the class T (or second class T) of the class T:"}, {"heading": "Experiments", "text": "The first experiment presents a similar problem to the two-circle problem in the field of visualization experiments. Specifically, the points are sampled uniformly within a circle. However, the result of this experiment is not primarily the selection of the two output aggregates (DOO = 5), but the distribution of the individual output aggregates in classes one and two. The result of this experiment is also that typical neural networks are used instead of COOL systems."}, {"heading": "CIFAR-10 and CIFAR-100 Experiments", "text": "Another potential advantage of COOL is its ability to train larger architectures. As already mentioned, COOL actually builds up an exponential number of competing models, and a fascinating consequence is that it is possible to train very large architectures without revising them. As the results in this section will show, this property alone can sometimes lead to performance improvements. Another important question is the convergence rate at COOL. Does aggressive competition in the output layer of 5Jain, Scheirer and Boult (2014) not lead to a rejection rate in their report. These networks impede convergence or vice versa to more robust updating of parameters in the training phase and therefore speed up learning? In this section, we will conduct experiments with different network architectures on the CIFAR-10 and CIFAR-100 datasets to evaluate the effects of COOL on the convergence rate in this training phase and check their synchronizability with the previous COOL processes."}, {"heading": "CIFAR-10 Experiment", "text": "In this experiment, on the first 45,000 training instances of the CIFAR-10 dataset, a CNN with five winding layers followed by two fully bonded layers is trained, each hidden layer is followed by a stack normalization layer, and ReLU is used as activation functions in all layers. Further details are given in the appendix. Figure 14 shows the average classification accuracy over ten runs of the COOL network (with DOO = 20) compared to ten runs of a conventional network with the same architecture, with the exception of the last layer. The main result is that COOL networks converge more than two times faster and perform significantly better (t-test returns a p-value < 0.001). Preliminary experiments with a variety of architectures have produced similar results."}, {"heading": "CIFAR-100 Experiment", "text": "In this experiment, a more comprehensive architecture is applied to the CIFAR 100 dataset. Unique features of this experiment are the application of Dropout and 6http: / / torch.ch / a light augmentation of the dataset using the same procedure as in Clevert, Unterthiner and Hochreiter (2016), where each image is augmented by four zero pixels at all boundaries, randomly cropped to extract a 32 \u00d7 32 image, and also randomly flipped horizontally. To keep the training process simple, the dropout rate is kept constant and no momentum or weight loss is applied. Further details can be found in the appendix. The classification accuracy of the conventional versus the COOL neural network without dropout across different stages of the run is shown in Figure 15. These results suggest that Dropout significantly slows down the learning process while improving performance."}, {"heading": "Discussion and Future Work", "text": "In fact, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves in the world, and we are able to assert ourselves in the world, we are able to assert ourselves, we are in the world."}, {"heading": "Appendix", "text": "The COOL architecture introduces two new hyperparameters, DOO and softness. Softness is always 1.0, except for the individual visualization experiment, which is supposed to show its impact on the decision boundary. Depending on the dataset type and problem, the DOO is typically 5, 10, or 20. Because it is basically the number of input space partitions, the two-circle problem with only two dimensions does not require as many partitions by COOL member units as in the 28 x 28 MNIST. For tasks such as the single-class neural network, a higher value makes sense due to the high data imbalance (and therefore tends to overgeneralize), but generally we do not optimize this parameter systematically. Most experiments target simple architectures, with the exception of CIFAR-10 and CIFAR-100, where state-of-the-art architectures are being tested."}, {"heading": "Visualization Experiments", "text": "The two-circle problem consists of 1,258 dots, scanned evenly from two concentric circles with different radii. Table 2 lists the various feed-forward architectures used in this essay to generate the visualizations."}, {"heading": "Experiments with MNIST", "text": "The MNIST database is a popular benchmark for machine learning, consisting of 60,000 training sessions and 10,000 test images. In all experiments, the first 50,000 training images are used as a training set and the rest as validation. Neural network architectures are kept simple (most of them inspired by LeNet-5) and all hidden units have a saturating activation function (logistics). In this process, we simply transform all pixel intensity values by substituting them for a division into 255 groups. Finally, the learning rate is constant and the weight initialization follows Glorot and Benno, who focus on themselves."}, {"heading": "Experiments on CIFAR-10", "text": "The CIFAR-10 dataset consists of 50,000 training images and 10,000 tests with 3 x 32 x 32 color images from 10 categories of some animal and transport media. The validation set consists of the last 5,000 images of the training images excluded from the training set. For pre-processing, we simply subtract the mean and divide the standard deviation of each pixel location from the pixel values. The architecture is: [Conv (3 \u2192 16, 3 x 3), Conv (16 \u2192 32, 5 x 5), maxPooling (2 x 2), Conv (32 \u2192 64, 3 x 3), Conv (64 \u2192 128, 5 x 5), maxPooling (2 x 2), Conv (128 \u2192 256, 3 x 3), fullyConnected (256 \u2192 800), fullyConnected (800 \u2192 10 x DOO)], with all layers (except output and pooling layers) following the normalization and / or LU-1 function."}, {"heading": "Experiments on CIFAR-100", "text": "The CIFAR-100 dataset is another dataset with small images, similar to the CIFAR-10, but containing 100 categories with the same total number of images. The experiment with CIFAR-100 applies the same pre-processing as with CIFAR-10, but to image derivatives (in relation to x) instead of raw pixel values. The architecture is: [Conv (3 \u2192 128, 1 \u00d7 1), Conv (128 \u2192 512, 3 \u00d7 3), Dropout (0.2 \u00d7 3), Dropout (0.1), Dropout (0.2 \u00d7 2), Conv (2 \u00d7 2), Conv (3072 \u2192 1024, 1 \u00d7 1), Conv (1024 \u2192 3072, 3 \u2192 512, 3 \u00d7 3), Dropout (0.2 \u00d7 3), Dropout (0.1 \u00d7 80.1), Conv (80.2), 8x7v (0.3), 80.7 (80.3), Conv (80.7), Conv (80.7), Conv (80.7), Conv (80.7), Conv (80.7), Conv (80.7), Conv (80.7), Conv (80.7)."}], "references": [{"title": "Fast and accurate deep network learning by exponential linear units (ELUs)", "author": ["D. Clevert", "T. Unterthiner", "S. Hochreiter"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Clevert et al\\.,? 2016", "shortCiteRegEx": "Clevert et al\\.", "year": 2016}, {"title": "Pattern Classification", "author": ["R.O. Duda", "P.E. Hart", "D.G. Stork"], "venue": "John Wiley & Sons.", "citeRegEx": "Duda et al\\.,? 2012", "shortCiteRegEx": "Duda et al\\.", "year": 2012}, {"title": "The use of multiple measurements in taxonomic problems", "author": ["R.A. Fisher"], "venue": "Annals of Eugenics 7(2):179\u2013188.", "citeRegEx": "Fisher,? 1936", "shortCiteRegEx": "Fisher", "year": 1936}, {"title": "The cognitive neurosciences", "author": ["M.S. Gazzaniga"], "venue": "MIT press.", "citeRegEx": "Gazzaniga,? 2004", "shortCiteRegEx": "Gazzaniga", "year": 2004}, {"title": "Object naming, vocabulary growth, and the development of word retrieval abilities", "author": ["L. Gershkoff-Stowe"], "venue": "Journal of Memory and Language 46(4):665\u2013687.", "citeRegEx": "Gershkoff.Stowe,? 2002", "shortCiteRegEx": "Gershkoff.Stowe", "year": 2002}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["X. Glorot", "Y. Bengio"], "venue": "AISTATS, volume 9, 249\u2013256.", "citeRegEx": "Glorot and Bengio,? 2010", "shortCiteRegEx": "Glorot and Bengio", "year": 2010}, {"title": "Maxout networks", "author": ["I.J. Goodfellow", "D. Warde-Farley", "M. Mirza", "A.C. Courville", "Y. Bengio"], "venue": "ICML (3) 28:1319\u20131327.", "citeRegEx": "Goodfellow et al\\.,? 2013", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2672\u20132680.", "citeRegEx": "Goodfellow et al\\.,? 2014", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "author": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "13th International Conference on Document Analysis and Recognition, ICDAR 2015, Nancy, France, August 23-26, 2015.", "citeRegEx": "Goodfellow et al\\.,? 2015", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2015}, {"title": "Fractional max-pooling", "author": ["B. Graham"], "venue": "arXiv preprint arXiv:1412.6071.", "citeRegEx": "Graham,? 2014", "shortCiteRegEx": "Graham", "year": 2014}, {"title": "Genealogy of the \u201cgrandmother cell", "author": ["C.G. Gross"], "venue": "The Neuroscientist 8(5):512\u2013518.", "citeRegEx": "Gross,? 2002", "shortCiteRegEx": "Gross", "year": 2002}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Multiclass open set recognition using probability of inclusion", "author": ["L.P. Jain", "W.J. Scheirer", "T.E. Boult"], "venue": null, "citeRegEx": "Jain et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jain et al\\.", "year": 2014}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "Technical report, University of Toronto.", "citeRegEx": "Krizhevsky and Hinton,? 2009", "shortCiteRegEx": "Krizhevsky and Hinton", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Deeply-supervised nets", "author": ["C.-Y. Lee", "S. Xie", "P. Gallagher", "Z. Zhang", "Z. Tu"], "venue": "AISTATS.", "citeRegEx": "Lee et al\\.,? 2015", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "International Conference on Learning Representations.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 427\u2013 436. IEEE.", "citeRegEx": "Nguyen et al\\.,? 2015", "shortCiteRegEx": "Nguyen et al\\.", "year": 2015}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV) 115(3):211\u2013252.", "citeRegEx": "Russakovsky et al\\.,? 2015", "shortCiteRegEx": "Russakovsky et al\\.", "year": 2015}, {"title": "Toward open set recognition", "author": ["W.J. Scheirer", "A. d. R. Rocha", "A. Sapkota", "T.E. Boult"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 35(7):1757 \u2013 1772.", "citeRegEx": "Scheirer et al\\.,? 2013", "shortCiteRegEx": "Scheirer et al\\.", "year": 2013}, {"title": "Estimating the support of a high-dimensional distribution", "author": ["B. Sch\u00f6lkopf", "J.C. Platt", "J.C. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural Computation 13(7):1443\u20131471.", "citeRegEx": "Sch\u00f6lkopf et al\\.,? 2001", "shortCiteRegEx": "Sch\u00f6lkopf et al\\.", "year": 2001}, {"title": "The curse of dimensionality and dimension reduction", "author": ["D.W. Scott"], "venue": "Multivariate Density Estimation: Theory, Practice, and Visualization 195\u2013217.", "citeRegEx": "Scott,? 2008", "shortCiteRegEx": "Scott", "year": 2008}, {"title": "Striving for simplicity: The all convolutional net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1412.6806.", "citeRegEx": "Springenberg et al\\.,? 2014", "shortCiteRegEx": "Springenberg et al\\.", "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training very deep networks", "author": ["R.K. Srivastava", "K. Greff", "J. Schmidhuber"], "venue": "Advances in neural information processing systems, 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "venue": "arXiv preprint arXiv:1312.6199.", "citeRegEx": "Szegedy et al\\.,? 2013", "shortCiteRegEx": "Szegedy et al\\.", "year": 2013}, {"title": "The nature of statistical learning theory", "author": ["V. Vapnik"], "venue": "Springer Science & Business Media.", "citeRegEx": "Vapnik,? 2013", "shortCiteRegEx": "Vapnik", "year": 2013}], "referenceMentions": [{"referenceID": 19, "context": "2 million images spanning one thousand of different classes, success is measured on a separate 150,000 images in the test set (Russakovsky et al., 2015).", "startOffset": 126, "endOffset": 152}, {"referenceID": 19, "context": "9% on the test set (Russakovsky et al., 2015), a level recently exceeded by deep learning algorithms (He et al.", "startOffset": 19, "endOffset": 45}, {"referenceID": 11, "context": ", 2015), a level recently exceeded by deep learning algorithms (He et al., 2016).", "startOffset": 63, "endOffset": 80}, {"referenceID": 26, "context": "Deep learning, in contrast, has proven easily fooled by such images into assigning them to a class with over 99% confidence, a phenomenon called fooling (Szegedy et al., 2013; Nguyen, Yosinski, and Clune, 2015).", "startOffset": 153, "endOffset": 210}, {"referenceID": 4, "context": "cabulary (Gershkoff-Stowe, 2002).", "startOffset": 9, "endOffset": 32}, {"referenceID": 15, "context": "Experiments in this paper in the MNIST digitclassification domain (LeCun et al., 1998) confirm that the COOL approach not only mitigates fooling as expected, but also indeed enables combining multiple separatelylearned classifiers into one, and even makes possible effective one-class neural networks, all of which are otherwise prohibitive with conventional convolutional neural networks (CNNs) (LeCun et al.", "startOffset": 66, "endOffset": 86}, {"referenceID": 15, "context": ", 1998) confirm that the COOL approach not only mitigates fooling as expected, but also indeed enables combining multiple separatelylearned classifiers into one, and even makes possible effective one-class neural networks, all of which are otherwise prohibitive with conventional convolutional neural networks (CNNs) (LeCun et al., 1998).", "startOffset": 317, "endOffset": 337}, {"referenceID": 13, "context": "Moreover, COOL networks actually outperform CNNs with identical architectures (except of course the outputs) in CIFAR-10 and CIFAR-100 (Krizhevsky and Hinton, 2009), not only reaching a higher level of test accuracy, but converging significantly faster (even with the greater number of outputs) thanks to the more opportunistic steps taken by COOL networks through gradient descent.", "startOffset": 135, "endOffset": 164}, {"referenceID": 20, "context": "The closest conceptual framework to fitted learning is probably the open set recognition problem (OSRP) (Scheirer et al., 2013; Jain, Scheirer, and Boult, 2014) in the vision community, where unknown classes may appear in the testing phase and the learning model should be able to detect such unknowns.", "startOffset": 104, "endOffset": 160}, {"referenceID": 21, "context": "An established method that is applied in OSRP and anomaly detection (amongst others) is the one-class SVM (Sch\u00f6lkopf et al., 2001), which tries to capture the data distribution, or more technically the support of the probability distribution, by turning the one-class problem into a classification problem.", "startOffset": 106, "endOffset": 130}, {"referenceID": 7, "context": "An analogy can be drawn with Generative Adversarial Nets (GANs) (Goodfellow et al., 2014), which try to capture the distribution of data through a generative model that competes against a discriminative model.", "startOffset": 64, "endOffset": 89}, {"referenceID": 24, "context": "Another method with a conceptual connection to COOL is dropout (Srivastava et al., 2014), where an exponential number of models are trained together by randomly dropping out some hidden units during the training phase.", "startOffset": 63, "endOffset": 88}, {"referenceID": 10, "context": "In this sense, a neuron aggregate acts as an ensemble and is inspired by the hypothesis (in contrast to the \u201cgrandmother cell\u201d) that in biological neural networks more than one neuron is involved in recognizing each concept (Gross, 2002; Gazzaniga, 2004).", "startOffset": 224, "endOffset": 254}, {"referenceID": 3, "context": "In this sense, a neuron aggregate acts as an ensemble and is inspired by the hypothesis (in contrast to the \u201cgrandmother cell\u201d) that in biological neural networks more than one neuron is involved in recognizing each concept (Gross, 2002; Gazzaniga, 2004).", "startOffset": 224, "endOffset": 254}, {"referenceID": 2, "context": "Finally, a brief experiment with the Iris dataset (Fisher, 1936) provides the opportunity to show the behavior of COOL in the presence of sparse datasets.", "startOffset": 50, "endOffset": 64}, {"referenceID": 24, "context": "The idea behind the overcomplete layer is that it produces a dynamic akin to training an exponential number of models all at once, which echoes the motivation behind dropout (Srivastava et al., 2014).", "startOffset": 174, "endOffset": 199}, {"referenceID": 26, "context": "A phenomenon called fooling has recently attracted attention in the deep learning community (Szegedy et al., 2013; Nguyen, Yosinski, and Clune, 2015).", "startOffset": 92, "endOffset": 149}, {"referenceID": 26, "context": "It is worth noting that ensemble techniques on their own have almost no mitigating effect on the fooling problem (Szegedy et al., 2013) because they merely generate a better decision boundary, in the form of a larger margin.", "startOffset": 113, "endOffset": 135}, {"referenceID": 26, "context": "In other words, one can easily find fooling examples that can fool almost all discriminative models only based on a limited knowledge of the training set (Szegedy et al., 2013; Nguyen, Yosinski, and Clune, 2015).", "startOffset": 154, "endOffset": 211}, {"referenceID": 26, "context": "Previous methods for generating fooling images from Szegedy et al. (2013) and Nguyen, Yosinski, and Clune (2015) search directly for images that fool the network.", "startOffset": 52, "endOffset": 74}, {"referenceID": 26, "context": "Previous methods for generating fooling images from Szegedy et al. (2013) and Nguyen, Yosinski, and Clune (2015) search directly for images that fool the network.", "startOffset": 52, "endOffset": 113}, {"referenceID": 27, "context": "This section defines such a measure by introducing the generalized classification problem that leads to a notion of extended empirical risk, which is a natural extension of empirical risk (Vapnik, 2013).", "startOffset": 188, "endOffset": 202}, {"referenceID": 27, "context": "Recall from classical statistical learning theory (Vapnik, 2013), where one assume access to a finite collection of i.", "startOffset": 50, "endOffset": 64}, {"referenceID": 27, "context": "Finding f\u2217 using the Re instead of R is called empirical risk minimization, which is a bedrock principle of statistical learning theory (Vapnik, 2013).", "startOffset": 136, "endOffset": 150}, {"referenceID": 20, "context": "This problem is closely related to the open set recognition problem (Scheirer et al., 2013), where a learning model can respond to unknown classes during the test time.", "startOffset": 68, "endOffset": 91}, {"referenceID": 20, "context": "The advantage of this new metric over previous evaluation procedures applied in open set recognition and object detection tasks (Scheirer et al., 2013) is that it does not rely on any threshold and measures the rejection ability of the model implicitly, which simplifies the evaluation procedure and reduces the processing time significantly.", "startOffset": 128, "endOffset": 151}, {"referenceID": 22, "context": "To the best of our knowledge, this version and its extensions are the only off-the-shelf methods for capturing a notion of data distribution in high-dimensional spaces (density estimation techniques are usually not effective in high-dimensional spaces because of the curse of dimensionality (Scott, 2008)) and is applied to anomaly detection, among other tasks.", "startOffset": 291, "endOffset": 304}, {"referenceID": 21, "context": "To capture the underlying distribution of a set of unlabeled data, Sch\u00f6lkopf et al. (2001) proposed the one-class SVM, where the origin is treated as the only member of the second class.", "startOffset": 67, "endOffset": 91}, {"referenceID": 16, "context": "For comparison, Table 1 lists some of the state-of-the-art results on CIFAR-100, including AlexNet (Krizhevsky, Sutskever, and Hinton, 2012), DSN (Lee et al., 2015), NiN (Lin, Chen, and Yan, 2014), Maxout (Goodfellow et al.", "startOffset": 146, "endOffset": 164}, {"referenceID": 6, "context": ", 2015), NiN (Lin, Chen, and Yan, 2014), Maxout (Goodfellow et al., 2013), All-CNN (Springenberg et al.", "startOffset": 48, "endOffset": 73}, {"referenceID": 23, "context": ", 2013), All-CNN (Springenberg et al., 2014), Highway Network (Srivastava, Greff, and Schmidhuber, 2015), Fractional Max-Pooling (Graham, 2014), and ELU-Network (Clevert, Unterthiner, and Hochreiter, 2016).", "startOffset": 17, "endOffset": 44}, {"referenceID": 9, "context": ", 2014), Highway Network (Srivastava, Greff, and Schmidhuber, 2015), Fractional Max-Pooling (Graham, 2014), and ELU-Network (Clevert, Unterthiner, and Hochreiter, 2016).", "startOffset": 92, "endOffset": 106}], "year": 2017, "abstractText": "Though deep learning has pushed the boundaries of classification forward, in recent years hints of the limits of standard classification have begun to emerge. Problems such as fooling, adding new classes over time, and the need to retrain learning models only for small changes to the original problem all point to a potential shortcoming in the classic classification regime, where a comprehensive a priori knowledge of the possible classes or concepts is critical. Without such knowledge, classifiers misjudge the limits of their knowledge and overgeneralization therefore becomes a serious obstacle to consistent performance. In response to these challenges, this paper extends the classic regime by reframing classification instead with the assumption that concepts present in the training set are only a sample of the hypothetical final set of concepts. To bring learning models into this new paradigm, a novel elaboration of standard architectures called the competitive overcomplete output layer (COOL) neural network is introduced. Experiments demonstrate the effectiveness of COOL by applying it to fooling, separable concept learning, one-class neural networks, and standard classification benchmarks. The results suggest that, unlike conventional classifiers, the amount of generalization in COOL networks can be tuned to match the problem.", "creator": "LaTeX with hyperref package"}}}