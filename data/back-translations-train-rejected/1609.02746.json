{"id": "1609.02746", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification", "abstract": "This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4. We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification. We achieve competitive results for two-point scale sentiment classification and quantification, ranking fifth and a close fourth (third and second by alternative metrics) respectively despite using only pre-trained embeddings that contain no sentiment information. We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on five-point scale sentiment classification and quantification. An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information. We propose improvements in order to address these and other issues.", "histories": [["v1", "Fri, 9 Sep 2016 11:16:56 GMT  (90kb)", "http://arxiv.org/abs/1609.02746v1", "Published in Proceedings of SemEval-2016, 5 pages"]], "COMMENTS": "Published in Proceedings of SemEval-2016, 5 pages", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["sebastian ruder", "parsa ghaffari", "john g breslin"], "accepted": false, "id": "1609.02746"}, "pdf": {"name": "1609.02746.pdf", "metadata": {"source": "CRF", "title": "INSIGHT-1 at SemEval-2016 Task 4: Convolutional Neural Networks for Sentiment Classification and Quantification", "authors": ["Sebastian Ruder", "Parsa Ghaffari", "John G. Breslin"], "emails": ["firstname.lastname@insight-centre.org", "firstname@aylien.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.02 746v 1 [cs.C L] 9S ep"}, {"heading": "1 Introduction", "text": "Social media allows hundreds of millions of people to interact and engage with each other while expressing their thoughts about the things that move them. Sentiment Analysis (Pang and Lee, 2008) enables us to gain insights into opinions about people, objects, and events in the public sphere, and is now used to measure public opinion about companies or products, analyze customer satisfaction, and identify trends. Its immediacy has allowed Twitter to become an important platform for expression and public discourse, while the accessibility of large amounts of data has made it the focus of social media sentiment analysis. Recently, learning-intensive approaches have shown remarkable results in text classification and sentiment analysis (Kim, 2014) and have proven successful in sentiment classification at the phrase and message level (Severyn and Moschitti, 2015). Past SemEval competitions in Twitter sentiment analysis (Rosenthal, 2014) have contributed to this field."}, {"heading": "2 Related work", "text": "Kim (2014) uses a single-layer Convolutionary Neural Network to achieve excellence in various sentiment analysis datasets, demonstrating the usefulness of pre-trained embeddings. State-of-the-art models in Twitter sentiment analysis use large amounts of data available on Twitter to further improve their embeddings by treating smileys as loud labels (Go et al., 2009): Tang et al. (2014) learn sentimental word embeddings from such remotely monitored data and use them as features for a monitored classification, while Severyn and Moschitti (2015) use remote monitored data to refine the embeddings of a Convolutionary Neural Network. In contrast, we observe that remotely monitored data is not as important for some tasks as long as sufficient training data is available."}, {"heading": "3 Model", "text": "The model architecture we use is an extension of the CNN structure used by Collobert et al. (2011). The model takes as input a text that is padded to the length n. We present the text as a concatenation of its word embeddings x1: n, where xi-Rk is the k-dimensional vector of the i-th word in the text. The twisted layer pushes filters of different window sizes over the word embeddings. Each filter with the weights w, Rhk generates a new attribute ci for a window of h words according to the following operation: ci = f (w \u00b7 xi: i + h \u2212 1 + b) (1) (1) Note that b \u00b2 R is a distorted term and f is a nonlinear function, ReLU (Nair and Hinton, 2010) in our case. Applying the filter over any window of h words or characters in the sentence generates the following attribute diagram: c = [1, c2, c2,... + 1 \u2212 n."}, {"heading": "4 Methodology", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "For each sub-task, the organizers provide a training, development and development test set for training and tuning. In addition, the organizers provide training and development data from SemEval 2013 and experimental data from 2016 that can be used for training and tuning for sub-task A and sub-tasks B, C, D and E. We are experimenting with adding these data sets to the respective sub-task. Interestingly, their addition slightly increases the loss over the validation set, while providing a significant performance boost over past development sets that we consider to be substitutes for the performance on the 2016 test set. Therefore, we include these data sets for training all of our models. In particular, we do not select the model that achieves the least loss on the validation set, but choose the one that maximizes the FPN1 value, i.e. the arithmetic mean from the F1 value of positive and negative tweets that has been used in the past to evaluate task N1."}, {"heading": "4.2 Pre-processing", "text": "For pre-processing, we use a script adapted to the pre-processing script1 that is used to form GloVe vectors (Pennington et al., 2014). In addition to normalizing URLs and mentions, we normalize happy and sad smileys, extract hashtags, and insert tags for repeated, elongated, and all caps characters."}, {"heading": "4.3 Word embeddings", "text": "Previous research (Kim, 2014; Severyn and Moschitti, 2015) has found that embedding words is critical for training a precise sentiment model1http: / / nlp.stanford.edu / projects / glove / preprocess-twitter.rbinitialization of word embeddings. Therefore, we evaluate the following evaluation programs: random initialization, initialization using pre-formed GloVe vectors, fine-tuning pre-formed embeddings on a distantly monitored corpus (Severyn and Moschitti, 2015), and fine-tuning pre-formed embeddings on 40k tweets with crowd-sourced annotations. Perhaps, in turn, we find that fine-tuning embeddings on a distantly monitored or crowd-sourced corpus does not improve the performance of past development tests when the additional data provided to the training is included. We expect that additional training data will facilitate learning the basic needs of the semantics of the training."}, {"heading": "4.4 Hyperparameters and pre-processing", "text": "We find that the following hyperparameters, similar to those of Kim (2014), perform best over all sub-tasks: mini-batch size of 10, maximum set length of 50 tokens, word embedding size of 200 dimensions, drop-out rate of 0.3, l2 regularization of 0.01, filter lengths of 3, 4 and 5 with 100 filter cards each. We train for 15 epochs with stochastic gradient descent in mini-batch, the Adadelta update rule (pointer, 2012) and early stop."}, {"heading": "4.5 Task adaptation and quantification", "text": "To adjust our model to the different tasks, we simply adjust the number of output neurons to the scale used for each task (two-point scale in tasks B and D, three-point scale in tasks A, five-point scale in tasks C and E. We perform a simple quantification for tasks D and E by aggregating the classified tweets for each topic and reporting their distribution across the individual emotional states. We would therefore expect that our results for tasks B and D and the results for tasks C and E closely correlate."}, {"heading": "5 Evaluation", "text": "We report the results of our model in Tables 1 and 2 (Subtask A), Tables 3 (Subtask B), Tables 5 and 6 (Subtask C), Tables 4 (Subtask D) and Tables 7 (Subtask E). For some Subtask E, the organizers provide alternative metrics. However, we observe that the choice of scoring metric significantly influences the results, placing our system higher and higher when ranked by one of the alternative metrics. Subtask A. We achieve competitive performance in Subtask A in Table 1. However, the analysis of the results of the advanced test sets in Table 2 shows that our system achieves competitive F1 values for positive and neutral tweets, but only low F1 values for negative tweets due to poor memory. This is reflected in Table 1, where we achieve higher values for accuracy than for memory. The scoring metric for Subtask A, FPOS1 accentuates F1 for positive and negative tweets, resulting in our good performance in neutral tweets leading to median underrating and only underperformance."}, {"heading": "5.1 Improvements", "text": "We propose several improvements to enable the model to better deal with some of the challenges that have arisen. Negative feelings. The simplest way to better capture negative feelings is to include more negative tweets in the training data. Furthermore, using remote monitored data for fine embedding would probably have helped to mitigate this deficit. To allow the model to better distinguish between different feelings on a five-point scale, it would be interesting to evaluate ways to create a finer-grained, more distantly monitored corpus, for example by using a larger selection of smileys and emoticons or specific hashtags that indicate a high level of pleasure or disturbance. Ordinary classification. Instead of treating all classes as independent, we can allow the model to take proper information into account by simply modifying the labels as in (Cheng et al., 2008) to directly influence mood."}, {"heading": "6 Conclusion", "text": "In this paper, we presented our in-depth learning-based approach to Twitter sentiment analysis for a two-step, three-step and five-step sentiment classification, as well as a two-step and five-step sentiment quantification. We reviewed the various aspects we took into account in creating our model. We analyzed our weaker performance on a three-step sentiment classification and quantification and a five-step sentiment classification, and found that the model lacks the meaningfulness to capture negative feelings, and is unable to take the class order into account. Finally, we proposed improvements to address these shortcomings."}, {"heading": "Acknowledgments", "text": "This project is the result of research carried out with financial support from the Irish Research Council (IRC) under grant number EBPPG / 2014 / 30 and with Aylien Ltd as Enterprise Partner. This publication is the result of research supported in part by a Science Foundation Ireland (SFI) research grant under grant number SFI / 12 / RC / 2289."}], "references": [{"title": "A neural network approach to ordinal regression", "author": ["Zheng Wang Zheng Wang", "G. Pollastri"], "venue": "IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intel-", "citeRegEx": "Cheng et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2008}, {"title": "Natural Language Processing (almost) from Scratch", "author": ["Jason Weston", "Leon Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Twitter Sentiment Classification using Distant Supervision", "author": ["Go et al.2009] Alec Go", "Richa Bhayani", "Lei Huang"], "venue": null, "citeRegEx": "Go et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Go et al\\.", "year": 2009}, {"title": "Convolutional Neural Networks for Sentence Classification", "author": ["Yoon Kim"], "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "From Group to Individual Labels using Deep Features", "author": ["Dimitrios Kotzias"], "venue": null, "citeRegEx": "Kotzias.,? \\Q2015\\E", "shortCiteRegEx": "Kotzias.", "year": 2015}, {"title": "Rectified Linear Units Improve Restricted Boltzmann Machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "Proceedings of the 27th Inter-", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "SemEval-2016 Task 4: Sentiment Analysis in Twitter", "author": ["Nakov et al.2016] Preslav Nakov", "Alan Ritter", "Sara Rosenthal", "Veselin Stoyanov", "Fabrizio Sebastiani"], "venue": "In Proceedings of the 10th International Workshop on Semantic Evaluation,", "citeRegEx": "Nakov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nakov et al\\.", "year": 2016}, {"title": "Opinion Mining and Sentiment Analysis. Foundations and trends in information retrieval, 2(1-2):1\u2013135", "author": ["Pang", "Lee2008] Bo Pang", "Lillian Lee"], "venue": null, "citeRegEx": "Pang et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pang et al\\.", "year": 2008}, {"title": "Glove: Global Vectors for Word Representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": null, "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "SemEval-2014 Task 9: Sentiment Analysis in Twitter", "author": ["Alan Ritter", "Preslav Nakov", "Veselin Stoyanov"], "venue": "Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval", "citeRegEx": "Rosenthal et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2014}, {"title": "SemEval-2015 Task 10: Sentiment Analysis in Twitter", "author": ["Preslav Nakov", "Svetlana Kiritchenko", "Saif M Mohammad", "Alan Ritter", "Veselin Stoyanov"], "venue": null, "citeRegEx": "Rosenthal et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosenthal et al\\.", "year": 2015}, {"title": "UNITN : Training Deep Convolutional Neural Network for Twitter Sentiment Classification", "author": ["Severyn", "Moschitti2015] Aliaksei Severyn", "Alessandro Moschitti"], "venue": null, "citeRegEx": "Severyn et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Severyn et al\\.", "year": 2015}, {"title": "Learning Sentiment-Specific Word Embedding", "author": ["Tang et al.2014] Duyu Tang", "Furu Wei", "Nan Yang", "Ming Zhou", "Ting Liu", "Bing Qin"], "venue": null, "citeRegEx": "Tang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2014}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "Recently, deep learning-based approaches have demonstrated remarkable results for text classification and sentiment analysis (Kim, 2014) and have performed well for phrase-level and message-level sentiment classification (Severyn and Moschitti, 2015).", "startOffset": 125, "endOffset": 136}, {"referenceID": 9, "context": "Past SemEval competitions in Twitter sentiment analysis (Rosenthal et al., 2014; Rosenthal et al., 2015) have contributed to shape research in this field.", "startOffset": 56, "endOffset": 104}, {"referenceID": 10, "context": "Past SemEval competitions in Twitter sentiment analysis (Rosenthal et al., 2014; Rosenthal et al., 2015) have contributed to shape research in this field.", "startOffset": 56, "endOffset": 104}, {"referenceID": 6, "context": "SemEval-2016 Task 4 (Nakov et al., 2016) is no exception, as it introduces both quantification and five-point-scale classification tasks, neither of which have been tackled with deep learning-based approaches before.", "startOffset": 20, "endOffset": 40}, {"referenceID": 3, "context": "Kim (2014) uses a one-layer convolutional neural network to achieve top performance on various sentiment analysis datasets, demonstrating the utility of pre-trained embeddings.", "startOffset": 0, "endOffset": 11}, {"referenceID": 2, "context": "State-of-the-art models in Twitter sentiment analysis leverage large amounts of data accessible on Twitter to further enhance their embeddings by treating smileys as noisy labels (Go et al., 2009): Tang et al.", "startOffset": 179, "endOffset": 196}, {"referenceID": 2, "context": "State-of-the-art models in Twitter sentiment analysis leverage large amounts of data accessible on Twitter to further enhance their embeddings by treating smileys as noisy labels (Go et al., 2009): Tang et al. (2014) learn sentiment-specific word embeddings from such distantly supervised data and use these as features for supervised classification, while Severyn and Moschitti (2015) use distantly supervised data to fine-tune the embeddings of a convolutional neural network.", "startOffset": 180, "endOffset": 217}, {"referenceID": 2, "context": "State-of-the-art models in Twitter sentiment analysis leverage large amounts of data accessible on Twitter to further enhance their embeddings by treating smileys as noisy labels (Go et al., 2009): Tang et al. (2014) learn sentiment-specific word embeddings from such distantly supervised data and use these as features for supervised classification, while Severyn and Moschitti (2015) use distantly supervised data to fine-tune the embeddings of a convolutional neural network.", "startOffset": 180, "endOffset": 386}, {"referenceID": 1, "context": "The model architecture we use is an extension of the CNN structure used by Collobert et al. (2011).", "startOffset": 75, "endOffset": 99}, {"referenceID": 8, "context": "For pre-processing, we use a script adapted from the pre-processing script1 used for training GloVe vectors (Pennington et al., 2014).", "startOffset": 108, "endOffset": 133}, {"referenceID": 3, "context": "Past research (Kim, 2014; Severyn and Moschitti, 2015) found a good", "startOffset": 14, "endOffset": 54}, {"referenceID": 3, "context": "We find that the following hyperparameters, which are similar to ones used by Kim (2014), yield the best performance across all subtasks: mini-batch size of 10, maximum sentence length of 50 tokens, word embedding size of 200 dimensions, dropout rate of 0.", "startOffset": 78, "endOffset": 89}, {"referenceID": 13, "context": "We train for 15 epochs using mini-batch stochastic gradient descent, the Adadelta update rule (Zeiler, 2012), and early stopping.", "startOffset": 94, "endOffset": 108}, {"referenceID": 3, "context": "These results are in line with past research (Kim, 2014) showcasing that even a conceptually simple neural network-based approach can achieve excellent results given enough training data per class.", "startOffset": 45, "endOffset": 56}, {"referenceID": 0, "context": "Instead of treating all classes as independent, we can enable the model to take into account ordinal information by simply modifying the labels as in (Cheng et al., 2008).", "startOffset": 150, "endOffset": 170}, {"referenceID": 4, "context": "If the feedback from optimizing this objective proves to be too indirect to provide sufficient signals, we can jointly optimize tweet-level as well as topic-level sentiment as in (Kotzias, 2015).", "startOffset": 179, "endOffset": 194}], "year": 2016, "abstractText": "This paper describes our deep learning-based approach to sentiment analysis in Twitter as part of SemEval-2016 Task 4. We use a convolutional neural network to determine sentiment and participate in all subtasks, i.e. two-point, three-point, and five-point scale sentiment classification and two-point and five-point scale sentiment quantification. We achieve competitive results for two-point scale sentiment classification and quantification, ranking fifth and a close fourth (third and second by alternative metrics) respectively despite using only pre-trained embeddings that contain no sentiment information. We achieve good performance on three-point scale sentiment classification, ranking eighth out of 35, while performing poorly on fivepoint scale sentiment classification and quantification. An error analysis reveals that this is due to low expressiveness of the model to capture negative sentiment as well as an inability to take into account ordinal information. We propose improvements in order to address these and other issues.", "creator": "LaTeX with hyperref package"}}}