{"id": "1609.02846", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "Dialogue manager domain adaptation using Gaussian process reinforcement learning", "abstract": "Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or outperform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data. Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge, Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems.", "histories": [["v1", "Fri, 9 Sep 2016 16:02:57 GMT  (219kb,D)", "http://arxiv.org/abs/1609.02846v1", "accepted for publication in Computer Speech and Language"]], "COMMENTS": "accepted for publication in Computer Speech and Language", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["milica gasic", "nikola mrksic", "lina m rojas-barahona", "pei-hao su", "stefan ultes", "david vandyke", "tsung-hsien wen", "steve young"], "accepted": false, "id": "1609.02846"}, "pdf": {"name": "1609.02846.pdf", "metadata": {"source": "CRF", "title": "Dialogue manager domain adaptation using Gaussian process reinforcement learning", "authors": ["Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Lina M. Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Tsung-Hsien Wen", "Steve Young"], "emails": ["mg436@cam.ac.uk", "nm480@cam.ac.uk", "lmr46@cam.ac.uk", "phs26@cam.ac.uk", "su259@cam.ac.uk", "djv27@cam.ac.uk", "thw28@cam.ac.uk", "sjy@cam.ac.uk"], "sections": [{"heading": null, "text": "Spoken dialogue systems allow humans to interact with machines using natural language, so they have many advantages. By using language as the primary means of communication, a computer interface can provide a fast, human-like way of gathering information. In recent years, language interfaces have become increasingly popular, as evidenced by the rise of personal assistants such as Siri, Google Now, Cortana, and Amazon Alexa. Recently, data-driven methods of machine learning have been applied to dialogue modeling, and the results achieved for applications in limited areas are comparable to, or better than, conventional approaches. Methods based on Gaussian processes are particularly effective, as they allow good models to be estimated based on limited training data. Moreover, they provide an explicit assessment of the uncertainty that is particularly useful for enhanced learning."}, {"heading": "1. Introduction", "text": "This year it is more than ever before."}, {"heading": "2. Gaussian process reinforcement learning", "text": "The input to a statistical dialog manager is typically an N-best list of the hypotheses determined, obtained by the spoken language comprehension unit Q = Q. Based on this input, a distribution of possible dialog states is called a state of faith in each dialog series; b-B, an element of the faith space, is estimated; the state of faith must accurately represent everything that happened in the dialogue before this turn in order to maximize the expected cumulative reward; the expected cumulative reward for a given state of faith and action is defined by the Q function: Q (a) = E function (T action), an element of the action space, at each turn, in order to maximize the expected cumulative reward; the expected cumulative reward for a given state of faith b and action is defined by the Q function."}, {"heading": "4. Committee of dialogue policies", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Bayesian committee machine", "text": "The Bayesian Committee Machine is an approach for combining estimators trained on different data sets. It is particularly suitable for Gaussian process regression [12]. Here, we apply the method to combine the results of several estimates of Q-values Qi with the mean Qi and the covariance \u03a3 Q i according to Equation 4. Each estimator is combined using different pairs of reward and belief actions ri, Bi for i,..., M}, with M being the number of actions in the political committee. Figure 2 shows as an example a Bayesian Committee Machine consisting of three estimators. According to the description in [12], the combined mean Q and covariance \u041aQ are calculated as follows: Q (b, a) = E-Q (b, a), E-Q (b, a), E-Q (b, a), E-Q (b, a), E-Q (b, a), E-Q (b), E-Q (b)."}, {"heading": "4.2. Multi-domain Dialogue Manager", "text": "Section 3 introduced the idea of a generic policy that can be formed from data from different domains. [2] This is a specific policy that can be derived from a generic policy that uses additional domain data. [3] In order to produce a generic policy that works across multiple domains, a core function must be defined for beliefs and actions originating from different domains. [4] In this case, domains are organized into a class hierarchy, so it is reasonable to assume that there are common parts of belief for different domains. These parts relate to common domains and are mapped directly to each other, and for slots that are different, the mapping can be defined either manually or by using some similarity metrics. [5] It is possible to have two domains that do not have common slots. Therefore, a different approach is required for building policies that can be operated (and trained on) belief classes that come from different domains."}, {"heading": "5. Multi-agent learning in the policy committee framework", "text": "In the standard framework for enhancing learning, there is a single agent who tries to solve a specific task in a given environment. However, in this case, each actor only needs to consider part of the state space, and this can significantly speed up the learning process. Learning in such multi-agent systems typically takes place in three steps [20]. First, each actor proposes an action. Second, a gating mechanism is used that can either be crafted or automatically optimized to select the actual system action. Finally, the reward is distributed among the actors and they each re-evaluate their policies. The multi-agent framework can be seen as an extension of the policy committee model (see Figure 4). In fact, the first two steps are exactly the same: each committee member values his own Q function and then is used as a dialogue to automatically combine the results."}, {"heading": "6. Experimental results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Experimental set-up", "text": "To investigate the effectiveness of the methods discussed above, a variety of experimental contrasts were examined using an agenda-based simulated user working at dialog file level [22, 23]. The reward function distributes \u2212 1 at each turn to promote shorter dialogs, plus 20 at the end of each successful dialog. The user simulator includes an error generator, which should generate erroneous user input 15% of the time. The proposed methods were also integrated into a real-time dialog system, in which the guidelines were trained online with volunteers recruited through Amazon Mturk. Each user was assigned specific tasks in a specific subdomain and then asked to call the system in a constellation similar to [24, 25]. After each dialog, users were asked whether they rated the dialog as successful or not. On the basis of this binary evaluation, the subjective success was calculated as well as the average reward."}, {"heading": "6.2. Generic policy performance in simulation", "text": "In order to investigate the effectiveness of the generic guidelines discussed in Section 3, generic guidelines were trained and then tested in two areas - SFR and SFH using an equal number of restaurant and hotel dialogs. In addition, in-domain guidelines were used as a reference. For each condition, 10 guidelines were trained using different random seeds and different number of training dialogs. Each policy was then evaluated using 1,000 dialogs in each sub-domain. The overall average reward, success rate and number of turns is given in Table 2 along with a 95% confidence interval. The most important metric is the average reward, as the guidelines are trained to maximize this. As shown in Table 2, all generic guidelines perform better than domain-specific guidelines, which are trained only on the basis of the data available for that sub-domain (i.e., half of the generic guidelines available in this case, especially if training data is limited)."}, {"heading": "6.3. Adaptation of in-domain policies using a generic policy as a prior in simulation", "text": "To investigate the best and worst case scenarios, the generic priors (from the 10 randomly selected examples) who performed best and worst in each subdomain trained with 500 and 5000 dialogs were selected; in addition, a no-before policy was also trained for each subdomain (i.e. the policy was retrained from scratch); after each 1000 training dialogs, each policy was evaluated with 1000 dialogs; the results are given in Fig. 5 and 6 with a confidence interval of 95%; the performance at 0 training dialogs corresponds to the use of the generic policy as described in the previous section."}, {"heading": "6.4. Adaptation in interaction with human users", "text": "In order to examine performance in training with real users, rather than a simulator, two training plans were implemented in the SFR subdomain - one training from scratch without prior implementation and one adaptation training with the best generic beforehand obtained after 5000 simulated training dialogs. Three test runs were performed for each training plan and the results averaged to reduce each random variation. Fig. 7 shows the moving average reward as a function of the number of training dialogs. The moving window was set to 100 dialogs, so that after the initial 100 dialogs, each point in the chart yields an average of 300 dialogs (3 samples \u00d7 window size). The shaded area represents a 95% confidence interval. The initial parts of the chart show more randomness in behavior because the number of training dialogs is small. The results show an upward trend in performance, especially for policies that do not apply any previous training, compared to both the previous performance and the performance achieved."}, {"heading": "6.5. Policy committee evaluation with simulated user", "text": "In this context, it should be noted that this is a very complex matter."}, {"heading": "6.6. Policy committee evaluation with human users", "text": "In order to fully investigate the effectiveness of the proposed adaptation program, policies have also been trained in direct interaction with human users. We compare two constellations: one in which an intra-domain L6 policy is trained online, and another in which a multi-domain Bayesian committee machine is trained from the ground up using data from the SFR, SFH, and L6 domains, resulting in a political committee that can operate on all three domains. To our knowledge, this is the first time a multi-domain dialogue policy has been trained online in interaction with real users. Figure 8 shows the moving average reward as a function of the number of training dialogs for the L6 domain, which compares policy in domain (INDOM), and defines the multi-political Bayesian Committee Machine (MBCM) within the meaning of Section 4.2."}, {"heading": "6.7. Multi-agent simulation results", "text": "In fact, most of them will be able to play by the rules that they have shown over the last five years, and they will be able to play by the rules."}, {"heading": "6.8. Multi-agent human user evaluation", "text": "To ensure that the benefits of the proposed reward distribution approach proposed by the above simulation results are transferred to online-trained systems, two systems were also trained in direct interaction with human users. Firstly, a Multipolicy Bayesian Committee Machine (MBCM) was trained from the ground up using data from the SFR restaurant, the SFH hotel and the laptop domains of L6. This MBCM Policy Committee machine works on all three domains, but is dependent on knowledge of the current domain for policy updating. This is compared to the Committee Reward Scaling (SCALE) machine presented in Section 6.7, which distributes the reward to each committee member for each dialogue regardless of the domain. The system was deployed in a phone-based facility, with the subjects recruited via Amazon MTurk and a recurring neural network model was used to estimate the reward to each committee member for each dialogue independently of the domain. [27] The system was used in a phone-based facility, whereby the subjects were recruited via Amazon MTurk and a recurring neural network model was used to estimate the reward [27] as the average function of the training committee, whereas the AL9 is an average training email for the reward committee."}, {"heading": "7. Conclusion", "text": "This paper describes three models that support the expansion of the dialog system. First, a distributed multi-domain dialog architecture has been proposed, in which dialog strategies are organized into a class hierarchy aligned with an underlying knowledge graph. Class hierarchy enables the use of a system by using a modest amount of data to train a small set of generic strategies. As more data is collected, generic strategies can be adapted to provide domain-specific performance. Using Gaussian process-based reinforcement learning, it has been shown that it is possible to construct generic strategies that provide acceptable usage performance in domains, and perform better than can be achieved with subcontracted domain-specific strategies. To construct a generic policy that consists of all common slots plus a number of abstract slots that can be assigned to domain-specific slots, it works well. It has also been shown that sufficient usage interdomain-specific performance is available to accommodate the subdomain-less while the data exchange is available to accommodate the subdomain-specific ones."}, {"heading": "8. Acknowledgements", "text": "The research that led to this work was funded by the EPSRC grant EP / M018946 / 1 \"Open Domain Statistical Spoken Dialogue Systems.\" The data used in these experiments is available at: https: / / www.repository.cam.ac.uk / handle / 1810 / 259963."}], "references": [{"title": "Pomdp-based statistical spoken dialogue systems: a review", "author": ["S. Young", "M. Ga\u0161i\u0107", "B. Thomson", "J. Williams"], "venue": "Proceedings IEEE 101 (5) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "T", "author": ["P. Szeredi", "G. Luk\u00e1csy"], "venue": "Benk\u00f6, The Semantic Web Explained: The Technology and Mathematics Behind Web 3.0, Cambridge University Press, New York, NY, USA", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploiting the semantic web for unsupervised natural language semantic parsing", "author": ["G. T\u00fcr", "M. Jeong", "Y.-Y. Wang", "D. Hakkani-T\u00fcr", "L.P. Heck"], "venue": "in: Proceedings of Interspeech", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "G", "author": ["L.P. Heck", "D. Hakkani-T\u00fcr"], "venue": "T\u00fcr, Leveraging knowledge graphs for web-scale unsupervised semantic parsing., in: Proceedings of Interspeech", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Knowledge graph inference for spoken dialog systems", "author": ["Y. Ma", "P.A. Crook", "R. Sarikaya", "E. Fosler-Lussier"], "venue": "in: Proceedings of 40th IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2015, IEEE Institute of Electrical and Electronics Engineers", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-language hypotheses ranking and domain tracking for open domain, in: Proceedings of the 16th Annual Conference of the International Speech Communication Association (INTERSPEECH 2015)", "author": ["R.S. Paul A. Crook", "Jean-Philippe Martin"], "venue": "ISCA - International Speech Communication Association,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Policy learning for domain selection in an extensible multi-domain spoken dialogue system", "author": ["Z. Wang", "H. Chen", "G. Wang", "H. Tian", "H. Wu", "H. Wang"], "venue": "in: EMNLP", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning domainindependent dialogue policies via ontology parameterisation", "author": ["Z. Wang", "T.-H. Wen", "P.-H. Su", "Y. Stylianou"], "venue": "in: 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Gaussian Processes for Machine Learning", "author": ["C. Rasmussen", "C. Williams"], "venue": "MIT Press, Cambridge, Massachusetts", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2005}, {"title": "Distributed dialogue policies for multi-domain statistical dialogue management", "author": ["M. Ga\u0161i\u0107", "D. Kim", "P. Tsiakoulis", "S. Young"], "venue": "in: Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on, IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "A Bayesian Committee Machine", "author": ["V. Tresp"], "venue": "Neural Comput. 12 (11) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2000}, {"title": "Policy committee for adaptation in multi-domain spoken dialogue systems", "author": ["M. Ga\u0161i\u0107", "N. Mrk\u0161i\u0107", "P.-H. Su", "D. Vandyke", "T.-H. Wen", "S. Young"], "venue": "in: Proceedings of ASRU", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Product of gaussians for speech recognition", "author": ["M.J.F. Gales", "S.S. Airey"], "venue": "Comput. Speech Lang. 20 (1) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement learning with Gaussian processes", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "in: Proceedings of ICML", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Bayesian update of dialogue state: A POMDP framework for spoken dialogue systems", "author": ["B. Thomson", "S. Young"], "venue": "Computer Speech and Language 24 (4) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Probability product kernels", "author": ["T. Jebara", "R. Kondor", "A. Howard"], "venue": "J. Mach. Learn. Res. 5 ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "The Generation And Use Of Regression Class Trees For MLLR Adaptation", "author": ["M.F. Gales"], "venue": "Tech. Rep. CUED/F-INFENG/TR.263, Cambridge University Engineering Dept ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1996}, {"title": "Frustratingly easy domain adaptation", "author": ["III H. Daume"], "venue": "in: Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, Association for Computational Linguistics, Prague, Czech Republic", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Parallel reinforcement learning using multiple reward signals", "author": ["P. Raicevic"], "venue": "Neurocomputing 69 (1618) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "W-learning: competition among selfish Q-learners", "author": ["M. Humphrys"], "venue": "Tech. Rep. UCAM-CL-TR-362, University of Cambridge, Computer Laboratory ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1995}, {"title": "Agenda- Based User Simulation for Bootstrapping a POMDP Dialogue System", "author": ["J. Schatzmann", "B. Thomson", "K. Weilhammer", "H. Ye", "S. Young"], "venue": "in: Proceedings of HLT", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "F", "author": ["S. Keizer", "M. Ga\u0161i\u0107"], "venue": "Jur\u010d\u0301\u0131\u010dek, F. Mairesse, B. Thomson, K. Yu, S. Young, Parameter estimation for agenda-based user simulation, in: Proceedings of SIGDIAL", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "On-line policy optimisation of Bayesian Dialogue Systems by human interaction", "author": ["M. Ga\u0161i\u0107", "C. Breslin", "M. Henderson", "M. Szummer", "B. Thomson", "P. Tsiakoulis", "S. Young"], "venue": "in: Proceedings of ICASSP", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Sample efficient reinforcement learning with gaussian processes", "author": ["R.C. Grande", "T.J. Walsh", "J.P. How"], "venue": "in: Proceedings of the 31th International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning from real users: Rating dialogue success with neural networks for reinforcement learning in spoken dialogue systems", "author": ["P.-H. Su", "D. Vandyke", "M. Ga\u0161i\u0107", "D. Kim", "N. Mrk\u0161i\u0107", "T.-H. Wen", "S. Young"], "venue": "in: Proceedings of Interspeech", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Statistical approaches to dialogue management have been shown to reduce design costs and provide superior performance to hand-crafted systems particularly in noisy environments [1].", "startOffset": 177, "endOffset": 180}, {"referenceID": 1, "context": "The semantic web is an effort to organise the large amount of information available on the Internet into a structure that can be more easily processed by a machine designed to perform reasoning on this data [2].", "startOffset": 207, "endOffset": 210}, {"referenceID": 2, "context": "There has been a significant amount of work in spoken language understanding focused on exploiting knowledge graphs in order to improve coverage [3, 4].", "startOffset": 145, "endOffset": 151}, {"referenceID": 3, "context": "There has been a significant amount of work in spoken language understanding focused on exploiting knowledge graphs in order to improve coverage [3, 4].", "startOffset": 145, "endOffset": 151}, {"referenceID": 4, "context": "More recently there have also been efforts to build statistical dialogue systems that operate on large knowledge graphs, but limited so far to the problem of belief tracking [5, 6].", "startOffset": 174, "endOffset": 180}, {"referenceID": 5, "context": "More recently there have also been efforts to build statistical dialogue systems that operate on large knowledge graphs, but limited so far to the problem of belief tracking [5, 6].", "startOffset": 174, "endOffset": 180}, {"referenceID": 6, "context": "A previously proposed model for multi-domain dialogue management [7] assumes a dialogue expert for each domain and the central controller which decides to which dialogue expert to pass the control.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "A related work in [8] proposes a domain independent feature representation of the dialogue state so that the dialogue policy can be applied to different domains.", "startOffset": 18, "endOffset": 21}, {"referenceID": 8, "context": "A Gaussian process is a Bayesian method which specifies a prior distribution over the unknown function and then given some observations estimates the posterior [9].", "startOffset": 160, "endOffset": 163}, {"referenceID": 9, "context": "This idea was first proposed in [11].", "startOffset": 32, "endOffset": 36}, {"referenceID": 10, "context": "The second approach is based on a Bayesian committee machine [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 11, "context": "This method was proposed in [13].", "startOffset": 28, "endOffset": 32}, {"referenceID": 12, "context": "It is similar to Products of Gaussians which have previously been applied to problems such as speech recognition [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "GP-Sarsa is an on-line reinforcement learning algorithm that models the Q-function as a Gaussian process [15]:", "startOffset": 105, "endOffset": 109}, {"referenceID": 8, "context": ", k((b, a), (b, a))], K is the Gram matrix [9], H is a band matrix with diagonal [1,\u2212\u03b3] and \u03c3 is an additive noise factor which controls how much variability in the Qfunction estimate is expected during the learning process.", "startOffset": 43, "endOffset": 46}, {"referenceID": 14, "context": "Here we use the Bayesian Update of Dialogue State (BUDS) dialogue model [16].", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "The kernel function over both dialogue history and user goal nodes is based on the expected likelihood kernel [17], which is a simple linear inner product.", "startOffset": 110, "endOffset": 114}, {"referenceID": 16, "context": "cf analogy with speech recognition adaptation using regression trees[18] 2 Here a model M is assumed to include input mappings for speech understanding, a dialogue policy \u03c0 and output mappings for generation.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "Other adaptation strategies are also possible but may result in increasing the dimensionality (see for example [19]).", "startOffset": 111, "endOffset": 115}, {"referenceID": 10, "context": "It is particularly suited to Gaussian process regression [12].", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "Following the description in [12], the combined mean Q and covariance \u03a3 are calculated as: Q(b, a) = \u03a3(b, a) \u2211M i=1 \u03a3 Q i (b, a) Qi(b, a), \u03a3(b, a)\u22121 = \u2212(M\u22121) \u2217 k((b, a), (b, a))\u22121 + \u2211M i=1 \u03a3 Q i (b, a) \u22121.", "startOffset": 29, "endOffset": 33}, {"referenceID": 18, "context": "However, for complex tasks it has been shown [20] that it is more effective to decompose the problem into sub-tasks and introduce a distinct agent to solve each subtask.", "startOffset": 45, "endOffset": 49}, {"referenceID": 18, "context": "Learning in such multi-agent systems is typically performed in three steps [20].", "startOffset": 75, "endOffset": 79}, {"referenceID": 18, "context": "n\u00e4\u0131ve approach: The total reward that the system obtains is directly fed back to each committee member [20].", "startOffset": 103, "endOffset": 107}, {"referenceID": 19, "context": "winner-takes-all approach: The total reward that the system obtains is fed back to the committee member that proposed the highest Q-value for the action that was finally chosen by the gating mechanism [21].", "startOffset": 201, "endOffset": 205}, {"referenceID": 18, "context": "reward scaling approach: The total reward is redistributed to each committee member in such a way as to reflect its contribution to the final action chosen by the gating mechanism [20].", "startOffset": 180, "endOffset": 184}, {"referenceID": 20, "context": "Experimental set-up In order to investigate the effectiveness of the methods discussed above, a variety of experimental contrasts were examined using an agenda-based simulated user operating at the dialogue act level [22, 23].", "startOffset": 217, "endOffset": 225}, {"referenceID": 21, "context": "Experimental set-up In order to investigate the effectiveness of the methods discussed above, a variety of experimental contrasts were examined using an agenda-based simulated user operating at the dialogue act level [22, 23].", "startOffset": 217, "endOffset": 225}, {"referenceID": 22, "context": "Each user was assigned specific tasks in a given subdomain and then asked to call the system in a similar set-up to that described in [24, 25].", "startOffset": 134, "endOffset": 142}, {"referenceID": 23, "context": "a prior can be seen as resetting the variance of a GP which may lead to better sample efficiency [26].", "startOffset": 97, "endOffset": 101}, {"referenceID": 7, "context": "Unlike domain-independent policy models [8], MBCM allows flexible selection of committee members.", "startOffset": 40, "endOffset": 43}, {"referenceID": 24, "context": "The system was deployed in a telephone-based set-up, with subjects recruited via Amazon MTurk and a recurrent neural network model was used to estimate the reward [27].", "startOffset": 163, "endOffset": 167}], "year": 2016, "abstractText": "Spoken dialogue systems allow humans to interact with machines using natural speech. As such, they have many benefits. By using speech as the primary communication medium, a computer interface can facilitate swift, human-like acquisition of information. In recent years, speech interfaces have become ever more popular, as is evident from the rise of personal assistants such as Siri, Google Now, Cortana and Amazon Alexa. Recently, data-driven machine learning methods have been applied to dialogue modelling and the results achieved for limited-domain applications are comparable to or outperform traditional approaches. Methods based on Gaussian processes are particularly effective as they enable good models to be estimated from limited training data. Furthermore, they provide an explicit estimate of the uncertainty which is particularly useful for reinforcement learning. This article explores the additional steps that are necessary to extend these methods to model multiple dialogue domains. We show that Gaussian process reinforcement learning is an elegant framework that naturally supports a range of methods, including prior knowledge, Bayesian committee machines and multi-agent learning, for facilitating extensible and adaptable dialogue systems.", "creator": "LaTeX with hyperref package"}}}