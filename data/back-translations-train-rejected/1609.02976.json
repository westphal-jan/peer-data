{"id": "1609.02976", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Sep-2016", "title": "An Integrated Classification Model for Financial Data Mining", "abstract": "Nowadays, financial data analysis is becoming increasingly important in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collected data to help make reasonable decisions for new customer requests, e.g. user credit category, churn analysis, real estate analysis, etc. Financial institutes have applied different data mining techniques to enhance their business performance. However, simple ap-proach of these techniques could raise a performance issue. Besides, there are very few general models for both understanding and forecasting different finan-cial fields. We present in this paper a new classification model for analyzing fi-nancial data. We also evaluate this model with different real-world data to show its performance.", "histories": [["v1", "Fri, 9 Sep 2016 23:45:19 GMT  (284kb)", "http://arxiv.org/abs/1609.02976v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["fan cai", "nhien-an le-khac", "m-t kechadi"], "accepted": false, "id": "1609.02976"}, "pdf": {"name": "1609.02976.pdf", "metadata": {"source": "CRF", "title": "An Integrated Classification Model for Financial Data Mining", "authors": ["Fan Cai"], "emails": ["fan.cai.cn@gmail.com", "tahar.kechadi}@ucd.ie"], "sections": [{"heading": null, "text": "As companies collect more and more data from day-to-day operations, they expect to extract useful knowledge from existing data collected to make reasonable decisions for new customer requests, such as user credit categories, settlement analyses, real estate analyses, etc. Financial institutions have used various data mining techniques to improve their business performance, but a simple approach to these techniques could pose a performance problem. In addition, there are very few general models for understanding and predicting different financial areas. In this paper, we present a new classification model for analyzing financial data. We also evaluate this model with different real data to show its performance.Keywords: data mining, decision tree, multi-layer perception, Gaussian process, classification model."}, {"heading": "1 Introduction", "text": "In fact, the fact is that most of them are not purely economic matters, but purely economic matters, which are purely economic matters."}, {"heading": "2 Background", "text": "In this section, we examine traditional data mining techniques widely used in the analysis of financial data sets: Decision Tree (DT), Bayes Network (BN), Clustering, Neural Networks (NN) and Gaussian Process (GP)."}, {"heading": "2.1 Data Mining Techniques", "text": "Decision Tree. C4.5 [15] is a popular decision tree model that greedily chooses the highest information gathering rate that needs to be built. However, small disruptions in the data set are likely to cause a significant difference in the decision tree produced. Pruning is used to avoid overmatch, but there is no theoretical guarantee of efficiency. Furthermore, double the growth of numerical attributes is inefficient. Bayes Network [16] is a probabilistic graphical model that represents conditional dependencies via a directed acyclic graph, efficient in learning and easy to compare. However, there is no universally accepted best training method and it involves experts to decide explainable causal influences. In business, clustering can be used to segment customers into a number of groups for additional analysis and marketing strategies. Clustering also has its drawbacks, e.g. traditional clustering, as Netcerematic clusters [17] can only have numerical attributes."}, {"heading": "2.2 Related Work", "text": "Since financial data sets are always very large, and the construction of a universal model for classification is generally impractical and inaccurate, many hybrid models for financial data sets are being developed, such as a hybrid model that uses rule learning, decision lists, decision tree and association rules but responds mainly to nominal labels; [21] uses the decision tree and genetic algorithm model but can handle only small disjunctions with a small number of training examples; [13] mixes genetic algorithms with SVM to optimize functional parts and parameters of SVM; [13] More importantly, SVMs only handle numerical attributes and binary labels; [14] integrate financial indicators, intellectual capital indicators and neural networks. However, these are only numerical indicators and it is not a common structure like [13]."}, {"heading": "3 A Combined Model for Financial Dataset Classification", "text": "In this section, we present an application of data mining techniques for structural understanding and prediction of financial data that have differently scaled attributes and consist of both nominal and numerical attributes, provided similar behavioral response clusters exist. Training and prediction processes are illustrated in Figure 1. We derive our scheme as G-KM-NC. The model consists of three parts: G stands for grouping; KM stands for K-mean cluster [17] for a specific group. NC stands for non-linear classification techniques, such as MLP and GPC, where KM can be omitted if the group is narrow by visions or cluster criteria discussed in Section 4 and 5."}, {"heading": "3.1 Training", "text": "First, the data set is grouped according to the nominal attribute with the largest gain ratio, without reference to the attribute dependency. However, a decision tree based on the gain ratio can replace the individual attribute grouping if the dependency relationship is known. Grouping helps the analyst to name the most significant nominal property in the classification. Second, the grouped data sets are normalized and fed to the KM-NC submodel. Grouped data sets are bundled by K-mean clusters for second-order parallel computations after grouping, and more detailed structural knowledge of the grouped data set on which statistical methods can be used. Centroids are stored for prediction. Third, a strong nonlinear classifier (MLP, GPC) is built for each cluster-da-taset. Clustered data sets are normalized again to train the NC model."}, {"heading": "3.2 Forecast", "text": "First, the corresponding nominal group is found and the data sample is normalized according to the preprocessing scheme used to form this group. Second, the closest cluster is found in the KM-NC by finding the closest cluster with the data sample.} {minarg 1 jinput kj cvD (2), where vinput is the input, cj is the centering of the j th cluster. Finally, the data sample is normalized according to the preprocessing scheme of the closest cluster and passed to nonlinear classifiers for output of the result."}, {"heading": "4 Criteria and Evaluation", "text": "In this section we specify the criteria for grouping and clustering processes and evaluations."}, {"heading": "4.1 Criteria for grouping and clustering", "text": "To compensate for this, Quinlan proposes to use the gain ratio as defined in [15]. In addition, a well-known internal criterion Davies-Bouldin-Index (DBI) [22] is used to evaluate clustering. Smaller DBI values result in more significant clustering. We present this gain ratio and DBI evaluate the grouping and clustering well in our model in Section 5."}, {"heading": "4.2 Result evaluation", "text": "Since MLP generates class output without any possibility, the predicted class is class 1 if the output of MLP > 0.5 is otherwise predicted class -1. Besides, GPC generates a presumption of possibility, and in the binary classification situation the sum of the possibilities of both classes is 100%, we consider the predicted class as class 1 if the output of GPC is > 50%, and otherwise as class 1. The accuracy of both nonlinear classifiers is: nnn Acc negativetruepositivetrue (3) Since G-KM-NC is an example classified either by MLP or GPC, it may end up being treated as a new nonlinear classifier using the same formula of accuracy (3)."}, {"heading": "4.3 Model evaluation", "text": "To evaluate our model, we are conducting four main experiments: in the first, four universal nonlinear classifiers, such as Decision Tree, Bayes Network, MLP and Gaussian Process, were performed on different datasets, comparing their performance with our model; next, we are evaluating the grouping part of our model (see section 3, page 4); the performance of the proposed model is shown in our third experiments; and finally, we are also testing the scalability of this model through its acceleration performance."}, {"heading": "5 Experiments and Analysis", "text": "The German credit dataset [23] uses a 10-fold cross-validation to form 900 training cases and 100 validation cases; the churn dataset [24] provides 3333 training cases and 1667 validation examples where the function of the phone number is omitted because it is unique to each customer and contains no useful information; the house price dataset [25] also uses a 10-fold cross-validation, consisting of 2633 training cases and 293 validation cases; the MLP structure x-y-1 indicates that it has x-input neurons and y-hidden layer neurons where the optimal y is a priori; both MLP and GPC use linear logistic functions as activation functions; and the search for interval / golden section-based conjugate optimization method [19]. Step size and tolerance value of the search for interval and golden average are 0.01 each."}, {"heading": "5.1 Universal nonlinear classifier", "text": "We specify validation performance of 4 universal nonlinear classifiers as baseline values: C4.5, Bayes Network (BN), MLP, and GPC. MLP selects the optimal hidden layer size from experiments to avoid both over- and underfits. GPC for churn records and house price records are omitted because the O (n3) complexity of GPC makes training impractical. MLP and GPC only use numerical attributes of the datasets. Table 1 shows that models using more nominal labels (C4.5, Bayes Network) do not always exceed numerical models (MLP, GPC). Table 1. Performance of universal nonlinear classification Test Accuracy German credit record Churn dataset House price dataset C4.5 71.00% 71.00% 86.44% 85.32% Bayes Network 73.00% 88.00% Optimization GL7-83.5% (ML1-93.5%) (83.1-83.5%)."}, {"heading": "5.2 Grouping", "text": "We test the grouping by using only one nominal attribute, the structure so far being G-NC. We use the same hidden layer size among grouped data sets, assuming that grouped data sets with the same nominal attribute are of the same spatial complexity. Table 2 shows that the quality of a grouping to understand the behavior of financial data sets is related to the return ratio of that nominal attribute. If GainRatio > 0.01, then predictive ability tends to improve. If GainRatio < 0.01, then grouping does not normally improve the prediction. A higher gain ratio indicates a better grouping model in general. 0.01 is a threshold to find out whether grouping by a nominal attribute is significant or meaningless for classification by MLP and GPC. Furthermore, the performance of the GPC and the optimal MLLP close-to-modeling and GLP universes is good."}, {"heading": "5.3 Model performance", "text": "Next, we discover the internal distribution structure within the grouped dataset in order to gain more structural knowledge about the financial dataset. K-mean clustering is used because it does not exclude much noise, since recorded financial examples are generally trustworthy. In addition, k-mean clustering itself can be comparable to [17]. In Table 4, A11, A12, A13 and A14, the 4 nominal names of attribute # 1 of the German dataset are the number of test cases of each group by cross-validation. The lowest DBIs give the best accuracy over 4 groups, as lower DBI have lower average similarity between clusters, indicating more significant clustering. However, it is not appropriate to set a universal threshold for cluster identification because business discrepancies exist. G-KM-NC discards gap ranges between clusters in the exchange of performance parallels and mutual cluster interferences."}, {"heading": "1 69.23% 90.70% 59.26% 50%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 65.38% 1.76 90.70% 1.96 62.96% 1.91 50% 1.92", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 69.23% 1.66 90.70% 1.34 48.15% 1.59 50% 1.80", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 61.54% 1.58 88.37% 1.49 51.85% 1.76 50% 1.37", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 69.23% 1.52 88.37% 1.42 62.96% 1.46 50% 1.16", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 61.54% 1.47 88.37% 1.42 59.26% 1.61 50% 1.34", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 69.23% 1.22 86.06% 1.35 59.26% 1.60 50% 1.58", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 65.38% 1.24 90.70% 1.30 48.15% 1.53 25% 1.52", "text": "The results of the churn and house price dataset are listed in Table 5, where International Plan (IP) is the fifth attribute of the churn dataset and Central Air (CA) is the eighth attribute of the house price dataset. Because the datasets are large and GPC impractical, MLP is used as a nonlinear classifier with the optimal hidden layer size in brackets and assumes that clusters in a group have the same distribution complexity. G5 [2.2] MLP for churn dataset achieves an overall accuracy of 95.62%, and G8 [2.7] MLP for house price datasets an accuracy of 90.78%. All three models exceed any basic universal model."}, {"heading": "1 (10)96.84\uff05 (10)75.33\uff05 (7)90.58% (7)88.24%", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2 (9)97.10% 1.85 (10)80.67% 1.88 (4)90.94% 1.09 (7)88.24% 1.60", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3 (9)96.77% 2.46 (9)78.00% 2.32 (4)90.22% 1.24 (7)88.24% 1.51", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 (7)95.85% 2.29 (9)75.33% 2.31 (3)90.58% 1.23 (7)88.24% 1.62", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 (7)95.19% 2.66 (9)72.00% 2.29 (3)90.22% 1,32 (3)88.24% 1.68", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 (7)96.18% 2.55 (9)66.67% 2.48 (3)90.22% 1.45 (2)88.24% 1.48", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 (7)95.39% 2.45 (9)69.33% 2.35 (3)90.22% 1.38 (1)88.24% 1.45", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8 (5)95.72% 2.40 (9)63.33% 2.23 (3)89.86% 1.38 (1)88.24% 1.45", "text": "Clustered clustering of data sets gives second-order parallelism by further distributing computational pressure. There is more detailed structure across groups. Clustering confirms validation accuracy when clustered by lowest DBI. If lowest DBI is not obvious, it indicates that the group is narrow and cannot be further divided in the house price like CA = N group. If lowest DBI obviously exists, clustering by other K higher DBI is illogical and reduces predictability. It is suggested that if the group is very large and an optimal K obviously exists, clustering is used, otherwise it is skipped."}, {"heading": "5.4 Speedup Analysis", "text": "GPC has a high complexity O (n3) dominated by cholesky decomposition when most calculations are at most O (n2) [20], where n is the number of training data. It becomes impractical to train universal GPC when the data set is large. When a data set is grouped or grouped into p-sub data sets, the expected GPC complexity per thread decreases to O (n2) [18] to O (n / p) 2) per thread, which reduces the total complexity by three times for parallel threads. G-KM-MLP similarly decreases the BP training complexity from O (n2) [18] to O (n / p) 2) per thread. Experiments use computers with 2.7 GHz Intel Core i5, 4GB 1333MHz DDR3 memory and 4 cores. The results in Table 6 show that our model is scalable DDR3 for both the DR3 DDR3 and DR3 DDR3 data sets as well as the DR4 GPC classics."}, {"heading": "6 Conclusion and Future Work", "text": "In this paper, we present an integrated classification model, G-KM-NC, which helps us to analyze various financial data sets that point to practical data storage and the cognitive need for group / cluster structure. This model is a combination of different data mining techniques: grouping based on profit ratios, clustering and nonlinear classification (MLP and GPC). Evidence that G-KM-NC surpasses other universal models based on a single technique is presented and efforts are made to logically reduce the computational complexity by parallelism. Through our model, experts can not only understand group financial data sets structurally, but also achieve good predictive capability. G-KMNC model is flatter compared to DT, more tightly structured than BN, whose structure differs between different areas, and easier to use than universal MLP and GPC LP, more accurately than universal classification techniques GLP, GLP, GLP, GLP, GLP and GLP, not to be used as Universal Classification Techniques GLM-P, the Techniques GLP, the classes are mentioned in this paper, and GLP, and GLP, and GLP, the classes are mentioned in this paper."}], "references": [], "referenceMentions": [], "year": 2012, "abstractText": "Nowadays, financial data analysis is becoming increasingly im-<lb>portant in the business market. As companies collect more and more data from daily operations, they expect to extract useful knowledge from existing collect-<lb>ed data to help make reasonable decisions for new customer requests, e.g. user<lb>credit category, churn analysis, real estate analysis, etc. Financial institutes have applied different data mining techniques to enhance their business performance.<lb>However, simple approach of these techniques could raise a performance issue.<lb>Besides, there are very few general models for both understanding and forecasting different financial fields. We present in this paper a new classification mod-<lb>el for analyzing financial data. We also evaluate this model with different real-<lb>world data to show its performance.", "creator": "Microsoft\u00ae Office Word 2007"}}}