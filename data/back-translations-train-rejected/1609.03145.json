{"id": "1609.03145", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2016", "title": "Relational Models", "abstract": "We provide a survey on relational models. Relational models describe complete networked {domains by taking into account global dependencies in the data}. Relational models can lead to more accurate predictions if compared to non-relational machine learning approaches. Relational models typically are based on probabilistic graphical models, e.g., Bayesian networks, Markov networks, or latent variable models. Relational models have applications in social networks analysis, the modeling of knowledge graphs, bioinformatics, recommendation systems, natural language processing, medical decision support, and linked data.", "histories": [["v1", "Sun, 11 Sep 2016 10:14:18 GMT  (71kb,D)", "http://arxiv.org/abs/1609.03145v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["volker tresp", "maximilian nickel"], "accepted": false, "id": "1609.03145"}, "pdf": {"name": "1609.03145.pdf", "metadata": {"source": "CRF", "title": "Relational Models", "authors": ["Volker Tresp", "Maximilian Nickel"], "emails": [], "sections": [{"heading": "1 Synonyms", "text": "Relational Learning, Statistical Relational Models, Statistical Relational Learning, Relational Data Mining"}, {"heading": "2 Glossary", "text": "The only question is whether the individuals are actually people who are able to identify themselves, and whether the others are people who are able to identify themselves, the only question is whether they are people who are able to identify themselves, the only question is whether they are people who are able to identify themselves, the only question is whether they are people who are able to identify themselves, the only question is whether they are people who are able to identify themselves, the only question is whether they are people who are able to identify themselves, the only question is whether they are people who are able to identify themselves, the only question is whether they are people who are able to identify themselves."}, {"heading": "3 Definition", "text": "Relational models are models of machine learning that are capable of truthfully representing some or all of the distinguishing features of a relationship area, such as extensive dependencies across multiple relationships. Typical examples of relationship areas are social networks and knowledge bases. Relational models concern non-trivial relationship areas with at least one relationship with a similarity of two or more that describes the relationship between entities, e.g. white, like, rejects. In the following, we will focus on non-trivial relationship areas."}, {"heading": "4 Introduction", "text": "Social networks can be modelled as graphs in which actors correspond to nodes and in which relationships between actors such as friendship, kinship, organizational position or sexual relationships are represented by directed marked links (or connections) between the respective nodes. Typical tasks of machine learning concern predicting unknown instances of relationships between actors as well as predicting attributes and class names of actors. Furthermore, one might be interested in clustering actors. In order to achieve optimal results, machine learning should take into account the network environment of an actor. Thus, two individuals could occur in the same cluster because they have common friends.Relational learning is a branch of machine learning that deals with these tasks, i.e. learning efficiently from data in which information is represented in the form of relationships between entities. Relational models are models of machine learning that use some or all distinctive characteristics of relational data as truth-model models, i.e., models of dependency models that are distributed via relationship-homogenous models."}, {"heading": "5 Key Points", "text": "Statistical relational learning is a sub-area of machine learning. Relational models learn a probabilistic model of a complete networked area by taking global dependencies into account in the data. Relational models can lead to more precise predictions compared to non-relational approaches to machine learning. Relational models are typically based on probabilistic graphical models such as Bayesian networks, Markov networks or latent variable models."}, {"heading": "6 Historical Background", "text": "Inductive Logical Programming (ILP) was perhaps the first machine learning effort to seriously focus on relational representation. It gained attention in the early 1990s and focuses on learning deterministic or near-deterministic dependencies, deriving representations from first-order logic. ILP was introduced as a field in a groundbreaking paper by Muggleton [21]. Quinlan's FOIL [29] is a very early and still very influential algorithm. Since then, many combinations of ILP and relational learning have been researched, as social networks primarily have statistical dependencies. Statistical Relational Learning began around the beginning of the millennium with the work of Koller, Pfeffer, Getoor and Friedman [17, 9]. Since then, many combinations of ILP and relational learning have been researched. Semantic Web, Linked Open Data, produces enormous amounts of relational data and [39, 27] describes the application of statistical relational learning to these emerging field databases of theoretical knowledge, already applying three-fold to current knowledge."}, {"heading": "7 Machine Learning in Relational Domains", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Relational Domains", "text": "The glossary defines key concepts such as a relationship, predicate, tuple, and database. Non-trivial relationship domains contain at least one relationship with an identity of two or more that describes the relationship between entities, e.g. white, mag, and rejects, focusing on non-trivial relationship domains. Social networks are typical relationship domains in which information is represented by multiple types of relationships (e.g. white, mag, rejects) between entities (here: actors) as well as by entity attributes."}, {"heading": "7.2 Generative Models for a Relational Database", "text": "Typically, relational models can exploit far-reaching or even global dependencies and have principled ways of dealing with missing data. Relational models are often presented as probabilistic graphical models and can be considered as relational versions of regular graphical models, such as Bayesian networks, Markov networks, and latently variable models. Approaches often have a \"Bayesian touch,\" but fully Bayesian statistical treatment is not always performed. The following section describes common relational graphical models."}, {"heading": "7.3 Non-relational Learning", "text": "Consider a database with a central actor of the entity class with elements ei and only simple relations; therefore, we look at a trivial area of relationship. Then, the random variables can be divided into independent disjunction groups according to the entities, and the common distribution factors become factors in the form of area factors in which the binary random variable XR, ei is assigned to the tuple ei in unary relation R (see glossary). Thus, the set of random variables can be reduced to non-overlapping independent sets of random variables. This is the usual non-relational learning environment with i.i.d. instances corresponding to the different actors."}, {"heading": "7.4 Non-relational Learning in a Relational Domain", "text": "A common approach to a relationship model is to model simple relationships of key units in a similar way as in a non-relationship-oriented model, as in the case of \u0418iP ({XR, ei} R | fi), where fi is a vector of relationship characteristics derived from the relationship environment of the actor i. Relational characteristics provide additional information to support learning and prediction tasks. For example, the average income of an individual's friends could be a good covariant for predicting an individual's income in a social network. The underlying mechanism that forms these patterns could be homophobic, the propensity of individuals to associate themselves with similar others. The aim of this approach is to be able to use i.i.d. machine learning by taking advantage of a portion of the relational information. This approach is often used in applications where probability models are computationally too expensive. Applying with non-relational learning is often referred to as having only two relational characteristics, but many of them are highly relational."}, {"heading": "7.5 Learning Rule-Premises in Inductive Logic Programming", "text": "Some researchers apply a systematic search for good characteristics and consider this to be an essential difference between relational learning and non-relational learning: in non-relational learning, characteristics are essentially defined before the training phase, while relational learning involves a systematic and automatic search for characteristics in the relational context of the units involved. Inductive logical programming (ILP) is a form of relational learning aimed at finding deterministic or near-deterministic dependencies, which are described in logical terms like horn clauses. Traditionally, ILP includes a systematic search for reasonable relational characteristics that form the rule requirements [6]."}, {"heading": "8 Relational Models", "text": "In this section we describe in detail the most important relational models, which are based on probabilistic graphical models that efficiently model high-dimensional probability distributions by exploiting dependencies between random variables. In particular, we look at Bayesian networks, Markov networks and latent variable models. We begin with a more detailed discussion of possible world models for relational domains and a discussion of the dual structures of the triple graph and the probability graph."}, {"heading": "8.1 Random Variables for Relational Models", "text": "In fact, it is such that most of them are able to move into another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to move, and in which they are able to change the world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "8.2 Triple Graphs and Probabilistic Graphical Networks", "text": "A triple database consists of binary relationships that are represented as subject-predicate-object-triple. An example of a triple database is: (Jack, knows, Mary). A triple database can be represented as a knowledge diagram, with units as nodes and predicates as directed links pointing from the subject node to the object node. Triple databases are capable of displaying web-based knowledge bases and sociograms that allow multiple types of directed links. Higher-order relationships can be reduced to binary relationships by introducing auxiliary units (\"empty nodes\"). Figure 2 shows an example of a triple diagram. The resource description framework (RDF) is based on triple bases and is the basic data model of the Linked Open Data of the Semantic Web. In the analysis of social networks, nodes would be individuals or actors and linkages, and links correspond to variations.For each triple variable, a random number is introduced as these random 2 red figures."}, {"heading": "8.3 Directed Relational Models", "text": "The probability distribution of a directed relationship model, i.e. a Bayesian relational model, can be written as P ({XR, t} R, t), while XR, t P (XR, t | par (XR, t))) denotes a certain random variable. (1) In a graphical representation, directed arcs point from all parent nodes par (XR, t) to the node XR, t (Figure 2). Since Equation 1 specifies the model, it requires specifying the parents of a node and specifying the probable dependence of a node in view of the states of its parent nodes. In specifying the former, one often follows a causal order of the nodes, i.e., one assumes that the parent nodes and their descendants are causally influenced. An important limitation is that the most important steps resulting are not directed to the direction curve (i.e.), but to the direction curve."}, {"heading": "8.3.1 Probabilistic Relational Models", "text": "Probabilistic relationship models (PRMs) were one of the first published directed relationship models and attracted great interest in the statistical community of machine learning [17, 10]. An example of a PRM is in Figure 3. PRMs combine a frame-based (i.e. object-oriented) logical representation with probabilistic semantics based on directed graphical models. PRM provides a template for specifying the graphical probabilistic structure and quantifying probabilistic dependencies for each PRM reason. In the basic PRM models, only the attributes of entities are uncertain, whereas the relationships between entities are assumed to be known. Of course, this assumption greatly simplifies the model. Subsequently, PRMs have been expanded to take into account also the case where relationships between entities are unknown, which is called structural uncertainty in the PRM framework [10]. In PRMs, one can distinguish parameter learning from structural learning. In the simplest case, the dependency structure and the uncertainties can be regarded as the preditors of all truth."}, {"heading": "8.3.2 More Directed Relational Graphical Models", "text": "A Bayesian logic program is defined as a set of Bayesian clauses [16]. A Bayesian clause specifies the conditional probability distribution of a random variable, taking into account its parents. A peculiarity is that for a given random variable, several such conditional probability distributions can be given and combined on the basis of various combination rules (e.g. noisy-or). In a Bayesian logic program, there is a conditional probability distribution for each clause, and for each random variable there is a combination rule. Relational Bayesian networks [14] are related to Bayesian logic programs and use probability formulas to specify conditional probabilities. The models of probability dependence (PER) [12] are related to the PRM framework and use the entity relationship model as the basis, which is frequently used in designing a relational database. Relational dependency networks [22] are also part of the dependency model of a small-scale dependency family, and the dependency model of a small-scale dependency system."}, {"heading": "8.4 Undirected Relational Graphical Models", "text": "The probability distribution of an undirected graphical model, i.e. a Markov network, is written as a log-linear model in formP (X = x) = 1Z exp \u2211 i wifi (xi), in which the attribute functions fi can be any real function on the set xi x and where wi-R. In a probabilistic graphical representation, undirected edges are formed between all nodes that occur jointly in a attribute function. Consequently, all nodes that occur jointly in a function form a clique in the graphical representation. Z is the partition function that normalizes the distribution. A great advantage is that undirected graphical models can elegantly model symmetrical dependencies common in social networks."}, {"heading": "8.4.1 Markov Logic Network (MLN)", "text": "A Markov logic network (MLN | C) is a probable logic that combines Markov networks with first-order logic. In MLNs, the random variables representing soil predicates are part of a Markov network whose dependency structure is derived from a set of first-order logic formulas (Figure 4).Formally, an MLN L is defined as: Let Fi be a first-order formula (i.e., a logical expression containing constants, variables, functions, and predicates), and let wi-R be a weight associated with each formula. Then, L is defined as a set of pairs (Fi, wi).From L the base Markov network ML, C is generated as follows. Generate nodes (random variables) by introducing a binary node for each possible grounding of each predicate that appears in L."}, {"heading": "8.4.2 Relational Markov Networks (RMNs)", "text": "RMNs generalize many concepts of PRMs to undirected relationship models [37]. RMNs use conjunctive database queries as clique templates, where a clique in an undirected graph is a subset of its nodes, so that all two nodes in the subset are connected by an edge. RMNs are usually trained in a discriminatory manner. Unlike MLNs and similar to PRMs, RMNs do not assume a closed world during learning."}, {"heading": "8.5 Relational Latent Variable Models", "text": "In the approaches described so far, the structures in the graphical models have been either defined with expert knowledge or learned directly from data using some form of structural learning. Both can be problematic, as appropriate expert knowledge may not be available, while structural learning can be very time-consuming and may lead to local optima that are difficult to interpret. In this context, the advantage of relational latent variable models lies in the fact that the structure in the associated graphical models is defined exclusively by the units and relationships in the domain. The additional complexity of working with a latent representation is offset by the great simplification by avoiding structural learning. In the following discussion, we assume that data is available in triple format; generalizations to relational databases have been described [41, 19]."}, {"heading": "8.5.1 The IHRM: A Latent Class Model", "text": "The infinite hidden relationship model (IHRM) [41] (a.k.a infinite relational model [15]) is a generalization to a probabilistic mixing model in which a latent variable is associated with the states 1,..., R of each unit. If the latent variable for the subject is s = i in state r and the latent variable for the object o = j in state q, then the triple (s = i, p = k, o = j) exists with the probability P (Xk (i, j) | r, q). Since the latent states are not observed, we obtain the relative states P (Xk (i, j) | i, j) P (Xk (i, j) | r, q). Since the latent states are not observed, we can obtain the model of the relative states in figure 5."}, {"heading": "8.5.2 RESCAL: A Latent Factor Model", "text": "The main differences are, firstly, that the Latin variables are not able to describe the factors mentioned, and, secondly, that they are not able to identify the factors mentioned, how they are able to achieve the objectives mentioned, secondly, that they are not able to achieve the objectives mentioned, and thirdly, that they are able to achieve their objectives."}, {"heading": "9 Key Applications", "text": "Typical applications of RATGRAFigure 6: In RESCAL, Eq.2 describes a decomposition of the tensor F = f (:,::,:) into the factor matrix A = a (:,:) and the core tensor G = g (:,:,:). In the multirelational adjacency tensor on the left, two modes represent the units in the domain and the third mode the relationship type. The i-th series of matrix A contains the factors of the i-th unit. GR is a disk in the G-tensor and encodes the relationship-type factor interactions. Factorization can be interpreted as restricted Tucker2 decomposition. Processing, medical decision support and linked open data."}, {"heading": "10 Future Directions", "text": "As a number of publications have shown, the best results can be achieved by integrating factorization approaches with user-defined or learned rule patterns [23, 5]. Most interesting in recent years have been projects with large knowledge graphics, in which performance and scalability could be clearly demonstrated [5, 24]. Applying relational learning to sequential data and time series opens up new areas of application, for example in clinical decision support and sensor networks [7, 8]. [38] Studies on the relevance of relational learning for cognitive brain function."}], "references": [{"title": "Mixed membership stochastic blockmodels", "author": ["Edoardo M. Airoldi", "David M. Blei", "Stephen E. Fienberg", "Eric P. Xing"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2008}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary G. Ives"], "venue": "In ISWC/ASWC,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Translating embeddings for modeling multirelational data", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "Markov logic: A unifying framework for statistical relational learning", "author": ["Pedro Domingos", "Matthew Richardson"], "venue": "Introduction to Statistical Relational Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Inductive logic programming in a nutshell", "author": ["Saso Dzeroski"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Predicting sequences of clinical events by using a personalized temporal latent embedding model", "author": ["Crist\u00f3bal Esteban", "Danilo Schmidt", "Denis Krompa\u00df", "Volker Tresp"], "venue": "In Healthcare Informatics (ICHI), 2015 International Conference on,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Predicting the co-evolution of event and knowledge graphs", "author": ["Crist\u00f3bal Esteban", "Volker Tresp", "Yinchong Yang", "Denis Baier", "Stephan Krompa\u00df"], "venue": "In International Conference on Information Fusion,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Learning probabilistic relational models", "author": ["Nir Friedman", "Lise Getoor", "Daphne Koller", "Avi Pfeffer"], "venue": "In IJCAI,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1999}, {"title": "Probabilistic relational models", "author": ["Lise Getoor", "Nir Friedman", "Daphne Koller", "Avi Pferrer", "Benjamin Taskar"], "venue": "Introduction to Statistical Relational Learning,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "Dependency networks for inference, collaborative filtering, and data visualization", "author": ["David Heckerman", "David Maxwell Chickering", "Christopher Meek", "Robert Rounthwaite", "Carl Myers Kadie"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Probabilistic entity-relationship models, prms, and plate models", "author": ["David Heckerman", "Christopher Meek", "Daphne Koller"], "venue": "Introduction to Statistical Relational Learning,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Nonlinear markov networks for continuous variables", "author": ["Reimar Hofmann", "Volker Tresp"], "venue": "In NIPS,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Relational bayesian networks", "author": ["Manfred Jaeger"], "venue": "In UAI, pages 266\u2013273,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1997}, {"title": "Learning systems of concepts with an infinite relational model", "author": ["Charles Kemp", "Joshua B. Tenenbaum", "Thomas L. Griffiths", "Takeshi Yamada", "Naonori Ueda"], "venue": "In AAAI,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2006}, {"title": "Bayesian logic programs", "author": ["Kristian Kersting", "Luc De Raedt"], "venue": "CoRR, cs.AI/0111058,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Probabilistic frame-based systems", "author": ["Daphne Koller", "Avi Pfeffer"], "venue": "In AAAI/IAAI, pages 580\u2013587,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1998}, {"title": "Type-constrained representation learning in knowledge graphs", "author": ["Denis Krompa\u00df", "Stephan Baier", "Volker Tresp"], "venue": "In International Semantic Web Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Probabilistic latent-factor database models. Linked Data for Knowledge Discovery, page", "author": ["Denis Krompa\u00df", "Xueyian Jiang", "Maximilian Nickel", "Volker Tresp"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Graph kernels for rdf data", "author": ["Uta L\u00f6sch", "Stephan Bloehdorn", "Achim Rettinger"], "venue": "In ESWC,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Inductive logic programming", "author": ["Stephen Muggleton"], "venue": "New Generation Comput.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1991}, {"title": "Dependency networks for relational data", "author": ["Jennifer Neville", "David Jensen"], "venue": "In ICDM, pages 170\u2013177,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Reducing the rank in relational factorization models by including observable patterns", "author": ["Maximilian Nickel", "Xueyan Jiang", "Volker Tresp"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "A review of relational machine learning for knowledge graphs", "author": ["Maximilian Nickel", "Kevin Murphy", "Volker Tresp", "Evgeniy Gabrilovich"], "venue": "Proceedings of the IEEE,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Holographic embeddings of knowledge graphs", "author": ["Maximilian Nickel", "Lorenzo Rosasco", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1510.04935,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}, {"title": "Factorizing yago: scalable machine learning for linked data", "author": ["Maximilian Nickel", "Volker Tresp", "Hans-Peter Kriegel"], "venue": "In WWW,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "Estimation and prediction for stochastic blockstructures", "author": ["Krzysztof Nowicki", "Tom A B Snijders"], "venue": "Journal of the American Statistical Association,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2001}, {"title": "Learning logical definitions from relations", "author": ["J. Ross Quinlan"], "venue": "Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1990}, {"title": "A statistical relational model for trust learning", "author": ["Achim Rettinger", "Matthias Nickles", "Volker Tresp"], "venue": "In AAMAS", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "Introducing the knowledge graph: things, not strings", "author": ["Amit Singhal"], "venue": "Technical report, Ofcial Google Blog,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2012}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2013}, {"title": "Yago: a core of semantic knowledge", "author": ["Fabian M. Suchanek", "Gjergji Kasneci", "Gerhard Weikum"], "venue": "In WWW,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Probabilistic Databases", "author": ["Dan Suciu", "Dan Olteanu", "Christopher R\u00e9", "Christoph Koch"], "venue": "Synthesis Lectures on Data Management. Morgan & Claypool Publishers,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Discriminative probabilistic models for relational data", "author": ["Benjamin Taskar", "Pieter Abbeel", "Daphne Koller"], "venue": "In UAI,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2002}, {"title": "Learning with memory embeddings", "author": ["Volker Tresp", "Crist\u00f3bal Esteban", "Yinchong Yang", "Stephan Baier", "Denis Krompa\u00df"], "venue": "arXiv preprint arXiv:1511.07972,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2015}, {"title": "Materializing and querying learned knowledge", "author": ["Volker Tresp", "Yi Huang", "Markus Bundschus", "Achim Rettinger"], "venue": "In First ESWC Workshop on Inductive Reasoning and Machine Learning on the Semantic Web (IRMLeS", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2009}, {"title": "Multi-relational learning with gaussian processes", "author": ["Zhao Xu", "Kristian Kersting", "Volker Tresp"], "venue": "In IJCAI,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2009}, {"title": "Infinite hidden relational models", "author": ["Zhao Xu", "Volker Tresp", "Kai Yu", "Hans-Peter Kriegel"], "venue": "In UAI,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Stochastic relational models for discriminative link prediction", "author": ["Kai Yu", "Wei Chu", "Shipeng Yu", "Volker Tresp", "Zhao Xu"], "venue": "In NIPS,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}], "referenceMentions": [{"referenceID": 20, "context": "As a field, ILP was introduced in a seminal paper by Muggleton [21].", "startOffset": 63, "endOffset": 67}, {"referenceID": 28, "context": "A very early and still very influential algorithm is Quinlan\u2019s FOIL [29].", "startOffset": 68, "endOffset": 72}, {"referenceID": 16, "context": "Statistical relational learning started around the beginning of the millennium with the work by Koller, Pfeffer, Getoor and Friedman [17, 9].", "startOffset": 133, "endOffset": 140}, {"referenceID": 8, "context": "Statistical relational learning started around the beginning of the millennium with the work by Koller, Pfeffer, Getoor and Friedman [17, 9].", "startOffset": 133, "endOffset": 140}, {"referenceID": 36, "context": "The Semantic Web, Linked Open Data are producing vast quantities of relational data and [39, 27] describe the application of statistical relational learning to these emerging fields.", "startOffset": 88, "endOffset": 96}, {"referenceID": 26, "context": "The Semantic Web, Linked Open Data are producing vast quantities of relational data and [39, 27] describe the application of statistical relational learning to these emerging fields.", "startOffset": 88, "endOffset": 96}, {"referenceID": 23, "context": "[24] is a recent review on the application of relational learning to knowledge graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "An interesting application is the semi-automatic completion of knowledge graphs by analysing information from the Web and other sources, in combination with relational learning, which exploits the information already present on the knowledge graph [5].", "startOffset": 248, "endOffset": 251}, {"referenceID": 19, "context": "For example a kernel can be defined based on counting the substructures of interest in the intersection of two graphs defined by neighborhoods of the two entities [20] (see also the discussion on RDF graphs further down).", "startOffset": 163, "endOffset": 167}, {"referenceID": 5, "context": "Traditionally, ILP involves a systematic search for sensible relational features that form the rule premises [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "Often in machine learning some form of a local closed-world assumption is applied with a mixture of true, false and unknown ground predicates [5, 18].", "startOffset": 142, "endOffset": 149}, {"referenceID": 17, "context": "Often in machine learning some form of a local closed-world assumption is applied with a mixture of true, false and unknown ground predicates [5, 18].", "startOffset": 142, "endOffset": 149}, {"referenceID": 33, "context": "In probabilistic databases [36] the canonical representation is used in tuple-independent databases, while multi-state random variables are used in block-independent-disjoint (BID) databases.", "startOffset": 27, "endOffset": 31}, {"referenceID": 16, "context": "Probabilistic relational models (PRMs) were one of the first published directed relational models and found great interest in the statistical machine learning community [17, 10].", "startOffset": 169, "endOffset": 177}, {"referenceID": 9, "context": "Probabilistic relational models (PRMs) were one of the first published directed relational models and found great interest in the statistical machine learning community [17, 10].", "startOffset": 169, "endOffset": 177}, {"referenceID": 9, "context": "Subsequently, PRMs have been extended to also consider the case that relationships between entities are unknown, which is called structural uncertainty in the PRM framework [10].", "startOffset": 173, "endOffset": 177}, {"referenceID": 15, "context": "A Bayesian logic program is defined as a set of Bayesian clauses [16].", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Relational Bayesian networks [14] are related to Bayesian logic programs and use probability formulae for specifying conditional probabilities.", "startOffset": 29, "endOffset": 33}, {"referenceID": 11, "context": "The probabilistic entity-relationship (PER) models [12] are related to the PRM framework and use the entity-relationship model as a basis, which is often used in the design of a relational database.", "startOffset": 51, "endOffset": 55}, {"referenceID": 21, "context": "Relational dependency networks [22] also belong to the family of directed relational models and learn the dependency of a node given its Markov blanket (the smallest node set that make the node of interest independent of the remaining network).", "startOffset": 31, "endOffset": 35}, {"referenceID": 10, "context": "by [11, 13].", "startOffset": 3, "endOffset": 11}, {"referenceID": 12, "context": "by [11, 13].", "startOffset": 3, "endOffset": 11}, {"referenceID": 3, "context": "Then L is defined as a set of pairs (Fi, wi) [32, 4].", "startOffset": 45, "endOffset": 52}, {"referenceID": 34, "context": "RMNs generalize many concepts of PRMs to undirected relational models [37].", "startOffset": 70, "endOffset": 74}, {"referenceID": 3, "context": "Redrawn from [4].", "startOffset": 13, "endOffset": 16}, {"referenceID": 38, "context": "In the following discussion, we assume that data is in triple format; generalizations to relational databases haven been described [41, 19].", "startOffset": 131, "endOffset": 139}, {"referenceID": 18, "context": "In the following discussion, we assume that data is in triple format; generalizations to relational databases haven been described [41, 19].", "startOffset": 131, "endOffset": 139}, {"referenceID": 38, "context": "The infinite hidden relational model (IHRM) [41] (a.", "startOffset": 44, "endOffset": 48}, {"referenceID": 14, "context": "a infinite relational model [15]) is a generalization to a probabilistic mixture model where a latent variable with states 1, .", "startOffset": 28, "endOffset": 32}, {"referenceID": 27, "context": "Models with a finite number of states have been studied as stochastic block models [28].", "startOffset": 83, "endOffset": 87}, {"referenceID": 29, "context": "The IHRM was the first relational model applied to trust learning [31].", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "In [1] the IHRM is generalized to a mixed-membership stochastic block model, where entities can belong to several classes.", "startOffset": 3, "endOffset": 6}, {"referenceID": 25, "context": "2 RESCAL: A Latent Factor Model The RESCAL model was introduced in [26] and follows a similar dependency structure as the IHRM as shown in Figure 5.", "startOffset": 67, "endOffset": 71}, {"referenceID": 32, "context": "One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.", "startOffset": 113, "endOffset": 117}, {"referenceID": 26, "context": "One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.", "startOffset": 170, "endOffset": 174}, {"referenceID": 1, "context": "One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.", "startOffset": 222, "endOffset": 225}, {"referenceID": 30, "context": "One of the great advantages of the RESCAL model is its scalability: RESCAL has been applied to the YAGO ontology [35] with several million entities and 40 relation types [27]! The YAGO ontology, closely related to DBpedia [2] and the Google Knowledge Graph [33], contains formalized knowledge from Wikipedia and other sources.", "startOffset": 257, "endOffset": 261}, {"referenceID": 39, "context": "[42] describes a Gaussian process-based approach for predicting a single relation type, which has been generalized to a mutli-relational setting in [40].", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "[42] describes a Gaussian process-based approach for predicting a single relation type, which has been generalized to a mutli-relational setting in [40].", "startOffset": 148, "endOffset": 152}, {"referenceID": 36, "context": "The SUNS approach [39] is based on a Tucker1 decomposition of the adjacency tensor, which can be computed by a singular value decomposition (SVD).", "startOffset": 18, "endOffset": 22}, {"referenceID": 31, "context": "The Neural Tensor Network [34] combines several tensor decompositions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 2, "context": "Approaches with a smaller memory footprint are TransE [3] and HolE [25].", "startOffset": 54, "endOffset": 57}, {"referenceID": 24, "context": "Approaches with a smaller memory footprint are TransE [3] and HolE [25].", "startOffset": 67, "endOffset": 71}, {"referenceID": 4, "context": "The multiway neural network in the Knowledge Vault project [5] combines the strengths of latent factor models and neural networks and was successfully used in semi-automatic completion of knowledge graphs.", "startOffset": 59, "endOffset": 62}, {"referenceID": 23, "context": "[24] is a recent review on the application of relational learning to knowledge graphs.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "As a number of publications have shown, best results can be achieved by committee solutions integrating factorization approaches with user defined or learned rule patterns [23, 5].", "startOffset": 172, "endOffset": 179}, {"referenceID": 4, "context": "As a number of publications have shown, best results can be achieved by committee solutions integrating factorization approaches with user defined or learned rule patterns [23, 5].", "startOffset": 172, "endOffset": 179}, {"referenceID": 4, "context": "The most interesting application in recent years was in projects involving large knowledge graphs, where performance and scalability could clearly be demonstrated [5, 24].", "startOffset": 163, "endOffset": 170}, {"referenceID": 23, "context": "The most interesting application in recent years was in projects involving large knowledge graphs, where performance and scalability could clearly be demonstrated [5, 24].", "startOffset": 163, "endOffset": 170}, {"referenceID": 6, "context": "The application of relational learning to sequential data and time series opens up new application areas, for example in clinical decision support and sensor networks [7, 8].", "startOffset": 167, "endOffset": 173}, {"referenceID": 7, "context": "The application of relational learning to sequential data and time series opens up new application areas, for example in clinical decision support and sensor networks [7, 8].", "startOffset": 167, "endOffset": 173}, {"referenceID": 35, "context": "[38] studies the relevance of relational learning to cognitive brain functions.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "We provide a survey on relational models. Relational models describe complete networked domains by taking into account global dependencies in the data. Relational models can lead to more accurate predictions if compared to non-relational machine learning approaches. Relational models typically are based on probabilistic graphical models, e.g., Bayesian networks, Markov networks, or latent variable models. Relational models have applications in social networks analysis, the modeling of knowledge graphs, bioinformatics, recommendation systems, natural language processing, medical decision support, and linked data.", "creator": "LaTeX with hyperref package"}}}