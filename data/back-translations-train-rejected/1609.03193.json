{"id": "1609.03193", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Sep-2016", "title": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System", "abstract": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.", "histories": [["v1", "Sun, 11 Sep 2016 18:56:53 GMT  (845kb,D)", "http://arxiv.org/abs/1609.03193v1", "8 pages, 4 figures (7 plots/schemas), 2 tables (4 tabulars)"], ["v2", "Tue, 13 Sep 2016 02:49:05 GMT  (844kb,D)", "http://arxiv.org/abs/1609.03193v2", "8 pages, 4 figures (7 plots/schemas), 2 tables (4 tabulars)"]], "COMMENTS": "8 pages, 4 figures (7 plots/schemas), 2 tables (4 tabulars)", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL", "authors": ["ronan collobert", "christian puhrsch", "gabriel synnaeve"], "accepted": false, "id": "1609.03193"}, "pdf": {"name": "1609.03193.pdf", "metadata": {"source": "CRF", "title": "Wav2Letter: an End-to-End ConvNet-based Speech Recognition System", "authors": ["Ronan Collobert", "Christian Puhrsch"], "emails": ["locronan@fb.com", "cpuhrsch@fb.com", "gab@fb.com"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to achieve our objectives, and that we are able to achieve these objectives."}, {"heading": "2 Architecture", "text": "Our speech recognition system is a standard Convolutionary Neural Network [12] powered by various functions trained by an alternative to the Connectionist Temporal Classification (CTC) [6] and coupled with a simple beam locator decoder. In the following subsections, we will go into each of these components.2.1 Characteristics We will consider three types of input characteristics for our model: MFCCs, power spectrum and raw wave. MFCCs are carefully designed language-specific characteristics that are often found in classic HMM / GMM speech systems [27] due to their dimensional compression (13 coefficients are often sufficient to cover speech frequencies). Power spectrum characteristics are found in most recent acoustic modeling functions [1]. Raw Wave has been somewhat explored in a few recent papers [15, 16]. ConvNets have the advantage of being flexible enough to be used with these two characteristics."}, {"heading": "2.2 ConvNet Acoustic Model", "text": "The acoustic models we look at in this paper are all based on standard 1D-Convolutionary Neural Networks (ConvNets). ConvNets interleave convolution operations with pointwise non-linearity operations. Often, ConvNets also go to pooling layers: These types of layers allow the network to \"see\" a larger context without increasing the number of parameters by locally aggregating the previous conversion operation outputs. Instead, our networks use bypassing convolutions. (xt) t = 1... Tx an output sequence with Tx dimension vectors, a convolution with kernel width and dy frame output output output output-output output output output-output output-output output-output-output-to-output output output-output-output-output-output-output-output-output-output-output-output-output-to-output-output-output-output-output-output-output-to-output-output-output-output-dimension vectors, a convolution with kernel width and dy-frame output-output-output-output-output output output-to-output output output Output-output-output Output-output-to-output-output-output-output-output-output-output-to-output-output-output-output-output-output-to-output-output-output-to-output-output-to-output-output-output-to-output-output-to-output-output-to-output-output-output-to-output-output-to-output-output-to-output-output-output-to-output-output-to-output-output-to-output-to-output-to-output-output-to-output-output-to-output-output-to-output-output-to-output-to-output-to-output-output-to-output-to-output-output-to-output-to-output-output-to-output-to-output-output-output-to-output-to-output-to-output-to-to-output-output-to-to-output-output-to-output-output-to-output-to-output-output-to-output-to-output-to-to-"}, {"heading": "2.3 Inferring Segmentation with AutoSegCriterion", "text": "Most large language databases offer only one text transcription for each audio file. In a classification framework (and given our acoustic model, one produces letter predictions), one would need to segment each letter in the transcription to learn the model properly. Unfortunately, the best segmentation of each letter is derived according to the current model by maximizing the common likelihood of the letter (or any sub-word unit), using transcription and input sequence. (ii) During the maximization step, the model is optimized by minimizing a frame level criterion (now fixed), segmentation is commonly used."}, {"heading": "2.4 Beam-Search Decoder", "text": "We have written our own one-pass decoder, which performs a simple beam search with beam emission, histogram section and speech model lubrication [26]. We have kept the decoder as simple as possible (below 1000 lines of C code). We have not implemented any type of model fit before decoding, nor any Word graph rescoring. Our decoder relies on KenLM [9] for the voice modeling part. It also accepts unnormalized acoustic values (transitions and emissions from the acoustic model) as input. The decoder tries to maximize the following: L (\u03b8) = log the probability of the speech model through a transcription."}, {"heading": "3 Experiments", "text": "We implemented everything with Torch71. Both the ASG criterion and the decoder were implemented in C (and then linked to Torch), and we consider LibriSpeech, a large language database available for free download [18], to be a benchmark. LibriSpeech has its own traction, validation and test kits. Unless otherwise specified, we used all available data (about 1000 hours of audio files) to train and validate our models. We use the original 16 KHz sampling rate. The L vocabulary contains 30 graphs: the standard English alphabet plus apostrophe, silence, and two special \"repeat graphics\" encoding the duplication (once or twice) of the previous letter (see Section 2.3). The architecture hyperparameters and decoder functions have been aligned with the validation functions."}, {"heading": "3.1 Results", "text": "We have chosen the CTC criterion provided by Baidu3, but both criteria result in the same LER. To compare the speed, we report performance for sequence sizes as originally reported by Baidu, but also for longer sequence sizes that correspond to our average usage spectrum. 2http: / / www.openslr.org / 11. 3https: / / github.com / baidu-research / warp-ctc.case. ASG appears faster on long sequences, even though it only runs on CPU. GPU-CTC implementation seems to focus on larger vocabularies."}, {"heading": "4 Conclusion", "text": "We have introduced a simple end-to-end automated speech recognition system that combines a standard 1D Convolutionary Neural Network, a sequence criterion that can infer segmentation, and a simple beam detection decoder, and the decoding results are competitive on the LibriSpeech corpus with MFCC capabilities (7.2% WER) and promising in power spectrum and raw language (9.4% WER and 10.1% WER, respectively). We have shown that our AutoSegCriterion can be faster than CTC [6] and just as accurate (Table 1), detaching our approach from HMM / GMM pre-training and power alignment, and not as computing-intensive as RNN-based approaches [1] (on average, a LibriSpeech set is processed by our ConvNet in less than 60 ms, and the decoder operates at 8.6 x on a single thread)."}], "references": [{"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. AMODEI", "R. ANUBHAI", "E. BATTENBERG", "C. CASE", "J. CASPER", "B. CATANZARO", "J. CHEN", "M. CHRZANOWSKI", "A. COATES", "G DIAMOS"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Maximum mutual information estimation of hidden markov model parameters for speech recognition", "author": ["L.R. BAHL", "P.F. BROWN", "P.V. DE SOUZA", "R.L. MERCER"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1986}, {"title": "Une approche theorique de l\u2019apprentissage connexionniste et applications a la reconnaissance de la parole", "author": ["L. BOTTOU"], "venue": "PhD thesis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1991}, {"title": "Global training of document processing systems using graph transformer networks", "author": ["L. BOTTOU", "Y. BENGIO", "Y. LE CUN"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Hypothesis spaces for minimum bayes risk training in large vocabulary speech recognition", "author": ["M. GIBSON", "T. HAIN"], "venue": "In Proceedings of INTERSPEECH (2006),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2006}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["A. GRAVES", "S. FERN\u00c1NDEZ", "F. GOMEZ", "J. SCHMIDHUBER"], "venue": "In Proceedings of the 23rd international conference on Machine learning (2006),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["A. GRAVES", "MOHAMED", "A.-R", "G. HINTON"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["A. HANNUN", "C. CASE", "J. CASPER", "B. CATANZARO", "G. DIAMOS", "E. ELSEN", "R. PRENGER", "S. SATHEESH", "S. SENGUPTA", "A COATES"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Scalable modified kneser-ney language model estimation", "author": ["K. HEAFIELD", "I. POUZYREVSKY", "J.H. CLARK", "P. KOEHN"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. HINTON", "L. DENG", "D. YU", "G.E. DAHL", "MOHAMED", "A.-R", "N. JAITLY", "A. SENIOR", "V. VANHOUCKE", "P. NGUYEN", "SAINATH", "T. N"], "venue": "Signal Processing Magazine, IEEE 29,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J. LAFFERTY", "A. MCCALLUM", "F. PEREIRA"], "venue": "In Eighteenth International Conference on Machine Learning,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "Convolutional networks for images, speech, and time series", "author": ["Y. LECUN", "Y. BENGIO"], "venue": "The handbook of brain theory and neural networks 3361,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1995}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. MIAO", "M. GOWAYYED", "F. METZE"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Acoustic modeling using deep belief networks. Audio, Speech, and Language Processing", "author": ["MOHAMED", "A.-R", "G.E. DAHL", "G. HINTON"], "venue": "IEEE Transactions on 20,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks", "author": ["D. PALAZ", "R. COLLOBERT", "M.M. DOSS"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Analysis of cnn-based speech recognition system using raw speech as input", "author": ["D. PALAZ", "R COLLOBERT"], "venue": "In Proceedings of Interspeech", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Joint phoneme segmentation inference and classification using crfs", "author": ["D. PALAZ", "M. MAGIMAI-DOSS", "R. COLLOBERT"], "venue": "In Signal and Information Processing (GlobalSIP),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Librispeech: an asr corpus based on public domain audio books", "author": ["V. PANAYOTOV", "G. CHEN", "D. POVEY", "S. KHUDANPUR"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Jhu aspire system: Robust lvcsr with tdnns, i-vector adaptation, and rnn-lms", "author": ["V. PEDDINTI", "G. CHEN", "V. MANOHAR", "T. KO", "D. POVEY", "S. KHUDANPUR"], "venue": "In Proceedings of the IEEE Automatic Speech Recognition and Understanding Workshop", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "A time delay neural network architecture for efficient modeling of long temporal contexts", "author": ["V. PEDDINTI", "D. POVEY", "S. KHUDANPUR"], "venue": "In Proceedings of INTERSPEECH", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "The ibm 2015 english conversational telephone speech recognition system", "author": ["G. SAON", "KUO", "H.-K. J", "S. RENNIE", "M. PICHENY"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Speaker adaptation of neural network acoustic models using i-vectors", "author": ["G. SAON", "H. SOLTAU", "D. NAHAMOO", "M. PICHENY"], "venue": "ASRU", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2013}, {"title": "Gmm-free dnn training", "author": ["A. SENIOR", "G. HEIGOLD", "M. BACCHIANI", "H. LIAO"], "venue": "In Proceedings of ICASSP", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. SERCU", "C. PUHRSCH", "B. KINGSBURY", "Y. LECUN"], "venue": "arXiv preprint arXiv:1509.08967", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Joint training of convolutional and nonconvolutional neural networks", "author": ["H. SOLTAU", "G. SAON", "T.N. SAINATH"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Improvements in beam search", "author": ["V. STEINBISS", "TRAN", "B.-H", "NEY"], "venue": "ICSLP", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1994}, {"title": "The htk tied-state continuous speech recogniser", "author": ["P.C. WOODLAND", "S.J. YOUNG"], "venue": "In Eurospeech", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1993}], "referenceMentions": [{"referenceID": 5, "context": "We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC [6] while being simpler.", "startOffset": 129, "endOffset": 132}, {"referenceID": 17, "context": "We show competitive results in word error rate on the Librispeech corpus [18] with MFCC features, and promising results from raw waveform.", "startOffset": 73, "endOffset": 77}, {"referenceID": 26, "context": "This approach takes its roots in HMM/GMM training [27].", "startOffset": 50, "endOffset": 54}, {"referenceID": 13, "context": "The improvements brought by deep neural networks (DNNs) [14, 10] and convolutional neural networks (CNNs) [24, 25] for acoustic modeling only extend this training pipeline.", "startOffset": 56, "endOffset": 64}, {"referenceID": 9, "context": "The improvements brought by deep neural networks (DNNs) [14, 10] and convolutional neural networks (CNNs) [24, 25] for acoustic modeling only extend this training pipeline.", "startOffset": 56, "endOffset": 64}, {"referenceID": 23, "context": "The improvements brought by deep neural networks (DNNs) [14, 10] and convolutional neural networks (CNNs) [24, 25] for acoustic modeling only extend this training pipeline.", "startOffset": 106, "endOffset": 114}, {"referenceID": 24, "context": "The improvements brought by deep neural networks (DNNs) [14, 10] and convolutional neural networks (CNNs) [24, 25] for acoustic modeling only extend this training pipeline.", "startOffset": 106, "endOffset": 114}, {"referenceID": 17, "context": "The current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too [18, 20], with an additional step of speaker adaptation [22, 19].", "startOffset": 114, "endOffset": 122}, {"referenceID": 19, "context": "The current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too [18, 20], with an additional step of speaker adaptation [22, 19].", "startOffset": 114, "endOffset": 122}, {"referenceID": 21, "context": "The current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too [18, 20], with an additional step of speaker adaptation [22, 19].", "startOffset": 170, "endOffset": 178}, {"referenceID": 18, "context": "The current state of the art on Librispeech (the dataset that we used for our evaluations) uses this approach too [18, 20], with an additional step of speaker adaptation [22, 19].", "startOffset": 170, "endOffset": 178}, {"referenceID": 22, "context": "Recently, [23] proposed GMM-free training, but the approach still requires to generate a force alignment.", "startOffset": 10, "endOffset": 14}, {"referenceID": 6, "context": "An approach that cut ties with the HMM/GMM pipeline (and with force alignment) was to train with a recurrent neural network (RNN) [7] for phoneme transcription.", "startOffset": 130, "endOffset": 133}, {"referenceID": 7, "context": "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in [8, 13, 21, 1], trained with a sequence criterion [6].", "startOffset": 98, "endOffset": 112}, {"referenceID": 12, "context": "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in [8, 13, 21, 1], trained with a sequence criterion [6].", "startOffset": 98, "endOffset": 112}, {"referenceID": 20, "context": "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in [8, 13, 21, 1], trained with a sequence criterion [6].", "startOffset": 98, "endOffset": 112}, {"referenceID": 0, "context": "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in [8, 13, 21, 1], trained with a sequence criterion [6].", "startOffset": 98, "endOffset": 112}, {"referenceID": 5, "context": "There are now competitive end-to-end approaches of acoustic models toppled with RNNs layers as in [8, 13, 21, 1], trained with a sequence criterion [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 12, "context": "Compared to sequence criterion based approaches that train directly from speech signal to graphemes [13], we propose a simple(r) architecture (23 millions of parameters for our best model, vs.", "startOffset": 100, "endOffset": 104}, {"referenceID": 0, "context": "100 millions of parameters in [1]) based on convolutional networks for the acoustic model, toppled with a graph transformer network [4], trained with a simpler sequence criterion.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "100 millions of parameters in [1]) based on convolutional networks for the acoustic model, toppled with a graph transformer network [4], trained with a simpler sequence criterion.", "startOffset": 132, "endOffset": 135}, {"referenceID": 7, "context": "Our word-error-rate on clean speech is slightly better than [8], and slightly worse than [1], in particular factoring that they train on 12,000 hours while we only train on the 960h available in LibriSpeech\u2019s train set.", "startOffset": 60, "endOffset": 63}, {"referenceID": 0, "context": "Our word-error-rate on clean speech is slightly better than [8], and slightly worse than [1], in particular factoring that they train on 12,000 hours while we only train on the 960h available in LibriSpeech\u2019s train set.", "startOffset": 89, "endOffset": 92}, {"referenceID": 14, "context": "some of our models are also trained on the raw waveform, as in [15, 16].", "startOffset": 63, "endOffset": 71}, {"referenceID": 15, "context": "some of our models are also trained on the raw waveform, as in [15, 16].", "startOffset": 63, "endOffset": 71}, {"referenceID": 11, "context": "Our speech recognition system is a standard convolutional neural network [12] fed with various different features, trained through an alternative to the Connectionist Temporal Classification (CTC) [6], and coupled with a simple beam search decoder.", "startOffset": 73, "endOffset": 77}, {"referenceID": 5, "context": "Our speech recognition system is a standard convolutional neural network [12] fed with various different features, trained through an alternative to the Connectionist Temporal Classification (CTC) [6], and coupled with a simple beam search decoder.", "startOffset": 197, "endOffset": 200}, {"referenceID": 26, "context": "MFCCs are carefully designed speech-specific features, often found in classical HMM/GMM speech systems [27] because of their dimensionality compression (13 coefficients are often enough to span speech frequencies).", "startOffset": 103, "endOffset": 107}, {"referenceID": 0, "context": "Power-spectrum features are found in most recent deep learning acoustic modeling features [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 14, "context": "Raw wave has been somewhat explored in few recent work [15, 16].", "startOffset": 55, "endOffset": 63}, {"referenceID": 15, "context": "Raw wave has been somewhat explored in few recent work [15, 16].", "startOffset": 55, "endOffset": 63}, {"referenceID": 15, "context": "In our experience, we surprisingly found that using hyperbolic tangents, their piecewise linear counterpart HardTanh (as in [16]) or ReLU units lead to similar results.", "startOffset": 124, "endOffset": 128}, {"referenceID": 15, "context": "Our architecture for raw wave is shown in Figure 1 and is inspired by [16].", "startOffset": 70, "endOffset": 74}, {"referenceID": 1, "context": "Other alternatives have been explored in the context of hybrid HMM/NN systems, such as the MMI criterion [2] which maximizes the mutual information between the acoustic sequence and word sequences or the Minimum Bayse Risk (MBR) criterion [5].", "startOffset": 105, "endOffset": 108}, {"referenceID": 4, "context": "Other alternatives have been explored in the context of hybrid HMM/NN systems, such as the MMI criterion [2] which maximizes the mutual information between the acoustic sequence and word sequences or the Minimum Bayse Risk (MBR) criterion [5].", "startOffset": 239, "endOffset": 242}, {"referenceID": 5, "context": "More recently, standalone neural network architectures have been trained using criterions which jointly infer the segmentation of the transcription while increase the overall score of the right transcription [6, 17].", "startOffset": 208, "endOffset": 215}, {"referenceID": 16, "context": "More recently, standalone neural network architectures have been trained using criterions which jointly infer the segmentation of the transcription while increase the overall score of the right transcription [6, 17].", "startOffset": 208, "endOffset": 215}, {"referenceID": 0, "context": "The most popular one is certainly the Connectionist Temporal Classification (CTC) criterion, which is at the core of Baidu\u2019s Deep Speech architecture [1].", "startOffset": 150, "endOffset": 153}, {"referenceID": 2, "context": "In that respect, avoiding normalized transitions is important to alleviate the problem of \u201clabel bias\u201d [3, 11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 10, "context": "In that respect, avoiding normalized transitions is important to alleviate the problem of \u201clabel bias\u201d [3, 11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 25, "context": "We wrote our own one-pass decoder, which performs a simple beam-search with beam threholding, histogram pruning and language model smearing [26].", "startOffset": 140, "endOffset": 144}, {"referenceID": 8, "context": "Our decoder relies on KenLM [9] for the language modeling part.", "startOffset": 28, "endOffset": 31}, {"referenceID": 17, "context": "We consider as benchmark LibriSpeech, a large speech database freely available for download [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 7, "context": "In (b) we provide Baidu Deep Speech 1 and 2 numbers on LibriSpeech, as a comparison [8, 1].", "startOffset": 84, "endOffset": 90}, {"referenceID": 0, "context": "In (b) we provide Baidu Deep Speech 1 and 2 numbers on LibriSpeech, as a comparison [8, 1].", "startOffset": 84, "endOffset": 90}, {"referenceID": 7, "context": "We observe that we compare very well against Deep Speech 1 & 2 which were trained with much more data [8, 1].", "startOffset": 102, "endOffset": 108}, {"referenceID": 0, "context": "We observe that we compare very well against Deep Speech 1 & 2 which were trained with much more data [8, 1].", "startOffset": 102, "endOffset": 108}], "year": 2017, "abstractText": "This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC [6] while being simpler. We show competitive results in word error rate on the Librispeech corpus [18] with MFCC features, and promising results from raw waveform.", "creator": "LaTeX with hyperref package"}}}