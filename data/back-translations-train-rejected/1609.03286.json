{"id": "1609.03286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks", "abstract": "Natural language understanding (NLU) is a core component of a spoken dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark Air Travel Information System (ATIS) data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the performance of the state-of-the-art neural network based frameworks.", "histories": [["v1", "Mon, 12 Sep 2016 07:29:59 GMT  (1620kb,D)", "http://arxiv.org/abs/1609.03286v1", "11 pages, 5 figures"]], "COMMENTS": "11 pages, 5 figures", "reviews": [], "SUBJECTS": "cs.AI cs.CL", "authors": ["yun-nung chen", "dilek hakkani-tur", "gokhan tur", "asli celikyilmaz", "jianfeng gao", "li deng"], "accepted": false, "id": "1609.03286"}, "pdf": {"name": "1609.03286.pdf", "metadata": {"source": "CRF", "title": "Knowledge as a Teacher: Knowledge-Guided Structural Attention Networks", "authors": ["Yun-Nung Chen", "Dilek Hakkani-T\u00fcr", "Gokhan Tur", "Asli Celikyilmaz", "Jianfeng Gao", "Li Deng"], "emails": ["gokhan}@ieee.org", "deng}@microsoft.com"], "sections": [{"heading": "1 Introduction", "text": "Over the past decade, target-oriented spoken dialog systems (SDS), such as Microsoft's Cortana and Apple's Siri virtual personal assistants, have been integrated into various devices, enabling the user to speak freely with systems to perform tasks more efficiently. A key component of these conversation systems is the Natural Language Understanding (NLU) module - it refers to the targeted understanding of human language aimed at machines (Tur and De Mori, 2011).The goal of such a \"target\" understanding is to transform the recognized user language into a task-specific semantic representation of the user's intention, adapting to the background knowledge and sources of action for task completion. The dialogue manager then interprets the semantics of the user's desire and the associated back-end results, and decides the most appropriate system action by using the semantic contexts and user-specific meta of the task completion, such as XTeo and XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTeo, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex 1999, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, XTex, X"}, {"heading": "O O O O B-origin O B-dest I-destO", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Related Work", "text": "There is an emerging trend of learning representations at various levels, such as word embedding (Mikolov et al., 2013), character embedding (Ling et al., 2015), and sentence embedding (Celikyilmaz and Hakkani-Tur, 2015). Besides completely unsupervised embedding of learning, knowledge bases have been widely used to learn embedding of entities with specific functions or relationships (Celikyilmaz and Hakkani-Tur, 2014). Unlike in previous work, this paper focuses on component-like substructures that are informative to understand."}, {"heading": "3 Knowledge-Guided Structural Attention Networks (K-SAN)", "text": "For the NLU task, our model is to predict semantic tags ~ y = y1,..., yT for each word / sign by including knowledge-led structures in an expression with a sequence of words / characters ~ s = w1,..., wT. The proposed model is in Figure 2. The module Knowledge Coding first uses external knowledge to generate a linguistic structure for the utterance, in which a discrete set of knowledge-led substructures {xi} is encoded in vector representations (\u00a7 3.1). The model learns the representation for the whole sentence by paying different attention to the substructures (\u00a7 3.2). Subsequently, the learned vector encoding the knowledge-led structure is used to improve the semantic tagger (\u00a7 4)."}, {"heading": "3.1 Knowledge Encoding Module", "text": "The prior knowledge gained from external resources, such as dependency relationships, knowledge databases, etc., provides more abundant information for deciding the semantic tags given by an input expression. This essay takes dependency relationships as an example of encoding knowledge, and other structured relations can be applied in the same way.The input expression is analyzed by a dependency saver, and the substructures are constructed according to the paths from root to all leaves (Chen and Manning, 2014).For example, the dependency analysis of the expression \"Show me the flights of cattle to San Francisco\" is presented in Figure 3, where the associated substructures from the parsing tree are obtained for coding knowledge. In this case, we do not use the dependency relationship designations in the experiments for better generalization, since the designations may not always be available for different knowledge resources. Note that the number of substructures cannot be the equivalent number of words in the substructure, because the number of the number of the words cannot be the one in the substructure."}, {"heading": "3.2 Model Architecture", "text": "The model embeds all knowledge-oriented substructures in a continuous space and stores the embedding of all x-th in the knowledge store. The representation of the input utterance is then compared with coded knowledge representations to integrate the supported structure through an attention mechanism. Then, we convert any substructure that reaches from the root to the leaf to estimate the semantic tags. Four main procedures are characterized by the embedding of the substructure in a continuous space."}, {"heading": "4 Recurrent Neural Network Tagger", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Chain-Based RNN Tagger", "text": "Given the fact that this is a model that predicts Y = y1,..., yT, where the tag yi is aligned with the word wi. We use the Elman RNN architecture, consisting of an input layer, a hidden layer, and an output layer (Elman, 1990). The input, hidden, and output layers consist of a series of neurons that represent the input, hidden, and output layers in each step t, wt, ht, and yt, resptively.ht = (Wwt + Uht \u2212 1), (7) y t = softmax (V ht), (8) where there is a smooth bounded function such as tanh, and y t is the probability distribution over semantic tags given the current hidden state. The sequence probability can be formulated asp (~ y)."}, {"heading": "4.2 Knowledge-Guided RNN Tagger", "text": "In order to model the encoded knowledge from earlier spins, for each time step t the knowledge-led sentence representation o in (5) together with the word wt is fed into the RNN model. For the simple RNN, the hidden layer asht = \u03c6 (Mo + Wwt + Uht \u2212 1) (14) can be formulated to replace (7) as in the right block of Figure 2. RNN-GRU can similarly integrate the encoded knowledge, whereby Mo can be added to gating mechanisms for modeling contextual knowledge."}, {"heading": "4.3 Joint RNN Tagger", "text": "Since the chain-based marker and the knowledge-based marker contain different information, the common RNN marker is proposed to balance the information between two model architectures. Figure 4 represents the architecture of the common RNN marker. H1t = \u03c6 (W 1wt + U 1ht \u2212 1), (15) h2t = \u03c6 (Mo + W 2wt + U 2ht \u2212 1), (16) y-t = softmax (V (\u03b1h1t + (1 \u2212 \u03b1) h2t))), (17) where \u03b1 is the weight for balancing chain-based and knowledge-based information. By taking chain-based information (h1t) and knowledge-based information (h2t) together, the common RNN marker is expected to achieve a better generalization, and the performance might be less sensitive to bad structures from external knowledge. In the experiments, \u03b1 is set to ~ 0.5 in order to balance ~ the two sides of the proposed model (the 9)."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Experimental Setup", "text": "The data set for experiments is the benchmark ATIS corpus, which is widely used by the NLU community (Mesnil et al., 2015). In the experiments, 4978 training explanations are selected from class A (context-independent) in the ATIS-2 and ATIS-3, while there are 893 statements from the ATIS-3 Nov93 and Dec94. In the experiments, we use only lexical characteristics. To demonstrate the robustness of the data shortage, we conduct the experiments with 3 different sizes of training data (Small, Medium and Large), with Small being 1 / 40 of the original set, Medium 1 / 10 of the original set and Large being the complete set. The evaluation metrics for NLU are F measures for the predicted slots 1. For experiments with K-SAN, we analyze all data with the Stanford Dependency Parser (Chen and Manning, 2014) and present words as their embedding."}, {"heading": "5.2 Baseline", "text": "To confirm the effectiveness of the proposed model, we compare the performance with the following baseline. \u2022 Baseline: - CRF Tagger (Tur et al., 2010): predicts one asemantic slot for each word with a context window (size = 5). - RNN Tagger (Mesnil et al., 2015): predicts one semantic slot for each word. - CNN Encoder Tagger (Kim, 2014): Tag semantic slots with consideration of sentence embeddings learned through a revolutionary model. \u2022 Structurally: The NLU models use linguistic information when tagging slots, with DCNN and Tree-RNN being the most modern approaches for embedding sentences in linguistic structures. - CRF Tagger (Tur et al., 2010): predicts slots based on the lexical (5-word win window) and syntactical approaches (depending on the head in the Tree et al)."}, {"heading": "5.3 Slot Filling Results", "text": "Table 1 shows the performance of filling slots at different sizes of training data, where there are three sets of data (Small, Medium, and Large use 1 / 40, 1 / 10, and whole training data). For baselines (models without knowledge characteristics), CNN encoder tagger performs best on all datasets. Among structural models (models with knowledge coding), Tree RNN encoder tagger performs better on Small Data, but slightly worse than the DCNN encoder tagger.CNN (Kim, 2014) performs better than DCNN (Ma et al., 2015a) and Tree RNN encoder tagger (Tai et al., 2015), although CNN does not resort to external encoding sets. When comparing NLU performance between baselines and other state-of-the-art models, there are no significant differences."}, {"heading": "5.4 Attention Analysis", "text": "In order to demonstrate the effectiveness of performance enhancement by learning the right attention from much smaller training data based on the proposed model, we present in Figure 5 the visualization of attention for both words and relationships decoded by K-SAN with CNN. The darker color of blocks and lines indicates greater attention to words or relationships. It is clear from the illustration that the words and relationships with higher attention are the most important parts for predicting correct slots, such as origin, destination and time. Furthermore, the difference in attention distribution between three sets of data is not significant; this indicates that our proposed model is capable of paying correct attention to important sub-structures guided by external knowledge, even when training data is scarce."}, {"heading": "5.5 Knowledge Generalization", "text": "Here we compare two types of knowledge formats: dependency tree and Abstract Meaning Representation (AMR). AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directional, acyclic graph (Banarescu et al., 2013) in which nodes represent concepts and steered edges represent the relationships between two concepts. Formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967) in which semantic concepts have been used in AMR to perform multiple NLP tasks (Liu et al., 2015). Unlike syntactic information from dependency trees and syntactic SAN graphs, the AMR graph contains semantic information that can provide more specific conceptual relationships. Figure 6 shows the comparison of a dependency tree and an AMR graph that are related to the same example."}, {"heading": "6 Conclusion", "text": "This paper proposes a novel model, Knowledge-based Structural Attention Networks (K-SAN), which use prior knowledge as an orientation tool to integrate non-flat topologies and to gain appropriate attention for various substructures relevant to specific tasks. Structured information can be gathered from small training data, so that the model exhibits better generalization and robustness. Experiments demonstrate the utility and effectiveness of the proposed model in the language comprehension task, where all knowledge-based substructures captured by different resources help to highlight performance, and current performance is achieved on the ATIS benchmark dataset."}], "references": [{"title": "Abstract meaning representation for sembanking", "author": ["Laura Banarescu", "Claire Bonial", "Shu Cai", "Madalina Georgescu", "Kira Griffitt", "Ulf Hermjakob", "Kevin Knight", "Philipp Koehn", "Martha Palmer", "Nathan Schneider."], "venue": "Proceedings of the Linguistic Annota-", "citeRegEx": "Banarescu et al\\.,? 2013", "shortCiteRegEx": "Banarescu et al\\.", "year": 2013}, {"title": "Convolutional neural network based semantic tagging with entity embeddings", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur."], "venue": "NIPS Workshop on Machine Learning for SLU and Interaction.", "citeRegEx": "Celikyilmaz and Hakkani.Tur.,? 2015", "shortCiteRegEx": "Celikyilmaz and Hakkani.Tur.", "year": 2015}, {"title": "Speech utterance classification", "author": ["Ciprian Chelba", "Monika Mahajan", "Alex Acero."], "venue": "2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP), volume 1, pages I\u2013280. IEEE.", "citeRegEx": "Chelba et al\\.,? 2003", "shortCiteRegEx": "Chelba et al\\.", "year": 2003}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning."], "venue": "EMNLP, pages 740\u2013750.", "citeRegEx": "Chen and Manning.,? 2014", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "Deriving local relational surface forms from dependency-based entity embeddings for unsupervised spoken language understanding", "author": ["Yun-Nung Chen", "Dilek Hakkani-Tur", "Gokan Tur."], "venue": "2014 IEEE Spoken Language Technology Workshop (SLT), pages", "citeRegEx": "Chen et al\\.,? 2014", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Matrix factorization with knowledge graph propagation for unsupervised spoken language understanding", "author": ["Yun-Nung Chen", "William Yang Wang", "Anatole Gershman", "Alexander I Rudnicky."], "venue": "Proceedings of ACL-IJCNLP.", "citeRegEx": "Chen et al\\.,? 2015", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1409.1259.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "The logical form of action sentences", "author": ["Donald Davidson"], "venue": null, "citeRegEx": "Davidson.,? \\Q1967\\E", "shortCiteRegEx": "Davidson.", "year": 1967}, {"title": "Deep belief network based semantic taggers for spoken language understanding", "author": ["Anoop Deoras", "Ruhi Sarikaya."], "venue": "INTERSPEECH, pages 2713\u20132717.", "citeRegEx": "Deoras and Sarikaya.,? 2013", "shortCiteRegEx": "Deoras and Sarikaya.", "year": 2013}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman."], "venue": "Cognitive science, 14(2):179\u2013211.", "citeRegEx": "Elman.,? 1990", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alan Graves", "Abdel-rahman Mohamed", "Geoffrey Hinton."], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6645\u20136649. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Optimizing svms for complex call classification", "author": ["Patrick Haffner", "Gokhan Tur", "Jerry H Wright."], "venue": "2003 IEEE International Conference on", "citeRegEx": "Haffner et al\\.,? 2003", "shortCiteRegEx": "Haffner et al\\.", "year": 2003}, {"title": "Leveraging knowledge graphs for web-scale unsupervised semantic parsing", "author": ["Larry P Heck", "Dilek Hakkani-T\u00fcr", "Gokhan Tur."], "venue": "INTERSPEECH, pages 1594\u20131598.", "citeRegEx": "Heck et al\\.,? 2013", "shortCiteRegEx": "Heck et al\\.", "year": 2013}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u2013 1780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Learning deep structured semantic models for web search using clickthrough data", "author": ["Po-Sen Huang", "Xiaodong He", "Jianfeng Gao", "Li Deng", "Alex Acero", "Larry Heck."], "venue": "Proceedings of the 22nd ACM international conference on Conference on information &", "citeRegEx": "Huang et al\\.,? 2013", "shortCiteRegEx": "Huang et al\\.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Quoc V Le", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1405.4053.", "citeRegEx": "Le and Mikolov.,? 2014", "shortCiteRegEx": "Le and Mikolov.", "year": 2014}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Query understanding enhanced by hierarchical parsing structures", "author": ["Jingjing Liu", "Panupong Pasupat", "Yining Wang", "Scott Cyphers", "James Glass."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 72\u201377. IEEE.", "citeRegEx": "Liu et al\\.,? 2013", "shortCiteRegEx": "Liu et al\\.", "year": 2013}, {"title": "Toward abstractive summarization using semantic representations", "author": ["Fei Liu", "Jeffrey Flanigan", "Sam Thomson", "Norman Sadeh", "Noah A Smith."], "venue": "In Proceedings of the Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Liu et al\\.,? 2015", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Dependency-based convolutional neural networks for sentence embedding", "author": ["Mingbo Ma", "Liang Huang", "Bing Xiang", "Bowen Zhou."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th Interna-", "citeRegEx": "Ma et al\\.,? 2015a", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Knowledge graph inference for spoken dialog systems", "author": ["Yi Ma", "Paul A Crook", "Ruhi Sarikaya", "Eric FoslerLussier."], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5346\u20135350. IEEE.", "citeRegEx": "Ma et al\\.,? 2015b", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Spoken dialogue technology: toward the conversational user interface", "author": ["Michael F McTear."], "venue": "Springer Science & Business Media.", "citeRegEx": "McTear.,? 2004", "shortCiteRegEx": "McTear.", "year": 2004}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": null, "citeRegEx": "Mesnil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in neural information processing systems, pages 3111\u20133119.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Events in the semantics of english: A study in subatomic semantics", "author": ["Terence Parsons"], "venue": null, "citeRegEx": "Parsons.,? \\Q1990\\E", "shortCiteRegEx": "Parsons.", "year": 1990}, {"title": "A speech understanding system based on statistical representation of semantics", "author": ["Roberto Pieraccini", "Evelyne Tzoukermann", "Zakhar Gorelov", "Jean-Luc Gauvain", "Esther Levin", "Chin-Hui Lee", "Jay G Wilpon."], "venue": "1992 IEEE International Conference on", "citeRegEx": "Pieraccini et al\\.,? 1992", "shortCiteRegEx": "Pieraccini et al\\.", "year": 1992}, {"title": "Recurrent neural network and lstm models for lexical utterance classification", "author": ["Suman Ravuri", "Andreas Stolcke."], "venue": "Sixteenth Annual Conference of the International Speech Communication Association.", "citeRegEx": "Ravuri and Stolcke.,? 2015", "shortCiteRegEx": "Ravuri and Stolcke.", "year": 2015}, {"title": "Neural semantic role labeling with dependency path embeddings", "author": ["Michael Roth", "Mirella Lapata."], "venue": "arXiv preprint arXiv:1605.07515.", "citeRegEx": "Roth and Lapata.,? 2016", "shortCiteRegEx": "Roth and Lapata.", "year": 2016}, {"title": "An agendabased dialog management architecture for spoken language systems", "author": ["Alexander Rudnicky", "Wei Xu."], "venue": "IEEE Automatic Speech Recognition and Understanding Workshop, volume 13, page 17.", "citeRegEx": "Rudnicky and Xu.,? 1999", "shortCiteRegEx": "Rudnicky and Xu.", "year": 1999}, {"title": "Deep belief nets for natural language call-routing", "author": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Bhuvana Ramabhadran."], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5680\u20135683. IEEE.", "citeRegEx": "Sarikaya et al\\.,? 2011", "shortCiteRegEx": "Sarikaya et al\\.", "year": 2011}, {"title": "Application of deep belief networks for natural language understanding", "author": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Anoop Deoras."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):778\u2013 784.", "citeRegEx": "Sarikaya et al\\.,? 2014", "shortCiteRegEx": "Sarikaya et al\\.", "year": 2014}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Richard Socher", "Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng."], "venue": "Transactions of the Association for Computational Linguistics, 2:207\u2013218.", "citeRegEx": "Socher et al\\.,? 2014", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["Sainbayar Sukhbaatar", "Jason Weston", "Rob Fergus"], "venue": null, "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le."], "venue": "Advances in neural information processing systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1503.00075.", "citeRegEx": "Tai et al\\.,? 2015", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Spoken language understanding: Systems for extracting semantic information from speech", "author": ["Gokhan Tur", "Renato De Mori."], "venue": "John Wiley & Sons.", "citeRegEx": "Tur and Mori.,? 2011", "shortCiteRegEx": "Tur and Mori.", "year": 2011}, {"title": "What is left to be understood in atis? In Spoken Language Technology Workshop (SLT), 2010 IEEE, pages 19\u201324", "author": ["Gokhan Tur", "Dilek Hakkani-T\u00fcr", "Larry Heck."], "venue": "IEEE.", "citeRegEx": "Tur et al\\.,? 2010", "shortCiteRegEx": "Tur et al\\.", "year": 2010}, {"title": "Towards deeper understanding: Deep convex networks for semantic utterance classification", "author": ["Gokhan Tur", "Li Deng", "Dilek Hakkani-T\u00fcr", "Xiaodong He."], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5045\u2013", "citeRegEx": "Tur et al\\.,? 2012", "shortCiteRegEx": "Tur et al\\.", "year": 2012}, {"title": "Spoken language understanding", "author": ["Ye-Yi Wang", "Li Deng", "Alex Acero."], "venue": "IEEE Signal Processing Magazine, 22(5):16\u201331.", "citeRegEx": "Wang et al\\.,? 2005", "shortCiteRegEx": "Wang et al\\.", "year": 2005}, {"title": "Memory networks", "author": ["Jason Weston", "Sumit Chopra", "Antoine Bordesa."], "venue": "International Conference on Learning Representations (ICLR).", "citeRegEx": "Weston et al\\.,? 2015", "shortCiteRegEx": "Weston et al\\.", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher."], "venue": "arXiv preprint arXiv:1603.01417.", "citeRegEx": "Xiong et al\\.,? 2016", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Convolutional neural network based triangular CRF for joint intent detection and slot filling", "author": ["Puyang Xu", "Ruhi Sarikaya."], "venue": "2013 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), pages 78\u201383. IEEE.", "citeRegEx": "Xu and Sarikaya.,? 2013", "shortCiteRegEx": "Xu and Sarikaya.", "year": 2013}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng."], "venue": "arXiv preprint arXiv:1412.6575.", "citeRegEx": "Yang et al\\.,? 2014", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Recurrent neural networks for language understanding", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu."], "venue": "INTERSPEECH, pages 2524\u20132528.", "citeRegEx": "Yao et al\\.,? 2013", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Spoken language understanding using long short-term memory neural networks", "author": ["Kaisheng Yao", "Baolin Peng", "Yu Zhang", "Dong Yu", "Geoffrey Zweig", "Yangyang Shi."], "venue": "2014 IEEE Spoken Language Technology Workshop (SLT), pages 189\u2013194. IEEE.", "citeRegEx": "Yao et al\\.,? 2014", "shortCiteRegEx": "Yao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 24, "context": "The dialogue manager then interprets the semantics of the user\u2019s request and associated back-end results, and decides the most appropriate system action, by exploiting semantic context and user specific meta-information, such as geo-location and personal preferences (McTear, 2004; Rudnicky and Xu, 1999).", "startOffset": 267, "endOffset": 304}, {"referenceID": 31, "context": "The dialogue manager then interprets the semantics of the user\u2019s request and associated back-end results, and decides the most appropriate system action, by exploiting semantic context and user specific meta-information, such as geo-location and personal preferences (McTear, 2004; Rudnicky and Xu, 1999).", "startOffset": 267, "endOffset": 304}, {"referenceID": 12, "context": "problems, where several classifiers such as support vector machines and maximum entropy have been employed (Haffner et al., 2003; Chelba et al., 2003; Chen et al., 2014).", "startOffset": 107, "endOffset": 169}, {"referenceID": 2, "context": "problems, where several classifiers such as support vector machines and maximum entropy have been employed (Haffner et al., 2003; Chelba et al., 2003; Chen et al., 2014).", "startOffset": 107, "endOffset": 169}, {"referenceID": 4, "context": "problems, where several classifiers such as support vector machines and maximum entropy have been employed (Haffner et al., 2003; Chelba et al., 2003; Chen et al., 2014).", "startOffset": 107, "endOffset": 169}, {"referenceID": 28, "context": "Then slot filling is framed as a word sequence tagging task, where the IOB (in-outbegin) format is applied for representing slot tags as illustrated in Figure 1, and hidden Markov models (HMM) or conditional random fields (CRF) have been employed for slot tagging (Pieraccini et al., 1992; Wang et al., 2005).", "startOffset": 264, "endOffset": 308}, {"referenceID": 41, "context": "Then slot filling is framed as a word sequence tagging task, where the IOB (in-outbegin) format is applied for representing slot tags as illustrated in Figure 1, and hidden Markov models (HMM) or conditional random fields (CRF) have been employed for slot tagging (Pieraccini et al., 1992; Wang et al., 2005).", "startOffset": 264, "endOffset": 308}, {"referenceID": 32, "context": "With the advances on deep learning, deep belief networks (DBNs) with deep neural networks (DNNs) have been applied to domain and intent classification tasks (Sarikaya et al., 2011; Tur et al., 2012; Sarikaya et al., 2014).", "startOffset": 157, "endOffset": 221}, {"referenceID": 40, "context": "With the advances on deep learning, deep belief networks (DBNs) with deep neural networks (DNNs) have been applied to domain and intent classification tasks (Sarikaya et al., 2011; Tur et al., 2012; Sarikaya et al., 2014).", "startOffset": 157, "endOffset": 221}, {"referenceID": 33, "context": "With the advances on deep learning, deep belief networks (DBNs) with deep neural networks (DNNs) have been applied to domain and intent classification tasks (Sarikaya et al., 2011; Tur et al., 2012; Sarikaya et al., 2014).", "startOffset": 157, "endOffset": 221}, {"referenceID": 44, "context": "For slot filling, deep learning has been viewed as a feature generator and the neural architecture can be merged with CRFs (Xu and Sarikaya, 2013).", "startOffset": 123, "endOffset": 146}, {"referenceID": 28, "context": "Recently, Ravuri and Stolcke (2015) proposed an RNN architecture for intent determination.", "startOffset": 10, "endOffset": 36}, {"referenceID": 28, "context": "Recently, Ravuri and Stolcke (2015) proposed an RNN architecture for intent determination. For slot filling, deep learning has been viewed as a feature generator and the neural architecture can be merged with CRFs (Xu and Sarikaya, 2013). Yao et al. (2013) and Mesnil et al.", "startOffset": 10, "endOffset": 257}, {"referenceID": 25, "context": "(2013) and Mesnil et al. (2015) later employed RNNs for sequence labeling in order to perform slot filling.", "startOffset": 11, "endOffset": 32}, {"referenceID": 39, "context": "Furthermore, prior knowledge would help in the tagging of sequences, especially when dealing with previously unseen sequences (Tur et al., 2010; Deoras and Sarikaya, 2013).", "startOffset": 126, "endOffset": 171}, {"referenceID": 9, "context": "Furthermore, prior knowledge would help in the tagging of sequences, especially when dealing with previously unseen sequences (Tur et al., 2010; Deoras and Sarikaya, 2013).", "startOffset": 126, "endOffset": 171}, {"referenceID": 13, "context": "Prior work exploited external web-scale knowledge graphs such as Freebase and Wikipedia for improving NLU (Heck et al., 2013; Ma et al., 2015b; Chen et al., 2014) Liu et al.", "startOffset": 106, "endOffset": 162}, {"referenceID": 23, "context": "Prior work exploited external web-scale knowledge graphs such as Freebase and Wikipedia for improving NLU (Heck et al., 2013; Ma et al., 2015b; Chen et al., 2014) Liu et al.", "startOffset": 106, "endOffset": 162}, {"referenceID": 4, "context": "Prior work exploited external web-scale knowledge graphs such as Freebase and Wikipedia for improving NLU (Heck et al., 2013; Ma et al., 2015b; Chen et al., 2014) Liu et al.", "startOffset": 106, "endOffset": 162}, {"referenceID": 4, "context": ", 2015b; Chen et al., 2014) Liu et al. (2013) and Chen et al.", "startOffset": 9, "endOffset": 46}, {"referenceID": 4, "context": ", 2015b; Chen et al., 2014) Liu et al. (2013) and Chen et al. (2015) proposed approaches that leverage linguistic knowledge encoded in parse trees for language understanding, where the extracted syntactic structural features and semantic dependency features enhance inference model learning, and the model achieves better language understanding performance in various domains.", "startOffset": 9, "endOffset": 69}, {"referenceID": 34, "context": "Even with the emerging paradigm of integrating deep learning and linguistic knowledge for different NLP tasks (Socher et al., 2014), most of the previous work utilized such linguistic knowledge and knowledge bases as additional features as input to neural networks, and then learned the models for tagging sequences.", "startOffset": 110, "endOffset": 131}, {"referenceID": 26, "context": "Knowledge-Based Representations There is an emerging trend of learning representations at different levels, such as word embeddings (Mikolov et al., 2013), character embeddings (Ling et al.", "startOffset": 132, "endOffset": 154}, {"referenceID": 19, "context": ", 2013), character embeddings (Ling et al., 2015), and sentence embeddings (Le and Mikolov, 2014; Huang et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 18, "context": ", 2015), and sentence embeddings (Le and Mikolov, 2014; Huang et al., 2013).", "startOffset": 33, "endOffset": 75}, {"referenceID": 15, "context": ", 2015), and sentence embeddings (Le and Mikolov, 2014; Huang et al., 2013).", "startOffset": 33, "endOffset": 75}, {"referenceID": 1, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014).", "startOffset": 160, "endOffset": 214}, {"referenceID": 45, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014).", "startOffset": 160, "endOffset": 214}, {"referenceID": 1, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014). Different from prior work, this paper focuses on learning composable substructure embeddings that are informative for understanding. Recently linguistic structures are taken into account in the deep learning framework. Ma et al. (2015a) and Tai et al.", "startOffset": 161, "endOffset": 453}, {"referenceID": 1, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014). Different from prior work, this paper focuses on learning composable substructure embeddings that are informative for understanding. Recently linguistic structures are taken into account in the deep learning framework. Ma et al. (2015a) and Tai et al. (2015) both proposed dependency-based approaches to combine deep learning and linguistic structures, where the model used tree-based n-grams instead of surface ones to capture knowledge-guided relations for sentence modeling and classification.", "startOffset": 161, "endOffset": 475}, {"referenceID": 1, "context": "In addition to fully unsupervised embedding learning, knowledge bases have been widely utilized to learn entity embeddings with specific functions or relations (Celikyilmaz and Hakkani-Tur, 2015; Yang et al., 2014). Different from prior work, this paper focuses on learning composable substructure embeddings that are informative for understanding. Recently linguistic structures are taken into account in the deep learning framework. Ma et al. (2015a) and Tai et al. (2015) both proposed dependency-based approaches to combine deep learning and linguistic structures, where the model used tree-based n-grams instead of surface ones to capture knowledge-guided relations for sentence modeling and classification. Roth and Lapata (2016) utilized lexicalized dependency paths to learn embedding representations for semantic role labeling.", "startOffset": 161, "endOffset": 736}, {"referenceID": 42, "context": "Neural Attention and Memory Model One of the earliest work with a memory component applied to language processing is memory networks (Weston et al., 2015; Sukhbaatar et al., 2015), which encode facts into vectors and store them in the memory for question answering (QA).", "startOffset": 133, "endOffset": 179}, {"referenceID": 35, "context": "Neural Attention and Memory Model One of the earliest work with a memory component applied to language processing is memory networks (Weston et al., 2015; Sukhbaatar et al., 2015), which encode facts into vectors and store them in the memory for question answering (QA).", "startOffset": 133, "endOffset": 179}, {"referenceID": 35, "context": ", 2015; Sukhbaatar et al., 2015), which encode facts into vectors and store them in the memory for question answering (QA). Following their success, Xiong et al. (2016) proposed dynamic memory networks (DMN) to additionally capture position and temporality of transitive reasoning steps for different QA tasks.", "startOffset": 8, "endOffset": 169}, {"referenceID": 3, "context": "The input utterance is parsed by a dependency parser, and the substructures are built according to the paths from the root to all leaves (Chen and Manning, 2014).", "startOffset": 137, "endOffset": 161}, {"referenceID": 10, "context": "We use the Elman RNN architecture, consisting of an input layer, a hidden layer, and an output layer (Elman, 1990).", "startOffset": 101, "endOffset": 114}, {"referenceID": 7, "context": "To overcome the frequent vanishing gradients issue when modeling long-term dependencies, gated RNN was designed to use a more sophisticated activation function than a usual activation function, consisting of affine transformation followed by a simple element-wise nonlinearity by using gating units (Chung et al., 2014), such as long shortterm memory (LSTM) and gated recurrent unit (GRU) (Hochreiter and Schmidhuber, 1997; Cho et al.", "startOffset": 299, "endOffset": 319}, {"referenceID": 14, "context": ", 2014), such as long shortterm memory (LSTM) and gated recurrent unit (GRU) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 77, "endOffset": 129}, {"referenceID": 6, "context": ", 2014), such as long shortterm memory (LSTM) and gated recurrent unit (GRU) (Hochreiter and Schmidhuber, 1997; Cho et al., 2014).", "startOffset": 77, "endOffset": 129}, {"referenceID": 25, "context": "RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 134, "endOffset": 218}, {"referenceID": 47, "context": "RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 134, "endOffset": 218}, {"referenceID": 11, "context": "RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 134, "endOffset": 218}, {"referenceID": 36, "context": "RNNs employing either of these recurrent units have been shown to perform well in tasks that require capturing long-term dependencies (Mesnil et al., 2015; Yao et al., 2014; Graves et al., 2013; Sutskever et al., 2014).", "startOffset": 134, "endOffset": 218}, {"referenceID": 6, "context": "In this paper, we use RNN with GRU cells to allow each recurrent unit to adaptively capture dependencies of different time scales (Cho et al., 2014; Chung et al., 2014), because RNN-GRU can yield comparable performance as RNN-LSTM with need of fewer parameters and less data for generalization (Chung et al.", "startOffset": 130, "endOffset": 168}, {"referenceID": 7, "context": "In this paper, we use RNN with GRU cells to allow each recurrent unit to adaptively capture dependencies of different time scales (Cho et al., 2014; Chung et al., 2014), because RNN-GRU can yield comparable performance as RNN-LSTM with need of fewer parameters and less data for generalization (Chung et al.", "startOffset": 130, "endOffset": 168}, {"referenceID": 7, "context": ", 2014), because RNN-GRU can yield comparable performance as RNN-LSTM with need of fewer parameters and less data for generalization (Chung et al., 2014) A GRU has two gates, a reset gate r, and an update gate z (Cho et al.", "startOffset": 133, "endOffset": 153}, {"referenceID": 6, "context": ", 2014) A GRU has two gates, a reset gate r, and an update gate z (Cho et al., 2014; Chung et al., 2014).", "startOffset": 66, "endOffset": 104}, {"referenceID": 7, "context": ", 2014) A GRU has two gates, a reset gate r, and an update gate z (Cho et al., 2014; Chung et al., 2014).", "startOffset": 66, "endOffset": 104}, {"referenceID": 25, "context": "The dataset for experiments is the benchmark ATIS corpus, which is extensively used by the NLU community (Mesnil et al., 2015).", "startOffset": 105, "endOffset": 126}, {"referenceID": 3, "context": "For experiments with K-SAN, we parse all data with the Stanford dependency parser (Chen and Manning, 2014) and represent words as their embeddings trained on the in-domain data, where the parser is pre-trained on PTB.", "startOffset": 82, "endOffset": 106}, {"referenceID": 17, "context": "cross-entropy, and the optimizer we use is adam with the default setting (Kingma and Ba, 2014), where the learning rate \u03bb = 0.", "startOffset": 73, "endOffset": 94}, {"referenceID": 39, "context": "\u2013 CRF Tagger (Tur et al., 2010): predicts a semantic slot for each word with a context window (size = 5).", "startOffset": 13, "endOffset": 31}, {"referenceID": 25, "context": "\u2013 RNN Tagger (Mesnil et al., 2015): predicts a semantic slot for each word.", "startOffset": 13, "endOffset": 34}, {"referenceID": 16, "context": "\u2013 CNN Encoder-Tagger (Kim, 2014): tag semantic slots with consideration of sentence embeddings learned by a convolutional model.", "startOffset": 21, "endOffset": 32}, {"referenceID": 39, "context": "\u2013 CRF Tagger (Tur et al., 2010): predicts slots based on the lexical (5-word window) and syntactic (dependent head in the parsing tree) features.", "startOffset": 13, "endOffset": 31}, {"referenceID": 22, "context": "\u2013 DCNN (Ma et al., 2015a): predicts slots by incorporating sentence embeddings learned by a convolutional model with consideration of dependency tree structures.", "startOffset": 7, "endOffset": 25}, {"referenceID": 37, "context": "\u2013 Tree-RNN (Tai et al., 2015): predicts slots with sentence embeddings learned by an RNN model based on the tree structures of sentences.", "startOffset": 11, "endOffset": 29}, {"referenceID": 16, "context": "CNN (Kim, 2014) performs better compared to DCNN (Ma et al.", "startOffset": 4, "endOffset": 15}, {"referenceID": 22, "context": "CNN (Kim, 2014) performs better compared to DCNN (Ma et al., 2015a) and Tree-RNN (Tai et al.", "startOffset": 49, "endOffset": 67}, {"referenceID": 37, "context": ", 2015a) and Tree-RNN (Tai et al., 2015), even though CNN does not leverage external knowledge when encoding sentences.", "startOffset": 22, "endOffset": 40}, {"referenceID": 0, "context": "AMR is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph (Banarescu et al., 2013), where nodes represent concepts, and labeled directed edges represent the relations between two concepts.", "startOffset": 111, "endOffset": 135}, {"referenceID": 27, "context": "The formalism is based on propositional logic and neoDavidsonian event representations (Parsons, 1990; Davidson, 1967).", "startOffset": 87, "endOffset": 118}, {"referenceID": 8, "context": "The formalism is based on propositional logic and neoDavidsonian event representations (Parsons, 1990; Davidson, 1967).", "startOffset": 87, "endOffset": 118}, {"referenceID": 21, "context": "The semantic concepts in AMR were leveraged to benefit multiple NLP tasks (Liu et al., 2015).", "startOffset": 74, "endOffset": 92}], "year": 2016, "abstractText": "Natural language understanding (NLU) is a core component of a spoken dialogue system. Recently recurrent neural networks (RNN) obtained strong results on NLU due to their superior ability of preserving sequential information over time. Traditionally, the NLU module tags semantic slots for utterances considering their flat structures, as the underlying RNN structure is a linear chain. However, natural language exhibits linguistic properties that provide rich, structured information for better understanding. This paper introduces a novel model, knowledge-guided structural attention networks (K-SAN), a generalization of RNN to additionally incorporate non-flat network topologies guided by prior knowledge. There are two characteristics: 1) important substructures can be captured from small training data, allowing the model to generalize to previously unseen test data; 2) the model automatically figures out the salient substructures that are essential to predict the semantic tags of the given sentences, so that the understanding performance can be improved. The experiments on the benchmark Air Travel Information System (ATIS) data show that the proposed K-SAN architecture can effectively extract salient knowledge from substructures with an attention mechanism, and outperform the performance of the state-of-the-art neural network based frameworks.", "creator": "LaTeX with hyperref package"}}}