{"id": "1609.03348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "A Threshold-based Scheme for Reinforcement Learning in Neural Networks", "abstract": "A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented, providing a general purpose learning machine. By reference to a node threshold three features are described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2) The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of forming long term strategy 3) The learning scheme is modified to use a threshold-based deep learning algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be used for supervised as well as unsupervised training regimes.", "histories": [["v1", "Mon, 12 Sep 2016 11:23:20 GMT  (1348kb)", "http://arxiv.org/abs/1609.03348v1", null], ["v2", "Sat, 17 Sep 2016 04:20:01 GMT  (1350kb)", "http://arxiv.org/abs/1609.03348v2", null], ["v3", "Tue, 15 Nov 2016 05:11:01 GMT  (1384kb)", "http://arxiv.org/abs/1609.03348v3", null], ["v4", "Sat, 14 Jan 2017 05:54:29 GMT  (1341kb)", "http://arxiv.org/abs/1609.03348v4", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["thomas h ward"], "accepted": false, "id": "1609.03348"}, "pdf": {"name": "1609.03348.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["thomas.holland.ward@gmail.com "], "sections": [{"heading": null, "text": "In fact, most of them are in a position to take responsibility for themselves. (...) Most of them are in a position to put themselves in a position to put themselves in a position to put themselves in a position. (...) Most of them are in a position to take responsibility. (...) Most of them are in a position to take responsibility for themselves. (...) Most of them are not in a position to put themselves in a position to take responsibility. (...) Most of them are not in a position to put themselves in a position. (...) Most of them are not in a position to put themselves in a position to act. (...) Most of them are not in a position to take responsibility for themselves. (...) Most of them are not in a position to take responsibility. (...)"}], "references": [{"title": "Learning and Problem Solving with Multilayer Connectionist Systems", "author": ["Charles W. Anderson"], "venue": "Technical Report", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1986}, {"title": "Playing atari with deep reinforcement learning.arXiv preprint arXiv:1312.5602", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "Riedmiller"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Learning representations by     back\u00adpropagating errors", "author": ["Rumelhart", "David E", "Hinton", "Geoffrey E", "Williams", "Ronald J"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1986}, {"title": "Is backpropagation biologically plausible", "author": ["D.G. Stork"], "venue": "\u200bNeural Networks, 1989. IJCNN., International      Joint Conference on  \u200b  , Washington, DC, USA, 1989, pp. 241\u00ad246 vol.2.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1989}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S Sutton", "Andrew G Barto"], "venue": "MIT press,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1998}, {"title": "Building machines   that learn and think like people", "author": ["Brenden M Lake", "Tomer D Ullman", "Joshua B Tenenbaum", "Samuel J Gershman"], "venue": "arXiv preprint arXiv:1604.00289,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Temporal difference learning and TD\u00adGammon\u201d, Communications of the ACM CACM", "author": ["Gerald Tesauro"], "venue": "Homepage archive, Volume 38 Issue", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Connectionism and the mind\u201d, Wiley,1991, pages 70\u00ad97Skinner", "author": ["William Bechtel", "Adele Abrahamsen"], "venue": "Science and human behavior", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1953}, {"title": "Measurement of Current\u00adVoltage Relations in the Membrane       of the Giant Axon of Loligo.", "author": ["A.L. Hodgkin", "A.F. Huxley", "B. Katz"], "venue": "The Journal of Physiology", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1952}, {"title": "Long short\u00adterm memory\" (PDF). Neural     Computation", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1997}, {"title": "Finding Structure in Time", "author": ["J.L. Elman"], "venue": "Cognitive Science,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1990}], "referenceMentions": [{"referenceID": 5, "context": "In contrast the work presented follows a distinctly top\u00addown approach attempting to model intelligence as a whole system; a causal agent interacting with the environment [6].", "startOffset": 170, "endOffset": 173}, {"referenceID": 1, "context": "Significant RL successes have been achieved in AI notably with the use of Q\u00adlearning [2][7].", "startOffset": 85, "endOffset": 88}, {"referenceID": 6, "context": "Significant RL successes have been achieved in AI notably with the use of Q\u00adlearning [2][7].", "startOffset": 88, "endOffset": 91}, {"referenceID": 4, "context": "Primary reinforcement reward conditions (eg hunger or thirst) typically drive some form of homeostatic, adaptive control function for the agent [5][8].", "startOffset": 144, "endOffset": 147}, {"referenceID": 2, "context": "Weight adjustments using backpropagation[3][9] are traditionally used in supervised learning schemes; that is desired activations (or training sets) are established prior to the learning phase.", "startOffset": 40, "endOffset": 43}, {"referenceID": 2, "context": "The learning scheme consists of the following components: \u25cf Artificial Neural Network \u25cf Learning Algorithm \u25cf Framework \u25cf Environment The network architecture is that of a familiar multilayer perceptron (MLP)[3].", "startOffset": 207, "endOffset": 210}, {"referenceID": 2, "context": "Weight updates are derived using a standard (supervised) back propagation learning algorithm[3][10].", "startOffset": 92, "endOffset": 95}, {"referenceID": 7, "context": "Weight updates are derived using a standard (supervised) back propagation learning algorithm[3][10].", "startOffset": 95, "endOffset": 99}, {"referenceID": 8, "context": "Thresholds for neuron activation are widely found in animals, where sufficient \u2018excitation\u2019 is required to result in an electrical action potential that can be signalled to other neurons [11].", "startOffset": 187, "endOffset": 191}, {"referenceID": 0, "context": "While \u200bQ learning with neural networks have been used principally to determine policy rather than action[1][7], in the present model they are integrated\u200b.", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "While \u200bQ learning with neural networks have been used principally to determine policy rather than action[1][7], in the present model they are integrated\u200b.", "startOffset": 107, "endOffset": 110}, {"referenceID": 10, "context": "In contrast to some other ANN predictors of time series events (eg Elman networks[13], LSTM[12]) the present solution does not rely on recurrency.", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "In contrast to some other ANN predictors of time series events (eg Elman networks[13], LSTM[12]) the present solution does not rely on recurrency.", "startOffset": 91, "endOffset": 95}, {"referenceID": 3, "context": "However backpropagation faces the additional biological plausibility concern: \u25cf How is error back propagated ? Thus far research has provided little neurobiological support for backpropagation [4].", "startOffset": 193, "endOffset": 196}], "year": 0, "abstractText": "A generic and scalable Reinforcement Learning scheme for Artificial Neural Networks is presented,   providing a general purpose learning machine. By reference to a node threshold three features are  described 1) A mechanism for Primary Reinforcement, capable of solving linearly inseparable problems 2)   The learning scheme is extended to include a mechanism for Conditioned Reinforcement, capable of   forming long term strategy 3) The learning scheme is modified to use a threshold\u00adbased deep learning   algorithm, providing a robust and biologically inspired alternative to backpropagation. The model may be   used for supervised as well as unsupervised training regimes.", "creator": null}}}