{"id": "1609.03426", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Multi-Label Learning with Provable Guarantee", "abstract": "Here we study the problem of predicting labels for large text corpora where each text can be assigned multiple labels. The problem might seem trivial when the number of labels is small, and can be easily solved using a series of one-vs-all classifiers. However, as the number of labels increases to several thousand, the parameter space becomes extremely large, and it is no longer possible to use the one-vs-all technique. Here we propose a model based on the factorization of higher order word vector moments, as well as the cross moments between the labels and the words for multi-label prediction. Our model provides guaranteed converge bounds on the extracted parameters. Further, our model takes only three passes through the training dataset to extract the parameters, resulting in a highly scalable algorithm that can train on GB's of data consisting of millions of documents with hundreds of thousands of labels using a nominal resource of a single processor with 16GB RAM. Our model achieves 10x-15x order of speed-up on large-scale datasets while producing competitive performance in comparison with existing benchmark algorithms.", "histories": [["v1", "Mon, 12 Sep 2016 14:38:08 GMT  (218kb,D)", "http://arxiv.org/abs/1609.03426v1", null], ["v2", "Tue, 13 Sep 2016 23:26:50 GMT  (242kb,D)", "http://arxiv.org/abs/1609.03426v2", null], ["v3", "Sun, 18 Sep 2016 14:57:20 GMT  (169kb,D)", "http://arxiv.org/abs/1609.03426v3", null], ["v4", "Tue, 1 Nov 2016 16:21:54 GMT  (176kb,D)", "http://arxiv.org/abs/1609.03426v4", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sayantan dasgupta"], "accepted": false, "id": "1609.03426"}, "pdf": {"name": "1609.03426.pdf", "metadata": {"source": "CRF", "title": "Large-Scale Label Prediction for Sparse Data with Probable Guarantees", "authors": ["Sayantan Dasgupta"], "emails": [], "sections": [{"heading": null, "text": "Most of the applications for this task use data with moderate to high dimensions, such as text or 1-vs-all models for label prediction is feasible."}], "references": [{"title": "Wsabie: Scaling up to large vocabulary image annotation", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "IJCAI, vol. 11, 2011, pp. 2764\u2013 2770.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, 2014, pp. 1725\u20131732.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical topic models for multi-label document classification", "author": ["T.N. Rubin", "A. Chambers", "P. Smyth", "M. Steyvers"], "venue": "Machine learning, vol. 88, no. 1-2, pp. 157\u2013208, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages", "author": ["R. Agrawal", "A. Gupta", "Y. Prabhu", "M. Varma"], "venue": "Proceedings of the 22nd international conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2013, pp. 13\u201324.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Large-scale multi-label learning with missing labels", "author": ["H.-F. Yu", "P. Jain", "P. Kar", "I.S. Dhillon"], "venue": "ICML, vol. 31, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Tensor decompositions for learning latent variable models", "author": ["A. Anandkumar", "R. Ge", "D. Hsu", "S.M. Kakade", "M. Telgarsky"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 2773\u20132832, 2014. [Online]. Available: http://jmlr.org/papers/v15/anandkumar14b.html", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "From o (k 2 n) to o (n): a fast complex-valued eigenvalue solver for large-scale onchip interconnect analysis", "author": ["J. Lee", "V. Balakrishnan", "C.-K. Koh", "D. Jiao"], "venue": "Microwave Symposium Digest, 2009. MTT\u201909. IEEE MTT-S International. IEEE, 2009, pp. 181\u2013184.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Matlab tensor toolbox version 2.6", "author": ["B.W. Bader", "T.G. Kolda"], "venue": "Available online, February 2015. [Online]. Available: http://www.sandia.gov/\u223ctgkolda/TensorToolbox/", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt-Thieme"], "venue": "Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence. AUAI Press, 2009, pp. 452\u2013461.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "The relationship between precision-recall and roc curves", "author": ["J. Davis", "M. Goadrich"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 233\u2013240.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "Spectral methods for supervised topic models", "author": ["Y. Wang", "J. Zhu"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 1511\u20131519.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Shifted power method for computing tensor eigenpairs", "author": ["T.G. Kolda", "J.R. Mayo"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 32, no. 4, pp. 1095\u20131124, October 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Sparse local embeddings for extreme multi-label classification", "author": ["K. Bhatia", "H. Jain", "P. Kar", "M. Varma", "P. Jain"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 730\u2013738.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Fastxml: A fast, accurate and stable treeclassifier for extreme multi-label learning", "author": ["Y. Prabhu", "M. Varma"], "venue": "Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2014, pp. 263\u2013272.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 3, "context": "It has several real world application such as image [1] or video annotation [2], annotation of keywords for large text corpora [3] or query keyword suggestion [4].", "startOffset": 159, "endOffset": 162}, {"referenceID": 0, "context": "Both WSABIE [1] and LEML [5] utilizes such mappings.", "startOffset": 12, "endOffset": 15}, {"referenceID": 4, "context": "Both WSABIE [1] and LEML [5] utilizes such mappings.", "startOffset": 25, "endOffset": 28}, {"referenceID": 5, "context": "Unlike the usual cases where such latent variable models are trained using EM, we use Method of Moments [6] to extract the parameters from the latent variable model.", "startOffset": 104, "endOffset": 107}, {"referenceID": 5, "context": "From [6], if we define M2 as the pairwise probability matrix, with [M2]i,j = P [ vi, vj ] , we can express it as,", "startOffset": 5, "endOffset": 8}, {"referenceID": 6, "context": "O ( ( \u2211N i=1 nnz(xi) )K ) , since the total number of nonzero entries in M2 is O (\u2211N i=1 nnz(xi) 2 ) [7].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "We used the Tensor Toolbox [8] for tensor decomposition.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "The scores of LEML and MoM are not directly comparable, since the score of LEML can be negative, whereas the score of MoM lies within [0, 1]", "startOffset": 134, "endOffset": 140}, {"referenceID": 4, "context": "Since LEML is shown to outperform WSABIE and other benchmark algorithms on various small and large-scale datasets in [5], we benchmark the performance of our method against LEML.", "startOffset": 117, "endOffset": 120}, {"referenceID": 4, "context": "For LEML, we ran ten iterations for the smaller datasets (Bibtex and Delicious) and five iterations for the larger datasets, since the authors of LEML chose a similar number of iterations for their experiments in [5].", "startOffset": 213, "endOffset": 216}, {"referenceID": 8, "context": "AUC is a versatile measure, and is used to evaluate the performance of classification as well as prediction algorithms [9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 9, "context": "Also, it is shown that there exists a one-to-one relation between AUC and PrecisionRecall curve in [10], i.", "startOffset": 99, "endOffset": 103}], "year": 2017, "abstractText": "Here we study the problem of predicting labels for large text corpora where each text can be assigned multiple labels. The problem might seem trivial when the number of labels is small, and can be easily solved using a series of one-vsall classifiers. However, as the number of labels increases to several thousand, the parameter space becomes extremely large, and it is no longer possible to use the one-vs-all technique. Here we propose a model based on the factorization of higher order word vector moments, as well as the cross moments between the labels and the words for multi-label prediction. Our model provides guaranteed converge bounds on the extracted parameters. Further, our model takes only three passes through the training dataset to extract the parameters, resulting in a highly scalable algorithm that can train on GB\u2019s of data consisting of millions of documents with hundreds of thousands of labels using a nominal resource of a single processor with 16GB RAM. Our model achieves 10x-15x order of speed-up on large-scale datasets while producing competitive performance in comparison with existing benchmark algorithms.", "creator": "LaTeX with hyperref package"}}}