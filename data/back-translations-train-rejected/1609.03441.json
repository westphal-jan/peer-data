{"id": "1609.03441", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Read, Tag, and Parse All at Once, or Fully-neural Dependency Parsing", "abstract": "We present a dependency parser implemented as a single deep neural network that reads orthographic representations of words and directly generates dependencies and their labels. Unlike typical approaches to parsing, the model doesn't require part-of-speech (POS) tagging of the sentences. With proper regularization and additional supervision achieved with multitask learning we reach state-of-the-art performance on Slavic languages from the Universal Dependencies treebank: with no linguistic features other than characters, our parser is as accurate as a transition- based system trained on perfect POS tags.", "histories": [["v1", "Mon, 12 Sep 2016 15:16:43 GMT  (110kb,D)", "http://arxiv.org/abs/1609.03441v1", null], ["v2", "Mon, 5 Jun 2017 18:46:40 GMT  (180kb,D)", "http://arxiv.org/abs/1609.03441v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jan chorowski", "micha{\\l} zapotoczny", "pawe{\\l} rychlikowski"], "accepted": false, "id": "1609.03441"}, "pdf": {"name": "1609.03441.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["jan.chorowski@ii.uni.wroc.pl"], "sections": [{"heading": "1 Introduction", "text": "The ability to communicate with natural language is one of the long-term objectives of artificial intelligence. In addition, due to the enormous volume of natural language texts, there is a growing need to develop effective algorithms to handle them satisfactorily. In recent decades, a shift in emphasis from linguistics to statistical text analysis and more recently to machine learning systems and neural networks has been observed. Deep learning methods have led to many breakthroughs in NLP tasks, such as language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), subtitling (Xu et al., 2015), question answering (Sukhbaatar et al., 2015), language regognition, POS taggers, and so on. Finding the syntactical structure of sentences is one of the essential needs in natural language text analysis. The parameter of automated language is required for understanding."}, {"heading": "2 Description of the Model", "text": "A dependency parser reads a sentence and finds a series of dependencies consisting of a header, a dependent word, and a caption that describes the dependency type. Each word has exactly one header, with a word in the sentence (typically the verb) having an artificial < ROOT > token as the header. Therefore, the set of dependencies can be interpreted as an oriented tree linking words in the sentence to each other. See Figure 1 for an example dependency parser. Our dependency parser is implemented as a single neural network with three parts, as in Figure 2. First, the reader subnetwork finds word embeddings based on their orthographic representations using convolutional and highway layers (Kim et al., 2015; Srivastava et al., 2015). Second, a bi-directional recursive tagger subnetwork uses the individual words in their context (Schuster and Schuster, 1997)."}, {"heading": "2.1 Reader subnetwork", "text": "Following Kim et al. (2015), we use a revolutionary filter bank, followed by a few layers of nonlinear transformations. Each word w is represented by a sequence of its characters enclosed by special beginning and end-of-word markers. We find low-dimensional embedding of characters and link them to a matrix Cw.Next, the matrix Cw is reduced to a vector of filter responses Rw and Rnf, where nf denotes the number of filters. Each filter response is calculated as: Rwi = max (C w ~ F i), where F i is the i-th filter and ~ denotes the evolution over the length of the word. Intuitively, the convolutions act like patterns that respond to certain parts of the word. Furthermore, the filters applied to the first and second filters are reduced to the first entix."}, {"heading": "2.2 Tagging subnetwork", "text": "The tagging subnetwork works on sequences of word representations Ew produced by the reader. It uses bidirectional recurring layers (BiRNN) to place them in a broader context (Schuster and Paliwal, 1997). Specifically, we use recurring units of GRU (Cho et al., 2014) to scan the sequence forwards and backwards. Hidden representations are combined by addition and passed to another layer of recurring units. Specifically, the tagger can be trained solely on the gradient signal that flows from the parsing subnetwork into the network. However, it is also possible to divert the signal from one of the BiRNN layers and use it to predict a portion of speech (POS) tags of individual words. This additional monitoring typically helps prevent parser upgrades. In the Experimental Results section, we present the effects of an explicit POS tag training."}, {"heading": "2.3 Parsing subnetwork", "text": "The analytical sub-network has two objectives: first, to assign dependent words to their heads, and second, to identify each pair of matching words with the correct dependency type. We have decided to use the pointer network approach (Vinyals et al., 2015) to find headers. For each sentence, the parser receives a sequence H1, H2,.., Hn of vectors of word annotations produced by the tagger. We presuppose this sequence with a special vector H0, which denotes the root word. This guarantees that each word of the original sentence has exactly one headers. To train the pointer network, we construct a probability distribution over possible headers l, 1,..., n. First, for each word w, 1, 2, n, we calculate a score over all possible places l: s (w, l) = f (Hw, Hl), (1) where the protector is implied as a small foreword."}, {"heading": "2.4 Training criterion", "text": "The network receives training signals from three sources: 1. The negative log probability loss in predicting dependency markers Ll. With the soft log probability label, this loss is propagated backwards through the entire network and could theoretically be used to train the entire network. With the hard log probability label, this error is not projected backwards onto the scorer. 2. The negative log probability loss in searching for correct header words Ls by the scorer. This loss is propagated backwards by the read and tagger subnetwork, but not by the labeler. 3. The optional loss of the negative log probability label Lt. This loss is propagated only by a few layers of the tagger and by the reader backwards. The final loss is calculated as a linear combination of individual losses: L = \u03b1lLl + \u03b1sLs + \u03b1tLt (3)"}, {"heading": "2.5 Parsing algorithm", "text": "At its core, the network produces values for each pair of words that reflect the probability that the words represent a dependency. These values can be used to form a parse tree by finding a series of dependencies that meet certain constraints (exactly one word depends on the root symbol, there are no cycles, the tree is projective).However, we have found that the values calculated by equivalents at the end of the training typically result in a very high probability distribution concentrated in a single place. Therefore, good results are obtained with a greedy analysis strategy in which the best parse partner for each word is simply selected. Only about 0.5% of the parses obtained by this method have cycles, so the use of Chu-Liu-Edmonds (Edmonds, 1966) is a maximum exciting arborescence algorithm (which deletes cycles) which is only a subtle improvement and is not represented in Table 2."}, {"heading": "3 Related work", "text": "There are two basic views on the syntactic structure of the sentence: \u2022 constituent based, where words are organized into nested components \u2022 dependence based, where words through dependency relationship This work focuses on dependence parsing. We believe that currently two approaches are the most important: transition and graph based. A transition based parser aims to predict the best parser action (such as moving the word to stack or add a dependence between current word and the word to a stack) looking at some characteristics (Nivre, 2008). A chart based parser finds the structure that maximizes a global score while maintaining some constraints (i.e. forcing the output to be well-shaped trees). Recent, deep neural networks have been used with great success in dependence parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Anal et al, 2016 and Graph)."}, {"heading": "4 Experimental Setup", "text": "We have evaluated our parsers in three languages, English, Czech and Polish on Universal Dependencies (UD) v. 1.2 dataset (Nivre et al., 2015). We have chosen these datasets because of their wide availability and because we want to investigate the possibility of a multilingual education in the future. While the English treebank used in UD is rather small and non-standard, treebanks for other languages are often used the typical and standard ones. In particular, we evaluate for which the UD project uses the only Polish treebank \"Sk\u0142adnica\" (S). We have selected the characteristics of the dataset in Table 1.Model selection We have performed a hyperparameter search on the Polish treebank, which is the smallest."}, {"heading": "5 Results", "text": "Our parser has achieved competitive performance with transition-based dependency savers, as shown in Table 2. For all data sets, we report: the percentage of correctly described dependencies (LA), the percentage of correctly attached heads (UAS), and the percentage of correctly attached heads and labels (LAS) as measured by the test set for the model that achieved the highest performance on the development set. Results were calculated using the MaltEval tool."}, {"heading": "5.1 Baseline Models", "text": "We used MaltParser v. 1.8.1 tuned with MaltOptimizer (Nivre et al., 2005; Ballesteros and Nivre, 2012) for all available information in UD Treebanks (gold POS), which gave us an optimistic starting point, as POS tags, when used normally, will contain errors due to the tagger. This error was analyzed in UD v. 1.0 by Tiedemann (2015). As an additionally optimized baseline, we also include results from Straka et al. (2015) reported on the same version of UD Treebanks that we use."}, {"heading": "5.2 Neural Parser on Golden Tags", "text": "To compare our parser with the optimistic baseline, we trained it on Gold POS tags. We observed that the best results were obtained when the POS attributes were split up and given to the network as multiple categorical inputs. In Czech and Polish, the neural network improves the optimistic baseline error rates, while in English the results are comparable."}, {"heading": "5.3 Neural Parser Without POS Tags", "text": "In the next experiment, we evaluated the network without POS knowledge. If we rely on single words, the performance of the network has significantly decreased, which is shown in the table. A solution that is contained in the spelling of each word requires embedding it in a large body and its use in spelling. We want to use the information contained in the spelling of each word to take advantage of the characteristics of Kim. Intuitively, in the morphologically rich languages as well as in the spelling of a word."}, {"heading": "6 Conclusions and Future Works", "text": "We have introduced a dependency parser that can work directly with characters, eliminating the need for a traditional NLP pipeline. It is uniformly designed and has separate cost terms that relate to identification accuracy, header localization and optional POS marking. In morphologically rich languages, the parser is competitive with traditional transitional solutions that use golden POS tag information, although no handcrafted linguistic features are used and all information comes directly from the orthographic form of words. In the future, we plan to explore the possibility of training parsers in multiple languages together to improve models in languages with very small companies such as Polish or Slovenian."}, {"heading": "7 Acknowledgments", "text": "The experiments were carried out with the libraries Theano (Bergstra et al., 2010; Bastien et al., 2012), Blocks and Fuel (van Merri\u00ebnboer et al., 2015). The authors would like to acknowledge the support of the following institutions for research funding and computer support: National Science Centre (Poland) grants Sonata 8 2014 / 15 / D / ST6 / 04402, National Research and Development Centre (Poland) grants Audioscope (Applied Research Programme, 3rd Competition, submission number 245755)."}], "references": [{"title": "Globally Normalized Transition-Based Neural Networks", "author": ["Daniel Andor", "Chris Alberti", "David Weiss"], "venue": "[cs],", "citeRegEx": "Andor et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andor et al\\.", "year": 2016}, {"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "MaltOptimizer: an optimization tool for MaltParser. In Proceedings of the Demonstrations at the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 58\u201362", "author": ["Miguel Ballesteros", "Joakim Nivre"], "venue": "Association for Computational Linguistics,", "citeRegEx": "Ballesteros and Nivre.,? \\Q2012\\E", "shortCiteRegEx": "Ballesteros and Nivre.", "year": 2012}, {"title": "Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien"], "venue": "In Proc. SciPy,", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Danqi Chen", "Christopher D Manning"], "venue": "In Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Chen and Manning.,? \\Q2014\\E", "shortCiteRegEx": "Chen and Manning.", "year": 2014}, {"title": "On the Properties of Neural Machine Translation: Encoder-Decoder Approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "End-to-end Continuous Speech Recognition using Attention-based Recurrent NN: First Results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": "arXiv preprint arXiv:1505.08075,", "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "Optimim Branchings", "author": ["Jack Edmonds"], "venue": "JOURNAL OF RESEARCH of the National Bureau of Standards - B.,", "citeRegEx": "Edmonds.,? \\Q1966\\E", "shortCiteRegEx": "Edmonds.", "year": 1966}, {"title": "Exploring the Limits of Language Modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "[cs],", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M. Rush"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations", "author": ["Eliyahu Kiperwasser", "Yoav Goldberg"], "venue": "[cs],", "citeRegEx": "Kiperwasser and Goldberg.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser and Goldberg.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Wang Ling", "Tiago Lu\u00eds", "Lu\u00eds Marujo"], "venue": "arXiv preprint arXiv:1508.02096,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "[cs],", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Recurrent neural network based language model", "author": ["Tom\u00e1s Mikolov", "Martin Karafi\u00e1t", "Luk\u00e1s Burget", "Jan Cernocky", "Sanjeev Khudanpur"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Algorithms for Deterministic Incremental Dependency Parsing", "author": ["Joakim Nivre"], "venue": "Comput. Linguist.,", "citeRegEx": "Nivre.,? \\Q2008\\E", "shortCiteRegEx": "Nivre.", "year": 2008}, {"title": "MaltParser: A language-independent system for data-driven dependency parsing", "author": ["Joakim Nivre", "Johan Hall", "Jens Nilsson"], "venue": "Natural Language Engineering,", "citeRegEx": "Nivre et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2005}, {"title": "Universal Dependencies", "author": ["Joakim Nivre", "\u017deljko Agi\u0107", "Maria Jesus Aranzabe"], "venue": "http://universaldependencies.github.io/docs/,", "citeRegEx": "Nivre et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2015}, {"title": "An effective neural network model for graph-based dependency parsing", "author": ["Wenzhe Pei", "Tao Ge", "Baobao Chang"], "venue": "In Proc. of ACL,", "citeRegEx": "Pei et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pei et al\\.", "year": 2015}, {"title": "Dropout improves Recurrent Neural Networks for Handwriting Recognition", "author": ["Vu Pham", "Th\u00e9odore Bluche", "Christopher Kermorvant", "J\u00e9r\u00f4me Louradour"], "venue": "[cs],", "citeRegEx": "Pham et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2013}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "Kuldip K. Paliwal"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "Schuster and Paliwal.,? \\Q1997\\E", "shortCiteRegEx": "Schuster and Paliwal.", "year": 1997}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["Jasper Snoek", "Hugo Larochelle", "Ryan P. Adams"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Snoek et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Snoek et al\\.", "year": 2012}, {"title": "Parsing natural scenes and natural language with recursive neural networks", "author": ["Richard Socher", "Cliff C. Lin", "Chris Manning", "Andrew Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q1929\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 1929}, {"title": "Parsing universal dependency treebanks using neural networks and search-based oracle", "author": ["Milan Straka", "Jan Haji\u010d", "Jana Strakov\u00e1", "jr. Jan Haji\u010d"], "venue": "In 14th International Workshop on Treebanks and Linguistic Theories (TLT", "citeRegEx": "Straka et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Straka et al\\.", "year": 2015}, {"title": "End-To-End Memory Networks", "author": ["Sainbayar Sukhbaatar", "Arthur Szlam", "Jason Weston", "Rob Fergus"], "venue": "[cs],", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "arXiv preprint arXiv:1409.3215,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Cross-Lingual Dependency Parsing with Universal Dependencies and Predicted PoS Labels", "author": ["J\u00f6rg Tiedemann"], "venue": "Depling", "citeRegEx": "Tiedemann.,? \\Q2015\\E", "shortCiteRegEx": "Tiedemann.", "year": 2015}, {"title": "Blocks and fuel: Frameworks for deep learning", "author": ["Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Vincent Dumoulin"], "venue": null, "citeRegEx": "Merri\u00ebnboer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Merri\u00ebnboer et al\\.", "year": 2015}, {"title": "Grammar as a Foreign Language", "author": ["Oriol Vinyals", "Lukasz Kaiser", "Terry Koo"], "venue": null, "citeRegEx": "Vinyals et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros"], "venue": "[cs],", "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler"], "venue": "[cs],", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Towards a bank of constituent parse trees for Polish", "author": ["Marek \u015awidzi\u0144ski", "Marcin Woli\u0144ski"], "venue": "In Text, Speech and Dialogue,", "citeRegEx": "\u015awidzi\u0144ski and Woli\u0144ski.,? \\Q2010\\E", "shortCiteRegEx": "\u015awidzi\u0144ski and Woli\u0144ski.", "year": 2010}], "referenceMentions": [{"referenceID": 15, "context": "Deep learning methods have led to many breakthrough in NLP tasks, such as language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al.", "startOffset": 92, "endOffset": 114}, {"referenceID": 1, "context": ", 2010), machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), caption generation (Xu et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 27, "context": ", 2010), machine translation (Bahdanau et al., 2014; Sutskever et al., 2014), caption generation (Xu et al.", "startOffset": 29, "endOffset": 76}, {"referenceID": 31, "context": ", 2014), caption generation (Xu et al., 2015), question answering (Sukhbaatar et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 26, "context": ", 2015), question answering (Sukhbaatar et al., 2015), speech regognition, POS-taggers and so on.", "startOffset": 28, "endOffset": 53}, {"referenceID": 11, "context": "First, the reader subnetwork finds word embeddings based on their orthographic representations using convolutional and highway layers (Kim et al., 2015; Srivastava et al., 2015).", "startOffset": 134, "endOffset": 177}, {"referenceID": 21, "context": "Second, a bidirectional recurrent tagger subnetwork puts the individual words into their contexts (Schuster and Paliwal, 1997).", "startOffset": 98, "endOffset": 126}, {"referenceID": 1, "context": "Finally, the parser subnetwork uses the soft-attention mechanism to point each word to its head (Vinyals et al., 2015; Bahdanau et al., 2014).", "startOffset": 96, "endOffset": 141}, {"referenceID": 11, "context": "Following Kim et al. (2015), we use a convolutional filterbank followed by a few layers of nonlinear transformations.", "startOffset": 10, "endOffset": 28}, {"referenceID": 21, "context": "It uses bidirectional recurrent layers (BiRNN) to put them into a broader context (Schuster and Paliwal, 1997).", "startOffset": 82, "endOffset": 110}, {"referenceID": 6, "context": "Specifically, we use GRU recurrent units (Cho et al., 2014) to scan the sequence forward and backward.", "startOffset": 41, "endOffset": 59}, {"referenceID": 9, "context": "5% of the parses obtained by this procedure have cycles, so using Chu-Liu-Edmonds (Edmonds, 1966) maximum spanning arborescence algorithm (which deletes cycles) gives only a subtle improvement and is not presented in the Table 2.", "startOffset": 82, "endOffset": 97}, {"referenceID": 16, "context": "action (such as moving the word to stack or add a dependency between current word and the word on a stack) looking at some features (Nivre, 2008).", "startOffset": 132, "endOffset": 145}, {"referenceID": 5, "context": "Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al.", "startOffset": 100, "endOffset": 195}, {"referenceID": 8, "context": "Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al.", "startOffset": 100, "endOffset": 195}, {"referenceID": 12, "context": "Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al.", "startOffset": 100, "endOffset": 195}, {"referenceID": 0, "context": "Recently, deep neural networks were used wih a great success in dependency parsing, both transition (Chen and Manning, 2014; Dyer et al., 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al.", "startOffset": 100, "endOffset": 195}, {"referenceID": 19, "context": ", 2016) and graph (Pei et al., 2015) based.", "startOffset": 18, "endOffset": 36}, {"referenceID": 12, "context": "Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016).", "startOffset": 57, "endOffset": 89}, {"referenceID": 14, "context": "The parser accessed the source sentence through word embeddings, which were initialized with Word2Vec (Mikolov et al., 2013) and adapted during parser training.", "startOffset": 102, "endOffset": 124}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks.", "startOffset": 40, "endOffset": 571}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al.", "startOffset": 40, "endOffset": 865}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al. (2016). A purely neural constituency parser was shown by Socher et al.", "startOffset": 40, "endOffset": 918}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al. (2016). A purely neural constituency parser was shown by Socher et al. (2011). It built a parse tree by repeatedly joining words or subtrees using a recursive network.", "startOffset": 40, "endOffset": 989}, {"referenceID": 0, "context": ", 2015; Kiperwasser and Goldberg, 2016; Andor et al., 2016) and graph (Pei et al., 2015) based. Our parser is most similar to the graph-based variant of (Kiperwasser and Goldberg, 2016). However, we replace the POS tagger with our reader subnetwork thus reducing the need for feature engineering, which is an important aspect of parser construction which requires knowledge of linguistics. Powerful learning techniques reduce the burden of this somewhat language-specific work. Our parsing network brings together ideas from many recent contributions. Ling et al. (2015) successfully applied character-based word embeddings computed with small BiRNNs (another possible implementation of our \u201creading\u201d subnetwork) to POS-tagging and language modeling with recurrent networks. The character-based word embeddings that we have used were described by Kim et al. (2015) and extensively analyzed by Jozefowicz et al. (2016). A purely neural constituency parser was shown by Socher et al. (2011). It built a parse tree by repeatedly joining words or subtrees using a recursive network. Later, Vinyals et al. (2014) have shown that good constituency parsers can be created by learning to \u201ctranslate\u201d between a given sentence and the linearization of its parse tree.", "startOffset": 40, "endOffset": 1108}, {"referenceID": 18, "context": "2 dataset (Nivre et al., 2015).", "startOffset": 10, "endOffset": 30}, {"referenceID": 33, "context": "In particular, we evaluate on Polish for which the UD project uses the only polish treebank \u201cSk\u0142adnica\u201d (\u015awidzi\u0144ski and Woli\u0144ski, 2010) and on Czech for which UD uses the large and standard \u201cPrague Treebank\u201d (Bej\u010dek et al.", "startOffset": 104, "endOffset": 135}, {"referenceID": 22, "context": "We have used the Spearmint system to choose network layer sizes and regularization hyperparameters (Snoek et al., 2012).", "startOffset": 99, "endOffset": 119}, {"referenceID": 25, "context": "8 (Straka et al., 2015) - 87.", "startOffset": 2, "endOffset": 23}, {"referenceID": 28, "context": "5 (Tiedemann, 2015) - - 85.", "startOffset": 2, "endOffset": 19}, {"referenceID": 28, "context": "3 Predicted POS tags or no POS tags (Tiedemann, 2015) - - 81.", "startOffset": 36, "endOffset": 53}, {"referenceID": 32, "context": "We have used the AdaDelta (Zeiler, 2012) learning rule with parameters = 10\u22128 and \u03c1 = 0.", "startOffset": 26, "endOffset": 40}, {"referenceID": 7, "context": "We have routinely used an adaptive gradient clipping mechanism (Chorowski et al., 2014).", "startOffset": 63, "endOffset": 87}, {"referenceID": 20, "context": "We have applied 20% Dropout just after the Reader subnetwork, 70% after every BiRNN layer in the tagger subnetwork (Pham et al., 2013) and 50% in the labeler.", "startOffset": 115, "endOffset": 134}, {"referenceID": 20, "context": "We have applied 20% Dropout just after the Reader subnetwork, 70% after every BiRNN layer in the tagger subnetwork (Pham et al., 2013) and 50% in the labeler. In contrast to Vinyals et al. (2014) we have not used data augmentations techniques.", "startOffset": 116, "endOffset": 196}, {"referenceID": 17, "context": "1 tuned with MaltOptimizer (Nivre et al., 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS).", "startOffset": 27, "endOffset": 76}, {"referenceID": 2, "context": "1 tuned with MaltOptimizer (Nivre et al., 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS).", "startOffset": 27, "endOffset": 76}, {"referenceID": 2, "context": ", 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS). This gave us an optimistic baseline, since during normal use POS tags will contain errors due to the tagger. This error has been analyzed on UD v. 1.0 by Tiedemann (2015). As an additional optimized baseline we include also results from Straka et al.", "startOffset": 8, "endOffset": 265}, {"referenceID": 2, "context": ", 2005; Ballesteros and Nivre, 2012) on all information available in UD treebanks (gold POS). This gave us an optimistic baseline, since during normal use POS tags will contain errors due to the tagger. This error has been analyzed on UD v. 1.0 by Tiedemann (2015). As an additional optimized baseline we include also results from Straka et al. (2015) that were reported on the same version of UD treebanks that we use.", "startOffset": 8, "endOffset": 352}, {"referenceID": 29, "context": "One solution, outlined by Vinyals et al. (2014) involves pre-training word embeddings on a large corpus and using them in the input look-up tables.", "startOffset": 26, "endOffset": 48}, {"referenceID": 11, "context": "However, we wanted to use the information present in the spelling of each word and decided to use the characterbased embedder by Kim et al. (2015). Intuitively, in morphologically rich languages such as Czech or Polish the spelling of a word conveys many hints about its grammatical function.", "startOffset": 129, "endOffset": 147}], "year": 2017, "abstractText": "We present a dependency parser implemented as a single deep neural network that reads orthographic representations of words and directly generates dependencies and their labels. Unlike typical approaches to parsing, the model doesn\u2019t require part-of-speech (POS) tagging of the sentences. With proper regularization and additional supervision achieved with multitask learning we reach state-of-the-art performance on Slavic languages from the Universal Dependencies treebank: with no linguistic features other than characters, our parser is as accurate as a transitionbased system trained on perfect POS tags.", "creator": "LaTeX with hyperref package"}}}