{"id": "1609.03448", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "Learning Sparse Graphs Under Smoothness Prior", "abstract": "In this paper, we are interested in learning the underlying graph structure behind training data. Solving this basic problem is essential to carry out any graph signal processing or machine learning task. To realize this, we assume that the data is smooth with respect to the graph topology, and we parameterize the graph topology using an edge sampling function. That is, the graph Laplacian is expressed in terms of a sparse edge selection vector, which provides an explicit handle to control the sparsity level of the graph. We solve the sparse graph learning problem given some training data in both the noiseless and noisy settings. Given the true smooth data, the posed sparse graph learning problem can be solved optimally and is based on simple rank ordering. Given the noisy data, we show that the joint sparse graph learning and denoising problem can be simplified to designing only the sparse edge selection vector, which can be solved using convex optimization.", "histories": [["v1", "Mon, 12 Sep 2016 15:31:20 GMT  (852kb)", "http://arxiv.org/abs/1609.03448v1", "ICASSP 2017 conference paper"]], "COMMENTS": "ICASSP 2017 conference paper", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sundeep prabhakar chepuri", "sijia liu", "geert leus", "alfred o hero iii"], "accepted": false, "id": "1609.03448"}, "pdf": {"name": "1609.03448.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Sundeep Prabhakar Chepuri", "Sijia Liu", "Geert Leus"], "emails": ["g.j.t.leus}@tudelft.nl,", "hero}@umich.edu."], "sections": [{"heading": null, "text": "ar Xiv: 160 9.03 448v 1 [cs.L G] 12 Sep 20Index terms - graph learning, graph signal processing, graph sparsification, topology deduction, sparse sampling."}, {"heading": "1. INTRODUCTION", "text": "This year it is as far as never before in the history of the Federal Republic of Germany."}, {"heading": "2. PROBLEM SETUP", "text": "Consider a dataset with N real elements defined on the vertices of an undirected graph G = (V, E), where the vertex V = {v1, \u00b7, vN} denotes the set of nodes, and the vertex set E denotes the connection between the nodes. We refer to such datasets as graph signals. We assume that the length of the graph signal (i.e. the number of nodes), i.e., N is known. However, the set of edges is not known. Therefore, we assume a complete graph G (V, E) as a candidate graph in which each node is connected to each other node, with the number of edges | E | = M = N (N \u2212 1) / 2, and aim to determine a subgraph of G by selecting a subset of edges, It, from the edge set E of this matrix diagram matrix.Each graph is revealed by its essence in the matrix topical graph."}, {"heading": "3. LEARNING FROM NOISELESS GRAPH SIGNALS", "text": "Let x = [x1, x2, \u00b7 \u00b7, xN] T-RN be a diagram signal defined on the vertices V of a diagram. Smoothness and spectral content of the signal both depend on the underlying diagram topology. The laplac square shape given by xTLs (w) x quantifies how smooth the diagram signal x is in relation to the underlying diagram [1]. Specifically, the signal x is smooth in relation to the diagram with K edges for low values of xTLs (w) x."}, {"heading": "3.1. Problem statement: noiseless setting", "text": "Suppose we get L-graph signals denoted by the vectors {xk} L k = 1, and they are collected in a N \u00b7 L matrix X = [x1, \u00b7 \u00b7, xL]. We are interested in recreating the graph in a laplazian manner (in other words, the graph topology) on the basis of the previous information that the graph signals are smooth with respect to a K-sparse graph. Formally, we specify the following problem: Problem 1. Given the graph signals {xk} Lk = 1, determine a graph with K edges so that the graph signals have smooth variations with respect to the resulting graph. Mathematically, the above problem can be cast as the following optimization problem: argmin w-W1LL-k = 1x T kLs (w) xk = 1 L tr {XTLs (w) (X)}, where {w-W = the number} is {w-1, where W-W = the number is limited."}, {"heading": "3.2. Solver", "text": "The problem (2) is a cardinality problem, the Boolean optimization problem, i.e. the insufficient costs (2). (3) The introduction of the length M vector c = [c1, c2,... cM] T with cm = XT (amam T) X). (3) The introduction of the length M vector c = [c1, c2,.... cM] T with cm = XT (amam T) X). (2) asargmin w (0.1) Mc T s.to. (4) The above mentioned Boolean programming problems allow an explicit solution and the calculation of the optimal solution."}, {"heading": "4. LEARNING FROM NOISY GRAPH SIGNALS", "text": "Suppose we observe a noisy version of the graph signal, xk, asyk = xk + nk \u0445 R N, (5) and we get L such observations for k = 1, 2,..., L, where we assume that nk is a zero-mean white Gaussian variance noise. To restore xk on the basis of the smoothness assumption, a problem with a tikhonow regularization, xTk Lxk, is typically solved to enforce the previous information that the noiseless graph signal xk is smooth in relation to the underlying graph. Specifically, the following optimization problem (assuming, for a moment, that the graph, i.e. w is known) is solved [1]: argmin {xk} Lk = 11LL \u00b2 k = 1 (yk \u2212 xk \u00b2 2 + tacx T Ls (w) xulk) (6), where the amount of the graph is pre-specified (L = 1)."}, {"heading": "4.1. Problem statement: noisy setting", "text": "After giving the present problem of inference, we will now formally present the problem of interest.Problem 2. In view of the observations {yk} L k = 1 related to the unknown graph signal xk as in (5), we determine the K-sparse graph in such a way that the estimate x-k has the smallest possible error in estimation, and it is smooth with respect to the restored graph. Short graph learning for the denoting inference problem can be mathematically formulated as follows: argmin {xk} L k = 1, w \u00b2 W1LL \u00b2 k = 1 (xk \u00b2 2 + \u03b3 x T kLs (w) xk) (7), the solution of which is called (x-k} Lk = 1, w \u00b2. This formulation differs from [4], since [4] an optimization problem over the space of all possible graphs of the deviation (instead of the parametricity of the additional graph, the deviation of the deviation can be solved by the deviation of the W \u00b2 without the penalty)."}, {"heading": "4.2. Alternating minimization", "text": "The optimization problem (7) can be solved by alternating minimization with respect to {xk} Lk = 1 and w. That is, the problem in (7) is reduced to a linear system in the unknown X that permits a solution in closed form; while {xk} Lk = 1 is reduced to a Boolean linear programming problem that permits an analytical solution with respect to w in order of precedence. These observations point to an iterative alternative minimization algorithm that yields successive estimates of {xk} Lk = 1 with fixed w, and alternately to w with fixed {xk} Lk = 1. Specifically, with the iteration of w there is 0, i.e. w [i] we solve for X [i] with a matrix inversion X [i]."}, {"heading": "4.3. Convex relaxation", "text": "In order to avoid the problems associated with the initialization of the alternating minimization algorithm, we can propose a one-step solution based on convex relaxation in the following sections. We can paraphrase the formula in (7) alternatively in (7): asw = argmin w (w); X = Xmin (w) (9) with (w) = x Y \u2212 Xmin (w). (10) The computational complexity of the solution of the linear equation system (10) decreases when the thrift in w increases. In addition, the estimates {x} Lk = 1 and w (9) Xmin (w) Xmin (w) = Y. (10) The computational complexity of the solution of the linear system of equations (10) decreases when the thrift in w increases."}, {"heading": "5. NUMERICAL RESULTS", "text": "We use temperature measurements collected from 32 weather stations in the French region of Brittany, and the goal is to learn the graph that explains the observed data; see Fig. 1. There are 744 observations per weather station, of which we use L = 50 snapshots as a training set and the remaining as an evaluation set. Such an observation (i.e., a graph signal) on a graph with N = 32 nodes is shown in Fig. 1, where the colored dots have different temperature measurements. Convex optimization problems are solved with the CVX toolbox that internally calls SDPT3."}, {"heading": "6. CONCLUSIONS", "text": "We have investigated the problem of learning a sparse graph that adequately explains the data under a previous smoothness. We have modeled the learning problem of the graph as a design of a sparse edge scanning function. In other words, we are expressing the graph succinctly in the form of an edge selection vector. We have taken into account both the noiseless and the low-noise setting. In the noiseless setting, the design of the edge selection vector is elegant, and it amounts to a simple sorting problem with low complexity. However, in the presence of noise, we propose a computationally favorable alternating minimization algorithm as well as a one-step relaxation-based solution. Software and datasets to produce the results of this paper can be downloaded from http: / / cas.et.tudelft.nl / \u02dc sundeep / sw / icassp17Graphlearning.zip."}, {"heading": "7. REFERENCES", "text": "In fact, it is a pure conspiracy theory, in which it is a conspiracy theory, in which it is a conspiracy theory, in which it is a conspiracy theory, in which it is nothing more than a conspiracy theory, in which the conspiracy theories of the conspiracy theory are brought to the fore."}], "references": [{"title": "The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains", "author": ["D.I. Shuman", "S.K. Narang", "P. Frossard", "A. Ortega", "P. Vandergheynst"], "venue": "IEEE Signal Process. Mag., vol. 30, no. 3, pp. 83\u201398, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data analysis with signal processing on graphs: Representation and processing of massive data sets with irregular structure", "author": ["A. Sandryhaila", "J.M. Moura"], "venue": "IEEE Signal Process. Mag., vol. 31, no. 5, pp. 80\u201390, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Modeling and optimization for big data analytics:(statistical) learning tools for our era of data deluge", "author": ["K. Slavakis", "G. Giannakis", "G. Mateos"], "venue": "IEEE Signal Process. Mag., vol. 31, no. 5, pp. 18\u201331, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning laplacian matrix in smooth graph signal representations", "author": ["X. Dong", "D. Thanou", "P. Frossard", "P. Vandergheynst"], "venue": "arXiv preprint arXiv:1406.7842, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "How to learn a graph from smooth signals", "author": ["V. Kalofolias"], "venue": "Proceedings of the 19th International Conference on Artificial Intelligence and Statistics, 2016, pp. 920\u2013929.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Growing well-connected graphs", "author": ["A. Ghosh", "S. Boyd"], "venue": "Proceedings of the 45th IEEE Conference on Decision and Control, 2006, pp. 6605\u20136611.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Network topology identification from spectral templates", "author": ["S. Segarra", "A.G. Marques", "G. Mateos", "A. Ribeiro"], "venue": "arXiv preprint arXiv:1604.02610, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Stationary graph processes and spectral estimation", "author": ["A. Marques", "S. Segarra", "G. Leus", "A. Ribeiro"], "venue": "arXiv preprint arXiv:1603.04667, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Subsampling for graph power spectrum estimation", "author": ["S.P. Chepuri", "G. Leus"], "venue": "arXiv preprint arXiv:1603.03697, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Spectral sparsification of graphs", "author": ["D.A. Spielman", "S.-H. Teng"], "venue": "SIAM Journal on Computing, vol. 40, no. 4, pp. 981\u2013 1025, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Twiceramanujan sparsifiers", "author": ["J. Batson", "D.A. Spielman", "N. Srivastava"], "venue": "SIAM Journal on Computing, vol. 41, no. 6, pp. 1704\u20131721, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Energy efficient signal detection in sensor networks using ordered transmissions", "author": ["R.S. Blum", "B.M. Sadler"], "venue": "IEEE Trans. Signal Process., vol. 56, no. 7, pp. 3229\u20133235, 2008.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse sensing for distributed detection", "author": ["S.P. Chepuri", "G. Leus"], "venue": "IEEE Trans. Signal Process., vol. 64, no. 6, pp. 1446\u2013 1460, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "CVX: Matlab software for disciplined convex programming, version 2.0 beta", "author": ["M. Grant", "S. Boyd"], "venue": "http://cvxr.com/cvx, Sep. 2012.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Graphs offer a way to describe and explain relationships in complex datasets, a central entity of modern data analysis, where data deluge is prominent [1\u20133].", "startOffset": 151, "endOffset": 156}, {"referenceID": 1, "context": "Graphs offer a way to describe and explain relationships in complex datasets, a central entity of modern data analysis, where data deluge is prominent [1\u20133].", "startOffset": 151, "endOffset": 156}, {"referenceID": 2, "context": "Graphs offer a way to describe and explain relationships in complex datasets, a central entity of modern data analysis, where data deluge is prominent [1\u20133].", "startOffset": 151, "endOffset": 156}, {"referenceID": 0, "context": "To realize this, we make a simple, but widely used assumption [1,4] that the data is smooth with respect to the discovered graph.", "startOffset": 62, "endOffset": 67}, {"referenceID": 3, "context": "To realize this, we make a simple, but widely used assumption [1,4] that the data is smooth with respect to the discovered graph.", "startOffset": 62, "endOffset": 67}, {"referenceID": 3, "context": "The problem of learning the graph Laplacian or the weighted adjacency matrix from smooth graph signals has been considered before [4,5].", "startOffset": 130, "endOffset": 135}, {"referenceID": 4, "context": "The problem of learning the graph Laplacian or the weighted adjacency matrix from smooth graph signals has been considered before [4,5].", "startOffset": 130, "endOffset": 135}, {"referenceID": 4, "context": "Learning sparse graphs from the true graph signals, which is the problem we consider in Section 3, has been studied in [5].", "startOffset": 119, "endOffset": 122}, {"referenceID": 5, "context": "Such a modelling is inspired from [6], where the problem to design edge weights that maximize the algebraic connectivity of the graph has been addressed.", "startOffset": 34, "endOffset": 37}, {"referenceID": 3, "context": "In [4], the joint graph learning and denoising problem has been addressed, i.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "Graph topology identification is also investigated in [7] under the assumption that the eigenvectors of the graph Laplacian are known, which is a much stronger assumption.", "startOffset": 54, "endOffset": 57}, {"referenceID": 7, "context": "Although the eigenvectors can be computed from graph data (or the sample covariance matrix) when it is stationary with respect to the graph [8, 9], the graph signals need not always be vertex stationary.", "startOffset": 140, "endOffset": 146}, {"referenceID": 8, "context": "Although the eigenvectors can be computed from graph data (or the sample covariance matrix) when it is stationary with respect to the graph [8, 9], the graph signals need not always be vertex stationary.", "startOffset": 140, "endOffset": 146}, {"referenceID": 3, "context": "In most of the existing approaches [4,5,7], graph sparsification is (or can be) achieved by penalizing the l1-norm of the graph Laplacian matrix, adjacency matrix or the shift operator, however, there is no explicit handle to control the number of edges, unlike the proposed approach.", "startOffset": 35, "endOffset": 42}, {"referenceID": 4, "context": "In most of the existing approaches [4,5,7], graph sparsification is (or can be) achieved by penalizing the l1-norm of the graph Laplacian matrix, adjacency matrix or the shift operator, however, there is no explicit handle to control the number of edges, unlike the proposed approach.", "startOffset": 35, "endOffset": 42}, {"referenceID": 6, "context": "In most of the existing approaches [4,5,7], graph sparsification is (or can be) achieved by penalizing the l1-norm of the graph Laplacian matrix, adjacency matrix or the shift operator, however, there is no explicit handle to control the number of edges, unlike the proposed approach.", "startOffset": 35, "endOffset": 42}, {"referenceID": 9, "context": "In a related line of research, [10, 11] investigate computing sparse graphs that approximate a given graph spectrally, which means that their Laplacian matrices have similar quadratic forms.", "startOffset": 31, "endOffset": 39}, {"referenceID": 10, "context": "In a related line of research, [10, 11] investigate computing sparse graphs that approximate a given graph spectrally, which means that their Laplacian matrices have similar quadratic forms.", "startOffset": 31, "endOffset": 39}, {"referenceID": 0, "context": "The Laplacian quadratic form given by xLs(w)x quantifies how smooth the graph signal x is with respect to the underlying graph [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 11, "context": ", on different processors), the computational complexity will be as low as O(K) [12, 13].", "startOffset": 80, "endOffset": 88}, {"referenceID": 12, "context": ", on different processors), the computational complexity will be as low as O(K) [12, 13].", "startOffset": 80, "endOffset": 88}, {"referenceID": 3, "context": "By modelling the graph topology through an edge selection vector, the graph learning problem can be solved optimally using a simple and elegant solution with a controlled sparsity level, whereas optimizing directly the graph Laplacian [4] or the adjacency matrix [5] leads to a more complicated suboptimal solution with no explicit handle to control the graph sparsity.", "startOffset": 235, "endOffset": 238}, {"referenceID": 4, "context": "By modelling the graph topology through an edge selection vector, the graph learning problem can be solved optimally using a simple and elegant solution with a controlled sparsity level, whereas optimizing directly the graph Laplacian [4] or the adjacency matrix [5] leads to a more complicated suboptimal solution with no explicit handle to control the graph sparsity.", "startOffset": 263, "endOffset": 266}, {"referenceID": 0, "context": ", w is known) is solved [1]:", "startOffset": 24, "endOffset": 27}, {"referenceID": 3, "context": "This formulation is different from [4], as [4] solves an optimization problem over the space of all possible graph Laplacians (instead of parameterizing the graph with w \u2208 W) without sparsifying the graph.", "startOffset": 35, "endOffset": 38}, {"referenceID": 3, "context": "This formulation is different from [4], as [4] solves an optimization problem over the space of all possible graph Laplacians (instead of parameterizing the graph with w \u2208 W) without sparsifying the graph.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "The algorithm proposed in [4] is also along the lines of alternating minimization, except that the graph learning step involves a complicated optimization over the space of all possible valid Laplacian matrices.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "Relaxing the cardinality constraint \u2016w\u20160 = K with 1w = K and the Boolean constraints {0, 1} with linear inequality constraints related to the box constraint [0, 1] , the optimization problem (9) will be convex on w \u2208 [0, 1] .", "startOffset": 157, "endOffset": 163}, {"referenceID": 0, "context": "Relaxing the cardinality constraint \u2016w\u20160 = K with 1w = K and the Boolean constraints {0, 1} with linear inequality constraints related to the box constraint [0, 1] , the optimization problem (9) will be convex on w \u2208 [0, 1] .", "startOffset": 217, "endOffset": 223}, {"referenceID": 4, "context": "tr { X T L s ( ) } proposed (noiseless, optimal) primal-dual [5]", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "from [4] one-step cvx opt in Sec.", "startOffset": 5, "endOffset": 8}, {"referenceID": 13, "context": "The convex optimization problems are solved using the CVX toolbox, which internally calls SDPT3 [14].", "startOffset": 96, "endOffset": 100}, {"referenceID": 4, "context": ", smoothness) of the proposed closed-form sorting solution, which is optimal, is lower than the existing iterative solution [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "2) is computationally much less expensive (involving two simple known solutions per iteration) as compared to the iterative solution in [4].", "startOffset": 136, "endOffset": 139}], "year": 2016, "abstractText": "In this paper, we are interested in learning the underlying graph structure behind training data. Solving this basic problem is essential to carry out any graph signal processing or machine learning task. To realize this, we assume that the data is smooth with respect to the graph topology, and we parameterize the graph topology using an edge sampling function. That is, the graph Laplacian is expressed in terms of a sparse edge selection vector, which provides an explicit handle to control the sparsity level of the graph. We solve the sparse graph learning problem given some training data in both the noiseless and noisy settings. Given the true smooth data, the posed sparse graph learning problem can be solved optimally and is based on simple rank ordering. Given the noisy data, we show that the joint sparse graph learning and denoising problem can be simplified to designing only the sparse edge selection vector, which can be solved using convex optimization.", "creator": "dvips(k) 5.991 Copyright 2011 Radical Eye Software"}}}