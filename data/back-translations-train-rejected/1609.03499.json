{"id": "1609.03499", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "WaveNet: A Generative Model for Raw Audio", "abstract": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.", "histories": [["v1", "Mon, 12 Sep 2016 17:29:40 GMT  (3057kb,D)", "http://arxiv.org/abs/1609.03499v1", null], ["v2", "Mon, 19 Sep 2016 18:04:35 GMT  (3055kb,D)", "http://arxiv.org/abs/1609.03499v2", null]], "reviews": [], "SUBJECTS": "cs.SD cs.LG", "authors": ["aaron van den oord", "sander dieleman", "heiga zen", "karen simonyan", "oriol vinyals", "alex graves", "nal kalchbrenner", "rew senior", "koray kavukcuoglu"], "accepted": false, "id": "1609.03499"}, "pdf": {"name": "1609.03499.pdf", "metadata": {"source": "CRF", "title": "WAVENET: A GENERATIVE MODEL FOR RAW AUDIO", "authors": ["A\u00e4ron van den Oord", "Sander Dieleman", "Heiga Zen", "Karen Simonyan", "Oriol Vinyals", "Alex Graves", "Nal Kalchbrenner", "Andrew Senior", "Koray Kavukcuoglu"], "emails": ["korayk}@google.com"], "sections": [{"heading": "1 INTRODUCTION", "text": "This paper examines raw audio production techniques inspired by recent advances in neural auto-regressive generative models that model complex distributions such as images (van den Oord et al., 2016a; b) and text (Jo \u0301 zefowicz et al., 2016). Modelling common probabilities via pixels or words using neural architectures as the products of conditional distributions results in a state-of-the-art generation.Remarkably, these architectures are able to model distributions across thousands of random variables (e.g. 64 \u00d7 64 pixels as in PixelRNN (van den Oord et al., 2016a).The question addressed in this paper is whether similar approaches to generating broad-band raw audio waveforms, which are signals with very high temporal resolution, can succeed in producing at least 16,000 samples per second (see Figure 1)."}, {"heading": "2 WAVENET", "text": "This year it is more than ever before."}, {"heading": "2.2 SOFTMAX DISTRIBUTIONS", "text": "One approach to modelling the conditional distributions p (xt | x1,.., xt \u2212 1) across the individual audio samples would be to use a mixing model such as a mixing density network (Bishop, 1994) or a mixture of conditional Gaussian scale mixes (MCGSM) (Theis & Bethge, 2015). However, van den Oord et al. (2016a) showed that a Softmax distribution tends to work better even when the data is implicitly continuous (as is the case with image pixel intensities or audio sample values), one of the reasons being that a categorical distribution is more flexible and can model arbitrary distributions more easily because it does not allow assumptions about their shape. Since raw audio is typically stored as a result of 16-bit integer values (one value per timestep), a Softmax layer would need 65,536 probabilities per timestep to model all possible values."}, {"heading": "2.3 GATED ACTIVATION UNITS", "text": "We use the same gated activation unit as in gated PixelCNN (van den Oord et al., 2016b): z = tanh (Wf, k \u0445 x) \u03c3 (Wg, k \u0445 x), (2) where \u043d denotes a folding operator, an elementary multiplication operator, \u03c3 (\u00b7) is a sigmoid function, k is the layer index, f and g represent filters or gates, and W is a learnable folding filter. In our first experiments, we observed that this nonlinearity worked significantly better than the reflected linear activation function (Nair & Hinton, 2010) for modeling audio signals. 2.4 RESIDUAL AND SKIP CONNECTIONSBoth residual (He et al., 2015) and parameterized skip connections are used throughout the network to accelerate convergence and enable training of much deeper models. In Figure 4, we show a time of our network, many of our residual blocks are used in the model."}, {"heading": "2.5 CONDITIONAL WAVENETS", "text": "Due to an additional input h, WaveNets can model the conditional distribution p (x | h) of the audio input. (3) By conditioning the model to other input variables, we can guide the WaveNet generation to produce audio with the required characteristics. For example, in a multi-speaker environment, we can select the speaker by connecting the speaker identity as an additional input to the model. Likewise, we need to feed information about the text as an additional input for TTS. We condition the model in two different ways: global conditioning and local conditioning. Global conditioning is characterized by a single latent representation h that affects the output distribution over all time levels, e.g. a speaker embedded in a TTS model."}, {"heading": "2.6 CONTEXT STACKS", "text": "A complementary approach is to use a separate, smaller context stack that processes a large portion of the audio signal and conditions a larger WaveNet locally that processes only a smaller portion of the (cut off) audio signal. You can use multiple context stacks with different lengths and numbers of hidden units. Stacks with larger receptive fields have fewer units per level. Context stacks can also have pool layers that run at a lower frequency, keeping the calculation requirements at a reasonable level and in line with the intuition that less capacity is needed to model time correlations at longer time levels."}, {"heading": "3 EXPERIMENTS", "text": "To measure WaveNet's audio modeling performance, we evaluate it using three different tasks: multi-speaker voice generation (without text conditioning), TTS, and music audio modeling. Examples from WaveNet for these experiments are available on its website: https: / / www.deepmind.com / blog / wavenet-generative-model-raw-audio /."}, {"heading": "3.1 MULTI-SPEAKER SPEECH GENERATION", "text": "For the first experiment, we looked at the (non-text conditioned) generation of free speech. We used the English multispeaker corpus of the CSTR voice cloning toolkit (VCTK) (Yamagishi, 2012) and conditioned WaveNet only on the loudspeaker. Conditioning was applied by feeding the loudspeaker ID in the form of a uniform vector into the model. The data set consisted of 44 hours of data from 109 different loudspeakers. Since the model is not conditioned on text, it generates non-existent but human speech-like words in a gentle way with realistic-sounding intonations. This is comparable to generative speech or image models, where the samples appear realistic at first glance, but upon closer inspection are clearly unnatural. The lack of consistency over a large area is partly due to the limited size of the model, the receptive field of the loudspeaker (about 300 milliseconds), which means that it can only remember the last 2-3."}, {"heading": "3.2 TEXT-TO-SPEECH", "text": "For the second experiment, we found ourselves in a position to ernei the aforementioned lcihsrc\u00fcehncS."}, {"heading": "3.3 MUSIC", "text": "For our third series of experiments, we trained WaveNets to model two sets of music: \u2022 the MagnaTagATune dataset (Law & Von Ahn, 2009), which consists of approximately 200 hours of audio music; \u2022 the YouTube Piano dataset, which consists of approximately 60 hours of solo piano music derived from YouTube videos; and since it is limited to a single instrument, it is much easier to model; although it is difficult to quantify these models quantitatively, subjective evaluation is possible by listening to the samples they produce; and we found that increasing the receptive field was critical to obtaining samples that sound musical; even with a receptive field of several seconds, the models have no binary effect on the consistency that led to secondary variations in genre, instrumentation, and sound quality."}, {"heading": "3.4 SPEECH RECOGNITION", "text": "Although WaveNet was designed as a generative model, it can easily be adapted to discriminatory audio tasks such as speech recognition. Traditionally, speech recognition research has largely focused on the use of log-mel filter bank energies or mel-frequency-cepstral coefficients (MFCCs), but has lately shifted to raw audio (Palaz et al., 2013; Tu \ufffd ske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015). Recurring neural networks such as LSTM-RNNNs (Hochreiter & Schmidhuber, 1997) have been a key component in these new language classification pipelines because they allow for the construction of models in broad contexts. WaveNets has shown that layers of digested convolutions allow the receptive field to grow longer than LSTM units."}, {"heading": "4 CONCLUSION", "text": "This paper presented WaveNet, a profound generative model of audio data that works directly at the waveform level. WaveNets are auto-regressive, combining causal filters with extended turns to allow their receiver fields to grow exponentially with depth, which is important to model the long-term time dependencies of audio signals. We demonstrated how WaveNets can be conditioned to other inputs globally (e.g. speaker identity) or locally (e.g. linguistic characteristics), using samples produced on TTS that surpass the current best TTS systems in subjective naturalness. Finally, WaveNets showed promising results when applied to music-audio modeling and speech recognition."}, {"heading": "ACKNOWLEDGEMENTS", "text": "The authors thank Lasse Espeholt, Jeffrey De Fauw and Grzegorz Swirszcz for their contributions, Adam Cain, Max Cant and Adrian Bolton for their help in designing, Helen King, StevenGaffney and Steve Crossan for their help in leading the project, Faith Mackinder for their help in creating the blog, James Besley for their legal support and Demis Hassabis for leading the project and his contributions."}, {"heading": "A TEXT-TO-SPEECH BACKGROUND", "text": "In fact, most people are able to understand themselves and what they are doing."}, {"heading": "B DETAILS OF TTS EXPERIMENT", "text": "Although LSTM RNNs were sampled from speech at 22.05 kHz, speech at 16 kHz sampling was synthesized at runtime with a resampling functionality in the vocaine vocoder (Agiomyrgiannakis, 2015). Both the LSTM-RNN-based statistical parameters and the HMM-driven unit selection synthesizers were built from the voice records in the 16-bit linear PCM, whereas the WaveNet-based statistical parameters were encoded from the language records in the 8-bit legislation. Linguistic features include phone, syllable, phrase and utterance-level features (Zen, 2006)."}], "references": [{"title": "Vocaine the vocoder and applications is speech synthesis", "author": ["Agiomyrgiannakis", "Yannis"], "venue": "In ICASSP, pp. 4230\u20134234,", "citeRegEx": "Agiomyrgiannakis and Yannis.,? \\Q2015\\E", "shortCiteRegEx": "Agiomyrgiannakis and Yannis.", "year": 2015}, {"title": "Mixture density networks", "author": ["Bishop", "Christopher M"], "venue": "Technical Report NCRG/94/004,", "citeRegEx": "Bishop and M.,? \\Q1994\\E", "shortCiteRegEx": "Bishop and M.", "year": 1994}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected CRFs", "author": ["Chen", "Liang-Chieh", "Papandreou", "George", "Kokkinos", "Iasonas", "Murphy", "Kevin", "Yuille", "Alan L"], "venue": "In ICLR,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "The Vowel: Its Nature and Structure", "author": ["Chiba", "Tsutomu", "Kajiyama", "Masato"], "venue": "Tokyo-Kaiseikan,", "citeRegEx": "Chiba et al\\.,? \\Q1942\\E", "shortCiteRegEx": "Chiba et al\\.", "year": 1942}, {"title": "Remaking speech", "author": ["Dudley", "Homer"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "Dudley and Homer.,? \\Q1939\\E", "shortCiteRegEx": "Dudley and Homer.", "year": 1939}, {"title": "An implementation of the \u201calgorithme \u00e0 trous\u201d to compute the wavelet transform", "author": ["Dutilleux", "Pierre"], "venue": null, "citeRegEx": "Dutilleux and Pierre.,? \\Q1989\\E", "shortCiteRegEx": "Dutilleux and Pierre.", "year": 1989}, {"title": "TTS synthesis with bidirectional LSTM based recurrent neural networks", "author": ["Fan", "Yuchen", "Qian", "Yao", "Xie", "Feng-Long", "Soong Frank K"], "venue": "In Interspeech,", "citeRegEx": "Fan et al\\.,? \\Q1964\\E", "shortCiteRegEx": "Fan et al\\.", "year": 1964}, {"title": "Acoustic Theory of Speech Production", "author": ["Fant", "Gunnar"], "venue": "Mouton De Gruyter,", "citeRegEx": "Fant and Gunnar.,? \\Q1970\\E", "shortCiteRegEx": "Fant and Gunnar.", "year": 1970}, {"title": "DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM. NIST speech disc 1-1.1", "author": ["Garofolo", "John S", "Lamel", "Lori F", "Fisher", "William M", "Fiscus", "Jonathon G", "Pallett", "David S"], "venue": "NASA STI/Recon technical report,", "citeRegEx": "Garofolo et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Garofolo et al\\.", "year": 1993}, {"title": "Recent advances in Google real-time HMM-driven unit selection synthesizer", "author": ["Gonzalvo", "Xavi", "Tazari", "Siamak", "Chan", "Chun-an", "Becker", "Markus", "Gutkin", "Alexander", "Silen", "Hanna"], "venue": "In Interspeech,", "citeRegEx": "Gonzalvo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gonzalvo et al\\.", "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["He", "Kaiming", "Zhang", "Xiangyu", "Ren", "Shaoqing", "Sun", "Jian"], "venue": "CoRR, abs/1512.03385,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber", "year": 1997}, {"title": "A real-time algorithm for signal analysis with the help of the wavelet transform", "author": ["Holschneider", "Matthias", "Kronland-Martinet", "Richard", "Morlet", "Jean", "Tchamitchian", "Philippe"], "venue": "Wavelets: Time-Frequency Methods and Phase Space,", "citeRegEx": "Holschneider et al\\.,? \\Q1989\\E", "shortCiteRegEx": "Holschneider et al\\.", "year": 1989}, {"title": "Speech acoustic modeling from raw multichannel waveforms", "author": ["Hoshen", "Yedid", "Weiss", "Ron J", "Wilson", "Kevin W"], "venue": "In ICASSP,", "citeRegEx": "Hoshen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hoshen et al\\.", "year": 2015}, {"title": "Unit selection in a concatenative speech synthesis system using a large speech database", "author": ["Hunt", "Andrew J", "Black", "Alan W"], "venue": "In ICASSP,", "citeRegEx": "Hunt et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Hunt et al\\.", "year": 1996}, {"title": "Unbiased estimation of log spectrum", "author": ["Imai", "Satoshi", "Furuichi", "Chieko"], "venue": "In EURASIP,", "citeRegEx": "Imai et al\\.,? \\Q1988\\E", "shortCiteRegEx": "Imai et al\\.", "year": 1988}, {"title": "Line spectrum representation of linear predictor coefficients of speech signals", "author": ["Itakura", "Fumitada"], "venue": "The Journal of the Acoust. Society of America,", "citeRegEx": "Itakura and Fumitada.,? \\Q1975\\E", "shortCiteRegEx": "Itakura and Fumitada.", "year": 1975}, {"title": "A statistical method for estimation of speech spectral density and formant frequencies", "author": ["Itakura", "Fumitada", "Saito", "Shuzo"], "venue": "Trans. IEICE,", "citeRegEx": "Itakura et al\\.,? \\Q1970\\E", "shortCiteRegEx": "Itakura et al\\.", "year": 1970}, {"title": "Exploring the limits of language modeling", "author": ["J\u00f3zefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "CoRR, abs/1602.02410,", "citeRegEx": "J\u00f3zefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2016}, {"title": "Mixture autoregressive hidden Markov models for speech signals", "author": ["Juang", "Biing-Hwang", "Rabiner", "Lawrence"], "venue": "IEEE Trans. Acoust. Speech Signal Process.,", "citeRegEx": "Juang et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Juang et al\\.", "year": 1985}, {"title": "Speech analysis with multi-kernel linear prediction", "author": ["Kameoka", "Hirokazu", "Ohishi", "Yasunori", "Mochihashi", "Daichi", "Le Roux", "Jonathan"], "venue": "In Spring Conference of ASJ, pp", "citeRegEx": "Kameoka et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kameoka et al\\.", "year": 2010}, {"title": "Text-to-speech conversion with neural networks: A recurrent TDNN approach", "author": ["Karaali", "Orhan", "Corrigan", "Gerald", "Gerson", "Ira", "Massey", "Noel"], "venue": "In Eurospeech,", "citeRegEx": "Karaali et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Karaali et al\\.", "year": 1997}, {"title": "Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequencybased f0 extraction: possible role of a repetitive structure in sounds", "author": ["Kawahara", "Hideki", "Masuda-Katsuse", "Ikuyo", "de Cheveign\u00e9", "Alain"], "venue": "Speech Commn.,", "citeRegEx": "Kawahara et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kawahara et al\\.", "year": 1999}, {"title": "Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modification and synthesis system STRAIGHT", "author": ["Kawahara", "Hideki", "Estill", "Jo", "Fujimura", "Osamu"], "venue": "In MAVEBA, pp", "citeRegEx": "Kawahara et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kawahara et al\\.", "year": 2001}, {"title": "Input-agreement: a new mechanism for collecting data using human computation games", "author": ["Law", "Edith", "Von Ahn", "Luis"], "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems,", "citeRegEx": "Law et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Law et al\\.", "year": 2009}, {"title": "Statistical parametric speech synthesis with joint estimation of acoustic and excitation model parameters", "author": ["Maia", "Ranniery", "Zen", "Heiga", "Gales", "Mark J. F"], "venue": "In ISCA SSW7,", "citeRegEx": "Maia et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Maia et al\\.", "year": 2010}, {"title": "WORLD: A vocoder-based high-quality speech synthesis system for real-time applications", "author": ["Morise", "Masanori", "Yokomori", "Fumiya", "Ozawa", "Kenji"], "venue": "IEICE Trans. Inf. Syst.,", "citeRegEx": "Morise et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Morise et al\\.", "year": 2016}, {"title": "Pitch synchronous waveform processing techniques for text-to-speech synthesis using diphones", "author": ["Moulines", "Eric", "Charpentier", "Francis"], "venue": "Speech Commn.,", "citeRegEx": "Moulines et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Moulines et al\\.", "year": 1990}, {"title": "A deep learning approach to data-driven parameterizations for statistical parametric speech synthesis", "author": ["P. Muthukumar", "Black", "Alan W"], "venue": null, "citeRegEx": "Muthukumar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Muthukumar et al\\.", "year": 2014}, {"title": "Rectified linear units improve restricted Boltzmann machines", "author": ["Nair", "Vinod", "Hinton", "Geoffrey E"], "venue": "In ICML, pp", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Integration of spectral feature extraction and modeling for HMM-based speech synthesis", "author": ["Nakamura", "Kazuhiro", "Hashimoto", "Kei", "Nankaku", "Yoshihiko", "Tokuda", "Keiichi"], "venue": "IEICE Trans. Inf. Syst.,", "citeRegEx": "Nakamura et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nakamura et al\\.", "year": 2014}, {"title": "Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks", "author": ["Palaz", "Dimitri", "Collobert", "Ronan", "Magimai-Doss", "Mathew"], "venue": "In Interspeech,", "citeRegEx": "Palaz et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Palaz et al\\.", "year": 2013}, {"title": "Nonlinear filter design: methodologies and challenges", "author": ["Peltonen", "Sari", "Gabbouj", "Moncef", "Astola", "Jaakko"], "venue": "In IEEE ISPA, pp", "citeRegEx": "Peltonen et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Peltonen et al\\.", "year": 2001}, {"title": "Linear predictive hidden Markov models and the speech signal", "author": ["Poritz", "Alan B"], "venue": "In ICASSP,", "citeRegEx": "Poritz and B.,? \\Q1982\\E", "shortCiteRegEx": "Poritz and B.", "year": 1982}, {"title": "Fundamentals of Speech Recognition", "author": ["Rabiner", "Lawrence", "Juang", "Biing-Hwang"], "venue": "PrenticeHall,", "citeRegEx": "Rabiner et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Rabiner et al\\.", "year": 1993}, {"title": "ATR \u03bd-talk speech synthesis system", "author": ["Sagisaka", "Yoshinori", "Kaiki", "Nobuyoshi", "Iwahashi", "Naoto", "Mimura", "Katsuhiko"], "venue": "In ICSLP, pp", "citeRegEx": "Sagisaka et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Sagisaka et al\\.", "year": 1992}, {"title": "Learning the speech front-end with raw waveform CLDNNs", "author": ["Sainath", "Tara N", "Weiss", "Ron J", "Senior", "Andrew", "Wilson", "Kevin W", "Vinyals", "Oriol"], "venue": "In Interspeech, pp", "citeRegEx": "Sainath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2015}, {"title": "A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis", "author": ["Takaki", "Shinji", "Yamagishi", "Junichi"], "venue": "In ICASSP,", "citeRegEx": "Takaki et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Takaki et al\\.", "year": 2016}, {"title": "Postfilters to modify the modulation spectrum for statistical parametric speech synthesis", "author": ["Takamichi", "Shinnosuke", "Toda", "Tomoki", "Black", "Alan W", "Neubig", "Graham", "Sakriani", "Sakti", "Nakamura", "Satoshi"], "venue": "IEEE/ACM Trans. Audio Speech Lang. Process.,", "citeRegEx": "Takamichi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Takamichi et al\\.", "year": 2016}, {"title": "Generative image modeling using spatial LSTMs", "author": ["Theis", "Lucas", "Bethge", "Matthias"], "venue": "In NIPS, pp. 1927\u20131935,", "citeRegEx": "Theis et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Theis et al\\.", "year": 2015}, {"title": "A speech parameter generation algorithm considering global variance for HMM-based speech synthesis", "author": ["Toda", "Tomoki", "Tokuda", "Keiichi"], "venue": "IEICE Trans. Inf. Syst.,", "citeRegEx": "Toda et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Toda et al\\.", "year": 2007}, {"title": "Statistical approach to vocal tract transfer function estimation based on factor analyzed trajectory hmm", "author": ["Toda", "Tomoki", "Tokuda", "Keiichi"], "venue": "In ICASSP,", "citeRegEx": "Toda et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Toda et al\\.", "year": 2008}, {"title": "Speech synthesis as a statistical machine learning problem. http://www.sp", "author": ["Tokuda", "Keiichi"], "venue": "nitech.ac.jp/ \u0303tokuda/tokuda_asru2011_for_pdf.pdf,", "citeRegEx": "Tokuda and Keiichi.,? \\Q2011\\E", "shortCiteRegEx": "Tokuda and Keiichi.", "year": 2011}, {"title": "Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis", "author": ["Tokuda", "Keiichi", "Zen", "Heiga"], "venue": "In ICASSP,", "citeRegEx": "Tokuda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tokuda et al\\.", "year": 2015}, {"title": "Directly modeling voiced and unvoiced components in speech waveforms by neural networks", "author": ["Tokuda", "Keiichi", "Zen", "Heiga"], "venue": "In ICASSP,", "citeRegEx": "Tokuda et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tokuda et al\\.", "year": 2016}, {"title": "Speech synthesis using artificial neural networks trained on cepstral coefficients", "author": ["Tuerk", "Christine", "Robinson", "Tony"], "venue": "In Proc. Eurospeech,", "citeRegEx": "Tuerk et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Tuerk et al\\.", "year": 1993}, {"title": "Acoustic modeling with deep neural networks using raw time signal for LVCSR", "author": ["T\u00fcske", "Zolt\u00e1n", "Golik", "Pavel", "Schl\u00fcter", "Ralf", "Ney", "Hermann"], "venue": "In Interspeech,", "citeRegEx": "T\u00fcske et al\\.,? \\Q2014\\E", "shortCiteRegEx": "T\u00fcske et al\\.", "year": 2014}, {"title": "Modelling acoustic feature dependencies with artificial neural networks: Trajectory-RNADE", "author": ["Uria", "Benigno", "Murray", "Iain", "Renals", "Steve", "Valentini-Botinhao", "Cassia", "Bridle", "John"], "venue": "In ICASSP,", "citeRegEx": "Uria et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Uria et al\\.", "year": 2015}, {"title": "Pixel recurrent neural networks", "author": ["van den Oord", "A\u00e4ron", "Kalchbrenner", "Nal", "Kavukcuoglu", "Koray"], "venue": "arXiv preprint arXiv:1601.06759,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Conditional image generation with PixelCNN decoders", "author": ["van den Oord", "A\u00e4ron", "Kalchbrenner", "Nal", "Vinyals", "Oriol", "Espeholt", "Lasse", "Graves", "Alex", "Kavukcuoglu", "Koray"], "venue": "CoRR, abs/1606.05328,", "citeRegEx": "Oord et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Oord et al\\.", "year": 2016}, {"title": "Minimum generation error training with direct log spectral distortion on LSPs for HMM-based speech synthesis", "author": ["Wu", "Yi-Jian", "Tokuda", "Keiichi"], "venue": "In Interspeech,", "citeRegEx": "Wu et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2008}, {"title": "English multi-speaker corpus for CSTR voice cloning", "author": ["Yamagishi", "Junichi"], "venue": null, "citeRegEx": "Yamagishi and Junichi.,? \\Q2012\\E", "shortCiteRegEx": "Yamagishi and Junichi.", "year": 2012}, {"title": "Simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for HMM-based text-to-speech systems", "author": ["Yoshimura", "Takayoshi"], "venue": "PhD thesis, Nagoya Institute of Technology,", "citeRegEx": "Yoshimura and Takayoshi.,? \\Q2002\\E", "shortCiteRegEx": "Yoshimura and Takayoshi.", "year": 2002}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["Yu", "Fisher", "Koltun", "Vladlen"], "venue": "In ICLR,", "citeRegEx": "Yu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2016}, {"title": "An example of context-dependent label format for HMM-based speech synthesis", "author": ["Zen", "Heiga"], "venue": null, "citeRegEx": "Zen and Heiga.,? \\Q2006\\E", "shortCiteRegEx": "Zen and Heiga.", "year": 2006}, {"title": "Statistical parametric speech synthesis", "author": ["2007. Zen", "Heiga", "Tokuda", "Keiichi", "Black", "Alan W"], "venue": null, "citeRegEx": "Zen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2007}, {"title": "Statistical parametric speech synthesis using deep", "author": ["Zen", "Heiga", "Senior", "Andrew", "Schuster", "Mike"], "venue": "Commn.,", "citeRegEx": "Zen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Zen et al\\.", "year": 2009}, {"title": "reported three major factors that can degrade the subjective naturalness; quality of vocoders, accuracy of generative models, and effect of oversmoothing. The first factor causes the artifacts and the second and third factors lead to the muffleness in the synthesized speech. There have been a number of attempts to address these issues individually, such as developing high-quality vocoders", "author": ["Zen"], "venue": "(Kawahara et al.,", "citeRegEx": "Zen,? \\Q2009\\E", "shortCiteRegEx": "Zen", "year": 2009}], "referenceMentions": [{"referenceID": 18, "context": ", 2016a;b) and text (J\u00f3zefowicz et al., 2016).", "startOffset": 20, "endOffset": 45}, {"referenceID": 12, "context": "signal processing (Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al.", "startOffset": 18, "endOffset": 62}, {"referenceID": 2, "context": ", 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu & Koltun, 2016).", "startOffset": 49, "endOffset": 87}, {"referenceID": 48, "context": "However, van den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the data is implicitly continuous (as is the case for image pixel intensities or audio sample values).", "startOffset": 17, "endOffset": 37}, {"referenceID": 10, "context": "Both residual (He et al., 2015) and parameterised skip connections are used throughout the network, to speed up convergence and enable training of much deeper models.", "startOffset": 14, "endOffset": 31}, {"referenceID": 9, "context": "As example-based and model-based speech synthesis baselines, hidden Markov model (HMM)-driven unit selection concatenative (Gonzalvo et al., 2016) and long short-term memory recurrent neural network (LSTM-RNN)-based statistical parametric (Zen et al.", "startOffset": 123, "endOffset": 146}, {"referenceID": 31, "context": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T\u00fcske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015).", "startOffset": 190, "endOffset": 273}, {"referenceID": 46, "context": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T\u00fcske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015).", "startOffset": 190, "endOffset": 273}, {"referenceID": 13, "context": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T\u00fcske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015).", "startOffset": 190, "endOffset": 273}, {"referenceID": 36, "context": "Traditionally, speech recognition research has largely focused on using log mel-filterbank energies or mel-frequency cepstral coefficients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T\u00fcske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015).", "startOffset": 190, "endOffset": 273}, {"referenceID": 8, "context": "As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset.", "startOffset": 80, "endOffset": 103}], "year": 2016, "abstractText": "This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.", "creator": "LaTeX with hyperref package"}}}