{"id": "1609.03528", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Sep-2016", "title": "The Microsoft 2016 Conversational Speech Recognition System", "abstract": "We describe Microsoft's conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.3%, representing an improvement over previously reported results on this benchmark task.", "histories": [["v1", "Mon, 12 Sep 2016 18:59:29 GMT  (85kb,D)", "http://arxiv.org/abs/1609.03528v1", null], ["v2", "Wed, 25 Jan 2017 08:27:28 GMT  (75kb,D)", "http://arxiv.org/abs/1609.03528v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["w xiong", "j droppo", "x huang", "f seide", "m seltzer", "a stolcke", "d yu", "g zweig"], "accepted": false, "id": "1609.03528"}, "pdf": {"name": "1609.03528.pdf", "metadata": {"source": "CRF", "title": "THE MICROSOFT 2016 CONVERSATIONAL SPEECH RECOGNITION SYSTEM", "authors": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Conversational speech recognition, convolutional neuronal networks, recurrent neuronal networks, VGG, ResNet, LACE, BLSTM."}, {"heading": "1. INTRODUCTION", "text": "While the basic structures have long been known [1, 2, 3, 4, 5, 6, 7], they have only recently emerged as the best models for speech recognition. Surprisingly, this is the case for both acoustic modeling [8, 9, 10, 11] and speech modeling [12, 13]. Compared to standard feed-forward MLPs or DNNs, these acoustic models describe the ability to model a large amount of acoustic contexts with temporal invariance, and in the case of revolutionary models, with frequency invariance [12, 13]. In speech modeling, recurring models seem to improve over classical N-gram models by generalizing continuous word representations."}, {"heading": "2. CONVOLUTIONAL AND LSTM NEURAL NETWORKS", "text": "We use three CNN variants. The first is the VGG architecture of [22]. Compared to the networks previously used in image recognition, this network uses small (3x3) filters, is deeper and applies up to five revolutionary layers before pooling. The second network is based on the ResNet architecture [23], which adds a linear transformation of the input factors of each layer to the output of the layer [24]. The only difference is that we move the batch normalization node directly before each ReLU activation to the location. The last CNN variant is the LACE model (Layer-wise context expansion with attention) [17]. LACE is a TDNN variant [3], in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layer frames. In other words, each higher layer uses the broader context than lower layer-M layers."}, {"heading": "3. SPEAKER ADAPTIVE MODELING", "text": "The adaptive modeling of loudspeakers in our system is based on conditioning the network based on an i-vector [27] characterization of each loudspeaker [20, 28]. For each conversation side, a 100-dimensional i-vector is generated. For the LSTM system, we add the conversation-side i-vector vs to each frame of the input. For conventional networks, this approach is inappropriate as we do not expect to see spatially coherent patterns in the input. Instead, for the CNNs, we add a learnable weight matrix W l to each layer and then add W lvs to activate the layer prior to nonlinearity. Therefore, the i-vector at CNN essentially serves as an additional distortion of each layer. Note that the i-vectors are estimated based on MFCC characteristics; by using them subsequently in systems based on log filter bank characteristics, we can benefit from a combination of features."}, {"heading": "4. LATTICE-FREE SEQUENCE TRAINING", "text": "This year it has come to the point where it will be able to ereniei the aforementioned hsci-eSrcnlhSrc\u00fch. \"s tsi rf\u00fc ide hsci-eSrteeSrc\u00fch,\" he says. \"It's like we're able to reewnn,\" he says. \"It's like we're able to be able to be able,\" he says. \"It's like we're able to reewnn,\" he says. \"It's like we're able to be able to remember ourselves.\""}, {"heading": "5. LM RESCORING AND SYSTEM COMBINATION", "text": "A first decoding is done with a WFST decoder using the architecture described in [31]. We use an N-gram language model trained and cropped with the SRILM toolkit [32]. The first pass-through LM contains about 15.9 million bigrams, trigrams, and 4grams, as well as a vocabulary of 30500 words, resulting in a confusion of 54 to RT-03 language transcripts. The initial decoding generates a grid with the marked pronunciation variants from which 500-best lists are generated for rescoring purposes. The following N-best rescoring uses an uncropped LM with 145 million N-grams. All N-gram LMs were estimated with a maximum entropy criterion, as described in [33]."}, {"heading": "5.1. RNNLM setup", "text": "The N-best hypotheses are then rescored using a combination of the large N-gram LM and several RNNLMs trained and evaluated with the CUED-RNNLM toolkit. [34] Our RNLM configuration has several characteristics: \u2022 We trained both standard, forward-facing RNNLMs and backward-facing RNNLMs that predict words in reverse chronological order. \u2022 In addition, we trained a second RNLM for each direction, which is achieved by different random initial weights. \u2022 To use the hidden initial weights, the two RNLMs and the N-gram LM are interpolated at word level with corresponding N-gram LM probabilities (separated for the forward and reverse models). \u2022 To use the hidden initial weights, the two RNLMs and the N-gram LM for each direction were interpolated with weights of (0.375, 0.375, 0.25)."}, {"heading": "5.2. Training data", "text": "The 4 gram decoding language model was trained using the available CTS transcripts from the DARPA-EARS program: PBX (3M words), BBN PBX-2 transcripts (850k), Fisher (21M), English CallHome (200k), and the University of Washington Conversation Webcorpus (191M). A separate model was trained from each source and interpolated with weights optimized for RT-03 transcripts. An additional LM component, trained on 133M words from LDC broadcast news texts, was added for untrimmed 4 gram rescoring, and the N gram LM configuration was modeled using the model described in [28], except for maximum smoothing."}, {"heading": "5.3. RNNLM performance", "text": "Table 3 shows perplexity and word errors for various RNNLM setups, from simple to complex. The acoustic model used was a preliminary CNN. As can be seen, each of the measures described above adds incremental gains, which, although small individually, result in a relative 14% reduction in word errors."}, {"heading": "5.4. System Combination", "text": "LM rescoring is done separately for each acoustic model. However, the rescored N leaderboards from each subsystem are then combined into a single confusion network using the SRILM nbest Rover tool [21]. However, the number of potential candidate systems is too large to allow an all-encompassing combination, both for practical reasons and due to overfitting issues. Instead, we conduct a greedy search, starting with the best single system and gradually adding additional systems to find a small group of systems that complement each other to the maximum. RT-02 Switchboard Set was used for this search procedure. Once a good subset of systems has been found, their relative weighting (for coordination mediated by confusion networks) is optimized using an EM algorithm, also using the development kit."}, {"heading": "6. EXPERIMENTAL SETUP AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Speech corpora", "text": "We train with the commonly used English CTS (Switchboard and Fisher) corpora. The evaluation is done with the NIST 2000 CTS test set, which includes both Switchboard (SWB) and CallHome (CH) subsets. The Switchboard-1 part of the NIST 2002 CTS test set was used for tuning and development; the acoustic training data consists of the LDC Korpora 97S62, 2004S13, 2005S13, 2004S11 and 2004S09; for a complete description see [18]."}, {"heading": "6.2. 1-bit SGD Training", "text": "To make the training feasible, we are paralleling the training with our previously proposed 1-bit SGD parallelization technique [36]. This data-parallel method distributes minibatches to multiple worker nodes and then aggregates the sub-gradients. While the required communication time would otherwise be prohibitive, the 1-bit SGD method eliminates the bottleneck by two techniques: 1-bit quantification of gradients and automatic scaling of the minibatch size. In [36] we showed that gradient values can be quantified to only one bit when transferring the quantization error from one minibatch to the next. Second, each time a sub-gradient is quantified, the quantization error is calculated and stored, and then added to the subgradient of the next minibatch. This reduces the required bandwidth 32-fold with minimal loss of accuracy."}, {"heading": "6.3. Acoustic Model Details", "text": "The CNN models used window sizes as shown in Figure 1, and the LSTMs processed an input frame every 10 milliseconds. Most of our models use three static left-right triphone models with 9000 bound states. In addition, we have trained several models with 27k bound states. Phonetic inventory includes specialized models for noise, vocalizednoise, laughter, and silence. We use a 30k vocabulary derived from the most common words in the switchboard and Fisher Corporation. The decoder uses a statically generated unigram graph and dynamically applies the language model evaluation. The unigram graph has about 300k states and 500k slurs. All acoustic models have been trained using the Open Source Computational Network Toolkit (CNTK)."}, {"heading": "6.4. Comparative System Performance", "text": "The model performance of our individual models as well as relevant comparisons from the literature are shown in Table 5. Of the 15 models built, only models are shown that are not weight-equal in the final system combination."}, {"heading": "7. RELATION TO PRIOR WORK", "text": "Compared to previous applications of CNNs for speech recognition [38, 39], our networks are much deeper and use linear bypass connections across sinuous layers, similar in spirit to those studied more recently by [11, 10, 28]. We are enhancing these architectures with the LACE model [17], which iteratively expands the effective window size layer by layer and adds an attention mask for the differently weighted remote context. Our use of grid-free MMI is distinctive and extends previous work [18, 19] by suggesting the use of a mixed triphon / phoneme gradient in the speech model. On the voice modeling side, we achieve a power boost by combining multiple RNNLMs in forward and backward directions and applying a two-phase training program to get the best results from data outside the domain."}, {"heading": "8. CONCLUSIONS", "text": "The use of CNNs in the acoustic model has proven to be uniquely effective, as has the use of RNN speech models. Our best single system achieves a 6.9% error rate in the NIST 2000 switchboard set. We believe this is the best performance yet for a recognition system that is not based on system combinations. An interplay of acoustic models brings the state of the art to 6.3% in the switchboard test data. Thank you. We thank X. Chen of CUED for valuable help with the CUED RNNLM toolkit and ICSI for computing and data resources."}, {"heading": "9. REFERENCES", "text": "[1] F. J. Pineda, \"Generalization of back-propagation to recurrent neural networks,\" Physical review letters, vol. 59, pp. 2229, 1987. [2] R. J. Williams and D. Zipser, \"A learning algorithm for continuously running fully recurrent neural networks,\" Neural computation, vol. 1, pp. 270-280, 1989. [3] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang, \"Phoneme recognition using time-delay neural networks,\" IEEE transactions on acoustics, speech, and signal processing, vol."}], "references": [{"title": "Generalization of back-propagation to recurrent neural networks", "author": ["F.J. Pineda"], "venue": "Physical review letters, vol. 59, pp. 2229", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1987}, {"title": "A learning algorithm for continually running fully recurrent neural networks", "author": ["R.J. Williams", "D. Zipser"], "venue": "Neural computation, vol. 1, pp. 270\u2013280", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Phoneme recognition using time-delay neural networks", "author": ["A. Waibel", "T. Hanazawa", "G. Hinton", "K. Shikano", "K.J. Lang"], "venue": "IEEE transactions on acoustics, speech, and signal processing, vol. 37, pp. 328\u2013 339", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1989}, {"title": "Convolutional networks for images", "author": ["Y. LeCun", "Y. Bengio"], "venue": "speech, and time series\u201d, The handbook of brain theory and neural networks, vol. 3361, pp. 1995", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural computation, vol. 1, pp. 541\u2013551", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1989}, {"title": "A recurrent error propagation network speech recognition system", "author": ["T. Robinson", "F. Fallside"], "venue": "Computer Speech & Language, vol. 5, pp. 259\u2013274", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1991}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, pp. 1735\u20131780", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1997}, {"title": "and F", "author": ["H. Sak", "A.W. Senior"], "venue": "Beaufays, \u201cLong short-term memory recurrent neural network architectures for large scale acoustic modeling.\u201d, in Interspeech, pp. 338\u2013342", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["H. Sak", "A. Senior", "K. Rao", "F. Beaufays"], "venue": "arXiv preprint arXiv:1507.06947", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "The ibm 2015 english conversational telephone speech recognition system", "author": ["G. Saon", "H.-K.J. Kuo", "S. Rennie", "M. Picheny"], "venue": "arXiv preprint arXiv:1505.05899", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["T. Sercu", "C. Puhrsch", "B. Kingsbury", "Y. LeCun"], "venue": "ICASSP, pp. 4955\u2013 4959. IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "and S", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3"], "venue": "Khudanpur, \u201cRecurrent neural network based language model.\u201d, in Interspeech, vol. 2, p. 3", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Context dependent recurrent neural network language model.", "author": ["T. Mikolov", "G. Zweig"], "venue": "in SLT,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Linguistic regularities in continuous space word representations.", "author": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "venue": "in HLT-NAACL,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, pp. 3104\u20133112", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A", "author": ["A. Hannun", "C. Case", "J. Casper", "B. Catanzaro", "G. Diamos", "E. Elsen", "R. Prenger", "S. Satheesh", "S. Sengupta"], "venue": "Coates, et al., \u201cDeep speech: Scaling up end-to-end speech recognition\u201d, arXiv preprint arXiv:1412.5567", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional neural networks with layer-wise context expansion and attention", "author": ["D. Yu", "W. Xiong", "J. Droppo", "A. Stolcke", "G. Ye", "J. Li", "G. Zweig"], "venue": "Interspeech", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Advances in speech transcription at IBM under the DARPA EARS program", "author": ["S.F. Chen", "B. Kingsbury", "L. Mangu", "D. Povey", "G. Saon", "H. Soltau", "G. Zweig"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, pp. 1596\u20131608", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free mmi", "author": ["D. Povey", "V. Peddinti", "D. Galvez", "P. Ghahrmani", "V. Manohar", "X. Na", "Y. Wang", "S. Khudanpur"], "venue": "Submitted to Interspeech", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "and M", "author": ["G. Saon", "H. Soltau", "D. Nahamoo"], "venue": "Picheny, \u201cSpeaker adaptation of neural network acoustic models using i-vectors.\u201d, in ASRU, pp. 55\u2013 59", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "arXiv preprint arXiv:1512.03385", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Linearly augmented deep neural network", "author": ["P. Ghahremani", "J. Droppo", "M.L. Seltzer"], "venue": "ICASSP, pp. 5085\u20135089. IEEE", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Consonant recognition by modular construction of large phonemic time-delay neural networks", "author": ["A. Waibel", "H. Sawai", "K. Shikano"], "venue": "Acoustics, Speech, and Signal Processing, 1989. ICASSP-89., 1989 International Conference on, pp. 112\u2013115. IEEE", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1989}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, vol. 18, pp. 602\u2013610", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Frontend factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, pp. 788\u2013798", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "The IBM 2016 English conversational telephone speech recognition system", "author": ["G. Saon", "T. Sercu", "S.J. Rennie", "H.J. Kuo"], "venue": "CoRR, vol. abs/1604.08242", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequential classification criteria for NNs in automatic speech recognition.", "author": ["G. Wang", "K. Sim"], "venue": "in Interspeech,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "and D", "author": ["K. Vesel\u1ef3", "A. Ghoshal", "L. Burget"], "venue": "Povey, \u201cSequencediscriminative training of deep neural networks.\u201d, in Interspeech, pp. 2345\u20132349", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Parallelizing WFST speech decoders", "author": ["C. Mendis", "J. Droppo", "S. Maleki", "M. Musuvathi", "T. Mytkowicz", "G. Zweig"], "venue": "ICASSP, pp. 5325\u20135329. IEEE", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "SRILM\u2014an extensible language modeling toolkit", "author": ["A. Stolcke"], "venue": "Interspeech, vol. 2002, p. 2002", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2002}, {"title": "Efficient estimation of maximum entropy language models with N-gram features: An SRILM extension", "author": ["T. Alum\u00e4e", "M. Kurimo"], "venue": "Interspeech, pp. 1820\u20131823", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "CUED- RNNLM: An open-source toolkit for efficient training and evaluation of recurrent neural network language models", "author": ["X. Chen", "X. Liu", "Y. Qian", "M.J.F. Gales", "P.C. Woodland"], "venue": "ICASSP, pp. 6000\u2013 6004. IEEE", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Noise-contrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["M. Gutmann", "A. Hyv\u00e4rinen"], "venue": "AISTATS, vol. 1, pp. 6", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs", "author": ["F. Seide", "H. Fu", "J. Droppo", "G. Li", "D. Yu"], "venue": "INTERSPEECH, pp. 1058\u20131062", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "H", "author": ["D. Yu", "A. Eversole", "M. Seltzer", "K. Yao", "Z. Huang", "B. Guenter", "O. Kuchaiev", "Y. Zhang", "F. Seide"], "venue": "Wang, et al., \u201cAn introduction to computational networks and the computational network toolkit\u201d, Technical report, Technical report, Tech. Rep. MSR, Microsoft Research, 2014, 2014. research. microsoft. com/apps/pubs", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep convolutional neural networks for lvcsr", "author": ["T.N. Sainath", "A.-r. Mohamed", "B. Kingsbury", "B. Ramabhadran"], "venue": "in ICASSP,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2013}, {"title": "Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition", "author": ["O. Abdel-Hamid", "A.-r. Mohamed", "H. Jiang", "G. Penn"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "While the basic structures have been well known for a long period [1, 2, 3, 4, 5, 6, 7], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 87}, {"referenceID": 1, "context": "While the basic structures have been well known for a long period [1, 2, 3, 4, 5, 6, 7], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 87}, {"referenceID": 2, "context": "While the basic structures have been well known for a long period [1, 2, 3, 4, 5, 6, 7], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 87}, {"referenceID": 3, "context": "While the basic structures have been well known for a long period [1, 2, 3, 4, 5, 6, 7], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 87}, {"referenceID": 4, "context": "While the basic structures have been well known for a long period [1, 2, 3, 4, 5, 6, 7], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 87}, {"referenceID": 5, "context": "While the basic structures have been well known for a long period [1, 2, 3, 4, 5, 6, 7], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 87}, {"referenceID": 6, "context": "While the basic structures have been well known for a long period [1, 2, 3, 4, 5, 6, 7], it is only recently that they have emerged as the best models for speech recognition.", "startOffset": 66, "endOffset": 87}, {"referenceID": 7, "context": "Surprisingly, this is the case for both acoustic modeling [8, 9, 10, 11] and language modeling [12, 13].", "startOffset": 58, "endOffset": 72}, {"referenceID": 8, "context": "Surprisingly, this is the case for both acoustic modeling [8, 9, 10, 11] and language modeling [12, 13].", "startOffset": 58, "endOffset": 72}, {"referenceID": 9, "context": "Surprisingly, this is the case for both acoustic modeling [8, 9, 10, 11] and language modeling [12, 13].", "startOffset": 58, "endOffset": 72}, {"referenceID": 10, "context": "Surprisingly, this is the case for both acoustic modeling [8, 9, 10, 11] and language modeling [12, 13].", "startOffset": 58, "endOffset": 72}, {"referenceID": 11, "context": "Surprisingly, this is the case for both acoustic modeling [8, 9, 10, 11] and language modeling [12, 13].", "startOffset": 95, "endOffset": 103}, {"referenceID": 12, "context": "Surprisingly, this is the case for both acoustic modeling [8, 9, 10, 11] and language modeling [12, 13].", "startOffset": 95, "endOffset": 103}, {"referenceID": 13, "context": "In language modeling, recurrent models appear to improve over classical N-gram models through the generalization ability of continuous word representations [14].", "startOffset": 156, "endOffset": 160}, {"referenceID": 14, "context": "In the meantime, ensemble learning has become commonly used in several neural models [15, 16, 13], to improve robustness by reducing bias and variance.", "startOffset": 85, "endOffset": 97}, {"referenceID": 15, "context": "In the meantime, ensemble learning has become commonly used in several neural models [15, 16, 13], to improve robustness by reducing bias and variance.", "startOffset": 85, "endOffset": 97}, {"referenceID": 12, "context": "In the meantime, ensemble learning has become commonly used in several neural models [15, 16, 13], to improve robustness by reducing bias and variance.", "startOffset": 85, "endOffset": 97}, {"referenceID": 16, "context": "An attention mechanism in the LACE CNN which differentially weights distant context [17]", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Lattice-free MMI training [18, 19]", "startOffset": 26, "endOffset": 34}, {"referenceID": 18, "context": "Lattice-free MMI training [18, 19]", "startOffset": 26, "endOffset": 34}, {"referenceID": 19, "context": "The use of i-vector based adaptation [20] in all models 5.", "startOffset": 37, "endOffset": 41}, {"referenceID": 11, "context": "Language model (LM) rescoring with multiple, recurrent neural net LMs [12] running in both forward and reverse direction", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "The first is the VGG architecture of [22].", "startOffset": 37, "endOffset": 41}, {"referenceID": 21, "context": "The second network is modeled on the ResNet architecture [23], which adds a linear transform of each layer\u2019s input to the layer\u2019s output [24].", "startOffset": 57, "endOffset": 61}, {"referenceID": 22, "context": "The second network is modeled on the ResNet architecture [23], which adds a linear transform of each layer\u2019s input to the layer\u2019s output [24].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "The last CNN variant is the LACE (layer-wise context expansion with attention) model [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 2, "context": "LACE is a TDNN [3] variant in which each higher layer is a weighted sum of nonlinear transformations of a window of lower layer frames.", "startOffset": 15, "endOffset": 18}, {"referenceID": 2, "context": "[3, 25] in the use of a learned attention mask and ResNet like linear pass-through.", "startOffset": 0, "endOffset": 7}, {"referenceID": 23, "context": "[3, 25] in the use of a learned attention mask and ResNet like linear pass-through.", "startOffset": 0, "endOffset": 7}, {"referenceID": 24, "context": "We use a bidirectional architecture [26] without frame-skipping [9].", "startOffset": 36, "endOffset": 40}, {"referenceID": 8, "context": "We use a bidirectional architecture [26] without frame-skipping [9].", "startOffset": 64, "endOffset": 67}, {"referenceID": 7, "context": "The core model structure is the LSTM defined in [8].", "startOffset": 48, "endOffset": 51}, {"referenceID": 25, "context": "Speaker adaptive modeling in our system is based on conditioning the network on an i-vector [27] characterization of each speaker [20, 28].", "startOffset": 92, "endOffset": 96}, {"referenceID": 19, "context": "Speaker adaptive modeling in our system is based on conditioning the network on an i-vector [27] characterization of each speaker [20, 28].", "startOffset": 130, "endOffset": 138}, {"referenceID": 26, "context": "Speaker adaptive modeling in our system is based on conditioning the network on an i-vector [27] characterization of each speaker [20, 28].", "startOffset": 130, "endOffset": 138}, {"referenceID": 27, "context": "As noted in [29, 30], the necessary gradient for use in backpropagation is a simple function of the posterior probability of a particular acoustic model state at a given time, as computed by summing over all possible word sequences in an unconstrained manner.", "startOffset": 12, "endOffset": 20}, {"referenceID": 28, "context": "As noted in [29, 30], the necessary gradient for use in backpropagation is a simple function of the posterior probability of a particular acoustic model state at a given time, as computed by summing over all possible word sequences in an unconstrained manner.", "startOffset": 12, "endOffset": 20}, {"referenceID": 17, "context": "As first done in [18], and more recently in [19], this can be accomplished with a straightforward alpha-beta computation over the finite state acceptor representing the decoding search space.", "startOffset": 17, "endOffset": 21}, {"referenceID": 18, "context": "As first done in [18], and more recently in [19], this can be accomplished with a straightforward alpha-beta computation over the finite state acceptor representing the decoding search space.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "In [18], the Table 1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "In [19], a language model on phonemes is used instead.", "startOffset": 3, "endOffset": 7}, {"referenceID": 18, "context": "As in [19], we use cross-entropy regularization.", "startOffset": 6, "endOffset": 10}, {"referenceID": 29, "context": "An initial decoding is done with a WFST decoder, using the architecture described in [31].", "startOffset": 85, "endOffset": 89}, {"referenceID": 30, "context": "We use an N-gram language model trained and pruned with the SRILM toolkit [32].", "startOffset": 74, "endOffset": 78}, {"referenceID": 31, "context": "All N-gram LMs were estimated by a maximum entropy criterion as described in [33].", "startOffset": 77, "endOffset": 81}, {"referenceID": 32, "context": "The N-best hypotheses are then rescored using a combination of the large N-gram LM and several RNNLMs, trained and evaluated using the CUED-RNNLM toolkit [34].", "startOffset": 154, "endOffset": 158}, {"referenceID": 11, "context": "This produced lower perplexity and word error than the standard, single-hiddenlayer RNNLM architecture [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 33, "context": "Training used noise-contrastive estimation (NCE) [35].", "startOffset": 49, "endOffset": 53}, {"referenceID": 26, "context": "The N-gram LM configuration is modeled after that described in [28], except that maxent smoothing was used.", "startOffset": 63, "endOffset": 67}, {"referenceID": 17, "context": "The acoustic training data is comprised by LDC corpora 97S62, 2004S13, 2005S13, 2004S11 and 2004S09; see [18] for a full description.", "startOffset": 105, "endOffset": 109}, {"referenceID": 34, "context": "To make training feasible, we parallelize training with our previously proposed 1-bit SGD parallelization technique [36].", "startOffset": 116, "endOffset": 120}, {"referenceID": 34, "context": "In [36], we showed that gradient values can be quantized to just a single bit, if one carries over the quantization error from one minibatch to the next.", "startOffset": 3, "endOffset": 7}, {"referenceID": 35, "context": "All acoustic models were trained using the open-source Computational Network Toolkit (CNTK) [37].", "startOffset": 92, "endOffset": 96}, {"referenceID": 26, "context": "[28] LSTM 15.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[19] LSTM 15.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] Combination - - 12.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Compared to earlier applications of CNNs to speech recognition [38, 39], our networks are much deeper, and use linear bypass connections across convolutional layers.", "startOffset": 63, "endOffset": 71}, {"referenceID": 37, "context": "Compared to earlier applications of CNNs to speech recognition [38, 39], our networks are much deeper, and use linear bypass connections across convolutional layers.", "startOffset": 63, "endOffset": 71}, {"referenceID": 10, "context": "They are similar in spirit to those studied more recently by [11, 10, 28].", "startOffset": 61, "endOffset": 73}, {"referenceID": 9, "context": "They are similar in spirit to those studied more recently by [11, 10, 28].", "startOffset": 61, "endOffset": 73}, {"referenceID": 26, "context": "They are similar in spirit to those studied more recently by [11, 10, 28].", "startOffset": 61, "endOffset": 73}, {"referenceID": 16, "context": "We improve on these architectures with the LACE model [17], which iteratively expands the effective window size, layer-by-layer, and adds an attention mask to differentially weight distant context.", "startOffset": 54, "endOffset": 58}, {"referenceID": 17, "context": "Our use of lattice-free MMI is distinctive, and extends previous work [18, 19] by proposing the use of a mixed triphone/phoneme history in the language model.", "startOffset": 70, "endOffset": 78}, {"referenceID": 18, "context": "Our use of lattice-free MMI is distinctive, and extends previous work [18, 19] by proposing the use of a mixed triphone/phoneme history in the language model.", "startOffset": 70, "endOffset": 78}], "year": 2016, "abstractText": "We describe Microsoft\u2019s conversational speech recognition system, in which we combine recent developments in neural-network-based acoustic and language modeling to advance the state of the art on the Switchboard recognition task. Inspired by machine learning ensemble techniques, the system uses a range of convolutional and recurrent neural networks. I-vector modeling and lattice-free MMI training provide significant gains for all acoustic model architectures. Language model rescoring with multiple forward and backward running RNNLMs, and word posterior-based system combination provide a 20% boost. The best single system uses a ResNet architecture acoustic model with RNNLM rescoring, and achieves a word error rate of 6.9% on the NIST 2000 Switchboard task. The combined system has an error rate of 6.3%, representing an improvement over previously reported results on this benchmark task.", "creator": "LaTeX with hyperref package"}}}