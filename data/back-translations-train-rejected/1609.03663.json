{"id": "1609.03663", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "An Experimental Study of LSTM Encoder-Decoder Model for Text Simplification", "abstract": "Text simplification (TS) aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning. Current automatic TS techniques are limited to either lexical-level applications or manually defining a large amount of rules. Since deep neural networks are powerful models that have achieved excellent performance over many difficult tasks, in this paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder model for sentence level TS, which makes minimal assumptions about word sequence. We conduct preliminary experiments to find that the model is able to learn operation rules such as reversing, sorting and replacing from sequence pairs, which shows that the model may potentially discover and apply rules such as modifying sentence structure, substituting words, and removing words for TS.", "histories": [["v1", "Tue, 13 Sep 2016 03:02:32 GMT  (167kb,D)", "http://arxiv.org/abs/1609.03663v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["tong wang", "ping chen", "kevin amaral", "jipeng qiang"], "accepted": false, "id": "1609.03663"}, "pdf": {"name": "1609.03663.pdf", "metadata": {"source": "CRF", "title": "An Experimental Study of LSTM Encoder-Decoder Model for Text Simplification", "authors": ["Tong Wang", "Ping Chen", "Kevin Amaral", "Jipeng Qiang"], "emails": ["tong.wang001@umb.edu", "ping.chen@umb.edu", "kevin.amaral001@umb.edu"], "sections": [{"heading": null, "text": "Keywords: Text Simplification, LSTM, encoder decoder"}, {"heading": "1 Introduction", "text": "Text Simplification (TS) aims to simplify the lexical, grammatical or structural complexity of the text while maintaining its semantic meaning; it can help different groups of people, including children, non-native speakers and people with cognitive disabilities, to better understand text; the field of automatic text simplification has been explored for decades; however, there are challenges to the LS approach; firstly, the number of transformation rules required for reasonable coverage; secondly, different transformation rules should be applied to the specific context; thirdly, the syntax and semantic meaning of the sentence is difficult to maintain."}, {"heading": "2 Related Work", "text": "Automatic TS is a complicated task of natural language processing (NLP), consisting of lexical and syntactical levels of simplification. Normally, handmade, supervised and unattended methods based on resources such as English Wikipedia (EW) and plain English Wikipedia (SEW) [6] are used for simplification. It is very easy to confuse the automatic TS task with the automatic summary task [7; 8]. TS differs from the text summary because the emphasis of the text summary is to reduce the length and redundant content. On the lexical level, [9] proposed a lexical simplification system that requires only a large corpus of regular text to obtain words similar to the complex word. [10] suggested an uncontrolled method of learning pairs of complex and simpler synonyms and a contextual method of replacing another."}, {"heading": "3 The Model", "text": "In this section we first briefly introduce the basic idea of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) and then describe the LSTM encoder decoder model."}, {"heading": "3.1 Recurrent Neural Network and Long Short-Term Memory", "text": "RNN is a class of neural networks in which internal units can form a directed cycle to demonstrate the state history of previous inputs. Of course, the structure of RNN is suitable for variable-length inputs such as records. However, for sequence data (x1,..., xT), in which the hidden state of the RNN is updated viaht = f (ht \u2212 1, xt) (1) with each t {1,..., T}, the optimization of basic RNN models is difficult because its histories disappear over long sequences. LSTM is very good at learning dependencies over long dities through its internal memory cells. Similar to RNN, LSTM updates its hidden state sequentially, but the updates depend heavily on memory cells containing three types of gates: The gate ct determines how much remembered information is to be forgotten, the update gate determines how the stored information is updated, \u2212 the output gate decides how much information is stored, \u2212 the output gate decides how much is not stored."}, {"heading": "3.2 LSTM Encoder-Decoder Model", "text": "Given a source sentence X = (x1, x2,..., xl) and the target sentence (simplified) Y = (y1, y2,..., yl \u2032) in which xi and yi are contained in the same vocabulary, l and l \u2032 are potentially the length of each sentence. Our goal is to build a neural network to model the conditional probability p (Y | X), and then train the model to maximize the probability. We show that adding an embedding layer can significantly improve performance when the vocabulary grows large. Then, we feed word embedding through two LSTM layers in the input layer and get a vector representation of the input sequence in the next embedding layer. We find that adding an embedding layer can significantly improve performance when the vocabulary grows large. Then, we feed word embedding through two LM layers, get two word reading by STM and STM layer insertion and one STM layer insertion."}, {"heading": "4 Experiments", "text": "In this section, we conduct experiments to show that LSTM encoder decoder can perform 99 basic operations on sequence data. Intuitively, TS should include operations such as replacing difficult words with simpler words, removing redundant words, simplifying the syntax structure by changing the order of words, etc. In the following experiments, we show that a very simple LSTM encoder decoder model is able to reverse, sort and replace the elements of a sequence. We implement the LSTM encoder decoder in Keras [21]. The model contains two LSTM layers for both the encoder and the decoder, the output is fed into a Softmax layer. RMSprop [22], which updates its parameters using an impulse on the rescaled gradient, was used as an optimizer in our experiment, as it achieves the best performance compared to other optimization methods."}, {"heading": "4.1 Reverse", "text": "First, we conduct experiments to show that the LSTM encoder decoder can reverse a sequence after training on a large set of sequence pairs (X, Y), where X = (x1, x2,..., x25) Y = (x25, x24,..., x1) xi-VThe results are given in Table 1. V, H, E, which represent the vocabulary size, the number of hidden neurons in the LSTM layer and the training epoch or training epoch. The size of our training data is extremely important for the model. As shown in Table 1, the performance decreases significantly when we reduce the size of our training set from 135k to 9k. The size of the vocabulary also affects performance. A larger vocabulary requires more training data and more hidden neurons in the LSTM layers. By increasing the number of neurons in the LSTM layers from 128k to general neurons, we are able to train with a higher accuracy with 256 more hits."}, {"heading": "4.2 Sort", "text": "Although Neural ProgrammerInterpreters (NPI) [23], a current model, can display and execute programs such as sorting and adding, the LSTM encoder decoder decoder is much simpler and lighter compared to NPI. In the following experiment, we show that the LSTM encoder decoder decoder is able to sort a sequence of integers. Data sets consist of sequence pairs (X, Y), where X = (x1, x2,..., x25) Y = sorted (x1, x2,..., x25) xi-VWe show the results in Table 2. Likewise, the size of the vocabulary, training data and neurons influence performance. It is more difficult to train if we increase the vocabulary to 1000, but the model can still learn the sorting rule with high accuracy if enough training data is available."}, {"heading": "4.3 Replace", "text": "Next, we show that the LSTM encoder can replace words in a sequence. In the case of sequence pairs (X, Y), where X = (x1, x2,..., x25) Y = (y1, y2,..., y25) yi = xi mod nxi, yi, VWe n = 2, 20, 200 when | V | = 10, 100, 1000. We only keep the top 20 percent of the words in the vocabulary and use these words to replace all matching words in the output sequence. Similarly, with lexical simplification, where we can consider the top 20 percent of the words in the vocabulary as simple and common words, and all other words are complex words. Likewise, we can imagine the top 20 percent of the words as meaningful and important words, other words are redundant."}, {"heading": "4.4 Combine Three Operations", "text": "We have shown that the LSTM encoder decoder can work well separately in reverse, sorting, and replacement operations, but in reality a set is usually simplified by a complex combination of these three different rules. Therefore, we combine the three operations to see if this model can still detect the mapping rules between the sequences. So, the data are sequence pairs (X, Y), with Y obtained by Modulo for each index first in X, then sort, and reverse. Results are in Table 4. The LSTM encoder decoder continues to work very well as expected, and even as well as any of the operations alone. Therefore, the LSTM encoder decoder can easily detect mapping patterns of combined operations between sequences, and thus find potentially complicated simplification rules."}, {"heading": "5 Conclusion and Future Work", "text": "In summary, we note that the LSTM encoder decoder model is capable of learning operating rules such as inverting, sorting, and replacing sequence pairs, showing that the model can potentially apply rules such as changing sentence structure, replacing words, and removing words to simplify text. This is a preliminary experimental study to solve the text simplification problem using deep neural networks. However, unlike machine translation, there is very little online training to simplify text. Therefore, our future work involves collecting complex and simple sentence pairs from online resources such as Wikipedia and Simple English Wikipedia, as well as training our model using natural languages."}], "references": [{"title": "Text simplification using neural machine translation", "author": ["Tong Wang", "Ping Chen", "John Rochford", "Jipeng Qiang"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun  Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1997}, {"title": "Simple english wikipedia: a new text simplification task. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers- Volume 2, pages 665\u2013669", "author": ["William Coster", "David Kauchak"], "venue": "Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "A new evaluation measure using compression dissimilarity on text summarization", "author": ["Tong Wang", "Ping Chen", "Dan Simovici"], "venue": "Applied Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "arXiv preprint arXiv:1509.00685,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Simplifying lexical simplification: Do we need simplified corpora? page", "author": ["Goran Glava\u0161", "Sanja \u0160tajner"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Putting it simply: a context-  aware approach to lexical simplification. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 496\u2013501", "author": ["Or Biran", "Samuel Brody", "No\u00e9mie Elhadad"], "venue": "Association for Computational Linguistics,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "A monolingual treebased translation model for sentence simplification", "author": ["Zhemin Zhu", "Delphine Bernhard", "Iryna Gurevych"], "venue": "In Proceedings of the 23rd international conference on computational linguistics,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "Learning to simplify sentences with quasi-synchronous grammar and integer programming. In Proceedings of the conference on empirical methods in natural language processing, pages 409\u2013420", "author": ["Kristian Woodsend", "Mirella Lapata"], "venue": "Association for Computational Linguistics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2011}, {"title": "Prediction of long-lead heavy precipitation events aided by machine learning", "author": ["Yahui Di"], "venue": "In 2015 IEEE International Conference on Data Mining Workshop (ICDMW),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Extended topic model for word dependency", "author": ["Tong Wang", "Vish Viswanath", "Ping Chen"], "venue": "In Proceedings of the 53th Annual Meeting of the Associatioin for Computational Linguistics (ACL-2015, Short Papers),", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Compression and data mining", "author": ["Dan A Simovici", "Tong Wang", "Ping Chen", "Dan Pletea"], "venue": "In 2015 International Conference on Computing, Networking and Communications (ICNC),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Longlead term precipitation forecasting by hierarchical clustering-based bayesian structural vector autoregression", "author": ["Kaixun Hua", "Dan A Simovici"], "venue": "In 2016 IEEE 13th International Conference on Networking, Sensing, and Control (ICNSC),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "An evaluation of big data  analytics in feature selection for longlead extreme floods forecasting", "author": ["Yong Zhuang", "Kui Yu", "Dawei Wang", "Wei Ding"], "venue": "In 2016 IEEE 13th International Conference on Networking, Sensing, and Control (IC- NSC),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Fresh: Fair and efficient slot configuration and scheduling for hadoop clusters", "author": ["Jiayin Wang", "Yi Yao", "Ying Mao", "Bo Sheng", "Ningfang Mi"], "venue": "In Cloud Computing (CLOUD),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Cielo: An evolutionary game theoretic framework for virtual machine placement in clouds", "author": ["Yi Ren", "Jun Suzuki", "Athanasios Vasilakos", "Shingo Omura", "Katsuya Oba"], "venue": "In Future Internet of Things and Cloud (FiCloud),", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Lstm: A search space odyssey", "author": ["Klaus Greff", "Rupesh Kumar Srivastava", "Jan Kout\u0144\u0131k", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "venue": "arXiv preprint arXiv:1503.04069,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude", "author": ["Tijmen Tieleman", "Geoffrey Hinton"], "venue": "COURSERA: Neural Networks for Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "It is generally divided into three categories: lexical simplification (LS), rulebased, and machine translation (MT) [1].", "startOffset": 116, "endOffset": 119}, {"referenceID": 4, "context": "In this paper, we propose to apply Long Short-Term Memory (LSTM) [5] EncoderDecoder on TS task.", "startOffset": 65, "endOffset": 68}, {"referenceID": 5, "context": "Usually, hand-crafted, supervised, and unsupervised methods based on resources like English Wikipedia (EW) and Simple English Wikipedia (SEW) [6] are utilized for extracting simplification rules.", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "At the lexical level, [9] proposed an lexical simplification system which only requires a large corpus of regular text to obtain word embeddings to get words similar to the complex word.", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "[10] proposed an unsupervised method for learning pairs of complex and simpler synonyms and a context aware method for substituting one for the other.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "At the sentence level, [11] proposed a sentence simplification model by tree transformation based on Statistical Machine Translation (SMT).", "startOffset": 23, "endOffset": 27}, {"referenceID": 11, "context": "[12] presented a data-driven model based on a quasi-synchronous grammar, a formalism that can naturally capture structural mismatches and complex rewrite", "startOffset": 0, "endOffset": 4}, {"referenceID": 0, "context": "[1] proposed a preliminary work to use RNN Encoder-Decoder model for text simplification task, which is similar to the proposed model in this paper.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Recent works have proposed many modified LSTM models such as the gated recurrent unit (GRU) [3].", "startOffset": 92, "endOffset": 95}, {"referenceID": 19, "context": "However, [20] showed that none of the LSTM variants can improve upon the standard architecture significantly.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "[15, 27, 6, 18]\u21d2 [1, 2, 12, 15]", "startOffset": 0, "endOffset": 15}, {"referenceID": 5, "context": "[15, 27, 6, 18]\u21d2 [1, 2, 12, 15]", "startOffset": 0, "endOffset": 15}, {"referenceID": 17, "context": "[15, 27, 6, 18]\u21d2 [1, 2, 12, 15]", "startOffset": 0, "endOffset": 15}, {"referenceID": 0, "context": "[15, 27, 6, 18]\u21d2 [1, 2, 12, 15]", "startOffset": 17, "endOffset": 31}, {"referenceID": 1, "context": "[15, 27, 6, 18]\u21d2 [1, 2, 12, 15]", "startOffset": 17, "endOffset": 31}, {"referenceID": 11, "context": "[15, 27, 6, 18]\u21d2 [1, 2, 12, 15]", "startOffset": 17, "endOffset": 31}, {"referenceID": 14, "context": "[15, 27, 6, 18]\u21d2 [1, 2, 12, 15]", "startOffset": 17, "endOffset": 31}, {"referenceID": 20, "context": "RMSprop [22], which generates its parameter updates using a momentum on the rescaled gradient, was used as the optimizer in out experiment since it achieves the best performance compared to other optimization methods.", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Reverse: [15, 27, 6, 18, 99]\u21d2 [99, 18, 6, 27, 15]", "startOffset": 9, "endOffset": 28}, {"referenceID": 5, "context": "Reverse: [15, 27, 6, 18, 99]\u21d2 [99, 18, 6, 27, 15]", "startOffset": 9, "endOffset": 28}, {"referenceID": 17, "context": "Reverse: [15, 27, 6, 18, 99]\u21d2 [99, 18, 6, 27, 15]", "startOffset": 9, "endOffset": 28}, {"referenceID": 17, "context": "Reverse: [15, 27, 6, 18, 99]\u21d2 [99, 18, 6, 27, 15]", "startOffset": 30, "endOffset": 49}, {"referenceID": 5, "context": "Reverse: [15, 27, 6, 18, 99]\u21d2 [99, 18, 6, 27, 15]", "startOffset": 30, "endOffset": 49}, {"referenceID": 14, "context": "Reverse: [15, 27, 6, 18, 99]\u21d2 [99, 18, 6, 27, 15]", "startOffset": 30, "endOffset": 49}, {"referenceID": 14, "context": "Sort: [15, 27, 6, 18, 99]\u21d2 [6, 15, 18, 27, 99]", "startOffset": 6, "endOffset": 25}, {"referenceID": 5, "context": "Sort: [15, 27, 6, 18, 99]\u21d2 [6, 15, 18, 27, 99]", "startOffset": 6, "endOffset": 25}, {"referenceID": 17, "context": "Sort: [15, 27, 6, 18, 99]\u21d2 [6, 15, 18, 27, 99]", "startOffset": 6, "endOffset": 25}, {"referenceID": 5, "context": "Sort: [15, 27, 6, 18, 99]\u21d2 [6, 15, 18, 27, 99]", "startOffset": 27, "endOffset": 46}, {"referenceID": 14, "context": "Sort: [15, 27, 6, 18, 99]\u21d2 [6, 15, 18, 27, 99]", "startOffset": 27, "endOffset": 46}, {"referenceID": 17, "context": "Sort: [15, 27, 6, 18, 99]\u21d2 [6, 15, 18, 27, 99]", "startOffset": 27, "endOffset": 46}, {"referenceID": 14, "context": "Replace: [15, 27, 6, 18, 99]\u21d2 [15, 7, 6, 18, 19]", "startOffset": 9, "endOffset": 28}, {"referenceID": 5, "context": "Replace: [15, 27, 6, 18, 99]\u21d2 [15, 7, 6, 18, 19]", "startOffset": 9, "endOffset": 28}, {"referenceID": 17, "context": "Replace: [15, 27, 6, 18, 99]\u21d2 [15, 7, 6, 18, 19]", "startOffset": 9, "endOffset": 28}, {"referenceID": 14, "context": "Replace: [15, 27, 6, 18, 99]\u21d2 [15, 7, 6, 18, 19]", "startOffset": 30, "endOffset": 48}, {"referenceID": 6, "context": "Replace: [15, 27, 6, 18, 99]\u21d2 [15, 7, 6, 18, 19]", "startOffset": 30, "endOffset": 48}, {"referenceID": 5, "context": "Replace: [15, 27, 6, 18, 99]\u21d2 [15, 7, 6, 18, 19]", "startOffset": 30, "endOffset": 48}, {"referenceID": 17, "context": "Replace: [15, 27, 6, 18, 99]\u21d2 [15, 7, 6, 18, 19]", "startOffset": 30, "endOffset": 48}, {"referenceID": 18, "context": "Replace: [15, 27, 6, 18, 99]\u21d2 [15, 7, 6, 18, 19]", "startOffset": 30, "endOffset": 48}, {"referenceID": 14, "context": "Combine: [15, 27, 6, 18, 99]\u21d2 [19, 18, 15, 7, 6]", "startOffset": 9, "endOffset": 28}, {"referenceID": 5, "context": "Combine: [15, 27, 6, 18, 99]\u21d2 [19, 18, 15, 7, 6]", "startOffset": 9, "endOffset": 28}, {"referenceID": 17, "context": "Combine: [15, 27, 6, 18, 99]\u21d2 [19, 18, 15, 7, 6]", "startOffset": 9, "endOffset": 28}, {"referenceID": 18, "context": "Combine: [15, 27, 6, 18, 99]\u21d2 [19, 18, 15, 7, 6]", "startOffset": 30, "endOffset": 48}, {"referenceID": 17, "context": "Combine: [15, 27, 6, 18, 99]\u21d2 [19, 18, 15, 7, 6]", "startOffset": 30, "endOffset": 48}, {"referenceID": 14, "context": "Combine: [15, 27, 6, 18, 99]\u21d2 [19, 18, 15, 7, 6]", "startOffset": 30, "endOffset": 48}, {"referenceID": 6, "context": "Combine: [15, 27, 6, 18, 99]\u21d2 [19, 18, 15, 7, 6]", "startOffset": 30, "endOffset": 48}, {"referenceID": 5, "context": "Combine: [15, 27, 6, 18, 99]\u21d2 [19, 18, 15, 7, 6]", "startOffset": 30, "endOffset": 48}], "year": 2016, "abstractText": "Text simplification (TS) aims to reduce the lexical and structural complexity of a text, while still retaining the semantic meaning. Current automatic TS techniques are limited to either lexical-level applications or manually defining a large amount of rules. Since deep neural networks are powerful models that have achieved excellent performance over many difficult tasks, in this paper, we propose to use the Long Short-Term Memory (LSTM) Encoder-Decoder model for sentence level TS, which makes minimal assumptions about word sequence. We conduct preliminary experiments to find that the model is able to learn operation rules such as reversing, sorting and replacing from sequence pairs, which shows that the model may potentially discover and apply rules such as modifying sentence structure, substituting words, and removing words for TS.", "creator": "LaTeX with hyperref package"}}}