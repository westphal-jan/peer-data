{"id": "1609.03675", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Deep Coevolutionary Network: Embedding User and Item Features for Recommendation", "abstract": "Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multidimensional point process model. The RNN learns a nonlinear representation of user and item features which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning the model parameters, which can readily scale up to millions of events. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.", "histories": [["v1", "Tue, 13 Sep 2016 04:39:33 GMT  (1544kb,D)", "http://arxiv.org/abs/1609.03675v1", "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)"], ["v2", "Sat, 5 Nov 2016 00:25:39 GMT  (4215kb,D)", "http://arxiv.org/abs/1609.03675v2", "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)"], ["v3", "Wed, 9 Nov 2016 04:12:13 GMT  (4219kb,D)", "http://arxiv.org/abs/1609.03675v3", "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)"], ["v4", "Tue, 28 Feb 2017 05:37:37 GMT  (4653kb,D)", "http://arxiv.org/abs/1609.03675v4", "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)"]], "COMMENTS": "Recsys Workshop on Deep Learning for Recommendation Systems (DLRS '16)", "reviews": [], "SUBJECTS": "cs.LG cs.IR", "authors": ["hanjun dai", "yichen wang", "rakshit trivedi", "le song"], "accepted": false, "id": "1609.03675"}, "pdf": {"name": "1609.03675.pdf", "metadata": {"source": "CRF", "title": "Recurrent Coevolutionary Feature Embedding Processes for Recommendation", "authors": ["Hanjun Dai", "Yichen Wang", "Rakshit Trivedi", "Le Song"], "emails": ["rstrivedi}@gatech.edu,", "lsong@cc.gatech.edu"], "sections": [{"heading": "1. INTRODUCTION", "text": "This year, we have reached a point where it can only take one year to reach an agreement."}, {"heading": "2. RELATED WORK", "text": "Recent work mainly fixes the latent properties assigned to each user and object [3, 4, 5, 6, 7, 8, 9, 10]. In more complex methods, time is divided into epochs, and static latent attribute models are applied to each epoch to capture some temporal aspects of the data [11, 12, 12, 13, 14, 15, 16, 17, 18, 19, 20. For such methods, it is not clear how to select the epoch length parameters. Firstly, different users may have very different time scales when interacting with these service elements, making it difficult to choose a uniform epoch length. Secondly, it is not easy for these methods to answer time-sensitive questions, such as when a user returns to the object of service. Predictions refer only to the resolution of the selected epoch length."}, {"heading": "3. BACKGROUND ON TEMPORAL POINT PROCESSES", "text": "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti-R + and i-Z +. Likewise, a given temporal point process can be represented as a counting process, N (t), which records the number of events before time. An important method for characterizing temporal point processes is via the conditional intensity function \u03bb (t), a stochastic model for the time of the next event that has given all previous events. Formally, \u03bb (t) dt is the conditional probability of observing an event in a small window [t, t + dt) given the history H (t) to t and that the event did not occur before t, i.e., \u03bb (t) dt: = P {event in [t, t + dt) | H (t)}} = E [dN (t) | H (t) \u2212 t), typically assuming that the probability (dt) is that an event (i.e., a small window) is n n n (t), i.e., > that the probability (dt) that an event (i.e., a small one in size is n n (n)."}, {"heading": "4. RECURRENT COEVOLUTIONARY FEATURE EMBEDDING PROCESSES", "text": "In this section, we present the generative framework for modeling the temporal dynamics of user-item interactions. First, we explicitly capture the coevolutionary nature of the latent characteristic of user and item. Then, based on the compatibility between the latent characteristic of the user and the item, we model the user-item interactions through a time-point process and parameterise the intensity function through compatibility."}, {"heading": "4.1 Event representation", "text": "Given m user and n items, we refer to the ordered list of N observed events as O = {ej = (uj, ij, tj, qj)} Nj = 1 in the time window [0, T], where t1 6... 6 N. Each event is modeled as tuples (uj, ij, tj, qj), where uj,..., m}, ij, n}, tj and R +, meaning that the interaction between user uj, item ij in due time tj, with the interaction context qj, Rd. Here, qj can be a high-dimensional vector, such as the text overview, or simply the embedding of static user / item characteristics such as the user profile and the categorical characteristics of the item. To simplify the notation, we define \u2022 Ou = (iuj, tuj, quj)} | Ou = 1 as the ordered time that is also related to all items."}, {"heading": "4.2 Recurrent feature embedding processes", "text": "In fact, it is such that it is a matter of a way in which people move in the world between time levels, in which they move, in which they move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they"}, {"heading": "4.3 User-item interactions as temporal point processes", "text": "For each user, we model the recurrent occurrence of interaction between users and objects \u2022 Short-term interaction with all objects as a multidimensional temporal process, with each element considered as a dimension. Specifically, the intensity in the i-dimension (item i) is modeled as a Rayleigh process: \"I (t \u2212 t0) = exp (Uu (t \u2212) > Ii (t \u2212))).\" The rationality behind this formulation is threefold: \"time as a random variable.\" Instead of discrediting time in epochs as in traditional mehtods [16, 17, 18, 19, 20] we will explicitly model the timing of each interaction event as a random variable. \"We model the heterogeneity of temporal interactions between users and objects.\" \"We model the timing of each interaction event as a random variable.\""}, {"heading": "5. PARAMETER LEARNING", "text": "After we have presented the model in this section, we propose an efficient algorithm to learn the individual parameters. Although we have presented the objective function in Equation 6, we try to apply the different methods to learn the embedding parameters. (BPTT) It is the standard way to train an RNN. (To make the backward exploitation more tractable, one typically has to change the flow data during the training, where one can easily split the sequences into several segments to make the BPTT tractable, we are related to all events, which makes it difficult to decompose all events. To do this, we first have to do all events globally and then mini-training."}, {"heading": "6. EXPERIMENTS", "text": "For each sequence of user activities, we use all events up to the time of T \u00b7 p as training data and the remaining events as test data, with T being the observation window. We report the results for two tasks: \u2022 Item Prediction. At each test point, we predict which user will interact with. We rate all Jacob1: 45pmSophieJacobSophie15: 45pm 17: 00pm 21: 00pm 22: 30pm3: 30pm3: 15pm2: 30s 16: 25pm9: 45pm10: 00pm8: 15pmMini-Batch 1 Mini-Batch 2 (user, forum)"}, {"heading": "6.1 Competitors", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2 Datasets", "text": "We use three sets of data from the real world: IPTV. It contains 7,100 user reports from 385 television programs in 11 months (January 1 - November 30, 2012) with approximately 2M events and 1,420 movie features (including 1,073 actors, 312 directors, 22,278 genres, 8 countries and 5 years).Yelp. This data was available in the 7th round of the Yelp dataset. It contains ratings for various companies from October 2004 to December 2015. Of the available 552K users, we used users with more than 100 contributions for our experiments. We cleaned up the review text by removing stopwords and punctuation marks and only words of length > 3 and frequency > 10. After this preprocessing, the dataset comprised 1,503 users, 47,924 groups (companies) and 34,508 text functions with a total of 2,92,000 reviews and interrupted records. To be able to compare with baseline datasets, we reduced the total size of this dataset and used 95,000 data sets."}, {"heading": "6.3 Results", "text": "The best MAR you can achieve is 1, but both our method and LowRankHawkes have achieved fairly accurate results. In terms of the MAR metric, the performance is also slightly better than LowRankHawkes. Because you only need the rank of conditional density f to perform the item prediction, LowRankHawkes may still be good at distinguishing f, but could not accurately determine the actual value of f, as shown in the time prediction task, where the value of f is needed for an accurate prediction. Time prediction R-coevolve clearly outperforms other methods in time prediction. Compared with LowRankHawkes, for example, it has 2 x time improvement on Yelp, 6 x improvement on Reddit, and 30 x improvement on IPTV. The unit of time is not essential in capturing items within two weeks of intuitive Items and interaction recommendations on TV."}, {"heading": "7. CONCLUSION", "text": "This is a generative model for modeling and understanding the user's online behavior that differs from previous work that focused solely on the predictive task in the recommendation system. In addition, the evolving and jointly evolving processes of the user and the article are captured by the RNN. We demonstrate the superior performance of our method in the predictive task that is not possible by most previous work. Future work will include extending to other applications such as modeling the dynamics of social news groups and understanding the behavior of people on Q & A pages. Recognition This project was partially supported by NSF / NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF IIS-1639792, NSF IIS-1218749, NSF CAREER IIS-1350983, Intel and NVIA."}, {"heading": "8. REFERENCES", "text": "[1] Nan Du, Yichen Wang, Niao He, and Le Song Timesensitive recommendations from recurrent user activities. In NIPS, 2015. [2] Thomas Josef Liniger. Multivariate Hawkes Processes. PhD thesis, Swiss Federal Institute of Technology Zurich, 2009. [3] R. Salakhutdinov and A. Mnih. Bayesian probabilistic matrix factorization using markov chain monte carlo.In W.W. Cohen, A. McCallum, and S.T. Roweis, Editors, ICML, volume 307, S. 880-887. ACM, 2008. [4] Y. Chen, D. Pavlov, and J.F. Canny. Large-scale behavioral targeting. In J.F. Elder, F. Fogelman-Soulie, volume 307, S. 880-887. ACM."}], "references": [{"title": "Time sensitive recommendation from recurrent user activities", "author": ["Nan Du", "Yichen Wang", "Niao He", "Le Song"], "venue": "In NIPS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multivariate Hawkes Processes", "author": ["Thomas Josef Liniger"], "venue": "PhD thesis, Swiss Federal Institute of Technology Zurich,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Bayesian probabilistic matrix factorization using markov chain monte carlo", "author": ["R. Salakhutdinov", "A. Mnih"], "venue": " In W.W. Cohen, A. McCallum, and S.T. Roweis, editors, ICML, volume 307, pages 880\u2013887. ACM", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Large-scale behavioral targeting", "author": ["Y. Chen", "D. Pavlov", "J.F. Canny"], "venue": "J.F. Elder, F. Fogelman-Souli\u00e9, P.A. Flach, and M. J. Zaki, editors, KDD, pages 209\u2013218. ACM", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Regression-based latent factor models", "author": ["D. Agarwal", "B.-C. Chen"], "venue": "J.F. Elder, F. Fogelman-Souli\u00e9, P.A. Flach, and M.J. Zaki, editors, KDD, pages 19\u201328. ACM", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Collaborative filtering recommender systems", "author": ["Michael D Ekstrand", "John T Riedl", "Joseph A Konstan"], "venue": "Foundations and Trends in Human-Computer Interaction,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Ordrec: an ordinal model for predicting personalized item rating distributions", "author": ["Yehuda Koren", "Joe Sill"], "venue": "In RecSys,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Like like alike: joint friendship and interest propagation in social networks", "author": ["Shuang-Hong Yang", "Bo Long", "Alex Smola", "Narayanan Sadagopan", "Zhaohui Zheng", "Hongyuan Zha"], "venue": "In WWW,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Beyond clicks: Dwell time for personalization", "author": ["Xing Yi", "Liangjie Hong", "Erheng Zhong", "Nanthan Nan Liu", "Suju Rajan"], "venue": "In RecSys,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Detecting emotions in social media: A constrained optimization approach", "author": ["Yichen Wang", "Aditya Pal"], "venue": "In IJCAI,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "Collaborative filtering with temporal dynamics", "author": ["Y. Koren"], "venue": "KDD", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Multiverse recommendation: n-dimensional tensor factorization for context-aware collaborative filtering", "author": ["Alexandros Karatzoglou", "Xavier Amatriain", "Linas Baltrunas", "Nuria Oliver"], "venue": "In Recsys,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Temporal collaborative filtering with bayesian probabilistic tensor factorization", "author": ["Liang Xiong", "Xi Chen", "Tzu-Kuo Huang", "Jeff G. Schneider", "Jaime G. Carbonell"], "venue": "In SDM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "On tensors, sparsity, and nonnegative factorizations", "author": ["Eric C Chi", "Tamara G Kolda"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "A collaborative kalman filter for time-evolving dyadic processes", "author": ["San Gultekin", "John Paisley"], "venue": "In ICDM,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Dynamic poisson factorization", "author": ["Laurent Charlin", "Rajesh Ranganath", "James McInerney", "David M Blei"], "venue": "In RecSys,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Who, what, when, and where: Multi-dimensional collaborative recommendations using tensor factorization on sparse user-generated data", "author": ["Thomas Phan"], "venue": "In WWW,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Scalable recommendation with hierarchical poisson factorization", "author": ["Prem Gopalan", "Jake M Hofman", "David M Blei"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "General factorization framework for context-aware recommendations", "author": ["Bal\u00e1zs Hidasi", "Domonkos Tikk"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Recommending groups to users using user-group engagement and time-dependent matrix factorization", "author": ["Xin Wang", "Roger Donaldson", "Christopher Nell", "Peter  Gorniak", "Martin Ester", "Jiajun Bu"], "venue": "In AAAI,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Collaborative deep learning for recommender systems", "author": ["Hao Wang", "Naiyan Wang", "Dit-Yan Yeung"], "venue": "In KDD. ACM,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Session-based recommendations with recurrent neural networks", "author": ["Balazs Hidasi", "Alexandros Karatzoglou", "Linas Baltrunas", "Domonkos Tikk"], "venue": "In Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2016}, {"title": "Recurrent marked temporal point processes: Embedding event history to vector", "author": ["Nan Du", "Hanjun Dai", "Rakshit Trivedi", "Utkarsh Upadhyay", "Manuel Gomez-Rodriguez", "Le Song"], "venue": "In KDD", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Point processes", "author": ["D.R. Cox", "V. Isham"], "venue": "volume 12. Chapman & Hall/CRC", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1980}, {"title": "Multivariate point processes", "author": ["D.R. Cox", "P.A.W. Lewis"], "venue": "Selected Statistical Papers of Sir David Cox: Volume 1, Design of Investigations, Statistical Methods and Applications, 1:159", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Survival and event history analysis: a process point of view", "author": ["Odd Aalen", "Ornulf Borgan", "Hakon Gjessing"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Spectra of some self-exciting and mutually exciting point processes", "author": ["Alan G Hawkes"], "venue": null, "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1971}, {"title": "Isotonic hawkes processes", "author": ["Yichen Wang", "Bo Xie", "Nan Du", "Le Song"], "venue": "In ICML,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Uncovering the temporal dynamics of diffusion networks", "author": ["Manuel Gomez-Rodriguez", "David Balduzzi", "Bernhard Sch\u00f6lkopf"], "venue": "In Proceedings of the International Conference on Machine Learning,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "An introduction to the theory of point processes: volume II: general theory and structure", "author": ["D.J. Daley", "D. Vere-Jones"], "venue": "volume 2. Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}, {"title": "Discriminative embeddings of latent variable models for structured data", "author": ["Hanjun Dai", "Bo Dai", "Le Song"], "venue": "In ICML,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2016}, {"title": "Rubik: Knowledge guided tensor factorization and completion for health data analytics", "author": ["Yichen Wang", "Robert Chen", "Joydeep Ghosh", "Joshua C Denny", "Abel Kho", "You Chen", "Bradley A Malin", "Jimeng Sun"], "venue": "In KDD,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Just in time recommendations: Modeling the dynamics of boredom in activity streams", "author": ["Komal Kapoor", "Karthik Subbian", "Jaideep Srivastava", "Paul Schrater"], "venue": "In WSDM,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Recent point process based models treat time as a random variable and improves over the traditional methods significantly [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "How can we obtain a more expressive model to capture the co-evolution features of user-item interactions, and learn such a model from large volume of data? To tackle this challenge, in this paper, we combine recurrent neural network (RNN) with multivariate point process models [2], and propose a recurrent coevolutionary feature embedding process framework.", "startOffset": 278, "endOffset": 281}, {"referenceID": 2, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 3, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 4, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 5, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 6, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 7, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 8, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 9, "context": "Recent work predominantly fix the latent features assigned to each user and item [3, 4, 5, 6, 7, 8, 9, 10].", "startOffset": 81, "endOffset": 106}, {"referenceID": 10, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 11, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 12, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 11, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 12, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 13, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 14, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 15, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 16, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 17, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 18, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 19, "context": "In more sophisticated methods, the time is divided into epochs, and static latent feature models are applied to each epoch to capture some temporal aspects of the data [11, 12, 13, 12, 13, 14, 15, 16, 17, 18, 19, 20].", "startOffset": 168, "endOffset": 216}, {"referenceID": 0, "context": "Recently, [1] proposed a low-rank point process based model for time- sensitive recommendations from recurrent user activities.", "startOffset": 10, "endOffset": 13}, {"referenceID": 20, "context": "In the deep learning community, [21] proposed collaborative deep learning, a hierarchical Bayesian model that jointly performs learning for the content features and collaborative filtering for the ratings matrix.", "startOffset": 32, "endOffset": 36}, {"referenceID": 21, "context": "[22] applied recurrent neural network based approach to recommender systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "Finally, our work is inspired from newly proposed recurrent marked temporal point process framework [23] that builds a connection between RNN and Point Processes.", "startOffset": 100, "endOffset": 104}, {"referenceID": 22, "context": "However, [23] focuses on the task of next event prediction given a sequence of past events for an entity and is only designed for one-dimension point process.", "startOffset": 9, "endOffset": 13}, {"referenceID": 23, "context": "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti \u2208 R and i \u2208 Z.", "startOffset": 25, "endOffset": 33}, {"referenceID": 24, "context": "A temporal point process [24, 25] is a random process whose realization consists of a list of discrete events localized in time, {ti} with ti \u2208 R and i \u2208 Z.", "startOffset": 25, "endOffset": 33}, {"referenceID": 25, "context": "Then, given a time t > 0, we can also characterize the conditional probability that no event happens during [0, t) as [26]:", "startOffset": 118, "endOffset": 122}, {"referenceID": 26, "context": "\u2022 Hawkes processes [27, 28], whose intensity models the excitation between events, i.", "startOffset": 19, "endOffset": 27}, {"referenceID": 27, "context": "\u2022 Hawkes processes [27, 28], whose intensity models the excitation between events, i.", "startOffset": 19, "endOffset": 27}, {"referenceID": 15, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 16, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 17, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 18, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 19, "context": "Instead of discretizing the time into epochs as in traditional mehtods [16, 17, 18, 19, 20], we explicitly model the timing of each interaction event as a random variable, which naturally captures the heterogeneity of the temporal interactions between users and items.", "startOffset": 71, "endOffset": 91}, {"referenceID": 28, "context": "This formulation assumes a Rayleigh distribution for the time intervals between consecutive events in each dimension [29].", "startOffset": 117, "endOffset": 121}, {"referenceID": 29, "context": "The joint negative log-likelihood is [30]:", "startOffset": 37, "endOffset": 41}, {"referenceID": 30, "context": "The Adam Optimizer [31] is used in our experiment, since it has shown good performance in training RNNs.", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "To make the learning efficient, we use the graph embedding framework [32] which allows training deep learning models where each term in the objective has a different computational graphs but with shared parameters.", "startOffset": 69, "endOffset": 73}, {"referenceID": 13, "context": "We compared our method to the following algorithms: \u2022 PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "We compared our method to the following algorithms: \u2022 PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.", "startOffset": 185, "endOffset": 197}, {"referenceID": 12, "context": "We compared our method to the following algorithms: \u2022 PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.", "startOffset": 185, "endOffset": 197}, {"referenceID": 32, "context": "We compared our method to the following algorithms: \u2022 PoissonTensor [14]: Poisson Tensor Factorization has been shown to perform better than factorization methods based on squared loss [12, 13, 33] on recommendation tasks.", "startOffset": 185, "endOffset": 197}, {"referenceID": 0, "context": "\u2022 LowRankHawkes [1]: This is a low rank point process based model which assumes user-item interactions to be independent of each other and does not capture the co-evolution of user and item features.", "startOffset": 16, "endOffset": 19}, {"referenceID": 33, "context": "\u2022 STIC [34]: it fits a semi-hidden markov model to each observed user-item pair and is only designed for time prediction.", "startOffset": 7, "endOffset": 11}, {"referenceID": 10, "context": "\u2022 TimeSVD++ [11] and FIP [8]: These two methods are only designed for explicit ratings, the implicit user feedbacks (in the form of a series of interaction events) are converted into the explicit ratings by the respective frequency of interactions with users.", "startOffset": 12, "endOffset": 16}, {"referenceID": 7, "context": "\u2022 TimeSVD++ [11] and FIP [8]: These two methods are only designed for explicit ratings, the implicit user feedbacks (in the form of a series of interaction events) are converted into the explicit ratings by the respective frequency of interactions with users.", "startOffset": 25, "endOffset": 28}], "year": 2016, "abstractText": "Recommender systems often use latent features to explain the behaviors of users and capture the properties of items. As users interact with different items over time, user and item features can influence each other, evolve and co-evolve over time. To accurately capture the fine grained nonlinear coevolution of these features, we propose a recurrent coevolutionary feature embedding process model, which combines recurrent neural network (RNN) with a multidimensional point process model. The RNN learns a nonlinear representation of user and item features which take into account mutual influence between user and item features, and the feature evolution over time. We also develop an efficient stochastic gradient algorithm for learning the model parameters, which can readily scale up to millions of events. Experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.", "creator": "LaTeX with hyperref package"}}}