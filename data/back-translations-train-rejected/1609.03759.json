{"id": "1609.03759", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "3D Simulation for Robot Arm Control with Deep Q-Learning", "abstract": "Intelligent control of robotic arms has huge potential over the coming years, but as of now will often fail to adapt when presented with new and unfamiliar environments. Recent trends to solve this problem have seen a shift to end-to-end solutions using deep reinforcement learning to learn policies from visual input, rather than relying on a handcrafted, modular pipeline. Building upon the recent success of deep Q-networks, we present an approach which uses three-dimensional simulations to train a 7-DOF robotic arm in a robot arm control task without any prior knowledge. Policies accept images of the environment as input and output motor actions. However, the high-dimensionality of the policies as well as the large state space makes policy search difficult. This is overcome by ensuring interesting states are explored via intermediate rewards that guide the policy towards higher reward states. Our results demonstrate that deep Q-networks can be used to learn policies for a task that involves locating a cube, grasping, and then finally lifting. The agent is able to learn to deal with a range of starting joint configurations and starting cube positions when tested in simulation. Moreover, we show that policies trained via simulation have the potential to be directly applied to real-world equivalents without any further training. We believe that robot simulations can decrease the dependency on physical robots and ultimately improve productivity of training robot control tasks.", "histories": [["v1", "Tue, 13 Sep 2016 10:40:24 GMT  (2177kb,D)", "https://arxiv.org/abs/1609.03759v1", null], ["v2", "Tue, 13 Dec 2016 16:09:17 GMT  (1214kb,D)", "http://arxiv.org/abs/1609.03759v2", "In NIPS 2016 Workshop: Deep Learning for Action and Interaction (this https URL)"]], "reviews": [], "SUBJECTS": "cs.RO cs.CV cs.LG", "authors": ["stephen james", "edward johns"], "accepted": false, "id": "1609.03759"}, "pdf": {"name": "1609.03759.pdf", "metadata": {"source": "CRF", "title": "3D Simulation for Robot Arm Control with Deep Q-Learning", "authors": ["Stephen James"], "emails": ["slj12@imperial.ac.uk", "e.johns@imperial.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "Traditionally, robot arm control has been solved by craftsmanship solutions in modular ways, such as: 3D reconstruction, scene segmentation, object recognition, object pose estimation, robot pose estimation, and finally path planning. However, this approach can lead to loss of information between individual modules, leading to a accumulation of errors, and it is not flexible to learn a number of tasks due to the need for prior knowledge. A current trend is for robot arm control to be learned directly from image pixels, in an end-to-end manner that uses deep amplification of learning. This harnesses the power of deep learning with its ability to learn complex functions from raw data, and avoids the need for hand-engineering cumbersome and complex modules. However, applying deep reinforcement learning to robot control is a challenge, as large amounts of training data are required to deal with the high-dimensional search of policy and government space."}, {"heading": "2 Related work", "text": "Recently, state-of-the-art results have been achieved with Deep Q Networks (DQN) [7], a novel variant of Q-Learning that is used to play 49 Atari 2600 games using only pixels as input, a method designed for use with discrete action spaces and adopted for our work. However, recent expansions have adapted Deep Q-Learning for the continuous range [15], enabling high-dimensional action spaces such as those in physical control tasks. Recently, a profound enhancement of learning has also been used in robot simulation to learn visually controlled robot arm controllers via Convolutionary Neural Networks [1, 3, 4]. Most notable is the guided political search [2, 1] which has tasks such as block insertion into a shape that sorts cubes and screams on a bottle."}, {"heading": "3 Approach", "text": "Our approach uses a DQN [7] that corresponds to the optimal action value (or Q) function for a given input (Q) (Q). \"Q\" (s, a) = max. \u03c0 E\u03c0 (s, b). \"(1) Here, Q\" (s, a) defines the q value of the action a and state s. \"A policy determines what action to take in each state - and is typically the action associated with the highest q value for that state. Q\" (s, a) is then the optimal policy, defined as the one that has the highest expected cumulative reward when this policy is pursued, over any time step between the initial state and some termination criteria (or infinite time). The discount factor gives weight to short-term rewards and ensures the convergence of equation 1.The Q Learning Update rule uses the following loss function on the iteration i: Li (s, a, s, s, s, s, s, s, e). \""}, {"heading": "4 Experiments and results", "text": "This year it is more than ever before."}, {"heading": "5 Conclusions", "text": "The experiments and results in this paper are an investigation into the training of the robot arm control using deep Q-learning using 3D simulation. While the experimental setup in our case is simple, the expansion to more complex and useful tasks can now be considered as one of scaling the training data and increasing the accuracy of graphics and physics simulation. Given the scalability of simulation compared to the real world training normally performed elsewhere, the range of possible tasks that could be learned today is significantly greater than before, along with the ability to generalize to scenes with new layouts, colors, and textures. This essay provides a solid foundation on which highly scalable, end-to-end training of complex robot arm control can now be fully investigated."}], "references": [{"title": "End-to-end training of deep visuomotor policies.", "author": ["Sergey Levine"], "venue": "Journal of Machine Learning Research", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Guided Policy Search.", "author": ["Sergey Levine", "Vladlen Koltun"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Deep Spatial Autoencoders for Visuomotor Learning.\" ICRA 2016", "author": ["Chelsea Finn"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Learning contact-rich manipulation skills with guided policy search.", "author": ["Sergey Levine", "Nolan Wagener", "Pieter Abbeel"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning Hand-Eye Coordination for Robotic Grasping with Deep Learning and Large-Scale Data Collection.\" ISER 2016", "author": ["Sergey Levine"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control.", "author": ["Fangyi Zhang"], "venue": "ARAA", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Human-level control through deep reinforcement learning.", "author": ["Volodymyr Mnih"], "venue": "Nature", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Supersizing self-supervision: Learning to grasp from 50k tries and 700 robot hours.", "author": ["Lerrel Pinto", "Abhinav Gupta"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Deep Reinforcement Learning for Robotic Manipulation.\" arXiv preprint 2016", "author": ["Shixiang Gu"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Deep Visual Foresight for Planning Robot Motion.\" arXiv preprint 2016", "author": ["Chelsea Finn", "Sergey Levine"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Pairwise Decomposition of Image Sequences for Active Multi-View Recognition.", "author": ["Edward Johns", "Stefan Leutenegger", "Andrew J. Davison"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Deep Learning a Grasp Function for Grasping under Gripper Pose Uncertainty.\" IROS 2016", "author": ["Edward Johns", "Stefan Leutenegger", "Andrew J. Davison"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Towards Adapting Deep Visuomotor Representations from Simulated to Real Environments.\" WARF 2016", "author": ["Eric Tzeng"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Sim-to-Real Robot Learning from Pixels with Progressive Nets.\" arXiv preprint 2016", "author": ["Andrei A. Rusu"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Continuous control with deep reinforcement learning.\" ICLR 2016", "author": ["Timothy P. Lillicrap"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Understanding real world indoor scenes with synthetic data.\" CVPR 2016", "author": ["Ankur Handa"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes.", "author": ["German Ros"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Progressive neural networks.\" arXiv preprint 2016", "author": ["Andrei A. Rusu"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2016}, {"title": "Adam: A method for stochastic optimization.", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "ICLR 2015", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}], "referenceMentions": [{"referenceID": 6, "context": "Building upon the recent success of deep Q-networks (DQNs) [7], we present an approach that uses deep Q-learning and 3D simulations to train a 7-DOF robotic arm in a robot control task in an end-to-end manner, without any prior knowledge.", "startOffset": 59, "endOffset": 62}, {"referenceID": 6, "context": "Recently, state-of-the-art results have been achieved with deep Q-networks (DQN) [7], a novel variant of Q-learning that is used to play 49 Atari-2600 games using only pixels as input.", "startOffset": 81, "endOffset": 84}, {"referenceID": 14, "context": "However, recent extensions adapted deep Q-learning for the continuous domain [15], allowing high dimensional action spaces such as that in physical control tasks.", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "For robot manipulation, recent work has also seen deep reinforcement learning used to learn visuallyguided robot arm controllers via convolutional neural networks [1, 3, 4].", "startOffset": 163, "endOffset": 172}, {"referenceID": 2, "context": "For robot manipulation, recent work has also seen deep reinforcement learning used to learn visuallyguided robot arm controllers via convolutional neural networks [1, 3, 4].", "startOffset": 163, "endOffset": 172}, {"referenceID": 3, "context": "For robot manipulation, recent work has also seen deep reinforcement learning used to learn visuallyguided robot arm controllers via convolutional neural networks [1, 3, 4].", "startOffset": 163, "endOffset": 172}, {"referenceID": 1, "context": "Most notably, guided policy search [2, 1] has accomplished tasks such as block insertion into a shape sorting cube and screwing on a bottle cap, by using trajectory optimization to direct policy learning.", "startOffset": 35, "endOffset": 41}, {"referenceID": 0, "context": "Most notably, guided policy search [2, 1] has accomplished tasks such as block insertion into a shape sorting cube and screwing on a bottle cap, by using trajectory optimization to direct policy learning.", "startOffset": 35, "endOffset": 41}, {"referenceID": 7, "context": "An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.", "startOffset": 63, "endOffset": 76}, {"referenceID": 4, "context": "An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.", "startOffset": 63, "endOffset": 76}, {"referenceID": 8, "context": "An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.", "startOffset": 63, "endOffset": 76}, {"referenceID": 9, "context": "An alternative approach is to use large-scale self-supervision [8, 5, 9, 10], but in practice this is limited due to the expense of acquiring many robots for learning in parallel, or the time consuming nature if only a single robot is available.", "startOffset": 63, "endOffset": 76}, {"referenceID": 10, "context": "Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17].", "startOffset": 109, "endOffset": 113}, {"referenceID": 15, "context": "Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17].", "startOffset": 137, "endOffset": 141}, {"referenceID": 11, "context": "Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17].", "startOffset": 158, "endOffset": 162}, {"referenceID": 16, "context": "Simulation has been used for a number of tasks in computer vision and robotics, including object recognition [11], semantic segmentation [16], robot grasping [12], and autonomous driving [17].", "startOffset": 187, "endOffset": 191}, {"referenceID": 5, "context": "For robot arm control, it was recently shown that policies could be learned in simulation for a 2D target reaching task [6], but this failed to show any feasibility of transferring to the real world.", "startOffset": 120, "endOffset": 123}, {"referenceID": 12, "context": "To address this issue of transfer learning, a cross-domain loss was proposed in [13] to incorporate both simulated and real-world data within the same loss function.", "startOffset": 80, "endOffset": 84}, {"referenceID": 17, "context": "An alternative approach has made use of progressive neural networks [18], which ensure that information from simulation is not forgotten when further training is carried out on a real robot [14].", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "An alternative approach has made use of progressive neural networks [18], which ensure that information from simulation is not forgotten when further training is carried out on a real robot [14].", "startOffset": 190, "endOffset": 194}, {"referenceID": 6, "context": "Our approach uses a DQN [7] which approximates the optimal action-value (or Q) function for a given input: Q\u2217(s, a) = max \u03c0 E\u03c0( \u221e \u2211", "startOffset": 24, "endOffset": 27}, {"referenceID": 18, "context": "Gradient descent is performed using the Adam optimisation algorithm [19] with the learning rate set at \u03b1 = 6\u00d7 10\u22126.", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "Recent trends in robot arm control have seen a shift towards end-to-end solutions, using deep reinforcement learning to learn a controller directly from raw sensor data, rather than relying on a hand-crafted, modular pipeline. However, the high dimensionality of the state space often means that it is impractical to generate sufficient training data with real-world experiments. As an alternative solution, we propose to learn a robot controller in simulation, with the potential of then transferring this to a real robot. Building upon the recent success of deep Q-networks, we present an approach which uses 3D simulations to train a 7-DOF robotic arm in a control task without any prior knowledge. The controller accepts images of the environment as its only input, and outputs motor actions for the task of locating and grasping a cube, over a range of initial configurations. To encourage efficient learning, a structured reward function is designed with intermediate rewards. We also present preliminary results in direct transfer of policies over to a real robot, without any further training.", "creator": "LaTeX with hyperref package"}}}