{"id": "1609.03777", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural Networks", "abstract": "Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling unseen words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different clock rates. Despite the multi-clock structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than Kneser-Ney (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%.", "histories": [["v1", "Tue, 13 Sep 2016 11:41:48 GMT  (174kb)", "http://arxiv.org/abs/1609.03777v1", "Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016) on May 20, 2016"], ["v2", "Thu, 2 Feb 2017 13:49:41 GMT  (178kb)", "http://arxiv.org/abs/1609.03777v2", "Submitted to NIPS 2016 on May 20, 2016 (v1), accepted to ICASSP 2017 (v2)"]], "COMMENTS": "Submitted to 29th Conference on Neural Information Processing Systems (NIPS 2016) on May 20, 2016", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["kyuyeon hwang", "wonyong sung"], "accepted": false, "id": "1609.03777"}, "pdf": {"name": "1609.03777.pdf", "metadata": {"source": "CRF", "title": "Character-Level Language Modeling with Hierarchical Recurrent Neural Networks", "authors": ["Kyuyeon Hwang"], "emails": ["kyuyeon.hwang@gmail.com;", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.03 777v 1 [cs.L G] 13 SE"}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to"}, {"heading": "2 Related work", "text": "One of the most successful approaches is to encode the arbitrary character sequence into a fixed dimensional vector called word embedding and feed that vector to the word level RNN LMs. However, Kim et al. (2015) used Convolutionary Neural Networks (CNNs) to generate word embedding, and reached the state of the art on the Penn Treebank Corpus (Marcus et al., 1993). The similar CNN-based embedding approach is used by Jozefowicz et al. (2016) with very large LSTM on the One Billion Word Benchmark (Chelba et al., 2013), also reaching the state of the art perplexity. In Ling et al. (2015a), bidirectional LSTMs are used instead of CNNs for word embedding."}, {"heading": "3 Character-level language modeling with RNNs", "text": "Character language models (CLMs) must take into account a longer sequence of history symbols to predict the next character than word-level language models (WLMs) because the number of symbols is smaller. Therefore, traditional N-gram models cannot be used for CLMs. Thanks to recent advances in RNNs, RNN-based CLMs are now performing satisfactorily (Sutskever et al., 2011; Hermans & Schrauwen, 2013). Specifically, short-term deep memory (LSTM) (Hochreiter & Schmidhuber, 1997) based on CLMs are performing well and are successfully applied to the end-to-end speech recognition system (Hwang & Sung, 2016). To train RNN-CLMs, training data should first be converted into the sequence of standardized character vectors, xt, in which the characters word boundary symbols, < w > or space, and, if applicable, boundary symbols."}, {"heading": "4 RNNs with external clock and reset signals", "text": "In this section we generalize the existing RNN structures and extend them with external clocks and reset signals. Extended models become the basic building blocks of hierarchical RNNs. Most types of RNNs can be generalized \u00dft = f (xt, st \u2212 1) (1) yt = g (st) (2), where xt is the input, st is the state, yt is the output in time step t, f \u2212 rt \u2212 is the activation of the recurrence function, and g (\u00b7) is the output function. For example, Elman networks can be represented ht = ht = ht (Whhht \u2212 1 + bh) (3) yt = ht (4) \u2212 rt is the activation of the hidden layer \u2212 rt function, g (\u00b7) is the output function, Whx and Whh are the weight matrices and bh is the bias vector. LSTMs (Hochreiter & Schmidhuer 1997, with previous Gepers and Gepers) are equal (2000)."}, {"heading": "5 Character-level LM with a hierarchical RNN", "text": "rrrrrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrlrteeteeteerrrrrrlrlrlrrlrlrlrlrlrlrrrrrlrlrrrlrrlrrrlrlrlrlrrrrrlrlrrlrlrrrlrlrrlrrlrrrrrrrrrrrlrrrlrrlrrlrrrlrrrlrlrlrrrrrrlrlrrrrrrrlrrrrrrrrrlrrrrrrrrrrrrrrlrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr"}, {"heading": "6 Experiments", "text": "The proposed HRNN-based CLMs will be evaluated using two sets of text: the Wall Street Journal's (WSJ) Corpus (Paul & Baker, 1992) and One Billion Word Benchmark (Chelba et al., 2013); and we will present an example of end-to-end speech recognition, where HLSTM CLMs are used to decode tree-based beam detectors and greatly improve detection accuracy; and RNNNs will be trained with shortened back propagation through time (BPTT) (Werbos, 1990; Williams & Peng, 1990)."}, {"heading": "6.1 Perplexity", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1.1 Wall Street Journal (WSJ) corpus", "text": "Dataset The Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) is designed for training and benchmarking automatic speech recognition systems. However, for the perplexity (PPL) experiments we used the nonverbalized punctuation (NVP) version of the LM training data within the corpus. However, the experimental results Table 1 show the perplexity of the traditional mono-clock deep LSTM and HLSTM based on the final evaluation and does not participate in the training. NxM means that the network consists of N-LSTM layers in which each layer contains M memory cells. HLSTM models show the perplexity of the traditional mono-clock deep LSTM and HLSTM based on the held CLM group."}, {"heading": "6.1.2 One Billion Word Benchmark", "text": "Dataset The One Billion Word Benchmark (Chelba et al., 2013) dataset, however, contains approximately 0.8 billion words and approximately 800,000 words of vocabulary. We followed the standard method of sharing training and test data as in Chelba et al. (2013). Each byte of the UTF-8 encoded text is considered as a character. Therefore, the size of the character set is 256.Experimental results Due to the large amount of training data and weeks of training time, however, only two HLSTM-B experiments with the size of 4x512 and 4x1024 are performed. As shown in Table 3, there is a large gap (22.5) in word-based perplexity between the two models. Therefore, a further improvement in perplexity can be achieved with larger networks.Comparison with WLMs The perplexity of other WLMs is summarized in Table 4."}, {"heading": "6.2 End-to-end automatic speech recognition (ASR)", "text": "In this section, we apply the proposed CLMs to the end-to-end Automatic Speech Recognition (ASR) system to evaluate models in more practical situations than just measuring helplessness. CLMs are trained with WSJ LM training data as in Section 6.1.1. Unlike WLMs, the proposed CLMs have a very small number of parameters so they can be used for real-time character beam search. All experiments in this section run in real-time with NVIDIA GeForce GTX Titan X GPU. The incremental speech recognition system proposed in Hwang & Sung (2016) is used for evaluation. The acoustic model is 4x512 unidirectional LSTM and fully trained with connectionist time classification (CTC). The loss (Graves et al., 2006) due to the use of non-verbalized punctuation (NVP) reduces the proportion of the WSJ SI-28gate training CLM with their acoustic loss of energy and are 40-dimensional features."}, {"heading": "7 Concluding remarks", "text": "The HRNN consists of several submodules with different clock rates, so it is able to learn long-term dependencies as well as short-term details. We introduced two HRNN structures, HLSTM-A and HLSTM-B. Experimental results on One Billion Benchmark show that HLSTM-B networks significantly outperform Kneser-Ney 5 gram LMs with only 2% of the parameters. Although other RNN-based Word LMs perform better than our models, they have impractically many parameters. On the other hand, the proposed model, as shown in the WSJ speech recognition example, can be used for real-time speech recognition with less than 10 million parameters."}], "references": [{"title": "Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition", "author": ["Bridle", "John S"], "venue": "In Neurocomputing,", "citeRegEx": "Bridle and S.,? \\Q1990\\E", "shortCiteRegEx": "Bridle and S.", "year": 1990}, {"title": "A statistical approach to machine translation", "author": ["Brown", "Peter F", "Cocke", "John", "Pietra", "Stephen A Della", "Vincent J Della", "Jelinek", "Fredrick", "Lafferty", "John D", "Mercer", "Robert L", "Roossin", "Paul S"], "venue": "Computational linguistics,", "citeRegEx": "Brown et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1990}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Chelba", "Ciprian", "Mikolov", "Tomas", "Schuster", "Mike", "Ge", "Qi", "Brants", "Thorsten", "Koehn", "Phillipp", "Robinson", "Tony"], "venue": "arXiv preprint arXiv:1312.3005,", "citeRegEx": "Chelba et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2013}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["Gers", "Felix A", "Schmidhuber", "J\u00fcrgen", "Cummins", "Fred"], "venue": "Neural computation,", "citeRegEx": "Gers et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Learning precise timing with LSTM recurrent networks", "author": ["Gers", "Felix A", "Schraudolph", "Nicol N", "Schmidhuber", "J\u00fcrgen"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Gers et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Gers et al\\.", "year": 2003}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Graves", "Alex", "Fern\u00e1ndez", "Santiago", "Gomez", "Faustino", "Schmidhuber", "J\u00fcrgen"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "citeRegEx": "Graves et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2006}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Michiel", "Schrauwen", "Benjamin"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Hinton", "Geoffrey E", "Srivastava", "Nitish", "Krizhevsky", "Alex", "Sutskever", "Ilya", "Salakhutdinov", "Ruslan R"], "venue": "arXiv preprint arXiv:1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "In Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "Hwang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2015}, {"title": "Character-level incremental speech recognition with recurrent neural networks", "author": ["Hwang", "Kyuyeon", "Sung", "Wonyong"], "venue": "arXiv preprint arXiv:1601.06581,", "citeRegEx": "Hwang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hwang et al\\.", "year": 2016}, {"title": "BlackOut: Speeding up recurrent neural network language models with very large vocabularies", "author": ["Ji", "Shihao", "SVN Vishwanathan", "Satish", "Nadathur", "Anderson", "Michael J", "Dubey", "Pradeep"], "venue": "arXiv preprint arXiv:1511.06909,", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Exploring the limits of language modeling", "author": ["Jozefowicz", "Rafal", "Vinyals", "Oriol", "Schuster", "Mike", "Shazeer", "Noam", "Wu", "Yonghui"], "venue": "arXiv preprint arXiv:1602.02410,", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Kim", "Yoon", "Jernite", "Yacine", "Sontag", "David", "Rush", "Alexander M"], "venue": "arXiv preprint arXiv:1508.06615,", "citeRegEx": "Kim et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2015}, {"title": "Improved backing-off for m-gram language modeling", "author": ["Kneser", "Reinhard", "Ney", "Hermann"], "venue": "In Acoustics, Speech, and Signal Processing,", "citeRegEx": "Kneser et al\\.,? \\Q1995\\E", "shortCiteRegEx": "Kneser et al\\.", "year": 1995}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling", "Wang", "Lu\u00eds", "Tiago", "Marujo", "Astudillo", "Ram\u00f3n Fernandez", "Amir", "Silvio", "Dyer", "Chris", "Black", "Alan W", "Trancoso", "Isabel"], "venue": "arXiv preprint arXiv:1508.02096,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Character-based neural machine translation", "author": ["Ling", "Wang", "Trancoso", "Isabel", "Dyer", "Chris", "Black", "Alan W"], "venue": "arXiv preprint arXiv:1511.04586,", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Marcus", "Mitchell P", "Marcinkiewicz", "Mary Ann", "Santorini", "Beatrice"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Statistical language models based on neural networks. Presentation at Google", "author": ["Mikolov", "Tom\u00e1\u0161"], "venue": "Mountain View,", "citeRegEx": "Mikolov and Tom\u00e1\u0161.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Tom\u00e1\u0161.", "year": 2012}, {"title": "Context dependent recurrent neural network language model", "author": ["Mikolov", "Tomas", "Zweig", "Geoffrey"], "venue": "In SLT,", "citeRegEx": "Mikolov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2012}, {"title": "Recurrent neural network based language model", "author": ["Mikolov", "Tomas", "Karafi\u00e1t", "Martin", "Burget", "Lukas", "Cernock\u1ef3", "Jan", "Khudanpur", "Sanjeev"], "venue": "In INTERSPEECH,", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "A method for unconstrained convex minimization problem with the rate of convergence O (1/k2)", "author": ["Nesterov", "Yurii"], "venue": "In Doklady AN SSSR,", "citeRegEx": "Nesterov and Yurii.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov and Yurii.", "year": 1983}, {"title": "The design for the Wall Street Journal-based CSR corpus", "author": ["Paul", "Douglas B", "Baker", "Janet M"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "Paul et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Paul et al\\.", "year": 1992}, {"title": "Fundamentals of speech recognition", "author": ["Rabiner", "Lawrence", "Juang", "Biing-Hwang"], "venue": null, "citeRegEx": "Rabiner et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Rabiner et al\\.", "year": 1993}, {"title": "Sparse non-negative matrix language modeling for skip-grams", "author": ["Shazeer", "Noam", "Pelemans", "Joris", "Chelba", "Ciprian"], "venue": "In Proceedings of Interspeech,", "citeRegEx": "Shazeer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shazeer et al\\.", "year": 2015}, {"title": "Generating text with recurrent neural networks", "author": ["Sutskever", "Ilya", "Martens", "James", "Hinton", "Geoffrey E"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "Sutskever et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Backpropagation through time: what it does and how to do it", "author": ["Werbos", "Paul J"], "venue": "Proceedings of the IEEE,", "citeRegEx": "Werbos and J.,? \\Q1990\\E", "shortCiteRegEx": "Werbos and J.", "year": 1990}, {"title": "An efficient gradient-based algorithm for on-line training of recurrent network trajectories", "author": ["Williams", "Ronald J", "Peng", "Jing"], "venue": "Neural Computation,", "citeRegEx": "Williams et al\\.,? \\Q1990\\E", "shortCiteRegEx": "Williams et al\\.", "year": 1990}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["Zeiler", "Matthew D"], "venue": "arXiv preprint arXiv:1212.5701,", "citeRegEx": "Zeiler and D.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler and D.", "year": 2012}], "referenceMentions": [{"referenceID": 25, "context": "Language models show the probability distribution over sequences of words or characters, and they are very important for many speech and document processing applications including speech recognition, text generation, and machine translation (Rabiner & Juang, 1993; Sutskever et al., 2011; Brown et al., 1990).", "startOffset": 241, "endOffset": 308}, {"referenceID": 1, "context": "Language models show the probability distribution over sequences of words or characters, and they are very important for many speech and document processing applications including speech recognition, text generation, and machine translation (Rabiner & Juang, 1993; Sutskever et al., 2011; Brown et al., 1990).", "startOffset": 241, "endOffset": 308}, {"referenceID": 20, "context": "In recent years, the language modeling based on recurrent neural networks (RNNs) are actively investigated (Mikolov et al., 2010).", "startOffset": 107, "endOffset": 129}, {"referenceID": 25, "context": "Thus, when considering the input and output, the proposed network is a character-level language model (CLM) (Sutskever et al., 2011), but it contains a wordlevel model inside.", "startOffset": 108, "endOffset": 132}, {"referenceID": 17, "context": "(2015) used convolutional neural networks (CNNs) to generate word embeddings, and achieved the state of the art results on English Penn Treebank corpus (Marcus et al., 1993).", "startOffset": 152, "endOffset": 173}, {"referenceID": 2, "context": "(2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al., 2013), also achieving the state of the art perplexity.", "startOffset": 60, "endOffset": 81}, {"referenceID": 11, "context": "Kim et al. (2015) used convolutional neural networks (CNNs) to generate word embeddings, and achieved the state of the art results on English Penn Treebank corpus (Marcus et al.", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "The similar CNN-based embedding approach is used by Jozefowicz et al. (2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al.", "startOffset": 52, "endOffset": 77}, {"referenceID": 2, "context": "(2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al., 2013), also achieving the state of the art perplexity. In Ling et al. (2015a), bidirectional LSTMs are employed instead of CNNs for word embedding.", "startOffset": 61, "endOffset": 154}, {"referenceID": 2, "context": "(2016) with very big LSTM on the One Billion Word Benchmark (Chelba et al., 2013), also achieving the state of the art perplexity. In Ling et al. (2015a), bidirectional LSTMs are employed instead of CNNs for word embedding. However, in all of these approaches, LMs still generate the output probabilities at the word-level. Although the character-level modeling approach of the output word probability is introduced using CNN softmax in Jozefowicz et al. (2016), the base LSTM still runs with a word-level clock.", "startOffset": 61, "endOffset": 462}, {"referenceID": 25, "context": "Thanks to the recent advances in RNNs, RNN-based CLMs has begun to show satisfactory performances (Sutskever et al., 2011; Hermans & Schrauwen, 2013).", "startOffset": 98, "endOffset": 149}, {"referenceID": 3, "context": "LSTMs (Hochreiter & Schmidhuber, 1997) with forget gates (Gers et al., 2000) and peephole connections (Gers et al.", "startOffset": 57, "endOffset": 76}, {"referenceID": 4, "context": ", 2000) and peephole connections (Gers et al., 2003) can also be converted to the generalize form.", "startOffset": 33, "endOffset": 52}, {"referenceID": 2, "context": "The proposed HRNN based CLMs are evaluated with two text datasets: the Wall Street Journal (WSJ) corpus (Paul & Baker, 1992) and One Billion Word Benchmark (Chelba et al., 2013).", "startOffset": 156, "endOffset": 177}, {"referenceID": 7, "context": "No regularization method, such as dropout (Hinton et al., 2012), is employed.", "startOffset": 42, "endOffset": 63}, {"referenceID": 11, "context": "input # Params PPL Sigmoid RNN-2048 (Ji et al., 2015) No 4.", "startOffset": 36, "endOffset": 53}, {"referenceID": 2, "context": "1B n-grams (Chelba et al., 2013) No 1.", "startOffset": 11, "endOffset": 32}, {"referenceID": 24, "context": "6 Sparse non-negative matrix LM (Shazeer et al., 2015) No 33 G 52.", "startOffset": 32, "endOffset": 54}, {"referenceID": 2, "context": "9 RNN-1024 + ME 9-gram feature (Chelba et al., 2013) No 20 G 51.", "startOffset": 31, "endOffset": 52}, {"referenceID": 12, "context": "3 CNN input + 2xLSTM-8192-2048 (Jozefowicz et al., 2016) Yes 1.", "startOffset": 31, "endOffset": 56}, {"referenceID": 2, "context": "2 One Billion Word Benchmark Dataset The One Billion Word Benchmark (Chelba et al., 2013) dataset contains about 0.", "startOffset": 68, "endOffset": 89}, {"referenceID": 2, "context": "2 One Billion Word Benchmark Dataset The One Billion Word Benchmark (Chelba et al., 2013) dataset contains about 0.8 billion words and roughly 800 thousand words of vocabulary. We followed the standard way of splitting the training and test data as in Chelba et al. (2013). Each byte of UTF-8 encoded text is regarded as a character.", "startOffset": 69, "endOffset": 273}, {"referenceID": 2, "context": "1 billion n-grams (Chelba et al., 2013) even though the number of parameters of our model is only 2% of that of the KN-5 model.", "startOffset": 18, "endOffset": 39}, {"referenceID": 2, "context": "However, much lower perplexities are reported with sparse non-negative matrix LM and the maximum entropy feature based RNN model (Chelba et al., 2013), where the number of parameters are 33 G and 20 G, respectively.", "startOffset": 129, "endOffset": 150}, {"referenceID": 2, "context": "1 billion n-grams (Chelba et al., 2013) even though the number of parameters of our model is only 2% of that of the KN-5 model. However, much lower perplexities are reported with sparse non-negative matrix LM and the maximum entropy feature based RNN model (Chelba et al., 2013), where the number of parameters are 33 G and 20 G, respectively. Recently, Jozefowicz et al. (2016)", "startOffset": 19, "endOffset": 379}, {"referenceID": 5, "context": "The acoustic model is 4x512 unidirectional LSTM and end-to-end trained with connectionist temporal classification (CTC) loss (Graves et al., 2006) using the non-verbalized punctuation (NVP) portion of WSJ SI-284 training set.", "startOffset": 125, "endOffset": 146}], "year": 2016, "abstractText": "Recurrent neural network (RNN) based character-level language models (CLMs) are extremely useful for modeling unseen words by nature. However, their performance is generally much worse than the word-level language models (WLMs), since CLMs need to consider longer history of tokens to properly predict the next one. We address this problem by proposing hierarchical RNN architectures, which consist of multiple modules with different clock rates. Despite the multiclock structures, the input and output layers operate with the character-level clock, which allows the existing RNN CLM training approaches to be directly applicable without any modifications. Our CLM models show better perplexity than KneserNey (KN) 5-gram WLMs on the One Billion Word Benchmark with only 2% of parameters. Also, we present real-time character-level end-to-end speech recognition examples on the Wall Street Journal (WSJ) corpus, where replacing traditional mono-clock RNN CLMs with the proposed models results in better recognition accuracies even though the number of parameters are reduced to 30%.", "creator": "LaTeX with hyperref package"}}}