{"id": "1609.03894", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Crafting a multi-task CNN for viewpoint estimation", "abstract": "Convolutional Neural Networks (CNNs) were recently shown to provide state-of-the-art results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP.", "histories": [["v1", "Tue, 13 Sep 2016 15:19:38 GMT  (52kb,D)", "http://arxiv.org/abs/1609.03894v1", "To appear in BMVC 2016"]], "COMMENTS": "To appear in BMVC 2016", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.NE", "authors": ["francisco massa", "renaud marlet", "mathieu aubry"], "accepted": false, "id": "1609.03894"}, "pdf": {"name": "1609.03894.pdf", "metadata": {"source": "CRF", "title": "Crafting a multi-task CNN for viewpoint estimation", "authors": ["Francisco Massa", "Renaud Marlet", "Mathieu Aubry"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them are able to outdo themselves if they don't outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves, most of them are able to outdo themselves."}, {"heading": "2 Overview", "text": "We will focus on the problem of recognizing and estimating the pose of objects in images as defined by the Pascal3D + Average Viewpoint Precision (AVP) metric. In particular, we will focus on estimating the azimuth angle. For object detection, we will use the standard Fast R CNN framework [7], which is based on the region's proposal but significantly faster than the original R-CNN [8]. In addition, we will associate a viewpoint for each boundary box and for each object class. Since standpoint conventions may not be coherent for the different classes, we will learn a different estimator for each class. However, to avoid having to learn one network per class, we will share all but the last network layer between the different classes. In Section 3, we will first discuss different approaches to viewing prediction with CNNs and in particular the differences between regression and classification approaches."}, {"heading": "3 Approaches for viewpoint estimation", "text": "In this section, we assume that the delimitation box and the class of the objects are known, and focus on the different approaches to estimating their pose. Section 3.1 first discusses the design of regression approaches. Section 3.2 then presents two variants of classification approaches. The intuition behind these different approaches is illustrated in Figure 1."}, {"heading": "3.1 Viewpoint estimation as regression", "text": "Since the azimuth angle of a vantage point is a continuous quantity, it is natural to approach the estimation as a regression problem. Of course, the choice of the pose representation F (\u03b8) of an azimuthal angle \u03b8 is decisive for the effectiveness of this regression. In fact, if we simply consider F (\u03b8) = \u03b8, the periodicity of the pose is not taken into account. Thus, as emphasized in [20], a good pose representation F (\u03b8) fulfills the following properties: (a) it is invariant for the periodicity of the angle, and (b) it is analytically inverted.We examine two representations that satisfy both properties: (i) F (\u03b8) = [cos), sin (\u03b8)]], probably the simplest way to represent orientations used, for example, in [21]."}, {"heading": "3.2 Viewpoint estimation as classification", "text": "As can be seen from [27], the main limitation of a regression approach to estimating the point of view is that it cannot represent well the ambiguities that may exist between different points of view. Indeed, objects such as a table have symmetries or near-symmetries that make the problem of estimating the point of view inherently ambiguous, and this ambiguity is not handled well by the representations discussed in the previous paragraph. One solution to this problem is to discredit the pose space and predict a probability for each orientation bucket, thereby formulating the problem as one of classification. Note that a similar difficulty is found in the problem of the keypoint prediction, for which the similar solution of predicting a thermal map for each key point has proved successful instead of directly predicting its position [28]. In the case of a classification approach, the output of the network belongs to RNc \u00d7 Nv and any value can be interpreted as probability."}, {"heading": "3.2.1 Direct classification", "text": "The approach successfully applied in [28] consists in simply predicting for each class independently of each other the recycle bin into which the orientation of the object falls. This classification problem can be solved for each object class with the default cross entropy loss: Lclassif (w) = \u2212 Ns \u2211 i = 1 protocol (exp (f w (xi) ci, \u03b8 \u0432 i) \u0445 Nvv = 1 exp (f (xi) ci, v))) (2) At test date the predicted square recycle bin \u00da (x, c) for an input x of class c is given by \u03b8 (x, c) = argmax v (1,..., Nv} f w (x) c, v (3)."}, {"heading": "3.2.2 Geometric structure aware classification", "text": "The disadvantage of the previous classification approach is that it learns to predict the poses without explicitly using the continuity between narrow viewpoints. A solution to this problem was proposed in [27]. The authors discredit the orientations in Nv = 360 containers finely and consider the angle estimation as a classification problem, but adjust the loss to include a structured relationship between adjacent containers and punish minor angular errors: Lgeom (w) = \u2212 Ns \u00b2 i = 1Nv \u00b2 v = 1exp (\u2212 d (v, \u2082 i) \u03c3) log (exp (f (xi) ci, v) \u2022 Nvv = 1 exp (f (xi) ci, v)) (4), where d (v \u00b2 i) is the distance between the centers of the two b \u00b2 and v \u00b2."}, {"heading": "4 Joint detection and pose estimation", "text": "The methods presented in the previous section assume that the object detector is already trained and held independently of the pose estimator. Since object recognition and pose estimation are based on related information, we expect a benefit from a joint training. We therefore present extensions of the methods from Section 3 to conduct this joint training."}, {"heading": "4.1 Joint model with regression", "text": "The first approach described in [20] is to encode the presence or absence of an object by a point near or far from the regression line described by F. In the space in which the regression is performed, an alternative approach discussed in [21] is to add an output to the regression network specifically designed for detection, and the loss used to build the network can then be broken down into two terms: a classification loss Ldet (w), which is independent of the pose, and a regression loss Lreg (w), which only takes into account the pose estimation. As the state-of-the-art performance for detection is achieved by means of a classification loss, we selected the second option below. Therefore, our network has two outputs: f w, det (x), and LNc + 1 for the detection part (predicting the probabilities for each of the nctional classes and the background classes) for the non-ascending and non-ascending classes."}, {"heading": "4.2 Joint model with classification", "text": "A similar approach, which separates two branches of the network, can be used for classification. However, we introduce a new, simpler and parameter-free method to jointly perform detection and estimation in a classification structure. In fact, one can simply add a component associated with the background fields to the output vector of the Pose estimation setup of Section 3.2 and normalize it globally, rather than independently for each class as in Equation (2). Each value is then interpreted as a protocol probability that the object belongs to a class and is in a given orientation basket, instead of the conditional probability that the object is in a given orientation basket that knows its class. To obtain the probability that the object belongs to a class, one can simply add the probabilities that correspond to all containers for that class. Similar to Section 3.2, we write f obj w, obj (x) c, obj (x) c, obv category exx exx exx exx exx exx the orientation element according to the network category exx exx exx exx the orientation element."}, {"heading": "5 Experiments", "text": "Our experiments are based on the Fast R-CNN Object Detection Framework [7], with Deep Mask [24] Bounding Boxes suggestions. We trained and evaluated our models using the Pascal3D + dataset [29], which contains notes to the Pascal VOC 2012 training and validation images [5] for 12 rigid classes, as well as for a subset of ImageNet [4]. We also expanded the training data by adding the synthetic images from [27]. The evaluation metric we use is the Average Viewpoint Precision (AVP) associated with Pascal3D, which is very similar to the standard metric Average Precision (AP) used in detection tasks, but only considers the detections as positive for which the point of view estimation is correct. Specifically, the points of view are discredited in K containers, and the point of view estimation is considered correct when it falls within the AP range, where the P24 is most truthful)."}, {"heading": "5.1 Training details", "text": "We have refined our networks, starting from a network designed for the ImageNet classification, using Stochastic Gradient Descent with a pulse of 0.9 and a weight loss of 0.0005. We extend all data sets by the horizontally rotated versions of each image and turn the target orientations accordingly. During the training of the articulation and pose detection models, 25% of the mini-stacks consist of positive examples. Our mini-stacks are of size 128, except when using synthetic images. When using synthetic images, we randomly create montages with the rendered views from [27], each assembly contains 9 objects, for a total size of 137 (96 backgrounds and 32 positive fields from real images and 9 positive synthetic objects). This allows an efficient training in the setup of Fast R-CNN.We initialized the learning rate at 0.001 and divided it by 10 after convergence of the training error. The number of iterations from the Pascal to the 50K amount of data depends on the number of the number of the learning rate."}, {"heading": "5.2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1 Comparison of the different approaches for pose estimation", "text": "First, we compare the different approaches to estimating the pose in Section 3. We use a solid object detector based on the AlexNet architecture, trained to detect on Pascal VOC 2012 training sets, and report the results in Table 1. We can first observe that depicting a pose with a higher dimensionality (3D) performs better than using a smaller dimensionality (2D). We believe that the redundancy in the representation helps to better manage ambiguities in the estimate. However, the classification approach significantly exceeds both regressions (19.3% AVP compared to 13.9% and 15.7%). Interestingly, the simplest classification approach in Section 3.2 is slightly better than the geometry-specific method. We think that the main reason for this difference is that the simple classification is optimized precisely for the target evaluated by the AVP, and this result can therefore be considered an artifact of evaluation."}, {"heading": "5.2.2 Benefits of joint training for detection and pose estimation", "text": "These advantages can be twofold: First, the sequence of detection candidates given by the new detector can promote self-confident orientation and thus increase AVP. Second, the position estimates for a given object can be better. To evaluate both effects independently, we report in Table 2 on the results based on both the sequence of the detector used in the previous section and the sequence of the new common classifier. All experiments were conducted as described above, using the AlexNet architecture and Pascal VOC training data. Comparing Table 2 with Table 1 shows two main effects. First, the mAVP is improved even when using the same classifier and shows improved point of view estimation through joint training. Second, the mAP is reduced, which shows that the detection works worse when trained together. However, one can also find that the best mAVP is still achieved with the common classifier."}, {"heading": "5.2.3 Influence of network architectures and training data", "text": "In this section, we look at our common classification approach, which performs best in the ratings of the previous section, and examine how its performance varies when different architectures and more training data are used. Comparing the left and right columns of Table 3 shows that using the VGGG16 network instead of AlexNet consistently improves performance, which is slightly less for the mAVP than for the mAP, suggesting that the mAVP thrust is mainly due to improved detection performance but is highly unbalanced between the different classes. Analysis of these results shows consistent improvements when the training set contains more data from Pascal VOC. Interestingly, the full part of the ImageNet dataset commented on in Pascal3D + is better than the mAP, which shows that the additional data is more useful for posing than detection. Analysis of these results shows consistent improvements when the training set contains more data than the training set contains more data."}, {"heading": "5.2.4 Comparison to the state of the art", "text": "Table 4 details the performance improvements of AVP24 across all classes, as well as a comparison with three baselines: DPM-VOC + VP [22], which uses a modified version of DPM to also predict poses, Render for CNN [27], which uses real images of Pascal VOC as well as CAD for training a CNN based on AlexNet, and [28], which uses a VGG16 architecture and ImageNet data to classify orientations for each object category. It is evident that we are consistently improving in all baselines except the chair class. A more detailed analysis shows that this exception is related to the difference between ImageNet and Pascal stools. When adding the ImageNet data to the Pascal data, the detection performance of chairs decreases from 34.5% AP to 19.23% AP. Similarly, the difference between the very different appearance of the rendered 3D models and real images is responsible for decreasing the performance of the reins on the reins and the fact that performance on the training wheels decreases."}, {"heading": "6 Conclusion", "text": "By combining our common classification approach with the enhancements through in-depth architecture and additional training data, we are increasing the current performance of the Pose estimate by 5% mAVP. We believe that highlighting the various factors of this enhancement and establishing a new baseline will support and stimulate further work on the Position Estimate."}], "references": [{"title": "Smooth object retrieval using a bag of boundaries", "author": ["R. Arandjelovi\u0107", "A. Zisserman"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Seeing 3D chairs: exemplar part-based 2D-3D alignment using a large dataset of CAD models", "author": ["M. Aubry", "D. Maturana", "A.A. Efros", "B.C. Russell", "J. Sivic"], "venue": "International Conference on Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Torch7: A Matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "BigLearn, NIPS Workshop", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "ImageNet: A largescale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "International Conference on Computer Vision and Pattern Recognition (CVPR), pages 248\u2013255", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "The Pascal visual object classes (VOC) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "IJCV, 88(2):303\u2013338", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Object detection with discriminatively trained part based models", "author": ["P. Felzenszwalb", "R. Girshick", "D. McAllester", "D. Ramanan"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 32(9)", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Fast R-CNN", "author": ["R. Girshick"], "venue": "International Conference on Computer Vision (ICCV), pages 1440\u20131448", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["Ross Girshick", "Jeff Donahue", "Trevor Darrell", "Jitendra Malik"], "venue": "In International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Viewpoint-aware object detection and pose estimation", "author": ["Daniel Glasner", "Meirav Galun", "Sharon Alpert", "Ronen Basri", "Gregory Shakhnarovich"], "venue": "In International Conference on Computer Vision (ICCV),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Spatial pyramid pooling in deep convolutional networks for visual recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "European Conf. on Comp. Vision (ECCV), pages 346\u2013361", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Analyzing 3D objects in cluttered images", "author": ["M. Hejrati", "D. Ramanan"], "venue": "Neural Information Processing Systems (NIPS)", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Object recognition using alignment", "author": ["D.P. Huttenlocher", "S. Ullman"], "venue": "International Conference on Computer Vision (ICCV)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1987}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, pages 1097\u20131105", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Backpropagation applied to handwritten zip code recognition", "author": ["Y. LeCun", "B. Boser", "J.S. Denker", "D. Henderson", "R.E. Howard", "W. Hubbard", "L.D. Jackel"], "venue": "Neural comp., 1 (4):541\u2013551", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1989}, {"title": "Worldwide pose estimation using 3D point clouds", "author": ["Y. Li", "N. Snavely", "D. Huttenlocher", "P. Fua"], "venue": "European Conference on Computer Vision (ECCV)", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Parsing ikea objects: Fine pose estimation", "author": ["J.J. Lim", "H. Pirsiavash", "A. Torralba"], "venue": "International Conference on Computer Vision (ICCV), pages 2992\u20132999. IEEE", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "The viewpoint consistency constraint", "author": ["D. Lowe"], "venue": "International Journal of Computer Vision (IJCV), 1(1):57\u201372", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1987}, {"title": "Convolutional neural networks for joint object detection and pose estimation: A comparative study", "author": ["Francisco Massa", "Mathieu Aubry", "Renaud Marlet"], "venue": "arXiv preprint arXiv:1412.7190,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Synergistic face detection and pose estimation with energy-based models", "author": ["M. Osadchy", "Y. LeCun", "M.L. Miller"], "venue": "The Journal of Machine Learning Research, 8:1197\u20131215", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving object classification using pose information", "author": ["H. Penedones", "R. Collobert", "F. Fleuret", "D. Grangier"], "venue": "Technical report, Idiap Research Institute", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Teaching 3D geometry to deformable part models", "author": ["Bojan Pepik", "Michael Stark", "Peter Gehler", "Bernt Schiele"], "venue": "In International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Learning to segment object candidates", "author": ["Pedro O Pinheiro", "Ronan Collobert", "Piotr Dollar"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Machine perception of 3-D solids", "author": ["L. Roberts"], "venue": "PhD. Thesis", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1965}, {"title": "Very deep convolutional networks for largescale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Beyond Pascal: A benchmark for 3D object detection in the wild", "author": ["Y. Xiang", "R. Mottaghi", "S. Savarese"], "venue": "IEEE Winter Conference on Applications of Computer Vision (WACV)", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2014}, {"title": "Estimating the aspect layout of object categories", "author": ["Yu Xiang", "Silvio Savarese"], "venue": "In International Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Detailed 3D representations for object recognition and modeling", "author": ["M. Zia", "M. Stark", "B. Schiele", "K. Schindler"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 25, "context": "By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset [29].", "startOffset": 152, "endOffset": 156}, {"referenceID": 12, "context": "While it was initially tackled for single objects with known 3D models [13, 18, 25], it was progressively investigated for complete object categories.", "startOffset": 71, "endOffset": 83}, {"referenceID": 17, "context": "While it was initially tackled for single objects with known 3D models [13, 18, 25], it was progressively investigated for complete object categories.", "startOffset": 71, "endOffset": 83}, {"referenceID": 23, "context": "While it was initially tackled for single objects with known 3D models [13, 18, 25], it was progressively investigated for complete object categories.", "startOffset": 71, "endOffset": 83}, {"referenceID": 25, "context": "The interest in this problem has recently increased both by the availability of the Pascal3D+ dataset [29], which provides a standard way to compare algorithms on diverse classes, and by the improved performance of object detection, which encouraged researchers to focus on extracting more complex information from the images than the position of objects.", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "In particular, several approaches have been proposed, such as a regression approach with joint training for detection [20, 21], a direct viewpoint classification [28], and a geometric structure aware fine-grained viewpoint classification [27], where the authors modify the classification objective to take into account", "startOffset": 118, "endOffset": 126}, {"referenceID": 20, "context": "In particular, several approaches have been proposed, such as a regression approach with joint training for detection [20, 21], a direct viewpoint classification [28], and a geometric structure aware fine-grained viewpoint classification [27], where the authors modify the classification objective to take into account", "startOffset": 118, "endOffset": 126}, {"referenceID": 20, "context": "While several of the elements that we employ have been used in previous work [21, 27, 28], we know of no systematic study of their respective and combined effect, resulting in an absence of clear good practices for viewpoint estimation and sub-optimal performances.", "startOffset": 77, "endOffset": 89}, {"referenceID": 14, "context": "[15]), their use has been generalized only in 2012 after the demonstration of their benefits by Krizhevsky et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] on the ImageNet large-scale visual recognition challenge [4].", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[14] on the ImageNet large-scale visual recognition challenge [4].", "startOffset": 62, "endOffset": 65}, {"referenceID": 7, "context": "[8] provided an important improvement over previous methods on the Pascal VOC dataset [5].", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[8] provided an important improvement over previous methods on the Pascal VOC dataset [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 10, "context": "This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].", "startOffset": 101, "endOffset": 105}, {"referenceID": 22, "context": "This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].", "startOffset": 137, "endOffset": 141}, {"referenceID": 6, "context": "This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].", "startOffset": 199, "endOffset": 206}, {"referenceID": 9, "context": "This method has then been improved in several ways, in particular using better network architectures [11], better bounding box proposals [24] and a better sharing of the computations inside an image [7, 10].", "startOffset": 199, "endOffset": 206}, {"referenceID": 0, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 12, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 15, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 16, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 17, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 23, "context": "Rigid object viewpoint estimation was first tackled in the case of object instances with known 3D models, together with their detection [1, 13, 16, 17, 18, 25].", "startOffset": 136, "endOffset": 159}, {"referenceID": 5, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 118, "endOffset": 132}, {"referenceID": 8, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 118, "endOffset": 132}, {"referenceID": 11, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 118, "endOffset": 132}, {"referenceID": 21, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 118, "endOffset": 132}, {"referenceID": 26, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 152, "endOffset": 160}, {"referenceID": 27, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 152, "endOffset": 160}, {"referenceID": 1, "context": "These approaches were extended to object categories detection using either extensions of Deformable Part Models (DPM) [6, 9, 12, 23], parametric models [30, 31] or large 3D instances collections [2, 27].", "startOffset": 195, "endOffset": 202}, {"referenceID": 25, "context": "With the advent of Pascal3D+ dataset [29], which extends Pascal VOC dataset [5] by aligning a set of 3D CAD models for 12 rigid object classes, learning-based approaches using only on example images became possible and proved their superior performance.", "startOffset": 37, "endOffset": 41}, {"referenceID": 4, "context": "With the advent of Pascal3D+ dataset [29], which extends Pascal VOC dataset [5] by aligning a set of 3D CAD models for 12 rigid object classes, learning-based approaches using only on example images became possible and proved their superior performance.", "startOffset": 76, "endOffset": 79}, {"referenceID": 25, "context": "[29] extended the method of [22], which uses an adaptation of DPM with 3D constraints to estimate the pose.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "CNN-based approaches, which were until the availability of the Pascal3D+ data limited to special cases such as faces [20] and small datasets", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "[21], also began to be applied to this problem at a larger scale.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "In [19], we explored different pose representations and showed the interest of joint training using AlexNet [14] and Pascal VOC [5] data.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "In [19], we explored different pose representations and showed the interest of joint training using AlexNet [14] and Pascal VOC [5] data.", "startOffset": 108, "endOffset": 112}, {"referenceID": 4, "context": "In [19], we explored different pose representations and showed the interest of joint training using AlexNet [14] and Pascal VOC [5] data.", "startOffset": 128, "endOffset": 131}, {"referenceID": 24, "context": "[28] used a simple classification approach with the VGG16 network [26] and annotations for ImageNet objects and established the current state-of-the-art on Pascal3D+.", "startOffset": 66, "endOffset": 70}, {"referenceID": 6, "context": "For object detection, we use the standard Fast R-CNN framework [7], which relies on region proposal but is significantly faster than the original R-CNN [8].", "startOffset": 63, "endOffset": 66}, {"referenceID": 7, "context": "For object detection, we use the standard Fast R-CNN framework [7], which relies on region proposal but is significantly faster than the original R-CNN [8].", "startOffset": 152, "endOffset": 155}, {"referenceID": 19, "context": "Thus, as highlighted in [20], a good pose representation F(\u03b8) satisfies the following properties: (a) it is invariant to the periodicity of the angle \u03b8 , and (b) it is analytically invertible.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "(i) F(\u03b8) = [ cos(\u03b8), sin(\u03b8) ] , probably the simplest way to represent orientations, used for example in [21];", "startOffset": 105, "endOffset": 109}, {"referenceID": 19, "context": "(ii) F(\u03b8) = [ cos ( \u03b8 \u2212 3 ) , cos(\u03b8) , cos ( \u03b8 + 3 )] , a formulation which was presented in [20], and that has a higher dimensionality than the previous one, allowing more flexibility for the network to better capture the pose information.", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Other regression approaches and loss are discussed in [19] but lead to lower performances.", "startOffset": 54, "endOffset": 58}, {"referenceID": 19, "context": "The first one, described in [20] is to encode respectively the presence or absence of an object by a point close or far from the regression line described by F in the space where the regression is performed.", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "in [21], is to add an output to the regression network specifically dedicated to detection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "Our experiments are based on the Fast R-CNN object detection framework [7], with Deep Mask [24] bounding boxes proposals.", "startOffset": 71, "endOffset": 74}, {"referenceID": 22, "context": "Our experiments are based on the Fast R-CNN object detection framework [7], with Deep Mask [24] bounding boxes proposals.", "startOffset": 91, "endOffset": 95}, {"referenceID": 25, "context": "We trained and evaluated our models using the Pascal3D+ dataset [29], which contains pose annotations for the training and validation images from Pascal VOC 2012 [5] for 12 rigid classes, as well as for a subset of ImageNet [4].", "startOffset": 64, "endOffset": 68}, {"referenceID": 4, "context": "We trained and evaluated our models using the Pascal3D+ dataset [29], which contains pose annotations for the training and validation images from Pascal VOC 2012 [5] for 12 rigid classes, as well as for a subset of ImageNet [4].", "startOffset": 162, "endOffset": 165}, {"referenceID": 3, "context": "We trained and evaluated our models using the Pascal3D+ dataset [29], which contains pose annotations for the training and validation images from Pascal VOC 2012 [5] for 12 rigid classes, as well as for a subset of ImageNet [4].", "startOffset": 224, "endOffset": 227}, {"referenceID": 25, "context": "We focus on the AVP24 metric, which discretizes the orientation into K = 24 bins and is the most fine-grained of the Pascal3D+ challenge [29].", "startOffset": 137, "endOffset": 141}, {"referenceID": 2, "context": "All experiments were conducted using the Torch7 framework [3] and we will release our full code upon publication.", "startOffset": 58, "endOffset": 61}], "year": 2016, "abstractText": "Convolutional Neural Networks (CNNs) were recently shown to provide state-of-theart results for object category viewpoint estimation. However different ways of formulating this problem have been proposed and the competing approaches have been explored with very different design choices. This paper presents a comparison of these approaches in a unified setting as well as a detailed analysis of the key factors that impact performance. Followingly, we present a new joint training method with the detection task and demonstrate its benefit. We also highlight the superiority of classification approaches over regression approaches, quantify the benefits of deeper architectures and extended training data, and demonstrate that synthetic data is beneficial even when using ImageNet training data. By combining all these elements, we demonstrate an improvement of approximately 5% mAVP over previous state-of-the-art results on the Pascal3D+ dataset [29]. In particular for their most challenging 24 view classification task we improve the results from 31.1% to 36.1% mAVP.", "creator": "LaTeX with hyperref package"}}}