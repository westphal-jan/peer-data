{"id": "1609.03976", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Sep-2016", "title": "Multimodal Attention for Neural Machine Translation", "abstract": "The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.", "histories": [["v1", "Tue, 13 Sep 2016 18:46:03 GMT  (782kb,D)", "http://arxiv.org/abs/1609.03976v1", "10 pages, under review COLING 2016"]], "COMMENTS": "10 pages, under review COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.NE", "authors": ["ozan caglayan", "lo\\\"ic barrault", "fethi bougares"], "accepted": false, "id": "1609.03976"}, "pdf": {"name": "1609.03976.pdf", "metadata": {"source": "CRF", "title": "Multimodal Attention for Neural Machine Translation", "authors": ["Ozan Caglayan", "Lo\u0131\u0308c Barrault", "Fethi Bougares"], "emails": ["FirstName.LastName@univ-lemans.fr", "ocaglayan@gsu.edu.tr"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people are able to understand themselves and understand what they do when they do it. (...) It is not so that they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...) It is as if they do it. (...). (...). (...). (...). (...). (...). (...). (...). (.). (...). (.). (.). (...). \"(.\" (. \"(.).\" (...). \"(.\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (. \"(.).\" (.). \"(.\" (.). \"(.\" (.). \"(.).\" (.). \"(.\" (.). \"(.).\" (. \"(.).\" (.). (. \"(.). (.).\" (.). \"(.\" (.). \"(.\" (.). \"(.).\" (.). \"(. (.).).\" (.). (. \"(.). (.).).\" (. (.). \"(. (.).).\" (.). \"(.).\" (. (.). \"(.).\" (. (.).). (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).). (.\" (.).). (.). \"(.).\" ("}, {"heading": "2 Related Work", "text": "Since the presented work is at the interface of several research topics, this part is divided into further sections for reasons of clarity."}, {"heading": "2.1 Neural Machine Translation (NMT)", "text": "End-to-end machine translation using deep learning was first proposed by Kalchbrenner and Blunsom (2013) and expanded by Cho et al. (2014), Sutskever et al. (2014) and Bahdanau et al. (2014) to achieve competitive results over modern phrase-based methods (Koehn et al., 2003). The prevailing approaches in the NMT differ in the way the source set is presented: Sutskever et al. (2014) and Cho et al. (2014) used the last hidden state of the coder as a fixed-size source set, while Bahdanau et al. (2014) introduced an attention mechanism whereby the recurring hidden states of the coder are assigned a series of attention weights to obtain a weighted sum of states, rather than just the last one."}, {"heading": "2.2 Neural Image Captioning", "text": "These image features are usually extracted from powerful state-of-the-art CNN architectures trained on large format image classification tasks such as ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN that differs in the selection and integration of image features: Mao et al. (2015) used a multimodal layer that embeds image features, current word embedding, and the current hidden state of the RNN in a common multimodal space. Image features experimented by the authors are extracted from two distinct CNNs, namely AlexNet (Krizhevsky et al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al al, 2012) and VGG (Simonyan and Zisserman, 2014)."}, {"heading": "2.3 Multi-Task Learning", "text": "Dong et al. (2015) proposed a multi-target NMT that translates a single source language into multiple target languages. Specifically, they used a single encoder and several language-specific decoders, each of which had its own attention mechanism. Firat et al. (2016) expanded this idea into a multilingual NMT that can translate between multiple languages via a single attention mechanism in all languages. Luong et al. (2015a) experimented with one-to-many, many-to-one, and many-to-many schemes to quantify the mutual benefit of multiple NLP, MT, and CV tasks."}, {"heading": "3 Architecture", "text": "The general architecture (Figure 1) consists of four building blocks, namely the textual encoder, the visual encoder, the attention mechanism and the decoder, all of which are described below."}, {"heading": "3.1 Textual Encoder", "text": "We define with X and Y a source record of length N and a target record of length M, where each word is represented in a source / target embed space with dimension E: X = (x1, x2,..., xN), xi-RE (1) Y = (y1, y2,..., yM), yj-RE (2) A bidirectional GRU (Cho et al., 2014) encoder with hidden units D reads the input X sequentially forward and backward to create two sets of hidden states based on the current word embedding and the previous hidden state of the encoder. atxti is called the textual annotation vector, which is achieved at step i by concatenating the forward and backward hidden states of the encoder: atxti = [~ hi ~ hi], atxt2, which corresponds txt2."}, {"heading": "3.2 Visual Encoder", "text": "To make the dimensionality compatible with the textual annotation vectors atxti, a linear TransformationWim is applied to the image characteristics that lead to the visual annotation vectors Aim = {aim1, aim2,..., aim196}."}, {"heading": "3.3 Decoder", "text": "A conditional GRU1 (CGRU) decoder has been extended for the multimodal context and equipped with a multimodal attention mechanism to generate the description in the target language. CGRU consists of two stacked GRU activations, each of which we will name g1 and g2. We experimented with several ways to initialize the hidden state of the decoder and found that the model works better when h (1) 0 = tanh (W Tinit (1N) i = 1 atxti) + binit), h (2) 1 (4) of the hidden state h (2) of g2 with h (1) 1: h (1) 0 = tanh (W Tinit) + binit), h (2) 0 = h (1) 1 (4) of the multimodal attention mechanism (detailed in section 4), t t t t t t = tanit), t = tanit), h (2), vt = the text of the first (V) context (1) and (1)."}, {"heading": "4 Attention Mechanism", "text": "In the context of multilingual NMT, Firat et al. (2016), we benefit from a single attention mechanism shared across different language pairs. Although this may be useful within a multilingual task where all inputs and outputs are based exclusively on textual representations, this may not be the optimal approach if the modalities are completely different. To verify the above statement, we consider various schemes for integrating the attention mechanism into multimodal learning. First, a common feed network is used to generate modality-specific attention weights {\u03b1txtt, \u03b1imt} {\u03b1txtt = softmax (UA tanh (WD h (1) t + WC A txt))). (9) \u03b1txtt = softmax (WD tanh (1) t + WC A im))."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Dataset", "text": "We used the Multi30K dataset (Elliott et al., 2016), an extended version of the Flickr30K Entities dataset (Young et al., 2014). Multi30K extends the original Flickr30K dataset, which contains 31K images with 5 English descriptions, by 5 additional independently created descriptions in German. It should be noted that the bilingual descriptions provided are not direct translations, but can be considered reasonably comparable.The training, validation and test set consists of 29000, 1014 and 1000 images, each with 5 English and 5 German annotations. A total of 25 sentence pairs can be achieved by taking the cross sections of these annotations, which lead to a training set of 725K samples. In the course of this work, we have reduced 725K to 145K, considering only 5 sentence pairs for each image."}, {"heading": "5.2 Training", "text": "We trained two monomodal baselines, namely NMT, IMGTXT and various attention variants of our proposed MNMT. The baselines have exactly the same architecture that was presented during this work, but only a single source modality. All models have Word embedding and recurring dimensionality layers 620 and 1000, respectively. We used Adam (Kingma and Ba, 2014) as a stochastic downward variant with a minibatch size of 32. The weights of the networks are initialized according to the Xavier scheme (Glorot and Bengio, 2010), while the distortions are initially set to 0. L2 regulation with \u03bb = 0.00001 is applied to the training costs to avoid overpassing.The performance of the networks is evaluated using the first validation splitting using BLEU (Papineni et al., 2002) at the end of each epoch and the training is stopped if BLEU does not improve for 20 evaluation evaluations."}, {"heading": "6.1 Quantitative Analysis", "text": "The description generation performance of the models is presented in Table 2 on the basis of BLEU, METEOR (Lavie and Agarwal, 2007) and CIDEr-D (Vedantam et al., 2015) as automatic evaluation metrics. Table 2 shows that the MNMT with CONCAT as merger operator improves in relation to both the NMT and the IMGTXT base line when combined with modality-dependent attention mechanisms (MNMT 6 to 8). The results are slightly worse than the NMT base line, regardless of the multimodal attention type, if the merger with the SUM operator is realized. This difference can be attributed to the fact that the concatenation uses a linear layer that learns to integrate the modality-specific activations in the multimodal context.The improvement in relation to the automatic metrics is more significant than the best source selection strategy for the first comparison of the valid and complete."}, {"heading": "6.2 Qualitative Analysis", "text": "In Table 3, we present the English descriptions generated by the NMT baseline and the best performing MNMT model using the same source description from the first validation slit. In the first image, MNMT clearly provides a richer description, which includes additional visual information such as the color and type of clothing, as well as the positioning of the woman. In the second case, MNMT again generates a coherent and rich description, but ignores the pink laptop bag mentioned in the NMT issue. An advantage of the attention mechanism is the ability to visualize where exactly the network is paying attention, while MNMT performs its work correctly, although METEOR punishes it, a phenomenon observed when the references contain less detail than the hypotheses. An advantage of the attention mechanism is the ability to visualize where exactly the network is paying attention. Although the visualization across the textual context is vial from the image, the region is vial, the spatial in the original model is visited."}, {"heading": "7 Conclusion", "text": "In this paper, we have proposed an architecture that performs multimodal machine translation with multimodal attention. By quantitative and qualitative analysis of various attention schemes, we have shown that modality-dependent MNMT outperforms the textual NMT baseline by up to 1 BLEU / METEOR and 2.5 CIDER-D points. The difference is even more significant in the best source selection strategy, where we achieve a gain of almost 1.6 BLEU / METEOR and 4.2 CIDER-D points. We continue to visualize multimodal attention and show that modality-dependent attention mechanisms are able to learn alignment with both source words and image attributes, but this is not the case when textual alignment is completely disturbed."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1406.1078.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei."], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248\u2013255. IEEE.", "citeRegEx": "Deng et al\\.,? 2009", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "ACL.", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Multi-language image description with neural sequence models", "author": ["Desmond Elliott", "Stella Frank", "Eva Hasler."], "venue": "CoRR, abs/1510.04709.", "citeRegEx": "Elliott et al\\.,? 2015", "shortCiteRegEx": "Elliott et al\\.", "year": 2015}, {"title": "Multi30k: Multilingual english-german image descriptions", "author": ["Desmond Elliott", "Stella Frank", "Khalil Sima\u2019an", "Lucia Specia"], "venue": "arXiv preprint arXiv:1605.00459", "citeRegEx": "Elliott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Elliott et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Orhan Firat", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1601.01073.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910). Society for Artificial Intelligence and Statistics.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR).", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "Proceedings of The 32nd International Conference on Machine Learning, pages 448\u2013456.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Recurrent continuous translation models", "author": ["Nal Kalchbrenner", "Phil Blunsom."], "venue": "Seattle, October. Association for Computational Linguistics.", "citeRegEx": "Kalchbrenner and Blunsom.,? 2013", "shortCiteRegEx": "Kalchbrenner and Blunsom.", "year": 2013}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3128\u20133137. IEEE.", "citeRegEx": "Karpathy and Fei.Fei.,? 2015", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["Philipp Koehn", "Franz Josef Och", "Daniel Marcu."], "venue": "Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 48\u201354. Association for Computational Linguistics.", "citeRegEx": "Koehn et al\\.,? 2003", "shortCiteRegEx": "Koehn et al\\.", "year": 2003}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton."], "venue": "Advances in neural information processing systems, pages 1097\u20131105.", "citeRegEx": "Krizhevsky et al\\.,? 2012", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Meteor: an automatic metric for mt evaluation with high levels of correlation with human judgments", "author": ["Alon Lavie", "Abhaya Agarwal."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, StatMT \u201907, pages 228\u2013231, Stroudsburg, PA, USA. Association for Computational Linguistics.", "citeRegEx": "Lavie and Agarwal.,? 2007", "shortCiteRegEx": "Lavie and Agarwal.", "year": 2007}, {"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton."], "venue": "Nature, 521(7553):436\u2013444.", "citeRegEx": "LeCun et al\\.,? 2015", "shortCiteRegEx": "LeCun et al\\.", "year": 2015}, {"title": "Microsoft coco: Common objects in context", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan", "Piotr Doll\u00e1r", "C Lawrence Zitnick."], "venue": "European Conference on Computer Vision, pages 740\u2013755. Springer.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "Multi-task sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "arXiv preprint arXiv:1511.06114.", "citeRegEx": "Luong et al\\.,? 2015a", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Minh-Thang Luong", "Hieu Pham", "Christopher D Manning."], "venue": "arXiv preprint arXiv:1508.04025.", "citeRegEx": "Luong et al\\.,? 2015b", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["Junhua Mao", "Wei Xu", "Yi Yang", "Jiang Wang", "Zhiheng Huang", "Alan Yuille."], "venue": "ICLR.", "citeRegEx": "Mao et al\\.,? 2015", "shortCiteRegEx": "Mao et al\\.", "year": 2015}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu."], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL \u201902, pages 311\u2013318, Stroudsburg, PA, USA.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR, abs/1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u2013 4575.", "citeRegEx": "Vedantam et al\\.,? 2015", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan."], "venue": "Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on, pages 3156\u20133164. IEEE.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["Kelvin Xu", "Jimmy Ba", "Ryan Kiros", "Kyunghyun Cho", "Aaron Courville", "Ruslan Salakhudinov", "Rich Zemel", "Yoshua Bengio."], "venue": "Proceedings of The 32nd International Conference on Machine Learning, pages 2048\u20132057.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Image captioning with semantic attention", "author": ["Quanzeng You", "Hailin Jin", "Zhaowen Wang", "Chen Fang", "Jiebo Luo."], "venue": "arXiv preprint arXiv:1603.03925.", "citeRegEx": "You et al\\.,? 2016", "shortCiteRegEx": "You et al\\.", "year": 2016}, {"title": "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions", "author": ["Peter Young", "Alice Lai", "Micah Hodosh", "Julia Hockenmaier."], "venue": "Transactions of the Association for Computational Linguistics, 2:67\u201378.", "citeRegEx": "Young et al\\.,? 2014", "shortCiteRegEx": "Young et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 16, "context": "Recently, deep neural networks (DNN) achieved state-of-the-art results in numerous tasks of computer vision (CV), natural language processing (NLP) and speech processing (LeCun et al., 2015) where the input signals are monomodal i.", "startOffset": 170, "endOffset": 190}, {"referenceID": 23, "context": "Machine translation (MT) is another field where purely neural approaches (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 0, "context": "Machine translation (MT) is another field where purely neural approaches (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al.", "startOffset": 79, "endOffset": 126}, {"referenceID": 13, "context": ", 2014) challenge the classical phrase-based approach (Koehn et al., 2003).", "startOffset": 54, "endOffset": 74}, {"referenceID": 29, "context": "(2016) have achieved state-of-the-art results on popular image captioning datasets Flickr30k (Young et al., 2014) and MSCOCO (Lin et al.", "startOffset": 93, "endOffset": 113}, {"referenceID": 17, "context": ", 2014) and MSCOCO (Lin et al., 2014).", "startOffset": 19, "endOffset": 37}, {"referenceID": 4, "context": "Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.", "startOffset": 164, "endOffset": 246}, {"referenceID": 3, "context": "Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.", "startOffset": 164, "endOffset": 246}, {"referenceID": 18, "context": "Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.", "startOffset": 164, "endOffset": 246}, {"referenceID": 6, "context": "Although several attempts have been made to incorporate features from different languages (multi-source/multi-target) or different tasks to improve the performance (Elliott et al., 2015; Dong et al., 2015; Luong et al., 2015a; Firat et al., 2016), according to our knowledge, there has not been any attentional neural translation or captioning approach trained using an auxiliary source modality.", "startOffset": 164, "endOffset": 246}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies. Another straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al.", "startOffset": 8, "endOffset": 587}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies. Another straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al. (2015), Xu et al.", "startOffset": 8, "endOffset": 610}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies. Another straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al. (2015), Xu et al. (2015), You et al.", "startOffset": 8, "endOffset": 628}, {"referenceID": 0, "context": ", 2014; Bahdanau et al., 2014) challenge the classical phrase-based approach (Koehn et al., 2003). This has been possible by formulating the translation problem as a sequence-to-sequence paradigm where a DNN may read and produce text in discrete steps with special recurrent building blocks that can model long range dependencies. Another straightforward application of this paradigm is to incorporate DNNs into the task of generating natural language descriptions from images, a task commonly referred as image captioning. A number of approaches proposed by Karpathy and Fei-Fei (2015) ,Vinyals et al. (2015), Xu et al. (2015), You et al. (2016) have achieved state-of-the-art results on popular image captioning datasets Flickr30k (Young et al.", "startOffset": 8, "endOffset": 647}, {"referenceID": 0, "context": "The modality specific pathways of the proposed architecture are inspired from two previously published approaches (Bahdanau et al., 2014; Xu et al., 2015) where an attention mechanism is learned to focus on different parts of the input sentence.", "startOffset": 114, "endOffset": 154}, {"referenceID": 27, "context": "The modality specific pathways of the proposed architecture are inspired from two previously published approaches (Bahdanau et al., 2014; Xu et al., 2015) where an attention mechanism is learned to focus on different parts of the input sentence.", "startOffset": 114, "endOffset": 154}, {"referenceID": 5, "context": "We compare the proposed architecture against single modality baseline systems using the recently published Multi30k multilingual image captioning dataset (Elliott et al., 2016).", "startOffset": 154, "endOffset": 176}, {"referenceID": 13, "context": "(2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003).", "startOffset": 87, "endOffset": 107}, {"referenceID": 19, "context": "The idea of attention still continues to be an active area of research in the NMT community (Luong et al., 2015b).", "startOffset": 92, "endOffset": 113}, {"referenceID": 8, "context": "1 Neural Machine Translation (NMT) End-to-end machine translation using deep learning has been first proposed by Kalchbrenner and Blunsom (2013) and further extended by Cho et al.", "startOffset": 113, "endOffset": 145}, {"referenceID": 0, "context": "1 Neural Machine Translation (NMT) End-to-end machine translation using deep learning has been first proposed by Kalchbrenner and Blunsom (2013) and further extended by Cho et al. (2014), Sutskever et al.", "startOffset": 169, "endOffset": 187}, {"referenceID": 0, "context": "1 Neural Machine Translation (NMT) End-to-end machine translation using deep learning has been first proposed by Kalchbrenner and Blunsom (2013) and further extended by Cho et al. (2014), Sutskever et al. (2014) and Bahdanau et al.", "startOffset": 169, "endOffset": 212}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al.", "startOffset": 11, "endOffset": 34}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003). The dominant approaches in NMT differ in the way of representing the source sentence: Sutskever et al. (2014) and Cho et al.", "startOffset": 11, "endOffset": 246}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003). The dominant approaches in NMT differ in the way of representing the source sentence: Sutskever et al. (2014) and Cho et al. (2014) used the last hidden state of the encoder as a fixed size source sentence representation while Bahdanau et al.", "startOffset": 11, "endOffset": 268}, {"referenceID": 0, "context": "(2014) and Bahdanau et al. (2014) to achieve competitive results compared to state-of-the-art phrase-based method (Koehn et al., 2003). The dominant approaches in NMT differ in the way of representing the source sentence: Sutskever et al. (2014) and Cho et al. (2014) used the last hidden state of the encoder as a fixed size source sentence representation while Bahdanau et al. (2014) introduced an attention mechanism where a set of attentional weights are assigned to the recurrent hidden states of the encoder to obtain a weighted sum of the states instead of just taking the last.", "startOffset": 11, "endOffset": 386}, {"referenceID": 2, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009).", "startOffset": 157, "endOffset": 176}, {"referenceID": 14, "context": "The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014).", "startOffset": 100, "endOffset": 125}, {"referenceID": 22, "context": ", 2012) and VGG (Simonyan and Zisserman, 2014).", "startOffset": 16, "endOffset": 46}, {"referenceID": 9, "context": "(2015) trained an ensemble of LSTM in which the image features extracted from a batch-normalized GoogLeNet (Ioffe and Szegedy, 2015) are presented to the LSTM sentence generator as the first input, before the special start word.", "startOffset": 107, "endOffset": 132}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al.", "startOffset": 158, "endOffset": 196}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al.", "startOffset": 158, "endOffset": 225}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al.", "startOffset": 158, "endOffset": 248}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space.", "startOffset": 158, "endOffset": 358}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space. The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014). Karpathy and Fei-Fei (2015) takes a simpler approach and used a vanilla RNN that incorporates VGG image features only at the first time step as a bias term.", "startOffset": 158, "endOffset": 702}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space. The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014). Karpathy and Fei-Fei (2015) takes a simpler approach and used a vanilla RNN that incorporates VGG image features only at the first time step as a bias term. Finally Vinyals et al. (2015) trained an ensemble of LSTM in which the image features extracted from a batch-normalized GoogLeNet (Ioffe and Szegedy, 2015) are presented to the LSTM sentence generator as the first input, before the special start word.", "startOffset": 158, "endOffset": 861}, {"referenceID": 1, "context": "These image features are generally extracted from powerful state-of-theart CNN architectures trained on large scale image classification tasks like ImageNet (Deng et al., 2009). Mao et al. (2015), Karpathy and Fei-Fei (2015), Vinyals et al. (2015) proposed a multimodal RNN which differs in the selection and integration of image features: Mao et al. (2015) made use of a multimodal layer which fuses image features, current word embedding and current hidden state of the RNN into a common multimodal space. The image features experimented by the authors are extracted from two different CNNs namely AlexNet (Krizhevsky et al., 2012) and VGG (Simonyan and Zisserman, 2014). Karpathy and Fei-Fei (2015) takes a simpler approach and used a vanilla RNN that incorporates VGG image features only at the first time step as a bias term. Finally Vinyals et al. (2015) trained an ensemble of LSTM in which the image features extracted from a batch-normalized GoogLeNet (Ioffe and Szegedy, 2015) are presented to the LSTM sentence generator as the first input, before the special start word. Different from the previously cited works, Xu et al. (2015) applied the attention mechanism over convolutional image features of size 14x14x512 extracted from VGG, which makes the image context a collection of feature maps instead of a single vector.", "startOffset": 158, "endOffset": 1143}, {"referenceID": 0, "context": "This model is similar to the attentional NMT introduced by Bahdanau et al. (2014) except that the source word annotations produced by the encoder are replaced by the convolutional features.", "startOffset": 59, "endOffset": 82}, {"referenceID": 0, "context": "This model is similar to the attentional NMT introduced by Bahdanau et al. (2014) except that the source word annotations produced by the encoder are replaced by the convolutional features. Another attention based model proposed by You et al. (2016) introduced two separate attention mechanisms, input and output attention models, that are applied over a set of visual attributes detected using different methods like k-NN and neural networks.", "startOffset": 59, "endOffset": 250}, {"referenceID": 24, "context": "(Szegedy et al., 2015) is only used as the initial input to the RNN.", "startOffset": 0, "endOffset": 22}, {"referenceID": 3, "context": "In the context of multimodality, Elliott et al. (2015) explore the effectiveness of conditioning a target language model on the image features from the last fully-connected layer of VGG and on the features from a source language model using the IAPR-TC12 multilingual image captioning dataset.", "startOffset": 33, "endOffset": 55}, {"referenceID": 3, "context": "Dong et al. (2015) proposed a multi-target NMT that translates a single source language into several target languages.", "startOffset": 0, "endOffset": 19}, {"referenceID": 3, "context": "Dong et al. (2015) proposed a multi-target NMT that translates a single source language into several target languages. Specifically, they made use of a single encoder and multiple language-specific decoders each embedded with its own attention mechanism. Firat et al. (2016) extended this idea into a multi-way multilingual NMT which can translate between multiple languages using a single attention mechanism shared across all the languages.", "startOffset": 0, "endOffset": 275}, {"referenceID": 3, "context": "Dong et al. (2015) proposed a multi-target NMT that translates a single source language into several target languages. Specifically, they made use of a single encoder and multiple language-specific decoders each embedded with its own attention mechanism. Firat et al. (2016) extended this idea into a multi-way multilingual NMT which can translate between multiple languages using a single attention mechanism shared across all the languages. Luong et al. (2015a) experimented with one-to-many, many-to-one and many-to-many schemes in order to quantify the mutual benefit of several NLP, MT and CV tasks to each other.", "startOffset": 0, "endOffset": 464}, {"referenceID": 1, "context": ", yM ), yj \u2208 R (2) A bi-directional GRU (Cho et al., 2014) encoder with D hidden units reads the input X sequentially in forwards and backwards to produce two sets of hidden states based on the current source word embedding and the previous hidden state of the encoder.", "startOffset": 40, "endOffset": 58}, {"referenceID": 6, "context": "In the context of multi-way multilingual NMT, Firat et al. (2016) benefit from a single attention mechanism shared across different language pairs.", "startOffset": 46, "endOffset": 66}, {"referenceID": 5, "context": "1 Dataset We used the Multi30K dataset (Elliott et al., 2016) which is an extended version of the Flickr30K Entities dataset (Young et al.", "startOffset": 39, "endOffset": 61}, {"referenceID": 29, "context": ", 2016) which is an extended version of the Flickr30K Entities dataset (Young et al., 2014).", "startOffset": 71, "endOffset": 91}, {"referenceID": 8, "context": "For the image part, convolutional image features of size 14x14x2014 are extracted from the res4f relu layer of ResNet-50 CNN (He et al., 2016) trained on ImageNet.", "startOffset": 125, "endOffset": 142}, {"referenceID": 12, "context": "We used Adam (Kingma and Ba, 2014) as the stochastic gradient descent variant with a minibatch size of 32.", "startOffset": 13, "endOffset": 34}, {"referenceID": 7, "context": "The weights of the networks are initialized using Xavier scheme (Glorot and Bengio, 2010) while the biases are initially set to 0.", "startOffset": 64, "endOffset": 89}, {"referenceID": 21, "context": "The performance of the networks is evaluated on the first validation split using BLEU (Papineni et al., 2002) at the end of each epoch and the training is stopped if BLEU does not improve for 20 evaluation periods.", "startOffset": 86, "endOffset": 109}, {"referenceID": 15, "context": "1 Quantitative Analysis The description generation performance of the models is presented in Table 2 using BLEU, METEOR (Lavie and Agarwal, 2007) and CIDEr-D (Vedantam et al.", "startOffset": 120, "endOffset": 145}, {"referenceID": 25, "context": "1 Quantitative Analysis The description generation performance of the models is presented in Table 2 using BLEU, METEOR (Lavie and Agarwal, 2007) and CIDEr-D (Vedantam et al., 2015) as automatic evaluation metrics.", "startOffset": 158, "endOffset": 181}, {"referenceID": 27, "context": "Here we adopt the approach proposed by Xu et al. (2015) which upsamples the attention weights with a factor of 16 in order to lay them over the original image as white smoothed regions.", "startOffset": 39, "endOffset": 56}, {"referenceID": 6, "context": "The failure of the shared attention which contrasts with Firat et al. (2016) can be attributed to a representational discrepancy between convolutional image features and the source side textual annotations.", "startOffset": 57, "endOffset": 77}], "year": 2016, "abstractText": "The attention mechanism is an important part of the neural machine translation (NMT) where it was reported to produce richer source representation compared to fixed-length encoding sequence-to-sequence models. Recently, the effectiveness of attention has also been explored in the context of image captioning. In this work, we assess the feasibility of a multimodal attention mechanism that simultaneously focus over an image and its natural language description for generating a description in another language. We train several variants of our proposed attention mechanism on the Multi30k multilingual image captioning dataset. We show that a dedicated attention for each modality achieves up to 1.6 points in BLEU and METEOR compared to a textual NMT baseline.", "creator": "TeX"}}}