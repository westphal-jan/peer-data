{"id": "1609.04186", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Neural Machine Translation with Supervised Attention", "abstract": "The attention mechanisim is appealing for neural machine translation, since it is able to dynam- ically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of re- ordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the super- vised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.", "histories": [["v1", "Wed, 14 Sep 2016 09:31:40 GMT  (261kb,D)", "http://arxiv.org/abs/1609.04186v1", "This paper was submitted into COLING2016 on July 10, and it is under review"]], "COMMENTS": "This paper was submitted into COLING2016 on July 10, and it is under review", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lemao liu", "masao utiyama", "rew finch", "eiichiro sumita"], "accepted": false, "id": "1609.04186"}, "pdf": {"name": "1609.04186.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Translation with Supervised Attention", "authors": ["Lemao Liu", "Masao Utiyama", "Andrew Finch", "Eiichiro Sumita"], "emails": ["lmliu@nict.go.jp", "first.last@nict.go.jp"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that you are able to play by the rules that you had in the past and that you are able to play by the rules that you had in the past."}, {"heading": "3 Supervised Attention", "text": "In this section, we present Monitored Attention to Improve Alignment, resulting in better translation performance for NMT. Our basic idea is simple: similar to traditional SMT, it first uses a conventional aligner to maintain alignment on the training corpus, and then uses these alignment results as monitoring to train the NMT. During the test, decoding is the same as standard NMT, as there is no alignment monitoring available for invisible test sets."}, {"heading": "3.1 Preprocessing Alignment Supervision", "text": "As described in \u00a7 2, the attention model outputs a soft alignment \u03b1, so \u03b1t is a normalized probability distribution. In contrast, most aligners typically orient themselves to the grammar induction for conventional SMTs and typically output \"hard\" alignments, such as (Och and Ney, 2000). They only specify whether a target word is aligned with a source word or not, and this might not coincide with a distribution for each target word. For example, a target word may align with multiple source words, or no source words at all. Therefore, we apply the following heuristics to prepare the hard alignment: If a target word is not aligned with any source word, we inherit its affiliation from the closest aligned word with preference for the following right one (Devlin et al., 2014); if a target word is aligned with multiple source words, we assume that it aligns with each one."}, {"heading": "3.2 Jointly Supervising Translation and Attention", "text": "We propose a method of soft constraints to jointly monitor the disappearing problem and attention, as follows: \u2212 \u2211 i log p (yi | xi; \u03b8) + \u03bb \u00b7 \u0445 (\u03b1i, \u03b1 \u00b2 i; \u03b8) (4), where \u03b1i is defined as in Equation. (1), \u0445 is a loss function that punishes the discrepancy between \u03b1i and \u03b1 \u00b2 i, and this differs substantially from the standard NMT as shown in Figure 1 (a). Note that this training goal is similar to that of multitaskic learning (Evgeniou and Pontil, 2004). Our supervised attention method has two other advantages: firstly, it is able to mitigate overadjustment through prophylaxis; and, secondly, it is able to address a disappearing problem (Evgeniou and Pontil, 2004). Our supervised attention method has two other advantages: firstly, it is able to mitigate overadjustment through prophylaxis; secondly, it is able to adapt it through overadaptation to the distribution; and secondly, it is unpleasant through overadaptation to the distribution."}, {"heading": "4 Experiments", "text": "We conducted experiments on two translation tasks from Chinese to English: one is the NIST task, which is based on the NEWS domain, which is a large-scale task and suitable for the NMT; and the other is the language translation, which is travel-oriented, which is a low-resource task and therefore a big challenge for the NMT. We used the case-insensitive BLEU4 to evaluate the quality of the translation and adopted the multi-bleu.perl as the implementation."}, {"heading": "4.1 The Large Scale Translation Task", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1.1 Preparation", "text": "We used data from the NIST2008 Open Machine Translation Campaign. The training data consisted of 1.8 million sentence pairs, the development set was nest02 (878 sentences), and the test sets were nest05 (1082 sentences), nest06 (1664 sentences), and nest08 (1357 sentences).We compared the proposed approach to three strong baselines: \u2022 Moses: a phrase-based machine translation system (Koehn et al., 2007); \u2022 NMT1: an attention based on NMT (Bahdanau et al., 2015) system at https: / github.com / lisagroundhog / lisagroundhog / GroundHog; \u2022 NMT2: another implementation of (Bahdanau et al., 2015) at http ps: / github.com / nyu-dl / dl4mt-tutorial. We developed the proposed approach based on NMT2, and named it as NMT2 growth + MTT.We followed the standard for implementing it."}, {"heading": "4.1.2 Settings on External Alignments", "text": "To study their behavior on the development set, we used the GIZA + + to generate the alignment on the training set prior to training SA-NMT. In Table 1, we see that MUL is better than MSE. In addition, CE performs best under all losses and therefore uses it for the following experiments. In addition, we also perform quick alignments to generate alignments as supervision for SA-NMT, and the results were reported in Table 2. We can see that GIZA + + performs slightly better than the quick alignment, so we set the external aligner as GIZA + + in the following experiments."}, {"heading": "4.1.3 Results on Large Scale Translation Task", "text": "Figure 2 shows the learning curves of NMT2 and SA-NMT on the development scale. On the other hand, SA-NMT provides much better BLEU for the incipient updates and performs more consistent results along with the updates, although more updates are needed to reach the climax. 4This excludes all sentences longer than 50 words on the source or destination page only for NMT systems, but for Moses we use the entire training data. Table 3 reports on the most important end-to-end translation results for the big task. We find that both standard NMT generally exceed Moses, except NMT1 on Nist05. The proposed SA-NMT achieves significant and consistent improvements across all three base systems, and it achieves the average gains of 2.2 BLEU points on the test categories via its direct base line NMT2."}, {"heading": "4.1.4 Results and Analysis on Alignment", "text": "As explained in Section 2, standard NMT + + cannot use the target word information to predict their aligned source words, and therefore could not predict the correct source words for some target words. For example, for the sentence in training in Figure 3 (a), NMT2 can use aligned \"following\" to \"although this word is relatively easy to align correctly (Glosse: Pinochet)\" instead of \"(Glosse: follow),\" and even worse it is aligned the word \"rather than.\" Although this word is relatively easy to align correctly. \"In contrast, with the help of information from the target word itself, GIZA + + + aligned successfully both\" following \"and.\" to the expected source words (see Figure 3 (c). With the alignment it results from GIZA + + + as an oversight, we can see that our SA-NMMMT is classified as an oversight."}, {"heading": "4.2 Results on the Low Resource Translation Task", "text": "For the low-resource translation task, we used the BTEC corpus as training data, which consisted of 30k sentence pairs with 0.27m Chinese words and 0.33m English words. We used the CSTAR03 and IWSLT04 sentences, respectively, as development and test sentences, which held each other. We trained a 4 gram language model on the target side of the training corpus to run Moses. For training all NMT systems, we used the same settings as in the large task, except that the vocabulary size is 6000, the stack size is 16, and the hyperparameter = 1 for SA-NMT.Table 5 reports on the final results. First, we can see that both standard systems for neuronal machine translation systems NMT1 and NMT2 are much worse than Moses with a significant gap. This result is not difficult to understand: Neural network systems typically require sufficient data to perform SO-SO-SO SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-O-SO-O-SO-SO-O-O-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-O-SO-SO-SO-SO-SO-SO-O-O-SO-O-SO-O-SO-SO-O-SO-O-O-SO-SO-O-SO-SO-SO-SO-SO-SO-O-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-SO-O-O-SO-SO-"}, {"heading": "5 Related Work", "text": "Many recent papers have led to remarkable improvements in the attention mechanism for neural machine translation. Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over- and undertranslation inherent in the NMT. Feng et al. (2016) proposed an additional, recurring structure for attention to capture long-term dependencies. Cheng et al. (2016) proposed an agreement-based bi-directional NMT model for symmetrical alignment. Cohn et al. (2016) integrated several structural alignment distortions in attention learning to achieve better alignment. All improved the attention models learned in an uncontrolled manner. While we do not modify the attention model itself, we learn it in a monitored way, so our approach is orthogonal to them. It has always been standard practice to rearrange word models from either conventional to phrational or phratical-based."}, {"heading": "6 Conclusion", "text": "It has been shown that the attention mechanism in the NMT is inferior in its alignment accuracy compared to traditional word alignment models. First, this paper provides an explanation by looking at the attention mechanism from the perspective of reordering, and then proposes supervised attention to the NMT, guided by external conventional alignment models inspired by the supervised reordering models in conventional SMT. Experiments on two Sino-English translation tasks show that the proposed approach achieves better alignment results, leading to significant gains over the standard attention-based NMT."}, {"heading": "Acknowledgements", "text": "We would like to thank Xugang Lu for the valuable discussions about this work."}], "references": [{"title": "Incorporating discrete translation lexicons into neural machine translation. CoRR, abs/1606.02006", "author": ["Arthur et al.2016] Philip Arthur", "Graham Neubig", "Satoshi Nakamura"], "venue": null, "citeRegEx": "Arthur et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arthur et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "A survey of word reordering in statistical machine translation: Computational models and language phenomena", "author": ["Bisazza", "Federico2016] Arianna Bisazza", "Marcello Federico"], "venue": "Computational Linguistics,", "citeRegEx": "Bisazza et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bisazza et al\\.", "year": 2016}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["Brown et al.1993] Peter F. Brown", "Vincent J. Della Pietra", "Stephen A. Della Pietra", "Robert L. Mercer"], "venue": null, "citeRegEx": "Brown et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Brown et al\\.", "year": 1993}, {"title": "Agreement-based joint training for bidirectional attention-based neural machine translation", "author": ["Cheng et al.2016] Yong Cheng", "Shiqi Shen", "Zhongjun He", "Wei He", "Hua Wu", "Maosong Sun", "Yang Liu"], "venue": "Proceedings of IJCAI", "citeRegEx": "Cheng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555", "author": ["Chung et al.2014] Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Incorporating structural alignment biases into an attentional neural translation model", "author": ["Cohn et al.2016] Trevor Cohn", "Cong Duy Vu Hoang", "Ekaterina Vymolova", "Kaisheng Yao", "Chris Dyer", "Gholamreza Haffari"], "venue": "Proceedings of NAACL-HLT", "citeRegEx": "Cohn et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cohn et al\\.", "year": 2016}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Devlin et al.2014] Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "Proceedings of ACL", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "A simple, fast, and effective reparameterization of ibm model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Regularized multi\u2013task learning", "author": ["Evgeniou", "Pontil2004] Theodoros Evgeniou", "Massimiliano Pontil"], "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "Evgeniou et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Evgeniou et al\\.", "year": 2004}, {"title": "Implicit distortion and fertility models for attention-based encoder-decoder NMT model. CoRR, abs/1601.03317", "author": ["Feng et al.2016] Shi Feng", "Shujie Liu", "Mu Li", "Ming Zhou"], "venue": null, "citeRegEx": "Feng et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2016}, {"title": "Moses: open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] P. Koehn", "H. Hoang", "A. Birch", "C. Callison-Burch", "M. Federico", "N. Bertoldi", "B. Cowan", "W. Shen", "C. Moran", "R. Zens", "C. Dyer", "O. Bojar", "A. Constantin", "E. Herbst"], "venue": "Proceedings of ACL: Demonstrations", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Theory of Point Estimation", "author": ["Lehmann", "Casella1998] E.L. Lehmann", "G. Casella"], "venue": null, "citeRegEx": "Lehmann et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Lehmann et al\\.", "year": 1998}, {"title": "A neural reordering model for phrase-based translation", "author": ["Li et al.2014] Peng Li", "Yang Liu", "Maosong Sun", "Tatsuya Izuha", "Dakun Zhang"], "venue": "In Proceedings of COLING", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Alignment by agreement", "author": ["Liang et al.2006] Percy Liang", "Ben Taskar", "Dan Klein"], "venue": "In Proceedings of HLT-NAACL", "citeRegEx": "Liang et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Liang et al\\.", "year": 2006}, {"title": "Contrastive unsupervised word alignment with non-local features", "author": ["Liu", "Sun2015] Yang Liu", "Maosong Sun"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Stanford neural machine translation systems for spoken language domains", "author": ["Luong", "Manning2015] Minh-Thang Luong", "Christopher D. Manning"], "venue": "In Proceedings of IWSLT", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Neural reranking improves subjective quality of machine translation: NAIST at WAT2015", "author": ["Neubig et al.2015] Graham Neubig", "Makoto Morishita", "Satoshi Nakamura"], "venue": "In Proceedings of the 2nd Workshop on Asian Translation", "citeRegEx": "Neubig et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neubig et al\\.", "year": 2015}, {"title": "Improved statistical alignment models", "author": ["Och", "Ney2000] Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of ACL,", "citeRegEx": "Och et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Och et al\\.", "year": 2000}, {"title": "The Cross Entropy Method: A Unified Approach To Combinatorial Optimization, Monte-carlo Simulation (Information Science and Statistics)", "author": ["Rubinstein", "Kroese2004] Reuven Y. Rubinstein", "Dirk P. Kroese"], "venue": null, "citeRegEx": "Rubinstein et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Rubinstein et al\\.", "year": 2004}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V. Le"], "venue": "In Proceedings of NIPS", "citeRegEx": "Sutskever et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of Computer Vision and Pattern Recognition (CVPR)", "citeRegEx": "Szegedy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "Recurrent neural networks for word alignment model", "author": ["Taro Watanabe", "Eiichiro Sumita"], "venue": "In Proceedings of ACL", "citeRegEx": "Tamura et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tamura et al\\.", "year": 2014}, {"title": "Modeling coverage for neural machine translation", "author": ["Tu et al.2016] Zhaopeng Tu", "Zhengdong Lu", "Yang Liu", "Xiaohua Liu", "Hang Li"], "venue": "In Proceedings of ACL", "citeRegEx": "Tu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2016}, {"title": "Maximum entropy based phrase reordering model for statistical machine translation", "author": ["Xiong et al.2006] Deyi Xiong", "Qun Liu", "Shouxun Lin"], "venue": "In Proceedings of ACL", "citeRegEx": "Xiong et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2006}, {"title": "Word alignment modeling with context dependent deep neural network", "author": ["Yang et al.2013] Nan Yang", "Shujie Liu", "Mu Li", "Ming Zhou", "Nenghai Yu"], "venue": "In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}, {"title": "ADADELTA: an adaptive learning rate method. CoRR", "author": ["Matthew D. Zeiler"], "venue": null, "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Learning local word reorderings for hierarchical phrase-based statistical machine translation", "author": ["Zhang et al.2016] Jingyi Zhang", "Masao Utiyama", "Eiichiro Sumita", "Hai Zhao", "Graham Neubig", "Satoshi Nakamura"], "venue": "Machine Translation", "citeRegEx": "Zhang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 1, "context": "1 Introduction Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015).", "startOffset": 115, "endOffset": 162}, {"referenceID": 21, "context": "1 Introduction Neural Machine Translation (NMT) has achieved great successes on machine translation tasks recently (Bahdanau et al., 2015; Sutskever et al., 2015).", "startOffset": 115, "endOffset": 162}, {"referenceID": 1, "context": "Among different variants of NMT, attention based NMT, which is the focus of this paper, is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 140, "endOffset": 183}, {"referenceID": 16, "context": "Among different variants of NMT, attention based NMT, which is the focus of this paper, is attracting increasing interests in the community (Bahdanau et al., 2015; Luong et al., 2015).", "startOffset": 140, "endOffset": 183}, {"referenceID": 8, "context": "However, it differs from conventional alignment models that are able to use the target word to infer its alignments (Och and Ney, 2000; Dyer et al., 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-toEnglish as reported in (Cheng et al.", "startOffset": 116, "endOffset": 173}, {"referenceID": 4, "context": ", 2013; Liu and Sun, 2015), and as a result there is a substantial gap in quality between the alignments derived by this attention based NMT and conventional alignment models (54 VS 30 in terms of AER for Chinese-toEnglish as reported in (Cheng et al., 2016)).", "startOffset": 238, "endOffset": 258}, {"referenceID": 8, "context": "Specifically, similar to conventional SMT, we first run off-theshelf aligners (GIZA++ (Och and Ney, 2000) or fast align (Dyer et al., 2013) etc.", "startOffset": 120, "endOffset": 139}, {"referenceID": 22, "context": "Furthermore, since the supervision of attention lies in the middle of the entire network architecture rather than the top ( as in the supervision of translation (see Figure 1(b)), it serves to mitigate the vanishing gradient problem during the back-propagation (Szegedy et al., 2015).", "startOffset": 261, "endOffset": 283}, {"referenceID": 1, "context": "The proposed approach is simple and easy to be implemented, and it is generally applicable to any attention-based NMT models, although in this case it is implemented on top of the model in (Bahdanau et al., 2015).", "startOffset": 189, "endOffset": 212}, {"referenceID": 5, "context": "a Gated Recurrent Unit (Chung et al., 2014); and the context vector ct is a dynamical source representation at timestep t, and calculated as the weighted sum of source encodingsEx, i.", "startOffset": 23, "endOffset": 43}, {"referenceID": 1, "context": "We skip the detailed definitions of a together with Ex, f and g, and refer the readers to (Bahdanau et al., 2015) instead.", "startOffset": 90, "endOffset": 113}, {"referenceID": 3, "context": "p(\u03b1 | x,y) = exp(F (x,y, \u03b1)) \u2211 a exp(F (x,y, \u03b1)) where F denotes either a log-probability log p(y, \u03b1 | x) for a generative model like IBM models (Brown et al., 1993) or a feature function for discriminative models (Liu and Sun, 2015).", "startOffset": 145, "endOffset": 165}, {"referenceID": 4, "context": "As a result, the attention based NMT might not deliver satisfying alignments, as reported in (Cheng et al., 2016), compared to conventional alignment models.", "startOffset": 93, "endOffset": 113}, {"referenceID": 7, "context": "Therefore, we apply the following heuristics to preprocess the hard alignment: if a target word does not align to any source words, we inherit its affiliation from the closest aligned word with preference given to the right, following (Devlin et al., 2014); if a target word is aligned to multiple source words, we assume it aligns to each one evenly.", "startOffset": 235, "endOffset": 256}, {"referenceID": 14, "context": "MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016).", "startOffset": 99, "endOffset": 139}, {"referenceID": 4, "context": "MUL is particularly designed for agreement in word alignment and it has been shown to be effective (Liang et al., 2006; Cheng et al., 2016).", "startOffset": 99, "endOffset": 139}, {"referenceID": 4, "context": "Note that different from those in (Cheng et al., 2016), \u03b1\u0302 is not a parametrized variable but a constant in this paper.", "startOffset": 34, "endOffset": 54}, {"referenceID": 11, "context": "We compared the proposed approach with three strong baselines: \u2022 Moses: a phrase-based machine translation system (Koehn et al., 2007);", "startOffset": 114, "endOffset": 134}, {"referenceID": 1, "context": "\u2022 NMT1: an attention based NMT (Bahdanau et al., 2015) system at https://github.", "startOffset": 31, "endOffset": 54}, {"referenceID": 1, "context": "com/lisagroundhog/GroundHog; \u2022 NMT2: another implementation of (Bahdanau et al., 2015) at https://github.", "startOffset": 63, "endOffset": 86}, {"referenceID": 1, "context": "Specifically, except the stopping iteration which was selected using development data, we used the default settings set out in (Bahdanau et al., 2015) for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, 4 the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by (Zeiler, 2012).", "startOffset": 127, "endOffset": 150}, {"referenceID": 27, "context": ", 2015) for all NMT-based systems: the dimension of word embedding was 620, the dimension of hidden units was 1000, the batch size was 80, the source and target side vocabulary sizes were 30000, the maximum sequence length was 50, 4 the beam size for decoding was 12, and the optimization was done by Adadelta with all hyper-parameters suggested by (Zeiler, 2012).", "startOffset": 349, "endOffset": 363}, {"referenceID": 26, "context": "(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014).", "startOffset": 165, "endOffset": 205}, {"referenceID": 23, "context": "(4) to bias the training objective towards word alignment task, or we can change the architecture slightly to add the target information crucial for alignment as in (Yang et al., 2013; Tamura et al., 2014).", "startOffset": 165, "endOffset": 205}, {"referenceID": 18, "context": "Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work.", "startOffset": 65, "endOffset": 105}, {"referenceID": 6, "context": "Our method is possible to advance Moses by using reranking as in (Neubig et al., 2015; Cohn et al., 2016), but it is beyond the scope of this paper and instead we remain it as future work.", "startOffset": 65, "endOffset": 105}, {"referenceID": 0, "context": "While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size.", "startOffset": 186, "endOffset": 207}, {"referenceID": 0, "context": "While our SA-NMT does not advance the state-of-the-art Moses as in large scale translation, this is a strong result if we consider that previous works on low resource translation tasks: Arthur et al. (2016) gained over Moses on the Japanese-to-English BTEC corpus, but they resorted to a corpus consisting of 464k sentence pairs; Luong and Manning (2015) revealed the comparable performance to Moses on English-to-Vietnamese with 133k sentences pairs, which is more than 4 times of our corprus size.", "startOffset": 186, "endOffset": 355}, {"referenceID": 28, "context": "At the word level, Bisazza and Federico (2016) surveyed many word reordering models learned from alignment models for SMT, and in particular there are some neural network based reordering models, such as (Zhang et al., 2016).", "startOffset": 204, "endOffset": 224}, {"referenceID": 19, "context": "Tu et al. (2016) introduced an explicit coverage vector into the attention mechanism to address the over-translation and under-translation inherent in NMT.", "startOffset": 0, "endOffset": 17}, {"referenceID": 8, "context": "Feng et al. (2016) proposed an additional recurrent structure for attention to capture long-term dependencies.", "startOffset": 0, "endOffset": 19}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment.", "startOffset": 0, "endOffset": 20}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment.", "startOffset": 0, "endOffset": 119}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs. It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al.", "startOffset": 0, "endOffset": 621}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs. It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al.", "startOffset": 0, "endOffset": 698}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs. It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al. (2014) proposed a neural network method to learn a BTG reordering model.", "startOffset": 0, "endOffset": 785}, {"referenceID": 4, "context": "Cheng et al. (2016) proposed an agreement-based bidirectional NMT model for symmetrizing alignment. Cohn et al. (2016) incorporated multiple structural alignment biases into attention learning for better alignment. All of them improved the attention models that were learned in an unsupervised manner. While we do not modify the attention model itself, we learn it in a supervised manner, therefore our approach is orthogonal to theirs. It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, Koehn et al. (2007) proposed a lexicalized MSD model for phrasal reordering; Xiong et al. (2006) proposed a feature-rich model to learn phrase reordering for BTG; and Li et al. (2014) proposed a neural network method to learn a BTG reordering model. At the word level, Bisazza and Federico (2016) surveyed many word reordering models learned from alignment models for SMT, and in particular there are some neural network based reordering models, such as (Zhang et al.", "startOffset": 0, "endOffset": 898}], "year": 2016, "abstractText": "The attention mechanisim is appealing for neural machine translation, since it is able to dynamically encode a source sentence by generating a alignment between a target word and source words. Unfortunately, it has been proved to be worse than conventional alignment models in aligment accuracy. In this paper, we analyze and explain this issue from the point view of reordering, and propose a supervised attention which is learned with guidance from conventional alignment models. Experiments on two Chinese-to-English translation tasks show that the supervised attention mechanism yields better alignments leading to substantial gains over the standard attention based NMT.", "creator": "LaTeX with hyperref package"}}}