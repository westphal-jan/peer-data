{"id": "1609.04243", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Convolutional Recurrent Neural Networks for Music Classification", "abstract": "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with two CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.", "histories": [["v1", "Wed, 14 Sep 2016 12:52:08 GMT  (100kb,D)", "http://arxiv.org/abs/1609.04243v1", "5 pages, ICASSP 2017 submitted"], ["v2", "Thu, 3 Nov 2016 07:50:14 GMT  (112kb,D)", "http://arxiv.org/abs/1609.04243v2", "5 pages, ICASSP 2017 submitted. Revised to fix previous CNN architectures and update experiment results"], ["v3", "Wed, 21 Dec 2016 06:52:30 GMT  (112kb,D)", "http://arxiv.org/abs/1609.04243v3", "5 pages, ICASSP 2017 submitted. Revised to fix previous CNN architectures and update experiment results"]], "COMMENTS": "5 pages, ICASSP 2017 submitted", "reviews": [], "SUBJECTS": "cs.NE cs.LG cs.MM cs.SD", "authors": ["keunwoo choi", "george fazekas", "mark sandler", "kyunghyun cho"], "accepted": false, "id": "1609.04243"}, "pdf": {"name": "1609.04243.pdf", "metadata": {"source": "CRF", "title": "CONVOLUTIONAL RECURRENT NEURAL NETWORKS FOR MUSIC CLASSIFICATION", "authors": ["Keunwoo Choi", "Gy\u00f6rgy Fazekas", "Mark Sandler", "Kyunghyun Cho"], "emails": ["keunwoo.choi@qmul.ac.uk", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": null, "text": "CRNNs use Convolutionary Neural Networks (CNNs) to extract local characteristics and recursive neural networks to summarize the time of the extracted characteristics. We compared CRNNNs to two CNN structures used for music marking while controlling the number of parameters in terms of performance and training time per sample. Overall, we found that CRNNNs perform strongly in terms of number of parameters and training time, indicating the effectiveness of their hybrid structure in music function extraction and summary."}, {"heading": "1. INTRODUCTION", "text": "In fact, most of us are able to abide by the rules we have set ourselves in order to comply with them, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \"He added,\" I don't think we will be able to put the world in order. \""}, {"heading": "2. MODELS", "text": "We compare CRNN with Conv1D and Conv2D, all of which are shown in Figure 1. Specifications are given in Table 1. For all networks, input is assumed to be 96 x 1366 (Mel frequency band \u00d7 timeframe). Sigmoid functions are used as activation at output nodes, because music marking is a multi-stage classification task. For a correct comparison, it is necessary to equip the networks with identical optimization techniques, since they significantly improve the performance of networks with essentially the same structure. Thus, for example, techniques such as Batch Normalization (BN) [10] and the exponential linear unit (ELU) [11] improve both training speed and accuracy by initiating the distribution of outputs of intermediate layers without changing the structure. In this paper, therefore, all the evolutionary and fully-working Xiv: 160 9.04 243v 1 [cs.N] sequence of 12 layers are equipped with an identical activation function, and all of 11 layers are equipped with an identical activation function."}, {"heading": "2.1. Conv1D", "text": "Conv1D in Figure 1a is motivated by structures for music marking [1] and genre classification [13]. The network consists of 4 revolutionary layers, followed by 2 completely connected layers. One-dimensional convolutionary layers (1 \u00d7 8 for all, i.e. folding along the timeline) and maxpooling layers ((((1 \u00d7 4) - (1 \u00d7 5) - (1 \u00d7 8) - (1 \u00d7 8))) alternate, resulting in narrow and high characteristic charts. They are flattened and fed into a fully connected layer that functions as a classification. This model is based on the assumption of instationarity along the frequency axis of music spectrograms. It preserves the frequency band information until the end of the revolutionary layers. In other words, each element of the last characteristic card (the output of the 4th sub-sampling layer) encodes a characteristic for each less than one sequencer [1-30er will be inserted directly into the last one]."}, {"heading": "2.2. Conv2D", "text": "CNN structures with two-dimensional folding have been used for music marking [2] and vocal / instrumental classification [14]. Conv2D consists of five convolutionary layers of 3 x 3 cores and layers with maximum bundling (((2 x 4) - (2 x 4) - (3 x 5) - (4 x 4))) as shown in Figure 1b. The network reduces the size of feature maps to 1 x 1 in the last layer, where each feature covers the entire input and not every frequency band. Time information is also gradually aggregated during size reduction. This model assumes that there are two-dimensional patterns in the time-frequency plane. In addition, the use of two-dimensional subsampling allows the network to be fully conditional, ultimately resulting in fewer parameters.1http: / / benanne.github.io / 2014 / 08 / 05 / spotify-cnns.htlThis part of the network is often fully responsible for the parameters."}, {"heading": "2.3. CRNN", "text": "CRNN uses a two-layer RNN with gated recurrent units (GRU) [15] to summarize temporal patterns on top of two-dimensional 4-layer CNNs, as shown in Figure 1c. Underlying this model is the assumption that the temporal pattern can be better aggregated with RNNNs than with CNNs, while relying on CNNs on the input side for the extraction of local features, a structure first proposed in [7] for the classification of documents. CRNN uses RNNs to aggregate temporal patterns rather than, for example, calculating the results from shorter segments as in [1] or folding and subsampling as in Conv2D. In its CNN substructure, the sizes of coil layers and max pooling layers are 3 x 3 and (2 x 2) - (4 x 4) - (4 x 4)."}, {"heading": "2.4. Scaling networks", "text": "The models are scaled with 2% tolerance by controlling the number of parameters to 100,000, 250,000, 0.5 million, 1M, 3M. Taking into account the limitation of the current hardware and the size of the dataset, it is assumed that 3M parameter networks represent an approximate upper limit of structural complexity. Table 1 summarizes the details of the various structures including the layer width (number of characteristic charts or hidden units), the layer widths are based on [1] for Conv1D and [2] for Conv2D. For CRNN, the widths are determined on the basis of preliminary experiments showing the relative importance of the number of characteristic sketches of hidden layers over the number of hidden units in RNNNs. Layer widths are modified to control the number of parameters of a network while the depth and conventitive core shapes are kept constant. Therefore, the hierarchy of learned characteristics is maintained while the number of characteristics in each network is changed (i.e. the hierarchical meaning)."}, {"heading": "3. EXPERIMENTS", "text": "We use the Million Song Dataset [17] with last.fm tags. We train the networks to predict the top 50 tags covering genres (e.g. rock, pop), moods (e.g. sad, happy), instruments (e.g. singer, guitar), and eras (60s-00s). 214,284 (201,680 for training and 12,605 for validation) and 25,940 clips are selected using the original training / test splits and filtering out items without Top50 tags. Tag occurrence ranges from 52,944 (rock) to 1,257 (happy). We use 30-60s preview clips provided after trimming to represent the highlight of the song. We trim audio signals to 29 seconds in the center of the preview clips and take them down from 22.05 kHz to 12 kHz with Librosa."}, {"heading": "3.1. Memory-controlled experiment", "text": "Figure 2 shows the AUCs for each network against the number of parameters. If the number of parameters is equal, the ranking of AUC CRNN > Conv2D > Conv1D is higher than the 3M parameter structures, suggesting that CRNN may be preferred when the bottleneck is in memory usage. Conv2D outperforms CRNN for 3M parameter structures, while both outperform the state-of-the-art structure [2]. CRNN outperforms Conv2D in all cases except for the use of 3M parameters. Since they share the same 2Dconvolutionary layers, this difference is likely a consequence of the difference in RNNNs and CNNs \"ability to summarize the features over time, suggesting that learning a global structure is more important than focusing on local structures for summing."}, {"heading": "3.2. Computation-controlled comparison", "text": "The calculation complexity is directly related to the training and prediction time and varies not only depending on the number of parameters but also on the structure. The training times of the wall clocks for 2500 samples are summarized in Table 1 and in Figure 2.With similar training time, CRNN shows the best performance for the training time < 150s. However, with a training time of 180s (3M parameters), Conv2D surpasses CRNN with similar or longer training times (0.5 M, 1M and 3M parameters), suggesting that either Conv2D or CRNN can be used depending on the target time budget. With the same number of parameters, the training speed is always Conv2D > Conv1D > CRNN. In other words, the speed correlates negatively with the depths of the networks (5, 6 and 20, respectively)."}, {"heading": "3.3. Performance per tag", "text": "Figure 3 illustrates the AUC score of each tag with 1Mparameter structures. Each tag is categorized as one of the genres, moods, instruments and epochs and sorted by AUC within its category. In this categorization, the music tagging task can be considered a multiple task problem that corresponds to four classification tasks in these four categories. CRNN performs better in each day than Conv2D and Conv2D in 47 out of 50 tags. From the perspective of classifying multiple tasks, this result shows that a structure that performs better in one of the four tasks performs best in the other assignments.Although the data set is unbalanced, the popularity of the tag (number of occurrences per day) does not correlate with performance."}, {"heading": "4. CONCLUSIONS", "text": "In the experiment, we controlled the size of the networks by varying the number of parameters to examine two scenarios, a memory-driven and a computer-driven comparison. Our experiments showed that Conv2D and CRNN are comparable on a large number of parameters, but we observed an interesting trade-off between speed and memory on a modest number of parameters. Conv2D runs faster than CRNN across all parameter settings, while CRNN tends to outperform Conv2D on the same number of parameters. Future work will examine RNN-based structures and audio input requirements for deep learning approaches."}, {"heading": "5. REFERENCES", "text": "[1] Sander Dieleman and Benjamin Schrauwen, \"End-to-end learning for music audio networks,\" in Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964-6968. [2] Keunwoo Choi, George Fazekas, and Mark Sandler, \"Automatic tagging using deep convolutional neural networks,\" in International Society of Music Information Retrieval Conference, 2016. [3] Siddharth Sigtia and Simon Dixon, \"Improved music feature learning with deep neural networks,\" in 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). XiXiXiXiXi. [4] Paulo Chiliguano and Gyorgy Fazekas, \"Hybrid music commender using content-based and social information,\" in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)."}], "references": [{"title": "End-toend learning for music audio", "author": ["Sander Dieleman", "Benjamin Schrauwen"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 6964\u20136968.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic tagging using deep convolutional neural networks", "author": ["Keunwoo Choi", "George Fazekas", "Mark Sandler"], "venue": "International Society of Music Information Retrieval Conference. ISMIR, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved music feature learning with deep neural networks", "author": ["Siddharth Sigtia", "Simon Dixon"], "venue": "2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Hybrid music recommender using content-based and social information", "author": ["Paulo Chiliguano", "Gyorgy Fazekas"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 2618\u20132622.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep content-based music recommendation", "author": ["Aaron Van den Oord", "Sander Dieleman", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 2643\u20132651.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Explaining deep convolutional neural networks on music classification", "author": ["Keunwoo Choi", "George Fazekas", "Mark Sandler"], "venue": "arXiv preprint arXiv:1607.02444, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["Duyu Tang", "Bing Qin", "Ting Liu"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1422\u20131432.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional recurrent neural networks: Learning spatial dependencies for image representation", "author": ["Zhen Zuo", "Bing Shuai", "Gang Wang", "Xiao Liu", "Xingxing Wang", "Bing Wang", "Yushi Chen"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2015, pp. 18\u201326.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "An end-to-end neural network for polyphonic piano music transcription", "author": ["Siddharth Sigtia", "Emmanouil Benetos", "Simon Dixon"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 5, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Fast and accurate deep network learning by exponential linear units (elus)", "author": ["Djork-Arn\u00e9 Clevert", "Thomas Unterthiner", "Sepp Hochreiter"], "venue": "arXiv preprint arXiv:1511.07289, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "The Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1929}, {"title": "Unsupervised learning of local features for music classification", "author": ["Jan W\u00fclfing", "Martin Riedmiller"], "venue": "International Society of Music Information Retrieval Conference. ISMIR, 2012, pp. 139\u2013144.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to pinpoint singing voice from weakly labeled examples", "author": ["Jan Schl\u00fcter"], "venue": "International Society of Music Information Retrieval Conference. ISMIR, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "The power of depth for feedforward neural networks", "author": ["Ronen Eldan", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1512.03965, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "The million song dataset", "author": ["Thierry Bertin-Mahieux", "Daniel PW Ellis", "Brian Whitman", "Paul Lamere"], "venue": "Proceedings of the 12th International Society for Music Information Retrieval Conference, Miami, Florida, USA, October 24-28, 2011, 2011.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "librosa: Audio and music signal analysis in python", "author": ["Brian McFee", "Colin Raffel", "Dawen Liang", "Daniel PW Ellis", "Matt McVicar", "Eric Battenberg", "Oriol Nieto"], "venue": "Proceedings of the 14th Python in Science Conference, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet"], "venue": "GitHub repository: https://github. com/fchollet/keras, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["The Theano Development Team", "Rami Al-Rfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Fr\u00e9d\u00e9ric Bastien", "Justin Bayer", "Anatoly Belikov"], "venue": "arXiv preprint arXiv:1605.02688, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 122, "endOffset": 128}, {"referenceID": 1, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 122, "endOffset": 128}, {"referenceID": 2, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 151, "endOffset": 157}, {"referenceID": 3, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 151, "endOffset": 157}, {"referenceID": 4, "context": "Convolutional neural networks (CNNs) have been actively used for various music classification tasks such as music tagging [1, 2], genre classification [3, 4], and user-item latent feature prediction for recommendation [5].", "startOffset": 218, "endOffset": 221}, {"referenceID": 5, "context": ", percussive instrument patterns) [6].", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "This structure was first proposed in [7] for document classification and later applied to image classification [8] and music transcription [9].", "startOffset": 37, "endOffset": 40}, {"referenceID": 7, "context": "This structure was first proposed in [7] for document classification and later applied to image classification [8] and music transcription [9].", "startOffset": 111, "endOffset": 114}, {"referenceID": 8, "context": "This structure was first proposed in [7] for document classification and later applied to image classification [8] and music transcription [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 9, "context": "For example, techniques such as batch normalization (BN) [10] and exponential linear unit (ELU) [11] improve both training speed and accuracy by nudging the distribution of the outputs of intermediate layers to be standardised without changing the structure.", "startOffset": 57, "endOffset": 61}, {"referenceID": 10, "context": "For example, techniques such as batch normalization (BN) [10] and exponential linear unit (ELU) [11] improve both training speed and accuracy by nudging the distribution of the outputs of intermediate layers to be standardised without changing the structure.", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "5 [12], batch normalization [10], and ELU activation function [11].", "startOffset": 2, "endOffset": 6}, {"referenceID": 9, "context": "5 [12], batch normalization [10], and ELU activation function [11].", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "5 [12], batch normalization [10], and ELU activation function [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Conv1D in Figure 1a is motivated by structures for music tagging [1] and genre classification [13].", "startOffset": 65, "endOffset": 68}, {"referenceID": 12, "context": "Conv1D in Figure 1a is motivated by structures for music tagging [1] and genre classification [13].", "startOffset": 94, "endOffset": 98}, {"referenceID": 0, "context": "In [1], 29s-long signal are trimmed into less than 4-sec subsegments, and then the final tag predictions are averaged.", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "CNN structures with two-dimensional convolution have been used in music tagging [2] and vocal/instrumental classification [14].", "startOffset": 80, "endOffset": 83}, {"referenceID": 13, "context": "CNN structures with two-dimensional convolution have been used in music tagging [2] and vocal/instrumental classification [14].", "startOffset": 122, "endOffset": 126}, {"referenceID": 14, "context": "CRNN CRNN uses a 2-layer RNN with gated recurrent units (GRU) [15] to summarise temporal patterns on the top of twodimensional 4-layer CNNs as shown in Figure 1c.", "startOffset": 62, "endOffset": 66}, {"referenceID": 6, "context": "This structure is first proposed in [7] for document classification.", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "In CRNN, RNNs are used to aggregate the temporal patterns instead of, for instance, averaging the results from shorter segments as in [1] or convolution and sub-sampling as in Conv2D.", "startOffset": 134, "endOffset": 137}, {"referenceID": 0, "context": "The widths of layers are based on [1] for Conv1D and [2] for Conv2D.", "startOffset": 34, "endOffset": 37}, {"referenceID": 1, "context": "The widths of layers are based on [1] for Conv1D and [2] for Conv2D.", "startOffset": 53, "endOffset": 56}, {"referenceID": 15, "context": "This is to maximise the representation capabilities of networks, considering the relative importance of depth over width [16].", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "EXPERIMENTS We use the Million Song Dataset [17] with last.", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "05 kHz to 12 kHz using Librosa [18].", "startOffset": 31, "endOffset": 35}, {"referenceID": 1, "context": "Log-amplitude mel-spectrograms are used as input since they have outperformed STFT and MFCCs, and linear-amplitude mel-spectrograms in earlier research [2, 1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 0, "context": "Log-amplitude mel-spectrograms are used as input since they have outperformed STFT and MFCCs, and linear-amplitude mel-spectrograms in earlier research [2, 1].", "startOffset": 152, "endOffset": 158}, {"referenceID": 18, "context": "The model is built with Keras [19] and Theano [20].", "startOffset": 30, "endOffset": 34}, {"referenceID": 19, "context": "The model is built with Keras [19] and Theano [20].", "startOffset": 46, "endOffset": 50}, {"referenceID": 20, "context": "We use ADAM for learning rate control [21] and binary crossentropy as a loss function.", "startOffset": 38, "endOffset": 42}, {"referenceID": 1, "context": "851 [2].", "startOffset": 4, "endOffset": 7}, {"referenceID": 1, "context": "parameter structures, Conv2D outperforms CRNN while both of them outperform the state-of-the-art structure [2].", "startOffset": 107, "endOffset": 110}], "year": 2016, "abstractText": "We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with two CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.", "creator": "LaTeX with hyperref package"}}}