{"id": "1609.04253", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Neural Machine Transliteration: Preliminary Results", "abstract": "Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoder-decoder model is able to achieve significantly higher transliteration quality over traditional statistical models.", "histories": [["v1", "Wed, 14 Sep 2016 13:12:12 GMT  (63kb,D)", "http://arxiv.org/abs/1609.04253v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["amir h jadidinejad"], "accepted": false, "id": "1609.04253"}, "pdf": {"name": "1609.04253.pdf", "metadata": {"source": "CRF", "title": "Neural Machine Transliteration: Preliminary Results", "authors": ["Amir H. Jadidinejad"], "emails": ["jadidinejad@bayan.co.ir"], "sections": [{"heading": "1 Introduction", "text": "Machine transliteration is defined as the phonetic transformation of names across languages (Zhang et al., 2015; Karimi et al., 2011). Transliteration of designated units is an essential component of many multilingual applications, such as machine translation (Koehn et al., 2010) and cross-lingual information gathering (Jadidinejad and Mahmoudi, 2010). Recent studies have focused much attention on the task of neural machine translation (Cho et al., 2014a; Sutskever et al., 2014). In neural machine translation, a single neural network is responsible for reading a source sentence and generates its trans translation. From a probable perspective, translation is tantamount to identifying a target y that maximizes the conditional probability of a source sentence x, i.e. argmaxy p (y | x), the entire neural network is jointly trained to maximize the conditional probability of a translation."}, {"heading": "2 Proposed Model", "text": "Here we will briefly describe the underlying framework, called the RNN encoder decoder, proposed by (Cho et al., 2014b) and (Sutskever et al., 2014), on which we build a machine transliteration model that learns to transliterate end-to-end. Enoder is a character-based recursive neural network that learns a highly nonlinear mapping from a notation to the phonetics of the input sequence, which reads the source name x = (x1,., xT) and encodes it into a sequence of hidden neural states h = (h1, \u00b7, hT): ht = f (xt, ht \u2212 1) (1) (1) (1) Each hidden state hi is a bi-directional recursive representation with forward and backward sequence information around the ith character. Thear Xiv: 160 Equong 253v 1 sequence [s.C] a forward sequence of a 2016p representation of a forward-directed character."}, {"heading": "3 Experiments", "text": "We have conducted a series of experiments to show the effectiveness of encoder decoder models (Cho et al., eSi asds rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the reG the rf\u00fc the reG the rf\u00fc the reG the rf\u00fc the rf\u00fc the reG the rf\u00fc the rf\u00fc the reG the rf\u00fc the rf\u00fc the rf the Su the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the rf the"}, {"heading": "4 Conclusion", "text": "In this paper, we proposed Neural Machine Transliteration, based on successful studies in Sequence to Sequence Learning (Sutskever et al., 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Jussa and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a). Neural Machine Transliteration typically consists of two components, the first of which encodes a source name sequence x and the second decodes to a target name sequence y. Various parts of the proposed model were jointly trained using stochastic gradient lineage to minimize the likelihood of logic. Experiments on various datasets using benchmark metrics showed that the proposed model is capable of achieving a significantly higher quality of transliteration compared to traditional statistical models (Koehn, 2010). In this paper, we did not focus on improving the benchmark for achieving highly modern hyper-perstra results (2012)."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the developers of the projects Theano (Theano Development Team, 2016) and DL4MT 1. The author would also like to thank Bayan Inc. for their support with research funding and computer support. The autor1https: / / github.com / nyu-dl / dl4mt-tutorial would also like to thank Yasser Souri for valuable comments."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "International Conference on Learning Representations, volume abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Report of news 2015 machine transliteration shared task", "author": ["Rafael E. Banchs", "Min Zhang", "Xiangyu Duan", "Haizhou Li", "A. Kumaran."], "venue": "Proceedings of the Fifth Named Entity Workshop, pages 10\u201323, Beijing, China, July. Association for Computational", "citeRegEx": "Banchs et al\\.,? 2015", "shortCiteRegEx": "Banchs et al\\.", "year": 2015}, {"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio."], "venue": "J. Mach. Learn. Res., 13(1):281\u2013305, February.", "citeRegEx": "Bergstra and Bengio.,? 2012", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "Alternative structures for character-level rnns", "author": ["Piotr Bojanowski", "Armand Joulin", "Tomas Mikolov."], "venue": "arXiv preprint arXiv:1511.06303.", "citeRegEx": "Bojanowski et al\\.,? 2015", "shortCiteRegEx": "Bojanowski et al\\.", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Dzmitry Bahdanau", "Yoshua Bengio."], "venue": "Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statisti-", "citeRegEx": "Cho et al\\.,? 2014a", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "Proceedings of", "citeRegEx": "Cho et al\\.,? 2014b", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["Marta R. Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa."], "venue": "CoRR, abs/1603.00810.", "citeRegEx": "Costa.Juss\u00e0 and Fonollosa.,? 2016", "shortCiteRegEx": "Costa.Juss\u00e0 and Fonollosa.", "year": 2016}, {"title": "Multi-task learning for multiple language translation", "author": ["Daxiang Dong", "Hua Wu", "Wei He", "Dianhai Yu", "Haifeng Wang."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint", "citeRegEx": "Dong et al\\.,? 2015", "shortCiteRegEx": "Dong et al\\.", "year": 2015}, {"title": "Neural network transduction models", "author": ["Andrew Finch", "Lemao Liu", "Xiaolin Wang", "Eiichiro Sumita"], "venue": null, "citeRegEx": "Finch et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Finch et al\\.", "year": 2015}, {"title": "MultiWay, Multilingual Neural Machine Translation with a Shared Attention Mechanism", "author": ["O. Firat", "K. Cho", "Y. Bengio."], "venue": "ArXiv e-prints, January.", "citeRegEx": "Firat et al\\.,? 2016", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Advances in natural language processing", "author": ["Julia Hirschberg", "Christopher D. Manning."], "venue": "Science, 349(6245):261\u2013266.", "citeRegEx": "Hirschberg and Manning.,? 2015", "shortCiteRegEx": "Hirschberg and Manning.", "year": 2015}, {"title": "Cross-language information retrieval using meta-language index construction and structural queries", "author": ["AmirHossein Jadidinejad", "Fariborz Mahmoudi."], "venue": "Carol Peters, GiorgioMaria Di Nunzio, Mikko Kurimo, Thomas Mandl, Djamel Mostefa,", "citeRegEx": "Jadidinejad and Mahmoudi.,? 2010", "shortCiteRegEx": "Jadidinejad and Mahmoudi.", "year": 2010}, {"title": "Machine transliteration survey", "author": ["Sarvnaz Karimi", "Falk Scholer", "Andrew Turpin."], "venue": "ACM Comput. Surv., 43(3):17:1\u201317:46, April.", "citeRegEx": "Karimi et al\\.,? 2011", "shortCiteRegEx": "Karimi et al\\.", "year": 2011}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba."], "venue": "CoRR, abs/1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Statistical Machine Translation", "author": ["Philipp Koehn."], "venue": "Cambridge University Press, New York, NY, USA, 1st edition.", "citeRegEx": "Koehn.,? 2010", "shortCiteRegEx": "Koehn.", "year": 2010}, {"title": "Character-based neural machine translation", "author": ["Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black."], "venue": "CoRR, abs/1511.04586.", "citeRegEx": "Ling et al\\.,? 2015", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Multitask sequence to sequence learning", "author": ["Minh-Thang Luong", "Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser."], "venue": "CoRR, abs/1511.06114.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "A comparison of different machine transliteration models", "author": ["Jong-Hoon Oh", "Key-Sun Choi", "Hitoshi Isahara."], "venue": "J. Artif. Intell. Res. (JAIR), 27:119\u2013 151.", "citeRegEx": "Oh et al\\.,? 2006", "shortCiteRegEx": "Oh et al\\.", "year": 2006}, {"title": "How to construct deep recurrent neural networks", "author": ["Razvan Pascanu", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1312.6026.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le."], "venue": "CoRR, abs/1409.3215.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints, abs/1605.02688, May.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Whitepaper of news 2015 shared task on machine transliteration", "author": ["Min Zhang", "Haizhou Li", "Rafael E. Banchs", "A. Kumaran."], "venue": "Proceedings of the Fifth Named Entity Workshop, pages 1\u20139, Beijing, China, July. Association for Computational Linguis-", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "Machine Transliteration is defined as phonetic transformation of names across languages (Zhang et al., 2015; Karimi et al., 2011).", "startOffset": 88, "endOffset": 129}, {"referenceID": 13, "context": "Machine Transliteration is defined as phonetic transformation of names across languages (Zhang et al., 2015; Karimi et al., 2011).", "startOffset": 88, "endOffset": 129}, {"referenceID": 15, "context": "Transliteration of named entities is the essential part of many multilingual applications, such as machine translation (Koehn, 2010) and cross-language information retrieval (Jadidinejad and Mahmoudi, 2010).", "startOffset": 119, "endOffset": 132}, {"referenceID": 12, "context": "Transliteration of named entities is the essential part of many multilingual applications, such as machine translation (Koehn, 2010) and cross-language information retrieval (Jadidinejad and Mahmoudi, 2010).", "startOffset": 174, "endOffset": 206}, {"referenceID": 4, "context": "Recent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014).", "startOffset": 79, "endOffset": 122}, {"referenceID": 20, "context": "Recent studies pay a great attention to the task of Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014).", "startOffset": 79, "endOffset": 122}, {"referenceID": 18, "context": "Transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015).", "startOffset": 155, "endOffset": 192}, {"referenceID": 9, "context": "Transforming a name from spelling to phonetic and then use the constructed phonetic to generate the spelling on the target language is a very complex task (Oh et al., 2006; Finch et al., 2015).", "startOffset": 155, "endOffset": 192}, {"referenceID": 4, "context": "Based on successful studies on Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Hirschberg and Manning, 2015), in this paper, we proposed a character-based encoder\u2013 decoder model which learn to transliterate endto-end.", "startOffset": 58, "endOffset": 131}, {"referenceID": 20, "context": "Based on successful studies on Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Hirschberg and Manning, 2015), in this paper, we proposed a character-based encoder\u2013 decoder model which learn to transliterate endto-end.", "startOffset": 58, "endOffset": 131}, {"referenceID": 11, "context": "Based on successful studies on Neural Machine Translation (Cho et al., 2014a; Sutskever et al., 2014; Hirschberg and Manning, 2015), in this paper, we proposed a character-based encoder\u2013 decoder model which learn to transliterate endto-end.", "startOffset": 58, "endOffset": 131}, {"referenceID": 5, "context": "Here, we describe briefly the underlying framework, called RNN Encoder\u2013Decoder, proposed by (Cho et al., 2014b) and (Sutskever et al.", "startOffset": 92, "endOffset": 111}, {"referenceID": 20, "context": ", 2014b) and (Sutskever et al., 2014) upon which we build a machine transliteration model that learns to transliterate end-to-end.", "startOffset": 13, "endOffset": 37}, {"referenceID": 8, "context": ", hT } (Dong et al., 2015; Chung et al., 2016).", "startOffset": 7, "endOffset": 46}, {"referenceID": 6, "context": ", hT } (Dong et al., 2015; Chung et al., 2016).", "startOffset": 7, "endOffset": 46}, {"referenceID": 5, "context": "We conducted a set of experiments to show the effectiveness of RNN Encoder\u2013Decoder model (Cho et al., 2014b; Sutskever et al., 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task (Banchs et al.", "startOffset": 89, "endOffset": 132}, {"referenceID": 20, "context": "We conducted a set of experiments to show the effectiveness of RNN Encoder\u2013Decoder model (Cho et al., 2014b; Sutskever et al., 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task (Banchs et al.", "startOffset": 89, "endOffset": 132}, {"referenceID": 1, "context": ", 2014) in the task of machine transliteration using standard benchmark datasets provided by NEWS 2015-16 shared task (Banchs et al., 2015).", "startOffset": 118, "endOffset": 139}, {"referenceID": 1, "context": "Table 1: Datasets provided by NEWS 2015 (Banchs et al., 2015).", "startOffset": 40, "endOffset": 61}, {"referenceID": 3, "context": "We leveraged a character-based encoder\u2013 decoder model (Bojanowski et al., 2015; Chung et al., 2016) with soft attention mechanism (Cho et al.", "startOffset": 54, "endOffset": 99}, {"referenceID": 6, "context": "We leveraged a character-based encoder\u2013 decoder model (Bojanowski et al., 2015; Chung et al., 2016) with soft attention mechanism (Cho et al.", "startOffset": 54, "endOffset": 99}, {"referenceID": 5, "context": ", 2016) with soft attention mechanism (Cho et al., 2014b).", "startOffset": 38, "endOffset": 57}, {"referenceID": 4, "context": "Using characters instead of words leads to longer sequences, so Gated Recurrent Units (Cho et al., 2014a) have been used for the encoder network to model long term dependencies.", "startOffset": 86, "endOffset": 105}, {"referenceID": 5, "context": "The encoder has 128 hidden units for each direction (forward and backward), and the decoder has 128 hidden units with soft attention mechanism (Cho et al., 2014b).", "startOffset": 143, "endOffset": 162}, {"referenceID": 14, "context": "We train the model using stochastic gradient descent with Adam (Kingma and Ba, 2014).", "startOffset": 63, "endOffset": 84}, {"referenceID": 19, "context": "The norm of the gradient is clipped with a threshold 1 (Pascanu et al., 2013).", "startOffset": 55, "endOffset": 77}, {"referenceID": 15, "context": "Also, beamsearch has been used to approximately find the most likely transliteration given a source sequence (Koehn, 2010).", "startOffset": 109, "endOffset": 122}, {"referenceID": 1, "context": "Table 2 shows the effectiveness of the proposed model on different datasets using standard measures (Banchs et al., 2015).", "startOffset": 100, "endOffset": 121}, {"referenceID": 1, "context": "The proposed neural machine transliteration model has been compared to the baseline method provided by NEWS 2016 organizers (Banchs et al., 2015).", "startOffset": 124, "endOffset": 145}, {"referenceID": 20, "context": "In this paper we proposed Neural Machine Transliteration based on successful studies in sequence to sequence learning (Sutskever et al., 2014) and Neural Machine Translation (Ling et al.", "startOffset": 118, "endOffset": 142}, {"referenceID": 16, "context": ", 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a).", "startOffset": 39, "endOffset": 133}, {"referenceID": 7, "context": ", 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a).", "startOffset": 39, "endOffset": 133}, {"referenceID": 0, "context": ", 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a).", "startOffset": 39, "endOffset": 133}, {"referenceID": 4, "context": ", 2014) and Neural Machine Translation (Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016; Bahdanau et al., 2015; Cho et al., 2014a).", "startOffset": 39, "endOffset": 133}, {"referenceID": 15, "context": "Experiments on different datasets using benchmark measures revealed that the proposed model is able to achieve significantly higher transliteration quality over traditional statistical models (Koehn, 2010).", "startOffset": 192, "endOffset": 205}, {"referenceID": 2, "context": "In this paper we did not concentrate on improving the model for achieving state-of-the-art results, so applying hyperparameter optimization (Bergstra and Bengio, 2012), multi-task sequence to sequence learning (Luong et al.", "startOffset": 140, "endOffset": 167}, {"referenceID": 17, "context": "In this paper we did not concentrate on improving the model for achieving state-of-the-art results, so applying hyperparameter optimization (Bergstra and Bengio, 2012), multi-task sequence to sequence learning (Luong et al., 2015) and multiway transliteration (Firat et al.", "startOffset": 210, "endOffset": 230}, {"referenceID": 10, "context": ", 2015) and multiway transliteration (Firat et al., 2016; Dong et al., 2015) are quite promising for future works.", "startOffset": 37, "endOffset": 76}, {"referenceID": 8, "context": ", 2015) and multiway transliteration (Firat et al., 2016; Dong et al., 2015) are quite promising for future works.", "startOffset": 37, "endOffset": 76}], "year": 2016, "abstractText": "Machine transliteration is the process of automatically transforming the script of a word from a source language to a target language, while preserving pronunciation. Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. In this paper a character-based encoder-decoder model has been proposed that consists of two Recurrent Neural Networks. The encoder is a Bidirectional recurrent neural network that encodes a sequence of symbols into a fixed-length vector representation, and the decoder generates the target sequence using an attention-based recurrent neural network. The encoder, the decoder and the attention mechanism are jointly trained to maximize the conditional probability of a target sequence given a source sequence. Our experiments on different datasets show that the proposed encoderdecoder model is able to achieve significantly higher transliteration quality over traditional statistical models.", "creator": "TeX"}}}