{"id": "1609.04325", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Transliteration in Any Language with Surrogate Languages", "abstract": "We introduce a method for transliteration generation that can produce transliterations in every language. Where previous results are only as multilingual as Wikipedia, we show how to use training data from Wikipedia as surrogate training for any language. Thus, the problem becomes one of ranking Wikipedia languages in order of suitability with respect to a target language. We introduce several task-specific methods for ranking languages, and show that our approach is comparable to the oracle ceiling, and even outperforms it in some cases.", "histories": [["v1", "Wed, 14 Sep 2016 15:58:55 GMT  (49kb,D)", "http://arxiv.org/abs/1609.04325v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["stephen mayhew", "christos christodoulopoulos", "dan roth"], "accepted": false, "id": "1609.04325"}, "pdf": {"name": "1609.04325.pdf", "metadata": {"source": "CRF", "title": "Transliteration in Any Language with Surrogate Languages", "authors": ["Stephen Mayhew", "Christos Christodoulopoulos"], "emails": ["danr}@illinois.edu"], "sections": [{"heading": "1 Introduction", "text": "Transliteration is the task of converting a word or phrase, typically a named entity, from one script to another while maintaining pronunciation, and the problem is largely of interest for downstream tasks such as cross-lingual recognition of designated entities (Darwish, 2013; Kim et al., 2012). Transliteration work can be divided into two areas: the discovery, where a query name is provided along with a list of candidates, and the task of selecting the best candidate and the generation where the task is to create a transliteration for a particular test name. We focus on the more difficult task, the generations.To date, all work in generation requires some level of monitoring in the target language T in the form of name pairs. Multilingual approaches typically collect this from Wikipedia (Pasternack and Roth, 2009; Irvine et al., 2010). However, languages with little or no Wikipedia presence are still inaccessible."}, {"heading": "2 Method", "text": "Central to our approach is the fact that we have name pair data from 281 languages in Wikipedia, although our method is sufficient for any named language, many of which have a trivial number of name pairs (< 20), so we only keep those 131 languages with 200 name pairs or more. We refer to this phrase as W. We used Uroman1 to romanize all of our data, which means that the text in any language represents Latin characters, and that we can train on any language, and use this model to test for any other language, unlimited by writing. This romanization is a deterministic process that is reversible with little loss. 2Our goal is to generate transcriptions from the target language (English) into a cenary resource."}, {"heading": "3 Transliteration-Driven Language Similarity", "text": "We start with the presentation of the monitored oracle ranking and then describe our proposed similarity indicators."}, {"heading": "3.1 Oracle Ranking", "text": "We collect this oracle ranking with respect to the language T as follows: For each language L in Wikipedia (including T if it is present) we learn a model, and test this model on2uroman currently does not support this. Algorithm 1 Our method InputN: Set of names in English T: Target language yields N \u2032: Names that are transliterated in T1: for the language L. Spare part selection 2: Simulation (L, T) 3: End for 4: Order W according to the score 5: S = Top language in W 6: Train model with (English, S) 7: Use the model to transliterate N in N \u2032 name pairs in language T. Now we have a score for each L that we use to achieve a ranking above W. This is the oracle ranking. We would expect that T ranks at the top, and languages that are significantly different from T Ukraine, are close to the lower range. For example, a model based on a Russian model should be performed well, but not in Ukrainian order."}, {"heading": "3.2 Phonetic Similarity", "text": "Our phonetic similarity metrics are based on phonetic inventories under the intuition that languages with similar inventories will have similar transliterations. This addresses the second challenge of Karimi et al. (2011), namely missing sounds. We use PHOIBLE (Moran et al., 2014), a database of phonetic inventories of over 1600 languages, each consisting of a common set of symbols from the International Phonetic Alphabet. We calculate similarities between two languages X and Y by measuring F1 between phoneminventories LX and LY. We call this a Simphon (X, Y)."}, {"heading": "3.3 Script Distribution Similarity", "text": "We calculate the font similarity of the languages X and Y as cosinal similarity of the character histogram vectors HX and HY. We romanize and reduce all texts first, so that these vectors usually have a length of about 26 (some languages contain wrong characters, and some do not use all 26 characters)."}, {"heading": "3.4 Genealogical Similarity", "text": "WALS (Dryer and Haspelmath, 2013) offers some basic characteristics of almost every language, including the language family and genus (Comrie et al., 2013), where family is a greater differentiation than the genus. We calculate the genealogical similarity of languages X and Y as follows: 1 if genus and family agree, 0.5 if only the family agrees, and 0 otherwise."}, {"heading": "3.5 Learned Similarity", "text": "We combined the similarities of the components by using them as characteristics in SVMrank (Joachims, 2006) and used the oracle rankings as supervision. In addition to the similarities of the components, we also used all 6 distance characteristics of URIEL (Littel et al., 2016), a collection of language resources. These distances are calculated from characteristics in WALS and PHOIBLE and distributed with the URIEL packet. Distance characteristics are: genetic, geographical, inventory, phonological, syntactic and featral. In predicting rankings in each target language, we trained all other experimental languages (see \u00a7 4). We call this simlearned (X, Y)."}, {"heading": "4 Experiments and Results", "text": "We selected nine languages to serve as low-resource target languages (although some of them do indeed have high resources): Bengali, Chuvash, Armenian (hye), Kannada, Mazandarani, Newar, Thai, Uzbek, and Mingrelian (xmf). We inserted a number of different writings and tended to use languages with a small number of training pairs. To test our hypotheses, we conducted three experiments: the first confirms similarity metrics in paragraph 4.1; the second in paragraph 4.2 shows that the use of surrogate languages can be effective; and the third in paragraph 4.3 shows that combining multiple surrogate languages can be even more effective."}, {"heading": "4.1 Similarity Metric Validation", "text": "In the first experiment, we compare our similarity rankings with the oracle ranking, using normalized discounted cumulative gains (NDCG), a metric commonly used in information retrieval (Ja \ufffd rvelin and Keka \ufffd la \ufffd inen, 2002). In this setting, an IR system ranks a series of documents, each of which has a pre-defined relevance value, which captures the notion that highly relevant documents should be placed at the top of a retrieved list. the normalized version (NDCG) divides each DCG score by the maximum possible score. In our situation, documents correspond to languages, and the relevance value of each language S is the score obtained when we train on S and test for T (i.e. results from the oracle ranking). We use k = 5, which means that we expect relevant languages to appear well in the first 5 results. Table 1 shows results from a random perevaluation of the perevaluation values."}, {"heading": "4.2 Effectiveness of Surrogate Languages", "text": "In the second experiment, we select the top language (as predicted by font similarity) and show that it can produce transliterations that are qualitatively comparable to the best language.The results are presented in Table 2, where each value is an MRR value. The \"top oracle\" line shows values from training in the top oracle language, and the \"top prediction\" shows the value from the top prediction language. The oracle value is by definition always higher or equal to the predicted ranking. Note that we removed T from all rankings to simulate a low-resource setting. The top prediction language is the same as the oracle in 4 out of 9 cases. In any other case, the score of the top prediction language is not far from the pass. The exception is Newar (new), which is 8 points below the best. We suspect this is due to the scriptness not taking into account the size of the training data."}, {"heading": "4.3 Combining Surrogate Languages", "text": "The third experiment combines the top-k prediction languages into a single prediction result using weighted voting. At the test date, each predicted language generates an n-best list of points for each word in the test sentence. We combine n-best lists by using the score for voting. Results can be seen in Table 2 in the \"Predicted top-k\" series, where k = 5. In all cases, if the predicted top value is not the best choice (as in Uzbek, for example), the combination value helps. In two cases, Nevar and Uzbek, the top-5 combination yields a score that is better than the oracle (indicated by an *)."}, {"heading": "5 Related Work", "text": "Transliteration is typically studied in the context of a single language or group of languages. To name a few, there are works on Arabic (Sherif and Kondrak, 2007) and Japanese (Knight and Graehl, 1998). For an excellent overview of the state of transliteration, see Karimi et al. (2011). Pasternack and Roth (2009) and Irvine et al. (2012) both use data from Wikipedia, but cannot deal with those languages that are little or non-existent in Wikipedia. There are works on transliteration that use only monolingual phonetic mappings (Jagarlamudi and Daume \u0301 III, 2012; Yoon et al., 2007). While these mappings are less costly than name pairs, they still require expert knowledge on creates.We contrast our similarity metric with a transliteracy measure called Weighted AVerage Entropy (Kumaran, 2010)."}, {"heading": "6 Discussion and Future Work", "text": "We have shown a way to extend transliteration capabilities to all languages by using Wikipedia as a source of data. We have shown a way to use task-driven linguistic similarity metrics to achieve a ranking of languages that is very close to an oracle ranking. Finally, our combination algorithm is capable of achieving values that exceed the oracle values. In the future, we will explore more sophisticated techniques for combining languages, with the hope of beating the oracle significantly each time."}], "references": [{"title": "Named entity recognition using cross-lingual resources: Arabic as an example", "author": ["Kareem Darwish"], "venue": null, "citeRegEx": "Darwish.,? \\Q2013\\E", "shortCiteRegEx": "Darwish.", "year": 2013}, {"title": "Transliterating from all languages. In Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA)", "author": ["Irvine et al.2010] Ann Irvine", "Chris Callison-Burch", "Alexandre Klementiev"], "venue": null, "citeRegEx": "Irvine et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Irvine et al\\.", "year": 2010}, {"title": "Processing informal, romanized pakistani text messages", "author": ["Irvine et al.2012] Ann Irvine", "Jonathan Weese", "Chris Callison-Burch"], "venue": "In Proceedings of the Second Workshop on Language in Social Media,", "citeRegEx": "Irvine et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Irvine et al\\.", "year": 2012}, {"title": "Regularized interlingual projections: evaluation on multilingual transliteration", "author": ["Jagarlamudi", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Process-", "citeRegEx": "Jagarlamudi et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jagarlamudi et al\\.", "year": 2012}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["J\u00e4rvelin", "Kek\u00e4l\u00e4inen2002] Kalervo J\u00e4rvelin", "Jaana Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS),", "citeRegEx": "J\u00e4rvelin et al\\.,? \\Q2002\\E", "shortCiteRegEx": "J\u00e4rvelin et al\\.", "year": 2002}, {"title": "Training linear svms in linear time", "author": ["Thorsten Joachims"], "venue": "In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining,", "citeRegEx": "Joachims.,? \\Q2006\\E", "shortCiteRegEx": "Joachims.", "year": 2006}, {"title": "Machine transliteration survey", "author": ["Falk Scholer", "Andrew Turpin"], "venue": "ACM Computing Surveys (CSUR),", "citeRegEx": "Karimi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Karimi et al\\.", "year": 2011}, {"title": "Multilingual named entity recognition using parallel data and metadata from wikipedia", "author": ["Kim et al.2012] Sungchul Kim", "Kristina Toutanova", "Hwanjo Yu"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-", "citeRegEx": "Kim et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2012}, {"title": "Compositional machine transliteration", "author": ["Kumaran et al.2010] A Kumaran", "Mitesh M Khapra", "Pushpak Bhattacharyya"], "venue": "ACM Transactions on Asian Language Information Processing (TALIP),", "citeRegEx": "Kumaran et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Kumaran et al\\.", "year": 2010}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["Slav Petrov", "Keith Hall"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "McDonald et al\\.,? \\Q2011\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2011}, {"title": "Learning better transliterations", "author": ["Pasternack", "Roth2009] J. Pasternack", "D. Roth"], "venue": "In Proc. of the ACM Conference on Information and Knowledge Management (CIKM),", "citeRegEx": "Pasternack et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pasternack et al\\.", "year": 2009}, {"title": "Klcpos3 - a language similarity measure for delexicalized parser transfer", "author": ["Rosa", "Zabokrtsk\u00fd2015] Rudolf Rosa", "Zdenek Zabokrtsk\u00fd"], "venue": null, "citeRegEx": "Rosa et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rosa et al\\.", "year": 2015}, {"title": "Substring-based transliteration", "author": ["Sherif", "Kondrak2007] Tarek Sherif", "Grzegorz Kondrak"], "venue": "In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "Sherif et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Sherif et al\\.", "year": 2007}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 conference of the North American chapter of the association", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Multilingual transliteration using feature based phonetic method", "author": ["Yoon et al.2007] Su-Youn Yoon", "Kyoung-Young Kim", "Richard Sproat"], "venue": "In ANNUAL MEETING-ASSOCIATION FOR COMPUTATIONAL LINGUISTICS,", "citeRegEx": "Yoon et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Yoon et al\\.", "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "The problem is largely of interest for downstream tasks such as cross lingual named entity recognition (Darwish, 2013; Kim et al., 2012).", "startOffset": 103, "endOffset": 136}, {"referenceID": 7, "context": "The problem is largely of interest for downstream tasks such as cross lingual named entity recognition (Darwish, 2013; Kim et al., 2012).", "startOffset": 103, "endOffset": 136}, {"referenceID": 1, "context": "Multilingual-minded approaches typically gather this from Wikipedia (Pasternack and Roth, 2009; Irvine et al., 2010).", "startOffset": 68, "endOffset": 116}, {"referenceID": 6, "context": "This addresses the second challenge from Karimi et al. (2011), that of missing sounds.", "startOffset": 41, "endOffset": 62}, {"referenceID": 5, "context": "We combined the component similarities by using them as features in SVMrank (Joachims, 2006), and using the oracle rankings as supervision.", "startOffset": 76, "endOffset": 92}, {"referenceID": 6, "context": "For an excellent survey on the state of the art in transliteration, see Karimi et al. (2011).", "startOffset": 72, "endOffset": 93}, {"referenceID": 1, "context": "Pasternack and Roth (2009) and Irvine et al. (2012) both do transliteration using data harvested from Wikipedia, but cannot address those languages with little or no presence in Wikipedia.", "startOffset": 31, "endOffset": 52}, {"referenceID": 14, "context": "There is work on transliteration using only monolingual phonetic mappings (Jagarlamudi and Daum\u00e9 III, 2012; Yoon et al., 2007).", "startOffset": 74, "endOffset": 126}, {"referenceID": 8, "context": "We contrast our similarity metric with a transliterability measure called Weighted AVerage Entropy (WAVE) (Kumaran et al., 2010).", "startOffset": 106, "endOffset": 128}, {"referenceID": 13, "context": "In the multilingual tradition, this paper is closely related to direct transfer techniques (T\u00e4ckstr\u00f6m et al., 2012; McDonald et al., 2011).", "startOffset": 91, "endOffset": 138}, {"referenceID": 9, "context": "In the multilingual tradition, this paper is closely related to direct transfer techniques (T\u00e4ckstr\u00f6m et al., 2012; McDonald et al., 2011).", "startOffset": 91, "endOffset": 138}], "year": 2016, "abstractText": "We introduce a method for transliteration generation that can produce transliterations in every language. Where previous results are only as multilingual as Wikipedia, we show how to use training data from Wikipedia as surrogate training for any language. Thus, the problem becomes one of ranking Wikipedia languages in order of suitability with respect to a target language. We introduce several task-specific methods for ranking languages, and show that our approach is comparable to the oracle ceiling, and even outperforms it in some cases.", "creator": "LaTeX with hyperref package"}}}