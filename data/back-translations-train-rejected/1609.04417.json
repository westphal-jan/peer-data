{"id": "1609.04417", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "An Adaptive Psychoacoustic Model for Automatic Speech Recognition", "abstract": "Compared with automatic speech recognition (ASR), the human auditory system is more adept at handling noise-adverse situations, including environmental noise and channel distortion. To mimic this adeptness, auditory models have been widely incorporated in ASR systems to improve their robustness. This paper proposes a novel auditory model which incorporates psychoacoustics and otoacoustic emissions (OAEs) into ASR. In particular, we successfully implement the frequency-dependent property of psychoacoustic models and effectively improve resulting system performance. We also present a novel double-transform spectrum-analysis technique, which can qualitatively predict ASR performance for different noise types. Detailed theoretical analysis is provided to show the effectiveness of the proposed algorithm. Experiments are carried out on the AURORA2 database and show that the word recognition rate using our proposed feature extraction method is significantly increased over the baseline. Given models trained with clean speech, our proposed method achieves up to 85.39% word recognition accuracy on noisy data.", "histories": [["v1", "Wed, 14 Sep 2016 20:02:42 GMT  (838kb)", "http://arxiv.org/abs/1609.04417v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["peng dai", "xue teng", "frank rudzicz", "ing yann soon"], "accepted": false, "id": "1609.04417"}, "pdf": {"name": "1609.04417.pdf", "metadata": {"source": "CRF", "title": "An Adaptive Psychoacoustic Model for Automatic Speech Recognition", "authors": ["Peng Dai", "Xue Teng", "Frank Rudzicz", "Ing Yann Soon"], "emails": ["*peng.dai.ca@ieee.org"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.04 417v 1 [cs.C L] 14 SE"}, {"heading": "1. Introduction", "text": "Language may be the most important form of human communication, and as a result automatic speech recognition (ASR) has received considerable attention. After decades of development, ASR is very effective in decoding clean speech, e.g. achieving over 95% word accuracy in small vocabulary and over 90% in large vocabulary contexts. This is in contrast to the human hearing system, which exhibits greater resistance to noise [26, 36]. For humans, speech perception is a sensory and perceptible process [7, 29, 13] and in this paper we focus on psychoacoustic and otoacoustic emission (OAE)."}, {"heading": "1.1. Auditory model", "text": "In this paper, we examine two sub-areas of auditory neuroscience, namely psychoacoustics and otoacoustic emissions (OAEs). Psychoacoustics covers many different topics, including the limits of perception, sound localization and the masking of effects. The masking effect is the phenomenon in which a clearly audible sound (mask) is influenced by another sound (mask). In order to quantify the effect of the masking, a masking threshold is usually set. The masking threshold is the sound pressure level of a test sound that is barely audible in the presence of a masker. Masking effects can be classified as simultaneous or temporal according to signal events [7]. Masking effects between two signals that occur simultaneously are simultaneous or frequency masking."}, {"heading": "2. Algorithm Description", "text": "In this part we will describe our proposed mathematical model for human hearing, which mainly consists of two parts, the 2D adaptive psychoacoustic filter and the OAE filter."}, {"heading": "2.1. 2D psychoacoustic filter", "text": "A masking threshold is usually defined to describe the extent to which the mask affects the mask. As masking effects modify both the time and frequency components of the acoustic signals, our proposed algorithm is conceived in the common time frequency domain. A speech signal, y (t), is split into frames and transformed into the time frequency domain, represented as Y (f, t), represented as Y (f), by the Fourier transformation. Here, f and t are frequency indexes of the signal. Since f and t can be transformed into the actual frequency and time of the signal signal, they are used interchangeably as the actual frequency and time in the following discourses."}, {"heading": "2.2. Adaptive 2D Psychoacoustic Filter", "text": "The human auditory system reacts differently to different frequencies and masking effects are also frequency dependent (Y = 14). That is, the frequency of the mask influences the total amount of masking, Mtotal, resulting in the parameter \u03b1 (\u2206 f, \u2206 t) (see Equation (8)) changes with frequency. Figure 2 shows the characteristic curve of forward masking, which describes how the amount of masking, Mtotal, changes over time, \u2206 t [16]. The parameters 1 kHz and 4 kHz are used for low-band and high-band temporary masking parameters, whereby the parameters of masking effects are changed with frequency, ideally there should be different 2D psychoacoustic filters for different frequencies, but this may be impractical. Therefore, in our current implementation, we divide each speech sample called Fs \u00d7 Ts Matrix Ys into two parts, namely the low and high frequencies."}, {"heading": "2.3. Otoacoustic emissions (OAEs)", "text": "Otoacoustic emissions (OAEs) are clinically important because they are the basis of a simple, non-invasive test for hearing disorders in newborn babies and children who are too young to participate in conventional hearing tests. OAEs are considered related to the cochlear amplification function [2] and are produced within the inner ear, in particular by the movement of nerve cells on the base membrane, as they energetically respond to auditory stimulation. Masking effects can also be partially described by the inner ear, and we assume that OAEs can also be calculated as masking effects based on similar equations. Previous theoretical studies have suggested that OAEs arise primarily from a linear process of coherent reflection [37, 34] which means that it can be treated as \"reverberation\" of the input acoustic signal."}, {"heading": "3. Theoretical Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Complex Spectral Processing", "text": "After the voice signal is split into frames and processed by Discrete Fourier Transformation (DFT), it is transformed into the time frequency domain Y (f, t) = Yr (f, t) + i \u00b7 Yim (f, t) (25), where i is the imaginary entity. In practical applications, often only power or size spectra extract from the language, and the phase information is simply ignored. However, the phase can contain useful information in language [30, 15]. Our proposed algorithm works directly in the time frequency, including the phase, in the noise-removing procedure. Y mask (f, t) = Y (f, t): Mask = [Yr (f, t) + i \u00d7 Yim (f, t)]: Mask = Yr (f, t) \u2022 Mask + i \u00d7 Yim (f, t)."}, {"heading": "3.2. Double Transform", "text": "A major difference between our 2D psychoacoustic filters and normal spectral filtering is that 2D psychoacoustic filters are implemented by folding in the time frequency range. Therefore, the analysis of our high-pass or low-pass filters should be done with respect to the 2D frequency spectrum of the time-frequency domain voice signal. 2D Fourier transformation of the time-frequency domain voice signal is referred to in later discussions as double transformation. While the high-pass 2D psychoacoustic filter preserves high frequency signals, it also attenuates signals with respect to the double transformation spectrum, i.e. the 2D Fourier transformation spectrum of the time-frequency domain signal double transformation parameter (Y (f, t))."}, {"heading": "4. Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Data and Methods", "text": "(.). (.). (.). (.). (.). (.). (.).). (.). (.).) In fact, it is so that most of them are able to obey the rules that they have imposed on themselves. (.). (.). (.). (.). \"In fact, it is so that they are able to obey the rules. (.). (.). (.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).\" (.). \").\" (.). \"(.).).\" (. \"(.).\" (.).). \"(.).\" (.).). \"(.\" (.).). \"(.).\" (.).). \"(.\" (. \"(.).).\" (.).). \"(.).\" (.).). \"(.\" (.). \"(.).\" (.). \"(.).).\" (.). \"(.).\" (.). \"(.).\" (.).). \"(.).).\" (. \"(.).).\" (.). \"(.).).\" (.).). \"(.).).\" (.). \"(.).\" (.).). \"(.).).).).\". \".\"."}, {"heading": "4.2. Double Transform", "text": "In Section 3.2, we proposed a novel method for the analysis of double transformations that can be used for the quanlitative analysis of the ASR performance of psychoacoustic filters with respect to the properties of the proposed filters, e.g. high-pass or low-pass. For the proposed adaptive 2D psychoacoustic filter, the final detection accuracy results from the combined effect of the two bands, which would otherwise be very difficult to analyze. Therefore, we take the time-distorted 2D psychoacoustic filter [7] as an example to show the general steps of double-pass analysis. Table 2 shows the experimental ASR results based on the AURORA2 database. Obviously, the 2D filter TFW is best suited for airport noise. It represents a peak value at the center column that can be blocked by the 2D psychoacoustic filter."}, {"heading": "4.3. Experimental Results", "text": "Detailed experimental results for the proposed 2D adaptive psychoacoustic filter are contained in Tables 3 and 4, including results for different types of noise and SNR levels. The AURORA2 database offers 7 different SNR levels. As SNR levels decrease, the detection rate decreases with increasing speed. Figure 8 shows the \"decrease\" in detection rate between adjacent SNR levels, e.g. Clean Vs. 20 dB (referred to as Clean / 20dB). At high SNR levels, e.g. SNR > 10dB, the addition of noise leads to a relatively lower impairment in system performance. However, as the SNR falls below 10 dB, the performance of the ASR system decreases significantly, by 14% or 30%. The experimental results for coparison targets are in Tables 5 and 6. All comparison methods are implemented with MFCC (39)."}, {"heading": "4.4. Clean Training Condition", "text": "Compared to MFCC (39), the advantage of the proposed algorithm is clear. The relative improvement is 19.62% for Avg 0-20 and 90.03% for SNR of -5 dB. For FM, LI and CMVN, the relative improvements are 10.27%, 15.29% and 9.64%. For SNR -5 dB, the relative improvements are 16.34%, 45.17% and 78.27%, respectively. We propose three different 2D psychoacoustic filters: TW-2D, TFW-2D and the adaptive 2D psychoacoustic filter. The relative improvements for TW-2D are 6.12% and 71.84% for Avg 0-20 and SNR -5 dB."}, {"heading": "4.5. Multi Training Condition", "text": "In the AURORA2 database, there are two training conditions: clean and multiple training conditions. Since noisy language is used to train HMMs, the detection results are all very good, even reaching a detection rate of about 80% at an SNR of 5 dB. Cohen's corresponding d-sizes are all below 1. Therefore, large or statistically significant improvements are not very possible at this level. However, the proposed algorithm still achieves very promising results. Figure 9 (b) shows the relative improvements of the proposed algorithm across all comparison targets. It is evident that the proposed algorithm achieves significant improvements. In terms of Avg 0-20, the relative improvements are 5.22% over MFCC (39), 5.67% over FM, 4.73% over LI, 0.76% over CMVN, 0.76% over CMVN. For SNR -5 dB, the relative improvements for T2W are 71.93% over MCI, compared to VMI 73.49% over VMI, very promising compared to VMI."}, {"heading": "5. Conclusion", "text": "The key feature of the proposed algorithm is that we incorporate an adaptive scheme that better reflects the frequency-dependent property of masking effects. Language spectrum is divided into several bands. Various psychoacoustic filters are designed to be better adapted to the specific frequency bandwidth. Furthermore, the proposed method does not require any additional training process, which means that the computational load is very low. Due to the simplicity of the proposed algorithm, it can easily be combined with other algorithms. Another important contribution to this work is the dual transformer analysis technique, which allows quantitative analysis of the performance of time-frequency filters for different noise types. In particular, we have successfully explained the performance difference between the result of the airport test and the result of the exhibitions test."}, {"heading": "6. Appendices", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. 2D Psychoacoustic Filters", "text": "Table 10 and Table 11 indicate the detailed parameters of the proposed low-band and high-band 2D psychoacoustic filters."}, {"heading": "7. References", "text": "In this context, it should be noted that the two are two different types, which differ from each other in the most varied forms: one, the one, the other, the other, which can be found in the most varied forms, the one, the other, the other, which can be found in the most varied forms, the other, which can be found in the most varied forms, the other, which can be found in the most varied forms, which can be found in the most varied forms, which can be found in the most varied forms."}], "references": [{"title": "Efferently mediated changes in the quadratic distortion product (f2f1)", "author": ["K.W. Chang", "S.J. Norton"], "venue": "The Journal of the Acoustical Society of America, vol. 102, no. 3, 1997.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1997}, {"title": "Efferently mediated changes in the quadratic distortion product (f2f1)", "author": ["\u2014\u2014"], "venue": "The Journal of the Acoustical Society of America, vol. 102, no. 3, 1997.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1997}, {"title": "MVA Processing of Speech Features", "author": ["C.P. Chen", "J.A. Bilmes"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, pp. 257\u2013270, 2007. 18", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Noise spectrum estimation in adverse environments: Improved minima controlled recursive averaging", "author": ["I. Cohen"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 11, pp. 466\u2013 475, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "A temporal warped 2D psychoacoustic modeling for robust speech recognition system", "author": ["P. Dai", "I.Y. Soon"], "venue": "Speech Communication, vol. 53, pp. 229\u2013241, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "2D psychoacoustic filtering for robust speech recognition", "author": ["P. Dai", "I.Y. Soon", "C.K. Yeo"], "venue": "Proceedings ICICS, 2009, pp. 1\u20135.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "A temporal frequency warped (TFW) 2D psychoacoustic filter for robust speech recognition system", "author": ["P. Dai", "I.Y. Soon"], "venue": "Speech Communication, vol. 54, no. 3, pp. 402\u2013413, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences", "author": ["S. Davis", "P. Mermelstein"], "venue": "IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 28, pp. 357\u2013366, 1980.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1980}, {"title": "Estimating cepstrum of speech under the presence of noise using a joint prior of static and dynamic features", "author": ["L. Deng", "J. Droppo", "A. Acero"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 12, pp. 218\u2013233, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "Speech enhancement using a minimum mean-square error shorttime spectral amplitude estimator", "author": ["Y. Ephraim", "D. Malah"], "venue": "IEEE Trans. Acoust., Speech, Signal Processing, vol. 32, pp. 1109\u20131121, 1984.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1984}, {"title": "RASTA processing of speech", "author": ["H. Hermansky", "N. Morgan"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 2, pp. 578\u2013589, 1994.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "Phase-sensitive speech enhancement for cochlear implant processing", "author": ["P.S. Jafari", "K. Hou-Yong", "W. Xiaosong", "F. Qian-Jie", "J. Hui"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pp. 5104\u20135107.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Forward masking as a function of frequency, masker level, and signal delay", "author": ["W. Jesteadt", "S.P. Bacon", "J.R. Lehman"], "venue": "Journal of the Acoustical Society of America, vol. 71, pp. 950\u2013962, 1982.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1982}, {"title": "Stimulated acoustic emissions from within the human auditory system.", "author": ["D.T. Kemp"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1978}, {"title": "Clinical characteristics and audiological significance of spontaneous otoacoustic emissions in tinnitus patients with normal hearing", "author": ["D.-K. Kim", "S.-N. Park", "K.-H. Park", "H.G. Choi", "E.-J. Jeon", "Y.-S. Park", "S.W. Yeo"], "venue": "The Journal of Laryngology & Otology, vol. 125, no. 03, pp. 246\u2013250, 2011. 19", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Short term adaptation of auditory receptive fields to dynamic stimuli", "author": ["M.N. Kvale", "C.E. Schreiner"], "venue": "Journal of Neurophysiology, vol. 91, pp. 604\u2013612, 2004.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "A database for speaker-independent digit recognition", "author": ["R. Leonard"], "venue": "ICASSP \u201984. IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 9, 1984.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1984}, {"title": "A minimum-mean-square-error noise reduction algorithm on mel-frequency cepstra for robust speech recognition", "author": ["B. Milner"], "venue": "Proceedings ICASSP, 2002, pp. 797\u2013800.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Forward masking: Adaptation or integration?", "author": ["A.J. Oxenham"], "venue": "Journal of the Acoustical Society of America,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2001}, {"title": "Effects of masker frequency and duration in forward masking: further evidence for the influence of peripheral nonlinearity", "author": ["A.J. Oxenham", "C.J. Plack"], "venue": "Hearing Research, vol. 150, pp. 258\u2013266, 2000.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "The Aurora experimental framework for the performance evaluation of speech recognition systems under noisy conditions", "author": ["D. Pearse", "H. Hirsch"], "venue": "ICSLP 2000 (6th International Conference on Spokel Language Processing), 2000, pp. 16\u201319.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Glattke, Otoacoustic Emissions", "author": ["T.J.M.S. Robinette"], "venue": "Clinical Applications. Thieme,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Listening to speech in a background of other talkers: Effects of talker number and noise vocoding", "author": ["S. Rosen", "P. Souza", "C. Ekelund", "A. Majeed"], "venue": "The Journal of the Acoustical Society of America, vol. 133, no. 4, pp. 2431\u20132443, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Using phase spectrum information for improved speech recognition performance", "author": ["R. Schluter", "H. Ney"], "venue": "Acoustics, Speech, and Signal Processing, 2001. Proceedings. (ICASSP \u201901). 2001 IEEE International Conference on, vol. 1, pp. 133\u2013136 vol.1.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2001}, {"title": "Speech processing in the auditory system I: The representation of speech sounds in the responses of the auditory nerve", "author": ["S.A. Shamma"], "venue": "Journal of the Acoustic Society of America, vol. 78, pp. 1612\u20131621, 1985.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1985}, {"title": "A model of dynamic auditory perception and its application to robust word recognition", "author": ["B. Strope", "A. Alwan"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 5, pp. 451\u2013464, 1997. 20", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1997}, {"title": "Modeling the combined effects of basilar membrane nonlinearity and roughness on stimulus frequency otoacoustic emission fine structure.", "author": ["C.L. Talmadge", "A. Tubis", "G.R. Long", "C. Tong"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2000}, {"title": "Clinically isolated syndrome manifested as acute vestibular syndrome: Bedside neurootological examination and suppression of transient evoked otoacoustic emissions in the differential diagnosis", "author": ["K. Veros", "S. Blioskas", "T. Karapanayiotides", "G. Psillas", "K. Markou", "M. Tsaligopoulos"], "venue": "American Journal of Otolaryngology, vol. 35, no. 5, pp. 683\u2013686, Sep. 2014.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2014}, {"title": "Single channel speech enhancement based on masking properties of the human auditory system", "author": ["N. Virag"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 7, 1999.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1999}, {"title": "The origin of periodicity in the spectrum of evoked otoacoustic emissions.", "author": ["G. Zweig", "C.A. Shera"], "venue": "The Journal of the Acoustical Society of America,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1995}], "referenceMentions": [{"referenceID": 28, "context": "This is in contrast to the human auditory system, which shows greater resilience to noise [26, 36].", "startOffset": 90, "endOffset": 98}, {"referenceID": 6, "context": "For humans, speech perception is a sensory and perceptual process [7, 29, 13] and in this paper we focus on the psychoacoustic and otoacoustic emission (OAE) aspects of that process.", "startOffset": 66, "endOffset": 77}, {"referenceID": 22, "context": "For humans, speech perception is a sensory and perceptual process [7, 29, 13] and in this paper we focus on the psychoacoustic and otoacoustic emission (OAE) aspects of that process.", "startOffset": 66, "endOffset": 77}, {"referenceID": 6, "context": "Psychoacoustics is the broad investigation of human speech perception and includes relationships between sound pressure level and loudness, human response to different frequencies, and a variety of masking effects [13, 7].", "startOffset": 214, "endOffset": 221}, {"referenceID": 7, "context": "To some extent, the popularity of Mel-frequency cepstral coefficients (MFCCs) are a result of this area of research [9, 22].", "startOffset": 116, "endOffset": 123}, {"referenceID": 17, "context": "To some extent, the popularity of Mel-frequency cepstral coefficients (MFCCs) are a result of this area of research [9, 22].", "startOffset": 116, "endOffset": 123}, {"referenceID": 4, "context": "Our previous work in psychoacoustics systematically investigated how speech signals are processed by the human auditory system and converted to neural spikes [5, 7, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 6, "context": "Our previous work in psychoacoustics systematically investigated how speech signals are processed by the human auditory system and converted to neural spikes [5, 7, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 5, "context": "Our previous work in psychoacoustics systematically investigated how speech signals are processed by the human auditory system and converted to neural spikes [5, 7, 6].", "startOffset": 158, "endOffset": 167}, {"referenceID": 19, "context": "We have also improved aspects of ASR by incorporating temporal integration [24, 23].", "startOffset": 75, "endOffset": 83}, {"referenceID": 18, "context": "We have also improved aspects of ASR by incorporating temporal integration [24, 23].", "startOffset": 75, "endOffset": 83}, {"referenceID": 6, "context": "Masking effects may be classified as simultaneous or temporal according to signal occurrence [7].", "startOffset": 93, "endOffset": 96}, {"referenceID": 25, "context": "Temporal masking can be viewed as a consequence of auditory adaptation [33].", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "These masking effects are caused by the principal mechanism of neuronal signal processing in both time and frequency [32, 31, 20].", "startOffset": 117, "endOffset": 129}, {"referenceID": 15, "context": "These masking effects are caused by the principal mechanism of neuronal signal processing in both time and frequency [32, 31, 20].", "startOffset": 117, "endOffset": 129}, {"referenceID": 13, "context": "Predicted by Thomas Gold in 1948, OAE was first demonstrated empirically by David Kemp in 1978 [17] and otoacoustic emissions have since been shown to arise through a number of different cellular and mechanical causes within the inner ear [1, 19].", "startOffset": 95, "endOffset": 99}, {"referenceID": 0, "context": "Predicted by Thomas Gold in 1948, OAE was first demonstrated empirically by David Kemp in 1978 [17] and otoacoustic emissions have since been shown to arise through a number of different cellular and mechanical causes within the inner ear [1, 19].", "startOffset": 239, "endOffset": 246}, {"referenceID": 4, "context": "2D psychoacoustic filter Forward masking (FM) reveals that over short durations, the usable dynamic range of the human auditory system depends on the spectral characteristics of the previous stimuli [5].", "startOffset": 199, "endOffset": 202}, {"referenceID": 6, "context": "Temporal masking can be modeled as Mtm(f, t,\u2206t) = Atm(\u2206t)Y (f, t+\u2206t), (1) where Atm(f,\u2206t) is the temporal masking parameter given in [7]; Mtm is the amount of temporal masking; and \u2206t is the signal delay [7, 24, 16].", "startOffset": 133, "endOffset": 136}, {"referenceID": 6, "context": "Temporal masking can be modeled as Mtm(f, t,\u2206t) = Atm(\u2206t)Y (f, t+\u2206t), (1) where Atm(f,\u2206t) is the temporal masking parameter given in [7]; Mtm is the amount of temporal masking; and \u2206t is the signal delay [7, 24, 16].", "startOffset": 204, "endOffset": 215}, {"referenceID": 19, "context": "Temporal masking can be modeled as Mtm(f, t,\u2206t) = Atm(\u2206t)Y (f, t+\u2206t), (1) where Atm(f,\u2206t) is the temporal masking parameter given in [7]; Mtm is the amount of temporal masking; and \u2206t is the signal delay [7, 24, 16].", "startOffset": 204, "endOffset": 215}, {"referenceID": 12, "context": "Temporal masking can be modeled as Mtm(f, t,\u2206t) = Atm(\u2206t)Y (f, t+\u2206t), (1) where Atm(f,\u2206t) is the temporal masking parameter given in [7]; Mtm is the amount of temporal masking; and \u2206t is the signal delay [7, 24, 16].", "startOffset": 204, "endOffset": 215}, {"referenceID": 6, "context": "Similarly, simultaneous masking can be modeled as Equation (2), and temporal-frequency masking can be modeled as Equation (3) [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 4, "context": "The masked speech that, in theory, is transmitted on the auditory nerves to the human brain can then be expressed as \u1ef8 (f, t) = Y (f, t)\u2212Mtotal (9) = Y (f, t)\u2297Mask where Mask is defined in Equation (7) [5, 7, 6].", "startOffset": 202, "endOffset": 211}, {"referenceID": 6, "context": "The masked speech that, in theory, is transmitted on the auditory nerves to the human brain can then be expressed as \u1ef8 (f, t) = Y (f, t)\u2212Mtotal (9) = Y (f, t)\u2297Mask where Mask is defined in Equation (7) [5, 7, 6].", "startOffset": 202, "endOffset": 211}, {"referenceID": 5, "context": "The masked speech that, in theory, is transmitted on the auditory nerves to the human brain can then be expressed as \u1ef8 (f, t) = Y (f, t)\u2212Mtotal (9) = Y (f, t)\u2297Mask where Mask is defined in Equation (7) [5, 7, 6].", "startOffset": 202, "endOffset": 211}, {"referenceID": 19, "context": "According to [24, 23], when signal durations increase, there is a considerable decrease in the mean masking thresholds (or the amount of masking).", "startOffset": 13, "endOffset": 21}, {"referenceID": 18, "context": "According to [24, 23], when signal durations increase, there is a considerable decrease in the mean masking thresholds (or the amount of masking).", "startOffset": 13, "endOffset": 21}, {"referenceID": 18, "context": "For example, Figure 1 (from [23]: Fig 1, pp735), shows that at an offset of 9 ms, mean thresholds decreased by nearly 14 dB as the signal duration increased from 2 to 7 ms.", "startOffset": 28, "endOffset": 32}, {"referenceID": 18, "context": "Temporal integration results, from [23].", "startOffset": 35, "endOffset": 39}, {"referenceID": 12, "context": "Figure 2 shows the characteristic curve of forward masking, which describes how the amount of masking, Mtotal, changes with time, \u2206t [16].", "startOffset": 133, "endOffset": 137}, {"referenceID": 12, "context": "Characteristic curve of forward masking [16, 7] As the parameters of masking effects change with frequency, ideally there should be different 2D psychoacoustic filters for different frequencies, but this can be impractical computationally.", "startOffset": 40, "endOffset": 47}, {"referenceID": 6, "context": "Characteristic curve of forward masking [16, 7] As the parameters of masking effects change with frequency, ideally there should be different 2D psychoacoustic filters for different frequencies, but this can be impractical computationally.", "startOffset": 40, "endOffset": 47}, {"referenceID": 3, "context": "A voice activity detector (energy ratio test [4]) is utilized to distinguish speech/non-speech frames.", "startOffset": 45, "endOffset": 48}, {"referenceID": 3, "context": "In our present implementation, noise is estimated using a minimum-controlled recursive movingaverage noise tracker similar to the one described in [4, 10].", "startOffset": 147, "endOffset": 154}, {"referenceID": 8, "context": "In our present implementation, noise is estimated using a minimum-controlled recursive movingaverage noise tracker similar to the one described in [4, 10].", "startOffset": 147, "endOffset": 154}, {"referenceID": 3, "context": "Generally, a decision on whether a frame contains speech or noise is made based on the energy ratio test [4], |Py (fi, ti)| 2 t |Pn (fi, ti)| 2 min > \u03bd (18) where \u03bd is the threshold, |Pn (f, t)|min is the smoothed minimum noise power within a sliding window which can be tracked efficiently and |Py (fi, ti)|t is the smoothed (using adjacent channels) power of the noisy speech [10].", "startOffset": 105, "endOffset": 108}, {"referenceID": 8, "context": "Generally, a decision on whether a frame contains speech or noise is made based on the energy ratio test [4], |Py (fi, ti)| 2 t |Pn (fi, ti)| 2 min > \u03bd (18) where \u03bd is the threshold, |Pn (f, t)|min is the smoothed minimum noise power within a sliding window which can be tracked efficiently and |Py (fi, ti)|t is the smoothed (using adjacent channels) power of the noisy speech [10].", "startOffset": 378, "endOffset": 382}, {"referenceID": 14, "context": "Otoacoustic emissions (OAEs) Otoacoustic emissions (OAEs) are clinically important because they are the basis of a simple, noninvasive, test for hearing defects in newborn babies and in children who are too young to cooperate in conventional hearing tests [18, 35].", "startOffset": 256, "endOffset": 264}, {"referenceID": 27, "context": "Otoacoustic emissions (OAEs) Otoacoustic emissions (OAEs) are clinically important because they are the basis of a simple, noninvasive, test for hearing defects in newborn babies and in children who are too young to cooperate in conventional hearing tests [18, 35].", "startOffset": 256, "endOffset": 264}, {"referenceID": 21, "context": "OAEs are considered to be related to the amplification function of the cochlea [28] and are generated within the inner ear, specifically by the motion of the nerve cells on the basilar membrane within the cochlea as they energetically respond to auditory stimulation [2].", "startOffset": 79, "endOffset": 83}, {"referenceID": 1, "context": "OAEs are considered to be related to the amplification function of the cochlea [28] and are generated within the inner ear, specifically by the motion of the nerve cells on the basilar membrane within the cochlea as they energetically respond to auditory stimulation [2].", "startOffset": 267, "endOffset": 270}, {"referenceID": 29, "context": "Previous theoretical studies have suggested that OAEs arise primarily from a linear process of coherent reflection [37, 34], which means it can be treated as the \u2018reverberation\u2019 of the input acoustic signal.", "startOffset": 115, "endOffset": 123}, {"referenceID": 26, "context": "Previous theoretical studies have suggested that OAEs arise primarily from a linear process of coherent reflection [37, 34], which means it can be treated as the \u2018reverberation\u2019 of the input acoustic signal.", "startOffset": 115, "endOffset": 123}, {"referenceID": 23, "context": "However, the phase can encapsulate useful information in speech [30, 15].", "startOffset": 64, "endOffset": 72}, {"referenceID": 11, "context": "However, the phase can encapsulate useful information in speech [30, 15].", "startOffset": 64, "endOffset": 72}, {"referenceID": 6, "context": "We provide the double transform spectrum of clean speech and the frequency reponse of the 2D psychoacoustic filter (introduced in our previous paper [7]) in Figure 5.", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "(d) 2D psychoacoustic filter [7] Fig.", "startOffset": 29, "endOffset": 32}, {"referenceID": 20, "context": "System Description: Evaluation is carried out using the AURORA2 database [25].", "startOffset": 73, "endOffset": 77}, {"referenceID": 20, "context": "The AURORA2 data are based on a version of the original TIDigits (available from LDC) downsampled to 8 kHz [25, 21].", "startOffset": 107, "endOffset": 115}, {"referenceID": 16, "context": "The AURORA2 data are based on a version of the original TIDigits (available from LDC) downsampled to 8 kHz [25, 21].", "startOffset": 107, "endOffset": 115}, {"referenceID": 20, "context": "In test set C, two types of noise (subway and street) processed by the modified intermediate reference system (MIRS) filter are added, which simulates the frequency characteristics of a telecommunication terminal [25, 21].", "startOffset": 213, "endOffset": 221}, {"referenceID": 16, "context": "In test set C, two types of noise (subway and street) processed by the modified intermediate reference system (MIRS) filter are added, which simulates the frequency characteristics of a telecommunication terminal [25, 21].", "startOffset": 213, "endOffset": 221}, {"referenceID": 20, "context": "One is \u201csil\u201d, which has 3 HMM states and models the pauses before and after each utterance, the other is \u201csp\u201d, which is a single state model (tied with the middle state of \u201csil\u201d) and models pauses among words [25, 7].", "startOffset": 209, "endOffset": 216}, {"referenceID": 6, "context": "One is \u201csil\u201d, which has 3 HMM states and models the pauses before and after each utterance, the other is \u201csp\u201d, which is a single state model (tied with the middle state of \u201csil\u201d) and models pauses among words [25, 7].", "startOffset": 209, "endOffset": 216}, {"referenceID": 20, "context": "Two pause models are defined: sil has 3 HMM states and models the pauses before and after each utterance, and sp has a single state tied with the middle state of sil and models pauses among words [25, 7].", "startOffset": 196, "endOffset": 203}, {"referenceID": 6, "context": "Two pause models are defined: sil has 3 HMM states and models the pauses before and after each utterance, and sp has a single state tied with the middle state of sil and models pauses among words [25, 7].", "startOffset": 196, "endOffset": 203}, {"referenceID": 10, "context": "The final set of comparisons is made against state-of-the-art noise removal methods frequently used in ASR systems namely RelAtive SpecTrAl (RASTA) noise removal [14], minimum mean square error (MMSE) [11], mean variance normalization & ARMA filtering (MVA, where the ARMA filter is an autoregressive moving average filter) [3], and the ETSI Advanced FrontEnd (AFE) [12].", "startOffset": 162, "endOffset": 166}, {"referenceID": 9, "context": "The final set of comparisons is made against state-of-the-art noise removal methods frequently used in ASR systems namely RelAtive SpecTrAl (RASTA) noise removal [14], minimum mean square error (MMSE) [11], mean variance normalization & ARMA filtering (MVA, where the ARMA filter is an autoregressive moving average filter) [3], and the ETSI Advanced FrontEnd (AFE) [12].", "startOffset": 201, "endOffset": 205}, {"referenceID": 2, "context": "The final set of comparisons is made against state-of-the-art noise removal methods frequently used in ASR systems namely RelAtive SpecTrAl (RASTA) noise removal [14], minimum mean square error (MMSE) [11], mean variance normalization & ARMA filtering (MVA, where the ARMA filter is an autoregressive moving average filter) [3], and the ETSI Advanced FrontEnd (AFE) [12].", "startOffset": 324, "endOffset": 327}, {"referenceID": 9, "context": "The MMSE estimator was first proposed for speech enhancement in 1984 [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 10, "context": "(28) The Relative Spectra (RASTA) was proposed by Hermansky in 1994 and is based on the fact that human perception tends to react to the relative value of an input [14].", "startOffset": 164, "endOffset": 168}, {"referenceID": 2, "context": ", \u2018lifter\u2019) and manages to effectively improve ASR performance empirically [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 6, "context": "psychoacoustic filter [7] as example to show the general steps of double transform analysis.", "startOffset": 22, "endOffset": 25}], "year": 2016, "abstractText": "Abstract: Compared with automatic speech recognition (ASR), the human auditory system is more adept at handling noise-adverse situations, including environmental noise and channel distortion. To mimic this adeptness, auditory models have been widely incorporated in ASR systems to improve their robustness. This paper proposes a novel auditory model which incorporates psychoacoustics and otoacoustic emissions (OAEs) into ASR. In particular, we successfully implement the frequency-dependent property of psychoacoustic models and effectively improve resulting system performance. We also present a novel double-transform spectrum-analysis technique, which can qualitatively predict ASR performance for different noise types. Detailed theoretical analysis is provided to show the effectiveness of the proposed algorithm. Experiments are carried out on the AURORA2 database and show that the word recognition rate using our proposed feature extraction method is significantly increased over the baseline. Given models trained with clean speech, our proposed method achieves up to 85.39% word recognition accuracy on noisy data.", "creator": "LaTeX with hyperref package"}}}