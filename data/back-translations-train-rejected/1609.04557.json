{"id": "1609.04557", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "Structured Dropout for Weak Label and Multi-Instance Learning and Its Application to Score-Informed Source Separation", "abstract": "Many success stories involving deep neural networks are instances of supervised learning, where available labels power gradient-based learning methods. Creating such labels, however, can be expensive and thus there is increasing interest in weak labels which only provide coarse information, with uncertainty regarding time, location or value. Using such labels often leads to considerable challenges for the learning process. Current methods for weak-label training often employ standard supervised approaches that additionally reassign or prune labels during the learning process. The information gain, however, is often limited as only the importance of labels where the network already yields reasonable results is boosted. We propose treating weak-label training as an unsupervised problem and use the labels to guide the representation learning to induce structure. To this end, we propose two autoencoder extensions: class activity penalties and structured dropout. We demonstrate the capabilities of our approach in the context of score-informed source separation of music.", "histories": [["v1", "Thu, 15 Sep 2016 09:50:55 GMT  (37kb,D)", "https://arxiv.org/abs/1609.04557v1", null], ["v2", "Mon, 26 Dec 2016 14:28:46 GMT  (37kb,D)", "http://arxiv.org/abs/1609.04557v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.SD", "authors": ["sebastian ewert", "mark b sandler"], "accepted": false, "id": "1609.04557"}, "pdf": {"name": "1609.04557.pdf", "metadata": {"source": "CRF", "title": "STRUCTURED DROPOUT FOR WEAK LABEL AND MULTI-INSTANCE LEARNING AND ITS APPLICATION TO SCORE-INFORMED SOURCE SEPARATION", "authors": ["Sebastian Ewert", "Mark B. Sandler"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Autoencoder, Unsupervised Learning, Penalties for Class Activity, Deep Learning."}, {"heading": "1. INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2. FROM NMF TO AN AUTOENCODER WITH STRUCTURED DROPOUT", "text": "The basic idea behind the non-negative matrix factorization is to integrate a set of N input vectors in the first world networks, which we do not actively apply, but actively apply. (That is, we associate a specific class that can be identified with one of the K-NMF components (or a group of them), we can incorporate information from weak labels into the NMF learning process by setting entries in H to zero: for example, that the class k will be inactive in input n, we can set Hk to zero - with multiplicative rules to iteratively update W andH as normally done, these constraints will remain active throughout the learning process [15]. In this way, we specify which classes are not active, but whether the class should be exactive or not."}, {"heading": "3. APPLICATION: SCORE-INFORMED SOURCE SEPARATION OF MUSIC SIGNALS", "text": "To this end, we assume that we receive an audio recording and a MIDI file that encodes the uninterpreted score for a piece of music. However, the information provided by the score provides rough information when certain instruments and pitches are active. However, the score does not specify exactly when and how notes are played, how they are manifested spectral, or what their temporal progression is. MIDI events can therefore be regarded as weak descriptions in the introduction, i.e. the temporal information is crude and uncertain. Based on these labels, we could use our proposed method as the basis for a (frame-wise) instrument or a pitch detection method that we need to train an actual classification based on our learned representation - either as an improved input or as a more timely goal."}, {"heading": "4. EXPERIMENTS", "text": "In fact, it is the case that most of them are in a position to go into a different world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "5. CONCLUSIONS", "text": "However, as these approaches usually only improve the treatment of labels or classes for which the network already delivers clear results, the impact on the learning process can be limited. Alternatively, we suggested treating the problem as an unattended learning problem in principle, i.e. starting without labels and then only creating some structure in the resulting data representations based on the weak labels learned. To this end, we introduced an activity cost concept that enabled us to train an autoencoder and express our uncertainty about the target value of a weak label. To speed up the training process, we also proposed a structured variant of the drop-out, using labels as opposed to regular drop-out labels, to enforce a specific structure on the network early during the training. Our experiments based on point-oriented source separation showed that our proposed method could actually be used to structure the proposed data structure in a future pre-stage so that we could apply the proposed method in a meaningful manner to either the proposed one."}, {"heading": "6. REFERENCES", "text": "[1] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton, \"Deep learning,\" Nature, vol. 521, no. 7553, pp. 436-444, 2015. [2] Geoffrey E Hinton, Simon Osindero, and Yee-Whye Teh, \"A fast learning algorithm for deep belief nets,\" Neural Computation, vol. 7, pp.6, pp.7, pp.7, pp.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.7, p.8, p.7, p.7, p.7, p.7, p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.8,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7,\" p.7, \"p.7"}], "references": [{"title": "Deep learning", "author": ["Yann LeCun", "Yoshua Bengio", "Geoffrey Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["Geoffrey E Hinton", "Simon Osindero", "Yee-Whye Teh"], "venue": "Neural Computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Integrated segmentation and recognition of hand-printed numerals", "author": ["James D Keeler", "David E Rumelhart", "Wee-Kheng Leow"], "venue": "Advances in Neural Information Processing Systems (NIPS), 1990, p. 557563.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1990}, {"title": "A review of multi-instance learning assumptions", "author": ["James Foulds", "Eibe Frank"], "venue": "The Knowledge Engineering Review, vol. 25, no. 01, pp. 1\u201325, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Support vector machines for multiple-instance learning", "author": ["Stuart Andrews", "Ioannis Tsochantaridis", "Thomas Hofmann"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2002, pp. 561\u2013568.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "MILES: Multipleinstance learning via embedded instance selection", "author": ["Yixin Chen", "Jinbo Bi", "James Ze Wang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 28, no. 12, pp. 1931\u20131947, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1931}, {"title": "Patch-based convolutional neural network for whole slide tissue image classification", "author": ["Le Hou", "Dimitris Samaras", "Tahsin M Kurc", "Yi Gao", "James E Davis", "Joel H Saltz"], "venue": "arXiv preprint 1504.07947, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiple-instance learning for music information retrieval", "author": ["Michael I Mandel", "Daniel PW Ellis"], "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2008, pp. 577\u2013582.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Neural networks for multi-instance learning", "author": ["Zhi-Hua Zhou", "Min-Ling Zhang"], "venue": "Proceedings of the International Conference on Intelligent Information Technology (ICIIT), 2002, pp. 455\u2013459.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Learning to pinpoint singing voice from weakly labeled examples", "author": ["Jan Schl\u00fcter"], "venue": "Proceedings of the International Society for Music Information Retrieval Conference (ISMIR), 2016, pp. 44\u201350.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["Geoffrey E Hinton", "Ruslan R Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, no. 1, pp. 1929\u20131958, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1929}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Using score-informed constraints for NMF-based source separation", "author": ["Sebastian Ewert", "Meinard M\u00fcller"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Kyoto, Japan, 2012, pp. 129\u2013132.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "Multipitch analysis with harmonic nonnegative matrix approximation", "author": ["Stanislaw Andrzej Raczynski", "Nobutaka Ono", "Shigeki Sagayama"], "venue": "Proceedings of the International Conference on Music Information Retrieval (ISMIR), Vienna, Austria, 2007, pp. 381\u2013386.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "Denoising without access to clean data using a partitioned autoencoder", "author": ["Dan Stowell", "Richard E Turner"], "venue": "arXiv preprint arXiv:1509.05982, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout training as adaptive regularization", "author": ["Stefan Wager", "Sida Wang", "Percy S Liang"], "venue": "Advances in Neural Information Processing Systems (NIPS), 2013, pp. 351\u2013359.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Score-informed source separation for musical audio recordings: An overview", "author": ["Sebastian Ewert", "Bryan Pardo", "Meinard M\u00fcller", "Mark D. Plumbley"], "venue": "IEEE Signal Processing Magazine, vol. 31, no. 3, pp. 116\u2013124, May 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "High resolution audio synchronization using chroma onset features", "author": ["Sebastian Ewert", "Meinard M\u00fcller", "Peter Grosche"], "venue": "Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Taipei, Taiwan, 2009, pp. 1869\u20131872.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["Daniel D. Lee", "H. Sebastian Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, 1999.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1999}, {"title": "Learning understandable neural networks with nonnegative weight constraints", "author": ["Jan Chorowski", "Jacek M Zurada"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 1, pp. 62\u201369, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "Proceedings of the International Conference for Learning Representations (arXiv preprint arXiv:1412.6980), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Performance measurement in blind audio source separation", "author": ["Emmanuel Vincent", "R\u00e9mi Gribonval", "C\u00e9dric F\u00e9votte"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 14, no. 4, pp. 1462\u20131469, 2006.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2006}], "referenceMentions": [{"referenceID": 0, "context": "Training neural networks is often a complex endeavor, which is somewhat simplified if clear labels are available in a supervised learning scenario [1].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "Annotating data, however, is typically an expensive process and hence there is increasing interest in using unlabeled data to support learning using smaller labeled dataset (semi-supervised learning) [2].", "startOffset": 200, "endOffset": 203}, {"referenceID": 2, "context": "Settings based on this type of weak labels are often referred to as multiple-instance learning, as one label provides spatially or temporally coarse information for a bag of instances [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 3, "context": "For learning, however, weak labels present considerable challenges [4].", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "Multiple-instance learning methods try to improve on this concept by iteratively relabeling instances as negative examples in a block [5] or pruning", "startOffset": 134, "endOffset": 137}, {"referenceID": 5, "context": "them [6, 7] if they are not clearly detected as positive after an initial naive training step, see also [8] for a comparison.", "startOffset": 5, "endOffset": 11}, {"referenceID": 6, "context": "them [6, 7] if they are not clearly detected as positive after an initial naive training step, see also [8] for a comparison.", "startOffset": 5, "endOffset": 11}, {"referenceID": 7, "context": "them [6, 7] if they are not clearly detected as positive after an initial naive training step, see also [8] for a comparison.", "startOffset": 104, "endOffset": 107}, {"referenceID": 8, "context": "Alternatively, one can learn using only the frame which yielded the clearest positive response in a block [9].", "startOffset": 106, "endOffset": 109}, {"referenceID": 9, "context": "As a special case, one can use so called saliency maps to improve the temporal accuracy for networks with multi-frame inputs [10].", "startOffset": 125, "endOffset": 129}, {"referenceID": 9, "context": "However, often such approaches are not effective [10], as re-labeling/pruning methods only re-interpret examples that are already correctly classified by the network (limiting the gain in information for the learning process), while highest-saliency learning ignores a considerable amount of annotated data that could be potentially useful for training.", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "More precisely, we start from the standard autoencoder architecture [11], where one network transforms the input to a low-dimensional representation, which is then used to re-synthesize the input \u2013 differences between the input and output are measured using a reconstruction error term.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "First, it is a variant of the well-known stochastic dropout technique for regularization [12], just that our dropout is deterministic and induces structure instead of noise.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "Second, the decoder part of the network can operate early during training under conditions it will find once the network is fully trained, resembling properties of batch-normalization [13].", "startOffset": 184, "endOffset": 188}, {"referenceID": 13, "context": "As a further interpretation, this procedure can be seen as a nonlinear extension of non-negative matrix factorization (NMF) in combination with certain activity constraints [14].", "startOffset": 173, "endOffset": 177}, {"referenceID": 13, "context": "Therefore, we motivate our procedure starting from the well-known NMF and demonstrate the capabilities of our method using a task previously addressed with NMF: Score-informed source separation of music signals [14].", "startOffset": 211, "endOffset": 215}, {"referenceID": 14, "context": "to express that class k will be inactive in input n, we can set Hk,n to zero \u2013 using multiplicative rules to iteratively updateW andH as usually done, these constraints will remain active throughout the learning process [15].", "startOffset": 220, "endOffset": 224}, {"referenceID": 10, "context": "We obtain V \u2248WWV , which is an autoencoder with linear activation functions and some non-negativity constraints on its weight matrices [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 10, "context": "learning has been found useful in a variety of tasks \u2013 however, the resulting low dimensional representation usually cannot easily be interpreted, in particular if f and h represent deep networks [11].", "startOffset": 196, "endOffset": 200}, {"referenceID": 15, "context": "This approach generalizes ideas used in [16], which used a single class: noise or no-noise.", "startOffset": 40, "endOffset": 44}, {"referenceID": 11, "context": "We therefore propose a different learning scheme, which is inspired by the idea of dropout [12].", "startOffset": 91, "endOffset": 95}, {"referenceID": 11, "context": "The idea is to disable certain units in the network, which is thus encouraged to make the most important information redundantly available in the network [12, 17], a form of regularization.", "startOffset": 154, "endOffset": 162}, {"referenceID": 16, "context": "The idea is to disable certain units in the network, which is thus encouraged to make the most important information redundantly available in the network [12, 17], a form of regularization.", "startOffset": 154, "endOffset": 162}, {"referenceID": 12, "context": "First, similar to batch normalization [13], we can accelerate the training process by supplying the synthesis layers with the conditions we expect the analysis part to deliver once it is fully trained (i.", "startOffset": 38, "endOffset": 42}, {"referenceID": 11, "context": "Further, similar to regular dropout [12], setting some units to zero naturally cuts the error back propagation and channels the entire error information to those network weights in the analysis part that are actually responsible for the active classes in a frame.", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "However, instead of introducing this added complexity, we chose a task where we can use our autoencoder directly and still demonstrate the induced structure in the representation layer: score-informed source separation [18].", "startOffset": 219, "endOffset": 223}, {"referenceID": 17, "context": "For the reconstruction error, we use the generalized Kullback-Leiber divergence, which was found useful in a variety of source separation applications [18]:", "startOffset": 151, "endOffset": 155}, {"referenceID": 13, "context": "As in [14], we improve the temporal accuracy of the score information by employing the method described in [19] to temporally align the MIDI file to the given audio recording.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "As in [14], we improve the temporal accuracy of the score information by employing the method described in [19] to temporally align the MIDI file to the given audio recording.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "Each instrument-pitch class is further subdivided following ideas presented in [14]: In each block of P units, we associate the first unit with the onset of a note being \u2018active\u2019 and the remaining units with the sustain phase being \u2018active\u2019.", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "The argument is the same used to compare Independent Component Analysis (ICA) and NMF [20]: the autoencoder is building the output not in a purely constructive way starting from the learned representation.", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "Note, as shown in [21], that this constraint does not lower the network\u2019s theoretical capability to approximate arbitrary functions.", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "To evaluate our method, we conducted a series of experiments following the experimental setup used in [14].", "startOffset": 102, "endOffset": 106}, {"referenceID": 13, "context": "As in [14], we conduct our quantitative experiments using synthetic data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 21, "context": "As optimizer, we used ADAM, a first-order method offering many features of secondorder approaches, a property often useful in cases where the objective function needs to be minimized with a relatively high accuracy [22].", "startOffset": 215, "endOffset": 219}, {"referenceID": 21, "context": "All parameters were left at their recommended values [22], except for the step size which was decreased to 1/10th of its default.", "startOffset": 53, "endOffset": 57}, {"referenceID": 22, "context": "Separation quality is assessed using the BSSEVAL toolkit [23].", "startOffset": 57, "endOffset": 61}, {"referenceID": 13, "context": "Note that, in contrast to [14], we here use the Normalized Signal-to-", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "(A): NMF baseline [14].", "startOffset": 18, "endOffset": 22}, {"referenceID": 13, "context": "As we can see, the NMF baseline [14] already yields a relatively high separation quality of 12.", "startOffset": 32, "endOffset": 36}], "year": 2016, "abstractText": "Many success stories involving deep neural networks are instances of supervised learning, where available labels power gradient-based learning methods. Creating such labels, however, can be expensive and thus there is increasing interest in weak labels which only provide coarse information, with uncertainty regarding time, location or value. Using such labels often leads to considerable challenges for the learning process. Current methods for weak-label training often employ standard supervised approaches that additionally reassign or prune labels during the learning process. The information gain, however, is often limited as only the importance of labels where the network already yields reasonable results is boosted. We propose treating weak-label training as an unsupervised problem and use the labels to guide the representation learning to induce structure. To this end, we propose two autoencoder extensions: class activity penalties and structured dropout. We demonstrate the capabilities of our approach in the context of score-informed source separation of music.", "creator": "LaTeX with hyperref package"}}}