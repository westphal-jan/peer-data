{"id": "1609.04621", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "Factored Neural Machine Translation", "abstract": "We present a new approach for neural machine translation (NMT) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network. This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary (OOV) words. By the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). In addition, we can produce new words that are not in the vocabulary. We use a morphological analyser to get a factored representation of each word (lemmas, Part of Speech tag, tense, person, gender and number). We have extended the NMT approach with attention mechanism in order to have two different outputs, one for the lemmas and the other for the rest of the factors. The final translation is built using some \\textit{a priori} linguistic information. We compare our extension with a word-based NMT system. The experiments, performed on the IWSLT'15 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate. We observe up to 2% BLEU point improvement in a simulated out of domain translation setup.", "histories": [["v1", "Thu, 15 Sep 2016 13:15:01 GMT  (356kb,D)", "http://arxiv.org/abs/1609.04621v1", "8 pages, 3 figures"]], "COMMENTS": "8 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["mercedes garc\\'ia-mart\\'inez", "lo\\\"ic barrault", "fethi bougares"], "accepted": false, "id": "1609.04621"}, "pdf": {"name": "1609.04621.pdf", "metadata": {"source": "CRF", "title": "Factored Neural Machine Translation", "authors": ["Mercedes Garc\u0131\u0301a-Mart\u0131\u0301nez", "Fethi Bougares"], "emails": ["FirstName.LastName@lium.univ-lemans.fr"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is the case that most of us are in a position to outdo ourselves and that they do not. (...) It is as if they are in a position to outdo themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2 Neural Machine Translation", "text": "The encoder decoder architecture used for NMT consists of two recurrent neural networks (RNN), one for the encoder and the other for the decoder. The encoder maps a source sequence into a continuous spatial representation, and the decoder maps the representation back to a target sequence. Our trained neural translation models are based on a bidirectional encoder decoder-deep neural network that is equipped with an attention mechanism (Bahdanau et al., 2014), as shown in Figure 2. This architecture consists of a bidirectional RNN as an encoder (as seen in level 1 of Figure 2). An input set is encoded in a sequence of annotations (one for each input word), according to the concatenation of the results of a forward and a reverse RNN. Each annotation represents the full set with a strong focus on the current word. The decoder or a conditional attention is the NNND, the attention is the ND-conditional."}, {"heading": "3 Factors in Neural Machine Translation", "text": "zjpjuj + lemma factorsAs we can see, lemmas and factors are generated separately, which in some cases results in sequences of different lengths. To solve this problem, we give priority to the length of the lemmas. Consequently, we limit the length of the factor sequence to the length of the lemmas. This is motivated by the fact that the lemmas are closer to the final destination (a word sequence) and that they are the symbols that carry the largest part of the meaning.Another problem is the feedback that the RNN receives. In the word-based model, this feedback is the embedding of the previously generated word. Since we have two outputs, we have to decide what is given to the decoder RNN. Several options are possible and will be examined in this essay (details in Section 4.3.2). For the first series of experiments, only the previous embedding of the lemmat was used as feedback (no information from the factor output)."}, {"heading": "3.1 Handling beam search with factors", "text": "The beam search method has also been extended in terms of the original approach, as we are actually dealing with two beams (one for Lemmata and one for Factors). We need to deal with the multiple outputs, because we do not want to rely solely on the Lemmata sequence to decide which are the best sequences. Then, we merge the two beams. Once the best Lemmata and Factor hypotheses are generated for each partial hypothesis, the cross-product of these output spaces is carried out. Thus, each Lemmata hypothesis is associated with each factor hypothesis. Afterwards, we retain the k-best combinations for each sample, where k is the beam size. Finally, the number of the best hypotheses is reduced back to the beam size for further processing."}, {"heading": "3.2 From factors to word", "text": "Once we have received the factorized results from the neural network, we have to resort to word representation, which is also performed with the MACAON tool, which provides the word candidate in the face of a problem and some factors."}, {"heading": "4 Experiments", "text": "We conducted several experiments in which we tried different architectures and vocabulary sizes for Factored NMT (FNMT) and compared them with the NMT system."}, {"heading": "4.1 Data processing and selection", "text": "We evaluate our approach to the task of translating spoken languages from English to French as part of the 2015 IWSLT evaluation campaign. A selection method (Rousseau, 2013) has been used, using the available parallel corpora (news commentaries, United Nations, Europarl, Wikipedia and two searched corpora) and the Technology Entertainment Design (TED3) corpus as an indomain corpus. We also perform pre-processing to convert HTML units and filter out sentences of more than 50 words for both source and target languages. Finally, we end up with a selected corpus of 2M sentences, 147K unique words for the English side and 266K unique words for the French side."}, {"heading": "4.2 Training", "text": "We chose the following hyperparameters to train the systems: the embedded and recurring layers have a dimensionality of 620 and 1000, respectively; we use a minibatch size of 80 sets trained with the Adadelta algorithm; the norm of the gradient is truncated to no more than 1 (Pascanu et al., 2012) and the weights are initialized with Xavier (Glorot and Bengio, 2010); validations begin in the second epoch and are performed every 5000 updates; the early stopping is based on BLEU with a patience of 10 (early stopping occurs after 10 evaluations without improvement in BLEU); the vocabulary size of the source languages is set to 30K; we varied the size of the initial layer from 5K to 30K to simulate different levels outside the domain data; once the model is trained, we set the bar size to 12 (since this is the default value for NMT (Bahal, if the development corresponds to us)."}, {"heading": "4.3 Factors models and results", "text": "The question which arises in this context is whether it is a pure diversion manoeuvre, or whether it is a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversification manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversification manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a diversion manoeuvre, a manoeuvre, a diversion manoeuvre, a manoeuvre"}, {"heading": "4.3.1 Evaluating each output", "text": "We have evaluated BLEU at various levels (word, lemmas or factors) using the basic NMT system with only one output (see Table 2). We compare the values with the results of the Factored NMT system, which models lemmas and factors at the same time. We observe that the difference between the results in BLEU for lemmas using the FNMT is similar to the NMT system. However, the differences in the evaluation factors between the two systems are large (2.44 difference of% BLEU).This experiment confirms that the task of predicting factors that manage very different output quantities in relation to the initial words is not easy. In the future, we will also implement factors on the input side of the neural network to test this hypothesis. In addition, we must take into account that we give more priority to the length of the lem sequence than to the factors during the bar search. This also indicates that we will adapt our architecture so that we will get better word evaluation factors in order to better predict the final BLEU."}, {"heading": "4.3.2 Feedback", "text": "As explained in Section 2, the RNN decoder is a conditional GRU [3] that is fed by the input context vector, its hidden state, and the feedback (i.e. the previously generated symbol). Since we now have two outputs, we must define what type of feedback is more suitable for the Factored NMT system. Several solutions are possible. The first assumption we made is highly dependent on the design of the factors considered, i.e. the lemmats are the most informative factors of all. Then, we tried to embed only the output lemma as feedback (see Eq.2). Lemma feedback: EL [yj] (2), where EL is the target language dilemma as a reference table, and EL [yj] is the embedding of the output word Yi. Another simple process is the sum of the embedding of the previous lemma feedback system with the previous embedding factor."}, {"heading": "4.3.3 Dependency model", "text": "One observation that can be made is that while the generation of factors might seem easier due to the small number of possible feedback factors (only 142), the BLEU value is not as high as we might expect. However, it could be argued that generating a sequence of factors in French from a sequence of English words is not an easy task. To help the factors in the prediction, we contextualize the corresponding output with the generated Lemma. This creates a dependence between the Lemma output and the Factor output. The dependence was implemented by including a transformer (see Figure 3) that projects the embedding of the Lemma into the hidden layer used to generate Factors. The results from applying these two techniques are presented in Table 4.zjpjuj + lemma factorsT Figure 3: Dependency Model% BLEU Model Feedback lemma Factors # OV Model Feedback Factors F38 Dependence MT 800 - M72.7488 M4174T 44.475 M3T M34.T with 44.T M44.T M44.T M417475 M3T"}, {"heading": "4.3.4 Qualitative analysis", "text": "We have observed some of the translation results to better understand in which cases our FNMT system performs better or worse than the NMT system.Translation examples with better BLEU performance In the first two examples of Table 5, the FNMT system receives a better BLEU score than the NMT system.The first example shows when our factor system can generate words when the NMT base system predicts unknown words. Firstly, the wordline in the source sentence is translated as a reference sentence by the FNMT system and mapped to UNK by the NMT base system. Secondly, the word adaptive is translated as adaptatifs by the FNMT system, the reference translation is adapte \u0301 s, but we can consider the FNMT choice as a better translation."}, {"heading": "5 Conclusion", "text": "We have shown that we are able to train factored NMT systems with similar performance to text-based systems, but with the advantage that we can model a vocabulary almost six times larger, with only a slight increase in computing costs. As a consequence, the OOV rate observed with the FNMT system has been reduced. In addition, the use of additional linguistic resources allows us to generate new forms that would not be included in the standard NMT system. By reducing the target-language vocabulary, we simulated an out-of-domain configuration, and we have shown that our factored NMT method works better in this case than the basic NMT system. As a future work, we would like to include linguistic features in the input. It is known that this can be helpful for NMT (Senndow and Hadding 2016) if the proposed factors could be better taken into account in the target language."}], "references": [{"title": "Factored neural language models", "author": ["Andrei Alexandrescu"], "venue": null, "citeRegEx": "Alexandrescu.,? \\Q2006\\E", "shortCiteRegEx": "Alexandrescu.", "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Factored language models and generalized parallel backoff", "author": ["Bilmes", "Kirchhoff2003] Jeff A. Bilmes", "Katrin Kirchhoff"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology: Companion Volume of the Proceedings of HLT-NAACL 2003\u2013short Papers - Volume 2, NAACL-Short", "citeRegEx": "Bilmes et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bilmes et al\\.", "year": 2003}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation. CoRR, abs/1406.1078", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "\u00c7aglar G\u00fcl\u00e7ehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation. CoRR, abs/1603.06147", "author": ["Chung et al.2016] Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chung et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Character-based neural machine translation. CoRR, abs/1603.00810", "author": ["Costa-Juss\u00e0", "Jos\u00e9 A.R. Fonollosa"], "venue": null, "citeRegEx": "Costa.Juss\u00e0 et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Costa.Juss\u00e0 et al\\.", "year": 2016}, {"title": "Multi-way, multilingual neural machine translation with a shared attention mechanism", "author": ["Firat et al.2016] Orhan Firat", "KyungHyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Firat et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Firat et al\\.", "year": 2016}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Glorot", "Bengio2010] Xavier Glorot", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS\u201910). Society for Artificial Intelligence and Statistics", "citeRegEx": "Glorot et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2010}, {"title": "Interpolated backoff for factored translation models. Association for Machine Translation in the Americas, AMTA", "author": ["Haddow", "Koehn2012] Barry Haddow", "Philipp Koehn"], "venue": null, "citeRegEx": "Haddow et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Haddow et al\\.", "year": 2012}, {"title": "On using very large target vocabulary for neural machine translation. CoRR, abs/1412.2007", "author": ["Jean et al.2014] S\u00e9bastien Jean", "Kyunghyun Cho", "Roland Memisevic", "Yoshua Bengio"], "venue": null, "citeRegEx": "Jean et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jean et al\\.", "year": 2014}, {"title": "Moses: Open source toolkit for statistical machine translation", "author": ["Koehn et al.2007] Philipp Koehn", "Hieu Hoang", "Alexandra Birch", "Chris Callison-Burch", "Marcello Federico", "Nicola Bertoldi", "Brooke Cowan", "Wade Shen", "Christine Moran", "Richard Zens", "Chris Dyer", "Ond\u0159ej Bojar", "Alexandra Constantin", "Evan Herbst"], "venue": "In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,", "citeRegEx": "Koehn et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Koehn et al\\.", "year": 2007}, {"title": "Large vocabulary SOUL neural network language models", "author": ["Le et al.2011] Hai-Son. Le", "Ilya Oparin", "Abdel. Messaoudi", "Alexandre Allauzen", "Jean-Luc Gauvain", "Fran\u00e7ois Yvon"], "venue": "In INTERSPEECH", "citeRegEx": "Le et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Le et al\\.", "year": 2011}, {"title": "Character-based neural machine translation. CoRR, abs/1511.04586", "author": ["Ling et al.2015] Wang Ling", "Isabel Trancoso", "Chris Dyer", "Alan W. Black"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Macaon, an nlp tool suite for processing word lattices", "author": ["Nasr et al.2011] Alexis Nasr", "Fred\u00e9ric B\u00e9chet", "Jean-Fran\u00e7ois Rey", "Beno\u0131\u0302t Favre", "Joseph Le Roux"], "venue": "In Proceedings of the ACL-HLT 2011 System Demonstrations,", "citeRegEx": "Nasr et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nasr et al\\.", "year": 2011}, {"title": "Understanding the exploding gradient problem. CoRR, abs/1211.5063", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": null, "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "XenC: An open-source tool for data selection in natural language processing", "author": ["Anthony Rousseau"], "venue": "The Prague Bulletin of Mathematical Linguistics,", "citeRegEx": "Rousseau.,? \\Q2013\\E", "shortCiteRegEx": "Rousseau.", "year": 2013}, {"title": "Linguistic input features improve neural machine translation. CoRR, abs/1606.02892", "author": ["Sennrich", "Haddow2016] Rico Sennrich", "Barry Haddow"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units. CoRR, abs/1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "We have extended the NMT approach with attention mechanism (Bahdanau et al., 2014) in order to have two different outputs , one for the lemmas and the other for the rest of the factors.", "startOffset": 59, "endOffset": 82}, {"referenceID": 1, "context": "Neural Machine Translation (NMT) has been further developed in the last years (Bahdanau et al., 2014).", "startOffset": 78, "endOffset": 101}, {"referenceID": 10, "context": "In contrast to the traditional phrased-based statistical machine translation (Koehn et al., 2007) that automatically translates subparts of the sentences, NMT uses the sequence to sequence of words approach (Cho et al.", "startOffset": 77, "endOffset": 97}, {"referenceID": 3, "context": ", 2007) that automatically translates subparts of the sentences, NMT uses the sequence to sequence of words approach (Cho et al., 2014).", "startOffset": 117, "endOffset": 135}, {"referenceID": 1, "context": "Recently, NMT has improved the results of the phrased-based systems (Bahdanau et al., 2014).", "startOffset": 68, "endOffset": 91}, {"referenceID": 11, "context": "This allows the system to always apply the softmax normalization on a layer with reduced size (Le et al., 2011).", "startOffset": 94, "endOffset": 111}, {"referenceID": 9, "context": "Jean et al. (2014), proposed to carefully organise the batches so that only a subset of the target vocabulary is possibly generated at training time.", "startOffset": 0, "endOffset": 19}, {"referenceID": 9, "context": "Jean et al. (2014), proposed to carefully organise the batches so that only a subset of the target vocabulary is possibly generated at training time. This allows the system to perform the softmax only on this subset during training (the complexity remains the same at test time). Another possibility is to define a structured output layer (SOUL) to handle the words not appearing in the shortlist. This allows the system to always apply the softmax normalization on a layer with reduced size (Le et al., 2011). Recently, some works have used subword units to translate instead of words. In Sennrich et al. (2015), the rare and some unknown words are encoded as subword units with the Byte Pair Encoding (BPE) ar X iv :1 60 9.", "startOffset": 0, "endOffset": 613}, {"referenceID": 4, "context": "As an extreme case, the character-level neural machine translation has been presented in several works (Chung et al., 2016; Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016) and showed very promising results.", "startOffset": 103, "endOffset": 175}, {"referenceID": 12, "context": "As an extreme case, the character-level neural machine translation has been presented in several works (Chung et al., 2016; Ling et al., 2015; Costa-Juss\u00e0 and Fonollosa, 2016) and showed very promising results.", "startOffset": 103, "endOffset": 175}, {"referenceID": 0, "context": "Some works have used factors as additional information for language modeling (Bilmes and Kirchhoff, 2003; Alexandrescu, 2006).", "startOffset": 77, "endOffset": 125}, {"referenceID": 6, "context": "Multiple output neural networks have been used before (Firat et al., 2016) with the difference that in our approach the system produces both outputs at the same time instead of scheduling them.", "startOffset": 54, "endOffset": 74}, {"referenceID": 1, "context": "Our trained neural translation models are based on a bidirectional encoder-decoder deep neural network equipped with an attention mechanism (Bahdanau et al., 2014), as described in Figure 2.", "startOffset": 140, "endOffset": 163}, {"referenceID": 1, "context": "As described in (Bahdanau et al., 2014), the annotation weights can be used to align the input words to the output words.", "startOffset": 16, "endOffset": 39}, {"referenceID": 13, "context": "The morphological and grammatical analysis is performed with the MACAON toolkit (Nasr et al., 2011).", "startOffset": 80, "endOffset": 99}, {"referenceID": 15, "context": "A selection method (Rousseau, 2013) has been applied using the available parallel corpora (news-commentary, united-nations, europarl, wikipedia, and two crawled corpora) and Technology Entertainment Design (TED3) corpus as in-domain corpus.", "startOffset": 19, "endOffset": 35}, {"referenceID": 14, "context": "The norm of the gradient is clipped to be no more than 1 (Pascanu et al., 2012) and the weights are initialized with Xavier (Glorot and Bengio, 2010).", "startOffset": 57, "endOffset": 79}, {"referenceID": 1, "context": "Once the model is trained, we set the beam size to 12 (as this is the standard value for NMT, (Bahdanau et al., 2014)) when translating the development corpus.", "startOffset": 94, "endOffset": 117}], "year": 2016, "abstractText": "We present a new approach for neural machine translation (NMT) using the morphological and grammatical decomposition of the words (factors) in the output side of the neural network. This architecture addresses two main problems occurring in MT, namely dealing with a large target language vocabulary and the out of vocabulary (OOV) words. By the means of factors, we are able to handle larger vocabulary and reduce the training time (for systems with equivalent target language vocabulary size). In addition, we can produce new words that are not in the vocabulary. We use a morphological analyser to get a factored representation of each word (lemmas, Part of Speech tag, tense, person, gender and number). We have extended the NMT approach with attention mechanism (Bahdanau et al., 2014) in order to have two different outputs , one for the lemmas and the other for the rest of the factors. The final translation is built using some a priori linguistic information. We compare our extension with a word-based NMT system. The experiments, performed on the IWSLT\u201915 dataset translating from English to French, show that while the performance do not always increase, the system can manage a much larger vocabulary and consistently reduce the OOV rate. We observe up to 2% BLEU point improvement in a simulated out of domain translation setup.", "creator": "LaTeX with hyperref package"}}}