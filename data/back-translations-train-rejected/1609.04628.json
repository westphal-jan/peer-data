{"id": "1609.04628", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "Context Aware Nonnegative Matrix Factorization Clustering", "abstract": "In this article we propose a method to refine the clustering results obtained with the nonnegative matrix factorization (NMF) technique, imposing consistency constraints on the final labeling of the data. The research community focused its effort on the initialization and on the optimization part of this method, without paying attention to the final cluster assignments. We propose a game theoretic framework in which each object to be clustered is represented as a player, which has to choose its cluster membership. The information obtained with NMF is used to initialize the strategy space of the players and a weighted graph is used to model the interactions among the players. These interactions allow the players to choose a cluster which is coherent with the clusters chosen by similar players, a property which is not guaranteed by NMF, since it produces a soft clustering of the data. The results on common benchmarks show that our model is able to improve the performances of many NMF formulations.", "histories": [["v1", "Thu, 15 Sep 2016 13:23:43 GMT  (4181kb,D)", "http://arxiv.org/abs/1609.04628v1", "6 pages, 3 figures. Full paper accepted to International Conference on Pattern Recognition ICPR 2016, Canc\\'un, Mexico"]], "COMMENTS": "6 pages, 3 figures. Full paper accepted to International Conference on Pattern Recognition ICPR 2016, Canc\\'un, Mexico", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.GT", "authors": ["rocco tripodi", "sebastiano vascon", "marcello pelillo"], "accepted": false, "id": "1609.04628"}, "pdf": {"name": "1609.04628.pdf", "metadata": {"source": "CRF", "title": "Context Aware Nonnegative Matrix Factorization Clustering", "authors": ["Rocco Tripodi", "Sebastiano Vascon", "Marcello Pelillo"], "emails": ["rocco.tripodi@unive.it", "sebastiano.vascon@unive.it", "pelillo@unive.it"], "sections": [{"heading": null, "text": "In fact, the meaning of this technique, which can be found in a particular area, depends on other dimensions as they can be found in a particular area. Thus, it can be regarded as an essential representation of the problem described by the vector space, and it can be regarded as a latent structure. The advantage of this technique is that it is delimited from other dimensions."}, {"heading": "II. NMF CLUSTERING", "text": "In fact, most of them will be able to play by the rules they have established in the past."}, {"heading": "III. GAME THEORY AND GAME DYNAMICS", "text": "Game theory was introduced by Neumann and Morgenstern [16] to develop a mathematical framework that is able to model the basics of decision-making in interactive situations. In its normal form representation, it consists of a finite set of players I = {1,.., n}, a set of pure strategies for each player Si = {s1,..., and a usage function u: S1 \u00b7... \u00b7 Sn \u2192 R, which associates strategies topayoffs. Any player can adopt a strategy to play a game and the usage function depends on the combination of strategies played simultaneously by the players in the game, not only on the strategy chosen by an individual player. An important assumption in game theory is that the players are rational and try to maximize the value of u. Moreover, in non-cooperative games, players choose their strategies independently of what other players can play and try to find the best strategy profile for a game."}, {"heading": "IV. OUR APPROACH", "text": "In this section, we present the Game Theoretic Nonnegative Matrix Factorization (GTNMF), our approach to NMF clustering refinement. The pipeline of this method is in Fig. 1. We extract the feature vectors of each object in a dataset then, depending on the NMF algorithm used, we input to NMF the feature vectors or a similarity matrix. GTNMF takes as input the matrix W obtained with NMF and the similarity graph A (see section V-C) of the dataset to generate a consistent clustering of the data. Each data point in our formulation is presented as a player who must choose his cluster affiliation. The weighted graph A measures the influence that each player has on the others. The matrix W is used to initialize the strategy space NIX."}, {"heading": "V. EXPERIMENTAL SETUP AND RESULTS", "text": "In this section we will show the performance of GTNMF on different text and image data sets and compare it with standard NMF1 [19], NMF-S [19] (like NMF, but with the similarity matrix as input instead of attributes), SymNMF [13] 2, and NNDSVD3 [4], which use the standard maximization technique to achieve hard clustering of the data. In Table II we refer to our approach as NMF algorithm + GT, which means that the GTNMF was initialized using the specialized NMFalgorithm."}, {"heading": "A. Datasets description", "text": "We used text (Reuters, RCV1, NIPS) and image datasets (COIL-20, ORL, Extended YaleB, and PIE-Expr). Authors in [13] discarded the objects belonging to small clusters to make the dataset more balanced, which simplified the task. We tested our method with this approach and also kept the datasets as they are (without reduction), resulting in situations where it is possible to have clusters with thousands of objects and clusters with only one object in the same dataset (e.g. RCV1)."}, {"heading": "B. Data preparation", "text": "The data sets were processed as suggested in [13]. In the case of an n \u00b7 m data matrix X, the similarity matrix A is constructed 1Code: http: / / github.com / kimjingu / nonnegfac-matlab 2Code: https: / / github.com / andybaoxv / symnmf 3Code: http: / / www.boutsidis.org / NNDSVD matlab Implementation.Rare according to the type of data set (text and image). In the case of textual data sets, each feature vector is standardized to Unit 2 and the cosine distance is calculated, A = xiTxj. In the case of image data sets, each feature (column) is first standardized to be in the range [0, 1] and then applied to the following kernel: Ai, j = exp {\u2212 | xi \u2212 xj | immatridemj}, where Aij is the saved phase."}, {"heading": "C. Games graph", "text": "In Sec.V-B it was explained how to generate the similarity matrix for NMF, the same methodology was used to create the payout matrix A for the GTNMF, with the only difference that in this case we use the partitioning obtained with NMF to determine the expected size of the clusters. Assuming here that the cluster number obtained by NMF provides a good insight into the size of the final clusters and, accordingly, can be used to select an appropriate number q (see Equation 8) with this information. A cluster C can be considered a fully contiguous subgraph, and therefore the number of neighbors of each element in cluster C should be at least qC = blog2 (| C |) c + 1 to guarantee the connectedness of the cluster itself. Therefore, the variable q is chosen on the basis of the same principle of [21], but instead of taking into account the apparent set of points (as in Sec.V-B), we are only referring to the subset of this game, the point 1 in the MQ."}, {"heading": "D. Evaluation measures", "text": "Our approach has been validated using two different metrics: Accuracy (AC) and Normalized Mutual Information (NMI). AC is calculated as \u2211 n = 1 \u03b4 (\u03b1i, map (li)) n, where n denotes the total number of documents in the dataset, \u03b4 (x, y) equals 1 if x and y are bundled in the same class; map (Li) maps each cluster label li to the corresponding label in the benchmark. The best image is calculated using the KuhnMunkres algorithm [22]. AC counts the number of correct cluster assignments. NMI indicates the degree of agreement between the cluster label C provided by the Ground Truth and the cluster algorithm generated. Mutual Information (MI) between the two clusters is calculated as follows:"}, {"heading": "E. Evaluation", "text": "In fact, it is that we are able to assert ourselves, that we are able to assert ourselves in the world, and that we are able to assert ourselves in the world, that we are able to stay in the world, \"he said."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we presented GTNMF, a game theory model to improve cluster results obtained with NMF that goes beyond the classical technique of final cluster assignment. The W matrix obtained with NMF can exhibit high entropy, which in many cases makes it very difficult to select a cluster. In our approach, we seek to reduce uncertainty in the matrix W by means of evolutionary dynamics and take into account context information in order to consistently label the data. In fact, our method assigns similar objects to similar clusters, taking into account the original solution achieved with NMF. We conducted a comprehensive analysis of the performance of our method and compared it with various NMF formulations and data sets of different kinds. The results of the evaluation showed that our approach is almost always able to improve the results of the NMF and that these results are practically meaningless if they have negative results."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the Samsung Global Research Outreach Program."}], "references": [{"title": "Document clustering based on nonnegative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval. ACM, 2003, pp. 267\u2013273.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning the parts of objects by nonnegative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, 1999.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1999}, {"title": "Improving non-negative matrix factorizations through structured initialization", "author": ["S. Wild", "J. Curry", "A. Dougherty"], "venue": "Pattern Recognition, vol. 37, no. 11, pp. 2217\u20132232, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "Svd based initialization: A head start for nonnegative matrix factorization", "author": ["C. Boutsidis", "E. Gallopoulos"], "venue": "Pattern Recognition, vol. 41, no. 4, pp. 1350\u20131362, 2008.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Projected gradient methods for nonnegative matrix factorization", "author": ["C.-J. Lin"], "venue": "Neural computation, vol. 19, no. 10, pp. 2756\u20132779, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "An nmf-framework for unifying posterior probabilistic clustering and probabilistic latent semantic indexing", "author": ["Z.-Y. Zhang", "T. Li", "C. Ding", "J. Tang"], "venue": "Communications in Statistics-Theory and Methods, vol. 43, no. 19, pp. 4011\u20134024, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A game-theoretic approach to word sense disambiguation", "author": ["R. Tripodi", "M. Pelillo"], "venue": "Computational Linguistics, in press.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 0}, {"title": "Document Clustering Games in Static and Dynamic Scenarios", "author": ["R. Tripodi", "M. Pelillo"], "venue": "ArXiv e-prints, Jul. 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Nonnegative matrix factorization and probabilistic latent semantic indexing: Equivalence chi-square statistic, and a hybrid method", "author": ["C. Ding", "T. Li", "W. Peng"], "venue": "Proceedings of the national conference on artificial intelligence, vol. 21, no. 1. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999, 2006, p. 342.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1999}, {"title": "Non-negative matrix factorization framework for face recognition", "author": ["Y. Wang", "Y. Jia", "C. Hu", "M. Turk"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence, vol. 19, no. 04, pp. 495\u2013511, 2005.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Multimodal representation, indexing, automated annotation and retrieval of image collections via non-negative matrix factorization", "author": ["J.C. Caicedo", "J. BenAbdallah", "F.A. Gonz\u00e1lez", "O. Nasraoui"], "venue": "Neurocomputing, vol. 76, no. 1, pp. 50\u201360, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Binary matrix factorization for analyzing gene expression data", "author": ["Z.-Y. Zhang", "T. Li", "C. Ding", "X.-W. Ren", "X.-S. Zhang"], "venue": "Data Mining and Knowledge Discovery, vol. 20, no. 1, pp. 28\u201352, 2010.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Symnmf: nonnegative low-rank approximation of a similarity matrix for graph clustering", "author": ["D. Kuang", "S. Yun", "H. Park"], "venue": "Journal of Global Optimization, vol. 62, no. 3, pp. 545\u2013574, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Nonnegative matrices", "author": ["A. Berman", "R.J. Plemmons"], "venue": "The Mathematical Sciences, Classics in Applied Mathematics, vol. 9, 1979.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1979}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Advances in neural information processing systems, 2001, pp. 556\u2013562.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Theory of Games and Economic Behavior (60th Anniversary Commemorative Edition)", "author": ["J. Von Neumann", "O. Morgenstern"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1944}, {"title": "Evolutionary stable strategies and game dynamics", "author": ["P.D. Taylor", "L.B. Jonker"], "venue": "Mathematical biosciences, vol. 40, no. 1, pp. 145\u2013156, 1978.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1978}, {"title": "Algorithms for nonnegative matrix and tensor factorizations: a unified view based on block coordinate descent framework", "author": ["J. Kim", "Y. He", "H. Park"], "venue": "Journal of Global Optimization, vol. 58, no. 2, pp. 285\u2013 319, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Self-tuning spectral clustering", "author": ["L. Zelnik-Manor", "P. Perona"], "venue": "Advances in neural information processing systems, 2004, pp. 1601\u2013 1608.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "A tutorial on spectral clustering", "author": ["U. Von Luxburg"], "venue": "Statistics and computing, vol. 17, no. 4, pp. 395\u2013416, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Matching theory north-holland mathematics studies, 121", "author": ["L. Lovasz", "M. Plummer"], "venue": "Annals of Discrete Mathematics, vol. 29, 1986.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1986}], "referenceMentions": [{"referenceID": 0, "context": "In fact, this representation gives an immediate and intuitive glance of the importance of the dimensions of each vector, a characteristic that makes NMF particularly suitable for soft and hard clustering [1].", "startOffset": 204, "endOffset": 207}, {"referenceID": 1, "context": "NMF uses different methods to initialize these matrices [2], [3], [4] and then optimization techniques", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": "NMF uses different methods to initialize these matrices [2], [3], [4] and then optimization techniques", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "NMF uses different methods to initialize these matrices [2], [3], [4] and then optimization techniques", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "are employed to minimize the differences between X and WH [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 3, "context": "The initialization of the matrices W and H [4], is crucial and can lead to different matrix decompositions, since it is performed randomly in many algorithms [6].", "startOffset": 43, "endOffset": 46}, {"referenceID": 5, "context": "The initialization of the matrices W and H [4], is crucial and can lead to different matrix decompositions, since it is performed randomly in many algorithms [6].", "startOffset": 158, "endOffset": 161}, {"referenceID": 5, "context": "In fact, the clusters are assigned independently with this approach and two different runs of the algorithm can result in different partitioning of the data, due to the random initializations [6].", "startOffset": 192, "endOffset": 195}, {"referenceID": 6, "context": "This perspective has demonstrated its efficacy in different semantic categorization scenarios [7], [8], which involve a high number of interrelated categories and require the use of contextual and similarity information.", "startOffset": 94, "endOffset": 97}, {"referenceID": 7, "context": "This perspective has demonstrated its efficacy in different semantic categorization scenarios [7], [8], which involve a high number of interrelated categories and require the use of contextual and similarity information.", "startOffset": 99, "endOffset": 102}, {"referenceID": 1, "context": "It has been successfully applied in parts-of-whole decomposition [2], object clustering [9], face recognition [10],", "startOffset": 65, "endOffset": 68}, {"referenceID": 8, "context": "It has been successfully applied in parts-of-whole decomposition [2], object clustering [9], face recognition [10],", "startOffset": 88, "endOffset": 91}, {"referenceID": 9, "context": "It has been successfully applied in parts-of-whole decomposition [2], object clustering [9], face recognition [10],", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "multimedia analysis [11], and DNA gene expression grouping [12].", "startOffset": 20, "endOffset": 24}, {"referenceID": 11, "context": "multimedia analysis [11], and DNA gene expression grouping [12].", "startOffset": 59, "endOffset": 63}, {"referenceID": 1, "context": "In many algorithms the initialization of the matrices W and H is done randomly [2] and have the drawback to always lead to different clustering results.", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "To overcome this limitation there were proposed different approaches to find the best initializations based on feature clustering [3] and SVD techniques [4].", "startOffset": 130, "endOffset": 133}, {"referenceID": 3, "context": "To overcome this limitation there were proposed different approaches to find the best initializations based on feature clustering [3] and SVD techniques [4].", "startOffset": 153, "endOffset": 156}, {"referenceID": 2, "context": "[3] uses spherical k-means to partition the columns of X into k clusters and selects the centroid of each cluster to initialize the corresponding column of W .", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Nonnegative Double Singular Value Decomposition (NNDSVD) [4] computes the k singular triplets of X , forms the unit rank matrices using the singular vector pairs, extracts from them their positive section and singular triplets and with this information initializes W and H .", "startOffset": 57, "endOffset": 60}, {"referenceID": 3, "context": "This approach has been shown to be almost as good as that obtained with random initialization [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 12, "context": "A different formulation of NMF as clustering algorithm was proposed by [13] (SymNMF).", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "Common approaches obtain an approximation of X minimizing the Frobenius norm of the difference ||X \u2212 WH || or the generalized Kullback-Leibler divergence DKL(X||WH ) [14] , using multiplicative update rules [15] or gradient methods [5].", "startOffset": 166, "endOffset": 170}, {"referenceID": 14, "context": "Common approaches obtain an approximation of X minimizing the Frobenius norm of the difference ||X \u2212 WH || or the generalized Kullback-Leibler divergence DKL(X||WH ) [14] , using multiplicative update rules [15] or gradient methods [5].", "startOffset": 207, "endOffset": 211}, {"referenceID": 4, "context": "Common approaches obtain an approximation of X minimizing the Frobenius norm of the difference ||X \u2212 WH || or the generalized Kullback-Leibler divergence DKL(X||WH ) [14] , using multiplicative update rules [15] or gradient methods [5].", "startOffset": 232, "endOffset": 235}, {"referenceID": 15, "context": "Game theory was introduced by Von Neumann and Morgenstern [16] in order to develop a mathematical framework able to model the essentials of decision making in interactive", "startOffset": 58, "endOffset": 62}, {"referenceID": 16, "context": "The replicator dynamic equation [17] is used in order to find those states, which correspond to the Nash equilibria of the games, x(t+ 1) = x(t) u(e, x) u(x, x) \u2200h \u2208 S (5)", "startOffset": 32, "endOffset": 36}, {"referenceID": 17, "context": "In this section, we show the performances of GTNMF on different text and image datasets, and compare it with standard NMF1 [19], NMF-S [19] (same as NMF but with the similarity matrix as input instead of the features), SymNMF [13]2 and NNDSVD3 [4], which use the standard maximization technique to obtain an hard clustering of the data.", "startOffset": 123, "endOffset": 127}, {"referenceID": 17, "context": "In this section, we show the performances of GTNMF on different text and image datasets, and compare it with standard NMF1 [19], NMF-S [19] (same as NMF but with the similarity matrix as input instead of the features), SymNMF [13]2 and NNDSVD3 [4], which use the standard maximization technique to obtain an hard clustering of the data.", "startOffset": 135, "endOffset": 139}, {"referenceID": 12, "context": "In this section, we show the performances of GTNMF on different text and image datasets, and compare it with standard NMF1 [19], NMF-S [19] (same as NMF but with the similarity matrix as input instead of the features), SymNMF [13]2 and NNDSVD3 [4], which use the standard maximization technique to obtain an hard clustering of the data.", "startOffset": 226, "endOffset": 230}, {"referenceID": 3, "context": "In this section, we show the performances of GTNMF on different text and image datasets, and compare it with standard NMF1 [19], NMF-S [19] (same as NMF but with the similarity matrix as input instead of the features), SymNMF [13]2 and NNDSVD3 [4], which use the standard maximization technique to obtain an hard clustering of the data.", "startOffset": 244, "endOffset": 247}, {"referenceID": 12, "context": "Authors in [13] discarded the objects belonging to small clusters in order to make the dataset more balanced, simplifying the task.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "The datasets have been processed as suggested in [13].", "startOffset": 49, "endOffset": 53}, {"referenceID": 12, "context": "F = THE DATASET HAS BEEN PRUNED AS IN [13].", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "For image datasets each feature (column) is first normalized to lie in the range [0, 1] and then it is applied the following kernel: Ai,j = exp{\u2212 ||xi\u2212xj || \u03c3i\u03c3j }, where \u03c3i is the Euclidean distance of the 7-th nearest neighbor [20].", "startOffset": 81, "endOffset": 87}, {"referenceID": 18, "context": "For image datasets each feature (column) is first normalized to lie in the range [0, 1] and then it is applied the following kernel: Ai,j = exp{\u2212 ||xi\u2212xj || \u03c3i\u03c3j }, where \u03c3i is the Euclidean distance of the 7-th nearest neighbor [20].", "startOffset": 229, "endOffset": 233}, {"referenceID": 19, "context": "The parameter q is set accordingly to [21] and represents a theoretical bound that guarantees the connectedness of a graph:", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "See [13] for further details on this phase.", "startOffset": 4, "endOffset": 8}, {"referenceID": 19, "context": "The variable q is thus chosen based on the same principle of [21] but instead of taking into account the entire set of points (as in Sec.", "startOffset": 61, "endOffset": 65}, {"referenceID": 20, "context": "The best mapping is computed using the KuhnMunkres algorithm [22].", "startOffset": 61, "endOffset": 65}], "year": 2016, "abstractText": "In this article we propose a method to refine the clustering results obtained with the nonnegative matrix factorization (NMF) technique, imposing consistency constraints on the final labeling of the data. The research community focused its effort on the initialization and on the optimization part of this method, without paying attention to the final cluster assignments. We propose a game theoretic framework in which each object to be clustered is represented as a player, which has to choose its cluster membership. The information obtained with NMF is used to initialize the strategy space of the players and a weighted graph is used to model the interactions among the players. These interactions allow the players to choose a cluster which is coherent with the clusters chosen by similar players, a property which is not guaranteed by NMF, since it produces a soft clustering of the data. The results on common benchmarks show that our model is able to improve the performances of many NMF formulations.", "creator": "LaTeX with hyperref package"}}}