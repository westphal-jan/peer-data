{"id": "1609.04747", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "15-Sep-2016", "title": "An overview of gradient descent optimization algorithms", "abstract": "Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.", "histories": [["v1", "Thu, 15 Sep 2016 17:32:34 GMT  (2226kb,D)", "http://arxiv.org/abs/1609.04747v1", "12 pages, 6 figures"], ["v2", "Thu, 15 Jun 2017 13:21:04 GMT  (7304kb,D)", "http://arxiv.org/abs/1609.04747v2", "Added derivations of AdaMax and Nadam"]], "COMMENTS": "12 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sebastian ruder"], "accepted": false, "id": "1609.04747"}, "pdf": {"name": "1609.04747.pdf", "metadata": {"source": "CRF", "title": "An overview of gradient descent optimization algorithms\u2217", "authors": ["Sebastian Ruder"], "emails": ["ruder.sebastian@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "However, these algorithms are often used as black box optimizers because practical explanations of their strengths and weaknesses are difficult to come by. This article aims to provide the reader with intuitions about the behavior of different algorithms to optimize gradient descents. / This article aims to provide the reader with intuitions about the behavior of different algorithms to optimize gradient descents that will help him use them. / In Section 2, we will first look at the different variants of gradient descents. / This article aims to provide the reader with intuitions about the behavior of different algorithms to optimize gradient descents. / In Section 2, we will first look at the different variants of gradient descents. We will then briefly summarize the challenges during the training in Section 3."}, {"heading": "2 Gradient descent variants", "text": "There are three variants of the gradient, which differ in how much data we use to calculate the gradient of the lens function. Depending on the amount of data, we make a compromise between the accuracy of the parameter update and the time it takes to update."}, {"heading": "2.1 Batch gradient descent", "text": "Vanilla Gradient Descent, also known as Batch Gradient Descent, calculates the gradient of the cost function w.r.t. to the parameters \u03b8 for the entire training data set: \u03b8 = \u03b8 \u2212 \u03b7 \u00b7 \u0432 \u03b8J (\u03b8) (1) Since we have to calculate the gradient for the entire data set in order to perform only one update, the Batch Gradient Descent can be very slow and is insoluble for data sets that do not fit into the memory. Batch Gradient Descent (loss _ function, data, parameters) also does not allow us to update our model online, i.e. with new examples on-the-fly. In the code, the Batch Gradient Descent looks like this: for i in range (nb _ epochs): params _ grad = evaluate _ gradient (loss _ function, data, params) parameters = Parameter - learning _ rate * params _ gradFor a pre-defined number of epochs, we first the computing gradient params of the grade function _ loss."}, {"heading": "2.2 Stochastic gradient descent", "text": "Stochastic Gradient Descent (SGD), on the other hand, performs a parameter update for each training example x (i) and label y (i), as it recalculates gradients for similar examples before each parameter update. SGD eliminates this redundancy by performing an update in each case. Therefore, it is usually much faster and can also be used online to learn. SGD performs frequent updates with high variance, which cause the objective function to fluctuate greatly as shown in Figure 1. While Descent Descent gradients converge to the minimum of the basin, the parameters are placed in the SGD section, SGD fluctuation on the one hand allows frequent updates with high variance, which cause the objective function to fluctuate greatly as shown in Figure 1. While Descent Descent gradients converge to the minimum of the basin convergence, the fluctuation in the SGD section is much faster and can also be used online."}, {"heading": "2.3 Mini-batch gradient descent", "text": "Mini-Batch Gradient Descent finally takes the best of both worlds and performs an update for each mini-batch of n training examples: \u03b8 = \u03b8 \u2212 \u03b7 \u00b7 \u03b8J (\u03b8; x (i: i + n); y (i: i + n))) (3) In this way, it reduces a) the variance of parameter updates that can lead to more stable convergence; and b) can fall back on highly optimized matrix optimizations common to modern deep learning libraries, which make the calculation of the gradient w.r.t. to a mini-batch very efficient. Common mini-batch sizes range from 50 to 256, but can vary for different applications. Mini-Batch Gradient Descent is typically the algorithm of choice when training a neural network, and the term SGD is usually also used when using mini-batch batches. Note: Modifications of this parameter will be left in the rest of the GD."}, {"heading": "3 Challenges", "text": "Too low a learning rate leads to painfully slow convergence, while too high a learning rate can impede convergence and cause the loss function to fluctuate or even deviate by the minimum. \u2022 Learning schedules [17] try to adjust the learning rate during the training, e.g. by reducing the learning rate according to a predefined schedule, or when the change of objective between eras falls below a threshold. However, these schedules and thresholds must be defined in advance and are therefore unable to adapt to the characteristics of a data set [4]. \u2022 In addition, the same learning rate applies to all parameter updates. If our data are sparse and our characteristics have very different frequencies, we might not want to update them all to the same extent, but perform a major update for rare characteristics. \u2022 Another challenge is that not all important points are subject to a different slowdown."}, {"heading": "4 Gradient descent optimization algorithms", "text": "In the following, we will outline some algorithms widely used by the deep learning community to address the above challenges. We will not discuss algorithms that are unfeasible in practice for high-dimensional datasets, such as second-order methods such as Newton's Method7."}, {"heading": "4.1 Momentum", "text": "In these scenarios, the SGD oscillates over the slopes of the ravine while making tentative progress toward the local optimum, as shown in Figure 2a. Momentum [16] is a method that helps accelerate the SGD in the appropriate direction and dampen vibrations as seen in Figure 2b by adding a fraction of the update vector of the past time step to the current update vector vector8vt = vvt \u2212 1 + \u03b7 \u03b8J (4). The dynamic term is usually set at 0.9 or a similar value. Essentially, when we use dynamics, we push a ball down a slope, the ball gathers momentum as it rolls downhill, and becomes faster and faster (until it reaches its final velocity when air resistance occurs)."}, {"heading": "4.2 Nesterov accelerated gradient", "text": "A sphere rolling down a hill blindly following the slope, however, is highly unsatisfactory. We would like a smarter sphere, a sphere that has an idea of where it is moving so that it knows it will slow down before the hill pushes up again. Nesterov's Accelerated Slope (NAG) [13] is one way to give our impulse concept this kind of foresight. We know that we will use our impulse concept to shift the parameters. We can now effectively look forward by calculating the gradient, not down, but down. Calculating the next position of the parameters (the gradient is missing for complete updating) gives us an approximation to the next position of the parameters (the gradient is missing for complete updating), a rough idea of where our parameters will be. We can now effectively look forward by calculating the gradient, not up, but down."}, {"heading": "4.3 Adagrad", "text": "Adagrad [7] is a gradient-based optimization algorithm that does just that: it adjusts the learning rate to the parameters and performs major updates for rare and minor updates for frequent parameters. Therefore, it is well suited for dealing with sparse data. Dean et al. [6] found that Adagrad greatly improves the robustness of SGD and uses it for the formation of large-scale neural networks at Google, which - among other things - have learned to recognize cats in Youtube videos10. [15] Adagrad uses to train GloVe word beds, as rare words require much larger updates than frequent ones. Previously, we have done an update for all parameters, like every parameter I used the same learningrate. As Adagrad uses a different learning rate for each parameter, we first show Adagrad's pro-parameter update, which we then vectorize."}, {"heading": "4.4 Adadelta", "text": "Adadelta [21] is an extension of Adagrad that attempts to reduce its aggressive, monotonously decreasing learning rate. Instead of accumulating all past quadratic gradients, Adadelta restricts the window of accumulated past gradients to a fixed size. Instead of inefficiently storing all previous quadratic gradients, the sum of gradients is recursively defined as the decreasing average of all past quadratic gradients. Then, the current average E [g2] t at the time of the step t depends (as a fraction similar to the MS term) only on the previous average and the current gradient: E [g2] t = quadratic gradients \u2212 1 + (1 \u2212 square) g2t (10) We use a similar value as a dynamic term, about 0.9. To get clarity, we are now rewriting our MS gradients: RGD we are updating our vanilla SGD update to quadratic parameterizing instead of:"}, {"heading": "4.5 RMSprop", "text": "RMSprop is an unpublished, adaptive learning rate method developed by Geoff Hinton in lecture 6e of his Coursera Class12.RMSprop and Adadelta at about the same time as Adagram's radically declining learning rates need to be solved. RMSprop is actually identical to the first Adadelta update vector we derived above: E [g2] t = 0.9E [g 2] t \u2212 1 + 0.1g 2 t\u03b8t + 1 = \u03b8t \u2212 \u03b7 \u221a E [g2] t + gt (18) RMSprop also divides the learning rate by an exponentially declining average of square degrees. Hinton suggests setting it to 0.9, while a good default for the learning rate is 0.001."}, {"heading": "4.6 Adam", "text": "Adaptive Moment Estimation (Adam) [9] is another method that calculates adaptive learning rates for each parameter. In addition to storing an exponentially decreasing average of past square gradients such as Ada\u03b2ta and RMSprop, Adam also holds an exponentially decreasing average of past gradients mt, similar to Momentum: mt = \u03b21mt \u2212 1 + (1 \u2212 \u03b21) gt = \u03b22vt \u2212 1 + (1 \u2212 \u03b22) g2t (19) mt and vt are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. Since mt and vt are initialized as vectors of 0, Adam's authors note that for the first moment (the mean) and the second moment (the uncentered variance) of the gradients they bias toward zero, especially during the initial time steps, and especially when the decay rates are low, i.e. they anticipate the first moment (i.e., 1 and 2)."}, {"heading": "4.7 Visualization of algorithms", "text": "The following two figures give some indication of the optimisation behaviour of the optimisation algorithms presented. 13In Figure 4a we see the path they took on the contours of a loss area, all of which started at the same point and took different paths to achieve the minimum. Note that Adagrad, Adadelta and RMSprop immediately moved in the right direction and converged at a similar speed, while Momentum and NAG lost their way and produced the image of a sphere rolling down the hill. However, the NAG was able to correct its course earlier due to its increased responsiveness, looking forward and heading towards the minimum. Figure 4b shows the behaviour of the algorithms at a saddle point, i.e. at a point where one dimension has a positive slope, while the other dimension has a negative slope, which is a difficulty for SGD, as we have already mentioned."}, {"heading": "4.8 Which optimizer to use?", "text": "So which optimizer should you use? If your input data is sparse, then you will probably get the best results with one of the adaptive learning rate methods. An additional advantage is that you do not have to adjust the learning rate, but will probably get the best results with the default value. In summary, RMSprop is an extension of Adagrad that deals with its radically declining learning rates. It is identical to Adadelta except that Adadelta uses the RMS of parameter updates in the Numinator Update Rule. Finally, Adam adds RMSprop bias correction and dynamics. To this extent, RMSprop, Adadelta and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. [9] show that its bias correction helps Adam do slightly better than RMSprop toward the end of the optimization, as the gradients become more economical. Adam may be the best overall setting in this respect."}, {"heading": "5 Parallelizing and distributing SGD", "text": "Given the ubiquity of large data solutions and the availability of resource-poor clusters, it is obvious to distribute SGD to further accelerate it. SGD itself is inherently sequential: step by step, we are getting closer to the minimum. Running SGD offers good convergence, but can be slow, especially for large data sets. In contrast, the asynchronous operation of SGD is faster, but suboptimal communication between workers can lead to poor convergence. In addition, we can also parallelise SGD on a machine without the need for a large computing cluster. Below, algorithms and architectures are proposed to optimize parallelized and distributed SGD."}, {"heading": "5.1 Hogwild!", "text": "Niu et al. [14] introduce an update scheme called Hogwild! that makes it possible to perform SGD updates on CPUs in parallel. Processors are allowed to access shared memory without locking the parameters; this only works if input data is sparse, as each update only modifies a fraction of all parameters, and they show that the update scheme achieves a near-optimal convergence rate in this case, as processors are unlikely to overwrite useful information."}, {"heading": "5.2 Downpour SGD", "text": "Downpour SGD is an asynchronous variant of SGD used by Dean et al. [6] in their DistBelief framework (predecessor of TensorFlow) at Google. It performs multiple replicas of a model in parallel on subsets of training data. These models send their updates to a parameter server that is split across many machines. Each machine is responsible for storing and updating a fraction of the parameters of the model. However, since replicas do not communicate with each other, e.g. by sharing weights or updates, their parameters constantly run the risk of divergence and impeding convergence."}, {"heading": "5.3 Delay-tolerant Algorithms for SGD", "text": "McMahan and Streeter [11] extend AdaGrad to the parallel setting by developing delay tolerant algorithms that adapt not only to past gradients, but also to update delays. This has proven to work well in practice."}, {"heading": "5.4 TensorFlow", "text": "TensorFlow14 [1] is Google's recently released framework for implementing and deploying large-scale machine learning models. It is based on the experience with DistBelief and is already used internally to perform calculations on a variety of mobile devices as well as on large-scale distributed systems. Released in April 2016, the distributed version is based on a calculation diagram that is split into a subgraph for each device, while communication takes place via send-receive node pairs."}, {"heading": "5.5 Elastic Averaging SGD", "text": "Zhang et al. [22] propose Elastic Averaging SGD (EASGD), which combines the parameters of workers of asynchronous SGD with an elastic force, i.e. a mean variable stored by the parameter server, which allows local variables to fluctuate further from the mean variable, theoretically allowing greater exploration of the parameter space. They demonstrate empirically that this increased exploration capacity leads to improved performance by finding new local optimizations. 14https: / / www.tensorflow.org / 15http: / / googleresearch.blogspot.ie / 2016 / 04 / announce-tensorflow-08-now-with.html"}, {"heading": "6 Additional strategies for optimizing SGD", "text": "Finally, we present additional strategies that can be used in addition to the above algorithms to further improve the performance of SGD. See [10] for a great overview of some other common tricks."}, {"heading": "6.1 Shuffling and Curriculum Learning", "text": "Therefore, it is often a good idea to re-sort the training data according to each epoch. In some cases, where we aim to solve increasingly difficult problems, providing the training examples in a meaningful sequence can actually lead to improved performance and better convergence. The method of establishing this meaningful sequence is called Curriculum Learning [3]. Zaremba and Sutskever [20] were only able to train LSTMs, evaluate simple programs using curriculum learning and show that a combined or mixed strategy is better than the naive one that sorts examples by increasing difficulty."}, {"heading": "6.2 Batch normalization", "text": "To facilitate learning, we typically normalize the initial values of our parameters by initializing them with zero mean and unit variance. As training progresses and parameters are updated to varying degrees, we lose this normalization, which slows the training down and amplifies the changes as the network deepens. Batch normalization [8] restores these normalizations for each mini-batch, and changes are also propagated retroactively through operation. By making normalization part of the model architecture, we are able to use higher learning rates and pay less attention to the initialization parameters. Batch normalization also acts as a regulator, reducing (and sometimes even eliminating) the need for dropout."}, {"heading": "6.3 Early stopping", "text": "According to Geoff Hinton: \"Early stop (is) beautiful free lunch\" 16. You should therefore always monitor errors on a validation kit during training and (with a little patience) abort them if your validation error does not improve sufficiently."}, {"heading": "6.4 Gradient noise", "text": "Neelakantan et al. [12] add noise to any gradient update following a Gaussian distribution N (0, \u03c32t): gt, i = gt, i + N (0, \u03c3 2 t) (22) They glow the variance according to the following schedule: \u03c32t = \u03b7 (1 + t) \u03b3 (23) They show that adding this noise makes networks more robust against bad initialization and helps train particularly deep and complex networks. They suspect that the added noise gives the model more chances to escape and find new local minima that are more common in deeper models.16NIPS 2015 Tutorial Slides, slide 63, http: / / www.iro.umontreal.ca / ~ bengioy / talks / DL-tutorial-NIPS2015.pdf"}, {"heading": "7 Conclusion", "text": "In this blog post, we first looked at the three variants of the slope, of which the slope is the most popular in the minibatch. Then, we examined algorithms most commonly used to optimize the SGD: Momentum, Nesterov Accelerated Gradient, Adagrad, Adadelta, RMSprop, Adam, and various algorithms to optimize the asynchronous SGD. Finally, we considered other strategies to improve the SGD, such as shuffling and curriculum learning, batch normalization, and early stopping."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.", "creator": "LaTeX with hyperref package"}}}