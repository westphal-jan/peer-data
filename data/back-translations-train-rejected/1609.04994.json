{"id": "1609.04994", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Exploration Potential", "abstract": "We introduce exploration potential, a quantity for that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem's reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.", "histories": [["v1", "Fri, 16 Sep 2016 10:55:27 GMT  (137kb,D)", "https://arxiv.org/abs/1609.04994v1", "9 pages"], ["v2", "Wed, 28 Sep 2016 14:22:32 GMT  (137kb,D)", "http://arxiv.org/abs/1609.04994v2", "9 pages, including proofs"], ["v3", "Fri, 18 Nov 2016 11:17:56 GMT  (196kb,D)", "http://arxiv.org/abs/1609.04994v3", "10 pages, including proofs"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["jan leike"], "accepted": false, "id": "1609.04994"}, "pdf": {"name": "1609.04994.pdf", "metadata": {"source": "CRF", "title": "Exploration Potential", "authors": ["Jan Leike"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "The state of art in deep RL (Mnih et al., 2015, 2016) is based on a policy that is too greedy: at every step of the way, the agent takes a random action with some probability. Nevertheless, it is a poor exploration strategy and for environments with scant rewards it is quite ineffective (for example, the Atari game \"Montezuma's Revenge\"): it simply takes too long for the agent randomwalks to enter the first reward. Exploration strategies have been proposed: with information about the environment (Sun et al., 2011; Orseau et al.; Houthooft et al., 2016) or pseudo-count (Bellemare et al., 2016). In practice, these exploration strategies are applied by adding an exploration bonus for the reward (\"intrinsic motivation\")."}, {"heading": "2 Preliminaries and Notation", "text": "An enhanced learning agent interacts with an environment in cycles: in time step t, the agent selects an action and receives a percept et = (ot, rt) consisting of an observation and a reward rt [0, 1]; the cycle then repeats for t + 1. We use \u00e6 < t to denote a story of length t. By misusing the notation, we treat history both as a result and as random variables. A policy is a function that maps a story < t and an action a to the probability probability \u03c0 (a | \u00e6 < t) of the action a after we have seen the story. < t. An environment is a function that maps a story 1: t to the probability probability."}, {"heading": "3 Exploration Potential", "text": "We look at model-based reinforcement learning, where the agent learns a model of his environment, with which we can estimate the value of any candidate policy. Specifically, let's step our estimate of the value of policy \u03c0 in due course. We assume that the agent's learning algorithm fulfills the convergence of value within politics (OPVC): V \u03c0\u00b5 (\u00e6 < t) \u2212 V \u043d\u0438\u043d\u0438\u0441\u0442\u0438\u0441\u0442\u0438\u0441\u0442\u043e (\u00e6 < t) \u2192 0 as t \u2192 \u043c - almost certainly. (1) This does not mean that our environmental model converges with the truth, but only that we learn to predict the value of the policy we are following. Convergence of value within politics does not require that we learn to predict outside politics, i.e. the value of other strategies. In particular, we could not learn to predict the value of \u00b5-optimal policies within the U.S. For example, a Bayesian mix or a true MDL-based estimator class both if the environment is OPVC."}, {"heading": "3.1 Definition", "text": "Definition 1 (exploration potential): Letter M is a class of environments and let \u00e6 < t be a story. The exploration potential is defined as EPM (\u00e6 < t): = \u2211 \u03bd \u0445 M w (\u03bd | \u00e6 < t). EP intuitively grasps the scope of exploration that is still required before learning the entire environmental class. Asymptotically, the hinterland focuses on environments that are compatible with the current environment. EP then quantifies how well the Model V understands the value of the optimal policy of compatible environments. Comment 2 (properties of EP). (i) EPM depends neither on the true environment, nor on the policy of the agent. (ii) EPM depends on the choice of the previous w and on the model of the agent of the world. (i.i.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I.I."}, {"heading": "3.2 Sufficiency", "text": "Proposal 3 (Restriction to optimality): For all \"M,\" \"V\" (\"M\"), \"V\" (\"M\"), \"V\" (\"M\"), \"V\" (\"M\"), \"V\" (\"M\"), \"V\" (\"M\"), \"V\" (\"M\"), \"V\" (\"M\"), \"V\" (\"M\"), V \"(\" M \"), V\" (\"M\"), V \"(\" M \"), V\" (\"M\"), V \"(\" M \"), M\" (\"M\"), V \"(\" M \"), V\" (\"M\"), V \"(\" M \"), V\" (\"M\"), V \"(\" M), M (\"), M\" (\"M\"), M \"(\" M), M (\"M\"), M \"(\" M \"),\" M \"(\" M \"), M\" (\"M\"), M \"(\"), M \"(\" M \"), M (\" (\"M\"), \"M\" (\"), M\" (\"M\"), M \"(\"), M (\"(\" M \"), M\" (\"), M\" (\"), M (\" (\"M\"), M \"(\"), M \"(\" M \"), M\" (\"(\" M \"), M\" (\"), M\" (\"), M\" (\"M\"), M \"(\"), M \"(\" (\"), M\" (\"M\" (\"), M\" (\"M\"), M \"(\"), M \"(\" M \"), M\" (\"), M\"), M (\"(\" (\"), M (\" M \"), M\" (\"), M (\" (\"M\"), M \"(\"), M \"(\" (\"), M\"), M (\"(\"), M (\"(\"), M (\"), M (\"), M (\"), M (\" (\"), M (\"), M (\"(\"),"}, {"heading": "3.3 Necessity", "text": "Definition 4 (political convergence) < < < p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p?? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p p? p? p? p? p? p? p? p p p? p? p p? p p? p p p p? p p? p? p p p? p p p p p p? p? p p p p? p p p p? p? p p p p? p? p p p p p? p p p p? p p p p p? p? p p p p? p p p? p p? p p p p p? p p p p p? p p p p p p? p p p p p p? p p p p p? p p p p p p p p? p? p? p p p p p? p p? p p? p p p p p p? p p p p p p? p p p p? p p p p p p? p p p p p p? p p p p p p p? p p p? p p p p p p p? p p p p p p p p p p p p p p? p p p p"}, {"heading": "4 Exploration Potential in Multi-Armed Bandits", "text": "In this section, we use experiments with multi-armed Bernoulli bandits to illustrate the properties of the exploration potential. Bernoulli bandits are an action (arm) i (1,.., k) and receive a reward rt (1,.) Bernoulli (2), where it is the true environment. Since each step is innumerable, the exploration potential is defined with an integral value instead of a sum: EPP (2,. < t): = EPP (3,. < t): = EPP (3,.): EPP (3,.): EPP (4,.): EPP (.): (. < t) EPP: (. < t) EPP: (.) EPP: (.) EPP: (.) EPP: (. < EP:."}, {"heading": "5 Discussion", "text": "Several variants of the exploration potential stated in Definition 1 are conceivable, but beyond that, they often do not fulfil at least one of the characteristics that make our definition attractive. Either they violate the necessity (Proposition 3), sufficiency (Proposition 6), our evidence for it, or they complicate the calculation of the EP. For example, we could replace the agent's exploration potential (e.g. for targeted exploration), then this definition becomes a self-referential equation and could be very difficult to solve. If one follows Dearden et al. (1999), we could consider the agent's future policy. If the actor uses the exploration potential for actions (e.g. for targeted exploration), then this definition becomes a self-referential equation and could be very difficult to solve."}, {"heading": "Acknowledgments", "text": "We would like to thank Tor Lattimore, Marcus Hutter and our staff at FHI for their valuable feedback and discussions."}], "references": [{"title": "Finite-time analysis of the multiarmed bandit problem", "author": ["Peter Auer", "Nicolo Cesa-Bianchi", "Paul Fischer"], "venue": "Machine learning,", "citeRegEx": "Auer et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2002}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["Marc G Bellemare", "Sriram Srinivasan", "Georg Ostrovski", "Tom Schaul", "David Saxton", "Remi Munos"], "venue": "Technical report,", "citeRegEx": "Bellemare et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bellemare et al\\.", "year": 2016}, {"title": "Model based Bayesian exploration", "author": ["Richard Dearden", "Nir Friedman", "David Andre"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Dearden et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Dearden et al\\.", "year": 1999}, {"title": "Curiosity-driven exploration in deep reinforcement learning via bayesian neural networks", "author": ["Rein Houthooft", "Xi Chen", "Yan Duan", "John Schulman", "Filip De Turck", "Pieter Abbeel"], "venue": "Technical report,", "citeRegEx": "Houthooft et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Houthooft et al\\.", "year": 2016}, {"title": "Near-optimal regret bounds for reinforcement learning", "author": ["Thomas Jaksch", "Ronald Ortner", "Peter Auer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Jaksch et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Jaksch et al\\.", "year": 2010}, {"title": "Optimally confident UCB: Improved regret for finite-armed bandits", "author": ["Tor Lattimore"], "venue": "Technical report,", "citeRegEx": "Lattimore.,? \\Q2015\\E", "shortCiteRegEx": "Lattimore.", "year": 2015}, {"title": "Nonparametric General Reinforcement Learning", "author": ["Jan Leike"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "Leike.,? \\Q2016\\E", "shortCiteRegEx": "Leike.", "year": 2016}, {"title": "Learning purposeful behaviour in the absence of rewards", "author": ["Marlos C Machado", "Michael Bowling"], "venue": "Technical report,", "citeRegEx": "Machado and Bowling.,? \\Q2016\\E", "shortCiteRegEx": "Machado and Bowling.", "year": 2016}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["Volodymyr Mnih", "Adri\u00e0 Puigdom\u00e8nech Badia", "Mehdi Mirza", "Alex Graves", "Timothy P Lillicrap", "Tim Harley", "David Silver", "Koray Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Mnih et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2016}, {"title": "Universal knowledge-seeking agents for stochastic environments", "author": ["Laurent Orseau", "Tor Lattimore", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Orseau et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Orseau et al\\.", "year": 2013}, {"title": "Infomax strategies for an optimal balance between exploration and exploitation", "author": ["Gautam Reddy", "Antonio Celani", "Massimo Vergassola"], "venue": "Journal of Statistical Physics,", "citeRegEx": "Reddy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Reddy et al\\.", "year": 2016}, {"title": "Learning to optimize via information-directed sampling", "author": ["Dan Russo", "Benjamin Van Roy"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Russo and Roy.,? \\Q2014\\E", "shortCiteRegEx": "Russo and Roy.", "year": 2014}, {"title": "Formal theory of creativity, fun, and intrinsic motivation (1990\u20132010)", "author": ["J\u00fcrgen Schmidhuber"], "venue": "IEEE Transactions on Autonomous Mental Development,", "citeRegEx": "Schmidhuber.,? \\Q2010\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2010}, {"title": "A Bayesian framework for reinforcement learning", "author": ["Malcolm Strens"], "venue": "In International Conference on Machine Learning,", "citeRegEx": "Strens.,? \\Q2000\\E", "shortCiteRegEx": "Strens.", "year": 2000}, {"title": "Planning to be surprised: Optimal bayesian exploration in dynamic environments", "author": ["Yi Sun", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "In Artificial General Intelligence,", "citeRegEx": "Sun et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Sun et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "More sophisticated exploration strategies have been proposed: using information gain about the environment (Sun et al., 2011; Orseau et al., 2013; Houthooft et al., 2016) or pseudo-count (Bellemare et al.", "startOffset": 107, "endOffset": 170}, {"referenceID": 9, "context": "More sophisticated exploration strategies have been proposed: using information gain about the environment (Sun et al., 2011; Orseau et al., 2013; Houthooft et al., 2016) or pseudo-count (Bellemare et al.", "startOffset": 107, "endOffset": 170}, {"referenceID": 3, "context": "More sophisticated exploration strategies have been proposed: using information gain about the environment (Sun et al., 2011; Orseau et al., 2013; Houthooft et al., 2016) or pseudo-count (Bellemare et al.", "startOffset": 107, "endOffset": 170}, {"referenceID": 1, "context": ", 2016) or pseudo-count (Bellemare et al., 2016).", "startOffset": 24, "endOffset": 48}, {"referenceID": 12, "context": "In practice, these exploration strategies are employed by adding an exploration bonus (\u2018intrinsic motivation\u2019) to the reward signal (Schmidhuber, 2010).", "startOffset": 132, "endOffset": 151}, {"referenceID": 1, "context": ", 2016) or pseudo-count (Bellemare et al., 2016). In practice, these exploration strategies are employed by adding an exploration bonus (\u2018intrinsic motivation\u2019) to the reward signal (Schmidhuber, 2010). While the methods above require the agent to have a model of its environment and formalize the strategy \u2018explore by going to where the model has high uncertainty,\u2019 there are also model-free strategies like the automatic discovery of options proposed by Machado and Bowling (2016). However, none of these explicit exploration strategies take the problem\u2019s reward structure into account.", "startOffset": 25, "endOffset": 483}, {"referenceID": 4, "context": "This is readily exposed in optimistic policies like UCRL (Jaksch et al., 2010) and stochastic policies like PSRL (Strens, 2000), but these do not make the exploration/exploitation tradeoff explicitly.", "startOffset": 57, "endOffset": 78}, {"referenceID": 13, "context": ", 2010) and stochastic policies like PSRL (Strens, 2000), but these do not make the exploration/exploitation tradeoff explicitly.", "startOffset": 42, "endOffset": 56}, {"referenceID": 10, "context": "Another exploration quantity that is both necessary and sufficient for asymptotic optimality is information gain about the optimal policy (Russo and Van Roy, 2014; Reddy et al., 2016).", "startOffset": 138, "endOffset": 183}, {"referenceID": 0, "context": "MinEP outperforms UCB1 (Auer et al., 2002) after 10 000 steps, but neither Thompson sampling nor OCUCB.", "startOffset": 23, "endOffset": 42}, {"referenceID": 2, "context": "Following Dearden et al. (1999), we could consider |V \u2217 \u03bd \u2212 V\u0302 \u2217 t | which has the convenient side-effect that it is model-free and therefore applies to more reinforcement learning algorithms.", "startOffset": 10, "endOffset": 32}, {"referenceID": 3, "context": "Based on the recent successes in approximating information gain (Houthooft et al., 2016), we are hopeful that exploration potential can also be approximated in practice.", "startOffset": 64, "endOffset": 88}], "year": 2016, "abstractText": "We introduce exploration potential, a quantity that measures how much a reinforcement learning agent has explored its environment class. In contrast to information gain, exploration potential takes the problem\u2019s reward structure into account. This leads to an exploration criterion that is both necessary and sufficient for asymptotic optimality (learning to act optimally across the entire environment class). Our experiments in multi-armed bandits use exploration potential to illustrate how different algorithms make the tradeoff between exploration and exploitation.", "creator": "LaTeX with hyperref package"}}}