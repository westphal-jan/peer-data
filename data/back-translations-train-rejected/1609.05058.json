{"id": "1609.05058", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "A Formal Solution to the Grain of Truth Problem", "abstract": "A Bayesian agent acting in a multi-agent environment learns to predict the other agents' policies if its prior assigns positive probability to them (in other words, its prior contains a \\emph{grain of truth}). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the \\emph{grain of truth problem}. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play {\\epsilon}-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.", "histories": [["v1", "Fri, 16 Sep 2016 14:00:51 GMT  (22kb)", "http://arxiv.org/abs/1609.05058v1", "UAI 2016"]], "COMMENTS": "UAI 2016", "reviews": [], "SUBJECTS": "cs.AI cs.GT cs.LG", "authors": ["jan leike", "jessica taylor", "benya fallenstein"], "accepted": false, "id": "1609.05058"}, "pdf": {"name": "1609.05058.pdf", "metadata": {"source": "CRF", "title": "A Formal Solution to the Grain of Truth Problem", "authors": ["Jan Leike"], "emails": ["jan.leike@anu.edu.au", "jessica@intelligence.org", "benya@intelligence.org"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.05 058v 1 [cs.A I] 1 6Se pA Bayesian agent acting in a multi-agent environment learns to predict the policies of other agents when the previous policy assigns them a positive probability (in other words, their previous policy contains a grain of truth). Finding a relatively large class of strategies containing the Bayes optimal strategies in relation to this class is known as the grain of the truth problem. Only small classes are known to have a grain of truth, and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of the truth problem: We construct a class of strategies that contains all compatible strategies as well as Bayes optimal strategies for each lower semicomputable before the class. If the environment is unknown, Bayes optimal agents may not be able to act optimally, even asymptotically."}, {"heading": "1 INTRODUCTION", "text": "Consider the general setup of multiple reinforcement agents that interact sequentially in a known environment with the goal of maximizing discounted reward. Each agent knows how the environment behaves, but he does not know the behavior of the other agents. The natural (Bayesian) approach would be to define a class of possible strategies that mostly use the terminology of reinforcement learning effects. For game theory readers, we have provided a dictionary in game theory in which the game theoretical strategies of pure strategy player environment infinite extensive-form-game reward (finite) of the story infinite paths of game theory Table 1: Terminology dictionary between reinforcement learning and game theory.agents could adopt and adopt a previous approach through this class. During interaction, this previous one is updated as our agent learns the behavior of others."}, {"heading": "2 REFLECTIVE ORACLES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 PRELIMINARIES", "text": "Let X denote the set of all finite strings above the alphabet X, the set X denotes the set of all infinite strings above the alphabet X, and the set X denotes the set of all finite strings above the alphabet X. For a (finite or infinite) string x of length \u2265 k we denote with x1: k the first k characters of x and with x < k the first k \u2212 1 characters of x. The notation x1: \u0430 denotes that x is an infinite string. A function f: X \u2022 R is lower semicomputable iff the set {(x, p): c the first k \u2212 1 character of x. The notation x1: \u0430 emphasizes that x is an infinite string. A function f: X \u2022 R is lower semicomputable iff the set {(x, p), the computable iff of the set {(x) > p} is computable."}, {"heading": "2.2 DEFINITION", "text": "A semiscale above the alphabet X is a functional principle: \"A.\" \"A.\" \"A.\" \"A.\" \".\" \"A.\" \"\". \"\" A. \"\" \".\" \"A.\" \"\" \".\" \"A.\" \"\" \".\" \"A.\" \"\". \"\" A. \"\". \"\". \"\". \"\". \"\" \".\" \"\". \"\" \".\" \"\" \".\" \"\" \".\" \"\". \"\". \"\". \"\". \"\". \".\" \".\" \".\" \".\". \".\" \".\". \".\". \"\". \"\". \"\" \".\" \".\" \"\". \"\". \"\". \"\". \".\" \".\" \".\". \"\". \".\". \"\". \".\". \"\". \".\". \".\". \".\". \".\" \"\". \".\" \".\". \".\" \".\" \".\". \".\". \"\". \".\" \"\". \"\". \".\". \"\". \"\". \".\" \".\" \".\" \".\". \".\" \"\". \".\". \".\". \"\". \".\". \".\". \".\". \".\". \".\". \"\" \".\". \".\". \"\" \".\". \".\". \".\". \".\". \".\". \".\" \".\". \".\" \".\". \"\". \".\". \"\". \".\". \".\". \".\". \".\". \"\". \".\". \"\". \".\". \"\". \".\". \".\". \".\" \".\". \".\". \"\". \".\". \".\". \"\" \"\". \".\" \".\". \".\" \".\". \".\" \".\". \".\". \".\". \"\". \".\". \"\". \".\". \"\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \".\". \""}, {"heading": "2.3 A LIMIT COMPUTABLE REFLECTIVE ORACLE", "text": "The proof for Theorem 4 in [FTC15a, App. B] is not constructive and uses the axiom of choice. In Section 2.4, we give constructive proof for the existence of reflective oracles and show that there is one that is limited in calculability. Theorem 6 (A Limit Computable Reflective Oracle). There is a reflective oracle that is limited in calculability. However, this theorem has the immediate consequence that reflective oracles cannot be used as braking oracles. At first, this result may seem surprising: according to the definition of reflective oracles, they make concrete statements about the performance of ministerial turing machines. However, the fact that the oracles actually move some of the time is such that a holding can no longer be decided from the oracle exit."}, {"heading": "2.4 PROOF OF THEOREM 6", "text": "The idea for proving theorem 6 is to construct an algorithm that outputs an infinite series of partial oracles that form a reflective oracle in the delimitation. The number of queries can be counted so that we can assume that we have an approximate number of partial oracles thereof: T \u00b7 0 \u00b7 1 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \u00b7 0 \"."}, {"heading": "3 A GRAIN OF TRUTH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 NOTATION", "text": "In reinforcement learning, an actor interacts with an environment in cycles: in time step t, the actor selects an action at \"A\" and gets a perception at \"A.\" < \"E\" consists of an observation at \"O\" and a real graded reward at \"R\"; the cycle then repeats at \"N.\" A story is an element of \"A\" and \"E.\" In this section, we use \"A\" and \"E\" to denote an interaction cycle, and \"E\" < \"T\" denotes a story of length t \u2212 1. We put a discount function at \"N\" R \"with\" 0 \"and\" T. \"<\" T. \"The goal of reinforcement learning is to maximize discounted rewards at\" T. \"The discount normalization factor is defined as\" T \": =\" K \"T.\""}, {"heading": "3.2 REFLECTIVE BAYESIAN AGENTS", "text": "From now on, we assume that the action space A: = = \u00b2, \u03b2 \u00b2, is two-dimensional. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < &lt"}, {"heading": "3.3 REFLECTIVE-ORACLE-COMPUTABLE POLICIES", "text": "This sub-section is devoted to the following result, which was previously listed in [FST15, Alg. 6] but not proven. < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "3.4 SOLUTION TO THE GRAIN OF TRUTH PROBLEM", "text": "Conclusion 24 (Solving the Truth Problem): For each lower semi-predictable prediction, the Bayes mixture is reflective-oracle-calculable if it corresponds to the Bayes mixture defined in sentence 3. Proof. From sentences 20 and sentence 22. Therefore, the environmental class MOrefl contains every reflective, oracle-calculable modification of the Bayes optimum policy, including in particular predictable multi-agent environments containing other Bayesian agents above class MOrefl. Thus, each Bayesian agent above class M O refl has a grain of truth, although the environment may contain other Bayesian agents of equal strength."}, {"heading": "4 MULTI-AGENT ENVIRONMENTS", "text": "This section summarizes our results for multi-agent systems, the evidence can be found in [Lei16]."}, {"heading": "4.1 SETUP", "text": "In a multi-agent environment, there are n actors who take sequential action from the finite action space. In each time step t = 1, 2,.. the environment receives an action ait of agent i and outputs n percepts e1t,.., e n t \u00b2 E, one foreach agent. Each percept eit = (o i t, r i t) contains an observation and a reward r i t \u00b2 [0, 1]. Important is that agent i only his own action ait and his own perception e i t (see Figure 2). We use the short formula: (a1t,.,., a n \u00b2 t) and et: (e 1 t,.,.) and denote i < t = a 1e i < t = a 1e i 1."}, {"heading": "4.2 INFORMED REFLECTIVE AGENTS", "text": "Theorem 25 (Optimal Multi-Agent Policies): For each reflective-oracle-compatible multi-agent environment there is an optimal policy. For each reflective-oracle-compatible multi-agent environment there is an optimal policy. For each reflective-oracle-compatible multi-agent environment there are the optimal policies. The optimal policies, which consist of several agents, are.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4.3 LEARNING REFLECTIVE AGENTS", "text": "Since our class MOrefl solves the grain-of-truth problem, the result of Kalai and Teacher [KL93] is immediately implied that for all Bayesian agents \u03c01,.., \u03c0n interacting agents in an endlessly repeated game and for all other i,.., n, there is almost certainly a t0 that for all t, the policy \u03c0i is an endless-best answer. However, this depends on the important fact that each agent must know the game and also that all other agents are Bayesian agents. Otherwise, the convergence to a \u03b5-Nash equilibrium may have failed, as explained by the following example.At the heart of the following construction it is a dogmatic previous [LH15a, Sec. 3.2]. A dogmatic previous assignment high probability to go to hell (reward 0 eternal) if the agent deviates from a given compatible policy."}, {"heading": "5 DISCUSSION", "text": "This work introduced the class of all reflective-oracle computable environments MOrefl. This class solves the grain-of-truth problem because it contains Bayesian agents defined by MOrefl (any computable modification): the optimal agents and Bayes optimal agents across the class are all reflective-oracle computable (Theorem 22 and logical sequence 24). If the environment is unknown, a Bayesian agent may end up playing suboptimally (Example 27). However, if each agent applies a policy that is asymptotically optimal on average (like the Thompson sampling policy [LLOH16]), the agents converge for each \u03b5 > 0 to a \u03b5 > Nash equilibrium (Theorem 28 and Corresponding 29). However, our solution to the truth problem is purely theoretical. However, Theorem 6 shows that our class allows MOrefl computable approximations (Theorem 28 and Corresponding 29)."}, {"heading": "Acknowledgements", "text": "We thank Marcus Hutter and Tom Everitt for valuable comments."}, {"heading": "A LIST OF NOTATION", "text": ": = defines as equal N the natural numbers, starting with 0 Q the rational numbers R the real numbers (current) time step, t \u00b2 N k, i time steps, natural numbers p a rational number X \u00b2 the amount of all finite strings above the alphabet X \u00b2 the set of all infinite strings above the alphabet X \u00b2 the set of all finite and infinite strings above the alphabet X \u00b7 a reflective Orakel O \u00b2 a subdivision Orakel q a query to a reflective Orakel T the set of all probable Turing machines that can query an Orakel T, T \u00b2 probable Turing machines that can query an Orakel, T \u00b2 T K (x) the Kolmogorov complexity of a string x \u00b2 the semidimensions corresponding to the probable Turing machine T \u00b2 the optimal environment, the semidimensions of the optimal Turrakel environment environment environment environment"}], "references": [{"title": "Reflective variants of Solomonoff induction and AIXI", "author": ["Benja Fallenstein", "Nate Soares", "Jessica Taylor"], "venue": "In Artificial General Intelligence. Springer,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Reflective oracles: A foundation for classical game theory", "author": ["Benja Fallenstein", "Jessica Taylor", "Paul F Christiano"], "venue": "Technical report, Machine Intelligence Research Institute,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "Reflective oracles: A foundation for game theory in artificial intelligence", "author": ["Benja Fallenstein", "Jessica Taylor", "Paul F Christiano"], "venue": "In Logic, Rationality, and Interaction,", "citeRegEx": "Fallenstein et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fallenstein et al\\.", "year": 2015}, {"title": "On the impossibility of predicting the behavior of rational agents", "author": ["Dean P Foster", "H Peyton Young"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "Foster and Young.,? \\Q2001\\E", "shortCiteRegEx": "Foster and Young.", "year": 2001}, {"title": "Open problems in universal induction & intelligence", "author": ["Marcus Hutter"], "venue": null, "citeRegEx": "Hutter.,? \\Q2009\\E", "shortCiteRegEx": "Hutter.", "year": 2009}, {"title": "Rational learning leads to Nash equilibrium", "author": ["Ehud Kalai", "Ehud Lehrer"], "venue": null, "citeRegEx": "Kalai and Lehrer.,? \\Q1993\\E", "shortCiteRegEx": "Kalai and Lehrer.", "year": 1993}, {"title": "Introduction to Metamathematics", "author": ["Stephen Cole Kleene"], "venue": "Wolters-Noordhoff Publishing,", "citeRegEx": "Kleene.,? \\Q1952\\E", "shortCiteRegEx": "Kleene.", "year": 1952}, {"title": "Nonparametric General Reinforcement Learning", "author": ["Jan Leike"], "venue": "PhD thesis, Australian National University,", "citeRegEx": "Leike.,? \\Q2016\\E", "shortCiteRegEx": "Leike.", "year": 2016}, {"title": "General time consistent discounting", "author": ["Tor Lattimore", "Marcus Hutter"], "venue": "Theoretical Computer Science,", "citeRegEx": "Lattimore and Hutter.,? \\Q2014\\E", "shortCiteRegEx": "Lattimore and Hutter.", "year": 2014}, {"title": "Bad universal priors and notions of optimality", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Conference on Learning Theory, pages 1244\u20131259,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "On the computability of AIXI", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "On the computability of Solomonoff induction and knowledge-seeking", "author": ["Jan Leike", "Marcus Hutter"], "venue": "In Algorithmic Learning Theory,", "citeRegEx": "Leike and Hutter.,? \\Q2015\\E", "shortCiteRegEx": "Leike and Hutter.", "year": 2015}, {"title": "Thompson sampling is asymptotically optimal in general environments", "author": ["Jan Leike", "Tor Lattimore", "Laurent Orseau", "Marcus Hutter"], "venue": "In Uncertainty in Artificial Intelligence,", "citeRegEx": "Leike et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Leike et al\\.", "year": 2016}, {"title": "An Introduction to Kolmogorov Complexity and Its Applications", "author": ["Ming Li", "Paul M.B. Vit\u00e1nyi"], "venue": "Texts in Computer Science. Springer,", "citeRegEx": "Li and Vit\u00e1nyi.,? \\Q2008\\E", "shortCiteRegEx": "Li and Vit\u00e1nyi.", "year": 2008}, {"title": "Prediction, optimization, and learning", "author": ["John H Nachbar"], "venue": "in repeated games. Econometrica,", "citeRegEx": "Nachbar.,? \\Q1997\\E", "shortCiteRegEx": "Nachbar.", "year": 1997}, {"title": "Asymptotic non-learnability of universal agents with computable horizon functions", "author": ["Laurent Orseau"], "venue": "Theoretical Computer Science,", "citeRegEx": "Orseau.,? \\Q2013\\E", "shortCiteRegEx": "Orseau.", "year": 2013}, {"title": "Multiagent Systems: Algorithmic, GameTheoretic, and Logical Foundations", "author": ["Yoav Shoham", "Kevin Leyton-Brown"], "venue": null, "citeRegEx": "Shoham and Leyton.Brown.,? \\Q2009\\E", "shortCiteRegEx": "Shoham and Leyton.Brown.", "year": 2009}, {"title": "Complexity-based induction systems: Comparisons and convergence theorems", "author": ["Ray Solomonoff"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "Solomonoff.,? \\Q1978\\E", "shortCiteRegEx": "Solomonoff.", "year": 1978}], "referenceMentions": [], "year": 2016, "abstractText": "A Bayesian agent acting in a multi-agent environment learns to predict the other agents\u2019 policies if its prior assigns positive probability to them (in other words, its prior contains a grain of truth). Finding a reasonably large class of policies that contains the Bayes-optimal policies with respect to this class is known as the grain of truth problem. Only small classes are known to have a grain of truth and the literature contains several related impossibility results. In this paper we present a formal and general solution to the full grain of truth problem: we construct a class of policies that contains all computable policies as well as Bayes-optimal policies for every lower semicomputable prior over the class. When the environment is unknown, Bayes-optimal agents may fail to act optimally even asymptotically. However, agents based on Thompson sampling converge to play \u03b5-Nash equilibria in arbitrary unknown computable multi-agent environments. While these results are purely theoretical, we show that they can be computationally approximated arbitrarily closely.", "creator": "LaTeX with hyperref package"}}}