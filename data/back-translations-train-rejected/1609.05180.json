{"id": "1609.05180", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Grammatical Templates: Improving Text Difficulty Evaluation for Language Learners", "abstract": "Language students are most engaged while reading texts at an appropriate difficulty level. However, existing methods of evaluating text difficulty focus mainly on vocabulary and do not prioritize grammatical features, hence they do not work well for language learners with limited knowledge of grammar. In this paper, we introduce grammatical templates, the expert-identified units of grammar that students learn from class, as an important feature of text difficulty evaluation. Experimental classification results show that grammatical template features significantly improve text difficulty prediction accuracy over baseline readability features by 7.4%. Moreover, we build a simple and human-understandable text difficulty evaluation approach with 87.7% accuracy, using only 5 grammatical template features.", "histories": [["v1", "Fri, 16 Sep 2016 19:12:30 GMT  (234kb,D)", "http://arxiv.org/abs/1609.05180v1", null], ["v2", "Wed, 15 Feb 2017 22:04:47 GMT  (206kb,D)", "http://arxiv.org/abs/1609.05180v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI", "authors": ["shuhan wang", "erik", "ersen"], "accepted": false, "id": "1609.05180"}, "pdf": {"name": "1609.05180.pdf", "metadata": {"source": "CRF", "title": "Grammatical Templates: Improving Text Difficulty Evaluation for Language Learners", "authors": ["Shuhan Wang"], "emails": ["forsona@cs.cornell.edu", "eland@cs.cornell.edu"], "sections": [{"heading": null, "text": "Keywords Text Difficulty Assessment, Grammar Templates, Language Learners"}, {"heading": "1 Introduction", "text": "Assessing the difficulty of texts or the readability of texts is an important topic in the processing of natural language and applied linguistics (Samanian and Heydari, 2012; Pitler and Nenkova, 2008; Fulcher, 1997).A key challenge in assessing text difficulty is that the linguistic difficulty results from both vocabulary and grammar (Richards and Schmidt, 2013).However, most existing tools either do not sufficiently take into account the effects of grammatical difficulty (Smith III et al., 2014; Renaissance Learning, 2016; Sheehan et al., 2014) or use traditional syntactic features that differ from what language students actually learn to estimate grammatical complexity (Swarm and Ostendorf, 2005; Heilman et al., 2008).In fact, language courses introduce grammar constructs along with vocabulary, and grammatical constructs that differ in frequency and difficulty, as vocabulary, as grammar, as vocabulary, etc."}, {"heading": "2 Related Work", "text": "The assessment of the difficulty of texts has been extensively studied in recent decades (Nelson et al., 2012; Sinha et al., 2012; Hancke et al., 2012; Jameel et al., 2012; Gonzalez-Dios et al., 2014; Sinha et al., 2014). However, researchers have developed more than 200 measures of the difficulty of texts (Collins-Thompson and Callan, 2004). Lexile measures, for example, the complexity and readability of texts with word frequency and sentence length (Smith III et al., 2014). ATOS includes two formulae for texts and books, both of which take into account three variables to predict the difficulty of texts: word length, note and sentence length (Renaissance Learning, 2016). Text Evaluator is a comprehensive text analysis system that helps teachers and test developers evaluate the complexity characteristics of reading materials and books (Sheehan), with both variables measuring the difficulty of text length depending on sentence length: word length, learning degree and suitability."}, {"heading": "3 Grammatical Template Analysis", "text": "A key challenge in modelling text templates is to provide all the knowledge required to understand a particular sentence. \u2022 Traditional methods measure the difficulty of the text primarily by assessing the complexity of the vocabulary (number of words, word frequency, word type, etc.). This is effective for native speakers who typically understand the grammar of their language but vary in their command of the vocabulary. \u2022 To solve this, we focus our research on grammatical difficulties. \u2022 We introduce the idea of grammar templates, which are identified by language teachers and linguists as the most important grammatical knowledge and which are typically highlighted as key points in each textbook lesson (Banno et al., 2011; People's Education Press, 2013)."}, {"heading": "3.1 Difficulty Evaluation Standard", "text": "To assess the difficulty of texts and grammar templates, we follow the standard of the JapaneseLanguage Proficiency Test (JLPT). The JLPT is the most widely used test to measure the language skills of non-native speakers, with approximately 610,000 examinees in 62 countries and territories worldwide in 20112. It comprises five different levels ranging from N5 (beginner) to N1 (advanced). A summary of levels can be found on the JLPT website 3."}, {"heading": "3.2 Grammatical Template Library", "text": "Due to their importance in Japanese education, grammatical templates are well studied by Japanese teachers and researchers. Grammar templates are summarized and collected for both Japanese learners (common templates) and native speakers (templates used in very formal Japanese or ancient Japanese).We refer to 3 books on grammatical templates for Japanese learners (Sasaki and Kiko, 2010; Xu and Reika, 2015; Liu and Ebihara, 2012), which divide their templates into N1-N5 levels to generate our template library at each appropriate level.Although not common, books may have different opinions about the difficulty of doing the same thing. For example, a N1 template in Book A can be recognized as a N2 template in Book B. To perform a long list of Japanese grammatical templates with English translations, we write it down under Ultra (JGram, 2016).There is also a nice and comprehensive book of Japanese grammatical templates."}, {"heading": "3.3 Grammatical Template Extraction", "text": "The framework for extracting grammatical templates is represented in algorithm 1. The program uses the dependency-based parse tree of a sentence as input, runs from bottom to top, and returns a set of all identified grammatical templates T. Line 7 extracts the templates in the children of node0 (and ignores the children's descendants) by comparing the sentence associated with the child nodes [node1, node2, \u00b7 \u00b7 \u00b7] with all templates stored in our library with respect to regular expressions. Matching is based both on the structure of the sentences and on the properties of the words. Line 8 shows T (node2, \u00b7 \u00b7] covers all templates identified in subtrees that are rooted in node0's children, and the templates extracted in the phrase that are associated with the child nodes [node1, node2, \u00b7] \u00b7 associated with the third row of the Japanese word templates. We will use a third row of the Japanese word algorithm and a row of the Japanese word templates."}, {"heading": "4 Statistics of Grammatical Templates", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Corpus", "text": "We build our corpus from two sources: past JLPT exams and textbooks. The reading texts from JLPT exams are ideal for experimenting with difficulty assessment, as they are all marked with significant difficulty grades and JLPT issues will be published before 2010.4 We have also collected reading texts from two popular series of Japanese textbooks: Standard Japanese (People's Education Press, 2013) and Genki (Banno et al., 2011). Standard Japanese I and Genki I are designed for level N5 (the first semester), Standard Japanese II and Genki II for level N4 (the second semester). Ultimately, our corpus consists of 220 texts (150 from past JLPT exams and 70 from textbooks), making a total of 167,292 words after segmentation."}, {"heading": "4.2 Results", "text": "For texts with different difficulties, we calculate the distribution of grammatical templates N1-N5, which are shown in Table 3. We can see that N1 texts have a higher proportion of N1 and N2 templates than N2 texts, which means that the increase in difficulty from N2 to N1 is due to the increasing use of advanced grammar. It is also clear that even in texts of advanced levels, the majority of sentences are organized by elementary grammatical templates, and the advanced templates at or above these levels are only occasionally used for formality or precision. We also calculate the number of templates per 100 sentence at each level, which is shown in Table 4. When comparing two adjacent levels (e.g. N2 and N3), the templates at or above these levels appear to be the most significant. For example, N1 / N2 texts differ in the number of templates N1 and N2, while they have a similar number of N3 numbers to N1, N5 numbers, and N3 notes."}, {"heading": "5 Difficulty Level Prediction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Multilevel Linear Classification", "text": "We distinguish between two adjacent levels by looking at knowledge \"at the border\" and \"outside the border.\" Specifically, when assessing whether a text is harder than level Ni, we look at a grammatical template as: \u2022 within the border if the template is easier than Ni (Ni + 1 to N5); \u2022 at the border if the template is exactly on level Ni; \u2022 outside the border if the template is harder than Ni (N1 to Ni \u2212 1). We found that texts of adjacent levels are almost linearly separable, with two characteristics: templates \"at the border\" and templates \"outside the border.\" For example, Figure 1 shows how texts from N1 and N2 are separated linearly, based on the numbers of N1 and N2 templates: We can easily obtain a two-dimensional linear classifier that separates N1 and N2 with 83.4% accuracy."}, {"heading": "5.2 Features", "text": "We conduct our experiments on the basis of the following 4 characteristics: First, our set of grammatical templates has only 5 characteristics: \u2022 Average number of grammatical templates N1-N5 per sentence We compare our work with recent studies on the evaluation of readability (Kim et al., 2014; Pitler and Nenkova, 2008). In our experiments, the set of basic readability features consists of the following 12 characteristics: \u2022 Number of words per sentence \u2022 Average number of clauses per sentence \u2022 Average number of words per sentence \u2022 Average depth of parse trees per sentence \u2022 Average number of nouns per sentence \u2022 Average number of verb phrases per sentence \u2022 Average number of pronouns per sentence \u2022 Average number of clauses per sentence \u2022 Average cosmic similarity between adjacent sentences \u2022 Average word overlap between adjacent sentences \u2022 Average word overlap between nouns per sentence \u2022 Average word overlap over nouns and pronouns per sentence \u2022 Average number of pronouns per sentence \u2022 Average number of clauses per sentence \u2022 Average cosmic similarity between adjacent sentences \u2022 Average cosmic similarity between adjacent sentences per sentence \u2022 Average word overlap between nouns per sentence \u2022 Average word overlap between nouns per sentence \u2022 Average verbal overlap between nouns per sentence \u2022 Average noun and pronouns per sentence"}, {"heading": "5.3 Result", "text": "We test k-Nearest Neighbor and Support Vector Machines (Joachims, 1998) for each feature set. Implementations of these two popular classification algorithms are provided by the WEKA Toolkit (Hall et al., 2009) and LibSVM (Chang and Lin, 2011). SVMs use RBF cores (Chang et al., 2010). We also test our Multilevel Linear Classification (MLC) algorithm using the grammatical template set. We use 5x cross-validation to avoid overfitting. Table 5 shows the results. Comparing the results of kNN and SVM across the four different feature sets in Table 5, it becomes clear that TF-IDF functions have the largest feature set, but the lowest accuracy, indicating the general word-based text classification techniques, do not work well on text difficulty level prediction. Comparison with base features of our graphics is higher, but the number of graphics is higher."}, {"heading": "6 Conclusion", "text": "We proposed a new method for assessing text difficulty that focuses on grammar and draws on proven grammatical templates, and significantly improved the accuracy of text difficulty assessment when learning the Japanese language. We also introduced a simple, human-comprehensible and effective approach to assessing text difficulty that uses only five grammatical templates. In future work, we are interested in extending our work to other languages such as English and adapting grammatical templates for native speakers of different languages. We also plan to develop a machine learning system that could detect discriminatory grammatical templates from competent-rated text. Finally, we hope to use our approach to recommend texts to individual learners at an appropriate level of difficulty."}, {"heading": "Acknowledgements", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [{"title": "GENKI: An Integrated Course in Elementary Japanese", "author": ["Banno et al.2011] Eri Banno", "Yoko Ikeda", "Yutaka Ohno"], "venue": "Japan Times and Tsai Fong Books", "citeRegEx": "Banno et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Banno et al\\.", "year": 2011}, {"title": "A constructivist approach to grammar: Teaching teachers to teach aspect", "author": ["Carl Blyth"], "venue": "The Modern Language Journal,", "citeRegEx": "Blyth.,? \\Q1997\\E", "shortCiteRegEx": "Blyth.", "year": 1997}, {"title": "Combining lexical and grammatical features to improve readability measures for first and second language texts", "author": ["Callan", "Eskenazi2007] Jamie Callan", "Maxine Eskenazi"], "venue": "In Proceedings of NAACL HLT,", "citeRegEx": "Callan et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Callan et al\\.", "year": 2007}, {"title": "Libsvm: a library for support vector machines", "author": ["Chang", "Lin2011] Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology (TIST),", "citeRegEx": "Chang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2011}, {"title": "Training and testing low-degree polynomial data mappings via linear svm", "author": ["Chang et al.2010] Yin-Wen Chang", "Cho-Jui Hsieh", "Kai-Wei Chang", "Michael Ringgaard", "Chih-Jen Lin"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Chang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Chang et al\\.", "year": 2010}, {"title": "A language modeling approach to predicting reading difficulty", "author": ["Collins-Thompson", "James P Callan"], "venue": "In HLT-NAACL,", "citeRegEx": "Collins.Thompson et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Collins.Thompson et al\\.", "year": 2004}, {"title": "Assisting european portuguese teaching: Linguistic features extraction and automatic readability classifier", "author": ["Curto et al.2015] Pedro Curto", "Nuno Mamede", "Jorge Baptista"], "venue": "In Computer Supported Education,", "citeRegEx": "Curto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Curto et al\\.", "year": 2015}, {"title": "Text difficulty and accessibility: Reading formulae and expert judgement", "author": ["Glenn Fulcher"], "venue": null, "citeRegEx": "Fulcher.,? \\Q1997\\E", "shortCiteRegEx": "Fulcher.", "year": 1997}, {"title": "Simple or complex? assessing the readability of basque texts", "author": ["Mar\u0131\u0301a Jes\u00fas Aranzabe", "Arantza D\u0131\u0301az de Ilarraza", "Haritz Salaberri"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Gonzalez.Dios et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gonzalez.Dios et al\\.", "year": 2014}, {"title": "The weka data mining software: an update", "author": ["Hall et al.2009] Mark Hall", "Eibe Frank", "Geoffrey Holmes", "Bernhard Pfahringer", "Peter Reutemann", "Ian H Witten"], "venue": "ACM SIGKDD explorations newsletter,", "citeRegEx": "Hall et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Hall et al\\.", "year": 2009}, {"title": "Readability classification for German using lexical, syntactic, and morphological features", "author": ["Hancke et al.2012] Julia Hancke", "Sowmya Vajjala", "Detmar Meurers"], "venue": "In Proceedings of COLING", "citeRegEx": "Hancke et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hancke et al\\.", "year": 2012}, {"title": "An analysis of statistical models and features for reading difficulty prediction", "author": ["Kevyn Collins-Thompson", "Maxine Eskenazi"], "venue": "In Proceedings of the Third Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Heilman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2008}, {"title": "N -gram fragment sequence based unsupervised domain-specific document readability", "author": ["Jameel et al.2012] Shoaib Jameel", "Xiaojun Qian", "Wai Lam"], "venue": "In Proceedings of COLING", "citeRegEx": "Jameel et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jameel et al\\.", "year": 2012}, {"title": "Text categorization with support vector machines: Learning with many relevant", "author": ["Thorsten Joachims"], "venue": null, "citeRegEx": "Joachims.,? \\Q1998\\E", "shortCiteRegEx": "Joachims.", "year": 1998}, {"title": "Device-dependent readability for improved text understanding", "author": ["Kim et al.2014] A-Yeong Kim", "Hyun-Je Song", "Seong-Bae Park", "Sang-Jo Lee"], "venue": "In EMNLP,", "citeRegEx": "Kim et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "Japanese dependency analysis using cascaded chunking", "author": ["Kudo", "Matsumoto2002] Taku Kudo", "Yuji Matsumoto"], "venue": "In proceedings of the 6th conference on Natural language learning-Volume", "citeRegEx": "Kudo et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kudo et al\\.", "year": 2002}, {"title": "What can language learners tell us about constructions? APPLICATIONS OF COGNITIVE LINGUISTICS, 9:197", "author": ["Manzanares", "Ana Mar\u0131\u0301a Rojo L\u00f3pez"], "venue": null, "citeRegEx": "Manzanares et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manzanares et al\\.", "year": 2008}, {"title": "Measures of text difficulty: Testing their predictive value for grade levels and student performance", "author": ["Charles Perfetti", "David Liben", "Meredith Liben"], "venue": "Council of Chief State School Officers,", "citeRegEx": "Nelson et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Nelson et al\\.", "year": 2012}, {"title": "Revisiting readability: A unified framework for predicting text quality", "author": ["Pitler", "Nenkova2008] Emily Pitler", "Ani Nenkova"], "venue": "In Proceedings of the conference on empirical methods in natural language processing,", "citeRegEx": "Pitler et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pitler et al\\.", "year": 2008}, {"title": "Longman dictionary of language teaching and applied linguistics", "author": ["Richards", "Schmidt2013] Jack C Richards", "Richard W Schmidt"], "venue": null, "citeRegEx": "Richards et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richards et al\\.", "year": 2013}, {"title": "Japanese Language Proficiency Test N1 GRAMMAR Summary", "author": ["Sasaki", "Kiko2010] Hitoko Sasaki", "Matsumoto Kiko"], "venue": null, "citeRegEx": "Sasaki et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sasaki et al\\.", "year": 2010}, {"title": "Reading level assessment using support vector machines and statistical language models", "author": ["Schwarm", "Ostendorf2005] Sarah E Schwarm", "Mari Ostendorf"], "venue": "In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Schwarm et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Schwarm et al\\.", "year": 2005}, {"title": "The textevaluator tool: Helping teachers and test developers select texts for use in instruction and assessment", "author": ["Irene Kostin", "Diane Napolitano", "Michael Flor"], "venue": null, "citeRegEx": "Sheehan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sheehan et al\\.", "year": 2014}, {"title": "New readability measures for Bangla and Hindi texts", "author": ["Sinha et al.2012] Manjira Sinha", "Sakshi Sharma", "Tirthankar Dasgupta", "Anupam Basu"], "venue": "In Proceedings of COLING 2012: Posters,", "citeRegEx": "Sinha et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sinha et al\\.", "year": 2012}, {"title": "Influence of target reader background and text features on text readability in bangla: A computational approach", "author": ["Sinha et al.2014] Manjira Sinha", "Tirthankar Dasgupta", "Anupam Basu"], "venue": "In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,", "citeRegEx": "Sinha et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sinha et al\\.", "year": 2014}, {"title": "Beyond the classroom", "author": ["Anne Schiano", "Elizabeth Lattanzio"], "venue": "Knowledge Quest,", "citeRegEx": "III et al\\.,? \\Q2014\\E", "shortCiteRegEx": "III et al\\.", "year": 2014}, {"title": "A statistical interpretation of term specificity and its application in retrieval", "author": [], "venue": "Journal of documentation,", "citeRegEx": "Jones.,? \\Q1972\\E", "shortCiteRegEx": "Jones.", "year": 1972}, {"title": "Essential Japanese Expression Dictionary: A Guide to Correct Usage of Key Sentence Patterns (New Edition)", "author": ["Tomomatsu Etsuko", "Waguri Masako"], "venue": null, "citeRegEx": "Etsuko et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Etsuko et al\\.", "year": 2010}, {"title": "Readability of texts:: State of the art", "author": ["Zamanian", "Heydari2012] Mostafa Zamanian", "Pooneh Heydari"], "venue": "Theory and Practice in Language Studies,", "citeRegEx": "Zamanian et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zamanian et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 7, "context": "Evaluating text difficulty, or text readability, is an important topic in natural language processing and applied linguistics (Zamanian and Heydari, 2012; Pitler and Nenkova, 2008; Fulcher, 1997).", "startOffset": 126, "endOffset": 195}, {"referenceID": 22, "context": "However, most existing tools either do not sufficiently take the impact of grammatical difficulty into account (Smith III et al., 2014; Renaissance Learning, 2016; Sheehan et al., 2014), or use traditional syntactic features, which differ from what language students actually learn, to estimate grammatical complexity (Schwarm and Ostendorf, 2005; Heilman et al.", "startOffset": 111, "endOffset": 185}, {"referenceID": 11, "context": ", 2014), or use traditional syntactic features, which differ from what language students actually learn, to estimate grammatical complexity (Schwarm and Ostendorf, 2005; Heilman et al., 2008).", "startOffset": 140, "endOffset": 191}, {"referenceID": 1, "context": "In fact, language courses introduce grammar constructs together with vocabulary, and grammar constructs vary in frequency and difficulty just like vocabulary (Blyth, 1997; Manzanares and L\u00f3pez, 2008; Waara, 2004).", "startOffset": 158, "endOffset": 212}, {"referenceID": 17, "context": "Text difficulty evaluation has been widely studied over the past few decades (Nelson et al., 2012; Sinha et al., 2012; Hancke et al., 2012; Jameel et al., 2012; Gonzalez-Dios et al., 2014; Sinha et al., 2014).", "startOffset": 77, "endOffset": 208}, {"referenceID": 23, "context": "Text difficulty evaluation has been widely studied over the past few decades (Nelson et al., 2012; Sinha et al., 2012; Hancke et al., 2012; Jameel et al., 2012; Gonzalez-Dios et al., 2014; Sinha et al., 2014).", "startOffset": 77, "endOffset": 208}, {"referenceID": 10, "context": "Text difficulty evaluation has been widely studied over the past few decades (Nelson et al., 2012; Sinha et al., 2012; Hancke et al., 2012; Jameel et al., 2012; Gonzalez-Dios et al., 2014; Sinha et al., 2014).", "startOffset": 77, "endOffset": 208}, {"referenceID": 12, "context": "Text difficulty evaluation has been widely studied over the past few decades (Nelson et al., 2012; Sinha et al., 2012; Hancke et al., 2012; Jameel et al., 2012; Gonzalez-Dios et al., 2014; Sinha et al., 2014).", "startOffset": 77, "endOffset": 208}, {"referenceID": 8, "context": "Text difficulty evaluation has been widely studied over the past few decades (Nelson et al., 2012; Sinha et al., 2012; Hancke et al., 2012; Jameel et al., 2012; Gonzalez-Dios et al., 2014; Sinha et al., 2014).", "startOffset": 77, "endOffset": 208}, {"referenceID": 24, "context": "Text difficulty evaluation has been widely studied over the past few decades (Nelson et al., 2012; Sinha et al., 2012; Hancke et al., 2012; Jameel et al., 2012; Gonzalez-Dios et al., 2014; Sinha et al., 2014).", "startOffset": 77, "endOffset": 208}, {"referenceID": 22, "context": "TextEvaluator is a comprehensive text analysis system desinged to help teachers and test developers evaluate the complexity characteristics of reading materials (Sheehan et al., 2014).", "startOffset": 161, "endOffset": 183}, {"referenceID": 11, "context": "Based on this fact, recent readability evaluation systems improved performance by incorporating syntactic features like parse tree depth (Schwarm and Ostendorf, 2005) and subtree patterns (Heilman et al., 2008) to measure grammatical complexity.", "startOffset": 188, "endOffset": 210}, {"referenceID": 1, "context": "However, these syntactic features differ from the grammatical knowledge that students actually learn in language lessons (Blyth, 1997; Manzanares and L\u00f3pez, 2008; Waara, 2004).", "startOffset": 121, "endOffset": 175}, {"referenceID": 14, "context": "The relationship between text readability and reading devices was also studied in the past two years (Kim et al., 2014).", "startOffset": 101, "endOffset": 119}, {"referenceID": 6, "context": "Language education researchers attempted to utilize these native-speaker-oriented techniques, which measure text difficulty based on traditional vocabulary and syntactic features, to predict text difficulty levels for Portuguese language learners (Curto et al., 2015).", "startOffset": 247, "endOffset": 267}, {"referenceID": 6, "context": "However, these vocabulary-based methods underperform for language learners who have limited knowledge of grammar (Callan and Eskenazi, 2007; Curto et al., 2015).", "startOffset": 113, "endOffset": 160}, {"referenceID": 0, "context": "We introduce the idea of grammatical templates, units of grammar that expert language instructors and linguists have identified as the most important grammatical knowledge, and are typically emphasized as key points in every textbook lesson (Banno et al., 2011; People\u2019s Education Press, 2013).", "startOffset": 241, "endOffset": 293}, {"referenceID": 0, "context": "We also collected reading texts from two popular series of Japanese textbooks: Standard Japanese (People\u2019s Education Press, 2013) and Genki (Banno et al., 2011).", "startOffset": 140, "endOffset": 160}, {"referenceID": 14, "context": "We compare our work with recent readability evaluation studies (Kim et al., 2014; Pitler and Nenkova, 2008).", "startOffset": 63, "endOffset": 107}, {"referenceID": 17, "context": "Since the text difficulty level prediction can be regarded as a special text classification problem, we also extract TF-IDF features (Sparck Jones, 1972) (Nelson et al., 2012) as an extra baseline, in order to see how general text classification techniques work on text difficulty evaluation.", "startOffset": 154, "endOffset": 175}, {"referenceID": 13, "context": "3 Result We test k-Nearest Neighbor and Support Vector Machines (Joachims, 1998) for each feature set.", "startOffset": 64, "endOffset": 80}, {"referenceID": 9, "context": "The implementations of these two popular classification algorithms are provided by the WEKA toolkit (Hall et al., 2009) and LibSVM (Chang and Lin, 2011).", "startOffset": 100, "endOffset": 119}, {"referenceID": 4, "context": "The SVMs use RBF kernels (Chang et al., 2010).", "startOffset": 25, "endOffset": 45}], "year": 2017, "abstractText": "Language students are most engaged while reading texts at an appropriate difficulty level. However, existing methods of evaluating text difficulty focus mainly on vocabulary and do not prioritize grammatical features, hence they do not work well for language learners with limited knowledge of grammar. In this paper, we introduce grammatical templates, the expert-identified units of grammar that students learn from class, as an important feature of text difficulty evaluation. Experimental classification results show that grammatical template features significantly improve text difficulty prediction accuracy over baseline readability features by 7.4%. Moreover, we build a simple and human-understandable text difficulty evaluation approach with 87.7% accuracy, using only 5 grammatical template features.", "creator": "LaTeX with hyperref package"}}}