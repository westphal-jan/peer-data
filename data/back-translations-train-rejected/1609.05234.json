{"id": "1609.05234", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning", "abstract": "User-machine interaction is important for spoken content retrieval. For text content retrieval, the user can easily scan through and select on a list of retrieved item. This is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. Besides, due to the high degree of uncertainty for speech recognition, the retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing to the user. The suitable actions depend on the retrieval status, for example requesting for extra information from the user, returning a list of topics for user to select, etc. In our previous work, some hand-crafted states estimated from the present retrieval results are used to determine the proper actions. In this paper, we propose to use Deep-Q-Learning techniques instead to determine the machine actions for interactive spoken content retrieval. Deep-Q-Learning bypasses the need for estimation of the hand-crafted states, and directly determine the best action base on the present retrieval status even without any human knowledge. It is shown to achieve significantly better performance compared with the previous hand-crafted states.", "histories": [["v1", "Fri, 16 Sep 2016 20:56:22 GMT  (205kb,D)", "http://arxiv.org/abs/1609.05234v1", "Accepted conference paper: \"The Annual Conference of the International Speech Communication Association (Interspeech), 2016\""]], "COMMENTS": "Accepted conference paper: \"The Annual Conference of the International Speech Communication Association (Interspeech), 2016\"", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["yen-chen wu", "tzu-hsiang lin", "yang-de chen", "hung-yi lee", "lin-shan lee"], "accepted": false, "id": "1609.05234"}, "pdf": {"name": "1609.05234.pdf", "metadata": {"source": "CRF", "title": "Interactive Spoken Content Retrieval by Deep Reinforcement Learning", "authors": ["Yen-Chen Wu", "Tzu-Hsiang Lin", "Yang-De Chen", "Hung-Yi Lee", "Lin-Shan Lee"], "emails": ["tlkagkb93901106}@gmail.com,", "lslee@gate.sinica.edu.tw"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them will be able to move to a different world in which they are able to escape than to another world in which they are able to escape."}, {"heading": "2. Proposed Approach", "text": "The framework for the proposed approach is illustrated in Fig. 1. On the left, the user first enters a query q into the system. By using the query q or other feedback information provided by the user during interaction, the retrieval module described in Section 2.1 (in the upper center) generates a list of retrieved results. The system takes action based on the retrieved results in order to interact with the user, as discussed in Section 2.2. A number of features extracted from the retrieved results (in the upper right corner), and the dialog manager (in the lower center) determining the action based on the retrieved function, are introduced in Section 2.3 and 2.4 respectively. In the dialog manager, there are paths (A) and (B) to determine the actions. Path (A) (in Section 2.4.1) is the previous approach with estimated states, while Path (B) (in Section 2.4.2) uses the reinforcement proposed in this paper."}, {"heading": "2.1. Retrieval Module", "text": "2.1.1. Language Modeling Retrieval ModulesThe basic idea of a language model based retrieval framework is to represent the query q and a document d both as language models \u03b8q and \u03b8d. Further details on the estimation of \u03b8d for spoken documents are omitted here [18, 19]. The relevance value S (q, d) for the specified query q and a document d, which is used to classify the documents d during the retrieval process, is evaluated on the basis of the KL divergence between Vicq and Vicd or S (q, d) = \u2212 KL. Furthermore, the user can name a number of terms that are irrelevant to the query model sought, which can be modeled as negative information model successN. Thus, the complete relevance value S (q, d) = \u2212 KL-Score S (q, d) considers the query model \u0445q and the negative information model \u0445N as insignificant."}, {"heading": "2.2. Actions to be taken by the system", "text": "To help the user provide useful information that allows the system to retrieve documents that are better matched to the user's target, five actions are defined for user-system interaction as shown below. (a) Return Documents: The dialog manager returns the current list of retrieved results in descending order after Sk (q, d) and asks the user to select a relevant document. (b) Return Keyword: The dialog manager asks the user if a key term t * is relevant. (c) Return List: The dialog manager asks the user to specify an additional search term t *. (d) Return Topic: The dialog manager returns a list of topics created with latent theme models [23, 24, 25, 26] and asks the user to select one. (e) Show List: The dialog manager displays the retrieved results after Sk (q, d) to the user and terminates the next interactive session (c), with the corresponding actions (b), the corresponding system (b)."}, {"heading": "2.3. Feature Extraction", "text": "Feature sets are extracted, on the basis of which the correct actions are selected in Section 2.4. Here, two sets of features are tested: \u2022 Human Knowledge Feature: A set of attributes based on human knowledge is extracted; these attributes have been used in previous work [15]. Examples of these attributes are Clarity Score [27], Query Score [27], the simplified Query Clarity Score (SCS) [27], Ambiguity Score [28], similarity between the query and the collection [29], weighted information gain (TIG) [30] and Query Feedback [30]. \u2022 Raw Relevance Scores: Given the power of deep learning, the dialogue manager can easily make decisions based on the raw relevance of retrieved elements without human knowledge. Here, the relevance values of the Top-N documents in the retrieved results are considered as N-dimensional attributes."}, {"heading": "2.4. Dialogue Manager", "text": "The dialogue manager is based on Markov decision-making processes (MDP) 1. Q (31) is defined as a tuple (S, A, T, R, \u03b3) where there are a number of states, of which there are a number of measures based on an action that is a raw probability of selection in each state in order to get to the state in which there is a reward for performing actions (QB) that gives a reward for performing actions (QB). The evaluation of the Q function (QB) is the reward that is the estimation of the expected discounted sum of all rewards that can be received via an infinite state transition path. (s, a): Q (s, a) = E [s]."}, {"heading": "3. Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Experiment Setting", "text": "There were a total of 5047 message documents with a total length of 198 hours. We used the best transcriptions and grids for the spoken archive. We used a Tri-gram language model trained on 39M words of Yahoo messages. 163 text requests and their relevant spoken documents (not necessarily including query terms) were provided by 22 students. We used DQN with two and four hidden layers of 1024 nodes and relief as an activation function [36]. The DQN framework was modified from an open source code 2. Mean Average Precision (MAP) was selected as the evaluation criterion for retrieval. The cost of actions was empirically determined on the relevant frequency of reproduction of the relevant documents (caused by the user) and 10-fold cross-validation was performed in all areas. We generated users with the following dialog for repetition of the document."}, {"heading": "3.2. Result and Discussion", "text": "In fact, it is so that it will be able to erenie.n the aforementioned lcihsrc\u00fcehncS"}, {"heading": "4. Conclusion", "text": "Due to the high level of language uncertainty and the difficulty of showing users how to interact between user systems on screen, retrieving spoken content is much appreciated. In this article, we use the Deep Q Network (DQN) to learn better state values without having to evaluate the previously used handcrafted states. This end-to-end learning can provide general optimization for user-system interaction and bring about significant improvements in return. We also found that even with simple relevance values without human knowledge, we performed very well. We hope that the results shown here can shed light on the future direction of DQN in interactive spoken speech systems."}, {"heading": "5. References", "text": "[1] D. Robins, Interactive information retrieval: Context and basicnotions, \"Informing Science, vol. 3, pp. 57-62, 2000. [2] I. Ruthven,\" Interactive information retrieval, \"\" Annual review of information science and technology, vol. 42, pp. 43-91, 2008. [3] J. Luo, S. Zhang, and H. Yang, \"Win-win search,\" in Proceedings of the 37th International ACM SIGIR Conference on Research & development in information retrieval., 2014, pp. 587-596. [4] X. Jin, M. Sloan, and J. Wang, \"Interactive exploratory search for multi page search results,\" in Proceedings of the 37th International ACM SIGIR Conference on World Wide Web on World Wide Web Conferering Committee, 2013."}], "references": [{"title": "Interactive information retrieval: Context and basic notions", "author": ["D. Robins"], "venue": "Informing Science, vol. 3, no. 2, pp. 57\u201362, 2000.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2000}, {"title": "Interactive information retrieval", "author": ["I. Ruthven"], "venue": "Annual review of information science and technology, vol. 42, no. 1, pp. 43\u201391, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Win-win search: Dual-agent stochastic game in session search", "author": ["J. Luo", "S. Zhang", "H. Yang"], "venue": "Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval. ACM, 2014, pp. 587\u2013596.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Interactive exploratory search for multi page search results", "author": ["X. Jin", "M. Sloan", "J. Wang"], "venue": "Proceedings of the 22nd international conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2013, pp. 655\u2013666.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Bayes risk-based dialogue management for document retrieval system with speech interface", "author": ["T. Misu", "T. Kawahara"], "venue": "Speech Communication, vol. 52, no. 1, pp. 61\u201371, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Speech-based interactive information guidance system using question-answering technique", "author": ["\u2014\u2014"], "venue": "Acoustics, Speech and Signal Processing, 2007. ICASSP 2007. IEEE International Conference on, vol. 4. IEEE, 2007, pp. IV\u2013145.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2007}, {"title": "Automating crowd-supervised learning for spoken language systems.", "author": ["I. McGraw", "S. Cyphers", "P. Pasupat", "J. Liu", "J.R. Glass"], "venue": "INTERSPEECH,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A conversational movie search system based on conditional random fields.", "author": ["J. Liu", "S. Cyphers", "P. Pasupat", "I. McGraw", "J.R. Glass"], "venue": "INTERSPEECH,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Interactive image storage, indexing and retrieval system", "author": ["D.L. Patton", "P.R. Ashe", "J.A. Manico"], "venue": "Jun. 18 2002, uS Patent 6,408,301.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2002}, {"title": "Emotional image and musical information retrieval with interactive genetic algorithm", "author": ["S.-B. Cho"], "venue": "Proceedings of the IEEE, vol. 92, no. 4, pp. 702\u2013711, 2004.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2004}, {"title": "The trec spoken document retrieval track: A success story", "author": ["J.S. Garofolo", "C.G. Auzanne", "E.M. Voorhees"], "venue": "Content-Based Multimedia Information Access-Volume 1. LE CENTRE DE HAUTES ETUDES INTERNATIONALES D\u2019INFORMATIQUE DOCUMENTAIRE, 2000, pp. 1\u201320.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2000}, {"title": "Information retrieval as card playing: A formal model for optimizing interactive retrieval interface", "author": ["Y. Zhang", "C. Zhai"], "venue": "Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, 2015, pp. 685\u2013694.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Interactive spoken document retrieval with suggested key terms ranked by a markov decision process", "author": ["Y.-C. Pan", "H.-Y. Lee", "L.-S. Lee"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 2, pp. 632\u2013645, 2012.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Interactive spoken content retrieval with different types of actions optimized by a markov decision process.", "author": ["T.-H. Wen", "H.-Y. Lee", "L.-S. Lee"], "venue": "INTERSPEECH,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Interactive spoken content retrieval by extended query model and continuous state space markov decision process", "author": ["T.-H. Wen", "H.-y. Lee", "P.-h. Su", "L.-S. Lee"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 8510\u20138514.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "Nature, vol. 529, no. 7587, pp. 484\u2013 489, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Document language models, query models, and risk minimization for information retrieval", "author": ["J. Lafferty", "C. Zhai"], "venue": "Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2001, pp. 111\u2013119.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Statistical latticebased spoken document retrieval", "author": ["T.K. Chia", "K.C. Sim", "H. Li", "H.T. Ng"], "venue": "ACM Transactions on Information Systems (TOIS), vol. 28, no. 1, p. 2, 2010.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Improved semantic retrieval of spoken content by language models enhanced with acoustic similarity graph", "author": ["H.-Y. Lee", "T.-H. Wen", "L.-S. Lee"], "venue": "Spoken Language Technology Workshop (SLT), 2012 IEEE. IEEE, 2012, pp. 182\u2013187.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Model-based feedback in the language modeling approach to information retrieval", "author": ["C. Zhai", "J. Lafferty"], "venue": "Proceedings of the tenth international conference on Information and knowledge management. ACM, 2001, pp. 403\u2013410.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2001}, {"title": "Regularized estimation of mixture models for robust pseudo-relevance feedback", "author": ["T. Tao", "C. Zhai"], "venue": "Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2006, pp. 162\u2013169.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 1999, pp. 50\u201357.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1999}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, vol. 3, pp. 993\u2013 1022, 2003.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2003}, {"title": "Dynamic topic models", "author": ["D.M. Blei", "J.D. Lafferty"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 113\u2013120.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proceedings of the 20th conference on Uncertainty in artificial intelligence. AUAI Press, 2004, pp. 487\u2013494.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2004}, {"title": "Query performance prediction", "author": ["B. He", "I. Ounis"], "venue": "Information Systems, vol. 31, no. 7, pp. 585\u2013594, 2006.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2006}, {"title": "Predicting query performance", "author": ["S. Cronen-Townsend", "Y. Zhou", "W.B. Croft"], "venue": "Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2002, pp. 299\u2013306.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Effective pre-retrieval query performance prediction using similarity and variability evidence", "author": ["Y. Zhao", "F. Scholer", "Y. Tsegay"], "venue": "Advances in Information Retrieval. Springer, 2008, pp. 52\u201364.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Query performance prediction in web search environments", "author": ["Y. Zhou", "W.B. Croft"], "venue": "Proceedings of the 30th annual international ACM SIGIR conference on Research and development in information retrieval. ACM, 2007, pp. 543\u2013550.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "A markovian decision process", "author": ["R. Bellman"], "venue": "DTIC Document, Tech. Rep., 1957.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1957}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Foundations and trends\u00ae in Machine Learning, vol. 2, no. 1, pp. 1\u2013127, 2009.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Reinforcement learning for robots using neural networks", "author": ["L.-J. Lin"], "venue": "DTIC Document, Tech. Rep., 1993.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1993}, {"title": "Rectifier nonlinearities improve neural network acoustic models", "author": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "venue": "Proc. ICML, vol. 30, 2013, p. 1.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Interactive Information Retrieval (IIR)[1, 2] enhances a retrieval system by incorporating the user-system interaction into the retrieval process.", "startOffset": 39, "endOffset": 45}, {"referenceID": 1, "context": "Interactive Information Retrieval (IIR)[1, 2] enhances a retrieval system by incorporating the user-system interaction into the retrieval process.", "startOffset": 39, "endOffset": 45}, {"referenceID": 2, "context": "The eventual goal of IIR is to guide the users to smoothly find out the desired information through the interactive process[3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 3, "context": "The eventual goal of IIR is to guide the users to smoothly find out the desired information through the interactive process[3, 4].", "startOffset": 123, "endOffset": 129}, {"referenceID": 4, "context": "Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search.", "startOffset": 58, "endOffset": 64}, {"referenceID": 5, "context": "Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search.", "startOffset": 58, "endOffset": 64}, {"referenceID": 6, "context": "Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search.", "startOffset": 86, "endOffset": 92}, {"referenceID": 7, "context": "Previous interactive systems, for example, the city guide [5, 6] or the movie browser [7, 8], usually had the content to be retrieved in text form stored in a semistructured database, and made major efforts on transforming users natural language queries into semantic slots for subsequent database search.", "startOffset": 86, "endOffset": 92}, {"referenceID": 8, "context": "Here we focus on interactive retrieval of multimedia[9, 10] or spoken content[11], which is radically different form text.", "startOffset": 52, "endOffset": 59}, {"referenceID": 9, "context": "Here we focus on interactive retrieval of multimedia[9, 10] or spoken content[11], which is radically different form text.", "startOffset": 52, "endOffset": 59}, {"referenceID": 10, "context": "Here we focus on interactive retrieval of multimedia[9, 10] or spoken content[11], which is radically different form text.", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "Also, it\u2019s difficult to show the retrieved multimedia or spoken information items on the screen, and it\u2019s hard for the users to scan through the retrieved results on the screen[12].", "startOffset": 176, "endOffset": 180}, {"referenceID": 12, "context": "In several previous works [13, 14, 15], Markov Decision Process (MDP) is used to model such spoken content IIR.", "startOffset": 26, "endOffset": 38}, {"referenceID": 13, "context": "In several previous works [13, 14, 15], Markov Decision Process (MDP) is used to model such spoken content IIR.", "startOffset": 26, "endOffset": 38}, {"referenceID": 14, "context": "In several previous works [13, 14, 15], Markov Decision Process (MDP) is used to model such spoken content IIR.", "startOffset": 26, "endOffset": 38}, {"referenceID": 13, "context": "Some successful results have been achieved by first estimating some human-defined indicators from a set of features as the states and then selecting the actions based on the estimated states [14, 15].", "startOffset": 191, "endOffset": 199}, {"referenceID": 14, "context": "Some successful results have been achieved by first estimating some human-defined indicators from a set of features as the states and then selecting the actions based on the estimated states [14, 15].", "startOffset": 191, "endOffset": 199}, {"referenceID": 15, "context": "Deep reinforcement learning has recently achieved great renown and success, and deep-QNetwork (DQN) serves as a capable solution to learn from very raw inputs [16, 17].", "startOffset": 159, "endOffset": 167}, {"referenceID": 16, "context": "Deep reinforcement learning has recently achieved great renown and success, and deep-QNetwork (DQN) serves as a capable solution to learn from very raw inputs [16, 17].", "startOffset": 159, "endOffset": 167}, {"referenceID": 17, "context": "More details about estimating \u03b8d for spoken documents are left out here [18, 19].", "startOffset": 72, "endOffset": 80}, {"referenceID": 18, "context": "More details about estimating \u03b8d for spoken documents are left out here [18, 19].", "startOffset": 72, "endOffset": 80}, {"referenceID": 18, "context": "where \u03b2 as an adjustable parameter [19, 20].", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "where \u03b2 as an adjustable parameter [19, 20].", "startOffset": 35, "endOffset": 43}, {"referenceID": 19, "context": "We adopt the query-regularized mixture model [20, 21, 22] previously proposed for pseudo-relevance feedback to estimate the new query models \u03b8\u2032 q .", "startOffset": 45, "endOffset": 57}, {"referenceID": 20, "context": "We adopt the query-regularized mixture model [20, 21, 22] previously proposed for pseudo-relevance feedback to estimate the new query models \u03b8\u2032 q .", "startOffset": 45, "endOffset": 57}, {"referenceID": 21, "context": "We adopt the query-regularized mixture model [20, 21, 22] previously proposed for pseudo-relevance feedback to estimate the new query models \u03b8\u2032 q .", "startOffset": 45, "endOffset": 57}, {"referenceID": 22, "context": "(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.", "startOffset": 99, "endOffset": 115}, {"referenceID": 23, "context": "(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.", "startOffset": 99, "endOffset": 115}, {"referenceID": 24, "context": "(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.", "startOffset": 99, "endOffset": 115}, {"referenceID": 25, "context": "(d) Return Topic: The dialogue manager returns a list of topics generated with latent topic models [23, 24, 25, 26] and asks the user to select one.", "startOffset": 99, "endOffset": 115}, {"referenceID": 14, "context": "These features were used in the previous works [15].", "startOffset": 47, "endOffset": 51}, {"referenceID": 26, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 137, "endOffset": 141}, {"referenceID": 28, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 191, "endOffset": 195}, {"referenceID": 29, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 229, "endOffset": 233}, {"referenceID": 29, "context": "Examples for these features include clarity score [27], query scope [27], the simplified query clarity score (SCS) [27], ambiguity score [28], similarity between the query and the collection [29], weighted information gain (WIG) [30] and query feedback [30].", "startOffset": 253, "endOffset": 257}, {"referenceID": 30, "context": "MDP [31] is defined as a tuple {S,A, T ,R, \u03b3}, where S is the set of states, A the set of actions, T (s\u2032|s, a) is the transition probability of ending up in state s\u2032 when executing action a in state s,R is the reward function, and \u03b3 the discount factor.", "startOffset": 4, "endOffset": 8}, {"referenceID": 13, "context": "The previous approach [14, 15] for the dialogue management is path (A) in Figure 1.", "startOffset": 22, "endOffset": 30}, {"referenceID": 14, "context": "The previous approach [14, 15] for the dialogue management is path (A) in Figure 1.", "startOffset": 22, "endOffset": 30}, {"referenceID": 31, "context": "The DQN is a deep neural network (DNN) [33, 34] with parameters \u03b8 to estimate the state-action value function Q(s, a; \u03b8).", "startOffset": 39, "endOffset": 47}, {"referenceID": 32, "context": "The DQN is a deep neural network (DNN) [33, 34] with parameters \u03b8 to estimate the state-action value function Q(s, a; \u03b8).", "startOffset": 39, "endOffset": 47}, {"referenceID": 33, "context": "In this way the efficiency in using the training data can be improve through reuse of the experience samples in multiple updates, and the correlation among the samples used in the update can be reduced through the uniform sampling from the replay buffer [35, 17].", "startOffset": 254, "endOffset": 262}, {"referenceID": 16, "context": "In this way the efficiency in using the training data can be improve through reuse of the experience samples in multiple updates, and the correlation among the samples used in the update can be reduced through the uniform sampling from the replay buffer [35, 17].", "startOffset": 254, "endOffset": 262}, {"referenceID": 33, "context": "Freezing the parameters of the target networkQ(s\u2032, a\u2032; \u03b8\u2212) for a fixed number of iterations while updating the online network Q(s, a; \u03b8) is another key innovation for the success of DQN [35, 17], which improved the stability of the algorithm.", "startOffset": 186, "endOffset": 194}, {"referenceID": 16, "context": "Freezing the parameters of the target networkQ(s\u2032, a\u2032; \u03b8\u2212) for a fixed number of iterations while updating the online network Q(s, a; \u03b8) is another key innovation for the success of DQN [35, 17], which improved the stability of the algorithm.", "startOffset": 186, "endOffset": 194}, {"referenceID": 34, "context": "We used DQN with two and four hidden layers of 1024 nodes and relu as the activation function [36].", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "User-machine interaction is important for spoken content retrieval. For text content retrieval, the user can easily scan through and select on a list of retrieved item. This is impossible for spoken content retrieval, because the retrieved items are difficult to show on screen. Besides, due to the high degree of uncertainty for speech recognition, the retrieval results can be very noisy. One way to counter such difficulties is through user-machine interaction. The machine can take different actions to interact with the user to obtain better retrieval results before showing to the user. The suitable actions depend on the retrieval status, for example requesting for extra information from the user, returning a list of topics for user to select, etc. In our previous work, some hand-crafted states estimated from the present retrieval results are used to determine the proper actions. In this paper, we propose to use Deep-Q-Learning techniques instead to determine the machine actions for interactive spoken content retrieval. Deep-Q-Learning bypasses the need for estimation of the hand-crafted states, and directly determine the best action base on the present retrieval status even without any human knowledge. It is shown to achieve significantly better performance compared with the previous hand-crafted states.", "creator": "LaTeX with hyperref package"}}}