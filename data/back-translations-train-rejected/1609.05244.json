{"id": "1609.05244", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "16-Sep-2016", "title": "Select-Additive Learning: Improving Generalization in Multimodal Sentiment Analysis", "abstract": "Multimodal sentiment analysis is drawing an increasing amount of attention these days. It enables mining of opinions in video reviews and surveys which are now available aplenty on online platforms like YouTube. However, the limited number of high-quality multimodal sentiment data samples may introduce the problem of the sentiment being dependent on the individual specific features in the dataset. This results in a lack of generalizability of the trained models for classification on larger online platforms. In this paper, we first examine the data and verify the existence of this dependence problem. Then we propose a Select-Additive Learning (SAL) procedure that improves the generalizability of trained discriminative neural networks. SAL is a two-phase learning method. In Selection phase, it selects the confounding learned representation. In Addition phase, it forces the classifier to discard confounded representations by adding Gaussian noise. In our experiments, we show how SAL improves the generalizability of state-of-the-art models. We increase prediction accuracy significantly in all three modalities (text, audio, video), as well as in their fusion. We show how SAL, even when trained on one dataset, achieves good accuracy across test datasets.", "histories": [["v1", "Fri, 16 Sep 2016 21:33:42 GMT  (583kb,D)", "http://arxiv.org/abs/1609.05244v1", "13 pages"], ["v2", "Wed, 12 Apr 2017 21:38:40 GMT  (836kb,D)", "http://arxiv.org/abs/1609.05244v2", "Supplementary files at:this http URL"]], "COMMENTS": "13 pages", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["haohan wang", "aaksha meghawat", "louis-philippe morency", "eric p xing"], "accepted": false, "id": "1609.05244"}, "pdf": {"name": "1609.05244.pdf", "metadata": {"source": "CRF", "title": "Select-Additive Learning: Improving Cross-individual Generalization in Multimodal Sentiment Analysis", "authors": ["Haohan Wang", "Eric P. Xing"], "emails": ["epxing}@cs.cmu.edu"], "sections": [{"heading": null, "text": "In fact, most people who fight for women's and men's rights are able to decide for themselves what they want and what they want."}, {"heading": "1 Related Work", "text": "For text modality, the sentiment analysis at the word level [4, 29], phrase level [30], sentence level [22] and document level [17] has been comprehensively performed. Recently, deep neural networks have also been used [25, 28]. Much work focuses on the recognition of certain emotional states such as anger or sadness [1,5] for the audio modality. Classification at the sensation level is still primarily performed on the basis of text properties and audio signals are converted to text using language [10, 11]. The coding system for facial expression [7] laid the foundation for the analysis of emotions and feelings in the visual modality. [24, 26] Starting from [16], the merging of these modalities for sentiment analysis has attracted increasing attention. A variety of methods have been proposed and extensively discussed in recent years. [6, 19, 21] The latest state-of-art selective network performance (we can improve by means of a neuronal performance in this section)."}, {"heading": "2 Select-Additive Learning", "text": "The main objective of our work is to increase the generalisability of models by encouraging the model to take more account of sentimental characteristics (i.e. the facial expression of the individual) than identity-related characteristics (i.e. the appearance or voice of the individual). See Figure 1 (c), SAL aims to first identify the red dimensions (as already mentioned as identity-related confusing dimensions) and then force the model to ignore them. We use X to denote a size n \u00b7 p matrix that encodes characteristics, for n expressions with p-characteristics each, use Z to denote a size n \u00b7 m matrix that encodes the identity of the individual, and y to denote a size n \u00b7 1 vector that represents commented feelings."}, {"heading": "2.1 Select-Additive Learning Architecture", "text": "In order to successfully select and remove the identity-related dimensions of confusion, SAL requires a special network architecture to support its algorithm. This architecture can be extended by any discriminatory neural network (e.g. CNN) that can be divided into a representation component for learners and a classification component. To simplify the notation, we use g (\u00b7; \u03b8) to mark the learning component for representation, and \u03b8 stands for its parameters. Similarly, we use f (\u00b7 \u03c6) to mark the classification component and the parameters. To select identity-related dimensions of confusion, SAL introduces a simple neural network (referred to as h (\u00b7; \u03b4), and \u03b4 stands for its parameters) to predict identity-related dimensions of confusion from individual identities Z, so that by minimizing the difference between h (Z; \u043c) and g (Z; GOP), h (Z), constellation, constellation and constellation."}, {"heading": "2.2 Select-Additive Learning Algorithm", "text": "In the addition phase: SAL adds Gaussian sounds to these dimensions, forcing the model to shift the focus to other dimensions associated with sentimentality. Before applying SAL, a model must be sufficiently trained to achieve the following dimensions: argmin \u03c6, 2 (y \u2212 f (G (X; \u03b8); 2This is the same as a standard deep learning phase associated with sentimentality. Selection phase This phase, as illustrated in Figure 3 (a), is used to tune these dimensions by solving: argmin \u03b41 2 (G (Z) \u2212 h (Z;))."}, {"heading": "3 Experiments", "text": "In this section, we conduct extensive experiments on three different sets of data to see if SAL can help improve the generalisability of previous models. We use CNN as a starting point and improve it with SAL. We ensure that there is no common person between the test set and the training / validation sets. We also test the generalisability of the cross-data sets by using two of these data sets exclusively for testing purposes. Our results show that SAL can significantly improve the state of the art."}, {"heading": "3.1 Models", "text": "We compare the following models: - CNN: We replicate the state-of-the-art CNN architecture described in [20] as a baseline. - SAL-CNN: Once the state-of-the-art CNN is fully trained, we use SAL to increase its generalisability and predict mood."}, {"heading": "3.2 Data Set", "text": "We conducted our experiment with three multimodal sentiment analysis datasets: - MOSI: This dataset consists of 93 videos obtained from YouTube channels; the dataset includes 2199 expressions manually generated from individual opinion videos; [32] - YouTube: This dataset consists of 47 opinion videos (280 expressions); [16] - MOUD: This dataset consists of 498 Spanish expressions of opinion over 55 Individ Uals. [23]"}, {"heading": "3.3 Feature Design", "text": "Text features We extracted an embedding for each word in the sentence of the utterance using a word2vec dictionary previously trained on a Google News corpus. [15] Text input for each utterance was formed by concatenating the word embedding for all words in the sentence and filling them with the corresponding zeros to have the same dimension. We set the maximum length to 60 and discarded additional words (only about 0.5% of utterances in our records have more than 60 words). For YouTube records we extracted the transcripts using the IBM Bluemix Sprach2text API1. For MOUD records we translated Spanish transcripts into English transcripts. http: / / www.ibm.com / watson / developopercloud / speech-to-text.htmlAudio features We used the openSMILE [9] to extract the lower level of utterances."}, {"heading": "3.4 Experiment Setup", "text": "We simulated a real-world setting by applying the caveat that training and validation had no persons in common with the test set. We generate our training and test set as follows: - Training / validation set: The first 62 people from the MOSI set are selected as the training / validation set.? MOSI: 775 comments from the remaining 31 people from the MOSI set.? YouTube: 195 comments from 47 people from the YouTube set, after removing neutral expressions.? MOUD: 450 comments from 55 people from the MOUD set, after removing neutral expressions. We first trained the CNN model sufficiently and stored one with the maximum validation accuracy.? MOUD: 450 comments from 55 people from the MOUD set, after removing neutral expressions."}, {"heading": "3.5 Experiment Results", "text": "The results are presented in Table 1. First, it is noteworthy that in some cases the performance of CNN is worse than mere coincidence. This inferior performance underscores the existence of the problems we are targeting because these models are selected as those that achieve a minimum error rate in validation sets and they are unlikely to perform well when tested across datasets. The results also suggest that SAL could help increase the generalizability of the trained model. On all three datasets, SAL corrects the error rate for confusion factors and increases testing accuracy significantly higher than previous CNN performance. All three datasets originate from the same web platform, but differ in recording quality and processing after curation. These differences are evident in the accuracy of YouTube and MOUD datasets, which are much lower than that of MOSI. Text functions are recovered from different ASR tools for all three datasets."}, {"heading": "4 Discussion", "text": "Figure 4 shows a diagram of h (Z, \u03b4) during the selection phase. It is a zoomed-in diagram for the first 50 expressions (lines) and the first 100 values of the representation vector (columns), blue shows the lowest values, and red shows the highest values, and other colors are interpolated linearly between the lines. Presentation of expressions forms clear clusters, and each cluster belongs to an individual. This diagram suggests that the confusing representation may be different between individuals, which favors the argument for adding selective learning as opposed to weight loss. Although each individual has its own pattern, there are dimensions that have generalized well across individuals, e.g. the blue vertical line at about index 100 or the green vertical line at about index 20. Our model will learn to assign more weight to these dimensions after noise has been introduced."}, {"heading": "5 Conclusion", "text": "In this paper, we first presented the existence of a problem for multimodal sentiment analysis. Feeling is not independent of individuals, and a model could be confused by individually sensitive features such as appearance. Therefore, a model trained on a group of individuals is not good for other individuals. After verifying the existence of the problem, we proposed a select-additive learning method to solve it. SAL is a two-phase learning method. In the selection phase, it selects the identity-related foundation dimensions. In the addition phase, it forces the classifier to discard these dimensions by adding Gaussian noise. In our experiments, we showed how SAL improves the generalizability of modern models. We significantly increased the prediction accuracy in all three modalities (text, audio, video) as well as in their merging. We also showed that SAL could achieve good anticipation even when tested across sets of data."}], "references": [{"title": "A speaker independent approach to the classification of emotional vocal expressions", "author": ["H. Atassi", "A. Esposito"], "venue": "2008 20th IEEE International Conference on Tools with Artificial Intelligence. vol. 2, pp. 147\u2013152. IEEE", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2008}, {"title": "3d constrained local model for rigid and nonrigid facial tracking", "author": ["T. Baltru\u0161aitis", "P. Robinson", "L.P. Morency"], "venue": "Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on. pp. 2610\u20132617. IEEE", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Importance weighted autoencoders", "author": ["Y. Burda", "R. Grosse", "R. Salakhutdinov"], "venue": "arXiv preprint arXiv:1509.00519", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Senticnet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis", "author": ["E. Cambria", "D. Olsher", "D. Rajagopal"], "venue": "Proceedings of the twenty-eighth AAAI conference on artificial intelligence. pp. 1515\u20131521. AAAI Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal emotion recognition from expressive faces, body gestures and speech", "author": ["G. Caridakis", "G. Castellano", "L. Kessous", "A. Raouzaiou", "L. Malatesta", "S. Asteriadis", "K. Karpouzis"], "venue": "IFIP International Conference on Artificial Intelligence Applications and Innovations. pp. 375\u2013388. Springer", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "magic mirror in my hand, what is the sentiment in the lens?: An action unit based approach for mining sentiments from multimedia contents", "author": ["L. Casaburi", "F. Colace", "M. De Santo", "L. Greco"], "venue": "Journal of Visual Languages & Computing 27, 19\u201328", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Facial action coding system", "author": ["P. Ekman", "W.V. Friesen"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1977}, {"title": "Confounding factors in the detection of species responses to habitat fragmentation", "author": ["R.M. Ewers", "R.K. Didham"], "venue": "Biological Reviews 81(01), 117\u2013142", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Opensmile: the munich versatile and fast open-source audio feature extractor", "author": ["F. Eyben", "M. W\u00f6llmer", "B. Schuller"], "venue": "Proceedings of the 18th ACM international conference on Multimedia. pp. 1459\u20131462. ACM", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Sentiment analysis of call centre audio conversations using text classification", "author": ["S. Ezzat", "N. El Gayar", "M. Ghanem"], "venue": "Int. J. Comput. Inf. Syst. Ind. Manag. Appl 4(1), 619\u2013627", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Sentiment extraction from natural audio streams", "author": ["L. Kaushik", "A. Sangwan", "J.H. Hansen"], "venue": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing. pp. 8485\u20138489. IEEE", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Auto-encoding variational bayes", "author": ["D.P. Kingma", "M. Welling"], "venue": "arXiv preprint arXiv:1312.6114", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Sentiment analysis: A perspective on its past, present and future", "author": ["A. Kumar", "M.S. Teeja"], "venue": "International Journal of Intelligent Systems and Applications 4(10), 1", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "Generalized linear mixed models", "author": ["C.E. McCulloch", "J.M. Neuhaus"], "venue": "Wiley Online Library", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards multimodal sentiment analysis: Harvesting opinions from the web", "author": ["L.P. Morency", "R. Mihalcea", "P. Doshi"], "venue": "Proceedings of the 13th international conference on multimodal interfaces. pp. 169\u2013176. ACM", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of the 42nd annual meeting on Association for Computational Linguistics. p. 271. Association for Computational Linguistics", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Opinion mining and sentiment analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and trends in information retrieval 2(1-2), 1\u2013135", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Utterance-level multimodal sentiment analysis", "author": ["V. P\u00e9rez-Rosas", "R. Mihalcea", "L.P. Morency"], "venue": "ACL (1). pp. 973\u2013982", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep convolutional neural network textual features and multiple kernel learning for utterance-level multimodal sentiment analysis", "author": ["S. Poria", "E. Cambria", "A. Gelbukh"], "venue": "Proceedings of EMNLP. pp. 2539\u20132544", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Fusing audio, visual and textual clues for sentiment analysis from multimodal content", "author": ["S. Poria", "E. Cambria", "N. Howard", "G.B. Huang", "A. Hussain"], "venue": "Neurocomputing 174, 50\u201359", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning extraction patterns for subjective expressions", "author": ["E. Riloff", "J. Wiebe"], "venue": "Proceedings of the 2003 conference on Empirical methods in natural language processing. pp. 105\u2013112. Association for Computational Linguistics", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "Multimodal sentiment analysis of spanish online videos", "author": ["V.P. Rosas", "R. Mihalcea", "L.P. Morency"], "venue": "IEEE Intelligent Systems (3), 38\u201345", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2013}, {"title": "On the relevance of sequence information for decoding facial expressions of pain and disgust?: An avatar study", "author": ["M. Siebers", "T. Engelbrecht", "U. Schmid"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J.Y. Wu", "J. Chuang", "C.D. Manning", "A.Y. Ng", "C. Potts"], "venue": "Proceedings of the conference on empirical methods in natural language processing (EMNLP). vol. 1631, p. 1642. Citeseer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Expression analysis/synthesis system based on emotion space constructed by multilayered neural network", "author": ["N. Ueki", "S. Morishima", "H. Yamada", "H. Harashima"], "venue": "Systems and Computers in Japan 25(13), 95\u2013107", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1994}, {"title": "On the definition of a confounder", "author": ["T.J. VanderWeele", "I. Shpitser"], "venue": "Annals of statistics 41(1), 196", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2013}, {"title": "Dimensional sentiment analysis using a regional cnn-lstm model", "author": ["J. Wang", "L.C. Yu", "K.R. Lai", "X. Zhang"], "venue": "The 54th Annual Meeting of the Association for Computational Linguistics. p. 225", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning subjective adjectives from corpora", "author": ["J. Wiebe"], "venue": "AAAI/IAAI. pp. 735\u2013740", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "Proceedings of the conference on human language technology and empirical methods in natural language processing. pp. 347\u2013354. Association for Computational Linguistics", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2005}, {"title": "Youtube movie reviews: Sentiment analysis in an audio-visual context", "author": ["M. Wollmer", "F. Weninger", "T. Knaup", "B. Schuller", "C. Sun", "K. Sagae", "L.P. Morency"], "venue": "Intelligent Systems, IEEE 28(3), 46\u201353", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Micro-opinion sentiment intensity analysis and summarization in online videos", "author": ["A. Zadeh"], "venue": "Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. pp. 587\u2013591. ACM", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 15, "context": "Sentiment analysis is the automatic identification of the private state of a human mind with a focus on determining whether this state is positive, negative or neutral [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 17, "context": "It has been extensively studied in the last few decades [18].", "startOffset": 56, "endOffset": 60}, {"referenceID": 12, "context": "Multimodal sentiment analysis extends traditional textual sentiment analysis with speech and visual modalities and enables sentiment identification in videos [13,31].", "startOffset": 158, "endOffset": 165}, {"referenceID": 30, "context": "Multimodal sentiment analysis extends traditional textual sentiment analysis with speech and visual modalities and enables sentiment identification in videos [13,31].", "startOffset": 158, "endOffset": 165}, {"referenceID": 22, "context": "To foster the research in this area, a few datasets have been created with quality annotations for sentiment such as [23], [32] etc.", "startOffset": 117, "endOffset": 121}, {"referenceID": 31, "context": "To foster the research in this area, a few datasets have been created with quality annotations for sentiment such as [23], [32] etc.", "startOffset": 123, "endOffset": 127}, {"referenceID": 31, "context": "This dependence between sentiment and identity is statistically confirmed with \u03c7 independence test in one of our data sets [32].", "startOffset": 123, "endOffset": 127}, {"referenceID": 7, "context": "In statistics, identity in this setting is known as a confounding factor [8, 27].", "startOffset": 73, "endOffset": 80}, {"referenceID": 26, "context": "In statistics, identity in this setting is known as a confounding factor [8, 27].", "startOffset": 73, "endOffset": 80}, {"referenceID": 13, "context": "Inspired by how traditional statistics correct confounding factors [14], we propose a Select-Additive Learning (SAL) procedure that builds on a special architecture as an extension to any discriminative neural network to improve performance.", "startOffset": 67, "endOffset": 71}, {"referenceID": 3, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 81, "endOffset": 88}, {"referenceID": 28, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 81, "endOffset": 88}, {"referenceID": 29, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 103, "endOffset": 107}, {"referenceID": 21, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "For the text modality, sentiment analysis has been carried out at the word level [4, 29], phrase level [30], sentences level [22] and document level [17] extensively.", "startOffset": 149, "endOffset": 153}, {"referenceID": 24, "context": "Recently, deep neural networks have also been used [25, 28].", "startOffset": 51, "endOffset": 59}, {"referenceID": 27, "context": "Recently, deep neural networks have also been used [25, 28].", "startOffset": 51, "endOffset": 59}, {"referenceID": 0, "context": "Many works focus on the detection of certain emotional states such as anger or sadness [1, 5] for the audio modality.", "startOffset": 87, "endOffset": 93}, {"referenceID": 4, "context": "Many works focus on the detection of certain emotional states such as anger or sadness [1, 5] for the audio modality.", "startOffset": 87, "endOffset": 93}, {"referenceID": 9, "context": "Sentiment level classification is still performed primarily based on text features and audio signals are converted to text using speech to text methods [10, 11].", "startOffset": 152, "endOffset": 160}, {"referenceID": 10, "context": "Sentiment level classification is still performed primarily based on text features and audio signals are converted to text using speech to text methods [10, 11].", "startOffset": 152, "endOffset": 160}, {"referenceID": 6, "context": "The facial expression coding system [7] laid the groundwork for analyzing emotions and sentiments in the visual modality.", "startOffset": 36, "endOffset": 39}, {"referenceID": 23, "context": "[24, 26] Starting from [16], fusion of these modalities for sentiment analysis has drawn increasing attention.", "startOffset": 0, "endOffset": 8}, {"referenceID": 25, "context": "[24, 26] Starting from [16], fusion of these modalities for sentiment analysis has drawn increasing attention.", "startOffset": 0, "endOffset": 8}, {"referenceID": 15, "context": "[24, 26] Starting from [16], fusion of these modalities for sentiment analysis has drawn increasing attention.", "startOffset": 23, "endOffset": 27}, {"referenceID": 5, "context": "[6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 18, "context": "[6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 20, "context": "[6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].", "startOffset": 0, "endOffset": 11}, {"referenceID": 19, "context": "[6, 19, 21] The latest state-of-art performance is achieved using a Convolutional Neural Network in [20].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "The noises are added through a Gaussian Sampling Layer, which is fully introduced in [3, 12].", "startOffset": 85, "endOffset": 92}, {"referenceID": 11, "context": "The noises are added through a Gaussian Sampling Layer, which is fully introduced in [3, 12].", "startOffset": 85, "endOffset": 92}, {"referenceID": 19, "context": "In our experiment, we use a state-of-the-art CNN as described in [20].", "startOffset": 65, "endOffset": 69}, {"referenceID": 19, "context": "\u2013 CNN: We replicate the state-of-the-art CNN architecture described in [20] as the baseline.", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "[32] \u2013 YouTube: This dataset consists of 47 opinion videos (280 utterances).", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16] \u2013 MOUD: This dataset consists of 498 Spanish opinion utterances across 55 individuals.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[23]", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "Text Features We extracted an embedding for each word in the text sentence of the utterance using a word2vec dictionary pre-trained on a Google News corpus [15].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "Audio Features We used the openSMILE [9] to extract the low-level audio descriptors of the utterances.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "We used the CLMZ library [2] for extracting facial characteristic points (415 features).", "startOffset": 25, "endOffset": 28}], "year": 2016, "abstractText": "Multimodal sentiment analysis is drawing an increasing amount of attention these days. It enables mining of opinions in video reviews and surveys which are now available aplenty on online platforms like YouTube. However, the limited number of high-quality multimodal sentiment data samples may introduce the problem of the sentiment being dependent on the individual specific features in the dataset. This results in a lack of generalizability of the trained models for classification on larger online platforms. In this paper, we first examine the data and verify the existence of this dependence problem. Then we propose a SelectAdditive Learning (SAL) procedure that improves the generalizability of trained discriminative neural networks. SAL is a two-phase learning method. In Selection phase, it selects the confounding learned representation. In Addition phase, it forces the classifier to discard confounded representations by adding Gaussian noise. In our experiments, we show how SAL improves the generalizability of state-of-the-art models. We increase prediction accuracy significantly in all three modalities (text, audio, video), as well as in their fusion. We show how SAL, even when trained on one dataset, achieves good accuracy across test datasets. Sentiment analysis is the automatic identification of the private state of a human mind with a focus on determining whether this state is positive, negative or neutral [16]. It has been extensively studied in the last few decades [18]. However, previous studies have been primarily based on textual data. With the recent proliferation of online avenues for expressing and sharing opinions, there is plenty of visual, textual and audio data of people expressing their opinions. This availability of data grants us the option of making use of all three modalities together. Multimodal sentiment analysis extends traditional textual sentiment analysis with speech and visual modalities and enables sentiment identification in videos [13,31]. Multimodal sentiment analysis has garnered considerable attention in both industry and academia. To foster the research in this area, a few datasets have been created with quality annotations for sentiment such as [23], [32] etc. A typical data collection procedure is as follows: researchers identify videos of individuals reviewing products from large scale corpora like YouTube. They select highquality videos while maintaining the diversity of data w.r.t. ethnicity, gender, subject ar X iv :1 60 9. 05 24 4v 1 [ cs .C L ] 1 6 Se p 20 16", "creator": "LaTeX with hyperref package"}}}