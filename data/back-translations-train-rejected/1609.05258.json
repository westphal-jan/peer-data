{"id": "1609.05258", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "17-Sep-2016", "title": "The ACRV Picking Benchmark (APB): A Robotic Shelf Picking Benchmark to Foster Reproducible Research", "abstract": "Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress as they make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking. The ACRV Picking Benchmark is designed to be reproducible by using a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparability of complete robotic systems -- including perception and manipulation -- instead of sub-systems only. Our paper describes this new benchmark challenge and presents results acquired by a baseline system based on a Baxter robot.", "histories": [["v1", "Sat, 17 Sep 2016 00:07:54 GMT  (9167kb,D)", "http://arxiv.org/abs/1609.05258v1", "7 pages, submitted to RA:Letters"], ["v2", "Wed, 14 Dec 2016 09:06:49 GMT  (9158kb,D)", "http://arxiv.org/abs/1609.05258v2", "8 pages, submitted to RA:Letters"]], "COMMENTS": "7 pages, submitted to RA:Letters", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.CV cs.SY", "authors": ["j\\\"urgen leitner", "adam w tow", "jake e dean", "niko suenderhauf", "joseph w durham", "matthew cooper", "markus eich", "christopher lehnert", "ruben mangels", "christopher mccool", "peter kujala", "lachlan nicholson", "trung pham", "james sergeant", "liao wu", "fangyi zhang", "ben upcroft", "peter corke"], "accepted": false, "id": "1609.05258"}, "pdf": {"name": "1609.05258.pdf", "metadata": {"source": "CRF", "title": "The ACRV Picking Benchmark: A Robotic Shelf Picking Benchmark to Foster Reproducible Research", "authors": ["J\u00fcrgen Leitner", "Adam W. Tow", "Niko S\u00fcnderhauf", "Jake E. Dean", "Joseph W. Durham", "Matthew Cooper", "Markus Eich", "Christopher Lehnert", "Ruben Mangels", "Christopher McCool", "Peter T. Kujala", "Lachlan Nicholson", "Trung Pham", "James Sergeant", "Fangyi Zhang", "Ben Upcroft", "Peter Corke"], "emails": ["j.leitner@roboticvision.org"], "sections": [{"heading": null, "text": "I. INTRODUCTIONRobot picking of randomly presented objects is an application with considerable economic potential in applications such as e-shopping and logistics. Despite a long history of research into skillful manipulation [1], many of the recent observable advances have been driven by technological developments such as high-quality, low-cost weapons (Rethink's Baxter, Kinova's MICO and JACO arms, etc.) and simple, compatible grippers based on coffee grounds [2] or vacuum grippers [3], which in turn have highlighted the shortcomings in state-of-the-art handling and robust perception in the real world. Over the course of more than a decade, the robotics community has taken on challenges to make progress - from self-driving cars to humanoids to robotic picking. The methodology of using standard problems is powerful, but the challenge events only occasionally occur on test conditions and are difficult to replicate."}, {"heading": "II. TOWARDS REPRODUCIBLE RESEARCH IN ROBOTICS", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "III. THE ACRV PICKING BENCHMARK (APB)", "text": "Inspired by the Amazon Picking Challenge (APC), we are introducing a new physical benchmark and evaluation protocol for robotic picking from shelves that promotes reproducible and comparable results. In contrast to existing data sets, our benchmark enables the evaluation of robotic systems on the overall task, not only on sub-tasks such as object recognition, gripper selection or route planning. The APB is designed to highlight current research issues, but also take more complex item arrangements and reasoning into account as progress is made. An important focus in the design of this benchmark was to maximize reproducibility: A set of carefully selected scenarios with precise instructions on how to place, orient and align objects with the help of pressure stencils. To make the benchmark as accessible as possible for the research community, a white IKEA shelf (fig. 1A) is used for all picking tasks."}, {"heading": "A. The Shelf", "text": "Our benchmark is the standard white IKEA Kallax shelf with eight 33 x 33 x 39 cm containers (W x H x D), which can be ordered from IKEA3 and, unlike the proprietary Kiva shelf used during the APC, is conveniently available worldwide. For the tasks described in this paper, only the top four containers are used and referred to as \"Bin A\" to \"Bin D.\" To improve reproducibility and make the containers look similar to the APC shelves, we cover the back of the shelf with cardboard from the packaging (Fig. 2).2For example, food in the YCB dataset and the raw dog decoration toy could not be shipped to Australia and are not subject to import regulations in some other countries of the world. 3IKEA item number: 102,758.95, price: $75 AUD"}, {"heading": "B. Object Dataset", "text": "This benchmark consists of 42 unique objects (Fig. 1B). The selected objects present different challenges for both perception and manipulation. Each object is divided into simple, medium and hard difficulties for both detection and manipulation.The selection of object categories was based on our experience in the most recent APC and our solution to the problem. Each object was selected to extend the presented challenges. Some objects, such as the tissue box, are both easy to manipulate and relatively easy to detect. Other objects, such as the nail, are both difficult to detect (small, reflective) and difficult to manipulate."}, {"heading": "C. General Benchmark Task Description", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "D. Evaluation Guidelines", "text": "We curate a list of submissions and a ranking on our website where teams can submit results (and new tasks); a video is required to verify the results of the robots and show the robot in operation; in addition, the teams can provide links to system descriptions, publications and code; the following guidelines are proposed for fair comparison and ranking of submissions; a complete run starts with a slight shift of the shelf (the position of each corner may change within a square of 2 cm); by slightly shifting the shelf position, we hope to encourage the development of more general, robust solutions; the templates are then placed in the shelves before the objects are aligned with the right makers; to check the rotation of the objects, consult the image provided with the task description; the case in which the objects are placed can be placed manually somewhere within a working area of 2 m in front of the shelf; the shelf must not touch the floor or be touched."}, {"heading": "IV. A BASELINE SYSTEM FOR SHELF PICKING", "text": "We present a benchmark rack picking system that consists primarily of low-cost, readily available components using a single seven-stage freedom arm of a Baxter static robot. To facilitate comparison with a variety of systems, a single-arm design was chosen."}, {"heading": "A. Approach", "text": "The basic system adds additional sensory features to Baxter, including a custom end effector with two suction cups at right angles, a small kinetic vacuum pump, and an Intel NUC PC mounted on the robot's elbow. First, the robot locates the shelf using the Kinect2. Then, the robot selects the next object from the job provided (in a JSON file) and moves the robot's end effector into the specified scan pose. The robot starts the Kinect Fusion algorithm and performs a diamond-shaped scanning process parallel to the front of the shelf. The fused point cloud is then sent into the perception line, which segments the objects and identifies the target object. The gripping subsystem detects gripping points (and which of the two suction cups to use) in the provided segment cloud."}, {"heading": "B. End-effector Design", "text": "The robot gripper is a bespoke hardware offering a mounting point for two suction cups, two vacuum lines and a small RGB-D camera. The gripper consists of two 3D printed components connected by PVC pipes. Existing mounting points enabled our gripper to be attached to Baxter non-destructively. It was designed with the following features and considerations in mind: \u2022 Small cross-section to get into filled containers \u2022 Two suction cups mounted at right angles to grip objects head-on, downwards and laterally \u2022 Camera plate to maximise proximise proximity to the shelf front while minimising the gap between glimpses and ready poses \u2022 Mechanically simple design that can be quickly manufactured and iteratively optimised, suction cups provide an easy and effective way to grab a wide variety of objects. Our design includes two suction cups to increase the number of grippers available."}, {"heading": "C. Perception Pipeline", "text": "1) Shelf localization: The system uses a head-mounted depth sensor (a Kinect 2) to locate the shelf wart. the robot. A single point cloud of the empty shelf was captured and the four front corners commented by a human. These were then used to extract the grid structure of the shelf front. A unique semi-autonomous pre-alignment is completed with ICP [18] registration between the grid points and a 3D model of the shelf. The autonomous alignment step registers the pre-aligned point cloud of the shelf to the current point cloud provided by the sensor through NDT [19]. Combining this with the shape found in the pre-alignment, a 3D model of the shelf would be oriented towards the \"live shelf position.\" This approach is widely applicable to vary shelf configurations, but has been chosen primarily to combat shelves reflecting shelves."}, {"heading": "D. Grasping Pipeline", "text": "The aim of the Gripping Pipeline is to provide accessible Gripping Points on the object to be selected. Provided a point-cloud segment is created by smoothing the cloud, estimating its boundaries and computing point standards in a grid-like pattern on the surface. This step is quick to perform and includes hyperparameters for the suction cup dimensions to reduce / increase the number of candidate points generated. A Gripping Selection Process follows. First, the inverse kinematics for each candidate are checked both during anticipation (5 cm above) and during gripping to ensure that the robot can reach the object along the candidate point normally. Accessible Grips are then sorted by Heuristics, such as distance to the cloud boundary, curvature at the normal and distance to the walls of the container. This hayristics has been chosen to give our system the best chance to capture an object in its current pose."}, {"heading": "E. Motion Planning", "text": "This is achieved in an open circuit based on an estimate of the real environment. Facilitated by a localized shelf model, RRT * [24] was used in conjunction with the inverse kinematic solver TRAC-IK to calculate collision-free paths to move the end effector to the desired target position. This combination of planner and ICT solver proved empirically to be more reliable and consistent than the other tested planners and ICT solvers. To optimize the working range of the Baxter arm, the Baxter was rotated about 45 degrees in relation to the shelf (see Fig. 1C)."}, {"heading": "F. Software", "text": "We use the Robot Operating System (ROS) framework and additional open source software to accelerate the development of the system and promote modularity, standardization and reproducibility; the source code of the base system is publicly available at: https: / / github. com / amazon-picking-challenge / team _ acrv. Software packages integrated into the system include: \u2022 MoveIt! and OMPL - Motion Planning \u2022 TRAC-IK - Inverse Kinematics Solver \u2022 PCL - Point Cloud Library [25] \u2022 Iai kinect2 - Microsoft Kinect 2 Driver \u2022 Librealsense - Intel RealSense SR300 Driver \u2022 SMACH - State Machine"}, {"heading": "V. BASELINE RESULTS", "text": "The robotic rack picking system described above was originally developed for our participation in the Amazon Picking Challenge 2016, where we ranked 6th out of 16 in the picking task. We evaluate our system based on the benchmark proposed in Section III. Only minor adjustments to shelf registration and additional scripted shelf movement waypoints were added to transfer from APC to APB."}, {"heading": "A. ACRV Picking Benchmark", "text": "This year it is more than ever before."}, {"heading": "B. Amazon Picking Challenge 2016", "text": "During the APC 2016 in Leipzig, 16 teams competed against each other. This year's picking task was significantly more difficult than the 2015 task, with tightly packed bins, sealed bins and products that were harder to see and capture. To complete the picking task, all 46 items began on the shelf, and the task assigned one item from each of the 12 bins to be selected and placed in the bins. Before the challenge, the teams were given a rough breakdown of how many items would belong in the bins, but no precise information on how they would be arranged. The bins and items to be selected are designed to represent a series of experiments. For example, each template has a cuboid on the back of a five bins and bins in which the target object is a book, a soft object and a packed object. Points are awarded for successful picks and may vary depending on how many items were in the bins. Our baseline system achieved a total of 42 points during the competition, with four of the objects successfully selected from the C list."}, {"heading": "VI. CONCLUSIONS", "text": "We propose a benchmark that includes readily available physical artifacts: a standard shelf, standard objects, and reproducible tasks. This benchmarking task will allow for more in-depth analysis, better comparison, and easier reproduction of complete robot picking tasks. A key focus of the benchmark design was to maximize reproducibility: a number of carefully selected scenarios with precise instructions on how to place, align, and align objects using printable templates. In addition, a variety of configurations can be created by combining the various stencils with all kinds of objects. We carefully selected 42 objects that vary in weight and size and include deformable, transparent, and closely related objects (baseball and softball, red and green toothbrushes, full and half-full water bottles)."}], "references": [{"title": "The opengrasp benchmarking suite: An environment for the comparative analysis of grasping and dexterous manipulation", "author": ["S. Ulbrich", "D. Kappler", "T. Asfour", "N. Vahrenkamp", "A. Bierbaum", "M. Przybylski", "R. Dillmann"], "venue": "International Conference on Intelligent Robots and Systems, 2011, pp. 1761\u20131767.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Universal robotic gripper based on the jamming of granular material", "author": ["E. Brown", "N. Rodenberg", "J. Amend", "A. Mozeika", "E. Steltz", "M.R. Zakin", "H. Lipson", "H.M. Jaeger"], "venue": "Proceedings of the National Academy of Sciences, vol. 107, no. 44, pp. 18 809\u201318 814, 2010. 4http://amazonpickingchallenge.org/results.shtml", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Vacuum tool for handling microobjects with a nanorobot", "author": ["W. Zesch", "M. Brunner", "A. Weber"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), vol. 2, 1997, pp. 1761\u20131766.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1997}, {"title": "Lessons from the amazon picking challenge", "author": ["N. Correll", "K.E. Bekris", "D. Berenson", "O. Brock", "A. Causo", "K. Hauser", "K. Okada", "A. Rodriguez", "J.M. Romano", "P.R. Wurman"], "venue": "CoRR, 2016, arXiV: 1601.05484.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Replicable and measurable robotics research [Special Issue", "author": ["F. Bonsignorio", "J. Hallam", "A.P. del Pobil (eds."], "venue": "IEEE Robotics Automation Magazine, vol. 22, no. 3, Sept 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013 252, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Microsoft COCO: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision (ECCV), 2014, pp. 740\u2013755.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "The pascal visual object classes challenge: A retrospective", "author": ["M. Everingham", "S.M.A. Eslami", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "venue": "International Journal of Computer Vision, vol. 111, no. 1, pp. 98\u2013136, Jan. 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "International Conference on Computer Vision (ICCV), 2015, pp. 2425\u20132433.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Latent-class hough forests for 3d object detection and pose estimation", "author": ["A. Tejani", "D. Tang", "R. Kouskouridas", "T.-K. Kim"], "venue": "European Conference on Computer Vision (ECCV), 2014, pp. 462\u2013477.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Rgb-d object recognition: Features, algorithms, and a large scale benchmark", "author": ["K. Lai", "L. Bo", "X. Ren", "D. Fox"], "venue": "Consumer Depth Cameras for Computer Vision, 2013, pp. 167\u2013192.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "A dataset for improved rgbd-based object detection and pose estimation for warehouse pick-and-place", "author": ["C. Rennie", "R. Shome", "K.E. Bekris", "A.F.D. Souza"], "venue": "IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 1179\u20131185, July 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Vision meets robotics: The KITTI dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "International Journal of Robotics Research, 2013.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "The Marulan Data Sets: Multi-Sensor Perception in Natural Environment with Challenging Conditions", "author": ["T. Peynot", "S. Scheding", "S. Terho"], "venue": "The International Journal of Robotics Research, vol. 29, no. 13, pp. 1602\u20131607, November 2010.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Bigbird: A large-scale 3d database of object instances", "author": ["A. Singh", "J. Sha", "K.S. Narayan", "T. Achim", "P. Abbeel"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2014, pp. 509\u2013516.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Lessons from the amazon picking challenge: Four aspects of building robotic systems", "author": ["C. Eppner", "S. H\u00f6fer", "R. Jonschkowski", "R. Mart\u0131n-Mart\u0131n", "A. Sieverling", "V. Wall", "O. Brock"], "venue": "Robotics: Science and Systems, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Method for registration of 3-d shapes", "author": ["P.J. Besl", "N.D. McKay"], "venue": "Robotics-DL tentative. International Society for Optics and Photonics, 1992, pp. 586\u2013606.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1992}, {"title": "The three-dimensional normal-distributions transform: an efficient representation for registration, surface analysis, and loop detection", "author": ["M. Magnusson"], "venue": "Ph.D. dissertation, \u00d6rebro universitet, 2009.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2009}, {"title": "Kinectfusion: Real-time dense surface mapping and tracking", "author": ["R.A. Newcombe", "S. Izadi", "O. Hilliges", "D. Molyneaux", "D. Kim", "A.J. Davison", "P. Kohi", "J. Shotton", "S. Hodges", "A. Fitzgibbon"], "venue": "IEEE Int\u2019l. Symp. on Mixed & Augmented Reality, 2011, p. 127.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2011}, {"title": "Geometrically Consistent Plane Extraction for Dense Indoor 3D Maps Segmentation", "author": ["T.T. Pham", "M. Eich", "I. Reid", "G. Wyeth"], "venue": "International Conference on Intelligent Robots and Systems, 2016.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["P.D. Larry Zitnick"], "venue": "European Conference on Computer Vision (ECCV), 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Sampling-based algorithms for optimal motion planning", "author": ["S. Karaman", "E. Frazzoli"], "venue": "The International Journal of Robotics Research, vol. 30, no. 7, pp. 846\u2013894, 2011.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2011}, {"title": "3d is here: Point cloud library (PCL)", "author": ["R.B. Rusu", "S. Cousins"], "venue": "IEEE International Conference on Robotics and Automation (ICRA), 2011, pp. 1\u20134.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Despite a long history of research into dexterous manipulation [1], much of the observable recent progress has been driven by technological developments such as quality low-cost arms (Rethink\u2019s Baxter,", "startOffset": 63, "endOffset": 66}, {"referenceID": 1, "context": ") and simple compliant grippers based on coffee grounds [2] or vacuum grippers [3], [4], [5].", "startOffset": 56, "endOffset": 59}, {"referenceID": 2, "context": ") and simple compliant grippers based on coffee grounds [2] or vacuum grippers [3], [4], [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 3, "context": ") and simple compliant grippers based on coffee grounds [2] or vacuum grippers [3], [4], [5].", "startOffset": 89, "endOffset": 92}, {"referenceID": 4, "context": "side challenges there is growing interest in reproducible research [6] which is relevant and related.", "startOffset": 67, "endOffset": 70}, {"referenceID": 5, "context": "problems such as object recognition and detection using dataset challenges such as the ImageNet-based ILSVRC [7], COCO [8], and Pascal-VOC [9].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "problems such as object recognition and detection using dataset challenges such as the ImageNet-based ILSVRC [7], COCO [8], and Pascal-VOC [9].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "problems such as object recognition and detection using dataset challenges such as the ImageNet-based ILSVRC [7], COCO [8], and Pascal-VOC [9].", "startOffset": 139, "endOffset": 142}, {"referenceID": 5, "context": "Challenges like ILSVRC [7] and Pascal-VOC [9] in partic-", "startOffset": 23, "endOffset": 26}, {"referenceID": 7, "context": "Challenges like ILSVRC [7] and Pascal-VOC [9] in partic-", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "requires algorithms to create a caption for a scene [8], while VQA (visual question answering) [10] asks open ended questions about more than 250 000 images.", "startOffset": 52, "endOffset": 55}, {"referenceID": 8, "context": "requires algorithms to create a caption for a scene [8], while VQA (visual question answering) [10] asks open ended questions about more than 250 000 images.", "startOffset": 95, "endOffset": 99}, {"referenceID": 9, "context": "For instance [11] introduced a 2D/3D database for multiple-instance detection with considerable clutter and foreground occlusion.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "segmentation [12] and 6D pose estimation [13], can be contributed to the ubiquity of low-cost depth sensors.", "startOffset": 13, "endOffset": 17}, {"referenceID": 11, "context": "segmentation [12] and 6D pose estimation [13], can be contributed to the ubiquity of low-cost depth sensors.", "startOffset": 41, "endOffset": 45}, {"referenceID": 12, "context": "In addition to datasets like KITTI [14] or the Marulan dataset [15], the robotics community have embraced challenges like the DARPA Robotics Challenge", "startOffset": 35, "endOffset": 39}, {"referenceID": 13, "context": "In addition to datasets like KITTI [14] or the Marulan dataset [15], the robotics community have embraced challenges like the DARPA Robotics Challenge", "startOffset": 63, "endOffset": 67}, {"referenceID": 14, "context": "2D images and point clouds were created [16]1, and raised questions about how to trade off generality versus engineered solutions in the research community [17].", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "2D images and point clouds were created [16]1, and raised questions about how to trade off generality versus engineered solutions in the research community [17].", "startOffset": 156, "endOffset": 160}, {"referenceID": 16, "context": "A one-off, semi-autonomous pre-alignment is finalised using ICP [18] registration between the grid", "startOffset": 64, "endOffset": 68}, {"referenceID": 17, "context": "to the current point cloud provided by the sensor by NDT [19].", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "2) Scan, Propose, Classify: Our perception pipeline was split into three distinct stages: a Kinect Fusion (KinFu) [20]", "startOffset": 114, "endOffset": 118}, {"referenceID": 19, "context": "A point-cloud-based segmentation algorithm [21] yields distinct object clouds.", "startOffset": 43, "endOffset": 47}, {"referenceID": 20, "context": "These are projected into the current image frame to generate non-overlapping 2D object proposals, significantly reducing overall computation compared to EdgeBoxes [22].", "startOffset": 163, "endOffset": 167}, {"referenceID": 21, "context": "Following the scan and segmentation, a pre-trained GoogLeNet [23] fine-tuned with 150 images per class (for 38 of the 39 object classes, as we were not able to import one object), was used to classify each object proposal.", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Facilitated by a localised shelf model, RRT\u2217 [24] coupled with the TRAC-IK inverse kinematics solver was used to compute collision-free paths for moving the end-effector to the desired target pose.", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "\u2022 MoveIt! and OMPL - Motion Planning \u2022 TRAC-IK - Inverse Kinematics Solver \u2022 PCL - Point Cloud Library [25] \u2022 Iai kinect2 - Microsoft Kinect 2 Driver \u2022 Librealsense - Intel RealSense SR300 Driver", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "Robotic challenges like the Amazon Picking Challenge (APC) or the DARPA Challenges are an established and important way to drive scientific progress as they make research comparable on a well-defined benchmark with equal test conditions for all participants. However, such challenge events occur only occasionally, are limited to a small number of contestants, and the test conditions are very difficult to replicate after the main event. We present a new physical benchmark challenge for robotic picking. The ACRV Picking Benchmark is designed to be reproducible by using a set of 42 common objects, a widely available shelf, and exact guidelines for object arrangement using stencils. A well-defined evaluation protocol enables the comparability of complete robotic systems \u2013 including perception and manipulation \u2013 instead of sub-systems only. Our paper describes this new benchmark challenge and presents results acquired by a baseline system based on a Baxter robot.", "creator": "LaTeX with hyperref package"}}}