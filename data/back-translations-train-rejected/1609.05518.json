{"id": "1609.05518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2016", "title": "Towards Deep Symbolic Reinforcement Learning", "abstract": "Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.", "histories": [["v1", "Sun, 18 Sep 2016 17:28:22 GMT  (981kb,D)", "http://arxiv.org/abs/1609.05518v1", null], ["v2", "Sat, 1 Oct 2016 16:19:56 GMT  (981kb,D)", "http://arxiv.org/abs/1609.05518v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["marta garnelo", "kai arulkumaran", "murray shanahan"], "accepted": false, "id": "1609.05518"}, "pdf": {"name": "1609.05518.pdf", "metadata": {"source": "CRF", "title": "Towards Deep Symbolic Reinforcement Learning", "authors": ["Marta Garnelo", "Kai Arulkumaran"], "emails": ["garnelo@ic.ac.uk", "kailash.arulkumaran13@ic.ac.uk", "m.shanahan@ic.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "It is not as if it is a complete system in which the fundamental aspects of learning in a number of areas, including the formal characterization of universal artificial intelligence presented by Hutter and Hutter, can be considered an essential element. (Legg and Hutter, 2007), a theoretical framework for general intelligence (AGI) based on enhanced learning. However, current DRL systems suffer from a number of deficiencies. Firstly, they inherit the need for a very large learning set, which they learn very slowly. Secondly, they are fragile in the sense that a trained network that trains well on a task is often very poorly trained."}, {"heading": "2 Experimental setup", "text": "As a benchmark for our prototype system, we have implemented several variants of a simple game where the agent (in the form of a \"+\") must learn to avoid or collect objects, depending on the shape. If the agent reaches an object with one of four possible pull actions (up, down, left or right), that object disappears and the agent receives either a positive or a negative reward. Encountering a circle (\"o\") results in a negative reward, while collecting a cross (\"x\") delivers a positive reward. We have applied the system to four variants of this game (Figure 2), where the type of objects involved and their starting positions are as follows: Variant 1. In this environment, there are only objects that return negative rewards (\"o\") and they are positioned in a grid across the screen. This layout is the same for each new game."}, {"heading": "3 Methods", "text": "One can imagine our algorithm as a pipeline consisting of three levels: low symbol generation, representation formation, and amplification learning."}, {"heading": "3.1 Low-level symbol generation", "text": "The goal of this first stage is to create, in an unattended manner, a set of symbols that can be used to represent the objects in a scene. To this end, we use a Convolutionary Neural Network, as such networks are well suited to mark the extraction of objects, especially from images. Specifically, we train a Convolutionary Autoencoder on 5000 randomly generated images of different numbers of game objects scattered across the screen. Given the simplicity of the images, which consist of objects with three different geometric shapes (cross, sign and circle) on a uniform background, we do not need a deep network to detect the different features of our current game. Our network consists of a 5x5 Convolutionary Layer followed by a 2x2 layer with the appropriate decoding layers. Activations on features in the middle layer of CNN are used directly for the detection of objects in the scene.Object detection and characterization."}, {"heading": "3.2 Representation building", "text": "In fact, the question of why is not a question of perception, but a question of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, of perception, perception, perception, perception, perception, perception, perception, perception, perception, perception."}, {"heading": "3.3 Reinforcement learning", "text": "The spatio-temporal representation constructed in levels one and two of the system pipeline can now be used to learn an effective policy for the game. At this point, the advantage of using the locality heuristically becomes clear. A representation that encompasses all possible relationships between objects would result in a very large state space and very long training periods, with most states rarely visited. However, most interactions between objects are independent of each other, both in the real world and in this particular game. So, instead of representing all relationships between two object types, our representation includes a series of localized representations, which leads to a significantly reduced state space and enables rapid generalization across object types. To implement this independence, we train a separate Q function for each interaction between two object types. The main idea is to learn several Q functions for the different interactions and query those that are relevant to the current situation. Given the simplicity of the game and the reduced state space, we can derive the optimal symbolic representation from the Qnoj based on the current state space."}, {"heading": "4 Results", "text": "Agents were trained in epochs of 100 time steps for a maximum of 1000 epochs. We trained 20 agents separately and tested them for 200 time steps in 10 games in every tenth epoch. The resulting average score is plotted in Figure 5 for all four games. In all four cases, the score increases within the first few hundred epochs and remains roughly constant for the remaining time. While the average score is plotted in this way, a random method of visualizing successful learning for this type of experiment, this measure can cause an incomplete characterization of the learning process in environments with positive and negative objects. There are two reasons for this: First, that the scores returned by the objects can cancel each other out, there is more than one way to achieve a certain endpoint number. For example, collecting ten positive and nine negative objects leads to the same endpoint number as collecting only one positive object. However, in the first case, the positive number of the objects is collected while the objects are collected, the positive ones are the objects."}, {"heading": "4.1 Comparison to DQN", "text": "Finally, we compare our approach to DQN1. The most suitable environments are those with two types of objects, since the initial value is independent of the speed of the agent at the beginning, as they cancel each other out. Figure 6 shows the performance of DQN over time. It is important to note that the revolutionary network of our system has been pre-trained to 5000 images, which corresponds to 50 frames worth of time. We do not include these in the actions, because this pre-training is applicable to all games and only needs to be run once. Thanks to the geometric simplicity of the grid scenario, the DQN agent learns to quickly, diagonally descend to collect only positive objects and avoid negative ones. As a result, we have used an open source implementation of DQN for this comparison: http ps: / / github.com / Kaixhin / Atarinum of objects with positive reward for this variant, a value of 100% of QN1 can be used effectively during our QN1 training period."}, {"heading": "5 Discussion", "text": "This year, it has come to the point that it will only be once before there is such a process, in which there is such a process."}, {"heading": "Acknowledgments", "text": "We thank Nvidia Corporation for donating a high-end GPU, Marta Garnelo is supported by an EPSRC doctoral training award, Author ContributionsMG & MS designed the problem and the technical framework, MG developed and tested the algorithms, MG & MS wrote the work, MG & KA performed the comparison with DQN."}], "references": [{"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": "MIT press Cambridge,", "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "End-to-end training of deep visuomotor policies", "author": ["Sergey Levine", "Chelsea Finn", "Trevor Darrell", "Pieter Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Levine et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Levine et al\\.", "year": 2016}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["David Silver", "Aja Huang", "Chris J. Maddison", "Arthur Guez", "Laurent Sifre", "George Van Den Driessche", "Julian Schrittwieser", "Ioannis Antonoglou", "Veda Panneershelvam", "Marc Lanctot"], "venue": "search. Nature,", "citeRegEx": "Silver et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Silver et al\\.", "year": 2016}, {"title": "Universal intelligence: A definition of machine intelligence", "author": ["Shane Legg", "Marcus Hutter"], "venue": "Minds and Machines,", "citeRegEx": "Legg and Hutter.,? \\Q2007\\E", "shortCiteRegEx": "Legg and Hutter.", "year": 2007}, {"title": "Data-efficient learning of feedback policies from image pixels using deep dynamical models", "author": ["John-Alexander M. Assael", "Niklas Wahlstr\u00f6m", "Thomas B. Sch\u00f6n", "Marc Peter Deisenroth"], "venue": null, "citeRegEx": "Assael et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Assael et al\\.", "year": 2015}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["Shixiang Gu", "Timothy P. Lillicrap", "Ilya Sutskever", "Sergey Levine"], "venue": null, "citeRegEx": "Gu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2016}, {"title": "Actor-mimic: Deep multitask and transfer reinforcement learning", "author": ["Emilio Parisotto", "Lei Jimmy Ba", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Parisotto et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Parisotto et al\\.", "year": 2015}, {"title": "Classifying options for deep reinforcement learning", "author": ["Kai Arulkumaran", "Nat Dilokthanakul", "Murray Shanahan", "Anil Anthony Bharath"], "venue": null, "citeRegEx": "Arulkumaran et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Arulkumaran et al\\.", "year": 2016}, {"title": "Successor features for transfer in reinforcement learning", "author": ["Andr\u00e9 Barreto", "R\u00e9mi Munos", "Tom Schaul", "David Silver"], "venue": null, "citeRegEx": "Barreto et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barreto et al\\.", "year": 2016}, {"title": "Deep learning for real-time atari game play using offline monte-carlo tree search planning", "author": ["Xiaoxiao Guo", "Satinder Singh", "Honglak Lee", "Richard L. Lewis", "Xiaoshi Wang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Guo et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Guo et al\\.", "year": 2014}, {"title": "Strategic attentive writer for learning macro-actions", "author": ["Alexander Vezhnevets", "Volodymyr Mnih", "John Agapiou", "Simon Osindero", "Alex Graves", "Oriol Vinyals", "Koray Kavukcuoglu"], "venue": null, "citeRegEx": "Vezhnevets et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vezhnevets et al\\.", "year": 2016}, {"title": "Graying the black box: Understanding dqns", "author": ["Tom Zahavy", "Nir Ben-Zrihem", "Shie Mannor"], "venue": null, "citeRegEx": "Zahavy et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zahavy et al\\.", "year": 2016}, {"title": "Generality in artificial intelligence", "author": ["John McCarthy"], "venue": "Communications of the ACM,", "citeRegEx": "McCarthy.,? \\Q1987\\E", "shortCiteRegEx": "McCarthy.", "year": 1987}, {"title": "The symbol grounding problem", "author": ["Stevan Harnad"], "venue": "Physica D: Nonlinear Phenomena,", "citeRegEx": "Harnad.,? \\Q1990\\E", "shortCiteRegEx": "Harnad.", "year": 1990}, {"title": "Perception as abduction: Turning sensor data into meaningful representation", "author": ["Murray Shanahan"], "venue": "Cognitive science,", "citeRegEx": "Shanahan.,? \\Q2005\\E", "shortCiteRegEx": "Shanahan.", "year": 2005}, {"title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets", "author": ["Xi Chen", "Yan Duan", "Rein Houthooft", "John Schulman", "Ilya Sutskever", "Pieter Abbeel"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Generative adversarial nets", "author": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Goodfellow et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2014}, {"title": "Tagger: Deep unsupervised perceptual grouping", "author": ["Klaus Greff", "Antti Rasmus", "Mathias Berglund", "Tele Hotloo Hao", "J\u00fcrgen Schmidhuber", "Harri Valpola"], "venue": null, "citeRegEx": "Greff et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Greff et al\\.", "year": 2016}, {"title": "Early visual concept learning with unsupervised deep learning", "author": ["Irina Higgins", "Loic Matthey", "Xavier Glorot", "Arka Pal", "Benigno Uria", "Charles Blundell", "Shakir Mohamed", "Alexander Lerchner"], "venue": null, "citeRegEx": "Higgins et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Higgins et al\\.", "year": 2016}, {"title": "Auto-encoding variational bayes", "author": ["Diederik P. Kingma", "Max Welling"], "venue": null, "citeRegEx": "Kingma and Welling.,? \\Q2013\\E", "shortCiteRegEx": "Kingma and Welling.", "year": 2013}, {"title": "Building machines that learn and think like people", "author": ["Brenden M. Lake", "Tomer D. Ullman", "Joshua B. Tenenbaum", "Samuel J. Gershman"], "venue": null, "citeRegEx": "Lake et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lake et al\\.", "year": 2016}, {"title": "An analysis of first-order logics of probability", "author": ["Joseph Y. Halpern"], "venue": "Artificial intelligence,", "citeRegEx": "Halpern.,? \\Q1990\\E", "shortCiteRegEx": "Halpern.", "year": 1990}, {"title": "Representation learning: A review and new perspectives", "author": ["Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence,", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "The second naive physics manifesto", "author": ["Patrick J. Hayes"], "venue": null, "citeRegEx": "Hayes.,? \\Q1985\\E", "shortCiteRegEx": "Hayes.", "year": 1985}, {"title": "Representations of commonsense knowledge", "author": ["Ernest Davis"], "venue": null, "citeRegEx": "Davis.,? \\Q1990\\E", "shortCiteRegEx": "Davis.", "year": 1990}, {"title": "Commonsense reasoning: An event calculus based approach", "author": ["Erik T. Mueller"], "venue": null, "citeRegEx": "Mueller.,? \\Q2014\\E", "shortCiteRegEx": "Mueller.", "year": 2014}, {"title": "Default reasoning about spatial occupancy", "author": ["Murray Shanahan"], "venue": "Artificial Intelligence,", "citeRegEx": "Shanahan.,? \\Q1995\\E", "shortCiteRegEx": "Shanahan.", "year": 1995}, {"title": "Relief impression image detection: Unsupervised extracting objects directly from feature arrangements of deep cnn", "author": ["Guiying Li", "Junlong Liu", "Chunhui Jiang", "Ke Tang"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "The frame problem", "author": ["Murray Shanahan"], "venue": "The Stanford Encyclopedia of Philosophy,", "citeRegEx": "Shanahan.,? \\Q2016\\E", "shortCiteRegEx": "Shanahan.", "year": 2016}, {"title": "Inductive logic programming", "author": ["Stephen Muggleton"], "venue": "New generation computing,", "citeRegEx": "Muggleton.,? \\Q1991\\E", "shortCiteRegEx": "Muggleton.", "year": 1991}, {"title": "Relational reinforcement learning", "author": ["Sa\u0161o D\u017eeroski", "Luc De Raedt", "Hendrik Blockeel"], "venue": "In Inductive Logic Programming: 8th International Conference,", "citeRegEx": "D\u017eeroski et al\\.,? \\Q1998\\E", "shortCiteRegEx": "D\u017eeroski et al\\.", "year": 1998}, {"title": "Computational models of analogy", "author": ["Dedre Gentner", "Kenneth D. Forbus"], "venue": "Wiley interdisciplinary reviews: cognitive science,", "citeRegEx": "Gentner and Forbus.,? \\Q2011\\E", "shortCiteRegEx": "Gentner and Forbus.", "year": 2011}, {"title": "Planning as satisfiability: heuristics", "author": ["Jussi Rintanen"], "venue": "Artificial intelligence,", "citeRegEx": "Rintanen.,? \\Q2012\\E", "shortCiteRegEx": "Rintanen.", "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Deep reinforcement learning (DRL), wherein a deep neural network (LeCun et al., 2015; Schmidhuber, 2015) is used as a function approximator within a reinforcement learning system (Sutton and Barto, 1998), has recently been shown to be effective in a number of domains, including Atari video games (Mnih et al.", "startOffset": 65, "endOffset": 104}, {"referenceID": 1, "context": ", 2015; Schmidhuber, 2015) is used as a function approximator within a reinforcement learning system (Sutton and Barto, 1998), has recently been shown to be effective in a number of domains, including Atari video games (Mnih et al.", "startOffset": 101, "endOffset": 125}, {"referenceID": 2, "context": ", 2015; Schmidhuber, 2015) is used as a function approximator within a reinforcement learning system (Sutton and Barto, 1998), has recently been shown to be effective in a number of domains, including Atari video games (Mnih et al., 2015), robotics (Levine et al.", "startOffset": 219, "endOffset": 238}, {"referenceID": 3, "context": ", 2015), robotics (Levine et al., 2016), and the game of Go (Silver et al.", "startOffset": 18, "endOffset": 39}, {"referenceID": 4, "context": ", 2016), and the game of Go (Silver et al., 2016).", "startOffset": 28, "endOffset": 49}, {"referenceID": 5, "context": "DRL can be thought of as a step towards instantiating the formal characterisation of universal artificial intelligence presented by Hutter(Legg and Hutter, 2007), a theoretical framework for artificial general intelligence (AGI) founded on reinforcement learning.", "startOffset": 138, "endOffset": 161}, {"referenceID": 6, "context": "Each of these shortcomings is an active area of research in the DRL community, where data efficient learning (Assael et al., 2015; Gu et al., 2016), transfer learning (Parisotto et al.", "startOffset": 109, "endOffset": 147}, {"referenceID": 7, "context": "Each of these shortcomings is an active area of research in the DRL community, where data efficient learning (Assael et al., 2015; Gu et al., 2016), transfer learning (Parisotto et al.", "startOffset": 109, "endOffset": 147}, {"referenceID": 8, "context": ", 2016), transfer learning (Parisotto et al., 2015; Arulkumaran et al., 2016; Barreto et al., 2016; Rusu et al., 2016), planning (Guo et al.", "startOffset": 27, "endOffset": 118}, {"referenceID": 9, "context": ", 2016), transfer learning (Parisotto et al., 2015; Arulkumaran et al., 2016; Barreto et al., 2016; Rusu et al., 2016), planning (Guo et al.", "startOffset": 27, "endOffset": 118}, {"referenceID": 10, "context": ", 2016), transfer learning (Parisotto et al., 2015; Arulkumaran et al., 2016; Barreto et al., 2016; Rusu et al., 2016), planning (Guo et al.", "startOffset": 27, "endOffset": 118}, {"referenceID": 11, "context": ", 2016), planning (Guo et al., 2014; Vezhnevets et al., 2016), and transparency (Zahavy et al.", "startOffset": 18, "endOffset": 61}, {"referenceID": 12, "context": ", 2016), planning (Guo et al., 2014; Vezhnevets et al., 2016), and transparency (Zahavy et al.", "startOffset": 18, "endOffset": 61}, {"referenceID": 13, "context": ", 2016), and transparency (Zahavy et al., 2016) are all hot topics.", "startOffset": 26, "endOffset": 47}, {"referenceID": 14, "context": "Thanks to their compositional structure, such representations are amenable to endless extension and recombination, an essential feature for the acquisition and deployment of high-level abstract concepts, which are key to general intelligence (McCarthy, 1987).", "startOffset": 242, "endOffset": 258}, {"referenceID": 15, "context": "A major obstacle here is the symbol grounding problem (Harnad, 1990; Shanahan, 2005).", "startOffset": 54, "endOffset": 84}, {"referenceID": 16, "context": "A major obstacle here is the symbol grounding problem (Harnad, 1990; Shanahan, 2005).", "startOffset": 54, "endOffset": 84}, {"referenceID": 0, "context": "Deep neural networks in particular have proven to be remarkably effective for supervised learning from large datasets using backpropagation (LeCun et al., 2015; Schmidhuber, 2015).", "startOffset": 140, "endOffset": 179}, {"referenceID": 17, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 18, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 19, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 20, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 21, "context": "Deep learning is therefore already a viable solution to the symbol grounding problem in the supervised case, and for the unsupervised case, which is essential for a full solution, rapid progress is being made (Chen et al., 2016; Goodfellow et al., 2014; Greff et al., 2016; Higgins et al., 2016; Kingma and Welling, 2013).", "startOffset": 209, "endOffset": 321}, {"referenceID": 22, "context": "(For a related set of desiderata see (Lake et al., 2016).", "startOffset": 37, "endOffset": 56}, {"referenceID": 2, "context": "In a conventional DRL system, such as DQN (Mnih et al., 2015), this is achieved through the generalising capabilities of the neural network that approximates the Q function (or the value function or policy function, depending on the style of reinforcement learning in question).", "startOffset": 42, "endOffset": 61}, {"referenceID": 14, "context": "Classically, the theoretical foundation for such a representational medium is first-order logic, and the underlying language comprises predicates, quantifiers, constant symbols, function symbols, and boolean operators (McCarthy, 1987).", "startOffset": 218, "endOffset": 234}, {"referenceID": 23, "context": "To handle uncertainty, we propose probabilistic first-order logic for the semantic underpinnings of the low-dimensional conceptual state space representation into which the neural front end must map the system\u2019s high-dimensional raw input (Halpern, 1990).", "startOffset": 239, "endOffset": 254}, {"referenceID": 24, "context": "For example, in most DRL systems that take visual input, spatial priors, such as the likelihood that similar 2D patterns will appear in different locations in the visual field, are implicit in the convolutional structure of the network (Bengio et al., 2013).", "startOffset": 236, "endOffset": 257}, {"referenceID": 25, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 24, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 26, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 22, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 27, "context": "But the everyday physical world is structured according to many other common sense priors (Hayes, 1985; Bengio et al., 2013; Davis, 1990; Lake et al., 2016; Mueller, 2014).", "startOffset": 90, "endOffset": 171}, {"referenceID": 28, "context": "Consisting mostly of empty space, it contains a variety of objects that tend to persist over time and have various attributes such as shape, colour, and texture (Shanahan, 1995).", "startOffset": 161, "endOffset": 177}, {"referenceID": 29, "context": "As shown by Li et al (Li et al., 2016), salient areas in an image will result in higher activations throughout the layers of a convolutional network.", "startOffset": 21, "endOffset": 38}, {"referenceID": 30, "context": "So future implementations will have to handle this question of circumscribing relevance in a more nuanced way (Shanahan, 2016).", "startOffset": 110, "endOffset": 126}, {"referenceID": 20, "context": "In particular, a more sophisticated deep network capable of unsupervised learning of disentangled representations (having a compositional structure) will be required to handle more realistic images than occur in our benchmark games (eg: (Higgins et al., 2016)).", "startOffset": 237, "endOffset": 259}, {"referenceID": 31, "context": "First, the incorporation of inductive logic programming (Muggleton, 1991) would enable a more powerful form of generalisation to be applied to the Q function (D\u017eeroski et al.", "startOffset": 56, "endOffset": 73}, {"referenceID": 32, "context": "First, the incorporation of inductive logic programming (Muggleton, 1991) would enable a more powerful form of generalisation to be applied to the Q function (D\u017eeroski et al., 1998).", "startOffset": 158, "endOffset": 181}, {"referenceID": 33, "context": "Second, to further amplify the system\u2019s ability to determine similarity between current and past situations, formal techniques for analogical reasoning can be deployed, such as the structure mapping engine or one of its relatives (Gentner and Forbus, 2011).", "startOffset": 230, "endOffset": 256}, {"referenceID": 34, "context": "methods are capable of efficiently finding large plans in complex domains (eg:(Rintanen, 2012)), and it would be rash not to exploit the potential of these techniques.", "startOffset": 78, "endOffset": 94}], "year": 2016, "abstractText": "Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-ofconcept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system \u2013 though just a prototype \u2013 learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.", "creator": "LaTeX with hyperref package"}}}