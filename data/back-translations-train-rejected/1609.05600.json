{"id": "1609.05600", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Graph-Structured Representations for Visual Question Answering", "abstract": "This paper proposes to improve visual question answering (VQA) with structured representations of both scene contents and questions. A key challenge in VQA is to require joint reasoning over the visual and text domains. The predominant CNN/LSTM-based approach to VQA is limited by monolithic vector representations that largely ignore structure in the scene and in the form of the question. CNN feature vectors cannot effectively capture situations as simple as multiple object instances, and LSTMs process questions as series of words, which does not reflect the true complexity of language structure. We instead propose to build graphs over the scene objects and over the question words, and we describe a deep neural network that exploits the structure in these representations. This shows significant benefit over the sequential processing of LSTMs. The overall efficacy of our approach is demonstrated by significant improvements over the state-of-the-art, from 71.2% to 74.4% in accuracy on the \"abstract scenes\" multiple-choice benchmark, and from 34.7% to 39.1% in accuracy over pairs of \"balanced\" scenes, i.e. images with fine-grained differences and opposite yes/no answers to a same question.", "histories": [["v1", "Mon, 19 Sep 2016 05:21:36 GMT  (4616kb,D)", "http://arxiv.org/abs/1609.05600v1", null], ["v2", "Thu, 30 Mar 2017 04:26:26 GMT  (4618kb,D)", "http://arxiv.org/abs/1609.05600v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["damien teney", "lingqiao liu", "anton van den hengel"], "accepted": false, "id": "1609.05600"}, "pdf": {"name": "1609.05600.pdf", "metadata": {"source": "CRF", "title": "Graph-Structured Representations for Visual Question Answering", "authors": ["Damien Teney", "Lingqiao Liu", "Anton van den Hengel"], "emails": ["damien.teney@adelaide.edu.au", "lingqiao.liu@adelaide.edu.au", "anton.vandenhengel@adelaide.edu.au"], "sections": [{"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to survive on their own without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which"}, {"heading": "2. Related work", "text": "Most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own, most of them are able to survive on their own."}, {"heading": "3. Graph representation of scenes and questions", "text": "The input data for each training or test instance is a text question and the description of a scene. The question is processed using the Stanford Dependency Parser [7], which provides the following results. \u2022 A set of NQ words that form the nodes of the question graph. Each word is represented by its index in the input vocabulary, an index among the possible types of dependencies. \u2022 A set of paired relationships between words that form the edges of our chart. An edge between words i and j is represented by eQij, an index among the possible types of dependencies. \u2022 The scene is represented as follows. \u2022 A set of NS objects that form the nodes of the scenario graph. Each node is represented by a vector xSi-RC of visual characteristics (i.. NS), an index among the possible types of dependencies. \u2022 A set of NS objects that represent the edges of the scenario graph."}, {"heading": "4. Processing graphs with neural networks", "text": "We describe a deep neural network that is suitable for processing the question and the scene diagrams in order to derive an answer from them. See Figure 2 for an overview. The two diagrams representing the question and the scene are processed independently of each other in a recurring architecture.The two diagrams representing the question and the scenery are each associated with a recurring unit and are processed via a specified number of iterations (typically T = 4): h0i = 0 (3) hti = GRU (ht \u2212 1i; ni]) t (1, T] ni = pool j (e \u00b2).The square brackets with a semicolon represent a concatenation of vectors and the hadamard (element by element) product.The final state of the GRU is used as a new representation of the nodes."}, {"heading": "5. Evaluation", "text": "In fact, the original dataset contains 20k / 10k / 20k scenes (for training / validation / testing) and 60k / 30k / 60k questions, each with 10 humane ground-truth answers. Questions are categorized based on the type of correct answer in yes / no, number and others, but the same method is used for all categories, the type of test questions that are unknown. The \"balanced\" version of the dataset contains only the subset of questions that have binary answers (yes / no) and furthermore complementary scenes that were created to find the opposite answer to each question. This is significant because the guess of the modal answer from the training is only half the time (slightly more than 50% in practice, because these inconsistencies between the two scenarios are contradictory)."}, {"heading": "5.1. Evaluation on the \u201cbalanced\u201d dataset", "text": "We compare our approach against the three models we have proposed, all of which use an interplay of models that either use an LSTM to process the question or a complex set of handmade rules to identify two objects as the focal point of the question. Visual features in the three models are either blank, global (scenewide) or focused on the two objects identified from the question. These models are specifically designed for binary issues, whereas we are generally applicable to them. Nevertheless, we get significantly better accuracy than all three (Table 1). Differences in performance are largely visible in the setting, which we believe there are ambiguous test questions that human annotators do not respond to. During training, we take care to build complementary scenes together, this has a significant positive effect on the stability of optimization."}, {"heading": "5.2. Evaluation on the \u201cabstract scenes\u201d dataset", "text": "We report our results on the original \"abstract scene\" dataset in Table 2. The evaluation is performed on an automated server that does not allow for comprehensive ablative analysis. Anecdotally, the performance of the validation set confirms all of the results presented above, in particular the strong benefits of upstream parsing, pre-trained text embedding and graph processing with a GRU. At the time of our submission, our method tops the rankings in both open and multiple choice setting. The advantage over the existing method is most pronounced in binary and counting questions. See Figure 5.2 and the supplement to visualize the results."}, {"heading": "6. Conclusions", "text": "We presented a deep neural network for visual response to questions that processes graph-structured representations of scenes and questions, allowing for the use of existing tools for natural language processing, particularly pre-trained text embedding and syntactic analysis, which showed significant advantages over traditional sequential processing of questions, such as LSTMs. We believe that VQA systems are unlikely to learn everything from question / answer examples alone. We believe that future leaps in performance will require additional sources of information and monitoring, and our explicit processing of the language part is a small step in that direction. It has clearly been shown that generalization improves without relying entirely on VQA-specific annotations. So far, we have applied our method to data sets of clip art scenes."}, {"heading": "B. Additional results", "text": "iSe \"n, so iesnlrsreeiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "This paper proposes to improve visual question answer-<lb>ing (VQA) with structured representations of both scene<lb>contents and questions. A key challenge in VQA is to require<lb>joint reasoning over the visual and text domains. The pre-<lb>dominant CNN/LSTM-based approach to VQA is limited by<lb>monolithic vector representations that largely ignore struc-<lb>ture in the scene and in the form of the question. CNN fea-<lb>ture vectors cannot effectively capture situations as simple<lb>as multiple object instances, and LSTMs process questions<lb>as series of words, which does not reflect the true complexity<lb>of language structure. We instead propose to build graphs<lb>over the scene objects and over the question words, and we<lb>describe a deep neural network that exploits the structure<lb>in these representations. This shows significant benefit over<lb>the sequential processing of LSTMs. The overall efficacy of<lb>our approach is demonstrated by significant improvements<lb>over the state-of-the-art, from 71.2% to 74.4% in accuracy<lb>on the \u201cabstract scenes\u201d multiple-choice benchmark, and<lb>from 34.7% to 39.1% in accuracy over pairs of \u201cbalanced\u201d<lb>scenes, i.e. images with fine-grained differences and oppo-<lb>site yes/no answers to a same question.", "creator": "LaTeX with hyperref package"}}}