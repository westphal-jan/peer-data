{"id": "1609.05610", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Enhancing LambdaMART Using Oblivious Trees", "abstract": "Learning to rank is a machine learning technique broadly used in many areas such as document retrieval, collaborative filtering or question answering. We present experimental results which suggest that the performance of the current state-of-the-art learning to rank algorithm LambdaMART, when used for document retrieval for search engines, can be improved if standard regression trees are replaced by oblivious trees. This paper provides a comparison of both variants and our results demonstrate that the use of oblivious trees can improve the performance by more than $2.2\\%$. Additional experimental analysis of the influence of a number of features and of a size of the training set is also provided and confirms the desirability of properties of oblivious decision trees.", "histories": [["v1", "Mon, 19 Sep 2016 07:03:29 GMT  (23kb)", "http://arxiv.org/abs/1609.05610v1", "Accepted for publication in proceedings of RUSSIR 2016"]], "COMMENTS": "Accepted for publication in proceedings of RUSSIR 2016", "reviews": [], "SUBJECTS": "cs.IR cs.LG", "authors": ["michal ferov", "marek modr\\'y"], "accepted": false, "id": "1609.05610"}, "pdf": {"name": "1609.05610.pdf", "metadata": {"source": "CRF", "title": "Enhancing LambdaMART Using Oblivious Trees", "authors": ["Marek Modr\u00fd", "Michal Ferov"], "emails": ["marek.modry@firma.seznam.cz", "michal.ferov@firma.seznam.cz"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.05 610v 1 [cs.I R] 19 Sep 20Keywords: Search Engine Document Search, LambdaMART, Learning to Rank, Oblivious Decision Trees."}, {"heading": "1 Introduction", "text": "In fact, most people who are able to move are able to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move, to move and to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "2 Related Work", "text": "This year is the highest in the history of the country."}, {"heading": "3 Learning to Rank", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Formal description of LTR", "text": "LTR can be formally described as follows: In the training phase of the LTR process, a set of queries is given Q = {q1, q2,.., qn}, where n indicates the number of queries. For each query qi, there is a set of documents di = {d i 1, d i 2,.., d i m (qi)}, where m (qi) is the number of documents specified for the query qi. For each query document pair (qi, d i j), in which i-1,., n} and j pattern {1,..., m (qi)}, a designation y i j is given. Likewise, a feature vector xij-R l, in which l is the number of characters, is given for each query document pair (qi, d i j), in which i-j), where i-points of the model,."}, {"heading": "3.2 Performance evaluation", "text": "Unlike the models created during the classification or regression tasks, the quality of the LTR model depends primarily on the order of a sorted list. As a result, the LTR evaluation functions are generally not smooth or differentiable, which makes direct optimization very difficult, since we cannot apply methods for gradient parentage. In the following paragraphs, the unit of measurement Normalized Discounted Cumulative Gain (NDCG) is introduced. Our experiments, described in the text below, are evaluated using NDCG. As described in [17], NDCG applies to a multi-level scale of relevance. Discounted Cumulative Gain (DCG) can be defined as follows: DCG (f; dj, yj) = m \u00b2 Documents belonging to a query qj and yj = (dB (i)))))) Disc (c), {2 = dj (dj)."}, {"heading": "3.3 LambdaMART", "text": "LambdaMART is an LTR algorithm introduced in [28]. It combines regression tree-enhancing technique used in the MART3 algorithm [13] and LambdaRank's idea of using a lambda trick that avoids the problem of non-smoothness in the order of performance measures [8]. There is no known way to derive the gradients directly from the power measurement. LambdaMART overcomes this problem by calculating the gradients based on the current state of the model and desirable changes in the order of the elements in the list, so-called lambdas. Lambda coefficient represents how the prediction of the model should change for a particular document in the next iteration to improve model performance. Since LambdaMART builds one tree per iteration, Lambdas represent the target function tk + 1: Rl \u2192 for the tree (k + 1)."}, {"heading": "3.4 Oblivious Decision Trees", "text": "It is characterized by the constraint that allows to select only one feature in a certain level of the decision tree, i.e. all decision rules at the specific level can involve3 Multiple Additive Regression Treesonly one selected feature. Although the definition of oblivious decision trees in [25] allows to use different thresholds for the selected feature, our implementation goes even further and all nodes at given depth can only use a single rule (i.e. feature and threshold is uniform at levels). In [19] Kohavi et al. proposed a way to represent oblivious decision trees by decision tables. a decision table can be implemented very efficiently and can be processed much faster than a standard decision tree without limitations. Such an efficiency advantage can be essential for commercial search engines (such as Seznam.cz) that process hundreds of requests per second. Moreover, as the authors of [4,23,26] have noted."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "The experiments were carried out with two sets of data: MSLR-WEB10k dataset 4, which is a public LTR dataset published by Microsoft in 2010, and sec-4 Microsoft Learning to Rank Datasets, 2010. Available at http: / / research.microsoft.com / mslrond dataset, which comes from the Czech search engine Seznam.cz. Statistics of both datasets are in tab. 1.MSLR-WEB10k dataset was selected for its availability, its sufficient size and also its multi-level relevance marking. On the other hand, as the authors of the dataset claim (see [1]), features in MSLR-WEB10k dataset are \"those that are widely used in the research community.\" Although Seznam.cz \"dataset is not publicly available, it enabled us to conduct the experiments with real data."}, {"heading": "4.2 Methodology", "text": "This paper compares two LambdaMART algorithm variants, one with standard regression trees (as described in [28]) and one with Oblivian decision trees. While RankLib [11] was used to train standard LambdaMART models, Seznam.cz \"own implementation of LambdaMART with Oblivian decision trees was used to train the other variant. As mentioned above, the comparison with the cross-validation of 60%, 20% and 20% queries was performed. There were 5 runs of each experiment with the same parameters but different data folds. Validation data sets were used to find the optimal number of trees to achieve the final test performance."}, {"heading": "4.3 Results", "text": "The first set of experiments aimed at comparing the two algorithm variants, LambdaMART (with regression trees) and RCRank (with forgotten trees). Tab. 2 and Tab. 3 presents the results of comparison on both public and internal datasets. LambdaMART in no way surpasses RCRank. Comparing the best results achieved by each of the algorithms, RCRank achieves NDCG @ 10 scores of 0.7135 and 0.5706, while LambdaMART score 0.7110 and 0.5582, on Seznam.cz and MSLR-WEB10k datasets, the improvement of 0.35% and 2.22%. Despite the improvement on Seznam.cz dataset, the improvement is consistent with all parameters."}, {"heading": "5 Conclusions", "text": "In this thesis, we proposed an improvement to the LambdaMART algorithm, which replaced the standard regression trees with obscene decision trees with several desirable properties. We experimentally compared the performance of both variants using the public MSLR WEB10k dataset and the internal dataset of the search engine Seznam.cz. In addition, the influence of various changes in the size of the dataset was investigated. Depending on the dataset, we showed that by using obscene trees instead of standard regression trees, performance can be increased by more than 2.2%. Our experimental results also show that obscene tree variants deal marginally better with smaller training datasets, since the NDCG score of LambdaMART decreases faster than the score of RCRank6 with the reduction in the size of training trees. On the other hand, if potentially noisy and irrelevant characteristics are removed from the LambdaMART dataset, there can be no improvement with the MART 6 dataset features."}, {"heading": "Acknowledgements", "text": "The authors would like to thank their colleagues from the Seznam.cz research group for many insightful discussions, especially the authors of the original RCRank implementation, which was based on Gradient Boosted Regression Trees without lambdas, Toma's, C\u0131 \"cha and Roman Roz."}], "references": [{"title": "Learning with many irrelevant features", "author": ["H. Almuallim", "T.G. Dietterich"], "venue": "In Proceedings of the ninth National conference on Artificial intelligence-Volume 2, pages 547\u2013552. AAAI Press,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1991}, {"title": "Learning boolean concepts in the presence of many irrelevant features", "author": ["H. Almuallim", "T.G. Dietterich"], "venue": "Artificial Intelligence, 69(1-2):279\u2013305,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1994}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine Learning, 45(1):5\u201332,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning to rank using gradient descent", "author": ["C. Burges", "T. Shaked", "E. Renshaw", "M. Deeds", "N. Hamilton", "G. Hullender"], "venue": "In In ICML, pages 89\u201396,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2005}, {"title": "From RankNet to LambdaRank to LambdaMART: An overview", "author": ["C.J.C. Burges"], "venue": "Technical report, Microsoft Research,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning to Rank with Nonsmooth Cost Functions", "author": ["C.J.C. Burges", "R. Ragno", "Q.V. Le"], "venue": "In B. Sch\u00f6lkopf, J. C. Platt, T. Hoffman, B. Sch\u00f6lkopf, J. C. Platt, and T. Hoffman, editors, NIPS, pages 193\u2013200. MIT Press,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning to rank: From pairwise approach to listwise approach", "author": ["Z. Cao", "T. Qin", "T.-Y. Liu", "M.-F. Tsai", "H. Li"], "venue": "In Proceedings of the 24th International Conference on Machine Learning, ICML \u201907, pages 129\u2013136, New York, NY, USA,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Pranking with ranking", "author": ["K. Crammer", "Y. Singer"], "venue": "In Advances in Neural Information Processing Systems 14, pages 641\u2013647. MIT Press,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2001}, {"title": "RankLib", "author": ["V. Dang"], "venue": "Online,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient boosting algorithm for combining preferences", "author": ["Y. Freund", "R. Iyer", "R.E. Schapire", "Y. Singer"], "venue": "J. Mach. Learn. Res., 4:933\u2013969, Dec.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2003}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["J.H. Friedman"], "venue": "Annals of Statistics, 29:1189\u20131232,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "Winning the transfer learning track of yahoo!\u2019s learning to rank challenge with yetirank", "author": ["A. Gulin", "I. Kuralenok", "D. Pavlov"], "venue": "In Yahoo! Learning to Rank Challenge, pages 63\u201376,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "A short introduction to learning to rank", "author": ["L. Hang"], "venue": "IEICE TRANSACTIONS on Information and Systems, 94(10):1854\u20131862,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "In A. Smola, P. Bartlett, B. Sch\u00f6lkopf, and D. Schuurmans, editors, Advances in Large Margin Classifiers, pages 115\u2013132, Cambridge, MA,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2000}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Trans. Inf. Syst., 20(4):422\u2013446, Oct.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Oblivious decision trees, graphs, and top-down pruning", "author": ["R. Kohavi", "C.-H. Li"], "venue": "In IJCAI, pages 1071\u20131079. Citeseer,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1995}, {"title": "Targeting business users with decision table classifiers", "author": ["R. Kohavi", "D. Sommerfield"], "venue": "In KDD, pages 249\u2013253,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Oblivious decision trees and abstract cases", "author": ["P. Langley", "S. Sage"], "venue": "Technical report, DTIC Document,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1994}, {"title": "Improving stability of decision trees", "author": ["M. Last", "O. Maimon", "E. Minkov"], "venue": "International Journal of Pattern Recognition and Artificial Intelligence, 16(02):145\u2013159,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Mcrank: Learning to rank using multiple classification and gradient boosting", "author": ["P. Li", "C.J.C. Burges", "Q. Wu"], "venue": "In J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, editors, NIPS. MIT Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving supervised learning by feature decomposition", "author": ["O. Maimon", "L. Rokach"], "venue": "In Foundations of Information and Knowledge Systems, pages 178\u2013196. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "Query-level loss functions for information retrieval", "author": ["T. Qin", "X.-D. Zhang", "M.-F. Tsai", "D.-S. Wang", "T.-Y. Liu", "H. Li"], "venue": "INFORMATION PROCESSING AND MANAGEMENT, 44(2),", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "Oblivious decision trees", "author": ["L. Rokach", "O. Maimon"], "venue": "In Data mining with decision trees, pages 76\u201377. World Scientific, Singapore,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Efficiently inducing determinations: A complete and systematic search algorithm that uses optimal pruning", "author": ["J.C. Schlimmer"], "venue": "In In Proceedings of the Tenth International Conference on Machine Learning,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1993}, {"title": "The newton-raphson method", "author": ["E. Whittaker", "G. Robinson"], "venue": "The calculus of observations: a treatise on numerical mathematics, 4th ed. Dover, New York, pages 84\u201387,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1967}, {"title": "Adapting boosting for information retrieval measures", "author": ["Q. Wu", "C.J. Burges", "K.M. Svore", "J. Gao"], "venue": "Inf. Retr., 13(3):254\u2013270, June", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2010}, {"title": "Listwise approach to learning to rank: Theory and algorithm", "author": ["F. Xia", "T.-Y. Liu", "J. Wang", "W. Zhang", "H. Li"], "venue": "In Proceedings of the 25th International Conference on Machine Learning, ICML \u201908, pages 1192\u20131199, New York, NY, USA,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2008}, {"title": "Adarank: A boosting algorithm for information retrieval", "author": ["J. Xu", "H. Li"], "venue": "In Proceedings of the 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201907, pages 391\u2013398, New York, NY, USA,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "T", "author": ["J. Xu"], "venue": "yan Liu, M. Lu, H. Li, and W. ying Ma. Directly optimizing evaluation measures in learning to rank. In SIGIR 2008 - Proceedings of the 31th annual international ACM SIGIR conference. ACM Press,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 19, "context": "Learning to rank (LTR) is a machine learning technique broadly used in many areas, such as document retrieval in search engines, collaborative filtering in recommender systems or question answering [22,9,15].", "startOffset": 198, "endOffset": 207}, {"referenceID": 6, "context": "Learning to rank (LTR) is a machine learning technique broadly used in many areas, such as document retrieval in search engines, collaborative filtering in recommender systems or question answering [22,9,15].", "startOffset": 198, "endOffset": 207}, {"referenceID": 12, "context": "Learning to rank (LTR) is a machine learning technique broadly used in many areas, such as document retrieval in search engines, collaborative filtering in recommender systems or question answering [22,9,15].", "startOffset": 198, "endOffset": 207}, {"referenceID": 3, "context": "Generally speaking, LTR is a subject of interest for any system that needs to order intermediate or final results with respect to a given utility function [6].", "startOffset": 155, "endOffset": 158}, {"referenceID": 2, "context": "Mean Squared Error (MSE) is then usually used as the objective function [5].", "startOffset": 72, "endOffset": 75}, {"referenceID": 2, "context": "Random Forest [5] or Multiple Additive Regression Trees (MART) [13] can be utilised to solve the task in the aforementioned manner.", "startOffset": 14, "endOffset": 17}, {"referenceID": 10, "context": "Random Forest [5] or Multiple Additive Regression Trees (MART) [13] can be utilised to solve the task in the aforementioned manner.", "startOffset": 63, "endOffset": 67}, {"referenceID": 7, "context": "Similarly, PRank algorithm proposed in [10] uses a neural network to predict the relevance label.", "startOffset": 39, "endOffset": 43}, {"referenceID": 7, "context": "However, the authors of [10] extend the task to ordinal regression, where the relevance score is converted to the relevance class (resp.", "startOffset": 24, "endOffset": 28}, {"referenceID": 19, "context": "For instance, McRank algorithm [22] uses gradient boosting tree algorithm and reformulates the task as a multiple ordinal classification.", "startOffset": 31, "endOffset": 35}, {"referenceID": 6, "context": "As was pointed out in [9], even though pairwise formalisations benefit from the possibility of using existing classification or regression methods, the results can be suboptimal as the models", "startOffset": 22, "endOffset": 25}, {"referenceID": 13, "context": "In [16], RankingSVM algorithm employs ordinal regression to determine relative relevance of document pairs.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "RankBoost [12] is a boosting algorithm based on AdaBoost\u2019s idea and uses a sequence of weak learners in order to minimise the number of incorrectly ordered pairs.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "[6] proposed RankNet algorithm that learns a neural network to predict the relevance score of a single query-document in such a way that the score can be used to correctly order any pair of query-document samples.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "Authors of PermuRank [31] use SVM technique to minimise a hinge loss function on permutations of documents.", "startOffset": 21, "endOffset": 25}, {"referenceID": 27, "context": "Similarly, AdaRank [30] repeatedly constructs weak rankers in order to minimise an exponential loss which is derived from the original performance measure.", "startOffset": 19, "endOffset": 23}, {"referenceID": 26, "context": "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].", "startOffset": 72, "endOffset": 76}, {"referenceID": 6, "context": "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].", "startOffset": 86, "endOffset": 89}, {"referenceID": 21, "context": "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].", "startOffset": 102, "endOffset": 106}, {"referenceID": 5, "context": "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Examples of other algorithms that employ list-wise approach are ListMLE [29], ListNet [9], RankCosine [24], LambdaRank [8] or LambdaMART [7].", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": "Experimental results of Almuallim and Dietterich [3] demonstrated that standard decision trees, e.", "startOffset": 49, "endOffset": 52}, {"referenceID": 17, "context": "This problem is addressed by Langley and Sage in [20] where they proposed tackling the problem of irrelevant features by using oblivious decision trees.", "startOffset": 49, "endOffset": 53}, {"referenceID": 23, "context": "The constraints on decision rules selection were introduced also by Schlimmer in [26].", "startOffset": 81, "endOffset": 85}, {"referenceID": 22, "context": "Although our modification uses a basic greedy top-down induction of oblivious trees, there have been several methods of oblivious tree construction proposed (see [25,18,21]).", "startOffset": 162, "endOffset": 172}, {"referenceID": 15, "context": "Although our modification uses a basic greedy top-down induction of oblivious trees, there have been several methods of oblivious tree construction proposed (see [25,18,21]).", "startOffset": 162, "endOffset": 172}, {"referenceID": 18, "context": "Although our modification uses a basic greedy top-down induction of oblivious trees, there have been several methods of oblivious tree construction proposed (see [25,18,21]).", "startOffset": 162, "endOffset": 172}, {"referenceID": 11, "context": "Authors of YetiRank algorithm [14] introduced oblivious trees into LTR task.", "startOffset": 30, "endOffset": 34}, {"referenceID": 14, "context": "As described in [17], NDCG is applicable to multi-graded relevance scale.", "startOffset": 16, "endOffset": 20}, {"referenceID": 25, "context": "Following [28] and [22], the gain function G is defined as G(r) = 2 \u2212 1 and the discount function disc is defined as", "startOffset": 10, "endOffset": 14}, {"referenceID": 19, "context": "Following [28] and [22], the gain function G is defined as G(r) = 2 \u2212 1 and the discount function disc is defined as", "startOffset": 19, "endOffset": 23}, {"referenceID": 3, "context": "The use of a cut-off constant can be motivated for instance by the paging of search engine results and limited number of initially displayed documents as noted in [6,28].", "startOffset": 163, "endOffset": 169}, {"referenceID": 25, "context": "The use of a cut-off constant can be motivated for instance by the paging of search engine results and limited number of initially displayed documents as noted in [6,28].", "startOffset": 163, "endOffset": 169}, {"referenceID": 25, "context": "LambdaMART is a LTR algorithm introduced in [28].", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "It combines regression trees boosting technique that is utilised in MART algorithm [13] and LambdaRank\u2019s idea of using lambda trick that avoids the non-smoothness problem of ranking performance measures [8].", "startOffset": 83, "endOffset": 87}, {"referenceID": 5, "context": "It combines regression trees boosting technique that is utilised in MART algorithm [13] and LambdaRank\u2019s idea of using lambda trick that avoids the non-smoothness problem of ranking performance measures [8].", "startOffset": 203, "endOffset": 206}, {"referenceID": 24, "context": "lambdas) can be further modified by using Newton\u2019s method (see [27]).", "startOffset": 63, "endOffset": 67}, {"referenceID": 4, "context": "For detailed explanation of the Newton\u2019s method step in LambdaMART see [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 22, "context": "Although the definition of oblivious decision trees in [25] permits to use different threshold values for the selected feature, our implementation goes even further and all nodes in the given depth use only a single rule (i.", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "In [19] Kohavi et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Furthermore, as the authors of [4,23,26] have noted, the constraint of oblivious decision trees has a positive effect on the effectiveness of feature selection which is a desired property, especially when dealing with datasets containing many uninformative irrelevant features.", "startOffset": 31, "endOffset": 40}, {"referenceID": 20, "context": "Furthermore, as the authors of [4,23,26] have noted, the constraint of oblivious decision trees has a positive effect on the effectiveness of feature selection which is a desired property, especially when dealing with datasets containing many uninformative irrelevant features.", "startOffset": 31, "endOffset": 40}, {"referenceID": 23, "context": "Furthermore, as the authors of [4,23,26] have noted, the constraint of oblivious decision trees has a positive effect on the effectiveness of feature selection which is a desired property, especially when dealing with datasets containing many uninformative irrelevant features.", "startOffset": 31, "endOffset": 40}, {"referenceID": 25, "context": "In this paper, two LambdaMART algorithm variants are compared, one using standard regression trees (as described in [28]) and one using oblivious decision trees instead.", "startOffset": 116, "endOffset": 120}, {"referenceID": 8, "context": "While RankLib [11] library was used to train standard LambdaMART models, Seznam.", "startOffset": 14, "endOffset": 18}], "year": 2016, "abstractText": "Learning to rank is a machine learning technique broadly used in many areas such as document retrieval, collaborative filtering or question answering. We present experimental results which suggest that the performance of the current state-of-the-art learning to rank algorithm LambdaMART, when used for document retrieval for search engines, can be improved if standard regression trees are replaced by oblivious trees. This paper provides a comparison of both variants and our results demonstrate that the use of oblivious trees can improve the performance by more than 2.2%. Additional experimental analysis of the influence of a number of features and of a size of the training set is also provided and confirms the desirability of properties of oblivious decision trees.", "creator": "LaTeX with hyperref package"}}}