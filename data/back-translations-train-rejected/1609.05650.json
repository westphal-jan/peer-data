{"id": "1609.05650", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Multi-view Dimensionality Reduction for Dialect Identification of Arabic Broadcast Speech", "abstract": "In this work, we present a new Vector Space Model (VSM) of speech utterances for the task of spoken dialect identification. Generally, DID systems are built using two sets of features that are extracted from speech utterances; acoustic and phonetic. The acoustic and phonetic features are used to form vector representations of speech utterances in an attempt to encode information about the spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used for the task of DID. The aim of this paper is to construct a single VSM that encodes information about spoken dialects from both the Phonotactic and Acoustic VSMs. Given the two views of the data, we make use of a well known multi-view dimensionality reduction technique known as Canonical Correlation Analysis (CCA), to form a single vector representation for each speech utterance that encodes dialect specific discriminative information from both the phonetic and acoustic representations. We refer to this approach as feature space combination approach and show that our CCA based feature vector representation performs better on the Arabic DID task than the phonetic and acoustic feature representations used alone. We also present the feature space combination approach as a viable alternative to the model based combination approach, where two DID systems are built using the two VSMs (Phonotactic and Acoustic) and the final prediction score is the output score combination from the two systems.", "histories": [["v1", "Mon, 19 Sep 2016 09:44:54 GMT  (1471kb,D)", "http://arxiv.org/abs/1609.05650v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sameer khurana", "ahmed ali", "steve renals"], "accepted": false, "id": "1609.05650"}, "pdf": {"name": "1609.05650.pdf", "metadata": {"source": "CRF", "title": "MULTI-VIEW DIMENSIONALITY REDUCTION FOR DIALECT IDENTIFICATION OF ARABIC BROADCAST SPEECH", "authors": ["Sameer Khurana", "Ahmed Ali", "Steve Renals"], "emails": [], "sections": [{"heading": null, "text": "Index Terms - Canonical Correlation Analysis (CCA), Multi-view Dimensionality Reduction, Vector Space Model (VSM), Arabic Dialect Identification (DID)"}, {"heading": "1. INTRODUCTION", "text": "This year it is more than ever before in the history of the city."}, {"heading": "2. VECTOR SPACE MODELS", "text": "This section describes the construction of the combined VSM, also called CCA VSM, ZC. We begin with the presentation of the phonotactic VSM, XP and acoustic VSM, XA used in this work, followed by the section on CCA VSM, ZC."}, {"heading": "2.1. Phonotactic VSM; XP", "text": "Phonotactic VSM is constructed by modelling the n-gram telephone statistics of the telephone sequences, which are extracted with the help of an Arabic telephone identifier. Details on telephone recognition can be found in [2]. VSM is constructed in two steps; 1) Construct a term-document matrix, X-RN-d (see Figure 1), in which each speech expression is represented by a phonotactic feature vector, p = (f (p, s1), f (p, s2),..., f (p, sd))) (Eq.2)) is the number of times a phone n-gram (term) s appears in the utterance (document) p and 2) on X (Eq.2) to learn a low-dimensional linear multiplicity value."}, {"heading": "2.2. Acoustic VSM; XA", "text": "In fact, most of us are able to play by the rules if they don't play by the rules, \"he told\" Welt am Sonntag. \"\" I don't think we can play by the rules, \"he said.\" But I don't think we have to play by the rules. \"He added,\" I don't think we have to play by the rules. \"He added,\" I don't think we have to play by the rules. \"He added,\" I don't think we have to play by the rules. \""}, {"heading": "2.3. CCA VSM, ZC", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.3.1. Brief Overview; CCA", "text": "Here we give a brief overview of the mathematical foundations of CCA. Figure 2 gives a probabilistic graphical model of CCA. Nodes of the diagram represent random variables (RVs) and the structure encodes conditional assumptions of independence. XP and XA are the random variables that correspond to the phonotactic and acoustic views of the data. Each data view is associated with two latent variables; 1) ZC, which is divided, and is the variable of interest that will form the final combined VSM, ZC, and 2) \u03c6p and \u03c6a, the subspaces associated with the phontactical and acoustic views, respectively."}, {"heading": "2.3.2. Modeling", "text": "Considering the two views, XP, RN x 1200 and XA, RN x 400, we form a common VSM, ZC, RN x 2c for our voice data using the CCA formulation discussed in Section 2.3.1. Note that XP, XA and ZC are instances of the RVs XP, XA and ZC, as shown in Figure 2. The construction of the ZC can be precisely indicated by the following two equations: XP, XA and ZC."}, {"heading": "3. DATA USED", "text": "The training and test data used in this paper are the same as in [2]. Table 1 indicates the number of hours of data available for each dialect for training and testing. Table 2 shows the number of language expressions available for training and testing the DID system. Training data consists of recordings from the Arabic broadcast area and includes statements spoken in all five dialects: EGY, GLF, LAV, MSA and NOR.The test set comes from the same broadcast area but is collected by Al-Jazeera and therefore, unlike training data, the recordings are of high quality. The test set is labelled with CrowdFlower, a crowdsource platform, by QCRI and is available on their webportal1. Further details on the train and test data can be found in [2, 18]."}, {"heading": "4. SYSTEM DESCRIPTION", "text": "Figure 3 provides an overview of our DID system, which can be considered a combination of two broad components; 1) vector-space modeling component and 2) back-end classification. The most important components of the DID system are the four latent vector subspaces; 1): which are learned by performing SVD on the n-gram phootactic term document matrix (Section 2.1), 2) T: is the total variability sub-space learned unattended in the i-vector framework (Section 2.2), 3) throup: is the latent vector sub-space learned in the CCA vector space modeling framework, and 4) \u03c6a: is the latent vector sub-space that corresponds to the acoustic VSM learned in the CCA vector space modeling framework (Section 2.3) a common vector sub-space is then backfilled."}, {"heading": "5. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Dialect Identification Results", "text": "The standalone phonotactic VSM, XP, performs rather poorly compared to the standalone acoustic VSM, XA. CCA VSM, ZC, which is formed by combining XP and XA using CCA, performs better than the two standalone VSMs, which is an expected result, as ZC contains discriminatory dialect information from XP and XA. Further performance improvements are expected due to the performance of LDA and WCCN on ZC. An interesting observation is that the LDA-based technology for reducing dimensionality, which forms a low-dimensional vector subspace, so that the classification separation between data points is maximized, and hence the performance increase we see from the LDA, is to be expected."}, {"heading": "5.2. Confusion", "text": "Table 5 indicates the confusion matrix for the Arabic DID task. This leads to the following conclusion: 1) EGY is most often confused with LAV, 2) GLF is most often confused with LAV and EGY, 3) LAV is most often confused with EGY and GLF, 4) MSA is quite well distinguished, which is also confirmed by the VSM projection in Figure 5.1, 5) NOR is most often confused with EGY and LAV, which can also be seen in the VSM projection, where LAV is given by cyan region, while EGY and LAV are given by blue and green dots."}, {"heading": "6. CONCLUSIONS", "text": "In this paper, we demonstrated our innovative approach to constructing a single VSM for DID that contains discriminatory information from the dialect of both acoustic and phonotactic VSMs. To this end, we use a known multi-dimensional reduction known as Canonical Correlation Analysis (CCA). The single VSM that was constructed performed better than any of the phonotkatic or acoustic VSMs alone, but the LDA-based CCA and acoustic VSMs performed equally well on the DID Taks, while their combination gave us our best VSM for Arabic DID. We come to the conclusion that some dialect-specific discriminatory information is lost while performing CCA between acoustic and phonotactic VSMs, and therefore the final combination performs better. CCA is an uncontrolled method and can easily import unlabeled data from another area and serve as domain adaptation or semi-supervised learning method, as shown in [21]."}, {"heading": "7. REFERENCES", "text": "[1] Fadi Biadsy, Automatic dialect and accent recognition and its application to speech recognition, Ph.D. control system, Columbia University, 2011. [2] Ahmed Ali, Najim Dehak, Patrick Cardinal, Sameer Khurana, Sree Harsha Yella, James Glass, Peter Bell, and Steve Renals, \"Automatic dialect detection in Arabic broadcast speech,\" IEEE Transactions on Audio, Speech, and Language Processing, vol. [3] Haizhou Li, Bin Ma, and Chin-Hui Lee, \"A vector space modeling approach to spoken language identification,\" IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. [4] Scott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard Harshman. \""}], "references": [{"title": "Automatic dialect and accent recognition and its application to speech recognition", "author": ["Fadi Biadsy"], "venue": "Ph.D. thesis, Columbia University,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Automatic dialect detection in arabic broadcast speech", "author": ["Ahmed Ali", "Najim Dehak", "Patrick Cardinal", "Sameer Khurana", "Sree Harsha Yella", "James Glass", "Peter Bell", "Steve Renals"], "venue": "Interspeech 2016, 2016, pp. 2934\u20132938.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "A vector space modeling approach to spoken language identification", "author": ["Haizhou Li", "Bin Ma", "Chin-Hui Lee"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 1, pp. 271\u2013284, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman"], "venue": "Journal of the American society for information science, vol. 41, no. 6, pp. 391, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "Principal component analysis in linear systems: Controllability, observability, and model reduction", "author": ["Bruce Moore"], "venue": "IEEE transactions on automatic control, vol. 26, no. 1, pp. 17\u201332, 1981.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1981}, {"title": "Automatic language identification", "author": ["Marc A Zissman", "Kay M Berkling"], "venue": "Speech Communication, vol. 35, no. 1, pp. 115\u2013124, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Spoken arabic dialect identification using phonotactic modeling", "author": ["Fadi Biadsy", "Julia Hirschberg", "Nizar Habash"], "venue": "Proceedings of the eacl 2009 workshop on computational approaches to semitic languages. Association for Computational Linguistics, 2009, pp. 53\u201361.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Dialect and accent recognition using phoneticsegmentation supervectors", "author": ["Fadi Biadsy", "Julia Hirschberg", "Daniel PW Ellis"], "venue": "INTERSPEECH, 2011, pp. 745\u2013748.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Approaches to language identification using gaussian mixture models and shifted delta cepstral features x, i", "author": ["Pedro A Torres-Carrasquillo", "Elliot Singer", "Mary A Kohler", "Richard J Greene", "Douglas A Reynolds", "JR Deller Jr"], "venue": ".", "citeRegEx": "9", "shortCiteRegEx": null, "year": 0}, {"title": "ivector-based prosodic system for language identification", "author": ["David Mart\u0131\u0301nez", "Luk\u00e1\u0161 Burget", "Luciana Ferrer", "Nicolas Scheffer"], "venue": "2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2012, pp. 4861\u20134864.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Language recognition via i-vectors and dimensionality reduction", "author": ["Najim Dehak", "Pedro A Torres-Carrasquillo", "Douglas A Reynolds", "Reda Dehak"], "venue": "INTER- SPEECH. Citeseer, 2011, pp. 857\u2013860.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Canonical correlation analysis (cca)", "author": ["H. Hotelling"], "venue": "Journal of Educational Psychology, 1935.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1935}, {"title": "Speaker adaptation using the i-vector technique for bottleneck features", "author": ["Patrick Cardinal", "Najim Dehak", "Yu Zhang", "James Glass"], "venue": "Proceedings of Interspeech, vol. 2015, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "A complete kaldi recipe for building arabic speech recognition systems", "author": ["Ahmed Ali", "Yifan Zhang", "Patrick Cardinal", "Najim Dahak", "Stephan Vogel", "James Glass"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 525\u2013529.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "A study of interspeaker variability in speaker verification", "author": ["Patrick Kenny", "Pierre Ouellet", "Najim Dehak", "Vishwa Gupta", "Pierre Dumouchel"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 16, no. 5, pp. 980\u2013988, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Eigenwords: Spectral word embeddings", "author": ["Paramveer S Dhillon", "Dean P Foster", "Lyle H Ungar"], "venue": "The Journal of Machine Learning Research, vol. 16, no. 1, pp. 3035\u2013 3078, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Canonical correlation analysis; an overview with application to learning methods", "author": ["David R Hardoon", "Sandor Szedmak", "John Shawe- Taylor"], "venue": "2003.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2003}, {"title": "Crowdsource a little to label a lot: Labeling a speech corpus of dialectal arabic", "author": ["Samantha Wray", "Ahmed Ali"], "venue": "INTERSPEECH, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Machine learning: a probabilistic perspective", "author": ["Kevin P Murphy"], "venue": "MIT press,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Transforming classifier scores into accurate multiclass probability estimates", "author": ["Bianca Zadrozny", "Charles Elkan"], "venue": "Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2002, pp. 694\u2013699.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-view dimensionality reduction via canonical correlation analysis", "author": ["Dean P Foster", "Sham M Kakade", "Tong Zhang"], "venue": ".", "citeRegEx": "21", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "A good DID system used as a front-end to an automatic speech recognition system, can help improve the recognition performance by providing dialectal data for acoustic and language model adaptation to the specific dialect being spoken [1].", "startOffset": 234, "endOffset": 237}, {"referenceID": 1, "context": "In this work, we focus on Arabic DID which can can be posed as a five class classification problem, given that the Arabic language can be divided into five major dialects; Egyptian (EGY), Gulf (GLF), Lavantine (LAV), Modern Standard Arabic (MSA) and North African (NOR) [2].", "startOffset": 270, "endOffset": 273}, {"referenceID": 2, "context": "These approaches are also known as Vector Space Modeling approaches [3], where speech utterances are represented by a continuous vector of high dimensions.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "A Vector Space Model (VSM) is then constructed using a term-document matrix [4], followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "A Vector Space Model (VSM) is then constructed using a term-document matrix [4], followed by an unsupervised dimensionality reduction technique, such as Principal Component Analysis (PCA) [5] to map the high dimensional feature space to a low dimensional Vector Subspace (Section 2.", "startOffset": 188, "endOffset": 191}, {"referenceID": 5, "context": "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8].", "startOffset": 101, "endOffset": 110}, {"referenceID": 6, "context": "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8].", "startOffset": 101, "endOffset": 110}, {"referenceID": 7, "context": "In other cases, a phone n-gram language model is used to model the phone statistics instead of a VSM [6, 7, 8].", "startOffset": 101, "endOffset": 110}, {"referenceID": 8, "context": "On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10].", "startOffset": 220, "endOffset": 227}, {"referenceID": 9, "context": "On the other hand, Acoustic approaches attempt to extract dialect discriminative information from speech using low level acoustic features, such as pitch, prosody, shifted delta ceptral coefficients, bottleneck features [9, 10].", "startOffset": 220, "endOffset": 227}, {"referenceID": 1, "context": "One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model [2, 11].", "startOffset": 272, "endOffset": 279}, {"referenceID": 10, "context": "One of the most successful acoustic approaches is, the use of i-Vector framework for LID, where i-Vectors are extracted for each speech utterance, using an i-Vector extractor that consists of a GMM-UBM trained on top of BNF, followed by a Total Variability Subspace Model [2, 11].", "startOffset": 272, "endOffset": 279}, {"referenceID": 1, "context": "This model combination approach has been shown to give performace improvements on the DID task [2].", "startOffset": 95, "endOffset": 98}, {"referenceID": 11, "context": "Hotelling [12] (Section 2.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "Details about the phone recognizer can be found in [2].", "startOffset": 51, "endOffset": 54}, {"referenceID": 1, "context": "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works [2, 13].", "startOffset": 103, "endOffset": 110}, {"referenceID": 12, "context": "We use the same Deep Neural Network (DNN) based ASR system to extract the BNF as in our previous works [2, 13].", "startOffset": 103, "endOffset": 110}, {"referenceID": 13, "context": "The target labels of dimension 3040 are provided by a GMM-HMM baseline system trained on 60 hours of Arabic Broadcast speech [14].", "startOffset": 125, "endOffset": 129}, {"referenceID": 14, "context": "For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11].", "startOffset": 99, "endOffset": 107}, {"referenceID": 10, "context": "For a detailed explanation of i-Vector modeling framework, reader is directed to excellent work in [15, 11].", "startOffset": 99, "endOffset": 107}, {"referenceID": 1, "context": "In this work, GMM-UBM model has 2048 gaussian components, MFCC features are extracted using a 25 ms window and the i-Vectors are 400 dimensional [2].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "This method has been shown to improve DID (LID) performance [2, 11].", "startOffset": 60, "endOffset": 67}, {"referenceID": 10, "context": "This method has been shown to improve DID (LID) performance [2, 11].", "startOffset": 60, "endOffset": 67}, {"referenceID": 15, "context": "Hence, CCA can be posed as the following optimization problem [16].", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "For details see [17].", "startOffset": 16, "endOffset": 20}, {"referenceID": 15, "context": "For the proof of this formulation, reader is referred to [16].", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "Training and test data used in this work is the same as used in [2].", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "More details about the train and test data can be found in [2, 18].", "startOffset": 59, "endOffset": 66}, {"referenceID": 17, "context": "More details about the train and test data can be found in [2, 18].", "startOffset": 59, "endOffset": 66}, {"referenceID": 18, "context": "In our case, we use multi-class logistic regression, also known as softmax classification for DID [19, 20].", "startOffset": 98, "endOffset": 106}, {"referenceID": 19, "context": "In our case, we use multi-class logistic regression, also known as softmax classification for DID [19, 20].", "startOffset": 98, "endOffset": 106}, {"referenceID": 20, "context": "CCA is an unsupervised method and can easily incorporate unlabeled data from a different domain and act as a domain adaptation or semi-supervised learning method such as co-training as shown in [21].", "startOffset": 194, "endOffset": 198}], "year": 2016, "abstractText": "In this work, we present a new Vector Space Model (VSM) of speech utterances for the task of spoken dialect identification. Generally, DID systems are built using two sets of features that are extracted from speech utterances; acoustic and phonetic. The acoustic and phonetic features are used to form vector representations of speech utterances in an attempt to encode information about the spoken dialects. The Phonotactic and Acoustic VSMs, thus formed, are used for the task of DID. The aim of this paper is to construct a single VSM that encodes information about spoken dialects from both the Phonotactic and Acoustic VSMs. Given the two views of the data, we make use of a well known multi-view dimensionality reduction technique known as Canonical Correlation Analysis (CCA), to form a single vector representation for each speech utterance that encodes dialect specific discriminative information from both the phonetic and acoustic representations. We refer to this approach as feature space combination approach and show that our CCA based feature vector representation performs better on the Arabic DID task than the phonetic and acoustic feature representations used alone. We also present the feature space combination approach as a viable alternative to the model based combination approach, where two DID systems are built using the two VSMs (Phonotactic and Acoustic) and the final prediction score is the output score combination from the two systems.", "creator": "LaTeX with hyperref package"}}}