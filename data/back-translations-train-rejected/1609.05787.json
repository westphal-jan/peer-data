{"id": "1609.05787", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Context-aware Sequential Recommendation", "abstract": "Since sequential information plays an important role in modeling user behaviors, various sequential recommendation methods have been proposed. Methods based on Markov assumption are widely-used, but independently combine several most recent components. Recently, Recurrent Neural Networks (RNN) based methods have been successfully applied in several sequential modeling tasks. However, for real-world applications, these methods have difficulty in modeling the contextual information, which has been proved to be very important for behavior modeling. In this paper, we propose a novel model, named Context-Aware Recurrent Neural Networks (CA-RNN). Instead of using the constant input matrix and transition matrix in conventional RNN models, CA-RNN employs adaptive context-specific input matrices and adaptive context-specific transition matrices. The adaptive context-specific input matrices capture external situations where user behaviors happen, such as time, location, weather and so on. And the adaptive context-specific transition matrices capture how lengths of time intervals between adjacent behaviors in historical sequences affect the transition of global sequential features. Experimental results show that the proposed CA-RNN model yields significant improvements over state-of-the-art sequential recommendation methods and context-aware recommendation methods on two public datasets, i.e., the Taobao dataset and the Movielens-1M dataset.", "histories": [["v1", "Mon, 19 Sep 2016 15:33:46 GMT  (629kb,D)", "http://arxiv.org/abs/1609.05787v1", "IEEE International Conference on Data Mining (ICDM) 2016, to apear"]], "COMMENTS": "IEEE International Conference on Data Mining (ICDM) 2016, to apear", "reviews": [], "SUBJECTS": "cs.IR cs.AI", "authors": ["qiang liu", "shu wu", "diyi wang", "zhaokang li", "liang wang"], "accepted": false, "id": "1609.05787"}, "pdf": {"name": "1609.05787.pdf", "metadata": {"source": "CRF", "title": "Context-aware Sequential Recommendation", "authors": ["Qiang Liu", "Shu Wu", "Diyi Wang", "Zhaokang Li", "Liang Wang"], "emails": ["shu.wu}@nlpr.ia.ac.cn,", "wang.di@husky.neu.edu,", "zhaokang.li@rice.edu,", "wangliang@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "II. PROPOSED MODEL", "text": "In this section, we present the proposed Context-Aware Recurrent Neural Networks (CA-RNN). First, we present the problem definition and conventional RNN model, then we explain our proposed model, and finally, we present the parameter learning process of the CA-RNN model."}, {"heading": "A. Problem Definition", "text": "We have a set of users referred to as U = {u1, u2,...}, and a set of users referred to as V = {v1, v2,...}. For each user u, the behavior history is given as V = {vu1, vu2,...}, with evuk denoting the k-th selected user u. Each selected user in the behavior history is associated with appropriate timestamps Tu = {tu1, tu2,...}, where Tuk denotes the lowest timestamp in the behavior sequence of user u. For each user u, at a certain time stamp tuk, the input context is referred to as cuI, k, such as weather, location, etc., and the corresponding transition context is called cuT, k, which is determined by the time interval between the timestamp tuk of the current behavior and the timestamp t u k \u2212 1 of the previous behavior."}, {"heading": "B. Recurrent Neural Networks", "text": "The architecture of a recursive neural network is a recursive structure with several hidden layers. At each step, we can predict the output unit and then return the new output to the next hidden state. In RNN, the hidden layer is formulated as: huk = f (rukM + h u k \u2212 1W), (1) where huk is the d-dimensional hidden state of the user u at the time of the timestamp tuk in the behavior sequence, and r u k is the d-dimensional latent vector of the corresponding item vuk. W is the d-dimensional transition matrix, which functions as a recursive link to propagate sequential signals from the previous state. M stands for the d-d-dimensional input matrix, which captures the current behavior of the user on input elements. The activation function f (x) is usually used as a sigmoid function f (x) = sequential (+ 1) information matrix, although it is successful in different architecture."}, {"heading": "C. Modeling Input Contexts", "text": "Input contexts describe the external situations in which users behave. Such contexts in behavioral history are important for predicting behavior. For example, if a man normally exercises in the morning, we can predict that he will go to the gym in the morning and not in the evening, even if many people go to the gym in the evening. Therefore, we plan to include input contexts in the conventional RNN model. (2) The constant input matrix in the conventional RNN is replaced by adaptive context-specific input matrix according to different input contexts. Then, Equation 1 can be rewritten to: huk = f (rukMcuI, k + h u k \u2212 1W), (2) where McuI, k denotes the d \u00b7 d dimensional context-specific input matrix of the input context cuI, k. It integrates external contexts into items to appropriately shape the interests of users in certain contexts."}, {"heading": "D. Modeling Transition Contexts", "text": "Transition contexts mean time intervals between adjacent behaviors in historical sequences. Different time intervals between two behaviors have different effects on prediction. In general, longer time intervals generally have a weaker effect in predicting the next behavior than shorter ones. Therefore, modeling time intervals is essential for predicting future behaviors. Therefore, we treat time intervals as transition contexts and integrate transition contexts into our model. The constant transition matrix in conventional RNN is replaced by adaptive context-specific transition matrices according to different transition contexts. Then, Equation 2 can be: huk = f (rukMcuI, k + h u \u2212 k \u2212 WcuT, k), (3) where WcuT, k is a d \u00d7 d dimensional fit matrix for context-specific intervals."}, {"heading": "E. Context-aware Prediction", "text": "When predicting user behavior in the timestamp tuk + 1, in spite of the historical behavioral contexts modelled above, current contexts cuI, k + 1 and c u T, k + 1 also have current contexts. Therefore, our method should include current contexts in order to make context-aware predictions. The prediction function, i.e. whether user u chooses the dot v in the timestamp tuk + 1 under the contexts cuI, k + 1 and c u T, k + 1, can be written as follows: yu, k + 1, v = h u kW \u2032 cuT, k + 1 (rvM \u2032 cuI, k + 1) T, (6) where M \u2032 cuI, k + 1 and W \u2032 cuT, k + 1 d \u00b7 d \u00b7 d dimensional matrix representations of the current input context cuI, k + 1 and transition context cuT, k + 1. W \u2032 cuT, k + 1 can be generated according to the calculation in equation 4."}, {"heading": "F. Parameter Learning", "text": "In this subsection, we present the learning process of CA-RNN with BPR [5] and BPTT [8]. These methods have been successfully applied to parameter estimation of RNN-based recommendation models [3].BPR is a widely used pair-by-pair ranking framework for implicit feedback data. The basic assumption of BPR is that a user prefers selected items over negative ones. In the context of BPR, the goal of CA-RNN is to maximize the following probability at each sequential position k: p (u, k, v \u2032) = g (yu, v \u2212 yu, k, v \u2032), (7) where v \u2032 denotes a negative sample and g (x) is a nonlinear function that can be defined as g (x) = 1 / 1 + e \u2212 x. Including the negative log parameters, we can solve the following objective function equivalently: J = u + lk + 1 k \u00b2 to calculate \u2212 CA \u2212"}, {"heading": "III. EXPERIMENTS", "text": "In this section, we conduct empirical experiments to demonstrate the effectiveness of CA-RNN on context-sensitive sequential recommendations. First, we present our experimental settings, then we conduct experiments to compare input contexts and transition contexts, then we compare our proposed CA-RNN model with some state-of-the-art sequential and context-sensitive recommendation methods, and finally, we examine the effects of dimensionality on CA-RNN."}, {"heading": "A. Experimental Settings", "text": "This year, it has come to the point where it is only a matter of time before a result is achieved."}, {"heading": "C. Performance Comparison", "text": "To evaluate the effectiveness of our proposed CA-RNN, we illustrate the performance comparison of CA-RNN and 4Code: http: / / qiangliucasia.github.io / homepage / files / CARNcode.zipother compared the performance in Figure II. This indicates that, compared to the output performance of POP and BPR, context-sensitive methods FM and CARS2 achieve more significant improvements, and sequential methods FPMC, HRM and RNN continue to perform better than FM and CARS2, suggesting that sequential information has more significant impacts, and RNN achieves the best performance of all the compared methods. Furthermore, we can find that our proposed CA-RNN methods perform significantly better on both sets of data across all metrics."}, {"heading": "IV. CONCLUSIONS", "text": "In this paper, we address the problem of context-sensitive sequential recommendations and propose a new method, i.e. context-sensitive recurring neural networks. In CA-RNN, the constant input matrix of conventional RNN is replaced by context-sensitive input matrices to model complex real contexts such as time, place, and weather. Meanwhile, context-sensitive transition matrices are included in the modeling of transition contexts, i.e. time intervals between adjacent behaviors in historical sequences, rather than the constants in conventional RNN. Experimental results from two real data sets show that CA-RNN surpasses competitive sequential and context-sensitive models."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is jointly supported by the Chinese Basic Research Programme (2012CB316300) and the Chinese National Natural Science Foundation (61403390, U1435221, 61420106015, 61525306)."}], "references": [{"title": "Context-aware recommender systems", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "In Recommender systems handbook,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Cot: Contextual operating tensor for context-aware recommender systems", "author": ["Q. Liu", "S. Wu", "L. Wang"], "venue": "In AAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Predicting the next location: A recurrent model with spatial and temporal contexts", "author": ["Q. Liu", "S. Wu", "L. Wang", "T. Tan"], "venue": "In AAAI,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Recurrent neural network based language model", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "In INTERSPEECH,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Bpr: Bayesian personalized ranking from implicit feedback", "author": ["S. Rendle", "C. Freudenthaler", "Z. Gantner", "L. Schmidt- Thieme"], "venue": "In UAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "Factorizing personalized markov chains for next-basket recommendation", "author": ["S. Rendle", "C. Freudenthaler", "L. Schmidt-Thieme"], "venue": "In WWW,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2010}, {"title": "Fast context-aware recommendations with factorization machines", "author": ["S. Rendle", "Z. Gantner", "C. Freudenthaler", "L. Schmidt- Thieme"], "venue": "In SIGIR,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1988}, {"title": "Cars2: Learning context-aware representations for context-aware recommendations", "author": ["Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "A. Hanjalic"], "venue": "In CIKM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Tfmap: Optimizing map for top-n contextaware recommendation", "author": ["Y. Shi", "A. Karatzoglou", "L. Baltrunas", "M. Larson", "A. Hanjalic", "N. Oliver"], "venue": "In SIGIR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Learning hierarchical representation model for next basket recommendation", "author": ["P. Wang", "J. Guo", "Y. Lan", "J. Xu", "S. Wan", "X. Cheng"], "venue": "In SIGIR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Contextual operation for recommender systems", "author": ["S. Wu", "Q. Liu", "L. Wang", "T. Tan"], "venue": "In TKDE,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "A dynamic recurrent model for next basket recommendation", "author": ["F. Yu", "Q. Liu", "S. Wu", "L. Wang", "T. Tan"], "venue": "In SIGIR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Sequential click prediction for sponsored search with recurrent neural networks", "author": ["Y. Zhang", "H. Dai", "C. Xu", "J. Feng", "T. Wang", "J. Bian", "B. Wang", "T.-Y. Liu"], "venue": "In AAAI,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Methods based on Markov assumption, including Factorizing Personalized Markov Chain (FPMC) [6] and Hierarchical Representation Model (HRM) [11], have been widely used for sequential prediction.", "startOffset": 91, "endOffset": 94}, {"referenceID": 10, "context": "Methods based on Markov assumption, including Factorizing Personalized Markov Chain (FPMC) [6] and Hierarchical Representation Model (HRM) [11], have been widely used for sequential prediction.", "startOffset": 139, "endOffset": 143}, {"referenceID": 3, "context": ", sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].", "startOffset": 20, "endOffset": 23}, {"referenceID": 13, "context": ", sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].", "startOffset": 42, "endOffset": 46}, {"referenceID": 2, "context": ", sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].", "startOffset": 68, "endOffset": 71}, {"referenceID": 12, "context": ", sentence modeling [4], click prediction [14], location prediction [3], and next basket recommendation [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 0, "context": "Besides, the contextual information has been proved to be useful in determining users\u2019 preferences in recommender systems [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 6, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 162, "endOffset": 165}, {"referenceID": 9, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 223, "endOffset": 227}, {"referenceID": 8, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 235, "endOffset": 238}, {"referenceID": 1, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 277, "endOffset": 280}, {"referenceID": 11, "context": "On the other hand, context-aware recommendation has been extensively studied, and several methods have been proposed recently, such as Factorization Machine (FM) [7], Tensor Factorization for MAP maximization (TFMAP) model [10], CARS2 [9] and Contextual Operating Tensor (COT) [2][12].", "startOffset": 280, "endOffset": 284}, {"referenceID": 4, "context": "Then, we incorporate Bayesian Personalized Ranking (BPR) [5] and Back Propagation Through Time (BPTT) [8] for the learning of CA-RNN.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "Then, we incorporate Bayesian Personalized Ranking (BPR) [5] and Back Propagation Through Time (BPTT) [8] for the learning of CA-RNN.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "In this subsection, we introduce the learning process of CA-RNN with BPR [5] and BPTT [8].", "startOffset": 73, "endOffset": 76}, {"referenceID": 7, "context": "In this subsection, we introduce the learning process of CA-RNN with BPR [5] and BPTT [8].", "startOffset": 86, "endOffset": 89}, {"referenceID": 2, "context": "These methods have been successfully used for parameter estimation of RNN based recommendation models [3][13].", "startOffset": 102, "endOffset": 105}, {"referenceID": 12, "context": "These methods have been successfully used for parameter estimation of RNN based recommendation models [3][13].", "startOffset": 105, "endOffset": 109}, {"referenceID": 4, "context": "Baseline methods contain POP and BPR [5].", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "Context-aware methods consist of FM [7] and CARS2 [9].", "startOffset": 36, "endOffset": 39}, {"referenceID": 8, "context": "Context-aware methods consist of FM [7] and CARS2 [9].", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "And sequential methods include FPMC [6], HRM [11] and RNN [13].", "startOffset": 36, "endOffset": 39}, {"referenceID": 10, "context": "And sequential methods include FPMC [6], HRM [11] and RNN [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 12, "context": "And sequential methods include FPMC [6], HRM [11] and RNN [13].", "startOffset": 58, "endOffset": 62}], "year": 2016, "abstractText": "Since sequential information plays an important role in modeling user behaviors, various sequential recommendation methods have been proposed. Methods based on Markov assumption are widely-used, but independently combine several most recent components. Recently, Recurrent Neural Networks (RNN) based methods have been successfully applied in several sequential modeling tasks. However, for realworld applications, these methods have difficulty in modeling the contextual information, which has been proved to be very important for behavior modeling. In this paper, we propose a novel model, named Context-Aware Recurrent Neural Networks (CA-RNN). Instead of using the constant input matrix and transition matrix in conventional RNN models, CA-RNN employs adaptive context-specific input matrices and adaptive context-specific transition matrices. The adaptive contextspecific input matrices capture external situations where user behaviors happen, such as time, location, weather and so on. And the adaptive context-specific transition matrices capture how lengths of time intervals between adjacent behaviors in historical sequences affect the transition of global sequential features. Experimental results show that the proposed CARNN model yields significant improvements over state-of-theart sequential recommendation methods and context-aware recommendation methods on two public datasets, i.e., the Taobao dataset and the Movielens-1M dataset.", "creator": "LaTeX with hyperref package"}}}