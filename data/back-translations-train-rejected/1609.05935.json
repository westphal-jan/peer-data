{"id": "1609.05935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "19-Sep-2016", "title": "Advances in All-Neural Speech Recognition", "abstract": "This paper advances the design of CTC-based all-neural (or end-to-end) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.", "histories": [["v1", "Mon, 19 Sep 2016 20:52:44 GMT  (20kb)", "http://arxiv.org/abs/1609.05935v1", null], ["v2", "Wed, 25 Jan 2017 08:30:10 GMT  (15kb)", "http://arxiv.org/abs/1609.05935v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["g zweig", "c yu", "j droppo", "a stolcke"], "accepted": false, "id": "1609.05935"}, "pdf": {"name": "1609.05935.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 9.05 935v 1 [cs.C L] 19 Sep 2016Index Terms - relapsing neural network, CTC, speech recognition, end-to-end training."}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them will be able to move to another world in which they are able, in which they are able to move, and in which they are able to move."}, {"heading": "2. RELATION TO PRIOR WORK", "text": "Letter-based or graphical systems have long been studied [24, 25, 26, 27] and are attractive because they reduce the need to create a dictionary of word pronunciations. Previous work was motivated by the need to quickly build systems in new languages without a dictionary, and retained the rest of a standard HMM system, in particular the use of a decision tree. In contrast, we follow current work [22, 18, 23, 19] in which a neural network implicitly learns context dependence. Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17]. Unlike [20, 28, 17] we use a ReLU RNN instead of an LSTM and find it is effective and much faster. Unlike [19] we use recurring networks at each level as opposed to deep neural networks (DNNs) at the lower level and an RNN at the top level."}, {"heading": "3. THE MODEL", "text": "Central to the CTC process is the use of a \"don't care\" or blank symbol, which may optionally occur between regular symbols. Standard alpha beta recursions are used to calculate the occupancy probabilities of the posterior state. Leave the input from t-acoustic frames together with a symbol sequence S. Specify an alignment of t-audio frames to the sequence S by \u03c0, and the product of the neural net probabilities at state level for alignment as P (S | \u03c0). Let ptq be the probability that the neural network assigns t to symbol q at a time, i.e. the output according to the Softmax function. The CTC objective function is given as P (\u03c0), tp t-notative for symbol q, taking into account the state probability of the neural network for the state signal."}, {"heading": "4. INTERPRETING THE OUTPUT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Raw CTC Output", "text": "After training, the output of the RNN can be directly converted into a readable string. There are two problems to solve: 1. Where to put the spaces between the words? 2. How to distinguish cases of repeated characters, e.g. ll in hello, from a sequence of frames labeled with the same letter, e.g. l in the help. Remember that the empty CTC symbol is optional, so that a sequence of frames labeled with a single letter cannot immediately be distinguished from multiple occurrences of that letter. If the symbol stock contains a space (as opposed to the empty symbol), the first problem is easy to solve. Past work, e.g. [18] solves the second problem by searching for alternatives or requiring a space between letters. Instead, we suggest a new symbol inventory as described below."}, {"heading": "4.2. Symbol Inventory", "text": "Previous work [18, 19, 22] has explicitly modelled the spaces between words in the acoustic model, e.g. with a special space that differs from the CTC space. Since words often converge, we propose an alternative representation that distinguishes word beginning from non-initial letters. A convenient representation for this is the use of uppercase letters in the beginning of the word. Since the forward-looking calculation in CTCer requires that the input sequence is longer than the output sequence, it also increases the set of expressions that can be aligned. This is also consistent with speech recognition systems that use position-dependent phonetic variants. To identify repeated letters, we use special double letter units to represent repeated letters such as ll. Finally, to improve the legibility of the output without further processing, we add the following letter apostrophe by creating units such as the \"d\" of this set \"is likely to be very large in total."}, {"heading": "4.3. Character Beam Search", "text": "Previous work [19, 18, 23] used a character-level language model to improve the output of a neural system, which implements the classic decoding paradigm of Equation 1 at the character level, with the character-level model providing P (w), or in this case P (c). The neural network provides P (c | a), and the beam search is used to find the most likely string. Results for this approach are presented in Section 7.4."}, {"heading": "4.4. Word Beam Search", "text": "The decoder is the dynamic decoder as described in [29]. We used the CUED RNNLM toolkit [30] to train two forward and two reverse RNN language models, which are interpolated with a standard 4 gram model and used to re-sort N-best lists produced by the N-gram decoder. Details can be found in an accompanying paper [31].5. ITERATED CTC (CTC2) The output described in the previous section is, of course, loud. One of the statements in the switchboard, for example, is \"no N-best lists produced by the N-gram decoder,\" but the raw network output is not exactly included, \"and now we have a specific sequence in which the ecural is.\" The classic approach to improving this approach is to repeat the approach that the STC model is repeated with a uniform Slextor feature."}, {"heading": "6. TRAINING PROCESS", "text": "For networks with a width of 1024 and a depth of 7 or more, we found that 64 utterances had to be processed simultaneously to achieve accurate gradient estimates and stable convergence. We use frame skipping [17], in which we stack three consecutive frames into a single vector to generate an input sequence one-third as long and three times as wide as the original input. When we train on the 300-hour switchboard, we use a learning rate per frame of 0.5 and decrease it by a factor of 4 if three iterations over the data do not improve the development data. If the Fisher data is added, resulting in about 2,000 hours of training data, the rate is reduced if a single iteration passes without increasing the resolution."}, {"heading": "6.1. Stabilization Methods", "text": "In initial experiments, we found it useful to introduce several stabilization techniques. Most importantly, we use gradient clipping to prevent \"exploding gradients\" during RNN training; the size of the gradients is capped to 1 before the impulse update; secondly, we noticed that when rare units are present (e.g., the \"ii\" in \"Hawaii\"), the training process tends to push their probabilities between events close to 0, resulting in poor performance and sometimes instability when the unit is finally seen; to avoid this, we interpolate the gradient with a small gradient that tends to uniform distribution; this is implemented by interpolating the \u03b3 values from the alpha beta calculation with a uniform distribution; we reserve 1% of the total truth mass for this uniform distribution; finally, we also found it important to perform the gradient over a large number of statements (32 or 64 before we perform a terameterization)."}, {"heading": "6.2. Model Initialization", "text": "Weight matrices are initialized with small random weights that are evenly distributed and inversely proportional to the square root of the fanin, with one exception. At the output level, which maps from the hidden dimension of the RNN (typically 1024) to the size of the symbol inventory (79 in our case), we assign explicit responsibility for each output symbol to a specific activation of the RNN by using an identity matrix for the first 79 dimensions and zeros elsewhere. In the case of bidirectional networks, this is done symmetrically, so that both forward and backward parts of the network contribute equally. Although we have not fully evaluated this scheme, we observed small increases in initial experiments. Simultaneously with this work, a similar scheme was recently proposed in [32] for standard systems of neural networks."}, {"heading": "6.3. Polishing with In-domain Data", "text": "Our training process begins and ends with a focus on the pure 300-hour data set. We start with the 300-hour set mainly for convenience, showing results with both the 300-hour data sets (Switchboard) and the 2000-hour data sets (Switchboard + Fisher). The 2000-hour models presented here were initialized with the output of 300-hour training. Although the training works from the ground up with 2000 hours of data, we found it consistently useful to complete all training runs with a few more iterations of training on pure in-domain switchboard data. We do this from a very low learning rate (one-tenth of the normal rate), and the biggest gain is observed in the first iteration of the training."}, {"heading": "7. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1. Corpus", "text": "For comparison with [18, 19, 23, 22] we present the results of the NIST 2000 Telephone Evaluation Set (CTS).The model selection uses the RT-02 CTS Evaluation Set for development. Input features are 40-dimensional log-mel filter bank energies, which are extracted every 10 milliseconds. Characteristic vectors are normalized to zero on one level per utterance. As logarithmic compression already limits the dynamic range to a reasonable level, we do not normalize variance."}, {"heading": "7.2. Network Architecture and Symbol Inventory", "text": "For computational efficiency, we limited ourselves to the models directly supported by the CUDNN v5.0 library, which includes unidirectional and bidirectional multi-layer networks. Support is provided for LSTMs, gated recurrent units, standard sigmoid RNNs, and ReLU RNs. In initial experiments, we found that ReLU RNNs are just as good for this task as LSTMs, and many times faster. Therefore, we use them exclusively in experiments. We continue to focus on bidirectional networks for improved performance. Table 1 shows the effect of our choice of symbol inventory. We see that the use of special word beginnings improves performance over the use of explicit blanks. Redundant modeling wordfinal characters does not provide any further improvement. To simplify interpretability, all models use explicit double-character symbols. In Table 2, we show the effect of network width, with a constant depth of 5. While the widest of this network is not 10- 1024 for our network, the widest layer is 10- for our 1024."}, {"heading": "7.3. Iterated CTC and Beam Search", "text": "We evaluate the post-processing methods in Table 4. Obviously, the RNN does not yet learn the full logic of a bar search decoder, and the effectiveness of a character-based bar search lies halfway between using raw output and a full word-based search. We see that iterated CTC can make a significant improvement, though not as much as a full bar search while remaining within a purely neural framework. An investigation of the iterated CTC errors shows that they largely reduce the substitution rates, as the global shifts caused by insertions and deletions seem difficult to compensate for the RNN."}, {"heading": "7.4. Comparison to Previously Published Results", "text": "We are summarizing our results based on the NIST 2000 CTS test set and comparing them with previous results in Tables 5 and 6. Systems are categorized based on the use of a lexicon to enforce the output of legal words and the use of a language model. We see an improvement over previous results with our RNN-based system. [28] presents an LSTM-CTC system that uses phonemic rather than graphical targets and achieves a 15% error rate on the panel portion of the 2000 age; this system still uses a phonetic dictionary. Compared to a standard system such as [33], which achieves 9.6% and 13% on the panel and CallHome, respectively, we see that current neural systems cannot yet replicate all the logic in a conventional system. However, our Relu-RNN system represents a new state of the art for a purely neural system, and we see that the performance of such systems is improving rapidly."}, {"heading": "8. CONCLUSIONS", "text": "We are extending the state of the art with purely neural speech recognition, mainly through the use of novel symbol encoding, and optimising the training process. In addition, we are presenting an iterated CTC approach for use without the decoding process. In this context, a network initially maps from audio to symbols, followed by a second Symbolto symbol mapping network. Using both raw network output and search-based post-processing, we systematically improve already published results in the paradigm of end-to-end neural speech recognition."}, {"heading": "9. REFERENCES", "text": "[1] George E Dahl, Dong Yu, Li Deng, and Alex Acerreiter Frank, \"Large vocabulary continuous speech recognition with context-dependent DBN-HMMs,\" in 2011 IEEE International Conference on Acoustics, speech and signal processing (ICASSP), 2011, pp. 4688-4691. [2] Abdel-rahman Mohamed, George Dahl, and Geoffrey Hinton, \"Deep belief networks for phone recognition,\" in NIPS workshop on deep learning for speech recognition and related applications, 2009, vol. 1, p. 39. [3] Frank Seide, Gang Li, and Dong Yu \"Conversational speech transcription using context-dependent deep neural networks,\" in Interspeech, 2011, pp. 437-440] Hasim Sak, Andrew W Senior, and Franc."}], "references": [{"title": "Large vocabulary continuous speech recognition with contextdependent DBN-HMMs", "author": ["George E Dahl", "Dong Yu", "Li Deng", "Alex Acero"], "venue": "2011 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2011, pp. 4688\u20134691.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep belief networks for phone recognition", "author": ["Abdel-rahman Mohamed", "George Dahl", "Geoffrey Hinton"], "venue": "NIPS workshop on deep learning for speech recognition and related applications, 2009, vol. 1, p. 39.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Conversational speech transcription using context-dependent deep neural networks", "author": ["Frank Seide", "Gang Li", "Dong Yu"], "venue": "Interspeech, 2011, pp. 437\u2013440.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew W Senior", "Fran\u00e7oise Beaufays"], "venue": "INTERSPEECH, 2014, pp. 338\u2013342.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist speech recognition: a hybrid approach, Springer", "author": ["Herve A Bourlard", "Nelson Morgan"], "venue": "Science & Business Media,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1993}, {"title": "A recurrent error propagation network speech recognition system", "author": ["Tony Robinson", "Frank Fallside"], "venue": "Computer Speech & Language, vol. 5, no. 3, pp. 259\u2013274, 1991.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1991}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["Hynek Hermansky", "Daniel PW Ellis", "Sangita Sharma"], "venue": "ICASSP 2000. IEEE, 2000, vol. 3, pp. 1635\u20131638.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "Treebased state tying for high accuracy acoustic modelling", "author": ["Steve J Young", "Julian J Odell", "Philip C Woodland"], "venue": "Proceedings of the workshop on Human Language Technology. Association for Computational Linguistics, 1994, pp. 307\u2013312.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1994}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Grammar as a foreign language", "author": ["Oriol Vinyals", "\u0141ukasz Kaiser", "Terry Koo", "Slav Petrov", "Ilya Sutskever", "Geoffrey Hinton"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2773\u20132781.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "ICML, 2014, vol. 14, pp. 1764\u20131772.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "Fast and accurate recurrent neural network acoustic models for speech recognition", "author": ["Ha\u015fim Sak", "Andrew Senior", "Kanishka Rao", "Fran\u00e7oise Beaufays"], "venue": "arXiv preprint arXiv:1507.06947, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["Andrew L Maas", "Ziang Xie", "Dan Jurafsky", "Andrew Y Ng"], "venue": "Proc. NAACL, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep speech: Scaling up endto-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167\u2013174.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end attention-based large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Yoshua Bengio"], "venue": "ICASSP 2016. IEEE, 2016, pp. 4945\u2013 4949.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["Liang Lu", "Xingxing Zhang", "Steve Renals"], "venue": "ICASSP 2016. IEEE, 2016, pp. 5060\u20135064.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Listen, attend and spell: A neural network for large vocabulary conversational speech recognition", "author": ["William Chan", "Navdeep Jaitly", "Quoc Le", "Oriol Vinyals"], "venue": "ICASSP 2016. IEEE, 2016, pp. 4960\u20134964.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic speech recognition without phonemes", "author": ["Ernst G\u00fcnter Schukat-Talamazzini", "Heinrich Niemann", "Wieland Eckert", "Thomas Kuhn", "S Rieck"], "venue": "Eurospeech, 1993.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1993}, {"title": "Grapheme based speech recognition for large vocabularies", "author": ["Christoph Schillo", "Gernot A Fink", "Franz Kummert"], "venue": "INTERSPEECH, 2000, pp. 584\u2013587.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "Context-dependent acoustic modeling using graphemes for large vocabulary speech recognition", "author": ["Stephan Kanthak", "Hermann Ney"], "venue": "ICASSP, 2002, vol. 2, pp. 845\u2013848.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2002}, {"title": "Grapheme based speech recognition", "author": ["Mirjam Killer", "Sebastian St\u00fcker", "Tanja Schultz"], "venue": "INTERSPEECH, 2003.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2003}, {"title": "An empirical exploration of ctc acoustic models", "author": ["Yajie Miao", "Mohammad Gowayyed", "Xingyu Na", "Tom Ko", "Florian Metze", "Alexander Waibel"], "venue": "ICASSP. IEEE, 2016, pp. 2623\u2013 2627.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallelizing wfst speech decoders", "author": ["Charith Mendis", "Jasha Droppo", "Saeed Maleki", "Madanlal Musuvathi", "Todd Mytkowicz", "Geoffrey Zweig"], "venue": "ICASSP 2016. IEEE, 2016, pp. 5325\u20135329.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "CUED-RNNLM: An open-source toolkit for efficient training and evaluation of recurrent neural network language models", "author": ["Xie Chen", "Xunying Liu", "Yanmin Qian", "MJF Gales", "PC Woodland"], "venue": "ICASSP 2016. IEEE, 2016, pp. 6000\u2013 6004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "The Microsoft 2016 conversational speech recognition system", "author": ["W. Xiong", "J. Droppo", "X. Huang", "F. Seide", "M. Seltzer", "A. Stolcke", "D. Yu", "G. Zweig"], "venue": "submitted to ICASSP, 2017.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2017}, {"title": "Purely sequence-trained neural networks for asr based on lattice-free MMI", "author": ["Daniel Povey", "Vijayaditya Peddinti", "Daniel Galvez", "Pegah Ghahrmani", "Vimal Manohar", "Xingyu Na", "Yiming Wang", "Sanjeev Khudanpur"], "venue": "Interspeech, 2016.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.", "startOffset": 63, "endOffset": 78}, {"referenceID": 1, "context": "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.", "startOffset": 63, "endOffset": 78}, {"referenceID": 2, "context": "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.", "startOffset": 63, "endOffset": 78}, {"referenceID": 3, "context": "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.", "startOffset": 63, "endOffset": 78}, {"referenceID": 4, "context": "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.", "startOffset": 63, "endOffset": 78}, {"referenceID": 5, "context": "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.", "startOffset": 118, "endOffset": 124}, {"referenceID": 6, "context": "In the recent renaissance of neural network speech recognition [1, 2, 3, 4, 5], as well as in pioneering earlier work [6, 7], the networks have been mainly used as a drop-in replacement for the acoustic model in an HMM system.", "startOffset": 118, "endOffset": 124}, {"referenceID": 7, "context": "They have also been used for featureaugmentation in a \u201ctandem\u201d GMM-HMM system [8], which again relied on a standard HMM backbone.", "startOffset": 78, "endOffset": 81}, {"referenceID": 8, "context": "The acoustic model still uses a decision tree [9] to further decompose the word sequence into context-dependent triphone states, and a decoder to perform a complex discrete search for the likeliest word sequence.", "startOffset": 46, "endOffset": 49}, {"referenceID": 11, "context": "\u201d Two main approaches have been used, both of which attempt to leverage a recurrent neural network\u2019s potential ability to \u201cremember\u201d information for a long period of time and then act on it [12, 13, 14].", "startOffset": 190, "endOffset": 202}, {"referenceID": 12, "context": "\u201d Two main approaches have been used, both of which attempt to leverage a recurrent neural network\u2019s potential ability to \u201cremember\u201d information for a long period of time and then act on it [12, 13, 14].", "startOffset": 190, "endOffset": 202}, {"referenceID": 13, "context": "\u201d Two main approaches have been used, both of which attempt to leverage a recurrent neural network\u2019s potential ability to \u201cremember\u201d information for a long period of time and then act on it [12, 13, 14].", "startOffset": 190, "endOffset": 202}, {"referenceID": 14, "context": "The first of these approaches uses an RNN trained in the \u201cconnectionist temporal classification\u201d (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].", "startOffset": 113, "endOffset": 117}, {"referenceID": 15, "context": "The first of these approaches uses an RNN trained in the \u201cconnectionist temporal classification\u201d (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].", "startOffset": 164, "endOffset": 184}, {"referenceID": 16, "context": "The first of these approaches uses an RNN trained in the \u201cconnectionist temporal classification\u201d (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].", "startOffset": 164, "endOffset": 184}, {"referenceID": 17, "context": "The first of these approaches uses an RNN trained in the \u201cconnectionist temporal classification\u201d (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].", "startOffset": 164, "endOffset": 184}, {"referenceID": 18, "context": "The first of these approaches uses an RNN trained in the \u201cconnectionist temporal classification\u201d (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].", "startOffset": 164, "endOffset": 184}, {"referenceID": 19, "context": "The first of these approaches uses an RNN trained in the \u201cconnectionist temporal classification\u201d (CTC) framework [15] to predict letter rather than phonetic output [16, 17, 18, 19, 20].", "startOffset": 164, "endOffset": 184}, {"referenceID": 9, "context": "1The vanishing gradient problem has been sufficiently overcome by longshort-term memory (LSTM) [10] and recurrent neural networks with rectified linear units (ReLU-RNNs) [11] that many problems in machine translation and language processing can be handled regardless.", "startOffset": 95, "endOffset": 99}, {"referenceID": 10, "context": "1The vanishing gradient problem has been sufficiently overcome by longshort-term memory (LSTM) [10] and recurrent neural networks with rectified linear units (ReLU-RNNs) [11] that many problems in machine translation and language processing can be handled regardless.", "startOffset": 170, "endOffset": 174}, {"referenceID": 20, "context": "an attention mechanism [21, 22, 23].", "startOffset": 23, "endOffset": 35}, {"referenceID": 21, "context": "an attention mechanism [21, 22, 23].", "startOffset": 23, "endOffset": 35}, {"referenceID": 22, "context": "an attention mechanism [21, 22, 23].", "startOffset": 23, "endOffset": 35}, {"referenceID": 23, "context": "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations.", "startOffset": 57, "endOffset": 73}, {"referenceID": 24, "context": "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations.", "startOffset": 57, "endOffset": 73}, {"referenceID": 25, "context": "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations.", "startOffset": 57, "endOffset": 73}, {"referenceID": 26, "context": "Letter-based or graphemic systems have been long studied [24, 25, 26, 27], and are attractive because they alleviate the need to produce a dictionary of word pronunciations.", "startOffset": 57, "endOffset": 73}, {"referenceID": 21, "context": "In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.", "startOffset": 43, "endOffset": 59}, {"referenceID": 17, "context": "In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.", "startOffset": 43, "endOffset": 59}, {"referenceID": 22, "context": "In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.", "startOffset": 43, "endOffset": 59}, {"referenceID": 18, "context": "In contrast to this, we follow recent work [22, 18, 23, 19] where a neural network learns context-dependence implicitly.", "startOffset": 43, "endOffset": 59}, {"referenceID": 19, "context": "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].", "startOffset": 51, "endOffset": 71}, {"referenceID": 27, "context": "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].", "startOffset": 51, "endOffset": 71}, {"referenceID": 18, "context": "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].", "startOffset": 51, "endOffset": 71}, {"referenceID": 17, "context": "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].", "startOffset": 51, "endOffset": 71}, {"referenceID": 16, "context": "Our approach is most similar to the CTC methods of [20, 28, 19, 18, 17].", "startOffset": 51, "endOffset": 71}, {"referenceID": 19, "context": "In contrast to [20, 28, 17], we use a ReLU-RNN rather than an LSTM, and find it to be effective and much faster.", "startOffset": 15, "endOffset": 27}, {"referenceID": 27, "context": "In contrast to [20, 28, 17], we use a ReLU-RNN rather than an LSTM, and find it to be effective and much faster.", "startOffset": 15, "endOffset": 27}, {"referenceID": 16, "context": "In contrast to [20, 28, 17], we use a ReLU-RNN rather than an LSTM, and find it to be effective and much faster.", "startOffset": 15, "endOffset": 27}, {"referenceID": 18, "context": "In contrast to [19], we use recurrent networks at every level as opposed to deep neural nets (DNNs) in the lower levels, and an RNN at the top level only.", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "Also in contrast to [19], we study performance in the absence of an external language model as well as with one.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "Interestingly, the attention-based approach of [22] also introduces an extra", "startOffset": 47, "endOffset": 51}, {"referenceID": 14, "context": "We adopt a multi-layer RNN trained with CTC [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 17, "context": "[18], solve the second problem with a search over alternatives, or requiring a blank between letters.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Past work [18, 19, 22] has explicitly modeled the spaces between words in the acoustic model, e.", "startOffset": 10, "endOffset": 22}, {"referenceID": 18, "context": "Past work [18, 19, 22] has explicitly modeled the spaces between words in the acoustic model, e.", "startOffset": 10, "endOffset": 22}, {"referenceID": 21, "context": "Past work [18, 19, 22] has explicitly modeled the spaces between words in the acoustic model, e.", "startOffset": 10, "endOffset": 22}, {"referenceID": 18, "context": "Previous work [19, 18, 23] has used a character-level language model to improve the output of a neural system.", "startOffset": 14, "endOffset": 26}, {"referenceID": 17, "context": "Previous work [19, 18, 23] has used a character-level language model to improve the output of a neural system.", "startOffset": 14, "endOffset": 26}, {"referenceID": 22, "context": "Previous work [19, 18, 23] has used a character-level language model to improve the output of a neural system.", "startOffset": 14, "endOffset": 26}, {"referenceID": 18, "context": "To provide a complete set of results comparable to [19], we have also used a word-based decoder that uses a graphemic dictionary, and uses the frame-level likelihoods in the standard way.", "startOffset": 51, "endOffset": 55}, {"referenceID": 28, "context": "The decoder is the dynamic decoder as described in [29].", "startOffset": 51, "endOffset": 55}, {"referenceID": 29, "context": "We used the CUED-RNNLM toolkit [30] to train two forward- and two backward-running RNN language models.", "startOffset": 31, "endOffset": 35}, {"referenceID": 30, "context": "Details can be found in a companion paper [31].", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "We use frame-skipping [17], where we stack three consecutive frames into a single vector to produce an input sequence one-third as long and three times as wide as the original input.", "startOffset": 22, "endOffset": 26}, {"referenceID": 17, "context": "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.", "startOffset": 23, "endOffset": 39}, {"referenceID": 18, "context": "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.", "startOffset": 23, "endOffset": 39}, {"referenceID": 22, "context": "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.", "startOffset": 23, "endOffset": 39}, {"referenceID": 21, "context": "For comparability with [18, 19, 23, 22], we present results on the NIST 2000 conversational telephone speech (CTS) evaluation set.", "startOffset": 23, "endOffset": 39}, {"referenceID": 17, "context": "[18] N N 56.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "0 [22] N N 48.", "startOffset": 2, "endOffset": 6}, {"referenceID": 17, "context": "[18] N Char NG 43.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "8 [18] N Char RNN 40.", "startOffset": 2, "endOffset": 6}, {"referenceID": 21, "context": "8 [22] Y Word NG 46.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "8 [19] Y Word NG 31.", "startOffset": 2, "endOffset": 6}, {"referenceID": 18, "context": "[19] (ensemble) Y Word NG 19.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "In [28], a LSTM-CTC system using phonemic rather than grapemic targets is presented, and achieves an error rate of 15% on the Switchboard portion of eval 2000; that system still uses a phonetic dictionary.", "startOffset": 3, "endOffset": 7}, {"referenceID": 31, "context": "Compared to a standard system, such as [33], which achieves 9.", "startOffset": 39, "endOffset": 43}], "year": 2016, "abstractText": "This paper advances the design of CTC-based all-neural (or end-toend) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.", "creator": "LaTeX with hyperref package"}}}