{"id": "1609.06026", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "An Approach for Self-Training Audio Event Detectors Using Web Data", "abstract": "Audio event detection in the era of Big Data has the constraint of lacking annotations to train robust models that match the scale of class diversity. This is mainly due to the expensive and time-consuming process of manually annotating sound events in isolation or as segments within audio recordings. In this paper, we propose an approach for semi-supervised self-training of audio event detectors using unlabeled web data. We started with a small annotated dataset and trained sound events detectors. Then, we crawl and collect thousands of web videos and extract their soundtrack. The segmented soundtracks are run by the detectors and different selection techniques were used to determine whether a segment should be used for self-training the detectors. The original detectors were compared to the self-trained detectors and the results showed a performance improvement by the latter when evaluated on the annotated test set.", "histories": [["v1", "Tue, 20 Sep 2016 05:52:06 GMT  (65kb,D)", "https://arxiv.org/abs/1609.06026v1", "5 pages"], ["v2", "Fri, 23 Sep 2016 14:15:13 GMT  (65kb,D)", "http://arxiv.org/abs/1609.06026v2", "5 pages"], ["v3", "Tue, 27 Jun 2017 17:09:59 GMT  (1760kb,D)", "http://arxiv.org/abs/1609.06026v3", "5 pages"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.MM", "authors": ["benjamin elizalde", "ankit shah", "siddharth dalmia", "min hun lee", "rohan badlani", "anurag kumar", "bhiksha raj", "ian lane"], "accepted": false, "id": "1609.06026"}, "pdf": {"name": "1609.06026.pdf", "metadata": {"source": "CRF", "title": "An Approach for Self-Training Audio Event Detectors Using Web Data", "authors": ["Benjamin Elizalde", "Ankit Shah", "Siddharth Dalmia", "Min Hun Lee", "Rohan Badlani", "Anurag Kumar", "Bhiksha Raj", "Ian Lane"], "emails": ["bmartin1@andrew.cmu.edu,", "ankit.tronix@gmail.com,", "sdalmia@andrew.cmu.edu,", "mhlee@cmu.edu,", "rohan.badlani@gmail.com,", "alnu@andrew.cmu.edu,", "bhiksha@cs.cmu.edu,", "lane@cs.cmu.edu"], "sections": [{"heading": null, "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "II. SEMI-SUPERVISED SELF-TRAINING OF AUDIO EVENT DETECTORS", "text": "Semi-Supervised Self-Training is an algorithm that retrains a model iteratively and illustrates our specific framework in Figure 1. First, the ten class detectors are trained and tested on the basis of the marked data set to calculate a base performance. Second, the blank data is executed by the detectors to obtain a class label with the corresponding confidence value. Third, we have applied a threshold based on the confidence value to determine candidates for self-training of the detectors. Fourth, the detectors are retrained and re-tested on the marked data to calculate the new performance and compare it with the previous one. Finally, steps two, three and four are performed iteratively until the performance converges."}, {"heading": "A. Datasets: Labeled and Unlabeled", "text": "The UrbanSound8K (US8K) dataset includes 10 classes: air conditioning, car horn, children playing, dog barking and street music, gunfire, drilling, idle, siren, jackhammer. Content of the audio can overlap and the target sound can occur in the background or in the foreground. The dataset includes approximately 8,732 audio segments with an average duration of 3.5 seconds. These files are distributed into 10 layered cross-validation folders.Unlabeled Data (Self-Training): YouTube Videos The unlabeled audio comes from YouTube videos and videos are the largest source of audio. The site was chosen because it offers a wide variety of class samples. The soundtracks of the videos were crawled and downloaded using the Pafy API1. The audio roughly matches the 10 classes of US8k. The acoustic content is unstructured and the target factors are multiple sound effects that overlap through 816 seconds."}, {"heading": "B. Data Preparation", "text": "The method we followed to calculate BoAW's properties is shown in Figure 2 and in detail in these papers [13], [14]. In the first step, we compile all MFCCs in the training set."}, {"heading": "III. EXPERIMENTS AND EVALUATION OF METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Computing the Baseline Performance and Running Detectors on Unlabeled Data", "text": "The detectors used the marked data from US8k, which is divided into 10 layered folds. We used 9 folds as training data and tested them on the left fold. This is done in 10 different ways, resulting in 100 passes for all 10 event classes and 10 folds. We evaluated our detectors with average precision because we wanted to detect reliable positive or negative samples. For each class, the average precision (AP) is calculated for each fold, as well as the mean AP for all folds designated as Mean AP. Subsequently, the detectors were run on the unlabeled data set to obtain confidence values and labels for each of the 200,000 segments. Note that the unlabeled data was carefully handled to be consistent with the 10x cross-validation setup. For example, the detectors trained on the first 9 folds may not perform the same performance as the detectors trained with each other fold."}, {"heading": "B. Selecting Candidates and Self-Training Detectors", "text": "The candidates were used for self-training of the detectors in combination with the monitored audio segments. Once the detectors were retrained, they were executed on the monitored test set and their performance was calculated. The mean AP value was compared with the measurement line and the entire process was repeated iteratively until the Mean AP converged.A key step in the self-training process is the selection of the candidates. We have tried three main approaches: Detector's Output Scores, Precision and Clarity Index.Score-based Under this approach, the output of the detector is a probability value that can be interpreted as a confidence value and used for self-training in the work. A score threshold of more or equal to 0.95 was selected to filter in each segment where 0 is the lowest confidence and 1 the strongest confidence."}, {"heading": "IV. RESULTS AND DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Baseline", "text": "The mean AP value was 57.8% for SVM and 66.3% for NN and is shown in Table I. One reason for the NN's better performance is that it used nonlinear decision boundaries to match the data, unlike the SVM, which used linear boundaries. As mentioned before, the SVM can also support nonlinear decision boundaries by using cores, but the computing time was a problem for processing 200,000 segments for the 10x combinations and retraining."}, {"heading": "B. Self-Training", "text": "The main findings of this study are that most people will be able to move around without being afraid that they will be able to move around."}, {"heading": "V. LIMITATIONS AND FUTURE WORK", "text": "Partially monitored approaches have limitations in how far they can help, as discussed in [17]. Specifically, self-training has an inherent problem with the detector bias that occurs when a detector is trained with an initial set of data, the detector is then run on the unlabeled data, and the reliability depends on the initial model. As we add new segments, we enforce the acoustic properties of the previous model and do not necessarily make our models more robust. Solving the problem was out of the question, but could be a reason for the rapid convergence of our results. Limit type The thresholds used are a reasonable approach supported in the literature, but a more sophisticated objective function should be considered to better select candidates."}, {"heading": "VI. CONCLUSIONS", "text": "In this paper, we proposed a framework for semi-supervised self-training of audio event detectors, in which the detectors were trained with the annotated US8K dataset, and the self-training used unlabeled audio from YouTube videos. The NN detectors delivered a higher base performance than SVM. Both detectors and almost all classes benefited from self-training. Despite the audio discrepancy and the possibility of having few or no target noises as candidates, the performance did not deteriorate after the self-training. Further investigations into candidate selection provide a valuable opportunity. Unlabeled audio from videos can help in the detection of audio events."}], "references": [{"title": "Multimedia information retrieval: content-based information retrieval from large text and audio databases", "author": ["P. Sch\u00e4uble"], "venue": "Springer Science & Business Media,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Content-based multimedia information retrieval: State of the art and challenges", "author": ["M.S. Lew", "N. Sebe", "C. Djeraba", "R. Jain"], "venue": "ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 2, no. 1, pp. 1\u201319, 2006.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2006}, {"title": "Sound representation and classification benchmark for domestic robots", "author": ["J. Maxime", "X. Alameda-Pineda", "L. Girin", "R. Horaud"], "venue": "2014 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2014, pp. 6285\u20136292.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Soundevent recognition with a companion humanoid", "author": ["M. Janvier", "X. Alameda-Pineda", "L. Girinz", "R. Horaud"], "venue": "2012 12th IEEE- RAS International Conference on Humanoid Robots (Humanoids 2012). IEEE, 2012, pp. 104\u2013111.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "A dataset and taxonomy for urban sound research", "author": ["J. Salamon", "C. Jacoby", "J.P. Bello"], "venue": "22st ACM International Conference on Multimedia (ACM-MM\u201914), Orlando, FL, USA, Nov. 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Detection and classification of acoustic scenes and events", "author": ["D. Stowell", "D. Giannoulis", "E. Benetos", "M. Lagrange", "M.D. Plumbley"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 10, pp. 1733\u20131746, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio concept classification with hierarchical deep neural networks", "author": ["M. Ravanelli", "B. Elizalde", "K. Ni", "G. Friedland"], "venue": "Proceedings of EUSIPCO, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["K.J. Piczak"], "venue": "2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2015, pp. 1\u20136.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Proceedings of the Detection and Classification of Acoustic Scenes and Events 2016 Workshop (DCASE2016)", "author": ["T. Virtanen", "A. Mesaros", "T. Heittola", "M. Plumbley", "P. Foster", "E. Benetos", "M. Lagrange"], "venue": "Tampere University of Technology. Department of Signal Processing,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Semi-supervised active learning for sound classification in hybrid learning environments", "author": ["W. Han", "E. Coutinho", "H. Ruan", "H. Li", "B. Schuller", "X. Yu", "X. Zhu"], "venue": "PloS one, vol. 11, no. 9, p. e0162075, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Semi-supervised learning helps in sound event classification", "author": ["Z. Zhang", "B. Schuller"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2012 IEEE International Conference on. IEEE, 2012, pp. 333\u2013336.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Improved audio features for largescale multimedia event detection", "author": ["F. Metze", "S. Rawat", "Y. Wang"], "venue": "Multimedia and Expo (ICME), 2014 IEEE International Conference on. IEEE, 2014, pp. 1\u20136.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "The perceived position of moving objects: Transcranial magnetic stimulation of area MT+ reduces the flash-lag effect", "author": ["F.-F. Li", "P. Perona"], "venue": "IEEE CVPR, vol. 2, 2005.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2005}, {"title": "Experiments on the dcase challenge 2016: Acoustic scene classification and sound event detection in real life recording", "author": ["B. Elizalde", "A. Kumar", "A. Shah", "R. Badlani", "E. Vincent", "B. Raj", "I. Lane"], "venue": "DCASE2016 Workshop on Detection and Classification of Acoustic Scenes and Events, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Active learning for interactive multimedia retrieval", "author": ["T.S. Huang", "C.K. Dagli", "S. Rajaram", "E.Y. Chang", "M.I. Mandel", "G.E. Poliner", "D.P. Ellis"], "venue": "Proceedings of the IEEE, vol. 96, no. 4, pp. 648\u2013667, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Audio concept ranking for video event detection on user-generated content", "author": ["B. Elizalde", "M. Ravanelli", "G. Friedland"], "venue": "Multimedia (SLAM 2013).", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Unlabeled data: Now it helps, now it doesn\u2019t", "author": ["A. Singh", "R. Nowak", "X. Zhu"], "venue": "Advances in neural information processing systems, 2009, pp. 1513\u20131520.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}], "referenceMentions": [{"referenceID": 0, "context": "The dominant application is multimedia video content analysis, where audio is combined with images and text [1], [2] to index, search and retrieve videos.", "startOffset": 108, "endOffset": 111}, {"referenceID": 1, "context": "The dominant application is multimedia video content analysis, where audio is combined with images and text [1], [2] to index, search and retrieve videos.", "startOffset": 113, "endOffset": 116}, {"referenceID": 2, "context": "Another task is human-robot interaction [3], [4], where sounds complement speech as non-verbal communication.", "startOffset": 40, "endOffset": 43}, {"referenceID": 3, "context": "Another task is human-robot interaction [3], [4], where sounds complement speech as non-verbal communication.", "startOffset": 45, "endOffset": 48}, {"referenceID": 4, "context": "Recently, a growing application is in smart cities [5], where sounds are used to detect sources of noise pollution.", "startOffset": 51, "endOffset": 54}, {"referenceID": 5, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 127, "endOffset": 130}, {"referenceID": 4, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 132, "endOffset": 135}, {"referenceID": 6, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 7, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 8, "context": "The related work on AED has mainly focused on using available datasets to train machine learning models in a supervised manner [6], [5], [7], [8], [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 7, "context": "data set, ESC-50 [8], contains only 40 samples per class.", "startOffset": 17, "endOffset": 20}, {"referenceID": 9, "context": "This approach has been explored for audio events in two papers [10], [11].", "startOffset": 63, "endOffset": 67}, {"referenceID": 10, "context": "This approach has been explored for audio events in two papers [10], [11].", "startOffset": 69, "endOffset": 73}, {"referenceID": 9, "context": "Particularly in [10], the authors collected 17,000 labeled audio-only recordings from FindSounds.", "startOffset": 16, "endOffset": 20}, {"referenceID": 4, "context": "The UrbanSound8K (US8K) dataset [5] has 10 classes: air", "startOffset": 32, "endOffset": 35}, {"referenceID": 11, "context": "The Mel Frequency Cepstral Coefficients (MFCCs) have been widely used in audio event detection [12], [5], [8].", "startOffset": 95, "endOffset": 99}, {"referenceID": 4, "context": "The Mel Frequency Cepstral Coefficients (MFCCs) have been widely used in audio event detection [12], [5], [8].", "startOffset": 101, "endOffset": 104}, {"referenceID": 7, "context": "The Mel Frequency Cepstral Coefficients (MFCCs) have been widely used in audio event detection [12], [5], [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 12, "context": "The method we followed to compute BoAWs features is broadly illustrated in Figure 2 and detailed in these papers [13], [14].", "startOffset": 113, "endOffset": 117}, {"referenceID": 13, "context": "The method we followed to compute BoAWs features is broadly illustrated in Figure 2 and detailed in these papers [13], [14].", "startOffset": 119, "endOffset": 123}, {"referenceID": 5, "context": "2) Training Detectors: SVM: One round of experiments was performed with Support Vector Machines (SVMs) because SVMs have been widely explored for sound events [6].", "startOffset": 159, "endOffset": 162}, {"referenceID": 9, "context": "Score-based Under this approach, the output of the detector is a probability score that can be interpreted as a confidence value and has been used for self-training in the paper [10].", "startOffset": 178, "endOffset": 182}, {"referenceID": 14, "context": "Clarity Index Clarity Index (CI), based on the paper [15], aims to determine those segments that are the most confusing for the detector.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": "These examples does not necessarily degrade the quality of the detector as shown in [16].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "Classifier bias Semi-supervised approaches have limitations related to what extent they can help, as discussed in [17].", "startOffset": 114, "endOffset": 118}], "year": 2017, "abstractText": "Audio Event Detection (AED) aims to recognize sounds within audio and video recordings. AED employs machine learning algorithms commonly trained and tested on annotated datasets. However, available datasets are limited in number of samples and hence it is difficult to model acoustic diversity. Therefore, we propose combining labeled audio from a dataset and unlabeled audio from the web to improve the sound models. The audio event detectors are trained on the labeled audio and ran on the unlabeled audio downloaded from YouTube. Whenever the detectors recognized any of the known sounds with high confidence, the unlabeled audio was use to re-train the detectors. The performance of the re-trained detectors is compared to the one from the original detectors using the annotated test set. Results showed an improvement of the AED, and uncovered challenges of using web audio from videos.", "creator": "LaTeX with hyperref package"}}}