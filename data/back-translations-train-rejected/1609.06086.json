{"id": "1609.06086", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Modelling Stock-market Investors as Reinforcement Learning Agents [Correction]", "abstract": "Decision making in uncertain and risky environments is a prominent area of research. Standard economic theories fail to fully explain human behaviour, while a potentially promising alternative may lie in the direction of Reinforcement Learning (RL) theory. We analyse data for 46 players extracted from a financial market online game and test whether Reinforcement Learning (Q-Learning) could capture these players behaviour using a risk measure based on financial modeling. Moreover we test an earlier hypothesis that players are \"na\\\"ive\" (short-sighted). Our results indicate that a simple Reinforcement Learning model which considers only the selling component of the task captures the decision-making process for a subset of players but this is not sufficient to draw any conclusion on the population. We also find that there is not a significant improvement of fitting of the players when using a full RL model against a myopic version, where only immediate reward is valued by the players. This indicates that players, if using a Reinforcement Learning approach, do so na\\\"ively", "histories": [["v1", "Tue, 20 Sep 2016 10:36:01 GMT  (260kb,D)", "http://arxiv.org/abs/1609.06086v1", "8 pages (including bibliography and appendix), 5 figures (2 in main body, 3 in appendix). IEEE EAIS 2015 Conference paper erratum"]], "COMMENTS": "8 pages (including bibliography and appendix), 5 figures (2 in main body, 3 in appendix). IEEE EAIS 2015 Conference paper erratum", "reviews": [], "SUBJECTS": "cs.CE cs.LG", "authors": ["alvin pastore", "umberto esposito", "eleni vasilaki"], "accepted": false, "id": "1609.06086"}, "pdf": {"name": "1609.06086.pdf", "metadata": {"source": "CRF", "title": "Modelling Stock-market Investors as Reinforcement Learning Agents [Correction]", "authors": ["Alvin Pastore", "Umberto Esposito", "Eleni Vasilaki"], "emails": ["apastore1@sheffield.ac.uk", "acp12ue@sheffield.ac.uk", "e.vasilaki@sheffield.ac.uk"], "sections": [{"heading": null, "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "A. Dataset", "text": "The data set was extracted from the publicly available online trading simulation game VirtualTrader1, managed by IEX Media Group BV in the Netherlands. Players can subscribe to the game for free and play with an allocated virtual cash budget of \u00a3100k. Players will then select the shares they prefer from the FTSE100 stock index pool (107 shares at the time of data collection) and create their own portfolio. These competitors will be ranked according to the return on their investment, which is made up of \"holdings\" and \"cash,\" the former representing the shares of a player, while the latter represents the amount of uninvested money (i.e. from shares sold or never invested).The simulation follows real-world data evolution, such as price fluctuations and price splits. The delay is usually in the order of 10-15 minutes and the player can access a visual representation of the stock time series. All transactions are stored for each player."}, {"heading": "B. Reinforcement Learning Setup", "text": "The learning rule of this model is: \u2206 Q (st, at) = \u03b1 [rt + 1 + \u03b3maxa Q (st + 1, a) \u2212 Q (st, at) \u03b2\u03b21), where Q (st, at) represents the value of the assets, while in the states for a while, at a certain time, the value of the assets is measured. [0, 2] is the step size parameter and controls the rate of learning. [0, 1] is the discount factor and represents how far-sighted the model is, it encodes how much a future reward is worth at a given time. [0, 2] when only immediate rewards are taken into account by the player. To test this framework, the task is mapped as follows. There are two states (win, loss) which are calculated according to the player's gain (details in equations 6 and 7). These two states reflect the density rooted in the values of Prospect Theory."}, {"heading": "C. Model Testing Routine", "text": "The Maximum Likelihood Estimate was used as a measure of model fitness, following Daw's comprehensive analysis of the methodology [76]. MLE is the appropriate method to assess the performance of the model because it evaluates which set of model parameters is more likely to generate the data using a probabilistic approach. Data probability is a powerful method because it takes into account the presence of noise in decisions, and does so by using probability estimates for potential action possibilities. In the face of a model M | D, M), the probability function is defined as P (D | M, \u03b8M), where D is the dataset (the list of decisions and associated rewards). Application of the Bayes rule: P (M, M). \u00b7 P (D, empirical M) \u00b7 P (empirical M) \u00b7 P (empirical M) is the left side of the proportionality."}, {"heading": "III. RESULTS", "text": "The results for testing the hypothesis that RL is a component of decision making are shown in Fig. 1. The best set of parameters was found according to MLE by gradient descend search. The best model MLE was compared to the random model MLE using the Likelihood Ratio Test [77]. The random model MLE is easy to estimate as: P (D | Mrand) = log Nt. 13 = Nt. Log 13 (10), where Nt is the number of transactions for each player in the data sets. As shown in Fig. 1 (a) and (b) 15% of the players in our data sets is better adjusted by a short-sighted RL model than by a random model. In Fig. 1 (c) and (d) we report an improvement in the adjustment for some players using a complete RL model with the short-sighted (nested) version of the model. This improvement is not reflected in the comparison of the complete model, as in Fig. 1 (c) and (d) we report an improvement in the adjustment for some players using a complete RL model with the short-sighted (nested) version of the model."}, {"heading": "IV. CONCLUSION", "text": "We examined a publicly available dataset consisting of trading transactions conducted by players in an investment game. We relied on the assumption that risk can capture the internal modeling that players perform in accomplishing this task, which proved to be accurate and statistically significant for a subset of players, 31 out of 46, and specifically for the 7 players best suited by an RL model. This could mean that the remaining players could use other types of discrediting techniques based on different metrics (or a combination of them), or they could use no technical analysis, but basic analyses (e.g. using financial statements and reports). In this paper, we examined a model that would fit two versions of a Reinforcement Learning Framework using Q-Learning as an updating rule and Soft-Max as an action selection strategy based on a discredited action margin according to the risk component of Learning Rules, and that different model rules are possible for different learning combinations."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank their colleagues from the interdisciplinary research group Sheffield Neuroeconomics for the insightful discussion."}], "references": [{"title": "Developments in Non-Expected Utility Theory: The Hunt for a Descriptive Theory of Choice under Risk.", "author": ["C. Stramer"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2000}, {"title": "and T", "author": ["S. Frederick", "G. Loewenstein"], "venue": "O\u2019Donoghue, \u201cTime Discounting and time preference: a critical review.\u201d Journal of Economic Literature, vol. XL, pp. 351\u2013401", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2002}, {"title": "Anomalies: The Endowment Effect", "author": ["D. Kahneman", "J.L. Knetsch", "R.H. Thaler"], "venue": "Loss Aversion, and Status Quo Bias.\u201d Journal of Economic Perspectives, vol. 5, no. 1, pp. 193\u2013206", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "and V", "author": ["U. Hoffrage", "A. Weber", "R. Hertwig"], "venue": "M. Chase, \u201cHow to Keep Children Safe in Traffic: Find the Daredevils Early.\u201d Journal of Experimental Psychology: Applied, vol. 9, no. 4, pp. 249\u2013260", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Pleskac, \u201cDecision making and learning while taking sequential risks.", "author": ["J. T"], "venue": "Journal of Experimental Psychology: Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2008}, {"title": "and C", "author": ["T.S. Wallsten", "T.J. Pleskac"], "venue": "W. Lejuez, \u201cModeling Behavior in a Clinically Diagnostic Sequential Risk-Taking Task.\u201d Psychological Review,, vol. 112, no. 4, pp. 862\u2013880", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "and J", "author": ["K.H. Britten", "W.T. Newsome", "M.N. Shadlen", "S. Celebrini"], "venue": "a. Movshon, \u201cA relationship between behavioral choice and the visual responses of neurons in macaque MT.\u201d Visual neuroscience, vol. 13, no. 1, pp. 87\u2013100", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "and J", "author": ["K.H. Britten", "M.N. Shadlen", "W.T. Newsome"], "venue": "a. Movshon, \u201cThe analysis of visual motion: a comparison of neuronal and psychophysical performance.\u201d The Journal of neuroscience: the official journal of the Society for Neuroscience, vol. 12, no. 12, pp. 4745\u20134765", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1992}, {"title": "Neural computations that underlie decisions about sensory stimuli.", "author": ["J.I. Gold", "M.N. Shadlen"], "venue": "Trends in Cognitive Sciences,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2001}, {"title": "and J", "author": ["M.N. Shadlen", "K.H. Britten", "W.T. Newsome"], "venue": "a. Movshon, \u201cA computational analysis of the relationship between neuronal and behavioral responses to visual motion.\u201d J Neurosci, vol. 16, no. 4, pp. 1486\u20131510", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Motion perception: seeing and deciding.", "author": ["M.N. Shadlen", "W.T. Newsome"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Neuroeconomics: cross-currents in research on decision-making.", "author": ["A.G. Sanfey", "G. Loewenstein", "S.M. McClure", "J.D. Cohen"], "venue": "Trends in cognitive sciences,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2006}, {"title": "Neuroeconomics: the consilience of brain and decision.", "author": ["P.W. Glimcher", "A. Rustichini"], "venue": "Science (New York, N.Y.),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2004}, {"title": "and D", "author": ["D.J. Barraclough", "M.L. Conroy"], "venue": "Lee, \u201cPrefrontal cortex and decision making in a mixed-strategy game.\u201d Nature neuroscience, vol. 7, no. 4, pp. 404\u2013410", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2004}, {"title": "and X", "author": ["A. Soltani", "D. Lee"], "venue": "J. Wang, \u201cNeural mechanism for stochastic behaviour during a competitive game.\u201d Neural Networks, vol. 19, no. 8, pp. 1075\u20131090", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2006}, {"title": "Reinforcement Learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "A Bradford Book, Ed. MIT Press, Cambridge, MA", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Reinforcement learning: Computational theory and biological mechanisms.", "author": ["K. Doya"], "venue": "HFSP Journal,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2007}, {"title": "TD-Gammon", "author": ["G. Tesauro"], "venue": "a Self-Teaching Backgammon Program, Achieves Master-Level Play.\u201d Neural Computation, vol. 6, no. 2, pp. 215\u2013219", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1994}, {"title": "An Application of Reinforcement Learning to Aerobatic Helicopter Flight.", "author": ["P A. Abbeel", "A Coates", "Q Morgan", "Ng"], "venue": "Advances in Neural Information Processing Systems 19: Proceedings of the 2006 Conference,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2006}, {"title": "Neural reinforcement learning controllers for a real robot application.", "author": ["R. Hafner", "M. Riedmiller"], "venue": "Proceedings - IEEE International Conference on Robotics and Automation,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "An Application of Reinforcement Learning to Dialogue Strategy Selection in a Spoken Dialogue System for Email.", "author": ["M. a. Walker"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2000}, {"title": "A", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "a. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, \u201cHuman-level control through deep reinforcement learning.\u201d Nature, vol. 518, no. 7540, pp. 529\u2013533", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Decision theory, reinforcement learning, and the brain.", "author": ["P. Dayan", "N.D. Daw"], "venue": "Cognitive, affective & behavioral neuroscience,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2008}, {"title": "A neural substrate of prediction and reward.", "author": ["W. Schultz", "P. Dayan", "P.R. Montague"], "venue": "Science (New York, N.Y.),", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1997}, {"title": "Complementary roles of basal ganglia and cerebellum in learning and motor control.", "author": ["K. Doya"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "The computational neurobiology of learning and reward.", "author": ["N.D. Daw", "K. Doya"], "venue": "Current Opinion in Neurobiology,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2006}, {"title": "Learning from delayed rewards.", "author": ["C.J.C.H. Watkins"], "venue": null, "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1989}, {"title": "A menu of designs for reinforcement learning over time", "author": ["P. Werbos"], "venue": "Neural Networks for Control .\u201d Neural Networks for Control, MIT Press, Cambridge, Massachusetts, pp. 67\u201395", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1990}, {"title": "and H", "author": ["O. Hikosaka", "K. Nakamura"], "venue": "Nakahara, \u201cBasal ganglia orient eyes to reward.\u201d Journal of neurophysiology, vol. 95, no. 2, pp. 567\u2013 584", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2006}, {"title": "W", "author": ["A. Schultz"], "venue": "Romo, R, Ljungberg, T, Mirenowicz, J, Hollerman, JR, and Dickson, \u201cReward-related signals carried by dopamine neurons.\u201d in Models of Information Processing in the Basal Ganglia, M. Cambridge, Ed.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1995}, {"title": "Learning of sequential movements by neural network model with dopamine-like reinforcement signal.", "author": ["R.E. Suri", "W. Schultz"], "venue": "Experimental Brain Research,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1998}, {"title": "a", "author": ["P. Waelti"], "venue": "Dickinson, and W. Schultz, \u201cDopamine responses comply with basic assumptions of formal learning theory.\u201d Nature, vol. 412, no. 6842, pp. 43\u201348", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2001}, {"title": "and M", "author": ["T. Satoh", "S. Nakai", "T. Sato"], "venue": "Kimura, \u201cCorrelated coding of motivation and outcome of decision by dopamine neurons.\u201d The Journal of neuroscience : the official journal of the Society for Neuroscience, vol. 23, no. 30, pp. 9913\u20139923", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2003}, {"title": "and O", "author": ["H. Nakahara", "H. Itoh", "R. Kawagoe", "Y. Takikawa"], "venue": "Hikosaka, \u201cDopamine Neurons Can Represent Context-Dependent Prediction Error.\u201d Neuron, vol. 41, no. 2, pp. 269\u2013280", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}, {"title": "and H", "author": ["G. Morris", "A. Nevet", "D. Arkadir", "E. Vaadia"], "venue": "Bergman, \u201cMidbrain dopamine neurons encode decisions for future action.\u201d Nature neuroscience, vol. 9, no. 8, pp. 1057\u20131063", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2006}, {"title": "Adaptive critics and the basal ganglia.\u201d in Models of Information Processing in the Basal Ganglia, M", "author": ["A. Barto"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 1995}, {"title": "and T", "author": ["P.R. Montague", "P. Dayan"], "venue": "J. Sejnowski, \u201cA framework for mesencephalic dopamine systems based on predictive Hebbian learning.\u201d The Journal of neuroscience : the official journal of the Society for Neuroscience, vol. 16, no. 5, pp. 1936\u20131947", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1996}, {"title": "and M", "author": ["K. Samejima", "Y. Ueda", "K. Doya"], "venue": "Kimura, \u201cRepresentation of action-specific reward values in the striatum.\u201d Science (New York, N.Y.), vol. 310, no. 5752, pp. 1337\u20131340", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "and O", "author": ["R. Kawagoe", "Y. Takikawa"], "venue": "Hikosaka, \u201cExpectation of reward modulates cognitive signals in the basal ganglia.\u201d Nature neuroscience, vol. 1, no. 5, pp. 411\u2013416", "citeRegEx": "44", "shortCiteRegEx": null, "year": 1998}, {"title": "Getting formal with dopamine and reward.", "author": ["W. Schultz"], "venue": "Neuron, vol. 36,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2002}, {"title": "and R", "author": ["J.J. Day", "M.F. Roitman", "R.M. Wightman"], "venue": "M. Carelli, \u201cAssociative learning mediates dynamic shifts in dopamine signaling in the nucleus accumbens.\u201d Nature neuroscience, vol. 10, no. 8, pp. 1020\u20131028", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2007}, {"title": "and E", "author": ["S.E. Hyman", "R.C. Malenka"], "venue": "J. Nestler, \u201cNeural mechanisms of addiction: the role of reward-related learning and memory.\u201d Annual review of neuroscience, vol. 29, pp. 565\u2013598", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2006}, {"title": "and E", "author": ["D. Joel", "Y. Niv"], "venue": "Ruppin, \u201cActor-critic models of the basal ganglia: new anatomical and computational perspectives.\u201d Neural Networks, vol. 15, no. 4-6, pp. 535\u2013547", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2002}, {"title": "and S", "author": ["J.R. Wickens", "J.C. Horvitz", "R.M. Costa"], "venue": "Killcross, \u201cDopaminergic mechanisms in actions and habits.\u201d The Journal of neuroscience : the official journal of the Society for Neuroscience, vol. 27, no. 31, pp. 8181\u20138183", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2007}, {"title": "Noise characteristics and prior expectations in human visual speed perception.", "author": ["A. a. Stocker", "E.P. Simoncelli"], "venue": "Nature neuroscience,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2006}, {"title": "Decision theory: what \u201dshould\u201d the nervous system do?\u201d Science (New York", "author": ["K. K\u00f6rding"], "venue": "N.Y.), vol. 318, no. 5850, pp. 606\u2013610", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2007}, {"title": "J", "author": ["N.D. Daw"], "venue": "P. O\u2019Doherty, P. Dayan, B. Seymour, and R. J. Dolan, \u201cCortical substrates for exploratory decisions in humans.\u201d Nature, vol. 441, no. 7095, pp. 876\u2013879", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Stress, genotype and norepinephrine in the prediction of mouse behavior using reinforcement learning.", "author": ["G. Luksys", "W. Gerstner", "C. Sandi"], "venue": "Nature neuroscience,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 2009}, {"title": "Sell in May and Go Away ? Learning and Risk Taking in Nonmonotonic Decision Problems.", "author": ["R. Frey", "R. Hertwig"], "venue": "Journal of Experimental Psychology,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "Heuristics and Biases in Retirement Savings Behavior.", "author": ["S. Benartzi", "R.H. Thaler"], "venue": "Journal of Economic Perspectives,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2007}, {"title": "Reinforcement learning and savings behavior.", "author": ["J. Choi", "D. Laibson"], "venue": "The Journal of Finance,", "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2009}, {"title": "Industry Investment Experience and Stock Selection.", "author": ["X. Huang"], "venue": "Available at SSRN 1786271,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2012}, {"title": "Are Investors Reluctant to Realize Their Losses ?\u201d vol", "author": ["T. Odean"], "venue": "LIII, no. 5, pp. 1775\u20131798", "citeRegEx": "59", "shortCiteRegEx": null, "year": 1998}, {"title": "and J", "author": ["Y. Chen", "S. Mabu", "K. Hirasawa"], "venue": "Hu, \u201cTrading rules on stock markets using genetic network programming with sarsa learning.\u201d Proceedings of the 9th annual conference on Genetic and evolutionary computation GECCO 07, vol. 12, p. 1503", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2007}, {"title": "Stock price prediction using reinforcement learning.", "author": ["J. Lee"], "venue": "Industrial Electronics. Proceedings. ISIE", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2001}, {"title": "Saffell, \u201cLearning to trade via direct reinforcement.", "author": ["M.J. Moody"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2001}, {"title": "Building an artificial stock market populated by reinforcementlearning agents.", "author": ["A.V. Rutkauskas", "T. Ramanauskas"], "venue": "Journal of Business Economics and Management,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2009}, {"title": "The Behavior of Individual Investors.", "author": ["B.M. Barber", "T. Odean"], "venue": "Handbook of the Economics of Finance,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2013}, {"title": "and T", "author": ["B.M. Barber", "Y.T. Lee", "Y.J. Liu"], "venue": "Odean, \u201cIs the aggregate investor reluctant to realise losses? Evidence from Taiwan.\u201d European Financial Management, vol. 13, no. 3, pp. 423\u2013447", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2007}, {"title": "R", "author": ["P. Brown", "N. Chappel"], "venue": "Da Silva Rosa, and T. Walter, \u201cThe Reach of the Disposition Effect: Large Sample Evidence Across Investor Classes.\u201d International Review of Finance, vol. 6, no. 1-2, p. 43", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2006}, {"title": "The disposition effect and underreaction to news.", "author": ["A. Frazzini"], "venue": "Journal of Finance,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 2006}, {"title": "What Makes Investors Trade?\u201d The Journal Of Finance", "author": ["M. Grinblatt", "M. Keloharju"], "venue": "vol. 56 (2), no. 2, pp. 549\u2013578", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2001}, {"title": "Psychological Factors and Stock Option Exercise.", "author": ["C. Heath", "M. Lang"], "venue": "Quarterly Journal of Economics vol. 114,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 1999}, {"title": "Do Investors Trade Too Much?\u201d American Economic Review", "author": ["T. Odean"], "venue": "vol. 89 (5), pp. 1279-1298", "citeRegEx": "71", "shortCiteRegEx": null, "year": 1998}, {"title": "Patterns of behavior of professionally managed and independent investors", "author": ["Z. Shapira", "I. Venezia"], "venue": "Journal of Banking and Finance, vol. 25, no. 8, pp. 1573\u20131587", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2001}, {"title": "Prospect Theory: An Analysis of Decision under Risk.", "author": ["D. Kahneman", "A. Tversky"], "venue": "Econometrica: Journal of the Econometric Society,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1979}, {"title": "Capital Asset Prices: A Theory of Market Equilibrium Under Conditions of Risk,", "author": ["W. Sharpe"], "venue": "The Journal of Finance, vol. XIX,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 1964}, {"title": "Trial-by-trial data analysis using computational models,\u201d in Decision Making, Affect, and Learning: Attention and Performance XXIII", "author": ["N.D. Daw"], "venue": null, "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2011}, {"title": "Phylogeny Estimation and Hypothesis Testing Using Maximum Likelihood,", "author": ["J.P. Huelsenbeck", "K. a. Crandall"], "venue": "Annual Review of Ecology and Systematics,", "citeRegEx": "77", "shortCiteRegEx": "77", "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "Expected Utility model [1]) but have been proven to be inadequate [2]\u2013[5], giving rise to new research areas like behavioural and experimental economics.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "Expected Utility model [1]) but have been proven to be inadequate [2]\u2013[5], giving rise to new research areas like behavioural and experimental economics.", "startOffset": 70, "endOffset": 73}, {"referenceID": 3, "context": "Sequential decision problems have been used to evaluate people\u2019s risk attitude, in order to predict actual risk proneness in real life scenarios [6]\u2013[8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 5, "context": "Sequential decision problems have been used to evaluate people\u2019s risk attitude, in order to predict actual risk proneness in real life scenarios [6]\u2013[8].", "startOffset": 149, "endOffset": 152}, {"referenceID": 6, "context": "While economics and psychology are focused on the high-level manifestations and implications of decision-making, neuroscience aims at understanding the biological machinery and the neural processes behind human (or animal) behaviour [9]\u2013[15].", "startOffset": 233, "endOffset": 236}, {"referenceID": 10, "context": "While economics and psychology are focused on the high-level manifestations and implications of decision-making, neuroscience aims at understanding the biological machinery and the neural processes behind human (or animal) behaviour [9]\u2013[15].", "startOffset": 237, "endOffset": 241}, {"referenceID": 11, "context": "Recently these fields of research have started to collaborate, contributing to the rise of an emerging multi-disciplinary field called neuroeconomics [16]\u2013[20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 14, "context": "Recently these fields of research have started to collaborate, contributing to the rise of an emerging multi-disciplinary field called neuroeconomics [16]\u2013[20].", "startOffset": 155, "endOffset": 159}, {"referenceID": 15, "context": "RL is a theoretical framework [21], extensively used in neuroeconomics literature for addressing a wide array of problems involving learning in partially observable environments [22]\u2013[27].", "startOffset": 30, "endOffset": 34}, {"referenceID": 16, "context": "RL is a theoretical framework [21], extensively used in neuroeconomics literature for addressing a wide array of problems involving learning in partially observable environments [22]\u2013[27].", "startOffset": 178, "endOffset": 182}, {"referenceID": 21, "context": "RL is a theoretical framework [21], extensively used in neuroeconomics literature for addressing a wide array of problems involving learning in partially observable environments [22]\u2013[27].", "startOffset": 183, "endOffset": 187}, {"referenceID": 22, "context": "The ability of this framework to model and therefore understand behavioural data and its underlying neural implications, is of pivotal importance in decision making [28].", "startOffset": 165, "endOffset": 169}, {"referenceID": 16, "context": "RL can accurately capture human and animal learning patterns and has been proven effective at describing the functioning of some areas of the human brain, like the basal ganglia, and the functions of neurotransmitters such as dopamine [22], [29]\u2013[31].", "startOffset": 235, "endOffset": 239}, {"referenceID": 23, "context": "RL can accurately capture human and animal learning patterns and has been proven effective at describing the functioning of some areas of the human brain, like the basal ganglia, and the functions of neurotransmitters such as dopamine [22], [29]\u2013[31].", "startOffset": 241, "endOffset": 245}, {"referenceID": 25, "context": "RL can accurately capture human and animal learning patterns and has been proven effective at describing the functioning of some areas of the human brain, like the basal ganglia, and the functions of neurotransmitters such as dopamine [22], [29]\u2013[31].", "startOffset": 246, "endOffset": 250}, {"referenceID": 15, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 133, "endOffset": 137}, {"referenceID": 26, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 139, "endOffset": 143}, {"referenceID": 28, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 145, "endOffset": 149}, {"referenceID": 29, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 199, "endOffset": 203}, {"referenceID": 34, "context": "One of the most remarkable similarities between biological functioning and RL models is the one about Temporal Difference (TD) error [21], [32]\u2013 [34] and the activation of mid-brain dopamine neurons [35]\u2013 [40].", "startOffset": 205, "endOffset": 209}, {"referenceID": 23, "context": "These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt.", "startOffset": 123, "endOffset": 127}, {"referenceID": 28, "context": "These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt.", "startOffset": 129, "endOffset": 133}, {"referenceID": 35, "context": "These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt.", "startOffset": 135, "endOffset": 139}, {"referenceID": 43, "context": "These findings supported the notion that TD Learning is implemented in the brain with dopaminergic neurons in the striatum [29], [34], [41]\u2013[50], [78], making it a reasonable first choice for a modelling attempt.", "startOffset": 140, "endOffset": 144}, {"referenceID": 44, "context": "Humans and animals are very advanced signal detectors whose behaviour is susceptible to changes in the rewards resulting from their choices [51], [52].", "startOffset": 140, "endOffset": 144}, {"referenceID": 45, "context": "Humans and animals are very advanced signal detectors whose behaviour is susceptible to changes in the rewards resulting from their choices [51], [52].", "startOffset": 146, "endOffset": 150}, {"referenceID": 14, "context": "Both neuroscience and psychology have extensively employed tasks in which the exploration-exploitation tradeoff was of crucial importance [20], [53]\u2013[56].", "startOffset": 138, "endOffset": 142}, {"referenceID": 46, "context": "Both neuroscience and psychology have extensively employed tasks in which the exploration-exploitation tradeoff was of crucial importance [20], [53]\u2013[56].", "startOffset": 144, "endOffset": 148}, {"referenceID": 49, "context": "Both neuroscience and psychology have extensively employed tasks in which the exploration-exploitation tradeoff was of crucial importance [20], [53]\u2013[56].", "startOffset": 149, "endOffset": 153}, {"referenceID": 49, "context": "After purchasing a stock, investors are faced with the decisions on when to sell it (Market timing problem [56]).", "startOffset": 107, "endOffset": 111}, {"referenceID": 50, "context": "[57], who studied individual investors decisions on 401(k) savings plans.", "startOffset": 0, "endOffset": 4}, {"referenceID": 52, "context": "This kind of behaviour follows a \u201cna\u0131\u0308ve reinforcement learning\u201d and is in contrast with the disposition effect [59], [65](the unwillingness of investors to sell \u201closing\u201d investments).", "startOffset": 112, "endOffset": 116}, {"referenceID": 57, "context": "This kind of behaviour follows a \u201cna\u0131\u0308ve reinforcement learning\u201d and is in contrast with the disposition effect [59], [65](the unwillingness of investors to sell \u201closing\u201d investments).", "startOffset": 118, "endOffset": 122}, {"referenceID": 51, "context": "investigated how personal experience in investments affects future decisions about the selection of stocks [58].", "startOffset": 107, "endOffset": 111}, {"referenceID": 53, "context": "RL has also been used, with promising results, to develop Stock Market Trading Systems [60]\u2013[63] and to build Agent Based Stock Market Simulations [64].", "startOffset": 87, "endOffset": 91}, {"referenceID": 55, "context": "RL has also been used, with promising results, to develop Stock Market Trading Systems [60]\u2013[63] and to build Agent Based Stock Market Simulations [64].", "startOffset": 92, "endOffset": 96}, {"referenceID": 56, "context": "RL has also been used, with promising results, to develop Stock Market Trading Systems [60]\u2013[63] and to build Agent Based Stock Market Simulations [64].", "startOffset": 147, "endOffset": 151}, {"referenceID": 52, "context": "This assumption follows a widely researched behaviour referred to as \u201cdisposition effect\u201d in literature [59], [65], the tendency of individual investors to sell stocks which increased in value since when they were purchased, while holding onto the stocks which lost value.", "startOffset": 104, "endOffset": 108}, {"referenceID": 57, "context": "This assumption follows a widely researched behaviour referred to as \u201cdisposition effect\u201d in literature [59], [65], the tendency of individual investors to sell stocks which increased in value since when they were purchased, while holding onto the stocks which lost value.", "startOffset": 110, "endOffset": 114}, {"referenceID": 58, "context": "and corporations [66]\u2013[72].", "startOffset": 17, "endOffset": 21}, {"referenceID": 64, "context": "and corporations [66]\u2013[72].", "startOffset": 22, "endOffset": 26}, {"referenceID": 15, "context": "We adopted a widely used off-policy RL framework called Q-learning [21].", "startOffset": 67, "endOffset": 71}, {"referenceID": 0, "context": "\u03b1 \u2208 [0, 2] is the step-size parameter and controls the rate of learning.", "startOffset": 4, "endOffset": 10}, {"referenceID": 65, "context": "These two states reflect the dichotomy rooted in the Prospect Theory\u2019s value function gain/loss spectrum [73].", "startOffset": 105, "endOffset": 109}, {"referenceID": 66, "context": "The acronym stands for Capital Asset Pricing Model, a model developed by Sharpe[74] used to explain the relationship between the expected return of a security and its risk .", "startOffset": 79, "endOffset": 83}, {"referenceID": 65, "context": "This choice is in line with prospect theory value function which is concave for gains and convex for losses [73].", "startOffset": 108, "endOffset": 112}, {"referenceID": 67, "context": "Maximum Likelihood Estimate has been used as a measure of the model fitness, following Daw\u2019s comprehensive analysis of methodology [76].", "startOffset": 131, "endOffset": 135}, {"referenceID": 68, "context": "To compare the selected model with a random model and for statistical significance we adopted the Likelihood Ratio Test [77].", "startOffset": 120, "endOffset": 124}, {"referenceID": 50, "context": "This setup represent the na\u0131\u0308ve learning procedure that could explain investors\u2019 behaviour showed in literature[57].", "startOffset": 111, "endOffset": 115}, {"referenceID": 68, "context": "The best model MLE has been compared to the random model MLE using the Likelihood Ratio Test [77].", "startOffset": 93, "endOffset": 97}, {"referenceID": 50, "context": "[57] and Huang et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 51, "context": "[58].", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "Previous literature pointed in the direction of investors being na\u0131\u0308ve (short-sighted) [57], [58] and these results, albeit for a subset of the dataset, confirm this indication.", "startOffset": 87, "endOffset": 91}, {"referenceID": 51, "context": "Previous literature pointed in the direction of investors being na\u0131\u0308ve (short-sighted) [57], [58] and these results, albeit for a subset of the dataset, confirm this indication.", "startOffset": 93, "endOffset": 97}], "year": 2016, "abstractText": "Decision making in uncertain and risky environments is a prominent area of research. Standard economic theories fail to fully explain human behaviour, while a potentially promising alternative may lie in the direction of Reinforcement Learning (RL) theory. We analyse data for 46 players extracted from a financial market online game and test whether Reinforcement Learning (Q-Learning) could capture these players behaviour using a risk measure based on financial modeling. Moreover we test an earlier hypothesis that players are \u201cna\u0131\u0308ve\u201d (short-sighted). Our results indicate that a simple Reinforcement Learning model which considers only the selling component of the task captures the decision-making process for a subset of players but this is not sufficient to draw any conclusion on the population. We also find that there is not a significant improvement of fitting of the players when using a full RL model against a myopic version, where only immediate reward is valued by the players. This indicates that players, if using a Reinforcement Learning approach, do so na\u0131\u0308vely.", "creator": "LaTeX with hyperref package"}}}