{"id": "1609.06119", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "FastBDT: A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for multivariate classification", "abstract": "Stochastic gradient-boosted decision trees are widely employed for multivariate classification and regression tasks. This paper presents a speed-optimized and cache-friendly implementation for multivariate classification called FastBDT. FastBDT is one order of magnitude faster during the fitting-phase and application-phase, in comparison with popular implementations in software frameworks like TMVA, scikit-learn and XGBoost. The concepts used to optimize the execution time and performance studies are discussed in detail in this paper. The key ideas include: An equal-frequency binning on the input data, which allows replacing expensive floating-point with integer operations, while at the same time increasing the quality of the classification; a cache-friendly linear access pattern to the input data, in contrast to usual implementations, which exhibit a random access pattern. FastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used in the field of high energy physics by the Belle II experiment.", "histories": [["v1", "Tue, 20 Sep 2016 11:50:52 GMT  (5861kb,D)", "http://arxiv.org/abs/1609.06119v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["thomas keck"], "accepted": false, "id": "1609.06119"}, "pdf": {"name": "1609.06119.pdf", "metadata": {"source": "CRF", "title": "A speed-optimized and cache-friendly implementation of stochastic gradient-boosted decision trees for multivariate classification", "authors": ["Thomas Keck"], "emails": ["thomas.keck2@kit.edu"], "sections": [{"heading": null, "text": "Keywords: enhanced decision trees, multivariate classification, DC binning, cache friendly, Belle"}, {"heading": "1. Introduction", "text": "In multivariate classification, the probability of a given data point receiving a signal, characterized by a set of explanatory features ~ x = {x1,.., xd} and a class designation y (signal y = 1 and background y = \u2212 1) is calculated. In supervised machine learning, this includes a fitting phase where training points with known labels are used and an application phase in which the built-in classifier is applied to new data points with unknown labels. During the fitting phase, the internal parameters (or model) of a multivariate classifier are adjusted so that the classifier can statistically distinguish signal and background data. Model complexity plays an important role during the fitting phase and can be controlled by the hyperparameters of the model. If the model is too simple (too complex), it is inserted (excessively complex) and behaves poorly at test points with 19.01 short-term parameters following it."}, {"heading": "1.1 Decision Tree (DT)", "text": "The maximum number of consecutive cuts is a hyperparameter and is called tree depth. Cuts are determined during the fit phase using a training sample with known labels. At each node, only training points that have passed the preceding cuts are considered. For each feature at each node, a cumulative probability histogram (CPH) for signal or background is calculated. Histograms are used to determine the separation gain for a cut at each position in these histograms. The feature and intersection position (or equivalent to the trash) with the highest separation gain are used as the cut for the node. Therefore, each cut locally maximizes the separation gain between signal and background on the given training profile. Predictions of a deep DT are often dominated by statistical fluctuations in the training data points. As a result, the classifier is overloaded and poorly executed."}, {"heading": "1.2 Boosted Decision Tree (BDT)", "text": "A BDT constructs a more robust classification model by constructing flat DTs sequentially during the adjustment phase. DTs are designed to minimize the expected value of a negative binomial log likelihood loss function. To avoid overfitting, the depth of the individual DTs is severely limited. Therefore, a single DT separates signal and background only roughly and is a so-called weak learner 1. By using many weak learners, a well-regulated classifier with a high separating force is constructed. The number of trees N (or equivalent the number of increment steps) and the learning rate \u03b7 are additional1. A simple model with few parameters. Hyperparameters of this model are correlated, which means that reducing the value of \u03b7 increases the best value for N."}, {"heading": "1.3 Gradient Boosted Decision Tree (GBDT)", "text": "A GBDT uses gradient decently in each gain step to reweight the original training sample. As a result, hard-to-classify data points (which are often close to the optimal separation hyperplane) gain influence during training. A boost weight calculated for each terminal node during the adjustment phase is used as the output of each DT, rather than the signal portion. The probability of a test data point (with unknown labeling) becoming a signal is the sigmoid-transformed sum of the output boost weights of each tree. The entire algorithm is derived and discussed in detail by Friedman (2000)."}, {"heading": "1.4 Stochastic Gradient Boosted Decision Tree (SGBDT)", "text": "A SGBDT uses a randomly drawn (no substitute) partial sample instead of the complete training sample in each training step during the adjustment phase. This approach further increases robustness against overfitting by averaging the statistical fluctuations in the training sample in sum over all trees. Friedman (2002) has extensively investigated the inclusion of randomization in the method, and the fraction of the samples used in each training step is another hyperparameter known as the sub-sampling rate \u03b1."}, {"heading": "1.5 Related Work", "text": "In general, there are two approaches to increasing the execution speed of an algorithm: modify the algorithm itself or optimize its implementation.It is easy to see that the first approach has great potential, and there are several authors who examined this approach for the SGBDT algorithm: Already the original work (Friedman, 2000) on GBDTs showed that 90% to 95% of the training data can be removed from the adjustment phase without sacrificing the accuracy of the classifier. Another approach was presented in Appel et al. (2013), where a subset of training data was used to prune traits early during the adjustment phase without affecting final performance. Traditional boosting, as discussed above, treats the tree learner as a black box, but it is possible to exploit the underlying tree structure in order to achieve higher accuracy and smaller (hence faster) models (Rie Johnson, 2014).FastBase uses a complementary approach to the above-mentioned implementation order, mainly by using an implementation algorithm."}, {"heading": "2. FastBDT implementation", "text": "On modern hardware, it is difficult to predict execution time, for example, in terms of consumed CPU cycles, because there are many mechanisms built into modern CPUs to exploit parallelizable code execution and memory access patterns. Therefore, it is important to compare all performance optimizations. In this work, perf (de Melo, 2010), valgrind (Nethercote and Seward, 2007) and std:: chrono:: high _ resolution _ clock (ISO, 2012) were used to measure execution time and identify critical code sections. The most time-consuming code section in the SGBDT algorithm is calculating the cumulative probability histograms (CPH) needed to calculate the best cut on each node of the tree. The main concepts used in implementing the customization phase of FastBT are described below."}, {"heading": "2.1 Memory Access Patterns", "text": "This year it is so far that it will be able to do the mentioned rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the ru the ru the ru the ru the rf the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the ru the"}, {"heading": "2.2 Preprocessing", "text": "Continuous input characteristics are represented by floating-point numbers. However, DTs only use the order statistics of the characteristics (as opposed to, say, artificial neural networks), so the algorithm only compares the values with each other and does not use the values themselves. FastBDT therefore bins the input characteristics equally often and maps them to integers. This has several advantages: Normally, integer operations can be performed faster by the CPU; the integers can be used directly as indices for the CPHs in calculating the best cuts; and the quality of the separation is often improved because the form of the input characteristics distribution (which may include sharp spikes or heavy tails) is mapped to a uniform distribution. This pre-processing occurs only during the adjustment phase. Once all sections are determined, the inverse transformation is used to map the integers used in the sections back to the original floating-point numbers. Therefore, there is no time consuming during the application phase."}, {"heading": "2.3 Parallelism", "text": "Normally, one can distinguish between two types of parallelism: task-level parallelism (multiple processors, multiple cores per processor, multiple threads per core) and command-level parallelism (command pipelining, multiple execution units and ports, vectorization). It is possible to use task-level parallelism to reduce execution time during the build-in phase of the SGBDT algorithm. However, FastBDT is not designed to do this, as our use cases typically require the installation of many classifiers in parallel, and thus already effectively exploit task-level parallelism, or use a shared infrastructure that is all about minimizing total CPU time. Other implementations, such as XGBoost, take advantage of this kind of parallelism; the application phase is embarrassingly parallel, and task-level parallelism can be used by all implementations, even if not directly supported."}, {"heading": "3. Comparison", "text": "Fast BDT (development version 24.04.2016) 3 was compared against other SGBDT implementations normally used in high energy physics: \u2022 TMVA (Hoecker et al., 2007) (ROOT version 6.06 / 00) - The Multivariate Analysis Package of the ROOT (Brun and Rademakers, 1997) data analysis library developed by CERN (GPLv2 license); \u2022 SciKit-Learning (Pedregosa et al., 2011) - A machine written in Python (BSD license); 2. E.g. by looping over the data points in parallel with OpenMPI (OpenMP Architecture Review Board, 2015) # omp performed in XG.Boost after calling its prediction function."}, {"heading": "3.1 Fitting-Phase", "text": "In general, the duration of the adjustment phase of the algorithms is expected to be linear in the depth of the trees, the number of trees, the number of characteristics, the number of training data points and the sampling rate. As shown in Figure 5, these expectations are not always met. For each varied hyperparameter, the gradient a and the offset c have been adjusted using an ordinary linear regression. FastBDT outperforms all other implementations during the adjustment phase, including the multi-core version of XGBoost. scikit-learn is the slowest participant and also violates the expected linear runtime behavior with respect to the depth of the trees (see 5b) and the number of training data points (see 5c). It is not clear why this is the case. TMVA shows a constant runtime for the depth of the trees > 6 (see 5b); this can be explained by the standard limitation of the number of data points per node."}, {"heading": "3.2 Application-Phase", "text": "During the application phase, the runtime should be scaled linearly in the depth of the trees, the number of trees and the number of test data points. Results are shown in Figure 6. Detailed acceleration comparisons between all tested implementations are shown in Table 2. The single-core version of FastBDT outperforms all other single-core implementations during the customization phase. On average, the multi-core version of FastBDT is 3.8 times faster than the multi-core version of XGBoost. Runtime behavior in the number of trees and the number of test data points is expected to be faster. TMVA appears to be faster at small values of the sampling rate and again shows a constant runtime behavior for the depth of the trees > 6. Both can be achieved by limiting the minimum number of data points per node. FastBDT violates the expected linear scaling at the depth of the trees."}, {"heading": "3.3 Classification Quality", "text": "The quality of the classification was assessed by the range below the Receiver Operation Characteristic (ROC), which is independent of the chosen operating point (i.e. the desired efficiency or purity) and is often used to compare the separation performance of different algorithms with each other. The higher the value, the better the separation between signal and background. The main difference between the implementation results from the different control methods. FastBDT uses the DC binning to prevent excessive cutting; XGBoost uses a modified separation gain that includes the structure of the current tree; and TMVA limits the minimum number of data points per event to 5%. FastBDT outperforms the other implementations in most situations, except for extremely deep trees. In this region, the overpass effect worsens the performance of the classifier, whereas XGBoost seems to have a superior control method to prevent overpass in this situation."}, {"heading": "4. Advanced Features", "text": "FastBDT offers advanced features, three of which are briefly described below."}, {"heading": "4.1 Support for Negative Weights", "text": "The boosting algorithm assigns a weight to each data point in the training dataset. FastBDT supports an additional weight per data point provided by the user. These individual weights are processed separately from the boosting weights. FastBDT in particular allows negative single weights, which are often used in data-driven techniques to statistically separate signal and background using a discriminatory variable (Martschei et al., 2012). Other frameworks such as TMVA, SKLearn and XGBoost also support negative weights."}, {"heading": "4.2 Support for Missing Values", "text": "There are at least two different types of missing values in a dataset. First, missing values that may contain useful information about the target, such as particle classification in the HEP, a feature provided by a detector may be missing because the detector has not been activated by the particle. Second, missing values that should not be used to infer the target, such as a feature provided by a detector, may be missing for technical reasons. FastBDT supports both types of missing values. The first type can be passed as negative or positive infinity, FastBDT puts these values in its under- or overflow. As a result, an intersection can be applied that separates the missing from the finite values, and the method can use the information provided by the presence of a missing value. The second type should be passed as a NaN (Not a Number) floating point according to the IEEE 754 standard (i.e. 1985). If the tree then tries to define itself as an end point, it considers itself to be a point."}, {"heading": "4.3 Feature Importance Estimation", "text": "The usual approach to calculating the global feature meaning is to calculate the separation gain of each feature by looping over all trees and nodes. Individual importance for a single data point can be calculated similarly by adding all separation gains along the path of the event through the trees. This approach suffers from possible correlation and nonlinear dependencies between the features, as shown in the simple example in Table 3. In a single decision tree, one of the features has a separation gain of zero, even though both have exactly the same amount of information. FastBDT's rapid adaptation phase makes it possible to apply another popular approach by measuring the reduction in performance (e.g. the range below the curve (AUC) of the receiver's operating character when one feature is omitted. This requires N-fit operations where the N number of features can be most improved (using the method with 12 + being the most accurate)."}, {"heading": "5. Conclusion", "text": "FastBDT implements the widely used Stochastic Gradient-Boosted Decision Tree (SGBDT) algorithm (Friedman, 2002), which provides good out-of-the-box performance and generates an interpretable model. Frequently, multivariate classifiers are trained once and then applied to large datasets. FastBDT performs well in this case, providing support for missing values, uniform preprocessing of features, and a fast application phase. The main advantage is the fast (in terms of CPU time) adaptation phase compared to popular implementations such as TMVA (Hoecker et al., 2007), scikit-learn (Pedregosa et al., 2011), and XGBoost (Chen and Guestrin, 2016). Possible use cases include: real-time learning applications, frequent retrofitting of classifiers, adjustment of a large number of ficators to the respective phases of time and the number of classifiers."}], "references": [{"title": "Quickly boosting decision trees \u2013 pruning underachieving features early", "author": ["Ron Appel", "Thomas Fuchs", "Piotr Dollar", "Pietro Perona"], "venue": "Proceedings of the 30th International Conference on Machine Learning (ICML13),", "citeRegEx": "Appel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Appel et al\\.", "year": 2013}, {"title": "Classification and Regression Trees. Statistics/Probability Series", "author": ["Leo Breiman", "J.H. Friedman", "R.A. Olshen", "C.J. Stone"], "venue": null, "citeRegEx": "Breiman et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Breiman et al\\.", "year": 1984}, {"title": "Root \u2014 an object oriented data analysis framework. Nuclear Instruments and Methods in Physics Research Section A: Accelerators, Spectrometers, Detectors and Associated Equipment", "author": ["Rene Brun", "Fons Rademakers"], "venue": "doi: http://dx.doi.org/10.1016/S0168-9002(97)00048-X. URL http://www.sciencedirect. com/science/article/pii/S016890029700048X. New Computing Techniques in Physics", "citeRegEx": "Brun and Rademakers.,? \\Q1997\\E", "shortCiteRegEx": "Brun and Rademakers.", "year": 1997}, {"title": "Xgboost: A scalable tree boosting system, 2016", "author": ["Tianqi Chen", "Carlos Guestrin"], "venue": null, "citeRegEx": "Chen and Guestrin.,? \\Q2016\\E", "shortCiteRegEx": "Chen and Guestrin.", "year": 2016}, {"title": "Higgs boson discovery with boosted trees", "author": ["Tianqi Chen", "Tong He"], "venue": "JMLR Workshop and Conference Proceedings,", "citeRegEx": "Chen and He.,? \\Q2015\\E", "shortCiteRegEx": "Chen and He.", "year": 2015}, {"title": "The new linux \u2019perf", "author": ["Arnaldo Carvalho de Melo"], "venue": "URL http://vger.kernel", "citeRegEx": "Melo.,? \\Q2010\\E", "shortCiteRegEx": "Melo.", "year": 2010}, {"title": "Greedy function approximation: A gradient boosting machine", "author": ["Jerome H. Friedman"], "venue": "Annals of Statistics,", "citeRegEx": "Friedman.,? \\Q2000\\E", "shortCiteRegEx": "Friedman.", "year": 2000}, {"title": "Stochastic gradient boosting", "author": ["Jerome H. Friedman"], "venue": "Computational Statistics and Data Analysis,", "citeRegEx": "Friedman.,? \\Q2002\\E", "shortCiteRegEx": "Friedman.", "year": 2002}, {"title": "TMVA: Toolkit for Multivariate Data Analysis", "author": ["Andreas Hoecker", "Peter Speckmayer", "Joerg Stelzer", "Jan Therhaag", "Eckhard von Toerne", "Helge Voss"], "venue": "PoS, ACAT:040,", "citeRegEx": "Hoecker et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Hoecker et al\\.", "year": 2007}, {"title": "Algorithms for Memory Hierarchies: Advanced Lectures, chapter An Overview of Cache Optimization Techniques and Cache-Aware Numerical Algorithms, pages 213\u2013232", "author": ["Markus Kowarschik", "Christian Wei\u00df"], "venue": null, "citeRegEx": "Kowarschik and Wei\u00df.,? \\Q2003\\E", "shortCiteRegEx": "Kowarschik and Wei\u00df.", "year": 2003}, {"title": "Advanced event reweighting using multivariate analysis", "author": ["D. Martschei", "M. Feindt", "S. Honc", "J. Wagner-Kuhr"], "venue": "J. Phys. Conf. Ser.,", "citeRegEx": "Martschei et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Martschei et al\\.", "year": 2012}, {"title": "An empirical comparison of pruning methods for decision tree induction", "author": ["John Mingers"], "venue": "Machine Learning,", "citeRegEx": "Mingers.,? \\Q1989\\E", "shortCiteRegEx": "Mingers.", "year": 1989}, {"title": "Valgrind: A framework for heavyweight dynamic binary instrumentation", "author": ["Nicholas Nethercote", "Julian Seward"], "venue": "SIGPLAN Not.,", "citeRegEx": "Nethercote and Seward.,? \\Q2007\\E", "shortCiteRegEx": "Nethercote and Seward.", "year": 2007}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Learning nonlinear functions using regularized greedy forest", "author": ["Tong Zhang Rie Johnson"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "Johnson.,? \\Q2014\\E", "shortCiteRegEx": "Johnson.", "year": 2014}], "referenceMentions": [{"referenceID": 7, "context": "In the following the stochastic gradient-boosted decision tree algorithm (Friedman, 2002) and its hyper-parameters are briefly described.", "startOffset": 73, "endOffset": 89}, {"referenceID": 11, "context": "There are pruning algorithms which automatically remove cuts prone to over-fitting from the DT (Mingers, 1989).", "startOffset": 95, "endOffset": 110}, {"referenceID": 1, "context": "A detailed description of decision trees is available in Breiman et al. (1984).", "startOffset": 57, "endOffset": 79}, {"referenceID": 6, "context": "The complete algorithm is derived and discussed in detail by Friedman (2000).", "startOffset": 61, "endOffset": 77}, {"referenceID": 6, "context": "The incorporation of randomization into the procedure was extensively studied by Friedman (2002). The fraction of samples used in each boosting step is another hyper-parameter called the sub-sampling rate \u03b1.", "startOffset": 81, "endOffset": 97}, {"referenceID": 6, "context": "It is easy to see that the first approach has large potential and there are several authors which investigated this approach for the SGBDT algorithm: Already the original paper (Friedman, 2000) on GBDTs showed that 90 % to 95 % of the training data-points can be removed from the fitting-phase after some boosting steps without sacrificing accuracy of the classifier.", "startOffset": 177, "endOffset": 193}, {"referenceID": 0, "context": "Another approach was presented in Appel et al. (2013) where a subset of the training data was used to prune underachieving features early during the fitting phase without affecting the final performance.", "startOffset": 34, "endOffset": 54}, {"referenceID": 12, "context": "In this work perf (de Melo, 2010), valgrind (Nethercote and Seward, 2007) and std::chrono::high_resolution_clock (ISO, 2012) were used to benchmark the execution time and identify critical code sections.", "startOffset": 44, "endOffset": 73}, {"referenceID": 9, "context": "CPU caches assume spatial locality (Kowarschik and Wei\u00df, 2003), which means that if the values in memory are accessed in linear order there is a high probability that they are already cached.", "startOffset": 35, "endOffset": 62}, {"referenceID": 9, "context": "In addition, CPU caches assume temporal locality (Kowarschik and Wei\u00df, 2003), which means that frequently accessed values in memory are cached as well.", "startOffset": 49, "endOffset": 76}, {"referenceID": 8, "context": "\u2022 TMVA (Hoecker et al., 2007) (ROOT version 6.", "startOffset": 7, "endOffset": 29}, {"referenceID": 2, "context": "06/00) \u2013 The multivariate analysis package of the ROOT (Brun and Rademakers, 1997) data analysis library developed by CERN (GPLv2 license); \u2022 scikit-learn (Pedregosa et al.", "startOffset": 55, "endOffset": 82}, {"referenceID": 13, "context": "06/00) \u2013 The multivariate analysis package of the ROOT (Brun and Rademakers, 1997) data analysis library developed by CERN (GPLv2 license); \u2022 scikit-learn (Pedregosa et al., 2011) (version 0.", "startOffset": 155, "endOffset": 179}, {"referenceID": 3, "context": "\u2022 xgboost (Chen and Guestrin, 2016) (development version 22.", "startOffset": 10, "endOffset": 35}, {"referenceID": 4, "context": "2016)4 \u2013 A modern implementation of BDTs which performed well in the Higgs Boson challenge (Chen and He, 2015) (Apache license).", "startOffset": 91, "endOffset": 110}, {"referenceID": 0, "context": "However, the nature of the data has no influence on the execution time of the SGBDT algorithm in the considered implementations, since no optimizations which prune features using their estimated separation power, as described in Appel et al. (2013), are employed.", "startOffset": 229, "endOffset": 249}, {"referenceID": 10, "context": "In particular FastBDT allows for negative individual weights, which are commonly used in data-driven techniques to statistically separate signal and background using a discriminating variable (Martschei et al., 2012).", "startOffset": 192, "endOffset": 216}, {"referenceID": 7, "context": "FastBDT implements the widely employed Stochastic Gradient-Boosted Decision Tree (SGBDT) algorithm (Friedman, 2002), which exhibits a good out-of-the-box performance and generates an interpretable model.", "startOffset": 99, "endOffset": 115}, {"referenceID": 8, "context": "The main advantage is the fast (in terms of CPU time) fitting-phase compared with popular implementations like TMVA (Hoecker et al., 2007), scikit-learn (Pedregosa et al.", "startOffset": 116, "endOffset": 138}, {"referenceID": 13, "context": ", 2007), scikit-learn (Pedregosa et al., 2011) and XGBoost (Chen and Guestrin, 2016).", "startOffset": 22, "endOffset": 46}, {"referenceID": 3, "context": ", 2011) and XGBoost (Chen and Guestrin, 2016).", "startOffset": 20, "endOffset": 45}], "year": 2016, "abstractText": "Stochastic gradient-boosted decision trees are widely employed for multivariate classification and regression tasks. This paper presents a speed-optimized and cache-friendly implementation for multivariate classification called FastBDT. FastBDT is one order of magnitude faster during the fitting-phase and application-phase, in comparison with popular implementations in software frameworks like TMVA, scikit-learn and XGBoost. The concepts used to optimize the execution time and performance studies are discussed in detail in this paper. The key ideas include: An equal-frequency binning on the input data, which allows replacing expensive floating-point with integer operations, while at the same time increasing the quality of the classification; a cache-friendly linear access pattern to the input data, in contrast to usual implementations, which exhibit a random access pattern. FastBDT provides interfaces to C/C++, Python and TMVA. It is extensively used in the field of high energy physics by the Belle II experiment.", "creator": "LaTeX with hyperref package"}}}