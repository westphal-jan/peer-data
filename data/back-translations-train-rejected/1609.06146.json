{"id": "1609.06146", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "18-Sep-2016", "title": "mlr Tutorial", "abstract": "This document provides and in-depth introduction to the mlr framework for machine learning experiments in R.", "histories": [["v1", "Sun, 18 Sep 2016 01:08:20 GMT  (1438kb,D)", "http://arxiv.org/abs/1609.06146v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["julia schiffner", "bernd bischl", "michel lang", "jakob richter", "zachary m jones", "philipp probst", "florian pfisterer", "mason gallo", "dominik kirchhoff", "tobias k\\\"uhn", "janek thomas", "lars kotthoff"], "accepted": false, "id": "1609.06146"}, "pdf": {"name": "1609.06146.pdf", "metadata": {"source": "META", "title": "mlr Tutorial", "authors": ["Julia Schiffner", "Bernd Bischl", "Michel Lang", "Jakob Richter", "Zachary M. Jones", "Philipp Probst", "Florian Pfisterer", "Mason Gallo", "Dominik Kirchhoff", "Tobias K\u00fchn Janek", "Thomas Lars Kotthoff"], "emails": [], "sections": [{"heading": null, "text": "mlr TutorialJulia Schiffner Bernd Bischl Michel Lang Jakob Judge Zachary M. Jones Philipp Probst Florian Pfisterer Maurer Gallo Dominik KirchhoffTobias K\u00fchn Janek Thomas Lars Kotthoff"}, {"heading": "Contents", "text": "The personal data will be published in the coming days, the personal data will be transmitted in the coming weeks and months, and the personal data will be revised in the coming weeks and months."}, {"heading": "Quick start", "text": "A simple layered cross-validation of linear discriminant analysis with mlr.library (mlr) data (iris) # # Define the task = makeClassifTask (id = \"tutorial,\" data = iris, target = \"species\") # # Define the learner lrn = makeLearner (\"classif.lda\") # # Define the resampling strategy rdesc = makeResampleDesc (method = \"CV,\" Stratify = TRUE) # # Define the resampling strategy r = resample (learner = lrn, task = task, resampling = rdesc, show.info = FALSE) # # Determine the mean misclassification error r $aggr # > mmce.test.mean # > 0.02"}, {"heading": "Basics", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Learning Tasks", "text": "Learning tasks include the data set and other relevant information about a machine learning problem, such as the name of the target variables for monitored problems."}, {"heading": "Task types and creation", "text": "The tasks are organized in a hierarchy, with the generic task at the top. The following tasks can be instantiated and all of them can be taken over by the virtual superclass task: \u2022 RegrTask for regression problems, \u2022 ClassifTask for binary and multi-class classification problems (cost-sensitive classification with class-related costs can also be handled), \u2022 SurvTask for survival analysis, \u2022 ClusterTask for cluster analysis, \u2022 MultilabelTask for multi-class classification problems, \u2022 CostSensTask for a general cost-sensitive classification (with example-specific costs). To create a task, simply run make < TaskType >, e.g. makeClassifTask. All tasks require an identification (argument ID) and a data.frame (argument data). If no ID is provided, it will be generated automatically using the variable data name."}, {"heading": "Regression", "text": "For supervised learning such as regression (as well as classification and survival analysis), we need to specify the name of the target variable in addition to the data. Data (BostonHousing, package = \"mlbench\") regr.task = makeRegrTask (id = \"bh,\" data = BostonHousing, target = \"medv\") regr.task # > Supervised task: bh # > Type: regr # > Objective: medv # > Observations: 506 # > Features: # > Numerical factors sorted # > Error: FALSE # > Has weights: FALSE # > Has blocked: FALSEAs you can see, the task records the type of the learning problem and basic information about the dataset, such as the types of characteristics (numerical vectors, factors or sorted factors), the number of observations, or whether there are missing values.Create tasks for classification and survival analysis and simply follow the same data scheme."}, {"heading": "Classification", "text": "For classification, the target column must be a factor. In the following example, we define a classification task for the breast cancer dataset and exclude the variable ID from all other models for adaptation and evaluation. data (BreastCancer, package = \"mlbench\") df = BreastCancer df $Id = NULL classif.task = makeClassifTask (id = \"BreastCancer,\" data = df, target = \"Class\") classif.task # > Supervised task: BreastCancer # > Type: classif # > Target: Class # > Observations: 699 # > Features: # > Numerical factors sorted # > 0 4 5 # > Misconceptions: TRUE # > Has weights: FALSE # > Has blocking: FALSE # > Classes: Positive Classes: 2 # > Benign malignant maigots # > Positive Class: benigant class."}, {"heading": "Survival analysis", "text": "For interval censored data, the two target columns must be specified in \"interval2\" format (see Surv).data (lung, package = \"survival\") lung $status = (lung $status = = 2) # convert to logical surv.task = makeSurvTask (data = lung, target = c (\"time,\" \"status\") surv.task # > Supervised task: lung # > Type: surv # > Target: time, status # > Observations: 228 # > Features: # > Numerical factors sorted # > 8 0 0 # > Misconceptions: TRUE # > Has weight: FALSE # > Has blocking: FALSEThe type of censorship can be specified using the argument censoring, which defaults to \"rcens\" for right censored data."}, {"heading": "Multilabel classification", "text": "In multi-label classification, each object can belong to more than one category.The data is expected to contain as many target columns as class label.The target columns should be logical vectors specifying which class label.The names of the target columns should be class label.yeast = getTaskData (yeast.task) labels = colnames (yeast.task # 1: 14] yeast.task = makeMultilabelTask (id = \"multi,\" data = yeast, task = yeast, target = labelTask.yeast = getTaskData (yeast.task) labels = colnames (yeast.task # 1: 14] yeast.task = makeMultilabelTask (id = \"multi,\" data = labels) yeast.task # > Superblocking task # > task: multi # getTaskData (yeast.task) labels = colnames (yeast.task) labels (yeast.task) labels = colnames (yeast.task # 1: 14] yeast.task) labels should be yeast.task = yeast.task = yeast.task _ Task (id = \"multi,\" data = labels) yeast.task # yeast.task #, \"data = yeast.task # yeast.task #,\" which should be yeast.task # labels."}, {"heading": "Cluster analysis", "text": "Since the cluster analysis is not monitored, the data is the only compelling argument to construct a cluster analysis task. In the following, we will create a learning task from the mtcars.data (mtcars, package = \"datasets\") cluster.task = makeClusterTask (data = mtcars) cluster.task # > Unmonitored task: mtcars # > Type: Cluster # > Observations: 32 # > Features: # > Numerical factors sorted # > 11 0 # > Misconceptions: FALSE # > Has weights: FALSE # > Has blocking: FALSE"}, {"heading": "Cost-sensitive classification", "text": "The standard goal in the classification is to achieve a high predictive accuracy, i.e. > has to minimize the number of errors. All types of misclassification errors are considered to be equally severe. In many applications, different types of errors incur different costs. In the case of class-related costs, which depend exclusively on the actual and predicted class name, it is sufficient to use an ordinary ClassifTask.In order to handle example-specific costs, it is necessary to generate a CostSensTask #. In this scenario, each example (x, y) is associated with an individual cost vector of length K, with K indicating the number of classes. The k component gives the cost of assigning x to the class # k.Of course, it is assumed that the cost of the intended class # y is minimal.Since the cost vector contains all relevant information about the intended class y, only the characteristic values and a cost matrix (cost matrix) containing the cost matrix are required for the task (the cost matrix are the cost matrix for the examples)."}, {"heading": "Further settings", "text": "For example, we could add a blocking factor to the task. This would indicate that some observations \"belong together\" and should not be separated when dividing the data into repetition training and test sets. Another option is to assign weights to observations. These can simply display observation frequencies or be derived from the sample scheme used to collect the data. Note that you should only use this option if the weights really belong to the task. If you plan to train some learning algorithms with different weights on the same task, mlr offers several other ways to set observation or class weights (for a supervised classification). See, for example, the Training tutorial page or the makeWeightedClassesWrapper function."}, {"heading": "Accessing a learning task", "text": "We offer many operators access to the items stored in a task. < The most important ones are in the documentation of Task and getTaskData.To access the task description, which contains basic information about the task you can use: getTaskDescription Factor (classif.task) # > $id # > [1]... \"< 10\" BreastCancer \"# > w..\" # > $$Type # 7, \"Tasf\" # > $7, \"Tasf\" # > $7, \"Tasf.\" # 7, \"Tasf.\" 7, \"Tasf.\" # 7, \"Tasf.\" # 7, \"Tasf.\", \"Tasf.\" # 7, \"Tasf.\", \"Tasf.\" $7, \"Tasf.\" 7, \"Tasf.\""}, {"heading": "Modifying a learning task", "text": "Here are some examples. # # # Select observations and / features cluster.task = subsetTask (cluster.task # > Min 0.5, subset = 4: 17) # # # It can happen, especially after selecting observations that are constant. # # # These should be removed. # > Numerical factors sorted # > Malfunctions: FALSE # > Has a data volume of 1 columns: am # > Unmonitored task: mtcars # > Type: Cluster # > Observations: 14 # > Properties: Median properties: # > Numerical factors sorted # 10 0 # > Abuses: FALSE # > Has blocked: FALSE # # # Blocking selected functions: 3737qFeatures (surv.task, c \"meal.cal,\". \"loss.\")."}, {"heading": "Learners", "text": "The following classes provide a unified interface to all common machine learning methods in the areas of R: (cost-sensitive) classification, regression, survival analysis and clustering. Many are already integrated into mlr, others are not, but the package is specifically designed to simplify extension.Section Integrated learners shows the already implemented machine learning methods and their characteristics. If your preferred method is missing, either open a problem or take a look at how to integrate a learning method yourself. This basic introduction shows how to use already implemented learners."}, {"heading": "Constructing a learner", "text": "In the USA, the number of those able to do so is in the United States, Europe, Asia, Latin America, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa, Africa"}, {"heading": "Accessing a learner", "text": "The learner object is a list and the following elements contain information regarding the hyperparameters and the type of prediction - > > Prediction - > Prediction - > Prediction. # # # Get the configured hyperparameter settings taken from the default parameters cluster.lrn $par.vals # > $centers # > [1] 5 # # # UE- > Prediction- > Prediction- > Prediction- > Prediction- > Prediction- > Prediction- > Prediction- > Prediction- > Prediction- > > Prediction- > > Prediction- > > Prediction- > > Prediction- > > > Prediction- > > > > > Prediction- > Prediction- # Prediction- # Prediction- # Prediction- > Prediction- > Prediction- # Prediction- > - > Prediction- > > Prediction- > > Prediction- > > Prediction- # Prediction- # Prediction- # Parameters - > > > > > > Prediction- > > > > Prediction- # Prediction- # - # Prediction- # Parameters - > - # Prediction- - # - Prediction- - - - - - - - - # 1 - Prediction- - - - - - # Configured Parameters # 1 - # -Tune- # -Tune- # # -Tune- # # Prediction- # Settings # - # 500 # - # Preparameters # - > 1 - > - # 1 - Prediction- > - # ALALALFORM # # # # # ALFORM # # # # # # # # # # ALFORM # # # # # # # # # # # # # # # # # # Translation # # # # #"}, {"heading": "Modifying a learner", "text": "Here are some examples. # # # Change the ID surv.lrn = setLearnerId (surv.lrn, \"CoxModel\") surv.lrn # > Learner CoxModel from package survival # > Type: surv # > Name: Cox Proportional Hazard Model; Short name: coxph # > Class: surv.coxph # > Properties: numerics, factors, weight, rcens # > Predict-Type: response # > Hyperparameter: # # # Change the prediction type, predict a factor with class names instead of probabilities classif.lrn = setPredictType (classif.lrn, \"response\") # # Change hyperparameter values cluster.lrn = setHyperPars (cluster.lrn, centers = 4) # # Return to default hyperparameter values regr.lrn remodictType (classif.lrn, \"\" response), Hyperparameter \"l.lrn (\")"}, {"heading": "Listing learners", "text": "A list of all learners integrated into mlr and their respective characteristics is shown in the appendix > > >. If you want a list of available learners, perhaps only with certain characteristics or suitable for a particular task, use the listLearners function. # # # Lists everything in mlr lrns = listLearners () head (lrns [c (\"class,\" \"package\")] # > class package # > classif.ada # > 2 classif.avNNet nnet # > 3 classif.bartMachine bartMachine # > 4 classif.bdk kohonen # > 5 classif.binomial stats # > 6 classif.black.blacknbsp # cebncebndhsrf.it # cnI rf\u00fc for the probabilities lrf\u00fc that a party will occur is very high."}, {"heading": "Training a Learner", "text": "In mlr this can be done by setting the task = makeClassifTask (data = iris, target = \"Species\") # # # Generate the learner lrn = makeLearner (\"classif.lda\") # # # Train the learner mod = train (lrn, task) mod # > Model for learner.id = classif.lda; learner.class = classif.lda # > Trained on: task.id = iris; features = 4 # > Hyperparameters: In the example above, the creation of the learner is explicitly not necessary."}, {"heading": "Accessing learner models", "text": "# 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8 8"}, {"heading": "Further options and comments", "text": "The subset argument of the train takes a logical or integer vector that indicates which observations to use, for example, if you want to split your data into a training and a test set, or if you want to fit different models to different subsets in the data. Below, we fit a linear regression model to the BostonHousing data set (bh.task) and can randomly select 1 / 3 of the data set for training. # # Get the number of observations n = getTaskSize (bh.task) # # Use 1 / 3 of the observations for training train.set = example (n, size = n / 3) # Train the learning class mod = Train (\"regr.lm,\" bh.task, subset = train.set) mod # > Model for training train.id = regr.lm; learner.class = > Train: \"ask.id.\""}, {"heading": "Predicting Outcomes for New Data", "text": "# Predicting the target values for new observations is implemented in the same way as most other prediction methods in R. = > > > > All you have to do is call predictions of the object returned from the trace and pass the data for which you want to make predictions. There are two ways of passing the data: \u2022 Either perform the task using the argument of the task or > pass a data frame using the argument of the new data.The first way is preferable if you want to make predictions for data already contained in a task # # #.Just as in the train, the prediction function has produced a subset argument so that you can put different parts of the data in the task for training and prediction aside (more advanced methods for dividing the data into train and test sentence # # # will be described in the section about amping #. In the following example, we will make a gradient boosting machine for each second observation of the boonsting data #, # make predictions for the remaining data # and make predictions for the remaining #:"}, {"heading": "Accessing the prediction", "text": "Its most important element is $data, a data frame that contains columns with the true values of the target variables (in the case of assisted learning problems) and the predictions. Use as.data.frame for direct access. Below, the predictions about the Boston Housing and the iris datasets are presented. As you may recall, the predictions in the first case were made from a task and in the second case from a data frame. # # Result of the prediction with data passed over the task rubber head (as.data.frame (task.pred)) # > id Truth Answer # > 2 21.6 22.28539 # > 4 33.4 23.33968 # > 6 6 28.7 22.40896 # > 8 27.1 22.12750 # > 10 22.12750 # > Prediction (prediction of the prediction with data passed over the new data argument header)."}, {"heading": "Extract Probabilities", "text": "The predicted probabilities can be derived from the prediction with the function getPredictionosa > > > > Probabilities = > > > Probabilities. (The function getProbabilities has been discarded in favor of getPredictionProbabilities in mlr version 2.5.) Here is another cluster analysis example. We use fuzzy c-means clustering on the mtcars data set.lrn = makeLearner (\"cluster.cmeans,\" predict.type # \"prob\") mod = train (lrn, mtcars.task) pred = prediction (mod, task = mtcars.task) head (getPredictionProbabilities (pred) # 2 # > Mazda RX4 0.97959529 # prob.Mazda 0.020404714 # > Mazda RX4 Wag 0.97963550 0.020364495 # > Datsun 710 0.99265984 0.007340164 # > Hornet ProbleDrive > 770b7970.79770737711 > 70.7973770.737711"}, {"heading": "Confusion matrix", "text": "A confusion matrix can be achieved by calling CalculateConfusionMatrix. # > The columns represent predicted > > > > > Confusion and the rows represent true class labels.calculateConfusionMatrix (pred) # > Predicted # > true setosa versicolor virginica -err. # > setosa 50 0 > > > > > Confusion 0 49 1 # > virginica 0 5 45 5 # > -err.- 0 5 # 6You can see the number of correctly classified observations on the diagonal of the matrix. # Misgraded observations are on the off-diagonal side. # The total number of errors for individual (true and predicted) classes is in the # -err.- row and column # # # # # # # # # # # # # # # Conversion frequencies in addition to absolute numbers we can TRUE."}, {"heading": "Adjusting the threshold", "text": "\",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\",, \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \""}, {"heading": "Visualizing the prediction", "text": "\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf"}, {"heading": "Evaluating Learner Performance", "text": "The quality of predictions of a model in mlr can be assessed against a number of different performance metrics. To calculate the performance metrics, call the performance on the object returned by the prediction and specify the desired performance metrics."}, {"heading": "Available performance measures", "text": "mlr offers a wide range of performance measures for all types of learning problems. Typical performance measures for the classification are mean misclassification error (mmce), accuracy (acc) or measures based on ROC analysis. For regression, the mean square error (mse) or mean absolute error (mae) is usually taken into account. For cluster tasks, measures such as the Dunn index (thin) are provided, while for survival predictions the concordance index (cindex) is supported and for cost-sensitive predictions the misclassification penalty (mcp) and others. It is also possible to access the time to train the learner (timetrain), the time to calculate the prediction (timepredict) and its sum (timeboth) as a performance measure. To see which performance measures are being implemented, take a look at the table of performance measures and the measure documentation. If you do not want to include a measure with a misclassification or fail to implement a section."}, {"heading": "Listing measures", "text": "The characteristics and requirements of each measure are shown in the Performance Measurement Table.If you are using a list of available measures with specific characteristics or for a specific learning task, use the listMeasures function. # # # Performance measures for multi-class classification listMeasures (\"classif,\" properties = \"classif.multi\") # > [1] \"Multiclass.brier,\" \"the message reads,\" multiclass.aunu \"# >\" qsr, \"\" timeboth, \"\" timepredict, \"\" acc, \".\" 10] \"lsc,\" \"lsasl,\" \"rf\u00fc,\" \"rf\u00fc\" csasd, \"easd,\" easds, \"easds,\" easds, \"easds,\" easds, \"easds,\" easds, \"easds,\" easds, \"easds,\" easds. \""}, {"heading": "Calculate performance measures", "text": "In the following example, we adjust a gradient-enhancing machine to a subset of the Boston Housing dataset and calculate the default Mean Square Error (mse) for the remaining observations. n = getTaskSize (bh.task) lrn = makeLearner (\"regr.gbm,\" n.trees = 1000) mod = train (lrn, task = bh.task, subset = seq (1, n, 2)) pred = predict (mod, task = bh.task, subset = seq (2, n, 2)) performance (pred) # > mse # > 42.68414The following code calculates the median square error (medse) instead.performance (pred, measures = medse) # > medse # > 9.134965Of course, we can also calculate multiple measures of performance at once by simply passing a list of measures that may include your own measurements. Calculate the square error (m48.4), the mean error (the other 4948.4) and the mean error (4948.4)."}, {"heading": "Requirements of performance measures", "text": "Note that in order to calculate some performance measures, in addition to the prediction, the task or adapted model must be passed. For example, to evaluate the time required for the training (timetable), the adapted model must be passed. Performance (pred, measures = timetrain, model = mod) # > timetrain # > 0.061For many performance measures in cluster analysis, the task is required.lrn = makeLearner (\"cluster.kmeans,\" centers = 3) mod = train (lrn, mtcars.task) pred = prediction (mod, task = mtcars.task) # # Calculate the Dunn index performance (pred, measures = dunn, task = mtcars.task) # > dunn # > 0.1462919In addition, some measures require a certain type of prediction (mod).For example, in binary classification, to calculate the AUC (auc) - the range below the ROC (receiver characteristic)."}, {"heading": "Access a performance measure", "text": "Performance measurements in mlr are objects of the class Measure. If you are interested in the properties or requirements of a single measure, you can access them directly. Information about the individual slots can be found on the help page of Measure. # # # Mean Misclassification error str (mmce) # > List of 10 # > $id: chr \"mmce\" # > $minimize: logi TRUE # > $properties: chr [1: 4] \"classif\" \"classif.multi\" req.pred \"\" req.truth \"# > $fun: function (task, model, pred, feats, extra.args) # > $extra.args: list () # > best: num 0 # > $worst: num 1 # > $name: chr\" Mean Misclassification error \"# > $worst: error\" (task, model, pred, feats, extra.args) # > $aggr: list () # best $: $> num # 0 um # # rechum: > 1 rechum: $$num: $nr # 1."}, {"heading": "Binary classification", "text": "For binary classification exist special techniques to analyze the performance.Plotpower versus threshold valueAs you may remember (see the previous section on the creation of predictions) in binary classification we can adjust the threshold that is used to map probabilities to class markings. Helpfully in this regard, the functions generateThreshVsPerfData and plotThreshVsPerf, which generate or plot the learning performance against the threshold. In the following example we consider the sonar datasets and plotting the false positive rate (fpr), the false negative rate (fpr) as well as the false classification rate (mmshrce) for all possible thresholds. lrn = makeLearner (\"classif.lda,\" predicte.type = \"prob\") n = getTaskSize (sonar.task) mod = train (lrn, sonar.fn.subtaskrate seq = 000000000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,00"}, {"heading": "ROC measures", "text": "For binary classification, there are a large number of specialized metrics that can be nicely formatted into a matrix, see, for example, the receiver characteristic curve on wikipedia.We can use a similar table with the calateROCMeasures function.r = calateROCMeasures (pred) r # > predicts # > true M R # > M 0.7 0.3 tpr: 0.7 fpr: 0.3 # > R 0.25 0.75 fpr: 0.25 tnr: 0.75 # > ppv: 0.76 for: 0.32 lrp: 2.79 acc: 0.72 # > fdr: 0.24 npv: 0.68 lrm: 0.4 dor: 6.88 # # # > Abbreviations: # > tpr - True Positive Rate (Sensitivity, Recall) # > fpr - False Positive Rate (Fall-out) # > > Predict False Negative Rate Rate (Miss) # tivate - > tnr - Positive Rate (-) picic value (- > (Picity)."}, {"heading": "Resampling", "text": "\"To assess the performance of a learning algorithm, resampling strategies are commonly used.\" The entire data set is divided into (multiple) training and test sets. \u2022 You train a learner on each training set, predict on the corresponding test set (sometimes on the training set) and calculate some performance measures. \u2022 Then the individual performance values are aggregated, typically by calculating the mean. # There are various resampling strategies, such as cross-validation and bootstrap, to name just two popular approaches. # If you want to read more details, the results of resampling strategies for model assessment and selection are probably not badly chosen by Simon. # Bernd has also published a resampling methodology for metamodel validation, which contains detailed descriptions and a lot of statistical background information about resampling methods."}, {"heading": "Stratified resampling", "text": "In fact, it is a way in which people are able to survive on their own, \"he said in an interview with The New York Times."}, {"heading": "Aggregating performance values", "text": "In the re-sample, we get (for each measure we want to calculate) for each iteration a performance value (on the test set, the training set or both), which is then aggregated. As mentioned above, mainly the mean is calculated above the performance values on the test datasets (test.mean).For example, a 10-fold cross-validation calculates 10 values for the selected power measurement, the aggregated value being the mean of these 10 numbers. mlr knows how to handle this, because each measure knows how to aggregate: # # Mean misclassification error mmce $aggr # > Aggregation function: test.mean # # Root mean square error rmse $aggr # > Aggregation function: test.rmseThe aggregation method of a measure can be changed via the function setAggregation."}, {"heading": "Example: Different measures and aggregations", "text": "\"We use the mean error rate = > > > = the mean error rate and mean of the actual positive rates m1 = mm.rpart # setAggregation (tpr, test.median) rdesc = > > > 667,\" iters = 3) r = resample (\"classif.rpart\"), rsrreeSrret # 3, \"resrreeSrret # 3,\" resrreeS # 3, \"resrreeS # 3,\" \"resrreeS # 3,\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\", \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \",\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "Convenience functions", "text": "For this reason, mlr provides some convenience features for commonly used resampling strategies, such as holdout, crossval, or BootstrapB632. Note, however, that you do not have as much control and flexibility as with resamples with resample description or instance.Holdout (\"regr.lm,\" bh.task, measures = list (mse, mae)) Crossval (\"classif.lda,\" iris.task, iters = 3, measures = list (mmce, ber))."}, {"heading": "Tuning Hyperparameters", "text": "Many machine learning algorithms have hyperparameters that need to be set. If selected by the user, they can be specified as explained on the learner tutorial page - just pass them on to makeLearner. Often, appropriate parameter values are not obvious and it is preferable to set the hyperparameters, i.e. automatically identify values that lead to the best performance."}, {"heading": "Basics", "text": "To optimize a machine learning algorithm, you must specify: \u2022 the search space \u2022 the optimization algorithm (also called tuning method) \u2022 an evaluation method, i.e. a resampling strategy and a performance measurement. An example of the search space could be search values of the C parameter for SVM: # # # Example: Creating a search space for the C hyperparameter from 0.01 to 0.1 ps = makeParamSet (makeNumericParam (\"C,\" lower = 0.01, above = 0.1)). An example of the optimization algorithm could be conducting a random search in the room: # # Example: Random search with 100 iterations ctrl = makeTuneControlRandom (maxit = 100L) An example of an evaluation method could be the triple CV, using accuracy as a performance measure: rdesc = makeResampleDesmakeDesmakeControlRandom = Resceptance Methodology (for this Acceptance Method = 3V)."}, {"heading": "Specifying the search space", "text": "We first need to define a space to search when tuning our learner. For example, maybe we want to set several specific values of a hyperparameter, or maybe we want to define a space from 10 \u2212 10 to 1010 and let the optimization algorithm decide which points to select. To define a search space, we create a ParamSet object that describes the parameter space we want to search for using the function makeParamSet. For example, we could define a search space with only the values 0.5, 1.0, 1.5, 2.0 for both C and Gamma. Note how we name each parameter as it is defined in the Kernlab package: discrete _ ps = makeParameSet (makeDiscreteParam ^) values = c (0.5, 1.0, 1.5, 2.0), makeDiscreteParameter parameter as it is defined in the Kernlab package: discrete _ Parameam = makeSet (makeSet Parameam ^) values = c (makeSet = 1.5, \"), makeSet = C (makeParameter = 1.5,\"), makeParameSet = 1.5, makeSet = C ()."}, {"heading": "Specifying the optimization algorithm", "text": "Now that we have specified the search space, we need to select an optimization algorithm to pass our parameters to the ksvm learner. Optimization algorithms are used as TuneControl objects in mlr.A grid search is one of the default - albeit slow - methods to select an appropriate set of parameters from a given search space.In the case of discrete _ ps: ctrl = makeTuneControlGrid: Since we set the values manually, grid search is simply the cross product. Since we have specified only the upper and lower limits for the search space, the grid search generates a grid with equally large steps. By default, the grid search extends across the space in 10 equally large steps. The number of steps can be changed with the resolution argument.Here, we change the grid search object in equally large steps within the defined space. = 15 steps within the defined space = 10 points are normally available."}, {"heading": "Performing the tuning", "text": "[caption: http: / / www.ftd.de / politics / index.php? re] \"We need to develop a new strategy to solve the problems,\" he told the \"world\" (Monday). \"# # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # #"}, {"heading": "Accessing the tuning result", "text": "The result object TuneResult gives you access to the best found settings $x and their estimated performance $y.res $x # > $C # > [1] 95.22422 # > $sigma # > [1] 0.006695534res $y # > acc.test.mean acc.test.sd # > 0.9866666667 0.02309401We can generate a learner with optimal hyperparameter settings as follows: lrn = setHyperPars (makeLearner (\"classif.ksvm\"), par.vals = res $x) lrn # > Learner classif.ksvm from package kernlab # > Type: classif # > Name: Support Vector Machines; Short name: ksvm # > Class: classif.ksvm # > Properties: twoclass, multiclass, numerics, factors # prob, class.weights # > Predict-Type: # response: > Hyperparameter task: > Fit = > 2, we can predict the task = > 00SE = Setset3, here: > 00vosa = Task."}, {"heading": "Investigating hyperparameter tuning effects", "text": "This year has so far been a \"real\" year in which there has been a \"reactionary process.\""}, {"heading": "Further comments", "text": "In long-running tuning experiments, it is very annoying when the calculation stops due to numerical or other errors. Take a look at on.learner.error in configureMlr, as well as the examples in the Configure mlr section of this tutorial. You may also want to learn about impute.val in TuneControl. \u2022 Since we are constantly optimizing the same data during tuning, the estimated performance may be optimistically distorted. A neat approach to ensuring an unbiased performance assessment is resampling, where we embed the entire selection process of the model in an outer resampling loop."}, {"heading": "Benchmark Experiments", "text": "In a benchmark experiment, different learning methods are applied to one or more data sets with the aim of comparing and classifying the algorithms in relation to one or more performance metrics. In mlr, a benchmark experiment can be performed by calling functional benchmarks on a list of learners and a list of tasks. Benchmark basically performs a resample for each combination of learners and tasks. You can define an individual resampling strategy for each task and select one or more performance metrics to be calculated."}, {"heading": "Conducting benchmark experiments", "text": "We start with a small example. Two learners, linear discriminant analysis (lda) and a classification tree (rpart), are applied to a classification problem (sonar.task). As a resume strategy, we choose \"holdout.\" Performance is therefore calculated only once per task, i.e., the same training and testing sets are used for all learners. It is also possible to use a ResampleInstance.If you want to use a fixed test set instead of a randomly selected one, you can create a suitable Resample instance, i.e., the same training and testing sets are used for all learners."}, {"heading": "Accessing benchmark results", "text": "Under the name getBMR < WhatToExtract >, mlr provides several access functions to retrieve information for further analysis, including, for example, the performance or prediction of the learning algorithms under consideration."}, {"heading": "Learner performances", "text": "Let's take a look at the benchmark result above. getBMRPerformances returns individual performance in resampling runs, while getBMRAggrPerformances returns the aggregated values.getBMRPerformances (bmr) # > $\"Sonar-example\" # > $\"Sonar-example\" $classif.lda # > iter mmce # > 1 0.3 # > $\"Sonar-example\" $classif.rpart # > iter mmce # > 1 1 1 0.2857143getBMRAggrPerformances (bmr) # > $\"Sonar-example\" # > $\"Sonar-example\" $classif.lda # > mmce.test.mean # > 0,3 # > $\"Sonar-example\" $classif.rpart # > mmce.test.mean # > Sonar-Sonar-Sonfce.test.mean # > Sonar-learfUgrE # > nerdout as resampling strategy."}, {"heading": "Predictions", "text": ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"}, {"heading": "Learner models", "text": "By default, the BenchmarkResult also contains the customized models for all learners for all tasks. If you do not want them to remain on the call to benchmarks, set models = FALSE. The customized models can be retrieved via the function getBMRModels. It returns a list of WrappedModel objects. getBMRModels (bmr) # > $'Sonar-example' # > $'Sonar-example' $classif.lda # > $'Sonar-example' $classif.lda [[[1] # > Model for learner.id = classif.lda; learner.class = classif.lda # > Trained on: task.id = Sonar-example; obs = 138; features = 60 # > Sonar-example: $> Sonar-example: $> classif.rpart # > nerf.rpart # > $'Sonar-example: $classif.rpart # > $' Sonar-example: $classif.rpart # > nerf.rpart # > > nerf.rpart"}, {"heading": "Learners and measures", "text": "In addition, you can use the Learners and Measures.getBMRLearners (bmr) # > $classif.lda # > Learner classif.lda from the package MASS # > Type: classif # > Name: Linear Discriminant Analysis; Short name: lda # > Class: classif.lda # > Properties: twoclass, multiclass, numerics, factors, prob # > Predict-Type: response # > Hyperparameter: # > $classif.rpart # > Learner classif.rpart # > Properties: classif.rpart from the package rpart # > Type: classif # > Name: Decision Tree; Short name: rpart # > Class: classif.rpart # > Properties: twoclass, multiclass, missings, numerics, factors, ordered, prob, weight, featimp # > Predict-Type: response # > Hyperparameter: BMgetRasures # > Best Measures (>: > mqures) >: >:"}, {"heading": "Merging benchmark results", "text": "\"It's like we're in a position to be in a position,\" he says."}, {"heading": "Benchmark analysis and visualization", "text": "My guide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "Integrated plots", "text": "Plots are generated with ggplot2. Further adjustments such as renaming plot elements or color changes are possible without any problems."}, {"heading": "Visualizing performances", "text": "rE \"s tis rf\u00fc ide r\u00fc the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f for the f"}, {"heading": "Visualizing aggregated performances", "text": "The aggregated performance values (resulting from getBMRAggrPerformances) can be visualized by function diagram BMRSummary. This diagram draws a line for each task on which the aggregated values of a performance measure are displayed for all learners. By default, the first measurement in the list of measurements passed to Benchmark, in our example mmce, is used. In addition, a small vertical jitter is added to prevent plotting. PlotBMRSummary (bmr) \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf iris \u2212 examplemlbench.ringnormmlbench.waveformPimaIndiansDiabetes \u2212 exampleSonar \u2212 example0,1 0,2 0,3 mmce.meanlearner.id \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf ldarpartrf"}, {"heading": "Calculating and visualizing ranks", "text": "This year it is more than ever before."}, {"heading": "Critical differences diagram", "text": "In order to represent learners of different performance levels, a diagram with critical differences can be displayed using either the Nemenyi test (test = \"nemenyi\") or the Bonferroni-Dunn test (test = \"bd\"). \u2022 The mean rank of the learners is displayed on the x-axis. \u2022 The selection test = \"nemenyi\" compares all pairs of learners with each other, so that the results cannot be considered as groups of significantly different learners. \u2022 The diagram connects all groups of learners in which the averages do not differ by more than the critical differences. \u2022 Students who are not connected by a bar are significantly different, and the learner (s) with the lower mean # # # # # can be considered \"better\" at the level of significance selected. \u2022 The selection test = \"bd\" performs a pair-wise comparison with a baseline. An interval that extends around the indicated critical difference in both directions is possible, although the baseline between the learners is stretched only by the same difference."}, {"heading": "Custom plots", "text": "This year, more than ever before, it will be able to retaliate."}, {"heading": "Further comments", "text": "\u2022 Note that mlr offers some additional charts for supervised classification that work on benchmark Result objects and allow you to compare the performance of learning algorithms - see, for example, the tutorial page on ROC curves and functions that generate ThreshVsPerfData, plotROCCurves and plotViperCharts, as well as the page on classification calibration and function generationCalibrationData. \u2022 In the examples shown in this section, we have used \"raw\" learning algorithms, but often things are more complicated. In addition, many learners have hyperparameters that need to be tuned to achieve reasonable results. Reliable performance estimates can be achieved by nested re-sampling, i.e. tuning in an inner resampling loop while assessing performance in an outer loop. In addition, you may want to combine learners with pre-processing steps such as import, scaling, or out-selection."}, {"heading": "Parallelization", "text": "R does not use parallelization by default. By integrating parallelMap into mlr, it will be easy to enable the parallel computing capabilities already supported by mlr. parallelMap: local multi-core execution using Parallel, socket and MPI clusters using Snow, temporary SSH clusters using BatchJobs, and high-performance computing clusters (managed by a scheduler such as SLURM, Torque / PBS, SGE, or LSF) also using BatchJobs.All you need to do is select a backend by calling one of the ParallelStart * functions. the first loop marked as executable in parallel is automatically parallelized. It is a good practice to call parallelStop at the end of your script.library (\"parallelMap\") # parallelStartSocket # >.Starting parallelization in mode =.. socket desc sample mode > OS = leves = (lep)."}, {"heading": "Parallelization levels", "text": "We offer different parallelization levels for fine-grained control over parallelization. For example, if you do not want to parallelize the benchmark function because it has very few iterations, but instead wants to parallelize the resampling of each learner, you can pass the level \"mlr.resample\" specifically to the ParallelStart * function. Currently, the following levels are supported: parallelGetRegisteredLevels () # > mlr: mlr.benchmark, mlr.resample, mlr.selectFeatures, mlr.tuneParamsHere is a brief explanation of what these levels do: \u2022 \"mlr.resample\": Every resampling iteration (a turn / test step) is a parallel job. \u2022 \"mlr.benchmark\": Every experiment \"executes this learner on this dataset\" is a parallel tuning. \"\u2022\" mltunesearch \"Each job parameter in the evaluation is a parallelization."}, {"heading": "Custom learners and parallelization", "text": "So if you see an error after, say, calling a parallelized version of resample: not an applicable method for \"trainLearner\" for an object of the class < my _ new _ learner >, just add the following line after calling parallelStart.parallelExport (\"trainLearner. < my _ new _ learner >,\" predictLearner. \"< my _ new _ learner >\")."}, {"heading": "The end", "text": "Further details can be found in the parallelMap tutorial and in the help."}, {"heading": "Visualization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Generation and plotting functions", "text": "The visualization functions of mlr are based on generation functions that generate data for diagrams and plotter functions that represent this output with either ggplot2 or ggvis (the latter is currently experimental).This separation allows the user to easily create custom visualizations by using the generation functions. \u2022 The only data transformation that is performed within the plotter functions is the transformation, the redesigned data is also accessible by calling the plotter functions and then extracting the data from the ggplot object. \u2022 The functions are named accordingly. \u2022 Names of the generation functions start with generate and are followed by a title case description of their function, followed by Data, i.e. generated FunctionPurposeData. These functions output objects of the class FunctionPurposeData. \u2022 Plotter functions are preceded by plotter functions, followed by their purpose, i.e. plotctionpose GPOSCIS \u2022 GPOSTERVIS have additional plotter functions."}, {"heading": "Some examples", "text": "In the following example, we create a graph of classification performance as a function of the decision threshold for the binary classification problem sonar.task. The generateThreshVsPerfData function creates an object of the class ThreshVsPerfData, which generates the data for the diagram in the slot $data.lrn = makeLearner (\"classif.lda,\" predict.type = \"prob\") n = getTasktype Size (sonar.task) mod = train (lrn, task = sonar.task, subset = seq (1, n, by = 2)) pred = prediction (mod, task = sonar.task, subset = seq (2, \"subtask seq = 2,\" subtask seq = 4q, \"pred = sonar.task # sonar.task,\" subtask seq = 4q, \"subtask seq (4q,\" subtask seq = 4generby), \"n = 4q,\" pred, \"pred = son.task # son.task seq,\" (4q, \"subtask seq = 4q,\")."}, {"heading": "Customizing plots", "text": "It is possible that it will take too long for there to be a'margin of error', as they say. 'It is very likely that the pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo-pseudo"}, {"heading": "Advanced", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Configuring mlr", "text": "mlr is designed to make usage errors due to typos or invalid parameter values as unlikely as possible > > > Occasionally you want to break through these barriers and get full access, for example to reduce console output or disable checks. For all available options, simply refer to the documentation provided by configureMlr. Below, we show some common use cases. In general, the functional configuration # can be # configured # # # # # # # configuration # for your current R session. \u2022 All options related to student behavior (these are all options except show.info) can be # configured for a single student. # # Configuration value # # # # # # # # # Configuration # The local precedes the global configuration. \u2022 Some functions such as Resample, Benchmark, selectFeatures, tuneParams, and tuneParamsMultiCrit have a show.info flag that controls progress messages when displayed."}, {"heading": "Accessing and resetting the configuration", "text": "Paradoxically, this is a paradox which is a paradox which is able to paradox the paradox-paradox-paradox-paradox-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxia-paradoxon-paradoxia-paradoxon-paradoxia-paradoxon-paradoxon-paradoxia-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-on-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-paradoxon-on-on-paradoxon-paradoxon-paradoxon-paradoxon-on-on-paradoxon-paradoxon-on-on-paradoxon-paradoxon-"}, {"heading": "Wrapper", "text": "The wide range of operations and methods implemented as wrappers underscores the flexibility of the wrap approach: \u2022 Data preprocessing \u2022 Imputation \u2022 Bagging \u2022 Tuning \u2022 Feature selection \u2022 Cost sensitive classification \u2022 Over- and subsample testing for unbalanced classification problems \u2022 Multi-class extension for binary learners \u2022 Multi-label classification All these operations and methods have some common features: First, they encompass all mlr learners and they bring back a new learner. Therefore, learners can be wound multiple times. Second, they are implemented and predicted using a pull method (pre-model hook)."}, {"heading": "Example: Bagging wrapper", "text": "This year it is so far that it will be able to leave the country, \"he said."}, {"heading": "Data Preprocessing", "text": "As a matter of fact, most of us are able to embark on the search for new ways that they are following, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to travel the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world in order to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world, to save the world"}, {"heading": "Preprocessing with makePreprocWrapperCaret", "text": "In fact, it is as if it were the kind of thing that would have happened in the last few years in most of the other countries of the world, 'he said, adding:' It's the kind of thing that's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years and it's going to happen in the next few years. '."}, {"heading": "Specifying the train function", "text": "The traction function must be a function with the following arguments: \u2022 data is a data.frame with columns for all features and the target variable. \u2022 target is a string and denotes the name of the target variable in the data. \u2022 args is a list of other arguments and parameters that affect the preprocessing. It must return a list with the elements $data and $control, in which $data is the preprocessed record and $control stores all the information required to pre-process the data before the prediction. The traction function for the scaling example is given below. It calls scale on the numerical characteristics and returns the scaled training data and the corresponding scaling parameters. # args contains the central and scaling arguments of the function scale and slot $control stores the scaling parameters that are specified in the prediction stage.With regard to the latter indication that the center and scaling arguments of the scaling of the scaling may be either a numerical value or the number of the number of the vector."}, {"heading": "Specifying the predict function", "text": "The prediction function has the following arguments: \u2022 data is a data.frame containing only attribute values (as with the prediction, of course, the target values are not known) \u2022 target is a string indicating the name of the target variable. \u2022 args are the args passed to the train function. \u2022 control is the object returned by the train function. It returns the preprocessed data. In our scaling example, the prediction function scales the numeric attributes using the parameters stored in control.predictfun = function (data, destination, args, control) from the training phase. predictfun = function (data, args, control) {# # Identify numeric attributes cns = column name (data) nums = cns [sapply (data, is.numeric)] # # Extract numeric attributes from the dataset and call scale x = as.matrix (data [, numbers, FALdata, FALDATA = #), combine data (FALdiSE =), ($)"}, {"heading": "Creating the preprocessing wrapper", "text": "Below is a pre-processing wrapper with a regression of neural network parameters (which itself does not have a scaling option = > > Scale = > Scale = > Scale = > Scale = > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - > Scale - # Scale - # Scale - # # Scale # # Scale # n, \"Scale - # Scale - # S.\""}, {"heading": "Preprocessing wrapper functions", "text": "If you have written a preprocessing wrapper that you would like to use from time to time, it is a good idea to encapsulate it into a separate function as shown below. If you believe that your preprocessing method is something others would like to use and should be integrated into mlr, just contact us. makePreprocWrapperScale = function (learner, center = TRUE, scale = TRUE) {trainfun = function (data, target, args = list (center, scale)) {cns = colnames (data) numbers = setdiff (cns [sapply (data, is.numeric)], target) x = as.matrix (data [, numbers, drop = FALk.prognos) x = scale (x, center = args $center = args $scale) control = (args if (is.logical control ($center) & control $atta.seta.s), control $center = att.WALkecontrol, prognoam center = att.center = att.s"}, {"heading": "Imputation of Missing Values", "text": "mlr provides several imputation methods listed on the Imputations help page, including standard techniques such as imputation by a constant value (such as a fixed constant, mean, median, or mode) and random numbers (either from the empirical distribution of the feature to be considered or from a particular distribution family), as well as predicting the missing values in a feature by a supervised learner integrated in mlr. If your preferred option is not yet implemented in mlr, you can easily create your own implementation method. Also, note that some of the learning algorithms included in mlr can deal with missing values in a meaningful way, i.e., unlike simply deleting observations with missing values. Those learners have the \"referrals\" property and can therefore be identified with the help of ListLearners."}, {"heading": "Imputation and reimputation", "text": "You can specify an imputation method for each characteristic individually or for classes of characteristics such as numbers or numbers. You can also create dummy variables that indicate that values are missing, either for classes of characteristics or for individual characteristics, which allow you to identify the patterns and reasons for missing data and treat the values differently. # Let's have a look at the air quality data (air quality). # > Ozone Solar.R Wind Temp # > Min.: 1.00 Min.: 1.700 Min."}, {"heading": "Generic Bagging", "text": "One reason why random forests do so well is that they use dredging as a technique to gain more stability = > But why limit yourself to the classifiers that are already implemented in known random forests, when it is really easy to build your own with mlr? The subsets are selected according to the parameters that makeBaggingWrappers: \u2022 bw.iters How many subsets (samples) do we want to train our learners on? \u2022 bw.replace sample with replacement (also known as bootstrapping)? \u2022 bw.size Percentage size of samples. If bw.replace = TRUE, bw.size = 1 is the default. (Samples) This does not mean that all observations occur as observations in each sample.rn"}, {"heading": "Changing the type of prediction", "text": "\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf"}, {"heading": "Advanced Tuning", "text": "In fact, it is the case that most of them are able to move to another world, in which they are able to integrate, \"he told the Deutsche Presse-Agentur.\" It is not as if they are able to move to another world, \"he told the Deutsche Presse-Agentur in an interview with the\" Welt am Sonntag. \""}, {"heading": "Multi-criteria evaluation and optimization", "text": "In the following example, we want to minimize both false positive and false negative rates (fpr and fnr). We re-calibrate the hyperparameters of an SVM (function ksvm) with a radial base core and use the sonar classification task for illustration. As a search strategy, we choose a random search engine. # For all available multi-criterion tuning algorithms, see TuneMultiCritControl.ps = makeParamSet (makeNumericParam (\"C,\" lower = -12, upper = 12, trafo = function (x) 2 ^.NumericParam (\"sigma\") lower = -12, upper #.12, trafo function (x).328000000S = makeTuneMultiCritlRandom (maxit = 30L) rdesc = makeResampleDesc."}, {"heading": "Feature Selection", "text": "The method of extracting a subset of relevant characteristics is called feature selection. Feature selection can improve the interpretability of the model, speed up the learning process and improve learning performance. mlr supports filtering and wrapper methods, and there are various approaches to identify the relevant characteristics."}, {"heading": "Filter methods", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with\" Welt am Sonntag. \""}, {"heading": "Wrapper methods", "text": "Wrap methods use the performance of a learning algorithm to evaluate the usefulness of a feature set. To select a feature subset, a learner is repeatedly trained on different feature subsets and the subset that leads to the best learning performance is selected. To apply the wrapper approach, we must decide: \u2022 How to assess performance: To do this, a performance metric is selected that serves as a feature selection criterion and resampling strategy. \u2022 Which learning method to use. \u2022 How to search the space of possible feature subsets. \u2022 The search strategy is defined by functions that follow the naming convention makeFeatSelControl < search _ strategy. The following search strategies are available: \u2022 Exhaust search (makeFeatSelControlExhaustive), \u2022 Genetic algorithm (makeFeatSelControlGA), \u2022 Random search (makeFeatSelRandom), \u2022 Seterministic search (forward or backward control)."}, {"heading": "Select a feature subset", "text": "\"This is the biggest plague that has ever existed,\" he said. \"This is the biggest plague that has ever existed.\" \"This is the biggest plague that has ever existed.\" \"This is the biggest plague that has ever existed.\" \"The biggest plague that has ever existed,\" he said, \"is the biggest plague that has ever existed.\" \"The biggest plague that we have ever experienced.\" \"The biggest plague that we have ever experienced.\" \"The biggest plague that we have ever experienced is the plague.\" \"The biggest plague that we have ever experienced is the plague.\" \"The biggest plague that we have ever experienced is the plague.\" \"The biggest plague that we have ever experienced.\" \"\" The biggest plague, the biggest plague, the biggest plague, the biggest plague, the biggest plague, the biggest plague, the biggest plague, the biggest plague. \""}, {"heading": "Nested Resampling", "text": "In this case, it is a case of reactionary behaviour, which is able to play by the rules."}, {"heading": "Tuning", "text": "As you may remember from the Tuning tutorial page, you need to define a search space with the function makeParamSet = > (SO = > SO = > SO = > SO = = search strategy of makeTuneControl * and a method to evaluate the hyperparameter settings (i.e. the internal resampling strategy and a performance measurement). Here is an example of classification: We evaluate the performance of a support vector machine (ksvm) with the tuned # cost parameters # C and # RBF kernel parameters sigma. We use triple cross-validation in the outer loop and subsampling with 2 iterations in the inner loop. For tuning, a grid search is used to find the hyperparameters with the lowest error rate # # amplep # (mmce is the default measurement for the classification)."}, {"heading": "Accessing the tuning result", "text": "In fact, there are only a very limited number of people who are able to play by the rules, who are able to play by the rules, who are able to play by the rules and who are able to play by the rules."}, {"heading": "Feature selection", "text": "As you may recall from the Feature Selection section, mlr supports the filter and wrapper approach."}, {"heading": "Wrapper methods", "text": "To select a feature subset, you must select a search strategy (function makeFeatSelControl *), a performance metric, and the internal resampling strategy, then use function makeFeatSelWrapper to link everything together.In the following, we will use sequential forward selection with linear regression on the Boston Housing dataset (bh.task). # # Feature selection in inner resampling loop # makeResampleDesc (\"CV,\" iters = 3) lrn = makeFeatSelWrapper (\"regr.lm,\" resampling = inner, control = makeFeatSelControlSequential (method = \"sfs\"), iters = lrn = makeFeatSelWrapper (\"regr.lm,\" ressampling = inner, control = makeFeatSelControlSequential \")."}, {"heading": "Accessing the selected features", "text": "The result of the feature selection can be obtained from the function getSelResult # > > Feature selection of models = TRUE when calling resample. r $extract # > FeatSel result: # > FeatSel result: # > FeatSel result: # > FeatSel result: # 0 FeatSe, rm, rad, ptratio, lstat # > mse.mean = 20,2 # > FeatSel result: # 0 FeatSel result: # 1, nox, rm, rad, ptratio, ptratio # 0 # 2 #.teptratio #, lstat # mse.mean = 20,2 # > FeatSel result: 0 # 0 FeatSe, nox 0 #.4 # 0, rsasD: 0 #.4 #.4"}, {"heading": "Benchmark experiments", "text": "In a \"benchmark\" experiment, multiple learners are compared to one or more benchmarks (see also the section on benchmark data). \"We can also apply different internal resampling strategies for different learner strategies.\" (\"It might be more practical to use less subsampling or bootstrap iterations for slower learners.If you use larger benchmark strategies for the section on parallelization.As.\" As mentioned in the section on benchmark experiments, you can also apply different resampling strategies for different learning tasks."}, {"heading": "Cost-Sensitive Classification", "text": "In regular classification, the goal is to minimize the misclassification rate and thus consider all types of misclassification errors to be equally serious. In the case of class-related costs, the cost depends on the true and predicted class designation. The costs c (k, l) for predicting class k, if the true label is l, are usually organized in a K \u00d7 K cost matrix, where K is the number of classes. Of course, the cost of predicting the correct class designation y is assumed to be minimal (i.e. c (y, y) \u2264 c (k, y) for all k = 1,..., K).Another generalization of this scenario is example-related misclassification costs, where each example (x, y) is coupled with an individual cost vector of length K. Its k component depresses the cost of assignment to the class s.s.s.Another generalization of this scenario is example-related misclassification costs, which do not depend on the actual cost of actual misclassification in the actual cost of the actual classification."}, {"heading": "Class-dependent misclassification costs", "text": "There are some classification methods that can take misclassification costs directly into account. One example is rpart.Alternatively, we can use cost-insensitive methods and manipulate the predictions or training data to account for misclassification costs. mlr supports thresholds and reweighting. 1. Thresholding: The thresholds used to convert posterior probabilities into class identifiers are chosen to minimize costs. This requires a learner to be able to predict posterior probabilities. 2. Rebalancing: The idea is to change the ratio of classes in the training dataset to take into account costs during the training, either by weighting or sampling. Rebalancing does not require that the learner can predict probabilities. i. For weighting, we need a learner to support class weights or observation weights. iii. If the learner cannot handle weights, the proportion of classes may be over- and undervalued."}, {"heading": "Binary classification problems", "text": "The positive and negative classes are labeled 1 and \u2212 1 respectively, and we consider the following > > > Cost Matrix > > Costs > > Costs where the lines indicate true classes and predicted classes: true / pred. + 1 \u2212 1 + 1 c (+ 1, + 1) c (\u2212 1, \u2212 1) c (\u2212 1, \u2212 1) c (\u2212 1) Often the diagonal entries zero or the cost matrix # are switched to # zeros in the diagonal # (see, for example, O'Brien et al, 2008).A well-known cost-sensitive classification problem is raised by the German credit record (see also the UCI Machine Learning Repository).The corresponding cost matrix (although Elkan (2001) argues that this matrix is economically unreasonable) is called: true / pred. Bad Good Bad Bad Bad Task 0 5 Good Tlex, Good Tmaxes in the table above, the rows indicate true and the cost-dependent matrix is sufficient in case the cost-class matrix is generic."}, {"heading": "1. Thresholding", "text": "We start with the adjustment of a logistic regression model to the German credit data and forecast posterior probabilities = > costs = > costs = > cost increase for the rear probabilities lrn = makeLearner (\"classif.multinom,\" forecast wave = > cost threshold = > cost threshold = > cost adjustment for the rear probabilities lrn = makeLearner (\"klassif.multinom,\" forecast wave = > threshold = > cost threshold for the train (lrn, credit.task) pred = predict (mod, task = credit.task) pred # > predictive power: 1,000 observations # > predictive power: 1,000 threshold value: bath = 0.50, Good = 0.50 # > time: 0.01 # > id truth prob.Good response # > 1 Good 0.03525092 0,9647491 Good # 2 prediction: 2 bath 0.6222363 0,3677764 bathroom # 3 bathroom 0.0714 Good probability 2897437414 # Good 37485."}, {"heading": "2. Rebalancing", "text": "This year it has come to the point where it will be able to do the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things for the aforementioned things."}, {"heading": "Multi-class problems", "text": "We look at the waveform dataset from the mlbench package and add an artificial cost matrix: true / pred. 1 2 3 1 0 30 80 2 5 0 4 10 8 0We start creating the task, the cost matrix and the corresponding performance measure. # # # Task df = mlbench:: mlbench.waveform (500) wf.task = makeClassifTask (id = \"waveform,\" data = as.data.frame (df), target = \"classes\") # # Cost matrix costs = matrix (c (0, 5, 10, 30, 0, 8, 80, 4, 0), 3) colnames (costs) = getTaskClassLevels (wf.task) # # Performance matrix costs = makeCostMeasure (id = \"wf.costs,\" name = \"Waveformcosts\" CokeClassLevels costs (wf.task) Costf.task wriure = Performance matrix."}, {"heading": "1. Thresholding", "text": "Given a vector of positive thresholds, as long as the number of classes K is predicted, the predicted probabilities = > > Cost for all classes are divided by the corresponding threshold, then the class with the highest adjusted probability is predicted, thus, as in the case of makeLearner, classes with a low threshold are preferred over classes with a higher threshold. Note that the threshold must have names that match the class labels.lrn = makeLearner (\"classif.rpart,\" Prediction.type = \"prob\") # prob # #.threshold # option of makeLearner (\"CV,\" iters = 3, Task = wf.task) r = resample (lrn, wf.task, rin, measures = list (wf.costs, mmce), show.info = FALSE # > Resample Result # > Task: waveform # > means: > Costs = > Round.11 = > Costs."}, {"heading": "2. Rebalancing", "text": "i. WeightIn the multi-level case, you must pass a vector of weights as long as the number of classes K is used for the function makeWeightedClassesWrapper. The weight vector can be used with the function tuneParams.lrn = makeLearner (\"classif.multinom,\" trace = FALSE) lrn = makeWeightedClassesWrapper (lrn) ps = makeParamSet (makeNumericVectorParam (\"wcw.weight,\" len = 3, lower = 0, upper = 1)) ctrl = makeTuneControlRandom () tune.res = tuneParams (lrn, wf.task, resampling = rin, par.set = ps, measures = list (wf.costs, mmce), control = ctrl, show.info = FALSE) tune.res # > Tune result: # > Op. Parameter: wcw.weight = 0,820,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,000,00"}, {"heading": "Example-dependent misclassification costs", "text": "This year it is more than ever before."}, {"heading": "Imbalanced Classification Problems", "text": "In binary classification, highly unbalanced classes often lead to unsatisfactory results in terms of predicting new observations, especially for the small class. In this context, unbalanced classes simply mean that the number of observations of one class (e.g. positive or majority class) far exceeds the number of observations of the other class (e.g. negative or minority class). This attitude can be observed quite frequently in practice and in various disciplines such as credit rating, fraud detection, medical diagnostics or waste management. Most classification methods work best when the number of observations per class is roughly the same. The problem with unbalanced classes is that due to the dominance of majority classifiers tend to ignore cases of minority class as noise and therefore predict the majority class much more frequently. In order to give more weight to minority class cases, there are numerous correction methods that can address the problem of unbalanced classification in general, cost-based and amplification methods."}, {"heading": "Sampling-based approaches", "text": "The basic principles of the sampling methods are to simply adjust the ratio of the classes to each other in order to increase the weight of the individual classes. (>) The sampling-based approaches can be further divided into three different categories: 1. sampling methods to reduce their effects on the classification, 2. sampling methods, 3. sampling strategies, 3. sampling methods, 4. sampling methods, 5. sampling methods, 6. sampling methods, 6. sampling methods, 6. sampling methods, 6. sampling methods, 7. sampling methods, 7. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 9. sampling methods, 9. sampling methods, 7. sampling methods, 7. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling, 8. sampling methods, 8. sampling methods, 8. sampling methods, 9. sampling methods, 9. sampling methods, 9. sampling methods, 9. sampling methods, 9. sampling methods, 7. sampling methods, 7. sampling methods, 7. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods, 8. sampling methods and methods, 8. sampling methods and methods, 8. sampling-methods, 8. sampling-methods and methods of sampling-sampling-sampling-methods and methods, and methods, and methods of sampling-sampling-methods."}, {"heading": "Over- and undersampling wrappers", "text": "Alternatively, mlr also offers the integration of over- and undersampling using a wrapper approach. In this way, oversampling and undersampling can be applied to existing learners in order to extend their functionality. The above example is repeated once more, but this time with extended learners instead of modified tasks (see makeOversampleWrapper and makeUndersampleWrapper). Just like before, the undersampling rate must be between 0 and 1, while the oversampling rate must be a lower limit of 1. lrn.over = makeOversampleWrapper (lrn, osw.rate = 8) lrn.under = makeUndersampleWrapper (lrn, usw.rate = 1 / 8) mod = train (lrn, task) mod.over = train (lrn.over, task) mod.under mod.under = train (lrn.rate = 8) mod.rn.300.rn.train = (.rn.3mmlbs) (lrn, usw.rate = 1 / 8) morn morn = task mod.ltrain, (l00.train = 5000.rn.train) (.rn.3mmlover) > (.rn.3mmlb = 5000.train)."}, {"heading": "Extensions to oversampling", "text": "Two extensions for (simple) oversampling are available in mlr."}, {"heading": "1. SMOTE (Synthetic Minority Oversampling Technique)", "text": "Because duplication of minority-class observations can lead to overmatch, the \"new cases\" are constructed differently within SMOTE, eventually creating a new artificial minority-class observation. Smote in mlr handles both numerical and factor characteristics, since the distance of the visitor is used to calculate the nearest neighbors; the factor level of the new artificial case is scanned by the default levels of the two input observations; similarly to oversampling, SMOTE pre-processing is possible by changing the task.task.smote = smote (task, rate = 8, nn = 5) table (getTaskTargets (task) # > smc (task) # > A B # > 100 5000table (getTask.smote)."}, {"heading": "2. Overbagging", "text": "rE \"s guides for the guides for the guides for the guides for the guides.\" rF \"for the guides,\" so rF \"s guides for the guides for the guides for the guides for the guides for the guides for the guides for the guides for the guides for the guides for the guides for the guides.\" rF \"for the guides for the guides.\" rF \"iDe\" for the guides for the guides for the guides. \"rF\" for the guides for the guides. \"rF\" for the guides for the guides. \"rF\" iDe \"for the guides for the guides for the guides.\" rF \"for the guides for the guides.\" rF \""}, {"heading": "Cost-based approaches", "text": "In contrast to the sample, cost-based approaches usually require certain learners who can handle different class-related costs (cost-sensitive classification)."}, {"heading": "Weighted classes wrapper", "text": "Another approach, independent of the underlying classifier, is to allocate the costs as class weights, so that each observation is weighted according to the class to which it belongs. Similar to sample-based approaches, the effect of minority class observations can be increased simply by increasing the weight of these cases, and vice versa for majority class observations. In this way, any student who supports weights can be augmented by the wrapper approach. If the student does not have a direct parameter for class weights but supports observation weights, the weights are determined internally in the wrap depending on the class. lrn = makeLearner (\"classif.logreg\") wcw.lrn = makeWeightedClassesWrapper (lrn, wcw.weight = 0.01) In binary classification, the single number passed to the classifier corresponds to the weight of the positive / majority class, while the negative / minority class has a weight of 1."}, {"heading": "ROC Analysis and Performance Curves", "text": "It is the only way in which we show the actual positive rate (receiver ratios such as elevator charts or recalls) at the vertical level. In addition to performance visualization ROC curves, we are helpful in determining an optimal decision threshold for predetermined class probabilities and misclassification costs (for alternative pages about cost-sensitive classification and imbalanced classification problems in this tutorial), \u2022 identifies the regions where a classified forms other and appropriate multi-classification systems, see also the pages on cost-sensitive classification and imbalanced classification problems in this tutorial."}, {"heading": "Viper charts", "text": "mlr also supports ViperCharts for displaying ROC and other performance curves. Like generateThreshVsPerfData, it has S3 methods for objects of the class Prediction, ResampleResult, and BenchmarkResult. Below the diagrams for the benchmark experiment (Example 2), diagrams are generated.z = plotViperCharts (bmr, chart = \"rocc,\" browse = FALSE) You can see the diagram created in this way here. Note that in addition to ROC curves, you get several other diagrams such as lift diagrams or cost curves. For details, see plotViperCharts."}, {"heading": "Multilabel Classification", "text": "Multi-label classification is a classification problem where multiple target labels can be assigned to each observation, rather than just one as in multi-label classification. There are two different approaches to multi-label classification. Problem transformation methods attempt to convert multi-label classification into binary or multiclass classification problems. Algorithm adaptation methods adapt multi-label algorithms so that they can be applied directly to the problem."}, {"heading": "Creating a task", "text": "The first thing you have to do for the multi-label classification in mlr is to get your data in the right format. You need a data frame consisting of the features and a logical vector for each label, which indicates whether the label exists in the observation or not. Afterwards, you can create a multi-labelTask like a normal ClassifTask. Instead of a target name, you have to specify a vector of targets, which corresponds to the name of logical variables in the database. In the following example, we get the yeast data frame from the already existing yeast. Task, extract the 14 label names and recreate the task. Heft = getTaskData (yeast.task) Labels = column name (yeast) [1: 14] Label.task = makeMultilabelTask: (id = \"multi,\" data = Yeast, data = labels, target = labels) yeast.task # > Labels #, \"Labels: Labels: Column name (yeast): [1: 14] LabelTask =: MultilabelTask: MultilabelTask =: Task: (id =: Yeast, data = labels, data =: Labels) yeast.task =: yeast.task #,\" Labels: Labelget: Labels: yeast.task #, \"Labels:"}, {"heading": "Constructing a learner", "text": "Multi-label classification in mlr can currently be done in two ways: \u2022 Algorithm customization methods: Treat the entire problem with a specific algorithm. \u2022 Problem transformation methods: Transform the problem so that simple binary classification algorithms can be applied."}, {"heading": "Algorithm adaptation methods", "text": "The currently available algorithm customization methods in R are the multivariate random forest in the randomForestSRC package and the multilabel random fern algorithm in the rFerns package. You can problems.lrn.rfsrc = makeLearner (\"multilabel.randomForestSRC\") lrn.rFerns = makeLearner (\"multilabel.rFerns\") lrn.rFerns # > Learner multilabel.rFerns from the package rFerns # > Type: multilabel # > Name: Random Ferns; Abbreviation: rFerns # > Class: multilabel.rFerns # > Properties: numerical, factors, ordered # > Predict-Type: response # > Hyperparameters:"}, {"heading": "Problem transformation methods", "text": "To create a packaged multilabel learner, first create a binary (or multiclass) classifier with makeLearner, then apply a function such as makeMultilabelBinaryRelevanceWrapper, makeMultilabelClassifierChainsWrapper, makeMultilabelNestedStackingWrapper, makeMultilabelDBRWrapper, or makeMultilabelStackingWrapper to the learner to transform him into a learner using the appropriate problem transformation method. you can also create a binary relevance learner directly, as you can see in the example. lrn.br = makeLearner (\"classif.rpart,\" predict.type = \"prob\") lrn.br = makeMultilabelBinaryRelevanceWrapper (lrn.br) Properties-label.br # > labellabel.label.relative # multilabelclassifie.multilabel.br > Class Type: > > Classified.br: > Class Type: > > Wrf: > Multilabel Classified.br."}, {"heading": "Binary relevance", "text": "This method of problem conversion converts the multi-label problem into binary classification problems for each label and applies a simple binary classifier to them. In mlr, this can be done by converting your binary learner into a wrapped multi-label learner with binary relevance."}, {"heading": "Classifier chains", "text": "Continuously trains the labels with the input data. In each step, the input data is supplemented by the already trained labels (with the values actually observed) by specifying a sequence of the labels. At the prediction time, the labels are predicted in the same order as during the training. The required labels in the input data are given by the previously performed prediction of the respective label."}, {"heading": "Nested stacking", "text": "Like classification chains, but the labels in the input data are not the real ones, but estimates of the labels received by the already trained learners."}, {"heading": "Dependent binary relevance", "text": "Each label is trained with the real-world observational values of all other labels. In the predictive phase for a label, the other necessary labels are determined in a previous step by a basic teacher such as the binary relevance method."}, {"heading": "Stacking", "text": "The same as the dependent binary relevance method, but in the training phase, the labels used as input for each label are determined by the binary relevance method."}, {"heading": "Train", "text": "You can train a model as usual with a multi-label learner and a multi-label task as input. You can also pass subset and weighting arguments if the learner supports them.mod = train (lrn.br, yeast.task) mod = train (lrn.br, yeast.task, subset = 1: 1500, weights = rep (1 / 1500, 1500))) mod # > Model for learner.id = multilabel.classif.rpart; learner.class = MultilabelBinaryRelevanceWrapper # > Trained on: task.id = multi; obs = 1500; features = 103 # > Hyperparameter: xval = 0mod2 = train (lrn.rfsrc, yeast.task, subset = 1: 100) mod2 # > Model for learner.id = multilabel.randomForestSRC; ner.class = multilabel.100 ForestRnaid # multinaid =: ="}, {"heading": "Predict", "text": "As always, you can specify a subset of the data to be predicted (mod, task = yeast.task, subset = 1: 10) pred = predict (mod, newdata = yeast [1501: 1600,]) names (as.data.frame (pred)) # > [1] \"truth.label1\" \"\" truth.label2 \"labellabel3\" # [4] \"labellabellabel3\" # labellabel.labelabel5 \"\" truth.labellabellabellabel3 \"\".labellabellabellabel.label.label.....el.label.....el.label.labellabellabel.labellabellabel.labellabel.label.label.label.label.label.label.label.label.label.label.label.label.label.label.label.label.label.label.label.label.labse.label.label.label.labse...........el.label.label.label.label.label.label.label.label.label.label.label.label.label.label.lab.label.lab.label.label.label.label.lab.label.label.lab.label.lab.label.lab.lab...label.label...label.....el.label.label.label....el.label.label.label.label.label.label.label.label.label.label.label.label.label.lab.label.label.lab.lab.lab.lab.label.label.lab.lab.lab.lab.lab.lab...lab...lab...lab...lab...lab.....lab....el..el.el..el.el...el..el....el..el...el...el.."}, {"heading": "Performance", "text": "All available metrics for multilabel classification can be represented by ListMeasures and found in the table of performance metrics and the measurement documentation page. Performance (pred) # > multilabel.hamloss # > 0.2257143performance (pred2, measures = list (multilabel.subset01, multilabel.hamloss, multilabel.acc, multilabel.f1, timeforecast) # > multilabel.subset01 multilabel.acc # > 0.8663633 0.2049471 0.4637509 # > multilabel.f1 timeforecast # > 0.5729926 1.0800000listMeasures (\"multilabel.hamloss multilabel.acc\") # > [1] \"multilabel.label.ppel.el.el.el.el7\""}, {"heading": "Resampling", "text": "To evaluate the overall performance of the learning algorithm, you can perform some resample.As usual, you need to define a resamplestrategy, either using makeResampleDesc or makeResampleInstance, and then you can execute the resample function. Below the standard measurement, the hamming loss is calculated.rdesc = makeResampleDesc (Method = \"CV,\" Stratify = FALSE, iters = 3) r = resample (Learner = lrn.br, task = yeast.task, resampling = rdesc, show.info = FALSE) r # > Resample Result # > Task: multi # > Learner: multilabel.classif.rpart # > Aggr perf: multilabel.hamloss.test.mean = 0.225 # > Runtime: 4.2915r = resample (Learner = lrn.rFerns, task = yeast.task, resampling = ham.show.info = RunALs.Resask _ Task: 439.Task #"}, {"heading": "Binary performance", "text": "If you want to calculate a binary performance measurement such as accuracy, mmce, or Auc for each label, you can use the getMultilabelBinaryPerformances function, which can be applied to any multi-label prediction, including the resample multilabel prediction. To calculate Auc, you need predicted probabilities. getMultilabelBinaryPerformances (pred, measures = list (acc, mmce, auc) # > acc.test.mean mmce.test.mean auc.test.mean # > label1 0.75 0.25 0.6321925 # > label2 0.64 # 0.36 # 6547917 # > label3 0.68 0.32 # > label4 0.7118227 # > label4 0.31 0.676480.335 # > label5 > labelel.mean # label.5 > labelelelel.lab # label3 label.label.label.label.label6 # labellabellab labellab.lab lablablablablablablablablab.lab # labellabellabel6 # labellabel.labelprint # label.label.print # label.print # label.print >.4 # 4, label.label.4 # 4,.label.label.4 # 4 # 4 # 4,.label.label.4 # 4,.label.label.4 # 4 # 4,.label.label.4 # 4"}, {"heading": "Learning Curve Analysis", "text": "To analyze how the increase in observations in the training set improves a learner's performance, the learning curve is a suitable visual tool. The experiment is performed as the sample size increases and performance is measured. In the diagram, the x-axis represents the relative sample size, while the y-axis represents performance. Note that this function internally uses benchmark in combination with makeDownsampleWrapper, so that new observations are drawn for each run, so the results are noisy. To reduce noise, increase the number of repeat samplings. You can define the repeat method in the resampling argument of generateLearningCurveData. It is also possible to pass a ResampleInstance (which is a result of makeResampleInstance) to make the repeat sampling consistent for all passed learners and each step to increase the number of observations."}, {"heading": "Plotting the learning curve", "text": "In fact, it's a way of being able to play by the rules that you've imposed on yourself. \"It's not like playing by the rules,\" he says. \"But it's not like playing by the rules.\" \"It's not like playing by the rules,\" he says, \"but it's like playing by the rules.\" \"It's not like playing by the rules.\" \"It's like playing by the rules.\" \"It's not like playing by the rules.\""}, {"heading": "Exploring Learner Predictions", "text": "Students use a prediction function and make predictions, but the effects of these properties are often unpredictable. (mlr) They can estimate the partial dependence of a learned function on a subset of attribute space located in a subset of subspace. (c) They can estimate the dependence on a possible high-dimensional function, and they can indicate a marginalized version of this function in a subdimensional space. (c) They can estimate the relationship between a subset of Xs and a subset of Xs (X, Y) pairs drawn independently of this statistical model. (c) They can estimate the high-dimensional capabilities of Xs, which cannot be interpreted. (c) They can estimate the relationship between a subset of Xs, Xs and Xs, Xs and Xc, so that they can expect such a subset of Xs. (c)"}, {"heading": "Generating partial dependences", "text": "Our implementation, following the visualization, consists of the above mentioned function > > 32,000 data, as well as two visualization functions, plotPartialDependence and plotPartialDependenceGGVIS. The former generates input (objects of the class PartialDependenceData) for the latter. The first step executed by generatePartialDependenceData is to create a feature grid for each element of the character properties #.The data is given by the input argument that a task or a data.frame. The function can be generated in several ways."}, {"heading": "Functional ANOVA", "text": "It is. It is. (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"It is.\" (It is.) \"(It is.\" It is. \"(It is.)\" (It is. \"It is.\" (It is.) \"It is.\" (It is.) \"(It is.\" It is. \"(It is.)\" (It is. \"It is.\" It is. \"(It is.)\" (It is. \"It is.\" It is. \"(It is.)\" (It is. \"It is.\" (It is.) \"(It is.\" It is. \"(It is.\" It is. \")\" (It is. \"It is.\") \"(It is.\" It is. \"It is.\" (It is. \"It is.\" It is. \")\" (It is. \"It is.\" It is. \"(it.\" It is. \"It is.\" It is. \")\" (It is. \"It is.\") \"(It is.\" It is. \"(it is.\" It is. \")\" (It is. \"it is.\" (it is. \")\" (it is. \"it is.\" it. \")\" (It is. \"(it is.\" it is. \"It is.\") \"(it is.\" (it is. \")\" (it is. \"it.\" it is. \")\" (it is. \"it is.\") \"(it is. (\" it is. \"it.\" it is. \")\" (it is. \")\" (it is. (it is. \"it is.\" it is. \")\" (It is. \"it is.\" it. \")\" (it is. (it is. \"it is.\" it is. \")\" it is. \""}, {"heading": "Plotting partial dependences", "text": "\u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf"}, {"heading": "Classifier Calibration", "text": "In fact, most of them will be able to abide by the rules that they have applied in practice."}, {"heading": "Evaluating Hyperparameter Tuning", "text": "What is the meaning of the respective hyperparameters???????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????"}, {"heading": "Visualizing the effect of a single hyperparameter", "text": "In fact: \"It is not easy to save the world,\" according to the author, \"but it is not easy to save the world.\" (\"It is not easy to save the world.\") (\"It is not easy to save the world,\" according to the author, \"but it is the world.\" (\"It is the world.\") (\"It is the world.\") (\"It is the world.\") (\"It is the world.\") (\"It is the world.\") (\"It is the world.\") (\"It is the world.\") (\"It is the world.\" (\"It is the world.\") (\"It is the world.\") (\"It is the world.\" (\"It is the world.\") (\"It is.\" It is. \"It is.\" (\"It is.\" It is. \") (\" It is. \"It is.\" It is. \"(\" It is the world. \") (\" It is the world. \"(\" It is the world. \") (\" It is the world. \"It is the world.\" (\"It is the world.\" It is the world. \") (\" It is the world. \"It is the world.\") (\"It is the world.\" It is the world. \"(\" It is the world. \"It is the world.\") (\"It is the world.\" It is the world. \"(\" It is. \"It is.\" It is. \"It is.\" It is. \"It is.\" It is. \"It is.\" It is. \""}, {"heading": "Visualizing the effect of 2 hyperparameters", "text": "\"We have to deal with the question of how far we are able to fill the interpolation of the heatmap and the color lines, depending on how the interpolation is able to explore the interpolation,\" he said. \"We have to deal with the interpolation between the individual regions and the individual regions,\" he said. \"We have to deal with the interpolation, in order to investigate the interpolation between the individual regions,\" he said. \"We have to deal with the interpolation, in order to fill the interpolation of the heatmap or the color lines, depending on how the interpolation between the individual regions works.\" \"We need the interpolation between the individual regions.\" \"We do not need it\" in order to interpolate between the individual regions and the individual regions. \"\" We need the interpolation between the regions. \"\" We need the interpolation between the individual regions. \"\""}, {"heading": "Extend", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Integrating Another Learner", "text": "Indeed, most people are capable of following the rules. < / M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M > M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M\" M \"M"}, {"heading": "Classification", "text": "We show how the linear discrimination analysis from the MASS package was integrated into the classification of learner classif.lda in mlr as an example."}, {"heading": "Definition of the learner", "text": "The minimum information required to define a learner is the learner's mlr name, its package, the parameter sets and the set of characteristics of your learner. \"These are all options that can be set when it comes to changing how it is learned, how input is interpreted, how and what output is generated, and so on. mlr offers a number of functions to define parameters, a complete list can be found in the learner's documentation. In our example, we have discrete and numeric parameters, so we use makeDiscreteLearnam and makeNumericLearnam to integrate the full description of the parameters."}, {"heading": "Creating the prediction method", "text": "Finally, the prediction function has to be defined.The name of this function starts with predictLearner., followed by the mlr name of the learner. The prototype of the function is as follows: function (.learner,.model,.newdata,...) {} It has to predict for the new observations in data.frame.newdata with the wrapped model.model returned by the training function.The actual model that the learner has built is stored in the $learner.model member and can simply go through.model $learner.model.For the classification, a factor of the predicted classes has to be returned when predicting. $predict.type \"or a matrix of predicted probabilities is present. $predict.type\" prob \"is and this type of prediction is supported by the learner. In the latter case, the matrix has to show the same number of predicted classes as there are classes in the task and the columns have to foretell = = the name.DA = the exact foretays for this definition.Prediction = = = = the learner."}, {"heading": "Regression", "text": "The main difference in regression is that the type of predictions is different (numerical instead of labels or probabilities) = = Default = = = that not all properties are relevant. In particular, whether single-, two- or multi-class problems and subordinate probabilities are supported is not applicable. Apart from that, everything explained above applies. Below is the definition for the Earth learner from the Earth packet (makeLogicalLearnerParam): makeRLearner.earth = function (cl = \"regr.earth,\" packet = \"earth,\" par.set = makeParkeParkeParkeParkeParkeParkeParkeam, \"makeIntegre,\" makeIntegre, \"IntekeParke.Parke.ParkeInteke.makee,\" Inteke.Parkee, \"Inteke.Parkegre,\" Inteke.Parke. \""}, {"heading": "Survival analysis", "text": "For survival analysis, you need to return so-called linear predictors to calculate the default size for this task type = = = = Default value (for.learner $predict.type = = \"answer\") = = Default value = = = Default value = = = (for.learner $predict.type = = = = \"prob,\" there is (yet) no substantially meaningful measure of survival. You can either ignore this case or return something like predicted survival curves (see example below). There are three characteristics that are specific to survival learners: \"rcens,\" \"lcens\" and \"icens,\" the definition of the type (s) of censoring a learner may not work - right, left and / or interval censored.Let's take a look at how the Cox Proportional Hazard Model from the survival of a package was integrated into the survival learner. \""}, {"heading": "Clustering", "text": "For clustering, you must return a numeric vector with the IDs of the clusters to which the respective date has been assigned. Numbering should start at 1.Below, which is the definition for the FarthestFirst learner from the RWeka package. weka starts the IDs of the clusters at 0, so we add 1 to the predicted clusters. RWeka has a different way of setting learner parameters; we use the special Weka _ control function to do this. makeRLearner.cluster.FarthestFirst = function () {makeRLearnerCluster (cl = \"cluster.FarthestFirst,\" package = \"RWeka,\" par.set = makeParamSet (makeIntegerLearnerParam (id = \"N,\" default = 2L, \"lower),\" Farner.Param (id = \"): (id =\" S, \"default = 1L)."}, {"heading": "Multilabel classification", "text": "rE \"s guide for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green for the green."}, {"heading": "Registering your learner", "text": "If your interface code to a new learning algorithm only exists locally, i.e. it is not (yet) integrated in mlr or does not live in an additional package with a suitable namespace, you should register the new S3 methods to ensure that they are found by listLearners, for example. You can do this as follows: registerS3method (\"makeRLearner,\" \"awesome _ new _ learner _ class,\" \"makeRLearner.\" < awesome _ new _ learner _ class >) registerS3method (\"trainLearner,\" \"< awesome _ new _ learner _ class >,\" trainLearner. \"< awesome _ new _ learner _ class >) registerS3method (\" predictLearner, \"awesome _ new _ learner _ class >) < awesome _ awesome _ awesome _ new _ learner _ class > # And if you have also written a method to extract the meaning of the\" fire _ learner \"class\" > _ awesome _ learner."}, {"heading": "Integrating Another Measure", "text": "In some cases, you may want to evaluate a prediction or ResamplePrediction \"> > > > $> means.\" < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < < <"}, {"heading": "Constructing a performance measure", "text": "The function makeMeasure provides a simple way to construct your own performance measurement = = > Costs = = = = My Cost (= = = = = = Costs = = = Costs = = = = = Costs = = = = Costs = = = (= = = = = Costs = = = = = Costs = = = = = Costs = = = = = = Costs = = = = = = = = = Costs = = = = = (= = = = = = Costs = = = = = = = = = = = = Costs = = = = = = = = = = Costs = = = = = = = = = Costs = = = = = = = = = = Costs = = = = = = = = = = = = = = = Costs = = = = = = = = = = = = = = = = = Costs = = = = = = = = = = = Costs = = = = = = = = = = = Costs = = = = = = = = = = = = Costs = = = = = = = = = = = = = Costs = = = = = = = = = = = = = = = Costs = = = = = = = = = = = = = = = = = = = = =, \"and,\" and, \"and,\" and, \"and,\" and, \"and,\" and, \"and,\" and, \"and,\", \"and,\", \"and,\" and, \",\" and, \",\", \"and,\", \",\", \",\" and, \",\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \"\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \"\", \",\", \"\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \",\", \","}, {"heading": "Creating an aggregation scheme", "text": "For the first time in a long time, I have been able to speak to the people who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me, who work for me."}, {"heading": "Creating an Imputation Method", "text": "iSe rf\u00fc ide rf\u00fc the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for the rf for"}, {"heading": "Integrating Another Filter Method", "text": "Many feature filtering methods are already integrated into mlr and a full list can be found in the appendix or can be obtained with listFilterMethods. You can easily add another filter, be it a brand new one or a method that is already implemented in another package, using the makeFilter function."}, {"heading": "Filter objects", "text": "In mlr, all filter methods are objects of the filter class and are registered in an environment with the name.FilterRegister > > Functions (where listFilterMethods visits them to compile the list of available methods).To get to know their structure, let's take a closer look at the filter \"rank.correlation\" # > Packages: \"FSelector\" # > Supported Tasks: regr # > Supported Tasks: numericsstr (Filter $rank.correlation) Filter $rank.correlation # > Filter: \"rank.correlation\" # > Packages: \"FSelector\" # > Supported Tasks: numericsstr (Filter $rank.correlation) # > List of 6 # > $name: chr \"rank.correlation\" # > $desc: chr \"Spearman's correlation between task and target (ltname: numericsstr: numericsstr (filters $rankcork.correlation).correlation. # # ltname: Spearman's correlation."}, {"heading": "Writing a new filter method", "text": "The reason for this is that the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU Commission, the EU, the EU Commission, the EU Commission, the EU, the EU Commission, the EU, the EU Commission, the EU Commission, the EU, the EU Commission, the EU Commission, the EU, the EU Commission, the EU Commission, the Commission, the EU Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the EU, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission, the Commission,"}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Basics 5 Learning Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Task types and creation . . . . . . . . . . . . . . . . . . . . . . . 5 Further settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Accessing a learning task . . . . . . . . . . . . . . . . . . . . . . 10 Modifying a learning task . . . . . . . . . . . . . . . . . . . . . . 13 Example tasks and convenience functions . . . . . . . . . . . . . 15 Learners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 Constructing a learner . . . . . . . . . . . . . . . . . . . . . . . . 15 Accessing a learner . . . . . . . . . . . . . . . . . . . . . . . . . . 17 Modifying a learner . . . . . . . . . . . . . . . . . . . . . . . . . . 20 Listing learners . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Training a Learner . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Accessing learner models . . . . . . . . . . . . . . . . . . . . . . . 23 Further options and comments . . . . . . . . . . . . . . . . . . . 26 Predicting Outcomes for New Data . . . . . . . . . . . . . . . . . . . . 27 Accessing the prediction . . . . . . . . . . . . . . . . . . . . . . . 29 Adjusting the threshold . . . . . . . . . . . . . . . . . . . . . . . 33 Visualizing the prediction . . . . . . . . . . . . . . . . . . . . . . 35 Evaluating Learner Performance . . . . . . . . . . . . . . . . . . . . . 38 Available performance measures . . . . . . . . . . . . . . . . . . . 38 Listing measures . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 Calculate performance measures . . . . . . . . . . . . . . . . . . 40 Access a performance measure . . . . . . . . . . . . . . . . . . . . 41 Binary classification . . . . . . . . . . . . . . . . . . . . . . . . . 42 Resampling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 Stratified resampling . . . . . . . . . . . . . . . . . . . . . . . . . 47", "creator": "LaTeX with hyperref package"}}}