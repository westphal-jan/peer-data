{"id": "1609.06204", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Italy goes to Stanford: a collection of CoreNLP modules for Italian", "abstract": "In this we paper present Tint, an easy-to-use set of fast, accurate and extendable Natural Language Processing modules for Italian. It is based on Stanford CoreNLP and is freely available as a standalone software or a library that can be integrated in an existing project.", "histories": [["v1", "Tue, 20 Sep 2016 14:53:05 GMT  (19kb)", "http://arxiv.org/abs/1609.06204v1", null], ["v2", "Thu, 13 Apr 2017 08:33:33 GMT  (19kb)", "http://arxiv.org/abs/1609.06204v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["alessio palmero aprosio", "giovanni moretti"], "accepted": false, "id": "1609.06204"}, "pdf": {"name": "1609.06204.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Giovanni Moretti"], "emails": ["aprosio@fbk.eu", "moretti@fbk.eu"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.06 204v 1 [cs.C L] 20 Sep 2016Italiano. In questo articolo presentiamo Tint, una collezione di moduli semplici, veloci e personalizzabili per l'analisi di testi in Italiano. Tint e \u0441 basato su Stanford CoreNLP e puo essere scaricato gratuitamente come software stand-alone o come libreria da integrare in progetti esistenti."}, {"heading": "1 Introduction", "text": "In recent years, Natural Language Processing (NLP) technologies have become a basic foundation for complex tasks such as answering questions, event identification and topic classification. While most of the freely available NLP tools on the web (such as Stanford CoreNLP1 and OpenNLP2) are designed for English and sometimes adapted to other languages, there is a lack of such resources in Italy. In this paper, we introduce Tint, a set of ready-to-use modules for NLP, that is: New. Tint is the first completely free and open source tool for NLP in Italy.Easily. Tint can be downloaded and used out-of-the-box (see Section 5). In addition, it relies on Stanford CoreNLP Java interface, so it can be easily integrated into an existing project."}, {"heading": "2 Architecture", "text": "The Tint pipeline is based on Stanford CoreNLP (Manning et al., 2014), an open source framework written in Java that provides most Common NLP tasks out-of-the-box in multiple languages. It also provides a simple interface for extending annotations to new tasks and / or languages. Unlike some similar tools such as UIMA (Ferrucci and Lally, 2004) and GATE (Cunningham et al., 2002), CoreNLP is easy to use and requires no learning: it requires basic object-oriented programming knowledge. In Tint, we use this framework to port the most common NLP tasks to Italian as well as add some new annotations to external tools such as entity linking, time expression identification, keyword extraction."}, {"heading": "3 Modules", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Tokenizer", "text": "This module provides text segmentation in tokens and sentences. First, the text is roughly tokenized; in a second step, tokens that need to be assembled are merged using two customizable lists of non-breaking Italian abbreviations (such as \"dott.\" or \"S.p.A.\") and regular expressions (for email addresses, web URIs, numbers, data)."}, {"heading": "3.2 Morphological Analyzer", "text": "The current version of this module has been trained with the Morph-it lexicon (Zanchetta and Baroni, 2005), but it is possible to expand or retrain it with other Italian datasets. To ensure fast performance, model storage has been implemented with the mapDB Java Library 3, which is an excellent variant of the assorted string table of Cassandra. To extend the coverage of the results, especially for complex forms such as \"porta-ce-ne,\" \"portar-glie-lo\" or \"bi-direzionale,\" the module tries to break down the token into prefix root infix suffix and tries to resolve the root shape."}, {"heading": "3.3 Part-of-speech tagger", "text": "The portion of the language comments is provided by the implementation of Maximum Entropy (Toutanova et al., 2003), which is included in Stanford CoreNLP. The model is based on the Universal Dependencies4 (UD) dataset for Italian (Bosco et al., 2013), a dataset - freely available for research purposes - that contains more than 300K tokens with lemmas, language comments, and syntactic dependencies. Alternatively, a wrapper commentator using TreeTagger in Tint is also available."}, {"heading": "3.4 Lemmatizer", "text": "The Lemmatization module is a rules-based system that works by combining the output of Part-of-Speech and the results of the Morphological Analyzer to clearly determine the morphological characteristics using the grammatical annotation. To increase the accuracy of the results, the module attempts to identify the genre of nouns \"latte\" by analyzing their processed articles. For example, to perform the correct Lemmatization of \"il latte / the milk,\" the module uses the singular article \"il\" to identify the correct gender / number of Lemmas \"latte\" and returns \"latte / milk\" (male, singular) instead of \"latta / metal sheet\" (female, the plural form is \"latte\"). 3http: / / www.mapdb.org 4 http: / / universaldependencies.org /"}, {"heading": "3.5 Named Entity Recognition and Classification", "text": "The NER module recognizes persons, places and organizations in the text. It uses a CRF sequence tagger (Finkel et al., 2005) contained in Stanford CoreNLP, and it is trained on the ICAB (Magnini et al., 2006), a data set of 180K words from the Italian newspaper \"L'Adige.\""}, {"heading": "3.6 Dependency Parsing", "text": "This module provides syntactical analysis of the text using a transition-based parser (included in Stanford CoreNLP) that generates typed dependency parses of natural language sentences (Chen and Manning, 2014), driven by a neural network that accepts text embedding: the model is trained on the UD dataset (see Section 3.3), and the word embedding is based on the Paisa corpus (Lyding et al., 2014), which contains 250 million tokens of freely available and distributable text harvested from the web."}, {"heading": "3.7 Entity Linking", "text": "The task of linking is to uniquely define a word (or set of words) and link it to a knowledge base (KB).The largest (and most commonly used) KB available is Wikipedia, and almost every linking tool relies on it. The Tint pipeline provides a wrapper annotator that can be linked to DBpedia Spotlight5 (Daiber et al., 2013) and The Wiki Machine6 (Giuliano et al., 2009).Both tools are distributed as open source software and can be used by the annotator both as external services and via a local installation."}, {"heading": "3.8 Temporal Expression Extraction and Normalization", "text": "The task of time expression extraction is included in Tint as a wrapper by HeidelTime (Stro \ufffd tgen and Gertz, 2013), a rules-based, up-to-date tagger developed at Heidelberg University. HeidelTime also normalizes expressions according to the annotation standard TIMEX3. The software is released under the GPL license and can therefore be used for both educational and commercial purposes.5http: / / bit.ly / dbpspotlight 6 http: / / bit.ly / thewikimachine"}, {"heading": "3.9 Keyword extraction", "text": "Keyword Extraction in Tint is performed by Keyphrase Digger (Moretti et al., 2015), a rules-based system for keyphrase extraction that combines statistical metrics with linguistic information provided by speech patterns to identify and extract weighted keyphrases from text. CoreNLP's annotator for Keyphrase Digger is included in the Tint pipeline, but the main software must be downloaded and installed from the official Website7 as it is not published as open source."}, {"heading": "4 Evaluation", "text": "In some cases, Tint is compared to existing pipelines that work with the Italian language: Tanl (Attardi et al., 2010), TextPro (Pianta et al., 2008) and TreeTagger (Schmid, 1994). In calculating the speed, we run each experiment ten times and take into account the average execution time. If available, the multi-thread capabilities have been disabled. All experiments were run on a 2.3 GHz Intel Core i7 with 16 GB of disk space. The Tanl API is not available as a downloadable package, but is only available online via a REST API, so the speed can be affected by the network connection. Furthermore, the Tanl API does not provide offsets for the commented text, nor does it allow a part to share this text for automatic text linking and use the tutation as a tool."}, {"heading": "4.1 Tokenization and sentence splitting", "text": "Tint outperforms both TextPro and Tanl in the task of tokenization and sentence splitting (see Table 1), and the number of tokens per second can be further increased by tuning functions (e.g. by disabling regular expressions that recognize e-mail or web addresses).7 http: / / dh.fbk.eu / technologies / kd"}, {"heading": "4.2 Part-of-speech tagging", "text": "Since the used tagsets differ for different tools, the accuracy is calculated only for five coarse-grained types: nouns (N), verbs (V), adverbs (B), adjectives (A) and others (O). For each tool, the corresponding tagset is converted into this tagset and the accuracy is calculated by dividing the number of times the tagger gets the correct answer by the total number of tags in the dataset. Table 2 shows the results."}, {"heading": "4.3 Lemmatization", "text": "Like part-of-speech tagging, the lemmatization is evaluated in terms of both accuracy and execution time on the UD test set. If the problem is guessed from a morphological analysis (as in Tint and TextPro), the speed is calculated by including both tasks. Table 3 shows the results. All tools achieve the same accuracy of 96% (with minor differences that are not statistically significant)."}, {"heading": "4.4 Named Entities Recognition", "text": "For Named Entity Recognition, we evaluate and compare our system with the test set available on the I-CAB dataset. We consider three classes: 8The (considerable) speed of TreeTagger includes both Lemmatization and Language Tagging.PER, ORG, LOC. Both Tanl and TextPro also deal with the GPE class, but we have merged them with LOC, as was done during Tint's training. We had to completely rework the EntityPro module of TextPro (with three classes), as the original model already contains the ICAB test set, so it would exceed the results. In Training Tint, we add some gazettes of names to help the classifier detect entifications that are not present in the training set. Specifically, we have extracted a list of people, places and organizations by classifying the Airpedia database (Palmero Aprosio et al., 2013) as an organization for Wikipedia or person."}, {"heading": "4.5 Dependency parsing", "text": "While Tint is trained on the UD dataset, the parsers contained in Tanl (Attardi et al., 2013) and TextPro (Lavelli, 2013) use part of the Turin University Database (Bosco et al., 2000) published for the Evalita 2011 parsing task (Magnini et al., 2013). Therefore, the comparison between the two systems is not entirely fair: on the one hand, the TUT dataset is smaller than the UD; on the other, the UD is an automatic combination of two different tree banks commented on the basis of different guidelines (Bosco et al., 2013)."}, {"heading": "5 The tool", "text": "The Tint pipeline is published as open source software under the GNU General Public License (GPL), version 3. It can be downloaded from theTint website9 as a standalone package, or it can be integrated with Mavenin an existing application. It is written using the Stanford CoreNLP paradigm, so third-party software can be easily integrated into the pipeline. Tint accepts Plain Text or Newsreader Annotation Format (NAF) (Fokkens et al., 2014) as input and CoNLL, JSON or NAF as output."}, {"heading": "6 Conclusion and Future Work", "text": "In this article, we presented Tint, a simple, fast and precise NLP pipeline for Italian, based on CoreNLP from Stanford. Currently, we offer standard NLP annotations for parts of the language, lemmas, named entities, links to Wikipedia, dependency parsing, time expression identification and keyword extraction; additional custom modules can be easily added and replaced by implementing the CoreNLP Java interfaces. In the future, we plan to better align the various modules based on machine learning (such as dependency parsing, part-of-speech tagging and named entity detection) that were trained in this preliminary version of Tint without linguistic optimization. We are currently working on new modules, in particular Word Sense Disambiguation (WSD) w.r.t. Linguistic resources such as MultiWordNet (1998 Piantole, Labantal 2002, and Semi-RDF)."}, {"heading": "Acknowledgments", "text": "The research that led to this paper was partially supported by the European Union's Horizon 2020 programme through the SIMPATICO project (H2020EURO-6-2015, n. 692819).9 http: / / tint.fbk.eu /"}], "references": [{"title": "The Tanl Pipeline", "author": ["Attardi et al.2010] G. Attardi", "S. Dei Rossi", "M. Simi"], "venue": "In Proc. of LREC Workshop on WSPP", "citeRegEx": "Attardi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Attardi et al\\.", "year": 2010}, {"title": "Tuning desr for dependency parsing of italian", "author": ["Maria Simi", "Andrea Zanelli"], "venue": "In Evaluation of Natural Language and Speech Tools for Italian,", "citeRegEx": "Attardi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Attardi et al\\.", "year": 2013}, {"title": "The berkeley framenet project", "author": ["Baker et al.1998] Collin F Baker", "Charles J Fillmore", "John B Lowe"], "venue": "In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Compu-", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Building a treebank for italian: a data-driven annotation", "author": ["Bosco et al.2000] Cristina Bosco", "Vincenzo Lombardo", "Daniela Vassallo", "Leonardo Lesmo"], "venue": null, "citeRegEx": "Bosco et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Bosco et al\\.", "year": 2000}, {"title": "Converting italian treebanks: Towards an italian stanford dependency treebank", "author": ["Bosco et al.2013] Cristina Bosco", "Simonetta Montemagni", "Maria Simi"], "venue": null, "citeRegEx": "Bosco et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bosco et al\\.", "year": 2013}, {"title": "A fast and accurate dependency parser using neural networks", "author": ["Chen", "Manning2014] Danqi Chen", "Christopher D Manning"], "venue": "In EMNLP,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "A 2-phase frame-based knowledge extraction framework", "author": ["Marco Rospocher", "Alessio Palmero Aprosio"], "venue": "In Proc. of ACM Symposium on Applied Computing (SAC\u201916)", "citeRegEx": "Corcoglioniti et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Corcoglioniti et al\\.", "year": 2016}, {"title": "Gate: An architecture for development of robust hlt applications", "author": ["Diana Maynard", "Kalina Bontcheva", "Valentin Tablan"], "venue": "In Proceedings of the 40th Annual Meeting on Association for Computational Lin-", "citeRegEx": "Cunningham et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Cunningham et al\\.", "year": 2002}, {"title": "Improving efficiency and accuracy in multilingual entity extraction", "author": ["Max Jakob", "Chris Hokamp", "Pablo N. Mendes"], "venue": "In Proceedings of the 9th International Conference on Semantic Systems (I-Semantics)", "citeRegEx": "Daiber et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Daiber et al\\.", "year": 2013}, {"title": "Uima: An architectural approach to unstructured information processing in the corporate research environment", "author": ["Ferrucci", "Lally2004] David Ferrucci", "Adam Lally"], "venue": null, "citeRegEx": "Ferrucci et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Ferrucci et al\\.", "year": 2004}, {"title": "Incorporating non-local information into information extraction systems by gibbs sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": null, "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "Naf and gaf: Linking linguistic annotations", "author": ["Aitor Soroa", "Zuhaitz Beloki", "Niels Ockeloen", "German Rigau", "Willem Robert van Hage", "Piek Vossen"], "venue": "In Proceedings 10th Joint ISO-ACL SIGSEM Workshop", "citeRegEx": "Fokkens et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Fokkens et al\\.", "year": 2014}, {"title": "Kernel methods for minimally supervised wsd", "author": ["Alfio Massimiliano Gliozzo", "Carlo Strapparava"], "venue": "Comput. Linguist.,", "citeRegEx": "Giuliano et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Giuliano et al\\.", "year": 2009}, {"title": "An ensemble model for the evalita 2011 dependency parsing task", "author": ["Alberto Lavelli"], "venue": "In Evaluation of Natural Language and Speech Tools for Italian,", "citeRegEx": "Lavelli.,? \\Q2013\\E", "shortCiteRegEx": "Lavelli.", "year": 2013}, {"title": "The paisa corpus of italian web texts", "author": ["Lyding et al.2014] Verena Lyding", "Egon Stemle", "Claudia Borghetti", "Marco Brunello", "Sara Castagnoli", "Felice Dell\u2019Orletta", "Henrik Dittmann", "Alessandro Lenci", "Vito Pirrelli"], "venue": "In Proceedings of the 9th Web", "citeRegEx": "Lyding et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lyding et al\\.", "year": 2014}, {"title": "I-cab: the italian content annotation bank", "author": ["Emanuele Pianta", "Christian Girardi", "Matteo Negri", "Lorenza Romano", "Manuela Speranza", "Valentina Bartalesi Lenzi", "Rachele Sprugnoli"], "venue": "Proceedings of LREC,", "citeRegEx": "Magnini et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Magnini et al\\.", "year": 2006}, {"title": "Evaluation of Natural Language and Speech Tool for Italian: International Workshop, EVALITA", "author": ["Francesco Cutugno", "Mauro Falcone", "Emanuele Pianta"], "venue": null, "citeRegEx": "Magnini et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Magnini et al\\.", "year": 2013}, {"title": "The stanford corenlp natural language processing toolkit", "author": ["Mihai Surdeanu", "John Bauer", "Jenny Rose Finkel", "Steven Bethard", "David McClosky"], "venue": "In ACL (System Demonstrations),", "citeRegEx": "Manning et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2014}, {"title": "Digging in the dirt: Extracting keyphrases from texts with kd", "author": ["Rachele Sprugnoli", "Sara Tonelli"], "venue": "CLiC it,", "citeRegEx": "Moretti et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Moretti et al\\.", "year": 2015}, {"title": "Automatic expansion of DBpedia exploiting Wikipedia cross-language information", "author": ["Claudio Giuliano", "Alberto Lavelli"], "venue": "In Proceedings of the 10th Extended Semantic Web Conference", "citeRegEx": "Aprosio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Aprosio et al\\.", "year": 2013}, {"title": "Developing an aligned multilingual database", "author": ["Luisa Bentivogli", "Christian Girardi"], "venue": "In Proc. 1st Int\u2019l Conference on Global WordNet. Citeseer", "citeRegEx": "Pianta et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Pianta et al\\.", "year": 2002}, {"title": "The textpro tool suite", "author": ["Christian Girardi", "Roberto Zanoli"], "venue": "In LREC. Citeseer", "citeRegEx": "Pianta et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Pianta et al\\.", "year": 2008}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["Helmut Schmid"], "venue": null, "citeRegEx": "Schmid.,? \\Q1994\\E", "shortCiteRegEx": "Schmid.", "year": 1994}, {"title": "Multilingual and cross-domain temporal tagging", "author": ["Str\u00f6tgen", "Gertz2013] Jannik Str\u00f6tgen", "Michael Gertz"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Str\u00f6tgen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Str\u00f6tgen et al\\.", "year": 2013}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D. Manning", "Yoram Singer"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "Morph-it! a free corpus-based morphological resource for the italian language", "author": ["Zanchetta", "Baroni2005] Eros Zanchetta", "Marco Baroni"], "venue": "Corpus Linguistics", "citeRegEx": "Zanchetta et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Zanchetta et al\\.", "year": 2005}], "referenceMentions": [{"referenceID": 17, "context": "The Tint pipeline is based on Stanford CoreNLP (Manning et al., 2014), an open-source framework written in Java, that provide most of the commons NLP tasks out-of-the-box in various language.", "startOffset": 47, "endOffset": 69}, {"referenceID": 7, "context": "Differently from some similar tools, such as UIMA (Ferrucci and Lally, 2004) and GATE (Cunningham et al., 2002), CoreNLP is easy to use and does not require it to be learnt: a basic object-oriented programming skill is enough.", "startOffset": 86, "endOffset": 111}, {"referenceID": 24, "context": "The part-of-speech annotation is provided through the Maximum Entropy implementation (Toutanova et al., 2003) included in Stanford CoreNLP.", "startOffset": 85, "endOffset": 109}, {"referenceID": 4, "context": "The model is trained on the Universal Dependencies4 (UD) dataset for Italian (Bosco et al., 2013), a dataset \u2013 freely available for research purpose \u2013 containing more than 300K tokens annotated with lemma, part-of-speech and syntactic dependencies.", "startOffset": 77, "endOffset": 97}, {"referenceID": 10, "context": "It uses a CRF sequence tagger (Finkel et al., 2005) included in Stanford CoreNLP and it is trained on the ICAB (Magnini et al.", "startOffset": 30, "endOffset": 51}, {"referenceID": 15, "context": ", 2005) included in Stanford CoreNLP and it is trained on the ICAB (Magnini et al., 2006), a dataset containing 180K words taken from the Italian newspaper \u201cL\u2019Adige\u201d.", "startOffset": 67, "endOffset": 89}, {"referenceID": 14, "context": "3) and the word embeddings are built on the Pais\u00e0 corpus (Lyding et al., 2014), that contains 250M tokens of freely available and distributable texts harvested from the web.", "startOffset": 57, "endOffset": 78}, {"referenceID": 8, "context": "The Tint pipeline provides a wrapper annotator that can connect to DBpedia Spotlight5 (Daiber et al., 2013) and The Wiki Machine6 (Giuliano et al.", "startOffset": 86, "endOffset": 107}, {"referenceID": 12, "context": ", 2013) and The Wiki Machine6 (Giuliano et al., 2009).", "startOffset": 30, "endOffset": 53}, {"referenceID": 18, "context": "Keyword extraction in Tint is performed by Keyphrase Digger (Moretti et al., 2015), a rulebased system for keyphrase extraction.", "startOffset": 60, "endOffset": 82}, {"referenceID": 0, "context": "When possible, Tint is compared with existing pipelines that work with the Italian language: Tanl (Attardi et al., 2010), TextPro (Pianta et al.", "startOffset": 98, "endOffset": 120}, {"referenceID": 21, "context": ", 2010), TextPro (Pianta et al., 2008) and TreeTagger (Schmid, 1994).", "startOffset": 17, "endOffset": 38}, {"referenceID": 22, "context": ", 2008) and TreeTagger (Schmid, 1994).", "startOffset": 23, "endOffset": 37}, {"referenceID": 1, "context": "While Tint is trained on the UD dataset, the parsers included in Tanl (Attardi et al., 2013) and TextPro (Lavelli, 2013) use part of the Turin University Treebank (TUT) (Bosco et al.", "startOffset": 70, "endOffset": 92}, {"referenceID": 13, "context": ", 2013) and TextPro (Lavelli, 2013) use part of the Turin University Treebank (TUT) (Bosco et al.", "startOffset": 20, "endOffset": 35}, {"referenceID": 3, "context": ", 2013) and TextPro (Lavelli, 2013) use part of the Turin University Treebank (TUT) (Bosco et al., 2000), as released for the Evalita 2011 parsing task (Magnini et al.", "startOffset": 84, "endOffset": 104}, {"referenceID": 16, "context": ", 2000), as released for the Evalita 2011 parsing task (Magnini et al., 2013).", "startOffset": 55, "endOffset": 77}, {"referenceID": 4, "context": "For this reason, the comparison between the two system is not completely fair: on the one hand, the TUT dataset is smaller than the UD; on the other hand, the UD is an automatic combination of two different treebanks, that have been annotated using different guidelines (Bosco et al., 2013).", "startOffset": 270, "endOffset": 290}, {"referenceID": 11, "context": "Tint accepts plain text or Newsreader Annotation Format (NAF) (Fokkens et al., 2014) as input, and CoNLL, JSON, or NAF as output.", "startOffset": 62, "endOffset": 84}, {"referenceID": 20, "context": "linguistic resources such as MultiWordNet (Pianta et al., 2002) and Semantic Role Labelling, by porting to Italian resources such as Framenet (Baker et al.", "startOffset": 42, "endOffset": 63}, {"referenceID": 2, "context": ", 2002) and Semantic Role Labelling, by porting to Italian resources such as Framenet (Baker et al., 1998), now available in English.", "startOffset": 86, "endOffset": 106}, {"referenceID": 6, "context": "The Tint pipeline will also be integrated into PIKES (Corcoglioniti et al., 2016), a tool that extracts knowledge from texts using NLP annotation and outputs it in a queryable form (such RDF triples).", "startOffset": 53, "endOffset": 81}], "year": 2017, "abstractText": "English. In this we paper present Tint, an easy-to-use set of fast, accurate and extendable Natural Language Processing modules for Italian. It is based on Stanford CoreNLP and is freely available as a standalone software or a library that can be integrated in an existing project. Italiano. In questo articolo presentiamo Tint, una collezione di moduli semplici, veloci e personalizzabili per l\u2019analisi di testi in Italiano. Tint \u00e8 basato su Stanford CoreNLP e pu\u00f2 essere scaricato gratuitamente come software stand-alone o come libreria da integrare in progetti esistenti.", "creator": "LaTeX with hyperref package"}}}