{"id": "1609.06221", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "An Efficient Method of Partitioning High Volumes of Multidimensional Data for Parallel Clustering Algorithms", "abstract": "An optimal data partitioning in parallel &amp; distributed implementation of clustering algorithms is a necessary computation as it ensures independent task completion, fair distribution, less number of affected points and better &amp; faster merging. Though partitioning using Kd Tree is being conventionally used in academia, it suffers from performance drenches and bias (non equal distribution) as dimensionality of data increases and hence is not suitable for practical use in industry where dimensionality can be of order of 100s to 1000s. To address these issues we propose two new partitioning techniques using existing mathematical models &amp; study their feasibility, performance (bias and partitioning speed) &amp; possible variants in choosing initial seeds. First method uses an n dimensional hashed grid based approach which is based on mapping the points in space to a set of cubes which hashes the points. Second method uses a tree of voronoi planes where each plane corresponds to a partition. We found that grid based approach was computationally impractical, while using a tree of voronoi planes (using scalable K-Means++ initial seeds) drastically outperformed the Kd-tree tree method as dimensionality increased.", "histories": [["v1", "Tue, 20 Sep 2016 15:26:37 GMT  (656kb)", "http://arxiv.org/abs/1609.06221v1", "5 pages, 6 figures"]], "COMMENTS": "5 pages, 6 figures", "reviews": [], "SUBJECTS": "cs.AI cs.DC", "authors": ["saraswati mishra", "avnish chandra suman"], "accepted": false, "id": "1609.06221"}, "pdf": {"name": "1609.06221.pdf", "metadata": {"source": "CRF", "title": "An Efficient Method of Partitioning High Volumes of Multidimensional Data for Parallel Clustering Algorithms", "authors": ["Saraswati Mishra", "Avnish Chandra Suman"], "emails": ["saraswatimishra18@gmail.com", "avnishchandrasuman@gmail.com"], "sections": [{"heading": null, "text": "The idea is that it's a way of doing it in a way that overlays the rest of the world, the method was to divide the existing data into different parts, the idea is to treat and operate each part as a separate input, the idea is that the individual parts of the whole are interwoven with each other, the idea is that it's a way in which the individual parts of the whole are interwoven with each other, the idea is that the individual parts of the whole are interwoven with each other, the idea is that the individual parts of the whole are interwoven with each other, the idea is that the individual parts of the whole are interwoven with each other, the idea is that the individual parts of the whole are interwoven with each other, the idea is that the individual parts of the whole are interwoven with each other."}, {"heading": "Algorithm: Construct Cubes", "text": "Input: Set of x points p in space (p1, p2... px) where each px is (x1, x2.. xn) Number of partitions m k: a natural number 1 < = k # higher values for k ensure better overall distribution but lower performance."}, {"heading": "Output: Set of Cubes (c1, c2 \u2026. cm )", "text": "Where m = (y + 1) n Approach: # Find an optimal y & initialize cube boundaries"}, {"heading": "Let minxn be min (px) in n", "text": "Let maxxn max (px) be in the nth dimension Let Y = ky. Let M = (Y + 1) n for each ci in (c1.. cM) Boundary (cMn) = {(i-1) * (maxn-minn) / M, i * (maxn-minn) / M} Binary sort c For each pi in (p1... px) Find the position of pi in p.Add pi to cM cM.totalPonits + +"}, {"heading": "Algorithm: Find Median", "text": "Input: set of cubes (c1... cM), cumulative points output: median along one dimension n say mn method: P: cumulative points in space x = 0; While x < P / 2 move on to the next cube, x = x + Cm.totalPoints # We stand by the cube that is our middle (or close approximation if k is too low or too high) number of points in cm. We can see that the cube structure is an O (n + logn) task, while the middle creation is an O (n / M) task that looks very efficient. The approach should have worked in two passes over data plus a single pass over grid cells. However, there were major design & implementation problems with this approach 1. The number of cubes is (y + 1) n. Even if y is 2, for a huge n array, we get 2n cells. This grows closer to the total number of data cells. 2. programmatically impracticable in the direct sense."}, {"heading": "Construction of v_tree", "text": "On each level you will find k (2) -center 1. Points must be reasonably distributed and far away 2. Use scalable means + + (Bahmani et all., 2012) or mosquito techniques (Brin, 1995) 3. Assign all points to one of the two centers based on distance 4. All points within the current + limit (Goel et all., 2015) are considered affected. 5. Delete additional points stored in the parent node to remove redundancy repetitions for new sets until the required number of partitions has been found."}, {"heading": "Data Structure", "text": "A minimum n-voronoi tree implementation data structure in c looks like this: typedef struct member {int id; float * val;} member; struct v _ node {int level; int core1, core2; member * mem1, * mem2, * mem _ overlap; struct v _ node * left, * right; int count _ mem1, count _ mem2, count _ overlap, total _ count;} struct v _ tree {int levels; struct node * head;} head node is the summary of all the data. Each parent node contains a summary of the points in child nodes. The exact points are stored until the data is partitioned and deleted at level as we move to the next level."}, {"heading": "How to choose initial seeds?", "text": "1. Random selection: Choosing the center does not guarantee or violate anything and simply leaves the matter to the chosen center and the distribution of the data. There is the same probability of obtaining any load balance. therefore, the probability of obtaining a perfect load balance tends toward zero. 2. GNAT approach: Let's say we need n seeds. Let's start by selecting a point randomly. The next point is chosen so that its distance from the first point is maximum, the third point is chosen so that its distance from the sum of the previous two points is maximum. Likewise, the fourth point should be the point that is furthest away from the sum of the first three points and so on. This approach is good in terms of load balancing for a large value of n, but cannot guarantee a good balance for small n (~ 2- 5). 3. Scalable K-Means + + approach: You need k centers, C be the set of initial seeds, then one."}, {"heading": "IV. EXPERIMENTATION & RESULTS", "text": "Execution environment: - Ubuntu 13.04, Intel Core i3 (3rd generation), 2.4GHz (2 cores hyper threaded), 4GB RAM, 3MB L2 cache. Translation environment: - C, gcc, gdb, gprof, vampir, Geany IDE. Output visualization was done using geogebra. Test data sets: (number of points x number of dimensions, double accuracy of data) 1. 100x2, 2. 700x9, 3. 1500x1024, 4. 4000x1024, 5. 40000x1024We noticed an increase in load distortion as an increase in the number of partitions. The distribution was almost uniform with uniform data. However, we can see that Kd tree (right) provides a better distribution with uniformly distributed data."}, {"heading": "APPENDIX", "text": "Related code, records & results can be obtained at: https: / / drive.google.com / file / d / 0Bxo9wQ432jhla1dQ NWFrbnM4bnc / view? usp = sharing"}], "references": [{"title": "Parallelizing OPTICS for Commodity Clusters", "author": ["Poonam Goyal", "Sonal Kumari", "Dhruv Kumar", "Sundar Balasubramaniam", "Navneet Goyal", "Saiyedul Islam", "Jagat Sesh Challa"], "venue": "In Proceedings of the 2015 International Conference on Distributed Computing and Networking (ICDCN '15)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Communications of the ACM,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1975}, {"title": "Near neighbor search in large metric spaces", "author": ["S. Brin"], "venue": "in: Proceedings of the International Conference on Very Large Databases (VLDB),", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1995}], "referenceMentions": [], "year": 2016, "abstractText": "An optimal data partitioning in parallel/distributed implementation of clustering algorithms is a necessary computation as it ensures independent task completion, fair distribution, less number of affected points and better & faster merging. Though partitioning using Kd-Tree is being conventionally used in academia, it suffers from performance drenches and bias (non equal distribution) as dimensionality of data increases and hence is not suitable for practical use in industry where dimensionality can be of order of 100\u2019s to 1000\u2019s. To address these issues we propose two new partitioning techniques using existing mathematical models & study their feasibility, performance (bias and partitioning speed) & possible variants in choosing initial seeds. First method uses an n-dimensional hashed grid based approach which is based on mapping the points in space to a set of cubes which hashes the points. Second method uses a tree of voronoi planes where each plane corresponds to a partition. We found that grid based approach was computationally impractical, while using a tree of voronoi planes (using scalable K-Means++ initial seeds) drastically outperformed the Kd-tree tree method as dimensionality increased.", "creator": "\u00fe\u00ff"}}}