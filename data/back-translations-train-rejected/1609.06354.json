{"id": "1609.06354", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches", "abstract": "We demonstrate that a person's behavioral and environmental context can be automatically recognized by harnessing the sensors built into smartphones and smartwatches. We propose a generic system that can simultaneously recognize many contextual attributes from diverse behavioral domains. By fusing complementary information from different types of sensors our system successfully recognizes fine details of work and leisure activities, body movement, transportation, and more. Health monitoring, clinical intervention, aging care, personal assistance and many more applications will benefit from automatic, frequent and detailed context recognition.", "histories": [["v1", "Tue, 20 Sep 2016 20:56:07 GMT  (2312kb,D)", "http://arxiv.org/abs/1609.06354v1", "This paper was submitted to IEEE Pervasive Computing magazine on September 20th, 2016"], ["v2", "Thu, 29 Dec 2016 22:47:22 GMT  (2314kb,D)", "http://arxiv.org/abs/1609.06354v2", "This paper was originally submitted to IEEE Pervasive Computing magazine on September 20th, 2016. This version is the major revision (following first review). This revision was submitted to IEEE Pervasive Computing on December 28th, 2016"], ["v3", "Wed, 17 May 2017 21:44:18 GMT  (640kb,D)", "http://arxiv.org/abs/1609.06354v3", "This paper was originally submitted to IEEE Pervasive Computing magazine on September 20th, 2016. After a major revision and a minor revision, the paper was accepted in April 2017. It is currently under the editing process"], ["v4", "Sat, 30 Sep 2017 15:25:23 GMT  (640kb,D)", "http://arxiv.org/abs/1609.06354v4", "This paper was accepted and is to appear in IEEE Pervasive Computing, vol. 16, no. 4, October-December 2017, pp. 62-74"]], "COMMENTS": "This paper was submitted to IEEE Pervasive Computing magazine on September 20th, 2016", "reviews": [], "SUBJECTS": "cs.AI cs.CY cs.HC cs.LG", "authors": ["yonatan vaizman", "katherine ellis", "gert lanckriet"], "accepted": false, "id": "1609.06354"}, "pdf": {"name": "1609.06354.pdf", "metadata": {"source": "CRF", "title": "Recognizing Detailed Human Context In-the-Wild from Smartphones and Smartwatches", "authors": ["Yonatan Vaizman", "Katherine Ellis"], "emails": ["yvaizman@eng.ucsd.edu"], "sections": [{"heading": null, "text": "Index terms - context awareness, mobile sensors, artificial intelligence, machine learning, human activity detection.F"}, {"heading": "1 INTRODUCTION", "text": "The ability to automatically recognise a person's context (i.e., where they are, what they do, who they are with, how they feel, etc.) is of great benefit in many areas. Health monitoring and lifestyle interventions, traditionally based on manual, subjective reporting [1], sometimes relying on a call-back with an end-of-day interview [2], can be improved by automatic (frequent, timely, effortless and objective) detection of behaviours such as exercise, eating [3], falling asleep [4] or mental states such as stress [5] and hypomania [6]. Just-in-time interventions (e.g. for addiction) often result in arbitrary times of day, possibly missing times when the patient needs the most support [7]. Adding (automatically recognised) context information will help to recognise critical times and provide immediate support (e.g. an alcoholic patient may be at high risk if the context is \"at a bar\")."}, {"heading": "Related work", "text": "In fact, the fact is that most of them are people who are not able to move."}, {"heading": "CONTEXT RECOGNITION SYSTEM", "text": "Figure 1 illustrates the workings of our detection system. The system is based on measurements of five sensors in a smartphone: accelerometer (Acc), gyroscope (Gyro), location (Loc), audio (Aud) and phone status (PS), and accelerometer measurements from a smartwatch (WAcc). For a given minute, the system takes measurements from these six sensors as input and the task is to detect the combination of relevant context identifiers (Figure 1 (A)), i.e. to explain a binary decision for each label l: yl = 1 (the label is relevant for that minute) or yl = 0 (not relevant).Simple sensor classifiers use sensor-specific characteristics to help us understand how informative each sensor can be independent of the other sensors, for a specific context identifier (Figure 1 (B)).The following procedure has been designed for a given sensor and a given label (for one) and (for one) for each label (for each label."}, {"heading": "Sensor fusion", "text": "The system combines the information from different sensors in several alternative ways. Early Fusion (EF) classifiers combine information from multiple sensors before the classification stage (Figure 1 (C)). The following procedure was performed for a specific label l: (a) Start with the sensor-specific feature vectors xs for each of the ns sensors. (b) Link the (standardized) sensor-specific feature vectors into a single vector x of dimension d = \u2211 ns s = 1 ds. (c) Learn a d-dimensional logistic regression classifier from the training set. (d) Use the logistic regression classifier on a test example to predict a binary classification yl and the probability value P (yl = 1 | x). Late fusion classifiers use the ns single sensor classifiers and combine the probability predictions P (yl = 1 | xs)."}, {"heading": "DATA COLLECTION", "text": "In fact, most of them are able to survive on their own, without having to put themselves centre stage."}, {"heading": "EVALUATION AND RESULTS", "text": "In fact, it is a way in which people are able to determine for themselves how they want to behave."}, {"heading": "USER PERSONALIZATION.", "text": "To explore the potential of personalization, we conducted experiments with a single test user. We compared three models: (1) universal (tailored to data from other users), (2) individual (trained on half of data from the same test user), and (3) customized (merging both other users and information from the same user). We tested the three models on the same, invisible data (the second half of the test user). Figure 5 shows the results of these experiments. The universal model shows good performance (average balanced accuracy of 0.85 and 0.56 on the 15 labels tested) and shows the basic ability of a universally trained product to recognize the context of a new user. As suspected, the individual model performed better than the universal model for some of the labels (\"lying,\" \"seated,\" \"sleeping,\" \"computer work\" and \"main workplace\")."}, {"heading": "Recall Prec F1 BA", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "DAILY ACTIVITY SCHEDULE.", "text": "To illustrate the possible use of detailed and more frequent behavioral context information, we present an analysis of daily activity planning. Figure 6 shows the distribution of eight activities around the daily (24-hour) clock for two groups of users: users who go to bed before 11: 45 p.m. (26 users, a total of 142,587 minutes. Figure 6 (A, C) and after 11: 45 p.m. (26 users, a total of 141,443 minutes. Figure 6 (B, D)) (see supplement material): The first group spends a total of statistically significantly more time sleeping than the second group and less time on the toilet than the second group. For the other activities, the total time between groups does not differ statistically significantly, but the distribution over the hours of the day is significantly different. The time spent at the main workplace is more concentrated at 9: 00 a.m. for the first group and more distributed for the second group. Meal times show a clearer pattern of breakfast, lunch and dinner genetics for the first group."}, {"heading": "FUTURE DIRECTIONS", "text": "The Context Recognition System can be implemented either in the \"cloud\" or on the phone itself - preventing sensitive information from being sent out. Recognition can be limited to certain behavioural aspects (e.g. physical activity) and ignore others. Depending on the application, sufficient information can be passed downstream without revealing the fully recognised behavioural context. For example, if a medical application is designed to monitor the total time of daily walking and running, these statistics can be collected during the day and sent at the end of the day without revealing the specific timetable of when (and where) the user has carried out what activity. In this work, we used simple computer-aided methods. Better detection can also be achieved by exploring extraction methods, modelling interlabel correlations, analysing information from the recent past, using online learning and active learning models to improve real-time research."}, {"heading": "SUPPLEMENTARY MATERIAL", "text": "Supplementary material contains technical details on the following components of the work: \u2022 Mobile app \u2022 Data acquisition methods \u2022 Sensor measurements \u2022 Extracted features \u2022 Label processing \u2022 Classification methods \u2022 Performance evaluation \u2022 User personalization evaluation \u2022 Daily activity analysis \u2022 Detailed results tables"}, {"heading": "Mobile app", "text": "In fact, the fact is that most of them will be able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there"}, {"heading": "Data collection procedure", "text": "The research plan and consent form for the study were approved by the University's Institutional Review Board (IRB). Human subjects were recruited for the study via campus leaflets, university mailing lists and word of mouth. Each subject read and signed the consent form. Researchers installed the app on each subject's personal phone (to maximize the authenticity of the natural behavior) and then conducted their usual behavior for about a week while running the app on their phone in the background as much as possible and conveniently as possible. Subjects were asked to report as many labels as possible without overly affecting their natural behavior. Subjects varied in terms of providing labels: some wanted to be very precise (with specific detailed combinations of labels and tried to be minute-by-minute accuracy) and others tended to be more specific and less specific."}, {"heading": "Sensor measurements", "text": "This year, it has come to the point that it will only be a matter of time before it happens, until it does."}, {"heading": "Extracted features", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "Label processing", "text": "In fact, the subject in which you find yourself is not able to indicate all the relevant labels, or is unaware of any other relevant label in the vocabulary. As part of the data purification process, we have created versions for some labels based on two different labels: on labels based on other labels and on labels in which the labels are applied. We have collected absolute coordinates of the labels that perform measurements (selecting the location with horizontal accuracy from each example) and have visualized them."}, {"heading": "Classification methods", "text": "Our system uses binary logistic regression classifiers (with an adapted section), logistic regression returns a real value interpreted as the probability of relevance of the label (value greater than 0.5 results in a decision of \"relevant\"), for each context label we used an independent model, then we used the web search to select the cost parameter C for logistic regression: for each value (from {0.001, 0.1, 1, 10, 100}) we trained a logistic regression model on the internal traction subset and tested the model on the validation subset. We selected the value of C for the logistic regression that resulted in the highest F1 measurements on the validation subset."}, {"heading": "Performance evaluation", "text": "This year, it has reached the point where it will be able to retaliate until it is able to retaliate."}, {"heading": "User personalization assessment", "text": "In order to discuss the benefits of such a solution, we need to look at how we can solve these problems."}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We demonstrate that a person\u2019s behavioral and environmental context can be automatically recognized by harnessing the sensors built into smartphones and smartwatches. We propose a generic system that can simultaneously recognize many contextual attributes from diverse behavioral domains. By fusing complementary information from different types of sensors our system successfully recognizes fine details of work and leisure activities, body movement, transportation, and more. Health monitoring, clinical intervention, aging care, personal assistance and many more applications will benefit from automatic, frequent and detailed context", "creator": "LaTeX with hyperref package"}}}