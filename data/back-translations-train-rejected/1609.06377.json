{"id": "1609.06377", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "20-Sep-2016", "title": "Geometry-Based Next Frame Prediction from Monocular Video", "abstract": "We propose a method for next frame prediction from video input. A convolutional recurrent neural network is trained to predict depth from monocular video input, which, along with the current video image and the camera trajectory, can then be used to compute the next frame. Unlike prior next-frame prediction approaches, we take advantage of the scene geometry and use the predicted depth for generating next frame prediction. A useful side effect of our technique is that it produces depth from video, which can be used in other applications.", "histories": [["v1", "Tue, 20 Sep 2016 22:49:34 GMT  (4218kb,D)", "http://arxiv.org/abs/1609.06377v1", null], ["v2", "Mon, 12 Jun 2017 21:52:06 GMT  (6581kb,D)", "http://arxiv.org/abs/1609.06377v2", "To appear in 2017 IEEE Intelligent Vehicles Symposium"]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["reza mahjourian", "martin wicke", "anelia angelova"], "accepted": false, "id": "1609.06377"}, "pdf": {"name": "1609.06377.pdf", "metadata": {"source": "CRF", "title": "Geometry-Based Next Frame Prediction from Monocular Video", "authors": ["Reza Mahjourian", "Martin Wicke", "Anelia Angelova"], "emails": ["reza@cs.utexas.edu", "wicke@google.com,", "anelia@google.com"], "sections": [{"heading": null, "text": "In fact, most of us are able to survive on our own by searching for new ways to travel the world. (...) It is not that we go in search of new ways. (...) It is not that we go in search of new ways. (...) It is not that we go in search of new ways. (...) It is not that we go in search of new ways. (...) It is not that we go in search of new ways. (...) It is that we go in search of new ways. (...) It is not that we go in search of new ways. (...) It is not that we go in search of new ways. (...)"}, {"heading": "II. RELATED WORK", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "III. NEXT FRAME PREDICTION METHOD", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Problem Formulation", "text": "The problem we suggest can be defined as follows: Given a sequence of RGB frames {X1, X2,.., Xk \u2212 1} and a sequence of camera positions {P1, P2,... Pk}, predict the next RGB frame Xk."}, {"heading": "B. Approach", "text": "Our proposed method predicts two depth maps Dk \u2212 1 and Dk corresponding to images k \u2212 1 and k. Depth map Dk \u2212 1 is constructed directly from the sequence of images X1.. Xk \u2212 1. Depth map Dk is constructed from Dk \u2212 1 and the ego movement of the camera from Pk \u2212 1 to Pk. Next image prediction Xk is constructed from the RGB frame Xk \u2212 1 and the two depth maps Dk \u2212 1, Dk using geometric projections and transformations."}, {"heading": "C. Depth Prediction from Monocular Video", "text": "In fact, most of us will be able to play by the rules we have set ourselves."}, {"heading": "D. Depth Prediction Loss Function", "text": "We experimented with the L2 and reverse Huber losses. The L2 loss minimizes the square Euclidean norm between predicted depth Di and soil truth denomination Yi for frame i: L2 (Di \u2212 Yi) = Di \u2212 Yi 22. The reverse Huber loss is defined in [16] as follows: B (\u03b4) = [16] = [16] \"Depth.\" The reverse Huber loss calculates the L1 standard for frame i \u2212 Yi and c = 15 maxi (D j i \u2212 Y i), where j is iterated over all pixels in the depth map. The reverse Huber loss calculates the L1 norm for cases where the L1 \u2212 c standard and the L2 standard are different. In addition, the loss equation may contain an optional term to minimize depth difference loss (GDL)."}, {"heading": "E. Next Frame Prediction", "text": "The next frame prediction is generated by additional transformation layers added after the depth output layer (not shown in Figure 4). For each frame i, the next frame prediction X \u2032 i is generated by the following means: \u2022 Video image Xi \u2212 1 from the last time frame. \u2022 Depth prediction Di \u2212 1 from the last time frame. \u2022 Camera sets Pi \u2212 1, Pi. Firstly, the points in the depth map Di \u2212 1 are projected into a three-dimensional point cloud. The x, y, z coordinates of the projected points in C depend on their two-dimensional coordinates on the depth map Di \u2212 1 and their depth values. In addition to the three-dimensional coordinates, each point in C is also assigned an RGB value. The RGB value for each point is determined by the corresponding image pixels in Xi \u2212 1 that are on the same image coordinates that are in the image coordination."}, {"heading": "IV. EXPERIMENTAL EVALUATION", "text": "We test our approach using the KITTI dataset [11] collected by a vehicle moving through urban environments.The vehicle is equipped with cameras, lidar, GPS and inertial sensors.The dataset consists of a series of videos with frame rates ranging from about 100 to a few thousand frames. For each image, the dataset contains RGB images, 3D point clouds and the vehicle pose as width, length, height as well as greed, inclination, rolling angle.We split the videos into training and evaluation kits, generating 10 frames sequences from each video. In total, our dataset contains about 38000 training sequences and 4200 validation sequences."}, {"heading": "A. Generating Ground Truth Depth Maps", "text": "The points included in the depth map have depths ranging from 3m (approximate abbreviation for camera visible points) to 80m (maximum range of the sensor). Instead of requiring the model to predict such large values, we use (3.0 / depth) as labels. We also experimented with using log (depth) as labels. In addition, the labels are scaled linearly to interval values [0,25,0.75]. This normalization helps to reduce the balance between the importance of accurate predictions for points that are far and near. Without normalization, loss terms such as L2 can result in disproportionate weights to near and far points. The generated depth maps contain areas that lack depth due to a number of reasons: 1) Because the camera and lidar are located elsewhere on the car, objects such as L2 may have disproportionate weights to near and distant points that are not in the shade."}, {"heading": "B. Quality Metrics", "text": "We use two metrics for image quality [14] to evaluate the predictions of the next image: \u2022 Peak Signal-to-Noise Ratio (PSNR) \u2022 Structural similarity (SSIM) Both metrics are standard for measuring the quality of image predictions [19], [17], [8]. Using these metrics, we can measure the quality of predictions regardless of the loss functions and depth transformation functions used. Measurements are calculated for pixels where the model makes a prediction."}, {"heading": "C. Training", "text": "Our model is implemented in TensorFlow [5]. We use the Adam Optimizer [15] with a learning rate of 0.0001. Model weights are initialized by a Gaussian distribution with a standard deviation of 0.01. LSTM states are initialized to 0.0 and Forge Gates to 1.0. Each training time step processed a mini-batch of eight 10-frame sequences. We use the L2 loss for training."}, {"heading": "D. Results", "text": "Figure 5 shows the results of our trained model. The figure shows depth predictions and predictions of the next frame from four frames of a sequence. Each column corresponds to one frame. The first line shows the last frame, and the last line shows predictions of the next frame, which are generated based on predicted depth. By comparing the quality of depth predictions for each frame, it can be determined that the predictions of the model 0 2000 4000 6000 8000 10000 12000 14000 16000 1015202530 PSNR compared to the state \u2212 of the art methods STP, DNA, CDNATimestepP SN RSSIM compared to the state \u2212 of the art methods STP, DNA, CDNA improves when more frames are seen. After seeing only the first frame, the model believes that the ground is closer than it actually is. This is reflected in the predicted depth map because the strong red tone of the premade. By obtaining more input images, the measurement size of the model improves the prediction of depth than it actually is."}, {"heading": "E. Comparison to Prior Work", "text": "To compare our model with state-of-the-art video prediction models, we trained three model variants DNA, CDNA and STP [8] on our data set. All of the above models are action-based, i.e. they have placeholders for receiver and action inputs. In addition to the video images, we pass the current camera as a state and the calculated ego motion as action to all three models. Figure 7 qualitatively compares the next frames generated by our model and the previous method by Finn et al. [8]. As we can see, [8] is usually able to place the objects in the next frame in the right place. However, it provides fuzzy predictions, especially when the scene background moves between frames. Our method, on the other hand, provides sharp and accurate predictions for previous frames: Figure 8 provides quantitative predictions generated by our model with DNA, CDNA and STP. The predictions of our method exceed previous values on both PR-93 models in depth and IM."}, {"heading": "F. Failure cases", "text": "We have observed cases where the depth of thin elongated objects, such as poles, is not correctly estimated. An example is shown in the leftmost image in Figure 5. Because our approach is based on depth estimation, this affects the quality of our next image predictions. The main reason for these errors is probably the low impact of these objects in the loss equation. Another factor contributing to this is the imperfect alignment of depth maps and video images in the training data, which has a greater impact on thin objects. These misalignments are primarily due to different time delays between the rotating lidar and the camera for different regions of the image."}, {"heading": "G. Simulating Hypothetical Next Frames", "text": "Our approach can be used to generate potential next frames based on hypothetical movements of a moving agent, such as a vehicle or a pedestrian. Such next frame simulations can be useful for exploring counterfactual world representations and examining the outcome of available actions in planning. Figures 9, 10 show exemplary next frame simulations based on a series of hypothetical first-person movements corresponding to forward / backward and sideways movements."}, {"heading": "V. CONCLUSIONS AND FUTURE WORK", "text": "We have presented a method for predicting the next image using monocular video. Our method uses an RNN that is trained to predict the depth from an image sequence. Our experience shows that the RNN can capture movement between subsequent images and improve its depth predictions. We want to improve the visual quality of predictions by 1) upsampling and inpainting ground temperature maps 2) predicting the next image if possible. Furthermore, predicting multiple images in the future is a useful extension of this work. Applying our approach to detecting anomalies is an important next step. For example, we can overlay our next frame prediction with the image actually observed and analyze inconsistencies in scene topology (depth) or appearance (RGB frame). Large deviations can be an indication of an object moving at an unexpected speed, and can be used as informative signals for safer navigation."}], "references": [{"title": "Single-image depth perception in the wild", "author": ["W. Chen", "Z. Fu", "D. Yang", "J. Deng"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "In ICCV,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Tensorflow: A system for large-scale machine learning", "author": ["M. Abadi"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Towards unified depth and semantic prediction from a single image", "author": ["P. Wang"], "venue": "In CVPR,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["X. Shi"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Unsupervised learning for physical interaction through video prediction", "author": ["C. Finn", "I. Goodfellow", "S. Levine"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Flownet: Learning optical flow with convolutional networks", "author": ["P. Fischer", "A. Dosovitskiy", "E. Ilg", "P. H\u00e4usser", "C. Haz\u0131rba\u015f", "V. Golkov", "P. van der Smagt", "D. Cremers", "T. Brox"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Learning predictive visual models of physics for playing billiards", "author": ["K. Fragkiadaki", "P. Agrawal", "S. Levine", "J. Malik"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Vision meets robotics: The kitti dataset", "author": ["A. Geiger", "P. Lenz", "C. Stiller", "R. Urtasun"], "venue": "IJRR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Long short-temp memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Image quality metrics: Psnr vs. ssim", "author": ["A. Hore", "D. Ziou"], "venue": "In Int. Conf. on Pattern Recognition,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Deeper depth prediction with fully convolutional residual networks", "author": ["I. Laina", "C. Rupprecht", "V. Belagiannis", "F. Tombari", "N. Navab"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Deep multi-scale video prediction beyond mean square error", "author": ["M. Mathieu", "C. Couprie", "Y. LeCun"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Modeling deep temporal dependencies with recurrent grammar cells", "author": ["V. Michalski", "R. Memisevic", "K. Konda"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R. L Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Video (language) modeling: a baseline for generative models of natural videos", "author": ["M. Ranzato", "A. Szlam", "J. Bruna", "M. Mathieu", "R. Collobert", "S. Chopra"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Learning a driving simulator", "author": ["E. Santana", "G. Hotz"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Unsupervised learning of video representations using lstms", "author": ["N. Srivastava", "E. Mansimov", "R. Salakhutdinov"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Anticipating visual representations from unlabeled video", "author": ["C. Vondrick", "H. Pirsiavash", "A. Torralba"], "venue": null, "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2016}, {"title": "Dense optical flow prediction from a static image", "author": ["J. Walker", "A. Gupta", "M. Hebert"], "venue": "In ICCV,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Probabilistic modeling of future frames from a single image", "author": ["T. Xue", "J. Wu", "K. Bouman", "Freeman W"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}], "referenceMentions": [{"referenceID": 16, "context": "Traditionally, many such approaches have been modelbased, with strong assumptions about what kind of scenes are permissible [18], [10] e.", "startOffset": 124, "endOffset": 128}, {"referenceID": 8, "context": "Traditionally, many such approaches have been modelbased, with strong assumptions about what kind of scenes are permissible [18], [10] e.", "startOffset": 130, "endOffset": 134}, {"referenceID": 16, "context": "While model-based methods [18], [10] perform well in restricted environments, they are not suitable for unconstrained environments.", "startOffset": 26, "endOffset": 30}, {"referenceID": 8, "context": "While model-based methods [18], [10] perform well in restricted environments, they are not suitable for unconstrained environments.", "startOffset": 32, "endOffset": 36}, {"referenceID": 22, "context": "The simplest such techniques use a 2D optical flow field computed from the video to warp the last frame [24].", "startOffset": 104, "endOffset": 108}, {"referenceID": 18, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 40, "endOffset": 44}, {"referenceID": 15, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 46, "endOffset": 50}, {"referenceID": 17, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 52, "endOffset": 56}, {"referenceID": 19, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "Recent methods based on neural networks [20], [17], [19], [21], [8] train a recurrent neural network (RNN) to predict the next frame directly from the video stream.", "startOffset": 64, "endOffset": 67}, {"referenceID": 15, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 29, "endOffset": 33}, {"referenceID": 17, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 35, "endOffset": 39}, {"referenceID": 19, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 41, "endOffset": 45}, {"referenceID": 19, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "Contrary to prior approaches [17], [19], [21], [21], [8], we use the RNN to predict not the next video frame, but the depth map of the next video frame.", "startOffset": 53, "endOffset": 56}, {"referenceID": 12, "context": "We show that this yields better outcomes in terms of visual quality, as well as, quantitative metrics, namely Peak Signal to Noise Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) [14].", "startOffset": 195, "endOffset": 199}, {"referenceID": 9, "context": "We evaluate our approach on the KITTI raw dataset [11].", "startOffset": 50, "endOffset": 54}, {"referenceID": 1, "context": "A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural", "startOffset": 14, "endOffset": 17}, {"referenceID": 0, "context": "A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural", "startOffset": 24, "endOffset": 27}, {"referenceID": 14, "context": "A few methods [3], [2], [6], [16] have demonstrated the possibility of learning depth from single images using deep neural", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "The pioneering work in [3] uses a multi-scale setup to predict depth at multiple resolutions.", "startOffset": 23, "endOffset": 26}, {"referenceID": 14, "context": "The state-ofthe-art work in [16] uses a ResNet [12] model to improve the quality of depth predictions.", "startOffset": 28, "endOffset": 32}, {"referenceID": 10, "context": "The state-ofthe-art work in [16] uses a ResNet [12] model to improve the quality of depth predictions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 20, "context": "large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17].", "startOffset": 67, "endOffset": 71}, {"referenceID": 21, "context": "large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17].", "startOffset": 73, "endOffset": 77}, {"referenceID": 18, "context": "large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17].", "startOffset": 79, "endOffset": 83}, {"referenceID": 15, "context": "large unlabeled video datasets has been a topic of recent interest [22], [23], [20], [17].", "startOffset": 85, "endOffset": 89}, {"referenceID": 18, "context": "Next frame prediction using recurrent neural networks has also been proposed in [20], [17], Fig.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "Next frame prediction using recurrent neural networks has also been proposed in [20], [17], Fig.", "startOffset": 86, "endOffset": 90}, {"referenceID": 17, "context": "[19], [25], [8].", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[19], [25], [8].", "startOffset": 6, "endOffset": 10}, {"referenceID": 6, "context": "[19], [25], [8].", "startOffset": 12, "endOffset": 15}, {"referenceID": 2, "context": "Reinforcement Learning (RL) has been experiencing renewed interest with recent successes in game environments [4].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "[44 , 1 44 , 3 2] 5x5 s=2", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "[44 , 1 44 , 3 2] 5x5 s=2", "startOffset": 0, "endOffset": 17}, {"referenceID": 1, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 0, "endOffset": 17}, {"referenceID": 0, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 0, "endOffset": 17}, {"referenceID": 20, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 18, "endOffset": 33}, {"referenceID": 5, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 18, "endOffset": 33}, {"referenceID": 0, "context": "[44 , 1 44 , 3 2] [22 , 7 2, 64 ]", "startOffset": 18, "endOffset": 33}, {"referenceID": 20, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 0, "endOffset": 15}, {"referenceID": 5, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 0, "endOffset": 15}, {"referenceID": 0, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 0, "endOffset": 15}, {"referenceID": 9, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 1, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 4, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 10, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 6, "context": "[22 , 7 2, 64 ] [11 , 3 6, 12 8]", "startOffset": 16, "endOffset": 32}, {"referenceID": 9, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 1, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 4, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 10, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 6, "context": "[11 , 3 6, 12 8] 3x3 s=2 3x3 s=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 0, "context": "[88 , 2 88 , 3 ] 5x5 Conv-LSTM5x5 Conv-LSTM5x5 Conv-LSTM [22 72 64 [22 72 64 5x5 Conv-LSTM 3x3 b=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 1, "context": "[88 , 2 88 , 3 ] 5x5 Conv-LSTM5x5 Conv-LSTM5x5 Conv-LSTM [22 72 64 [22 72 64 5x5 Conv-LSTM 3x3 b=2", "startOffset": 0, "endOffset": 16}, {"referenceID": 17, "context": "Most RL applications are developed for simulated environments [19] and transferring the methods to realworld environments has been difficult.", "startOffset": 62, "endOffset": 66}, {"referenceID": 19, "context": "Recent work [21] uses an adversarial learning approach for next frame generation in real-world environments, generating future frames with minor modifications.", "startOffset": 12, "endOffset": 16}, {"referenceID": 11, "context": "similar to regular LSTM cells [13], however, their gates are implemented by convolutions instead of fully-connected layers [7].", "startOffset": 30, "endOffset": 34}, {"referenceID": 5, "context": "similar to regular LSTM cells [13], however, their gates are implemented by convolutions instead of fully-connected layers [7].", "startOffset": 123, "endOffset": 126}, {"referenceID": 7, "context": "2) Producing and consuming intermediate low-resolution predictions as done in FlowNet [9].", "startOffset": 86, "endOffset": 89}, {"referenceID": 14, "context": "The reverse Huber loss is defined in [16] as:", "startOffset": 37, "endOffset": 41}, {"referenceID": 1, "context": "Additionally, the loss equation can include an optional term to minimize the depth Gradient Difference Loss (GDL) [3], which is defined as:", "startOffset": 114, "endOffset": 117}, {"referenceID": 9, "context": "We test our approach on the KITTI dataset [11] which is collected from a vehicle moving through urban environments.", "startOffset": 42, "endOffset": 46}, {"referenceID": 6, "context": "Comparison of next frame prediction by our method with state-of-the-art video prediction model STP [8].", "startOffset": 99, "endOffset": 102}, {"referenceID": 12, "context": "We employ two image quality metrics [14] to evaluate next frame predictions produced by the model:", "startOffset": 36, "endOffset": 40}, {"referenceID": 17, "context": "Both metrics are standard in measuring the quality of image predictions [19], [17], [8].", "startOffset": 72, "endOffset": 76}, {"referenceID": 15, "context": "Both metrics are standard in measuring the quality of image predictions [19], [17], [8].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "Both metrics are standard in measuring the quality of image predictions [19], [17], [8].", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "Our model is implemented in TensorFlow [5].", "startOffset": 39, "endOffset": 42}, {"referenceID": 13, "context": "We use the Adam optimizer [15] with a learning rate of 0.", "startOffset": 26, "endOffset": 30}, {"referenceID": 6, "context": "Comparison of quality metrics on next frame predictions by our model against STP, DNA, and CDNA methods [8] over the validation dataset (higher values are better for both metrics).", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "tion models, we trained three model variants DNA, CDNA, and STP [8] on our dataset.", "startOffset": 64, "endOffset": 67}, {"referenceID": 6, "context": "Figure 7 qualitatively compares the next frame predictions generated by our model and by the prior method of Finn et al [8].", "startOffset": 120, "endOffset": 123}, {"referenceID": 6, "context": "As we can see, [8] is usually able to place the objects at the right location in the next frame.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "In terms of PSNR, our model performs much better by producing results in the order of 24-25, whereas the three prior methods from [8] produce values in range 15-17.", "startOffset": 130, "endOffset": 133}], "year": 2016, "abstractText": "We propose a method for next frame prediction from video input. A convolutional recurrent neural network is trained to predict depth from monocular video input, which, along with the current video image and the camera trajectory, can then be used to compute the next frame. Unlike prior next-frame prediction approaches, we take advantage of the scene geometry and use the predicted depth for generating next frame prediction. A useful side effect of our technique is that it produces depth from video, which can be used in other applications. We evaluate the proposed approach on the KITTI raw dataset, which is collected from a vehicle moving through urban environments. The results are compared with the state-of-theart models for next frame prediction. We show that our method produces visually and numerically superior results to existing methods that directly predict the next frame.", "creator": "LaTeX with hyperref package"}}}