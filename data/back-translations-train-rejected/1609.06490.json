{"id": "1609.06490", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "One Sentence One Model for Neural Machine Translation", "abstract": "Neural machine translation (NMT) becomes a new state-of-the-art and achieves promising translation results using a simple encoder-decoder neural network. This neural network is trained once on the parallel corpus and the fixed network is used to translate all the test sentences. We argue that the general fixed network cannot best fit the specific test sentences. In this paper, we propose the dynamic NMT which learns a general network as usual, and then fine-tunes the network for each test sentence. The fine-tune work is done on a small set of the bilingual training data that is obtained through similarity search according to the test sentence. Extensive experiments demonstrate that this method can significantly improve the translation performance, especially when highly similar sentences are available.", "histories": [["v1", "Wed, 21 Sep 2016 10:28:57 GMT  (769kb,D)", "http://arxiv.org/abs/1609.06490v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["xiaoqing li", "jiajun zhang", "chengqing zong"], "accepted": false, "id": "1609.06490"}, "pdf": {"name": "1609.06490.pdf", "metadata": {"source": "CRF", "title": "One Sentence One Model for Neural Machine Translation", "authors": ["Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong"], "emails": ["xqli@nlpr.ia.ac.cn", "jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": "Introduction", "text": "In fact, it is that we are able to assert ourselves, that we are able to comply, that we are able to comply with the rules, and that we are able to comply with the rules that we have set ourselves."}, {"heading": "Background", "text": "In this section we briefly present the NMT system by Bahdanau et al. (2015), which will be used later in the experiments. However, our approach is model independent and can be applied to other NMT systems. At a source set s = (s1, s2,... sm) and its translation t = (t1, t2,..., tn), NMT models the translation probability using a single neural network as follows, ar Xiv: 160 9.06 490v 1 [cs.C L] 21. September 2016"}, {"heading": "Test n", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Fine tune", "text": "The encoder reads the source set and encodes it in a sequence of hidden states h = h1, h2,..., hm with bidirectional GRU.hi = [\u2212 \u2192 h i; \u2190 \u2212 h i] (2) \u2212 \u2192 h i = \u2212 \u2192 \u03c6 (\u2212 \u2192 h i \u2212 1, xi) (3) \u2190 \u2212 h i = \u2190 \u2212 \u03c6 (\u2190 \u2212 h i + 1, xi) (4), where xi is the embedding of the current word and the recurring activation functions are \u2212 receptively recurring units. The decoder consists of a recursive neural network and an attention mechanism. The recurrent neural network calculates a hidden state for each target position as follows, zj = zj \u2212 receptional (zj \u2212 1, yj > cj, i \u2212 ek), whereby the recursive neural mechanism consists of a recursive network and an attention mechanism."}, {"heading": "Tuning on-the-fly", "text": "As illustrated in Figure 1, the learning strategy of our approach is simple. First, we learn a general model of the entire training corpus; then, for each test set, we extract a small subset of training data consisting of pairs of sentences whose source pages resemble the test set; this subset is used to refine the general model and obtain a specific model for the test set. This approach can be formulated as a two-step optimization; the first step is to find a set of network parameters to maximize the protocol probability of all training data. D = (s (s (1), t (1), (s (2), t (2),..., (s (N), t (N)))); the first step is to find a set of network parameters to maximize the protocol probability of all training data."}, {"heading": "Similarity Measure", "text": "rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc"}, {"heading": "Fine tuning", "text": "The main difference is that the size of the data used for fine-tuning is very small and usually contains only a few pairs of sentences, so we have to be careful to verify the overmatch of translation knowledge from the tuning data. To this end, we will go over the adaptive rate optimization methods, such as Adadelta (line 2012), Labor, and SGD with carefully tuned learning rates so that we adopt them in our experiments. Handle the case with low similarity We cannot always find very similar sentences to the test sentence, especially if there is not enough in-domain training data. In this case, we suggest finding sentences to maximize phrase coverage. The phrase we mention here has the same meaning as the phrase-based machine translation that has a consecutive word."}, {"heading": "Experiments", "text": "We rate our method on the translation task from Chinese to English. The quality of the translation is measured using the BLEU metric (Papineni et al. 2002)."}, {"heading": "Datasets", "text": "We are conducting experiments with two sets of data, one of which is located on the United Nations Parallel Corpus2, which is composed of official records and other United Nations parliamentary documents. As these data come from a narrow range, we can easily find similar sets for many sets of tests. Training data contain 1M pairs of sentences extracted from the corpus, and the test data contain 5 groups of pairs of sentences, each containing 200 pairs of sentences. The most similar set we can find for the sentences in each group falls within the similarity range of 0-0.2, 0.2-0.4, 0.4-0.6, 0.6-0.8, and 0.8-1.0, respectively. We also randomly have 1,000 pairs of sentences as development set.The training data of the other data set is selected from LDC4, which contains about 1.2M pairs of sentences, whose sources of news, laws, hansard records, weblogs, spoken dialogues, and so on. And we are using NIST 03 as development 1.We have also tried to make the most urgent word pairs, but are only slightly worse at correcting them."}, {"heading": "Experiment Setting", "text": "The hyperparameters used in our network are described as follows: We limit both the source and target vocabulary in our experiments to 30k; the number of hidden units is 1,000 for both the encoder and the decoder; and the embedding dimension is 500 for all source and target tokens; the network parameters are updated both for training and for fine-tuning with the Adadelta algorithm. If we find similar sentences based on phrase coverage, we retain two top target sentences for each source phrase; and if a source phrase appears more than 1,000 times in the bilingual corpus, it is discarded because it is unnecessary to re-learn how to translate these common phrases."}, {"heading": "Experiments on UN Data", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "Experiments on LDC Data", "text": "In this experiment, for more than 90% of the test sets, we can only find similar sets in the range of 0-0.4. And, according to our study on the development set, the number of sets used for fine-tuning must be increased to 128 in order to achieve the best performance when the similarity is low. We think the reason for this is the variety of training data. Sets in the low similarity range can have completely different themes and styles with the test set. To avoid the influence of these unwanted data, more sets need to be used. The performances on the test data are in Table 2. They are achieved with the following setting, if very similar sets (0.4-1) can be found, we will only use one set for fine-tuning, otherwise we will use 128 sets. On average, 1.2 BLEU points can be achieved on the three test sets, which is in line with the experimental results on the UN data set if very similar sets (0.4-1) cannot be found."}, {"heading": "Result Analysis", "text": "We show two examples in Table 3. The above example is the case where a very similar sentence to the test sentence can be found. After fine-tuning, the model remembers how to generate the translation for the similar sentence. On the basis of the backbone, it can generate a correct translation for the test sentence with a slight modification. In the example below, we only find a sentence that is not so similar to the test sentence. However, the pair of sentences found in the example can remind the model how to translate the phrase,,, that is missing in the baseline system."}, {"heading": "Related Work", "text": "Neural machine translation has a short history of only a few years. Kalchbrenner and Blunsom (2013) and Cho et al. (2014) initially propose using the encoder decoder architecture to create sequence mapping sequences, while Sutskever et al. (2014) apply it to end-to-end machine translation. Bahdanau et al. (2015) propose the attention mechanism to dynamically pay attention to different source words when generating different target words that become the standard component of current NMT systems. Recent advances in NMT include fixing deficiencies in the model, such as the inability to use large vocabularies (Luong et al. 2015; Jean et al. 2015), the lack of reach (Tu et al. 2016; Mi et al. 2016), and so on, the use of monolingual data (Chal et. 2016, Srich, and Kodow)."}, {"heading": "Linguistics and the 7th International Joint Conference on", "text": "Natural Language Processing (Volume 1: Long Papers), 11-19. Beijing, China: Association for Computational Linguistics. [Ma et al. 2011] Ma, Y.; He, Y.; Way, A.; and van Genabith, J. 2011. Consistent translation with discriminatory learning: a translation memory-inspired approach. [Wed et al. 2016] Wed, H.; Sankaran, B.; Wang, Z.; and Ittycheriah, A. 2016. An Embedding Model for Neural Machine Translation. arXiv preprint arXiv: 1605.03148. [Mikolov et al. 2013.] Mikolov, T.; Sutskever, K.; Corrado, S. Li."}], "references": [{"title": "Y", "author": ["D. Bahdanau", "K. Cho", "Bengio"], "venue": "2015. Neural machine translation by jointly learning to align and translate. In ICLR", "citeRegEx": "Bahdanau. Cho. and Bengio 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Scheduled sampling for sequence prediction with recurrent neural networks", "author": ["Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Bengio,? \\Q2015\\E", "shortCiteRegEx": "Bengio", "year": 2015}, {"title": "1997", "author": ["Broder", "A. Z"], "venue": "On the resemblance and containment of documents. In Compression and Complexity of Sequences", "citeRegEx": "Broder 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Semi-supervised learning for neural machine translation", "author": ["Cheng"], "venue": "arXiv preprint arXiv:1606.04596", "citeRegEx": "Cheng,? \\Q2016\\E", "shortCiteRegEx": "Cheng", "year": 2016}, {"title": "Learning phrase representations using rnn encoder\u2013 decoder for statistical machine translation", "author": ["Cho"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["Dong"], "venue": "In Proceedings of the 53rd Annual Meeting of the ACL and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Dong,? \\Q2015\\E", "shortCiteRegEx": "Dong", "year": 2015}, {"title": "and Riezler", "author": ["J. Hitschler"], "venue": "S.", "citeRegEx": "Hitschler and Riezler 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["Jean"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural", "citeRegEx": "Jean,? \\Q2015\\E", "shortCiteRegEx": "Jean", "year": 2015}, {"title": "and Blunsom", "author": ["N. Kalchbrenner"], "venue": "P.", "citeRegEx": "Kalchbrenner and Blunsom 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "and Senellart", "author": ["P. Koehn"], "venue": "J.", "citeRegEx": "Koehn and Senellart 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "A discriminative framework of integrating translation memory features into smt", "author": ["Way Li", "L. Liu 2014] Li", "A. Way", "Q. Liu"], "venue": "In Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association", "citeRegEx": "Luong,? \\Q2015\\E", "shortCiteRegEx": "Luong", "year": 2015}, {"title": "Consistent translation using discriminative learning: a translation memory-inspired approach", "author": ["Ma"], "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language", "citeRegEx": "Ma,? \\Q2011\\E", "shortCiteRegEx": "Ma", "year": 2011}, {"title": "A coverage embedding model for neural machine translation", "author": ["Mi"], "venue": "arXiv preprint arXiv:1605.03148", "citeRegEx": "Mi,? \\Q2016\\E", "shortCiteRegEx": "Mi", "year": 2016}, {"title": "G", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "Corrado"], "venue": "S.; and Dean, J.", "citeRegEx": "Mikolov et al. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Papineni"], "venue": "In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,", "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "Improving neural machine translation models with monolingual data", "author": ["Haddow Sennrich", "R. Birch 2015] Sennrich", "B. Haddow", "A. Birch"], "venue": "arXiv preprint arXiv:1511.06709", "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Minimum risk training for neural machine translation", "author": ["Shen"], "venue": "arXiv preprint arXiv:1512.02433", "citeRegEx": "Shen,? \\Q2015\\E", "shortCiteRegEx": "Shen", "year": 2015}, {"title": "Q", "author": ["I. Sutskever", "O. Vinyals", "Le"], "venue": "V.", "citeRegEx": "Sutskever. Vinyals. and Le 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Coverage-based neural machine translation", "author": ["Tu"], "venue": "arXiv preprint arXiv:1601.04811", "citeRegEx": "Tu,? \\Q2016\\E", "shortCiteRegEx": "Tu", "year": 2016}, {"title": "Integrating translation memory into phrase-based machine translation during decoding", "author": ["Wang"], "venue": "In ACL", "citeRegEx": "Wang,? \\Q2013\\E", "shortCiteRegEx": "Wang", "year": 2013}, {"title": "A", "author": ["S. Wiseman", "Rush"], "venue": "M.", "citeRegEx": "Wiseman and Rush 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "M", "author": ["Zeiler"], "venue": "D.", "citeRegEx": "Zeiler 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "and Knight", "author": ["B. Zoph"], "venue": "K.", "citeRegEx": "Zoph and Knight 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Neural machine translation (NMT) becomes a new state-ofthe-art and achieves promising translation results using a simple encoder-decoder neural network. This neural network is trained once on the parallel corpus and the fixed network is used to translate all the test sentences. We argue that the general fixed network cannot best fit the specific test sentences. In this paper, we propose the dynamic NMT which learns a general network as usual, and then fine-tunes the network for each test sentence. The fine-tune work is done on a small set of the bilingual training data that is obtained through similarity search according to the test sentence. Extensive experiments demonstrate that this method can significantly improve the translation performance, especially when highly similar sentences are available.", "creator": "LaTeX with hyperref package"}}}