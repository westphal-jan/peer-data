{"id": "1609.06530", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Weakly supervised spoken term discovery using cross-lingual side information", "abstract": "Recent work on unsupervised term discovery (UTD) aims to identify and cluster repeated word-like units from audio alone. These systems are promising for some very low-resource languages where transcribed audio is unavailable, or where no written form of the language exists. However, in some cases it may still be feasible (e.g., through crowdsourcing) to obtain (possibly noisy) text translations of the audio. If so, this information could be used as a source of side information to improve UTD. Here, we present a simple method for rescoring the output of a UTD system using text translations, and test it on a corpus of Spanish audio with English translations. We show that it greatly improves the average precision of the results over a wide range of system configurations and data preprocessing methods.", "histories": [["v1", "Wed, 21 Sep 2016 12:43:53 GMT  (708kb,D)", "http://arxiv.org/abs/1609.06530v1", "5 pages, 4 figures, submitted for ICASSP 2017"]], "COMMENTS": "5 pages, 4 figures, submitted for ICASSP 2017", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["sameer bansal", "herman kamper", "sharon goldwater", "adam lopez"], "accepted": false, "id": "1609.06530"}, "pdf": {"name": "1609.06530.pdf", "metadata": {"source": "CRF", "title": "WEAKLY SUPERVISED SPOKEN TERM DISCOVERY USING CROSS-LINGUAL SIDE INFORMATION", "authors": ["Sameer Bansal", "Herman Kamper", "Sharon Goldwater", "Adam Lopez"], "emails": ["h.kamper}@sms.ed.ac.uk,", "alopez}@inf.ed.ac.uk"], "sections": [{"heading": null, "text": "Index terms - unsupervised terminology, low-resource language processing, language translation, poorly supervised learning"}, {"heading": "1. INTRODUCTION", "text": "High-quality Automated Speech Recognition (ASR) systems require hundreds of hours of transcribed training data, and as a result, they are currently only available for a tiny fraction of the several thousand languages in the world. [1] In order to expand accessibility, considerable progress has been made in this area lately [8-11], with learning from audio alone being very challenging. Here, we are asking whether the use of page information could improve performance. In particular, we are looking at the task of Unattended Discovery of Terms (UTD), which aims to identify and cluster repetitive word-like units of audio. We show that UTD can be improved by means of text translations into another language. Such translations can be achieved quickly through crowdsourcing, for example in disaster relief scenarios such as the earthquake in Haiti [12]."}, {"heading": "2. UNSUPERVISED TERM DISCOVERY", "text": "Zero resource language technology addresses a number of different problems, from the automatic discovery of subwords [16,17] and improved feature representations [18, 19] to the complete segmentation and clustering of audio into word-like units [9, 11, 20, 21]. Unmonitored terminology is one of the most developed areas. Essentially, UTD systems are looking for pairs of similar audio segments, measured by their dynamic time warping (DTW) [22] spacing. This task is inherently square in input size, and early systems [2,23] were prohibitively slow. In this context, we are using the open source implementation in Zero Resource Toolkit (ZRTools) 1 [4], a state-of-the-art system that uses a more efficient two-pass approach. It is also the only freely available UTD system we know of."}, {"heading": "2.1. Overview of the ZRTools UTD system", "text": "In its first pass, ZRTools uses an approximate random algorithm and image processing techniques to extract potentially matching segments. Image processing is based on the intuition that if we draw the cosinal similarity between each frame of the input function vector representation (e.g. MFCCs), all repeating segments in the pair of utterances will show up as diagonal line patterns. Figure 1 (a) illustrates this and shows a clear diagonal pattern that corresponds to similar words in two utterances. In its second pass, ZRTools calculates a normalized DTW value over potential matches to extract the final output. It provides segment pairs that are longer than a minimum duration (we used the recommended value of 500 ms) along with their DTW value (between 0 and 1, with higher values indicating greater similarity). These segment-type or keyword phrases can then be used for modeling and tagging."}, {"heading": "2.2. Limitations of UTD", "text": "Like all UTD systems, ZRTools identifies patterns solely using acoustic information. This can lead to various types of errors, which we hope to reduce through cross-linguistic page information."}, {"heading": "2.2.1. Mismatch between acoustic and semantic information", "text": "Some phonetically similar pairs identified by UTD still differ semantically, as shown in Figure 1 (b). The words in the utterances are different, but UTD identifies similar phoneme sequences a n o p w e and e n o p w e. Acoustic information alone cannot eliminate these errors, yet they cause semantic errors in downstream applications such as machine translation or spoken retrieval of documents. On the other hand, due to noise and variability (both within and between speakers), not all semantically correct matches are rated with a high DTW score. ZRTools documentation recommends the use of a DTW score threshold of 0.88 to filter out good matches, but there are many correct pairs with values less than 0.88. An example is illustrated in Figure 1 (c), where a correct match will return a score just below the Cut-off threshold, but we can (of course) lower the number of returned TW pairs."}, {"heading": "2.2.2. Silence and filler words as valid matches", "text": "We observed that these phenomena generate a large number of detected pairs, as they occur frequently and often have good acoustic matches with each other. UTD's number of non-word pairs found due to these phenomena depends on the pre-processing of the data. To show that our method improves UTD output regardless of pre-processing, we are experimenting with two different pre-processing methods. First, we are using the Automatic Speech Activity Recognition Script (VAD) supplied with ZRTools, which uses Root Mean Squared (RMS) Energy Detection to identify VAD regions. Only these regions are then used in the search for patterns. This method aggressively filters out the silence, but removes a significant amount of valid language. It also retains many filler words, which often have high energy and long life. Alternatively, we can use a forced alignment (FA) of the language data with the transcripts to extract a larger number of valid words, but a more valid one word."}, {"heading": "3. IMPROVING UTD USING TRANSLATIONS", "text": "Given two language expressions, we assume that the similarity in their translations provides a (noisy) signal of semantic similarity between the acoustic units detected, and that this signal can improve the UTD. Consider the examples in Table 1, which shows the English translations of the expressions containing the segments shown in Figure 1. (The expressions are truncated, so not all Spanish words are displayed.) Stop words are displayed in brackets; we filter them out by using the NLTK toolkit2 before calculating translation similarities. Note that the two pairs (a and c) that have matching Spanish words also have matching English contents, although one of them falls below the recommended threshold of 0.88 for a UTD (acoustic) match. On the other hand, pair (b) has a high UTD score due to phonetic similarity, but there is no match between the English words."}, {"heading": "4. EXPERIMENTAL SETUP AND EVALUATION", "text": "In all the experiments, the input consists of speech from the CALLHOME Spanish corpus = = average English translations of [15]. The corpus consists of speech from telephone conversations between native speakers. We use the default property representation used by ZRtools: 39-dimensional Relative Spectral Transform - Perceptual Linear Prediction (PLP) feature vectors. We perform four sets of experiments, as summarized in Table 2. Note that the energy-based VAD filter filters out far more of the data than forced alignment. For each phone call, we have two channels of audio, each with at least one speaker, but sometimes more. The same speaker may be on several calls. However, for the purposes of our cross-speaker evaluation, we assume that each channel corresponds to a unique speaker. To evaluate the results of the raw UTD system and our rescoring method, we use the original CALLME transcripts, which are actually a few Spanish transcriptions."}, {"heading": "5. RESULTS AND DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Baseline UTD system", "text": "The number of pairs detected using energy-based VAD is expected to be small, as large portions of the voice data are filtered out, and the use of forced adjustments leads to a higher number of pairs detected, but at the cost of precision. The small number of pairs of crossovers listed in Table 3 underscores the difficulty of detecting them, even though they are important for downstream tasks [26, 27]. Translation information may be particularly useful for identifying crossover pairs, but our method is limited by the small number of pairs that are detected at all, as shown in the xspk columns in Table 3. In the future, we plan to investigate whether translation information could be fed into the UTD system at an earlier stage to detect more pairs of crossovers."}, {"heading": "5.2. Improvements using translations", "text": "Figure 2 illustrates the utility of our system by showing precision / recall curves for the (50, FA) setting. By using only acoustic information and variing the value of D, only points on the lower (blue) curve can actually be reached (e.g. the red dot for D = 0.88). Using translations (here \u03b1 = 0.4) significantly improves the results. To show that these advantages are not highly sensitive to \u03b1, Figure 3 records the AP for all configurations listed in Table 2, for \u03b1 between 0 and 1. For each configuration and each \u03b1 > 0 setting, we get higher AP values than baseline \u03b1 = 0, often by a large distance. AP values for \u03b1 = 0.4, which are one of many good values, are listed in Table 4. Note that the AP numbers are only comparable between systems that use the same data / preprocessing configuration, as the total number of pairs that need to be detected to achieve 100% precision is different."}, {"heading": "6. CONCLUSION", "text": "We have shown that page information in the form of translations improves the output of UTD in a wide range of settings. In future work, we will use the improved UTD output to learn better cross-language properties for settings with limited resources, and investigate the use of translations as a pre-processing step for UTD by helping to find matches. We also aim to extend this work to a semi-monitored environment by using additional unlabeled language data to improve UTD. We believe that this can be done with approaches such as label propagation [28] and label dissemination [29, 30]."}, {"heading": "7. ACKNOWLEDGMENTS", "text": "We thank David Chiang and Antonios Anastasopoulos for agreeing the CALLHOME speech and transcripts; Aren Jansen for supporting ZRTools; and Marco Damonte, Federico Fancellu, Sorcha Gilroy, Ida Szubert and Clara Vania for comments on previous drafts. This work was partially supported by a Scholar Award from the James S McDonnell Foundation and a Research Award from the Google Faculty."}, {"heading": "8. REFERENCES", "text": "[1]. Besacier, E. Barnard, A. Karpov, and T. Schultz, \"Automatic speech recognition for languages with too few resources: A survey,\" Speech Communication, vol. 56, pp. 85-100, 2014. [2] A. S. Park and J. R. Glass and J. R. Glass, \"Unsupervised pattern discovery in speech,\" IEEE Trans. Audio, Speech, Language Process. [4] A. Jansen and B. Van Durme, \"Efficient spoken discovery using randomized algorithms,\" in Proc. ASRU, 2011. [5] Y. Zhang, R. Salakhutdinov, H.-A. Chang, and J. R. Glass, \"Resource configurable spoken discovery using proc."}], "references": [{"title": "Automatic speech recognition for under-resourced languages: A survey", "author": ["L. Besacier", "E. Barnard", "A. Karpov", "T. Schultz"], "venue": "Speech Communication, vol. 56, pp. 85\u2013100, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised pattern discovery in speech", "author": ["A.S. Park", "J.R. Glass"], "venue": "IEEE Trans. Audio, Speech, Language Process., vol. 16, no. 1, pp. 186\u2013197, 2008.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2008}, {"title": "Towards multi-speaker unsupervised speech pattern discovery", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ICASSP, 2010.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient spoken term discovery using randomized algorithms", "author": ["A. Jansen", "B. Van Durme"], "venue": "Proc. ASRU, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Resource configurable spoken query detection using deep Boltzmann machines", "author": ["Y. Zhang", "R. Salakhutdinov", "H.-A. Chang", "J.R. Glass"], "venue": "Proc. ICASSP, 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "The spoken web search task at MediaEval 2012", "author": ["F. Metze", "X. Anguera", "E. Barnard", "M. Davel", "G. Gravier"], "venue": "Proc. ICASSP, 2013.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental acoustic indexing for zero resource keyword search", "author": ["K. Levin", "A. Jansen", "B. Van Durme"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "The Zero Resource Speech Challenge 2015", "author": ["M. Versteegh", "R. Thiolli\u00e8re", "T. Schatz", "X.N. Cao", "X. Anguera", "A. Jansen", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word discovery from speech using automatic segmentation into syllable-like units", "author": ["O.J. R\u00e4s\u00e4nen", "G. Doyle", "M.C. Frank"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Fully unsupervised small-vocabulary speech recognition using a segmental bayesian model", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Unsupervised word segmentation and lexicon discovery using acoustic word embeddings", "author": ["H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "IEEE/ACM Trans. Audio, Speech, Language Process., vol. 24, no. 4, pp. 669\u2013679, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Crowdsourced translation for emergency response in Haiti: the global collaboration of local knowledge", "author": ["R. Munro"], "venue": "AMTA Workshop on Collaborative Crowdsourcing for Translation, 2010, pp. 1\u20134.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Utterance classification in speech-to-speech translation for zero-resource languages in the hospital administration domain", "author": ["L.J. Martin", "A. Wilkinson", "S.S. Miryala", "V. Robison", "A.W. Black"], "venue": "Proc. ASRU, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "An attentional model for speech translation without transcription", "author": ["L. Duong", "A. Anastasopoulos", "D. Chiang", "S. Bird", "T. Cohn"], "venue": "Proc. NAACL HLT, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved speech-to-text translation with the Fisher and Callhome Spanish\u2013English speech translation corpus", "author": ["M. Post", "G. Kumar", "A. Lopez", "D. Karakos", "C. Callison-Burch", "S. Khudanpur"], "venue": "Proc. IWSLT, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "A nonparametric Bayesian approach to acoustic model discovery", "author": ["C.-y. Lee", "J.R. Glass"], "venue": "Proc. ACL, 2012.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised training of an HMM-based self-organizing unit recognizer with applications to topic classification and keyword discovery", "author": ["M.-H. Siu", "H. Gish", "A. Chan", "W. Belfield", "S. Lowe"], "venue": "Comput. Speech Lang., vol. 28, no. 1, pp. 210\u2013223, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Discovering discrete subword units with binarized autoencoders and hidden-Markovmodel encoders", "author": ["L. Badino", "A. Mereta", "L. Rosasco"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling", "author": ["R. Thiolli\u00e8re", "E. Dunbar", "G. Synnaeve", "M. Versteegh", "E. Dupoux"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A hierarchical system for word discovery exploiting DTW-based initialization", "author": ["O. Walter", "T. Korthals", "R. Haeb-Umbach", "B. Raj"], "venue": "Proc. ASRU, 2013.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised lexicon discovery from acoustic input", "author": ["C.-y. Lee", "T. O\u2019Donnell", "J.R. Glass"], "venue": "Trans. ACL, vol. 3, pp. 389\u2013403, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic programming algorithm optimization for spoken word recognition", "author": ["H. Sakoe", "S. Chiba"], "venue": "IEEE Trans. Acoustics, Speech, Signal Process., vol. 26, no. 1, pp. 43\u201349, 1978.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1978}, {"title": "Unsupervised spoken keyword spotting via segmental dtw on gaussian posteriorgrams", "author": ["Y. Zhang", "J.R. Glass"], "venue": "Proc. ASRU, 2009.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "NLP on spoken documents without ASR", "author": ["M. Dredze", "A. Jansen", "G. Coppersmith", "K. Church"], "venue": "Proc. EMNLP, 2010.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Distribution de la Flore Alpine: dans le Bassin des dranses et dans quelques r\u00e9gions voisines", "author": ["P. Jaccard"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 1901}, {"title": "Unsupervised neural network based feature extraction using weak top-down constraints", "author": ["H. Kamper", "M. Elsner", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. ICASSP, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of neural network methods for unsupervised representation learning on the Zero Resource Speech Challenge", "author": ["D. Renshaw", "H. Kamper", "A. Jansen", "S.J. Goldwater"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning from labeled and unlabeled data with label propagation", "author": ["X. Zhu", "Z. Ghahramani"], "venue": "Tech. Rep., CMU-CALD- 02-107, 2002.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2002}, {"title": "Label propagation and quadratic criterion", "author": ["Y. Bengio", "O. Delalleau", "N. Le Roux"], "venue": "Semi-supervised learning, vol. 10, 2006.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning with local and global consistency", "author": ["D. Zhou", "O. Bousquet", "T.N. Lal", "J. Weston", "B. Sch\u00f6lkopf"], "venue": "Advances in Neural Information Processing Systems, vol. 16, no. 16, pp. 321\u2013328, 2004.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": "As a result, they are currently available for only a tiny fraction of the world\u2019s several thousand languages [1].", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 137, "endOffset": 142}, {"referenceID": 2, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 137, "endOffset": 142}, {"referenceID": 3, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 137, "endOffset": 142}, {"referenceID": 4, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 163, "endOffset": 168}, {"referenceID": 5, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 163, "endOffset": 168}, {"referenceID": 6, "context": "To broaden accessibility, research on zeroresource speech technology aims to develop useful systems, such as unsupervised term discovery [2\u20134] or query-by-example [5\u20137], without the need for transcribed audio data.", "startOffset": 163, "endOffset": 168}, {"referenceID": 7, "context": "While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging.", "startOffset": 64, "endOffset": 70}, {"referenceID": 8, "context": "While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging.", "startOffset": 64, "endOffset": 70}, {"referenceID": 9, "context": "While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging.", "startOffset": 64, "endOffset": 70}, {"referenceID": 10, "context": "While considerable progress has been made in this area recently [8\u201311], learning from audio alone is very challenging.", "startOffset": 64, "endOffset": 70}, {"referenceID": 11, "context": "Such translations can often be obtained rapidly through crowd-sourcing, for example in disaster relief scenarios such as the 2010 Haiti earthquake [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 12, "context": "And when the low-resource language has no written form, text translations (ideally into a related language, as in [13]) may be considerably easier to obtain than a phonetic transcription.", "startOffset": 114, "endOffset": 118}, {"referenceID": 13, "context": "Recent work [14] has begun to explore how to translate key words and phrases based on the kind of training data we use here.", "startOffset": 12, "endOffset": 16}, {"referenceID": 14, "context": "Although our ultimate goal is to work with truly low-resource languages, ours is the first attempt we know of to address this task setting, so as a proof of concept we present results using a dataset of Spanish speech paired with English text translations [15].", "startOffset": 256, "endOffset": 260}, {"referenceID": 15, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 124, "endOffset": 131}, {"referenceID": 16, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 124, "endOffset": 131}, {"referenceID": 17, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 169, "endOffset": 177}, {"referenceID": 18, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 169, "endOffset": 177}, {"referenceID": 8, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 248, "endOffset": 263}, {"referenceID": 10, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 248, "endOffset": 263}, {"referenceID": 19, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 248, "endOffset": 263}, {"referenceID": 20, "context": "Zero-resource speech technology addresses a number of different problems, ranging from automatic discovery of subword units [16,17] and improved feature representations [18, 19] to full segmentation and clustering of the audio into word-like units [9, 11, 20, 21].", "startOffset": 248, "endOffset": 263}, {"referenceID": 21, "context": "Essentially, UTD systems search for pairs of audio segments that are similar, as measured by their dynamic time warping (DTW) [22] distance.", "startOffset": 126, "endOffset": 130}, {"referenceID": 1, "context": "This task is inherently quadratic in the input size, and early systems [2,23] were prohibitively slow.", "startOffset": 71, "endOffset": 77}, {"referenceID": 22, "context": "This task is inherently quadratic in the input size, and early systems [2,23] were prohibitively slow.", "startOffset": 71, "endOffset": 77}, {"referenceID": 3, "context": "Here, we use the open-source implementation in the Zero Resource Toolkit (ZRTools) [4], a stateof-the-art system which uses a more efficient two-pass approach.", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "These word-like or phrase-like segments can then be used for downstream tasks like keyword search and topic modeling [2, 23, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 22, "context": "These word-like or phrase-like segments can then be used for downstream tasks like keyword search and topic modeling [2, 23, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 23, "context": "These word-like or phrase-like segments can then be used for downstream tasks like keyword search and topic modeling [2, 23, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 3, "context": "Full details of the system can be found in [4].", "startOffset": 43, "endOffset": 46}, {"referenceID": 24, "context": "words), and use Jaccard similarity [25]:", "startOffset": 35, "endOffset": 39}, {"referenceID": 14, "context": "In all experiments, the input consists of speech from the CALLHOME Spanish corpus and the crowdsourced English translations of [15].", "startOffset": 127, "endOffset": 131}, {"referenceID": 25, "context": "The low number of cross-speaker pairs listed in Table 3 highlights the difficulty of discovering these, though they are important for downstream tasks [26, 27].", "startOffset": 151, "endOffset": 159}, {"referenceID": 26, "context": "The low number of cross-speaker pairs listed in Table 3 highlights the difficulty of discovering these, though they are important for downstream tasks [26, 27].", "startOffset": 151, "endOffset": 159}, {"referenceID": 27, "context": "We believe this can be done using approaches such as label propagation [28] and label spreading [29, 30].", "startOffset": 71, "endOffset": 75}, {"referenceID": 28, "context": "We believe this can be done using approaches such as label propagation [28] and label spreading [29, 30].", "startOffset": 96, "endOffset": 104}, {"referenceID": 29, "context": "We believe this can be done using approaches such as label propagation [28] and label spreading [29, 30].", "startOffset": 96, "endOffset": 104}], "year": 2016, "abstractText": "Recent work on unsupervised term discovery (UTD) aims to identify and cluster repeated word-like units from audio alone. These systems are promising for some very low-resource languages where transcribed audio is unavailable, or where no written form of the language exists. However, in some cases it may still be feasible (e.g., through crowdsourcing) to obtain (possibly noisy) text translations of the audio. If so, this information could be used as a source of side information to improve UTD. Here, we present a simple method for rescoring the output of a UTD system using text translations, and test it on a corpus of Spanish audio with English translations. We show that it greatly improves the average precision of the results over a wide range of system configurations and data preprocessing methods.", "creator": "LaTeX with hyperref package"}}}