{"id": "1609.06570", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning", "abstract": "Imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub:", "histories": [["v1", "Wed, 21 Sep 2016 14:16:14 GMT  (12kb)", "http://arxiv.org/abs/1609.06570v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["guillaume lemaitre", "fernando nogueira", "christos k aridas"], "accepted": false, "id": "1609.06570"}, "pdf": {"name": "1609.06570.pdf", "metadata": {"source": "CRF", "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning", "authors": ["Guillaume Lem\u00e2\u0131tre", "Fernando Nogueira", "Christos K. Aridas"], "emails": ["g.lemaitre58@gmail.com", "fmfnogueira@gmail.com", "char@upatras.gr"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.06 570v 1 [csKeywords: Imbalanced Dataset, Over-Sampling, Under-Sampling, Ensemble Learning, Machine Learning, Python."}, {"heading": "1. Introduction", "text": "This imbalance leads to the problem of class imbalance (Prati et al., 2009) (or \"Curse of Unbalanced Data Sets\"), which is the problem of learning a concept from a class that has a small number of samples (Yang and Wu, 2006; Rastgoo et al., 2016). Unbalanced data significantly impair the learning process, since most of the standard machine learning algorithms expect a balanced class distribution or equal misclassification costs (He and Garcia, 2009). Therefore, several ap-c \u00a9 2016 Guillaume Lema, Kugueira and Christos K. Aridas.proaches are a balanced class distribution or equal misclassification costs (he and Garcia, 2009)."}, {"heading": "2. Project management", "text": "Quality Assurance To ensure code quality, a series of unit tests is provided, resulting in 99% coverage for release 0.1.8 of the toolbox. In addition, code consistency is ensured through compliance with PEP8 standards, and each new post is automatically reviewed through the landscape, providing key metrics related to code quality. Continuous Integration To enable users and developers to either use or contribute to this toolbox, Travis CI is used to easily integrate new code and ensure feedback. Community-based development All development is done in a collaborative manner. Tools such as git, GitHub and lattice are used to facilitate collaborative programming, problem tracking, code integration and idea discussions. Documentation Uniform API documentation is provided using Sphinx and Numpydoc. Additional installation instructions and examples are also provided per week, and less than 300 are attended per week on Gitanz Repositioned at the time of the 2,000 project level."}, {"heading": "3. Implementation design", "text": "1 of sk l e a rn. d a t a s import mak e c l a s i f i c a t i o n 2 of sk l e a rn. decompos it ion import PCA 3 from imblearn. over sampl ing import SMOTE 4 5 # Generate the datase t 6 X, y = mak e c l a s i f i c a t i o n (n c l a s s s s s s s s = 2, weights = [0.1, 0. 9], 7 n f e a t u r e = 20, n samples = 5000) 89 # Apply the SMOTE over \u2212 sampling 10 sm = SMOTE (r a t i = 'auto', kind = 'r e gu l a r') 11 X resampled, y resampled = sm."}, {"heading": "4. Implemented methods", "text": "The Unbalanced Learning toolbox offers four different strategies to address the problem of the unbalanced data set: (i) too little sampling, (ii) too much sampling, (iii) a combination of both, and (iv) ensemble learning. The following subsections give an overview of the techniques implemented."}, {"heading": "4.1 Notation and background", "text": "Let us consider an unbalanced dataset, where \u03c7min and \u03c7maj are the subset of samples belonging to the minority or majority class, respectively. The equilibrium of the dataset is defined as: r\u0442 = | \u03c7min | Khalmaj |, (1) where | \u00b7 | denotes the cardinality of a set. The equilibrium process is equivalent to the conversion of a dataset into a new dataset."}, {"heading": "4.2 Under-sampling", "text": "The implemented methods can be divided into two groups: (i) defined subsamples and (ii) cleaning of subsamples. Fixed subsamples refer to the methods that perform subsamples to obtain the appropriate balance ratio. The implemented methods perform the subsamples on the basis of different criteria, such as: (i) random selection, (ii) clusters, (iii) closest neighboring rule (i.e. NearMiss (Mani and Zhang, 2003) and (iv) classification accuracy (i.e. hardness threshold (Smith et al., 2014). Unlike the previous methods, cleaning subsample samples does not allow specific achievement of the balance ratio, but purifies the characteristic space on the basis of some empirical criteria. These criteria are derived from the closest neighbor rule, namely: (i) condensed subneighbors (1976 Laudengelesed, 1968, Cubie Neighborhood Wilson)."}, {"heading": "4.3 Over-sampling", "text": "Unlike the subsample, data evaluation can be carried out by over-sampling, so that new samples are generated in \u03c7min to achieve the balance ratio r\u0443res. Currently, two methods are available: (i) Random over-sampling is performed by random replication of \u03c7min samples to obtain the corresponding balance ratio r\u0438res and SMOTE, which randomly generate new samples between several of \u03c7min's closest neighbours (Chawla et al., 2002). Various variants of this algorithm have been proposed: SMOTE borderline 1 & 2 (Han et al., 2005) and SMOTE SVM (Nguyen et al., 2011)."}, {"heading": "4.4 Combination of over- and under-sampling", "text": "SMOTE overstitch samples can lead to overmatch, which can be avoided by applying cleaning substitue samples (Prati et al., 2009). In this respect, Batista et al. (2003) combined SMOTE either with Tomek links or processed the nearest neighbors."}, {"heading": "4.5 Ensemble learning", "text": "Ensemble methods offer an alternative to using most samples. In fact, an ensemble of balanced sets is created and used to train any classifier later on. Two methods are available to form such an ensemble proposed by Liu et al. (2009): EasyEnsemble and BalanceCascade. The former is based on the iterative application of the random sub-sampling method to form several sets, each of which has a desired balance ratio. The latter differs in such a way that a classifier is used for each iteration to determine the class of randomly selected samples. Wrong-classified samples are stored and distributed in the next subset."}, {"heading": "5. Future plans and conclusion", "text": "In this paper, we briefly outlined the basics of the vision and API of the imbalanced-learning toolbox. Additional methods based on prototype / instance selection, generation and reduction, as well as additional user manuals, will be added as possibilities for future work."}], "references": [{"title": "Balancing training data for automated annotation of keywords: a case study", "author": ["G.E. Batista", "A.L. Bazzan", "M.C. Monard"], "venue": "In WOB,", "citeRegEx": "Batista et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Batista et al\\.", "year": 2003}, {"title": "SMOTE: synthetic minority over-sampling technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Chawla et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Chawla et al\\.", "year": 2002}, {"title": "Racing for unbalanced methods selection", "author": ["A. Dal Pozzolo", "O. Caelen", "S. Waterschoot", "G. Bontempi"], "venue": "In International Conference on Intelligent Data Engineering and Automated Learning,", "citeRegEx": "Pozzolo et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pozzolo et al\\.", "year": 2013}, {"title": "Borderline-smote: a new over-sampling method in imbalanced data sets learning", "author": ["H. Han", "W.-Y. Wang", "B.-H. Mao"], "venue": "In International Conference on Intelligent Computing,", "citeRegEx": "Han et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Han et al\\.", "year": 2005}, {"title": "The condensed nearest neighbor rule", "author": ["P. Hart"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "Hart.,? \\Q1968\\E", "shortCiteRegEx": "Hart.", "year": 1968}, {"title": "Learning from imbalanced data", "author": ["H. He", "E. Garcia"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "He and Garcia.,? \\Q2009\\E", "shortCiteRegEx": "He and Garcia.", "year": 2009}, {"title": "Addressing the curse of imbalanced training sets: one-sided selection", "author": ["M. Kubat", "S. Matwin"], "venue": "In International Conference in Machine Learning,", "citeRegEx": "Kubat and Matwin,? \\Q1997\\E", "shortCiteRegEx": "Kubat and Matwin", "year": 1997}, {"title": "Caret: classification and regression training", "author": ["M. Kuhn"], "venue": "Astrophysics Source Code Library,", "citeRegEx": "Kuhn.,? \\Q2015\\E", "shortCiteRegEx": "Kuhn.", "year": 2015}, {"title": "Improving identification of difficult small classes by balancing class distribution", "author": ["J. Laurikkala"], "venue": null, "citeRegEx": "Laurikkala.,? \\Q2001\\E", "shortCiteRegEx": "Laurikkala.", "year": 2001}, {"title": "Exploratory undersampling for class-imbalance learning", "author": ["X.-Y. Liu", "J. Wu", "Z.-H. Zhou"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics),", "citeRegEx": "Liu et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "knn approach to unbalanced data distributions: a case study involving information extraction", "author": ["I. Mani", "I. Zhang"], "venue": "In Proceedings of Workshop on Learning from Imbalanced Datasets,", "citeRegEx": "Mani and Zhang.,? \\Q2003\\E", "shortCiteRegEx": "Mani and Zhang.", "year": 2003}, {"title": "Borderline over-sampling for imbalanced data classification", "author": ["H.M. Nguyen", "E.W. Cooper", "K. Kamei"], "venue": "International Journal of Knowledge Engineering and Soft Data Paradigms,", "citeRegEx": "Nguyen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nguyen et al\\.", "year": 2011}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Pedregosa et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa et al\\.", "year": 2011}, {"title": "Data mining with imbalanced class distributions: concepts and methods", "author": ["R.C. Prati", "G.E. Batista", "M.C. Monard"], "venue": "In Indian International Conference Artificial Intelligence,", "citeRegEx": "Prati et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Prati et al\\.", "year": 2009}, {"title": "Tackling the problem of data imbalancing for melanoma classification", "author": ["M. Rastgoo", "G. Lemaitre", "J. Massich", "O. Morel", "F. Marzani", "R. Garcia", "F. Meriaudeau"], "venue": "Bioimaging,", "citeRegEx": "Rastgoo et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastgoo et al\\.", "year": 2016}, {"title": "An instance level analysis of data complexity", "author": ["M.R. Smith", "T. Martinez", "C. Giraud-Carrier"], "venue": "Machine learning,", "citeRegEx": "Smith et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2014}, {"title": "The SHOGUN machine learning toolbox", "author": ["S.C. Sonnenburg", "S. Henschel", "C. Widmer", "J. Behr", "A. Zien", "F. de Bona", "A. Binder", "C. Gehl", "V. Franc"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Sonnenburg et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sonnenburg et al\\.", "year": 2010}, {"title": "Two modifications of CNN. Systems, Man, and Cybernetics", "author": ["I. Tomek"], "venue": "IEEE Transactions on,", "citeRegEx": "Tomek.,? \\Q1976\\E", "shortCiteRegEx": "Tomek.", "year": 1976}, {"title": "Data mining with R: learning with case studies", "author": ["L. Torgo"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "Torgo.,? \\Q2010\\E", "shortCiteRegEx": "Torgo.", "year": 2010}, {"title": "Asymptotic properties of nearest neighbor rules using edited data", "author": ["D.L. Wilson"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on,", "citeRegEx": "Wilson.,? \\Q1972\\E", "shortCiteRegEx": "Wilson.", "year": 1972}, {"title": "10 challenging problems in data mining research", "author": ["Q. Yang", "X. Wu"], "venue": "International Journal of Information Technology & Decision Making,", "citeRegEx": "Yang and Wu.,? \\Q2006\\E", "shortCiteRegEx": "Yang and Wu.", "year": 2006}], "referenceMentions": [{"referenceID": 13, "context": "This imbalance gives rise to the \u201cclass imbalance\u201d problem (Prati et al., 2009) (or \u201ccurse of imbalanced datasets\u201d) which is the problem of learning a concept from the class that has a small number of samples.", "startOffset": 59, "endOffset": 79}, {"referenceID": 20, "context": "The class imbalance problem has been encountered in multiple areas such as telecommunication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition (Yang and Wu, 2006; Rastgoo et al., 2016).", "startOffset": 252, "endOffset": 293}, {"referenceID": 14, "context": "The class imbalance problem has been encountered in multiple areas such as telecommunication managements, bioinformatics, fraud detection, and medical diagnosis, and has been considered one of the top 10 problems in data mining and pattern recognition (Yang and Wu, 2006; Rastgoo et al., 2016).", "startOffset": 252, "endOffset": 293}, {"referenceID": 5, "context": "Imbalanced data substantially compromises the learning process, since most of the standard machine learning algorithms expect balanced class distribution or an equal misclassification cost (He and Garcia, 2009).", "startOffset": 189, "endOffset": 210}, {"referenceID": 18, "context": "Such standalone methods have been implemented mainly in R language (Torgo, 2010; Kuhn, 2015; Dal Pozzolo et al., 2013).", "startOffset": 67, "endOffset": 118}, {"referenceID": 7, "context": "Such standalone methods have been implemented mainly in R language (Torgo, 2010; Kuhn, 2015; Dal Pozzolo et al., 2013).", "startOffset": 67, "endOffset": 118}, {"referenceID": 12, "context": "Up to our knowledge, however, there is no python toolbox allowing such processing while cutting edge machine learning toolboxes are available (Pedregosa et al., 2011; Sonnenburg et al., 2010).", "startOffset": 142, "endOffset": 191}, {"referenceID": 16, "context": "Up to our knowledge, however, there is no python toolbox allowing such processing while cutting edge machine learning toolboxes are available (Pedregosa et al., 2011; Sonnenburg et al., 2010).", "startOffset": 142, "endOffset": 191}, {"referenceID": 10, "context": ", NearMiss (Mani and Zhang, 2003)), and (iv) classification accuracy (i.", "startOffset": 11, "endOffset": 33}, {"referenceID": 15, "context": ", instance hardness threshold (Smith et al., 2014)).", "startOffset": 30, "endOffset": 50}, {"referenceID": 4, "context": "These criteria are derived from the nearest neighbours rule, namely: (i) condensed nearest neighbours (Hart, 1968), (ii) edited nearest neighbours (Wilson, 1972), (iii) one-sided selection (Kubat et al.", "startOffset": 102, "endOffset": 114}, {"referenceID": 19, "context": "These criteria are derived from the nearest neighbours rule, namely: (i) condensed nearest neighbours (Hart, 1968), (ii) edited nearest neighbours (Wilson, 1972), (iii) one-sided selection (Kubat et al.", "startOffset": 147, "endOffset": 161}, {"referenceID": 8, "context": ", 1997), (iv) neighbourhood cleaning rule (Laurikkala, 2001), and (v) Tomek links (Tomek, 1976).", "startOffset": 42, "endOffset": 60}, {"referenceID": 17, "context": ", 1997), (iv) neighbourhood cleaning rule (Laurikkala, 2001), and (v) Tomek links (Tomek, 1976).", "startOffset": 82, "endOffset": 95}, {"referenceID": 3, "context": "Different variants of this algorithm have been proposed: SMOTE borderline 1 & 2 (Han et al., 2005) and SMOTE SVM (Nguyen et al.", "startOffset": 80, "endOffset": 98}, {"referenceID": 11, "context": ", 2005) and SMOTE SVM (Nguyen et al., 2011).", "startOffset": 22, "endOffset": 43}, {"referenceID": 13, "context": "4 Combination of over- and under-sampling SMOTE over-sampling can lead to over-fitting which can be avoided by applying cleaning under-sampling methods (Prati et al., 2009).", "startOffset": 152, "endOffset": 172}, {"referenceID": 0, "context": "In that regard, Batista et al. (2003) combined SMOTE either with Tomek links or edited nearest neighbours.", "startOffset": 16, "endOffset": 38}, {"referenceID": 9, "context": "Two methods are available to build such ensemble proposed by Liu et al. (2009): EasyEnsemble and BalanceCascade.", "startOffset": 61, "endOffset": 79}], "year": 2016, "abstractText": "imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of overand under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub https://github.com/scikit-learn-contrib/imbalanced-learn.", "creator": "LaTeX with hyperref package"}}}