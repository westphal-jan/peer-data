{"id": "1609.06578", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Twitter Opinion Topic Model: Extracting Product Opinions from Tweets by Leveraging Hashtags and Sentiment Lexicon", "abstract": "Aspect-based opinion mining is widely applied to review data to aggregate or summarize opinions of a product, and the current state-of-the-art is achieved with Latent Dirichlet Allocation (LDA)-based model. Although social media data like tweets are laden with opinions, their \"dirty\" nature (as natural language) has discouraged researchers from applying LDA-based opinion model for product review mining. Tweets are often informal, unstructured and lacking labeled data such as categories and ratings, making it challenging for product opinion mining. In this paper, we propose an LDA-based opinion model named Twitter Opinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM leverages hashtags, mentions, emoticons and strong sentiment words that are present in tweets in its discovery process. It improves opinion prediction by modeling the target-opinion interaction directly, thus discovering target specific opinion words, neglected in existing approaches. Moreover, we propose a new formulation of incorporating sentiment prior information into a topic model, by utilizing an existing public sentiment lexicon. This is novel in that it learns and updates with the data. We conduct experiments on 9 million tweets on electronic products, and demonstrate the improved performance of TOTM in both quantitative evaluations and qualitative analysis. We show that aspect-based opinion analysis on massive volume of tweets provides useful opinions on products.", "histories": [["v1", "Wed, 21 Sep 2016 14:25:23 GMT  (551kb,D)", "http://arxiv.org/abs/1609.06578v1", "CIKM paper"]], "COMMENTS": "CIKM paper", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["kar wai lim", "wray buntine"], "accepted": false, "id": "1609.06578"}, "pdf": {"name": "1609.06578.pdf", "metadata": {"source": "CRF", "title": "Twitter Opinion Topic Model: Extracting Product Opinions from Tweets by Leveraging Hashtags and Sentiment Lexicon", "authors": ["Kar Wai Lim", "Wray Buntine"], "emails": ["karwai.lim@anu.edu.au", "wray.buntine@monash.edu", "permissions@acm.org."], "sections": [{"heading": null, "text": "Categories and Theme Descriptions I.2.7 [Artificial Intelligence]: NLP - TextanalyseGeneral Conceptual Design, ExperimentationTags Opinion Research, Mood Analysis, Twitter, Theme Modeling, Product Evaluation, Mood Encyclopedia, Emoticons"}, {"heading": "1. INTRODUCTION", "text": "When we make a purchase decision, a determining factor can often be the reviews of other consumers. However, these reviews are a permission to make digital or hard copies of all or part of this work for personal or class-specific use without charging a fee, provided that copies are not made or distributed for profit or commercial advantage, and that copies include this note and the complete citation on the first page. Copyrights to components of this work owned by others than the author (s) must be honored. Credit Abstracting is permitted to otherwise copy or post on servers, requires prior specific permission and / or a fee. Request Permissions @ acm.org. CIXS. \""}, {"heading": "2. RELATED WORK", "text": "The latent dirichlet allocation (LDA) is a topic model successfully applied by many to the sentiment analysis of Li Li data on social mining methods. Notable examples based on LDA are the MaxEnt-LDA hybrid model [48], Joint Sentiment Topic (JST) model [18], Multi-Aspect Sentiment (MAS) model [43], Interdependent LDA (ILDA) model [27], Aspect and Sentiment Unification Model (ASUM) [15] and Multi-Aspect Sentiment (MAS) model [42]. The Topic-Sentiment Mixture (TSM) model [25] performs sentiment analysis using the Multinomial distribution. These models perform aspect-based opinion analysis and they have been successfully applied to review data from different domains, such as electronic product, hotel and restaurant evaluations."}, {"heading": "3. OPINION MINING TASK ON TWEETS", "text": "In this section, we describe the problem of opinion mining that we are tackling and outline our key contributions to solving the problem."}, {"heading": "3.1 Problem Definition", "text": "A target-opinion pair < t, o > consists of two phrases: a target phrase t, which is the object described, and an opinion phrase o, which is the description. Target phrases are usually nouns and opinion phrases, examples are usually adjectives, including < image quality, good >, < iPhone app, expensive >, etc. Note that a phrase can be either a collocation (multi-word phrase) or a single word. For convenience, we use \"word\" to mean a single word or phrase in this paper.For example, if we give a tweet corpus consisting of different electronic products, we would like to group different products > a second opinion, where a second opinion pair < 2 is a second one."}, {"heading": "3.2 Major Contributions", "text": "We are making two important contributions as follows: First, we are designing an LDA-based topic model (TOTM) for performing aspect-based goal opinion analysis based on tweets. < TOTM is novel in that it directly models the interaction between goal opinion and sensation, resulting in a significant improvement in opinion formation. However, existing aspect-based methods only model the interaction between aspects and sensations, with the goals and opinions weakly associated by aspects and sensations. Without this explicit modeling, existing models have failed to reasonably match the goals. For example, from a restaurant review with friendly staff and delicious cakes, the existing LDA-based opinion model is not recognized that friendly cannot be used to describe cakes. Likewise, as mentioned in the introduction, TOTM uses available helper variables in tweets (hashtags, mentions, emoticons and strong sentiment) to suggest a new word-based formulation to enhance the opinions."}, {"heading": "5. TWITTER OPINION TOPIC MODEL", "text": "The model is shown in Figure 2. Unlike ILDA, we are not able to model the distribution of feelings, but to describe the distribution of feelings, but it is unlikely to be used to describe feelings. [1] Also known as evaluation in Moghaddam and Ester [27].TOTM uses the views of the Griffiths-Engen-McCloskey (GEM) [34] distribution to generate probability vectors and the Pitman-Yor process to generate probability vectors. [39] Both GEM and PYP are parameterized by discount parameters."}, {"heading": "6. INCORPORATING SENTIMENT PRIOR", "text": "He [11] has proposed a simple but effective method to integrate sentiment into LDA by directly modifying the Dirichlet data before it is published based on the available sentiments. [11] He has proposed a simple but effective method to integrate sentiment into LDA by replacing the themes in LDA with latent sentiment labels and allowing the word Priors to be custom probability distributions. [11] The generative process of LDA-DP is identical to the LDA icon, the word distribution is Dirichlet distributed with the parameters (~ 2) in which r = {\u2212 1, 1} is the sentiment label brand corresponding to negative, neutral and positive sentiment2. The Sentively2 is initialized to 1 / 3, and subsequently updated when the Sentile labels.We assume that the entile label will include the original ones for this case, we include the Sentile.11]"}, {"heading": "7. INFERENCE TECHNIQUE", "text": "In this section we discuss the collapsed Gibbs sampler for TOTM and then discuss sampling the hyperparameters."}, {"heading": "7.1 Collapsed Gibbs Sampling for TOTM", "text": "The key to Gibbs sampling with PYPs is to marginalize out the probability vectors (e.g.) in the model and record different counts instead, so that a collapsed sampler can occur. While a 3We simply has a random aspect to each targetopinion pair, sampling the customer label, and building the relevant customer counts cNk and table counts c \u2032 N for all nodes.2 For each document d: (a). Decrement counts associated with tdn.ii. Sample new aspect adn and corresponding parts of C from Equation 4.iii. Increment associated counts for the new adn. (b) For each target phrase d: i. Decrement associated with tdn.i. Sample new aspect adn and corresponding parts of C from Equation 4.iii."}, {"heading": "7.2 Hyperparameters Sampling", "text": "During the conclusion, sample the hyperparameters of the PYP using an auxiliary variable sampler (38). Furthermore, we propose a new method of updating hyperparameter b, which controls the strength of the previous feeling. Instead of sampling hyperparameter b (e.g. using the sampler [30]), we use an optimization approach, since the back part of b is concentrated in a small region (thin-tailed). The back density is given by the following equation, where crv is the number of times a word v is associated with a sentiment r, and p (b) is the lower part of b."}, {"heading": "8. DATA", "text": "For experiments, we perform aspect-based opinion analysis on tweets characterized by their limited 140-character text. We queried tweets referring to electronic products such as cameras and mobile phones from the Twitter 7 dataset 5 [46] (see list of our search terms in the supplementary material) and then removed non-English tweets using langid.py [21]. Since most spam tweets contain a URL, we also take a conservative approach to removing spam by discarding tweets with URLs, resulting in a data set of approximately 9 million tweets that we call an electronic product dataset. Due to the absence of mood labels on the electronic product dataset, we use the Sentiment140 (sentence 140) Tweets 6 [8] to evaluate the mood classification. Each Sent140 tweet contains a sentimental label (positive or negative) determined by emoticons. The entire corpus contains 1.6 million tweets, half of which are positive and half negative."}, {"heading": "8.1 Data Preprocessing", "text": "Here we describe the pre-processing steps we apply to tweets. First, we use Twitter NLP [31], a state-of-the-art tool for processing tweets (POS) tags that are applied to tweets. We often use words from tweets to clean up the tweets. We use the lexical words we are interested in. [9], but modify them so that correct terms such as \"iphone\" and \"xbox\" are not normalized because they are the targets we are interested in. We perform post-POS normalization because tweets degrade the performance of Twitter NLP [10]. Next, we proceed to extract target opinion pairs from the data. Following Moghaddam and Ester [28], we use the Stanford Dependency Parser [5] to extract dependencies that are used to form the opinions."}, {"heading": "8.2 Corpus Statistics", "text": "Of the electronic tweets containing at least one target opinion pair, 17.9% contain an emotion indicator. After pre-processing, the number of unique target word tokens in electronic product tweets is 4402, while the number of unique opinion word tokens is 25,188. We present a summary of the Corpus statistics for all records in Table 3.9http: / / en.wikipedia.org / wiki / Kaomoji and http: / / en.wikipedia.org / wiki / List _ of _ emoticonsFor electronic product tweets, the top tags are # apple, # phone, # iphone, # computer and # laptop. Please note that some tags are associated with products, brands or companies, such as # playstation and # xbox, while # sony and # canon are associated with companies. In subsection 9.3 below, we show that the aggregation of these tags between companies allows for the consideration of certain products as comparison options."}, {"heading": "9. EXPERIMENTS AND RESULTS", "text": "In this section, we show the usefulness of TOTM for opinion polls. We evaluate TOTM quantitatively compared to ILDA and LDA-DP in terms of perplexity and mood classification. To compare the effectiveness of different mood icons, we propose a novel mood metric to evaluate the mood word distributions of mood polls. Qualitatively, we use TOTM to perform the poll from the tweets of electronic products and show that we are able to extract various useful opinions on technological products such as the iPhone."}, {"heading": "9.1 Experiment Settings", "text": "For all experiments, we initialize the hyperparameters of the PYP to \u03b1 = \u03b2 = 0.1 and the sentiment hyperparameter to b = 10, pointing out that the hyperparameters are automatically optimized, as described in Section 7.2. To determine the optimal number of latent aspects (A) for ILDA, we set aside 5% of the training data as a development set and select A (tested in steps of 10) to minimize the perplexity of the development set. To allow a fair comparison between TOTM and ILDA, we limit the maximum number of aspects of the TOTM to that of ILDA. Our experiment finds that the number of aspects in TOTM always corresponds to the upper limit. We find that LDA-DP has only three fixed \"themes,\" i.e. the number of feelings. During the inferences, we perform the collapsed Gibbs algorithm until the convergence criteria are met, based on which the probability of the training protocol differing by no more than 0.1% in ten consecutive Items."}, {"heading": "9.2 Quantitative Evaluations", "text": "Perplexity refers negatively to the probability of the test data. Since aspect-based opinion analysis refers to two types of vocabulary, we calculate the perplexity for both the target words and the opinion words, in this case: Perplexity (W) = explicitness of the target words and opinion words O, Nd is the number of target words in document D. We also calculate the general perplexity given in view of the byperplexity. (\u2212 exp) D = 1 logP (~ od) 2) D = 1Nd). We present the perplexity of the results (the subordinate tweets in Table 4. We present the plexity result of the 140 tweets and tweets."}, {"heading": "9.3 Qualitative Analysis and Applications", "text": "We calculate the paired Hellinger distance between the individual documentary aspect distributions and find that the aspects are distinctive. Hellinger distance is often used to measure the dissimilarity between two probability distributions. Hellinger distance between all aspect distributions of TOTM is presented as a thermal image in Figure 4, we can see that the distances between the topics are large, indicating that there is no double aspect. We note that the thermal imaging camera is similar to ILDA and is therefore not presented. We also show an extract from the top target words of TOTM in Figure 7. Our empirical review of the aspect target distributions suggests that both TOTM and clustering of target word distributions work well."}, {"heading": "9.3.3 Extracting Contrastive Opinions on Products", "text": "Although the above comparison is useful to provide a high-level summary, it is also important to review the original tweets as they provide more detailed opinions. We use TOTM to extract tweets that contain the opinions of people on the iPhone. In Table 10 we show an excerpt of contrasting tweets that contain the target \"iPhone\" with positive or negative sentiment (r = {\u2212 1, 1})."}, {"heading": "10. CONCLUSION", "text": "In this paper, we examine the use of LDA-based models to analyze the opinion of tweets polled with electronic product terms, motivated by the fact that Twitter is a popular platform for opinions and tweets that are publicly available. Unlike ratings, tweets do not contain ratings or ratings, they are more informal and are usually accompanied by emoticons and strong sentiment words. Taking advantage of the informal nature of tweets, we have developed a theme model called Twitter Opinion Topic Model (TOTM) for opinion analysis. Our innovative formulation significantly improves opinion prediction by directly modelling the target opinion. By incorporating a sentiment lexicon into theme models, we propose a new formulation for the topicmodel pioneers, who learn and update the data given. Our qualitative analysis shows that Opinion Mining in tweets provide useful opinions about electronic products."}, {"heading": "11. ACKNOWLEDGMENTS", "text": "NICTA is funded by the Australian government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. We would also like to thank Scott Sanner, Shamin Kinathil, Rishi Dua and the anonymous critics for their feedback and comments."}, {"heading": "12. REFERENCES", "text": "[1] S. Baccianella, A. Manning. Generating ed dependency parses from structure phrases. SentiWordNet3.0: An enhanced lexical resource for sentiments analysis and opinion mining. In LREC, pp. 2200-2204, 2010. [2] W. Buntine and M. Hutter. A Bayesian review of the Poisson-Dirichlet process. arXiv: 1007.0296v2, 2012. [3] C. Chen, L. Du, and W. Buntine. Sampling table pages configurations for the hierarchical Poisson-Dirichlet Process. In ECML, pp. 296-311, 2011. [4] D. Davidov, O. Tsur, and A. Rappoport. Enhanced sentiment learning using Twitter hashtags and Smileys. In COLING, pp. 241-249, 2010. [5] M. De Marneffe, B. MacCartney, and C. Manning."}], "references": [{"title": "SentiWordNet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining", "author": ["S. Baccianella", "A. Esuli", "F. Sebastiani"], "venue": "In LREC,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "A Bayesian review of the Poisson-Dirichlet process", "author": ["W. Buntine", "M. Hutter"], "venue": "arXiv:1007.0296v2,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Sampling table configurations for the hierarchical Poisson-Dirichlet Process", "author": ["C. Chen", "L. Du", "W. Buntine"], "venue": "ECML, pages 296\u2013311,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Enhanced sentiment learning using Twitter hashtags and smileys", "author": ["D. Davidov", "O. Tsur", "A. Rappoport"], "venue": "COLING, pages 241\u2013249,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Generating typed dependency parses from phrase structure parses", "author": ["M. De Marneffe", "B. MacCartney", "C. Manning"], "venue": "LREC, pages 449\u2013454,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "A holistic lexicon-based approach to opinion mining", "author": ["X. Ding", "B. Liu", "P. Yu"], "venue": "WSDM. ACM,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "WordNet", "author": ["C. Fellbaum"], "venue": "Wiley Online Library,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Twitter sentiment classification using distant supervision", "author": ["A. Go", "R. Bhayani", "L. Huang"], "venue": "CS224N Project Report, Stanford, pages 1\u201312,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Automatically constructing a normalisation dictionary for microblogs", "author": ["B. Han", "P. Cook", "T. Baldwin"], "venue": "EMNLP-CoNLL, pages 421\u2013432. ACL,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2012}, {"title": "Lexical normalization for social media text", "author": ["B. Han", "P. Cook", "T. Baldwin"], "venue": "ACM TIST, 4(1):5:1\u20135:27, Feb.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Incorporating sentiment prior knowledge for weakly supervised sentiment analysis", "author": ["Y. He"], "venue": "ACM TALIP, 11(2):4,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Mining opinion features in customer reviews", "author": ["M. Hu", "B. Liu"], "venue": "AAAI, volume 4, pages 755\u2013760,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2004}, {"title": "Incorporating lexical priors into topic models", "author": ["J. Jagarlamudi", "III H. Daum\u00e9", "R. Udupa"], "venue": "In EACL. ACM,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Target-dependent Twitter sentiment classification", "author": ["L. Jiang", "M. Yu", "M. Zhou", "X. Liu", "T. Zhao"], "venue": "ACL, pages 151\u2013160,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2011}, {"title": "Aspect and sentiment unification model for online review analysis", "author": ["Y. Jo", "A. Oh"], "venue": "WSDM, pages 815\u2013824,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2011}, {"title": "Structure-aware review mining and summarization", "author": ["F. Li", "C. Han", "M. Huang", "X. Zhu", "Y.-J. Xia", "S. Zhang", "H. Yu"], "venue": "COLING, pages 653\u2013661. ACL,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "A non-negative matrix tri-factorization approach to sentiment classification with lexical prior knowledge", "author": ["T. Li", "Y. Zhang", "V. Sindhwani"], "venue": "AFNLP, pages 244\u2013252,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint sentiment/topic model for sentiment analysis", "author": ["C. Lin", "Y. He"], "venue": "CIKM, pages 375\u2013384. ACM,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2009}, {"title": "Sentiment analysis and opinion mining", "author": ["B. Liu"], "venue": "Synthesis Lectures on HLT, 5(1):1\u2013167,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Adaptive co-training SVM for sentiment classification on tweets", "author": ["S. Liu", "F. Li", "F. Li", "X. Cheng", "H. Shen"], "venue": "CIKM, pages 2079\u20132088. ACM,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2013}, {"title": "langid.py: An off-the-shelf language identification tool", "author": ["M. Lui", "T. Baldwin"], "venue": "In ACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Challenges in developing opinion mining tools for social media", "author": ["D. Maynard", "K. Bontcheva", "D. Rout"], "venue": "@NLP can u tag #usergeneratedcontent,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Spam detection on Twitter using traditional classifiers", "author": ["M. McCord", "M. Chuah"], "venue": "Autonomic and Trusted Computing, pages 175\u2013186. Springer,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving LDA topic models for microblogs via Tweet pooling and automatic labeling", "author": ["R. Mehrotra", "S. Sanner", "W. Buntine", "L. Xie"], "venue": "SIGIR, pages 889\u2013892. ACM,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Topic Sentiment Mixture: Modeling facets and opinions in weblogs", "author": ["Q. Mei", "X. Ling", "M. Wondra"], "venue": "In WWW,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2007}, {"title": "Opinion Digger: An unsupervised opinion miner from unstructured product reviews", "author": ["S. Moghaddam", "M. Ester"], "venue": "CIKM, pages 1825\u20131828. ACM,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "ILDA: Interdependent LDA model for learning latent aspects and their ratings from online product reviews", "author": ["S. Moghaddam", "M. Ester"], "venue": "SIGIR, pages 665\u2013674,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2011}, {"title": "On the design of LDA models for aspect-based opinion mining", "author": ["S. Moghaddam", "M. Ester"], "venue": "CIKM. ACM,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "SemEval-2013 task 2: Sentiment analysis in Twitter", "author": ["P. Nakov", "Z. Kozareva", "A. Ritter", "S. Rosenthal", "V. Stoyanov", "T. Wilson"], "venue": "Workshop on Semantic Evaluation,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Slice sampling", "author": ["R. Neal"], "venue": "Ann. Statist., 31(3):705\u2013767,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2003}, {"title": "Improved part-of-speech tagging for online conversational text with word clusters", "author": ["O. Owoputi", "B. O\u2019Connor", "C. Dyer"], "venue": "In NAACL-HLT,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Twitter as a corpus for sentiment analysis and opinion mining", "author": ["A. Pak", "P. Paroubek"], "venue": "LREC,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Opinion Mining and Sentiment Analysis", "author": ["B. Pang", "L. Lee"], "venue": "Foundations and Trends in Information Retrieval, 2(1-2):1\u2013135,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2008}, {"title": "Some developments of the Blackwell-Macqueen urn scheme", "author": ["J. Pitman"], "venue": "Lecture Notes-Monograph Series,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1996}, {"title": "Extracting product features and opinions from reviews", "author": ["A.-M. Popescu", "O. Etzioni"], "venue": "Natural language processing and text mining, pages 9\u201328. Springer,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2007}, {"title": "Named entity recognition in Tweets: An experimental study", "author": ["A. Ritter", "S. Clark", "Mausam", "O. Etzioni"], "venue": "In EMNLP,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Lexicon-based methods for sentiment analysis", "author": ["M. Taboada", "J. Brooke", "M. Tofiloski", "K. Voll", "M. Stede"], "venue": "Computational linguistics, 37(2):267\u2013307,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "A Bayesian interpretation of interpolated Kneser-Ney", "author": ["Y.W. Teh"], "venue": "Tech Report A2/06, NUS,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2006}, {"title": "A hierarchical Bayesian language model based on Pitman-Yor processes", "author": ["Y.W. Teh"], "venue": "ACL, pages 985\u2013992. ACL,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2006}, {"title": "Hierarchical Bayesian nonparametric models with applications", "author": ["Y.W. Teh", "M. Jordan"], "venue": "Bayesian Nonparametrics: Principles and Practice, pages 158\u2013207,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2010}, {"title": "Sentiment strength detection in short informal text", "author": ["M. Thelwall", "K. Buckley", "G. Paltoglou", "D. Cai", "A. Kappas"], "venue": "JASIST, 61(12):2544\u20132558,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}, {"title": "A joint model of text and aspect ratings for sentiment summarization", "author": ["I. Titov", "R. McDonald"], "venue": "ACL08: HLT,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2008}, {"title": "Modeling online reviews with multi-grain topic models", "author": ["I. Titov", "R. McDonald"], "venue": "WWW, pages 111\u2013120,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2008}, {"title": "ICWSM-A great catchy name: Semi-supervised recognition of sarcastic sentences in online product reviews", "author": ["O. Tsur", "D. Davidov", "A. Rappoport"], "venue": "ICWSM,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2010}, {"title": "Recognizing contextual polarity in phrase-level sentiment analysis", "author": ["T. Wilson", "J. Wiebe", "P. Hoffmann"], "venue": "HLT-EMNLP, pages 347\u2013354,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2005}, {"title": "Patterns of temporal variation in online media", "author": ["J. Yang", "J. Leskovec"], "venue": "WSDM, pages 177\u2013186,", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparing Twitter and traditional media using topic models", "author": ["W. Zhao", "J. Jiang", "J. Weng", "J. He", "E.-P. Lim", "H. Yan", "X. Li"], "venue": "ECIR, pages 338\u2013349,", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Jointly modeling aspects and opinions with a MaxEnt-LDA hybrid", "author": ["W. Zhao", "J. Jiang", "H. Yan", "X. Li"], "venue": "EMNLP, pages 56\u201365,", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 18, "context": "The task of analyzing opinions from text data such as reviews is known as opinion mining or opinion extraction [19, 33].", "startOffset": 111, "endOffset": 119}, {"referenceID": 32, "context": "The task of analyzing opinions from text data such as reviews is known as opinion mining or opinion extraction [19, 33].", "startOffset": 111, "endOffset": 119}, {"referenceID": 27, "context": "LDA-based models are considered to be state-of-the-art for aspect-based opinion mining [28].", "startOffset": 87, "endOffset": 91}, {"referenceID": 46, "context": "Social media text is short and is regarded as \u201cdirty\u201d, and hence less useful for more sophisticated language analysis [47].", "startOffset": 118, "endOffset": 122}, {"referenceID": 35, "context": "The same problem also leads to degradation when applying NLP tools [36].", "startOffset": 67, "endOffset": 71}, {"referenceID": 23, "context": "Additionally, hashtags are strong indicators of topics for tweets [24].", "startOffset": 66, "endOffset": 70}, {"referenceID": 26, "context": "In Section 4, we present Interdependent LDA (ILDA) [27] which will be used as a baseline for comparison.", "startOffset": 51, "endOffset": 55}, {"referenceID": 47, "context": "Notable examples based on LDA include the MaxEnt-LDA hybrid model [48], Joint Sentiment Topic (JST) model [18], Multi-grain LDA (MG-LDA) [43], Interdependent LDA (ILDA) [27], Aspect and Sentiment Unification Model (ASUM) [15] and Multi-Aspect Sentiment (MAS) model [42].", "startOffset": 66, "endOffset": 70}, {"referenceID": 17, "context": "Notable examples based on LDA include the MaxEnt-LDA hybrid model [48], Joint Sentiment Topic (JST) model [18], Multi-grain LDA (MG-LDA) [43], Interdependent LDA (ILDA) [27], Aspect and Sentiment Unification Model (ASUM) [15] and Multi-Aspect Sentiment (MAS) model [42].", "startOffset": 106, "endOffset": 110}, {"referenceID": 42, "context": "Notable examples based on LDA include the MaxEnt-LDA hybrid model [48], Joint Sentiment Topic (JST) model [18], Multi-grain LDA (MG-LDA) [43], Interdependent LDA (ILDA) [27], Aspect and Sentiment Unification Model (ASUM) [15] and Multi-Aspect Sentiment (MAS) model [42].", "startOffset": 137, "endOffset": 141}, {"referenceID": 26, "context": "Notable examples based on LDA include the MaxEnt-LDA hybrid model [48], Joint Sentiment Topic (JST) model [18], Multi-grain LDA (MG-LDA) [43], Interdependent LDA (ILDA) [27], Aspect and Sentiment Unification Model (ASUM) [15] and Multi-Aspect Sentiment (MAS) model [42].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "Notable examples based on LDA include the MaxEnt-LDA hybrid model [48], Joint Sentiment Topic (JST) model [18], Multi-grain LDA (MG-LDA) [43], Interdependent LDA (ILDA) [27], Aspect and Sentiment Unification Model (ASUM) [15] and Multi-Aspect Sentiment (MAS) model [42].", "startOffset": 221, "endOffset": 225}, {"referenceID": 41, "context": "Notable examples based on LDA include the MaxEnt-LDA hybrid model [48], Joint Sentiment Topic (JST) model [18], Multi-grain LDA (MG-LDA) [43], Interdependent LDA (ILDA) [27], Aspect and Sentiment Unification Model (ASUM) [15] and Multi-Aspect Sentiment (MAS) model [42].", "startOffset": 265, "endOffset": 269}, {"referenceID": 24, "context": "The Topic-Sentiment Mixture (TSM) model [25] performs sentiment analysis by utilizing the Multinomial distribution.", "startOffset": 40, "endOffset": 44}, {"referenceID": 21, "context": "[22] studied the challenges in developing an opinion mining tool for social media and they advocated the use of shallow techniques in linguistic processing of tweets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 34, "context": "Notable non-LDA-based methods for opinion analysis include OPINE [35], which uses relaxation labeling to classify sentiment, and Opinion Digger [26], an aspect-based review miner using k nearest neighbor.", "startOffset": 65, "endOffset": 69}, {"referenceID": 25, "context": "Notable non-LDA-based methods for opinion analysis include OPINE [35], which uses relaxation labeling to classify sentiment, and Opinion Digger [26], an aspect-based review miner using k nearest neighbor.", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "Hu and Liu [12] performed rule-based target-opinion extraction from online product reviews, while Li et al.", "startOffset": 11, "endOffset": 15}, {"referenceID": 15, "context": "[16] extracted opinions from reviews using Conditional Random Fields.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "On tweets, Pak and Paroubek [32] performed opinion analysis using a Naive Bayes classifier; while Liu et al.", "startOffset": 28, "endOffset": 32}, {"referenceID": 19, "context": "[20] performed sentiment classification using an adaptive co-training SVM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[8] and Davidov et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[4] made use of emoticons (smileys), which were found to provide improvement for sentiment classification on tweets.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Since tweets are always short, existing work [8, 32, 4, 20] tends to assume a single polarity for each tweet.", "startOffset": 45, "endOffset": 59}, {"referenceID": 31, "context": "Since tweets are always short, existing work [8, 32, 4, 20] tends to assume a single polarity for each tweet.", "startOffset": 45, "endOffset": 59}, {"referenceID": 3, "context": "Since tweets are always short, existing work [8, 32, 4, 20] tends to assume a single polarity for each tweet.", "startOffset": 45, "endOffset": 59}, {"referenceID": 19, "context": "Since tweets are always short, existing work [8, 32, 4, 20] tends to assume a single polarity for each tweet.", "startOffset": 45, "endOffset": 59}, {"referenceID": 13, "context": "[14] performed target-dependent sentiment analysis, where the sentiments apply to a specific target.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "He [11] used a sentiment lexicon to modify the priors of LDA for sentiment classification, though with an approach with ad hoc constants.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "[17] incorporated a lexical dictionary into a nonnegative matrix tri-factorization model, using a simple rule-based polarity assignment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[6] and Taboada et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 36, "context": "[37] for a detailed review on applying lexicon-based methods in sentiment analysis.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] used seeded words as lexical priors for semi-supervised topic modeling.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "Interdependent LDA (ILDA) [27] is an extension of LDA that performs aspect-based opinion analysis.", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Also known as rating in Moghaddam and Ester [27].", "startOffset": 44, "endOffset": 48}, {"referenceID": 33, "context": "TOTM uses the Griffiths-Engen-McCloskey (GEM) [34] distribution to generate probability vectors and the Pitman-Yor process (PYP) [39] to generate probability vector given another mean probability vector.", "startOffset": 46, "endOffset": 50}, {"referenceID": 38, "context": "TOTM uses the Griffiths-Engen-McCloskey (GEM) [34] distribution to generate probability vectors and the Pitman-Yor process (PYP) [39] to generate probability vector given another mean probability vector.", "startOffset": 129, "endOffset": 133}, {"referenceID": 10, "context": "He [11] proposed a simple yet effective way to incorporate sentiment prior information into LDA by directly modifying the Dirichlet prior based on available sentiment lexicons.", "startOffset": 3, "endOffset": 7}, {"referenceID": 10, "context": "We redefined the original sentiment labels [11] for consistency.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "Sentiment lexicons that are freely available online include SentiWordNet [1], SentiStrength [41], MPQA Subjectivity lexicon [45] and others.", "startOffset": 73, "endOffset": 76}, {"referenceID": 40, "context": "Sentiment lexicons that are freely available online include SentiWordNet [1], SentiStrength [41], MPQA Subjectivity lexicon [45] and others.", "startOffset": 92, "endOffset": 96}, {"referenceID": 44, "context": "Sentiment lexicons that are freely available online include SentiWordNet [1], SentiStrength [41], MPQA Subjectivity lexicon [45] and others.", "startOffset": 124, "endOffset": 128}, {"referenceID": 6, "context": "SentiWordNet is built on WordNet [7] by researchers from Italy.", "startOffset": 33, "endOffset": 36}, {"referenceID": 39, "context": "common approach here is to use the Chinese Restaurant Process (CRP) representation of Teh and Jordan [40], we use another representation that requires no dynamic memory and has better inference efficiency [3].", "startOffset": 101, "endOffset": 105}, {"referenceID": 2, "context": "common approach here is to use the Chinese Restaurant Process (CRP) representation of Teh and Jordan [40], we use another representation that requires no dynamic memory and has better inference efficiency [3].", "startOffset": 205, "endOffset": 208}, {"referenceID": 1, "context": "where S y,\u03b1 is the generalized Stirling number, whereas (x)C and (x|y)C denote the Pochhammer symbol [2].", "startOffset": 101, "endOffset": 104}, {"referenceID": 2, "context": "[3] for inference.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "While for the ratio of Stirling numbers, such as S x+1,\u03b1/S y x,\u03b1, can be computed quickly via caching [2].", "startOffset": 102, "endOffset": 105}, {"referenceID": 37, "context": "During inference, we sample the hyperparameters of the PYP using an auxiliary variable sampler [38].", "startOffset": 95, "endOffset": 99}, {"referenceID": 29, "context": "using the slice sampler [30]), we adopt an optimization approach since the posterior of b is highly concentrated in a small region (thin-tailed).", "startOffset": 24, "endOffset": 28}, {"referenceID": 45, "context": "From the Twitter 7 dataset [46], we queried for tweets that are related to electronic products such as camera and mobile phones (see the list of our query words in the supplementary material).", "startOffset": 27, "endOffset": 31}, {"referenceID": 20, "context": "py [21].", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "Due to the lack of sentiment labels on the electronic product dataset, we make use of the Sentiment140 (Sent140) tweets [8] for sentiment classification evaluation.", "startOffset": 120, "endOffset": 123}, {"referenceID": 28, "context": "In addition, we also use the SemEval 2013 dataset [29] for evaluation.", "startOffset": 50, "endOffset": 54}, {"referenceID": 30, "context": "Firstly, we apply Twitter NLP [31], a state-of-the-art tool for partof-speech (POS) tagging on tweets.", "startOffset": 30, "endOffset": 34}, {"referenceID": 8, "context": "[9], but modify it such that proper nouns are not normalized.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "We perform normalization after POS tagging since tweets normalization degrades the performance of Twitter NLP [10].", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "Following Moghaddam and Ester [28], we apply the Stanford Dependency Parser [5] to extract dependency relations that will be used to form the target-opinion pairs.", "startOffset": 30, "endOffset": 34}, {"referenceID": 4, "context": "Following Moghaddam and Ester [28], we apply the Stanford Dependency Parser [5] to extract dependency relations that will be used to form the target-opinion pairs.", "startOffset": 76, "endOffset": 79}, {"referenceID": 35, "context": "Additionally, since standard NLP tools perform less optimally on tweets [36], we use the POS tagging from Twitter NLP to clean up the target-opinion pairs.", "startOffset": 72, "endOffset": 76}, {"referenceID": 23, "context": "We then perform tweet aggregation, which is found to give significant improvement for LDA [24].", "startOffset": 90, "endOffset": 94}, {"referenceID": 43, "context": "We acknowledge that although there is existing work on removing sarcastic tweets and spam [44, 23], we did not incorporate them due to the lack of publicly available software.", "startOffset": 90, "endOffset": 98}, {"referenceID": 22, "context": "We acknowledge that although there is existing work on removing sarcastic tweets and spam [44, 23], we did not incorporate them due to the lack of publicly available software.", "startOffset": 90, "endOffset": 98}], "year": 2016, "abstractText": "Aspect-based opinion mining is widely applied to review data to aggregate or summarize opinions of a product, and the current stateof-the-art is achieved with Latent Dirichlet Allocation (LDA)-based model. Although social media data like tweets are laden with opinions, their \u201cdirty\u201d nature (as natural language) has discouraged researchers from applying LDA-based opinion model for product review mining. Tweets are often informal, unstructured and lacking labeled data such as categories and ratings, making it challenging for product opinion mining. In this paper, we propose an LDA-based opinion model named Twitter Opinion Topic Model (TOTM) for opinion mining and sentiment analysis. TOTM leverages hashtags, mentions, emoticons and strong sentiment words that are present in tweets in its discovery process. It improves opinion prediction by modeling the target-opinion interaction directly, thus discovering target specific opinion words, neglected in existing approaches. Moreover, we propose a new formulation of incorporating sentiment prior information into a topic model, by utilizing an existing public sentiment lexicon. This is novel in that it learns and updates with the data. We conduct experiments on 9 million tweets on electronic products, and demonstrate the improved performance of TOTM in both quantitative evaluations and qualitative analysis. We show that aspect-based opinion analysis on massive volume of tweets provides useful opinions on products.", "creator": "LaTeX with hyperref package"}}}