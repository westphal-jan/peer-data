{"id": "1609.06616", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text", "abstract": "We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more fine-grained word sources: individual Presidents and (2-year) Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vector-arithmetic-based investigations of complex relationships between word sources. We are extending this to create a comprehensive legal semantic map.", "histories": [["v1", "Wed, 21 Sep 2016 16:09:12 GMT  (137kb,D)", "http://arxiv.org/abs/1609.06616v1", "Forthcoming paper in the 2016 Proceedings of Empirical Methods in Natural Language Processing Workshop on NLP and Computational Social Science"], ["v2", "Sun, 25 Sep 2016 22:20:12 GMT  (136kb,D)", "http://arxiv.org/abs/1609.06616v2", "Forthcoming paper in the 2016 Proceedings of the Conference on Empirical Methods in Natural Language Processing Workshop on Natural Language Processing and Computational Social Science"]], "COMMENTS": "Forthcoming paper in the 2016 Proceedings of Empirical Methods in Natural Language Processing Workshop on NLP and Computational Social Science", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.NE cs.SI", "authors": ["john j nay"], "accepted": false, "id": "1609.06616"}, "pdf": {"name": "1609.06616.pdf", "metadata": {"source": "CRF", "title": "Gov2Vec: Learning Distributed Representations of Institutions and Their Legal Text\u2217", "authors": ["John J. Nay"], "emails": ["john.j.nay@gmail.com"], "sections": [{"heading": "1 Introduction", "text": "Methods have been developed to efficiently obtain word representations in Rd that capture subtle semantics in all dimensions of vectors (Collobert and Weston, 2008). For example, relationships encoded in difference vectors can be uncovered after a successful training in vector arithmetic: vec (\"king\") - vec (\"man\") + vec (\"woman\") delivers a vector near vec (\"queen\") (Mikolov et al. 2013a). By applying this powerful concept of representation of words in distributed vector space, we embed institutions and the words from their laws and political documents into the common semantic space. We can then incorporate positively and negatively weighted word and government vectors into the same implicit representation of words."}, {"heading": "2 Methods", "text": "In fact, it is as if most of us are able to follow the rules that they have imposed on ourselves. (...) In fact, it is as if they are able to determine themselves. (...) It is as if they were able to outdo themselves. (...) It is as if they were able to outdo themselves. (...) It is as if they were able to outdo themselves. (...) It is as if they were able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves. (...) It is as if they are able to outdo themselves."}, {"heading": "3 Data", "text": "We created a unique corpus of 59 years of Supreme Court opinions (1937-1975, 1991-2010), 227 years of bills submitted to Congress (1973-2014), and we used official summaries instead of the full text, since full texts have only been available since 1993, and summaries since 1973. We scraped down all presidential memorandums (1465), provisions (801), executive orders (5634), and proclamations (7544) from the website of the American Presi-Presidency Project. The Sunlight Foundation downloaded official legislative summaries from the GPO that we had downloaded, downloaded Supreme Court rulings from 1937-1975 (volumes 300-422) from the GPO, and the PDFs of the 1991-2010 decisions (volumes 502-561) that excluded only words and deeds from jurisprudence."}, {"heading": "4 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 WordVec-GovVec Similarities", "text": "Figure 2 shows similarities between these queries and the branches that a priori reflect known differences. Gov2Vec has unique capabilities that are lacking in summary statistics such as word frequency: it can calculate similarities between each source and each word as long as the word occurs in at least one source, whereas word counting cannot provide meaningful similarities if a word never occurs in the corpus of a source. Most importantly, Gov2Vec can combine complex combinations of positively and negatively weighted vectors in a similarity query."}, {"heading": "4.2 GovVec-GovVec Similarities", "text": "To investigate whether the representations capture important latent relationships between institutions, we compared the cosmic similarities between the congresses over time (93-113.) and the corresponding sitting presidents (Nixon-Obama) with the veto rate of the bill. We expected that a lower veto rate would be reflected in more similar vectors, and indeed, similarity and veto rate between Congress and President correlate negatively (Spearman's calculated result based on raw veto rates and similarities: -0.74; see also Fig. 3).3As a third validation, we learn vectors only from text and project them into two dimensions by analyzing the main components. Fig. 4 shows that temporal and institutional relationships have been implicitly learned.4 One dimension orders presidents and congresses almost perfectly according to time, and another separates the president from Congress."}, {"heading": "4.3 GovVec-WordVec Policy Queries", "text": "Fig. 5 (above) asks: How do Obama and the 113th House of Representatives differ in combating climate change and how does this differ in environmental and economic contexts? The most commonly used word in the entire learning process, Structured Gov2Vec, and only the use of the text provide very similar (impressive) results on this task. Fig. 3 and 4 and the correlation reported come from the purely text-based Gov2Vec quadrant. 4These are the only results reported in this paper from a single model within the ensemble. We conducted PCAs to other models of the ensemble and the same relationships. Ensemble (from words with > 0.35 similarity to query) for the Obama economic quadrant is \"unprecedented.\" Greenhouse \"and\" ghg \"are more common across models and exhibit a higher mean similarity for Obama-Environmental than for the 113th."}, {"heading": "5 Additional Related Work", "text": "Political scientists model texts in order to understand political processes (Grimmer 2010; Roberts et al. 2014); however, most of this work focuses on variants of subject models. Djuric et al. (2015) applies a learning process similar to Structured Gov2Vec to the streaming of documents to learn representations of documents that are similar to those near time. Structured Gov2Vec applies this common hierarchical learning process (using units for predicting words and other units) to non-textual units. Kim et al. (2014) and Kulkarni et al. (2015) train neural language models for each year of a temporally ordered corpora to detect changes in words. Instead of learning models for different times, we learn a global model with embedding of time-dependent units that can be included in queries for analyzing changes. Kiros et al. (2014) learn embedding of text properties by treating them as gating units into our word is an intensive tendencies process."}, {"heading": "6 Conclusions and Future Work", "text": "We learned vector representations of text metadata on a novel set of legal texts containing the case, 5For cross-industry comparisons, e.g. 113th House and Obama, Structured Gov2Vec learned more qualitatively useful representations, so we present this here. For cross-industry comparisons, e.g. 106th House and 107th House, to maximize the uniqueness of word sources, we used exclusively text-based Gov2Vec.statute and administrative laws. The representations effectively encoded important relationships between institutional actors that were not explicitly provided during the training. Finally, we demonstrated fine-grained investigations of policy differences between actors based on vector arithmetics. More generally, the method can be applied to measuring similarity between all entities that produce text, and used for recommendations, e.g. what the narrowest think tank for the nonprofit vector representation of the Sierra Club can find useful when our next goal is to find out textual relationships with schools."}], "references": [{"title": "A Neural Probabilistic Language Model", "author": ["Bengio", "Yoshua", "Rjean Ducharme", "Pascal Vincent", "Christian Janvin."], "venue": "J. Mach. Learn. Res. 3 (March): 1137\u201355.", "citeRegEx": "Bengio et al\\.,? 2003", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures", "author": ["Bergstra", "James S.", "Daniel Yamins", "David Cox."], "venue": "Proceedings of the 30th International Conference on Machine Learn-", "citeRegEx": "Bergstra et al\\.,? 2013", "shortCiteRegEx": "Bergstra et al\\.", "year": 2013}, {"title": "A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning", "author": ["Collobert", "Ronan", "Jason Weston."], "venue": "Proceedings of the 25th International Conference on Machine Learning. 160\u2013167. ACM.", "citeRegEx": "Collobert et al\\.,? 2008", "shortCiteRegEx": "Collobert et al\\.", "year": 2008}, {"title": "Hierarchical Neural Language Models for For instance, learn a vector for the 111th House using its text and temporal relationships to other Houses, learn a vec", "author": ["Djuric", "Nemanja", "Hao Wu", "Vladan Radosavljevic", "Mihajlo Grbovic", "Narayan Bhamidipati"], "venue": null, "citeRegEx": "Djuric et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Djuric et al\\.", "year": 2015}, {"title": "A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases.", "author": ["Grimmer", "Justin"], "venue": "Political Analysis", "citeRegEx": "Grimmer and Justin.,? \\Q2010\\E", "shortCiteRegEx": "Grimmer and Justin.", "year": 2010}, {"title": "Temporal Analysis of Language Through Neural Language Models", "author": ["Kim", "Yoon", "Yi-I. Chiu", "Kentaro Hanaki", "Darshan Hegde", "Slav Petrov."], "venue": "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Sci-", "citeRegEx": "Kim et al\\.,? 2014", "shortCiteRegEx": "Kim et al\\.", "year": 2014}, {"title": "A Multiplicative Model for Learning Distributed Text-Based Attribute Representations", "author": ["Kiros", "Ryan", "Richard Zemel", "Ruslan R Salakhutdinov."], "venue": "Advances in Neural Information Processing Systems 27, edited by Z. Ghahramani, M.", "citeRegEx": "Kiros et al\\.,? 2014", "shortCiteRegEx": "Kiros et al\\.", "year": 2014}, {"title": "Statistically Significant Detection of Linguistic Change", "author": ["Kulkarni", "Vivek", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "Proceedings of the 24th International Conference on World Wide Web, 625\u201335. WWW \u201915. New York, NY, USA:", "citeRegEx": "Kulkarni et al\\.,? 2015", "shortCiteRegEx": "Kulkarni et al\\.", "year": 2015}, {"title": "Distributed Representations of Sentences and Documents", "author": ["Le", "Quoc", "Tomas Mikolov."], "venue": "Proceedings of the 31st International Conference on Machine Learning, 1188\u201396.", "citeRegEx": "Le et al\\.,? 2014", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "Linguistic Regularities in Continuous Space Word Representations", "author": ["T. Mikolov", "W.T. Yih", "G. Zweig."], "venue": "HLT-NAACL, 746\u201351.", "citeRegEx": "Mikolov et al\\.,? 2013a", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Distributed Representations of Words and Phrases and Their Compositionality", "author": ["Mikolov", "Tomas", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean."], "venue": "Advances in Neural Information Processing Systems 26, edited by C. J. C. Burges,", "citeRegEx": "Mikolov et al\\.,? 2013b", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Structural Topic Models for Open-Ended Survey Responses.", "author": ["Roberts", "Margaret E", "Brandon M. Stewart", "Dustin Tingley", "Christopher Lucas", "Jetson LederLuis", "Shana Kushner Gadarian", "Bethany Albertson", "David G. Rand"], "venue": null, "citeRegEx": "Roberts et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roberts et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 9, "context": "ficient training, relationships encoded in difference vectors can be uncovered with vector arithmetic: vec(\u201cking\u201d) - vec(\u201cman\u201d) + vec(\u201cwoman\u201d) returns a vector close to vec(\u201cqueen\u201d) (Mikolov et al. 2013a).", "startOffset": 182, "endOffset": 204}, {"referenceID": 0, "context": "direction of higher probability of observing the correct target word (Bengio et al. 2003; Mikolov et al. 2013b).", "startOffset": 69, "endOffset": 111}, {"referenceID": 10, "context": "direction of higher probability of observing the correct target word (Bengio et al. 2003; Mikolov et al. 2013b).", "startOffset": 69, "endOffset": 111}, {"referenceID": 10, "context": "After iterating over many word contexts, words with similar meaning are embedded in similar locations in vector space as a by-product of the prediction task (Mikolov et al. 2013b).", "startOffset": 157, "endOffset": 179}, {"referenceID": 1, "context": "We use a tree of Parzen estimators search algorithm (Bergstra et al. 2013) to sample from parameter space2 and save all models estimated.", "startOffset": 52, "endOffset": 74}, {"referenceID": 10, "context": "Due to stochasticity in training and the We use a binary Huffman tree (Mikolov et al. 2013b) for efficient hierarchical softmax prediction of words, and conduct 25 epochs while linearly decreasing the learning rate from 0.", "startOffset": 70, "endOffset": 92}, {"referenceID": 0, "context": "direction of higher probability of observing the correct target word (Bengio et al. 2003; Mikolov et al. 2013b). After iterating over many word contexts, words with similar meaning are embedded in similar locations in vector space as a by-product of the prediction task (Mikolov et al. 2013b). Le and Mikolov (2014) extend this word2vec method to learn representations of documents.", "startOffset": 70, "endOffset": 316}], "year": 2016, "abstractText": "We compare policy differences across institutions by embedding representations of the entire legal corpus of each institution and the vocabulary shared across all corpora into a continuous vector space. We apply our method, Gov2Vec, to Supreme Court opinions, Presidential actions, and official summaries of Congressional bills. The model discerns meaningful differences between government branches. We also learn representations for more finegrained word sources: individual Presidents and (2-year) Congresses. The similarities between learned representations of Congresses over time and sitting Presidents are negatively correlated with the bill veto rate, and the temporal ordering of Presidents and Congresses was implicitly learned from only text. With the resulting vectors we answer questions such as: how does Obama and the 113th House differ in addressing climate change and how does this vary from environmental or economic perspectives? Our work illustrates vectorarithmetic-based investigations of complex relationships between word sources. We are extending this to create a comprehensive legal semantic map.", "creator": "LaTeX with hyperref package"}}}