{"id": "1609.06649", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Minimally Supervised Written-to-Spoken Text Normalization", "abstract": "In speech-applications such as text-to-speech (TTS) or automatic speech recognition (ASR), \\emph{text normalization} refers to the task of converting from a \\emph{written} representation into a representation of how the text is to be \\emph{spoken}. In all real-world speech applications, the text normalization engine is developed---in large part---by hand. For example, a hand-built grammar may be used to enumerate the possible ways of saying a given token in a given language, and a statistical model used to select the most appropriate pronunciation in context. In this study we examine the tradeoffs associated with using more or less language-specific domain knowledge in a text normalization engine. In the most data-rich scenario, we have access to a carefully constructed hand-built normalization grammar that for any given token will produce a set of all possible verbalizations for that token. We also assume a corpus of aligned written-spoken utterances, from which we can train a ranking model that selects the appropriate verbalization for the given context. As a substitute for the carefully constructed grammar, we also consider a scenario with a language-universal normalization \\emph{covering grammar}, where the developer merely needs to provide a set of lexical items particular to the language. As a substitute for the aligned corpus, we also consider a scenario where one only has the spoken side, and the corresponding written side is \"hallucinated\" by composing the spoken side with the inverted normalization grammar. We investigate the accuracy of a text normalization engine under each of these scenarios. We report the results of experiments on English and Russian.", "histories": [["v1", "Wed, 21 Sep 2016 17:51:11 GMT  (1355kb)", "http://arxiv.org/abs/1609.06649v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["ke wu", "kyle gorman", "richard sproat"], "accepted": false, "id": "1609.06649"}, "pdf": {"name": "1609.06649.pdf", "metadata": {"source": "CRF", "title": "Minimally Supervised Written-to-Spoken Text Normalization", "authors": ["Ke Wu"], "emails": ["wuke@google.com", "kbg@google.com", "rws@google.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.06 649v 1 [cs.C L] 21 Sep 2016"}, {"heading": "1 Introduction", "text": "In fact, it is such that most people will be able to move into another world, in which they are able to live, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, in which they, live, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, in which they, in which they, in which they, live, in which they, in which they, in which they, live, in which they, live, in which they, in which they, in which they, in which they, in which they, in which they, in"}, {"heading": "2 Data and Conventions", "text": "In fact, most of them will be able to play by the rules they have set themselves, and they will be able to play by the rules they have set themselves."}, {"heading": "3 Ranking models", "text": "The discussion in this section refers to both the language-specific normalization grammar and the overarching grammar, and thus we both refer to the grammar as simple. In the case of a written character xi in the sentence x, the set of possible outputs Yi consists of both the result of the composition of xi with the grammar and xi itself (which corresponds to the passing of the symbol by unmodified ones). If | Yi | > 1 - i.e., if there are several normalization options for xi - we need a way to select the contextually appropriate output y-i-Yi, which comes closest to the reference verbalization zi."}, {"heading": "3.1 Local ranking", "text": "We can use a local character function \u03a6I (X, i, y), which sees the entire input and current verbalization, but not the verbalization of other written characters. Such character functions allow independent inferences for each written character. For the training, we simply generate a training example for each xi, for which | Yi | > 1. In our experiments, we use the following characteristics: Local output n-grams: n-grams (with n = 1, 2, 3) within the output yi; boundary trimmers: two written words on the left (xi \u2212 2, xi \u2212 1) and the first word of yi; two written words on the right (xi + 1, xi + 2) and the last word of yi; written / spoken skip programs: pairs of a written word on the left or right within a 4-word window and an output word in yi; bias: 1xi = yi, i.e. whether xi will pass through. 4 In the hope that they will be more useful on the scale of 2001 on the Russian map, we will use the 5mmatics and 5mmatics."}, {"heading": "3.2 Discriminative language model", "text": "In normalization, the majority of written characters are actually traversed, so there is a feature function \u03a6O (y1, j1,.., yi, ji, 1xi = yi, ji) that contains the verbalization history, but not much about the written characters that overlap with \u03a6I (\u00b7). Therefore, we limit the features to spoken symbols n-gram suffixes ending in yi, j and the bias 1xi = yi, j. Furthermore, we set the weight of the bias feature to a constant negative number, because walking through should be discouraged, leaving the n-gram weights as the only adjustable parameters. Then, the trained model can be coded as WFST (Wu et al., 2014) to achieve efficient conclusions. This feature parameterization also allows us to correct a model of spoken symbols without any information about the written characters."}, {"heading": "3.3 Candidate pruning", "text": "Although the number of candidates for normalization for a single written character in English almost never exceeds 100, the number of candidates in Russian can be prohibitively high. This is because Russian numerical grammar allows all possible morphological variants for each output word, even though most combinations are poorly formed, leading to a combinatorial explosion. We therefore distort the output by composing numerical grammar with a local language model over the spoken (output) side of the numerical grammar. The language model was trained using Witten Bell smoothing and a reserved corpus of about 10,000 randomly generated numbers. We find that it is also possible to extract this data from the web by identifying strings that match the output projection of unbiased grammar (Sprojat, 2010)."}, {"heading": "3.4 Discriminative LM vs local ranking", "text": "As we will see, there is a large gap in the error rate between discriminatory LMs trained on real data and local rankings, especially with language-specific grammars. This is due to the absence of two pieces of information that are not readily available for hallucinated data, the more interesting use case of the discriminatory LM method: Tuned Pass-Through Bias. We fix the bias because we cannot tune it with hallucinated data. However, grammar sometimes produces incorrect verbalizations for ordinary words that should really be traversed, often due to peculiarities in the annotation of the data. When training on real data, we can easily fix this problem by tuning the bias together with n-gram weights. Proverb limit Consider entering 1911 9mm, whose correct verb is nineteen eleven nine millimeters. The discriminatory LM prefers nine a single nine millimeter, since the hallucinated form we can interpret as having 19-millimeter, nine-millimeter, and five-strong data, there is a tendency to discriminate between these numbers."}, {"heading": "4 Results", "text": "In fact, most of them are able to survive on their own if they do not put themselves in a position to survive on their own."}, {"heading": "5 Qualitative error analysis", "text": "We conducted a qualitative error analysis to determine whether there were general generalizations about the types of errors that could be attributed to the use of cover grammar over language-specific grammar and hallucinations over real training data. We present these results below."}, {"heading": "5.1 English errors", "text": "For enveloping grammar (as opposed to language-specific grammar), the most important error categories were: \u2022 Derived numerical expressions such as 49ers, which are not supported by the enveloping grammar. \u2022 Occasional readings of words letter by letter: s o m e t h i ng. \u2022 Explicit readings at times such as 2200 - two thousand two hundred versus twenty-two hundred. Many of these error categories can be traced back to language-specific constructions that are simply not handled by the enveloping grammars, such as the case of 49. For hallucinated versus real data, the main categories were again double-digit readings of some numbers; and the reading of + as and not as a plus. This second error is probably due to the fact that + is hallucinated over real data and that we prefer the system as ordinary and + over the candidates, and V-as-as-as-as-and-as-as-as-way-back-of-as-A and-as-as-as-as-far."}, {"heading": "5.2 Russian errors", "text": "Russian errors in grammar vs. language-specific grammars can largely be divided into the following categories: \u2022 inability to read \"full stop\" in URLs. \u2022 use of cardinal numbers when ordinals would be more appropriate: \"August 15\" instead. \u2022 A small number of special cases of the use of \"one\" in the count (-1 2 3), which the numerical grammar does not allow. \u2022 The case of \"full stop\" seems to be due to a lapse in congruent grammar that does not take the top level ru domain into account. In hallucinated versus real data, the main errors were: \u2022 reading words as sequences of letters or vice versa: \"AK-47\" reads as (i.e. \"ak forty seven\" instead of \"a k forty seven\" versus) \u2022 numerical values grouped as: 4400 as (\"four four four zero zero\") versus (\"forty four zero zero zero\")."}, {"heading": "6 Discussion and Future Work", "text": "We have examined the relative contribution of various handmade data to the performance of text normalization systems trained on rankings. Specifically: \u2022 language-specific normalization grammatics versus language-independent grammar that covers grammars with a small amount of language-specific dictionaries; and \u2022 aligned written spoken data versus only spoken data that \"hallucinates\" the written page. We have shown that the performance impairment for using a language-specific grammar over a language-specific grammar does not have to be large - and in a configuration for Russian, the former actually outperformed the latter. Similarly, the degradation caused by hallucinated data does not have to be large; even in Russian, the hallucinated scenario easily outperforms the discriminatory LM trained on real data. The choice of English and Russian in these experiments was motivated by the fact that we already had text normalization systems for these languages (see footnote 7), and therefore we could not directly compare the existing approaches to specific languages."}, {"heading": "Acknowledgments", "text": "We thank Michael Riley for the many discussions about different versions of this work."}], "references": [{"title": "The Kestrel TTS text normalization system", "author": ["Ebden", "Sproat2014] Peter Ebden", "Richard Sproat"], "venue": "Natural Language Engineering,", "citeRegEx": "Ebden et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Ebden et al\\.", "year": 2014}, {"title": "Minimally supervised models for number normalization. Transactions of the Association for Computational Linguistics", "author": ["Gorman", "Sproat2016] Kyle Gorman", "Richard Sproat"], "venue": null, "citeRegEx": "Gorman et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gorman et al\\.", "year": 2016}, {"title": "Lexical normalization of short text messages: Makn sens a #twitter", "author": ["Han", "Baldwin2011] Bo Han", "Timothy Baldwin"], "venue": "In ACL,", "citeRegEx": "Han et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Han et al\\.", "year": 2011}, {"title": "Insertion, deletion, or substitution? Normalizing text messages without pre-categorization nor supervision", "author": ["Liu et al.2011] Fei Liu", "Fuliang Weng", "Bingqing Wang", "Yang Liu"], "venue": "In NAACL,", "citeRegEx": "Liu et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2011}, {"title": "Toward text message normalization: Modeling abbreviation generation", "author": ["Pennell", "Liu2011] Deana Pennell", "Yang Liu"], "venue": "In ICASSP,", "citeRegEx": "Pennell et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Pennell et al\\.", "year": 2011}, {"title": "Hippocratic abbreviation expansion", "author": ["Roark", "Sproat2014] Brian Roark", "Richard Sproat"], "venue": "In ACL,", "citeRegEx": "Roark et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Roark et al\\.", "year": 2014}, {"title": "The OpenGrm open-source finite-state grammar software libraries", "author": ["Roark et al.2012] Brian Roark", "Richard Sproat", "Cyril Allauzen", "Michael Riley", "Jeffrey Sorensen", "Terry Tai"], "venue": "In ACL,", "citeRegEx": "Roark et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Roark et al\\.", "year": 2012}, {"title": "Hallucinated N-best lists for discriminative language modeling", "author": ["K. Sagae", "M. Lehr", "E. Prud\u2019hommeaux", "P. Xu", "N. Glenn", "D. Karakosc", "S. Khudanpur", "B. Roark", "M. Saralar", "I. Shafran", "D. Bikel", "C. Callison-Burch", "Y. Cao", "K. Hall", "E. Hasler", "P. Koehn", "A. Lopez", "M. Post", "D. Riley"], "venue": null, "citeRegEx": "Sagae et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Sagae et al\\.", "year": 2012}, {"title": "Text normalization with varied data sources for conversational speech language modeling", "author": ["Schwarm", "Ostendorf2002] Sarah Schwarm", "Mari Ostendorf"], "venue": "In ICASSP,", "citeRegEx": "Schwarm et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Schwarm et al\\.", "year": 2002}, {"title": "Formatting time-aligned ASR transcripts for readability", "author": ["Maria Shugrina"], "venue": "In NAACL,", "citeRegEx": "Shugrina.,? \\Q2010\\E", "shortCiteRegEx": "Shugrina.", "year": 2010}, {"title": "Contrastive estimation: Training log-linear models on unlabeled data", "author": ["Smith", "Eisner2005] Noah Smith", "Jason Eisner"], "venue": "In ACL,", "citeRegEx": "Smith et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Smith et al\\.", "year": 2005}, {"title": "Applications of maximum entropy rankers to problems in spoken language processing", "author": ["Sproat", "Hall2014] Richard Sproat", "Keith B. Hall"], "venue": "In INTERSPEECH,", "citeRegEx": "Sproat et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sproat et al\\.", "year": 2014}, {"title": "Normalization of non-standard words", "author": ["Alan W. Black", "Stanley Chen", "Shankar Kumar", "Mari Ostendorf", "Christopher Richards"], "venue": "Computer Speech and Language,", "citeRegEx": "Sproat et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Sproat et al\\.", "year": 2001}, {"title": "Lightly supervised learning of text normalization: Russian number names", "author": ["Richard Sproat"], "venue": "In IEEE Workshop on Speech and Language Technology,", "citeRegEx": "Sproat.,? \\Q2010\\E", "shortCiteRegEx": "Sproat.", "year": 2010}, {"title": "Text to speech synthesis", "author": ["Paul Taylor"], "venue": null, "citeRegEx": "Taylor.,? \\Q2009\\E", "shortCiteRegEx": "Taylor.", "year": 2009}, {"title": "Sequence-based class tagging for robust transcription in ASR", "author": ["Vlad Schogol", "Keith Hall"], "venue": "In INTERSPEECH,", "citeRegEx": "Vasserman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vasserman et al\\.", "year": 2015}, {"title": "Encoding linear models as weighted finite-state transducers", "author": ["Wu et al.2014] Ke Wu", "Cyril Allauzen", "Keith B. Hall", "Michael Riley", "Brian Roark"], "venue": "In INTERSPEECH,", "citeRegEx": "Wu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wu et al\\.", "year": 2014}, {"title": "A log-linear model for unsupervised text normalization", "author": ["Yang", "Eisenstein2013] Yi Yang", "Jacob Eisenstein"], "venue": "In EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 12, "context": "Over the past 15 years there has been substantial progress on machine-learning approaches to text normalization (Sproat et al., 2001; Schwarm and Ostendorf, 2002; Han and Baldwin, 2011; Liu et al., 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013).", "startOffset": 112, "endOffset": 253}, {"referenceID": 3, "context": "Over the past 15 years there has been substantial progress on machine-learning approaches to text normalization (Sproat et al., 2001; Schwarm and Ostendorf, 2002; Han and Baldwin, 2011; Liu et al., 2011; Pennell and Liu, 2011; Yang and Eisenstein, 2013).", "startOffset": 112, "endOffset": 253}, {"referenceID": 3, "context": "There are, for example, several training corpora for social media text normalization (Han and Baldwin, 2011; Liu et al., 2011; Yang and Eisenstein, 2013), but these are of little use", "startOffset": 85, "endOffset": 153}, {"referenceID": 12, "context": "Minimally supervised approaches that attempt to construct normalizers using unannotated text data based on matching unnormalized forms with possible expansions given some corpus of \u2018clean\u2019 text (Sproat et al., 2001; Yang and Eisenstein, 2013; Roark and Sproat, 2014) are relevant, but are only applicable to certain classes of tokens, such as abbreviations.", "startOffset": 194, "endOffset": 266}, {"referenceID": 6, "context": "The normalization grammars are represented as weighted finite-state transducers (WFST) compiled using the Thrax grammar development system (Roark et al., 2012).", "startOffset": 139, "endOffset": 159}, {"referenceID": 5, "context": "The normalization grammars are represented as weighted finite-state transducers (WFST) compiled using the Thrax grammar development system (Roark et al., 2012). This grammar is designed to overgenerate; that is, it produces, for any input token, a (potentially weighted) lattice of possible spoken strings \u2014 see (Gorman and Sproat, 2016). Thus for example 123 might produce one hundred twenty three, one two three, one twenty three, among possibly others. In Section 3 we describe how we combine this grammar with a reranker that selects the appropriate choice in context. These language-specific grammars are written by native speaker linguists and designed to cover a range of types of normalization phenomena, including numbers, some mathematical expressions, some abbreviations, times and currency amounts. Following Taylor (2009), we refer to these categories targeted by normalization as semiotic classes.", "startOffset": 140, "endOffset": 835}, {"referenceID": 9, "context": "the normalizer to turn it into a denormalizer (Shugrina, 2010; Vasserman et al., 2015), and then composing the resulting lattice of putative written forms with the normalizer.", "startOffset": 46, "endOffset": 86}, {"referenceID": 15, "context": "the normalizer to turn it into a denormalizer (Shugrina, 2010; Vasserman et al., 2015), and then composing the resulting lattice of putative written forms with the normalizer.", "startOffset": 46, "endOffset": 86}, {"referenceID": 6, "context": "The language-specific normalization grammar for English consists of about 2,500 lines of rules and lexical specifications written using the Thrax finite-state grammar development toolkit (Roark et al., 2012), and the equivalent Russian grammar has about 4,100 lines.", "startOffset": 187, "endOffset": 207}, {"referenceID": 16, "context": "Then, the trained model can be encoded as a WFST (Wu et al., 2014) for efficient inference.", "startOffset": 49, "endOffset": 66}, {"referenceID": 13, "context": "We note it is also possible to mine this data from the web by identifying strings which match the output projection of the unbiased grammar (Sproat, 2010).", "startOffset": 140, "endOffset": 154}], "year": 2016, "abstractText": "In speech-applications such as text-to-speech (TTS) or automatic speech recognition (ASR), text normalization refers to the task of converting from a written representation into a representation of how the text is to be spoken. In all real-world speech applications, the text normalization engine is developed\u2014in large part\u2014by hand. For example, a handbuilt grammar may be used to enumerate the possible ways of saying a given token in a given language, and a statistical model used to select the most appropriate pronunciation in context. In this study we examine the tradeoffs associated with using more or less language-specific domain knowledge in a text normalization engine. In the most datarich scenario, we have access to a carefully constructed hand-built normalization grammar that for any given token will produce a set of all possible verbalizations for that token. We also assume a corpus of aligned written-spoken utterances, from which we can train a ranking model that selects the appropriate verbalization for the given context. As a substitute for the carefully constructed grammar, we also consider a scenario with a language-universal normalization covering grammar, where the developer merely needs to provide a set of lexical items particular to the language. As a substitute for the aligned corpus, we also consider a scenario where one only has the spoken side, and the corresponding written side is \u201challucinated\u201d by composing the spoken side with the inverted normalization grammar. We investigate the accuracy of a text normalization engine under each of these scenarios. We report the results of experiments on English and Russian.", "creator": "LaTeX with hyperref package"}}}