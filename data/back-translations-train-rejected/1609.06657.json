{"id": "1609.06657", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)", "abstract": "Visual Question Answering (VQA) task has showcased a new stage of interaction between language and vision, two of the most pivotal components of artificial intelligence. However, it has mostly focused on generating short and repetitive answers, mostly single words, which fall short of rich linguistic capabilities of humans. We introduce Full-Sentence Visual Question Answering (FSVQA) dataset, consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying a number of rule-based natural language processing techniques to original VQA dataset and captions in the MS COCO dataset. This poses many additional complexities to conventional VQA task, and we provide a baseline for approaching and evaluating the task, on top of which we invite the research community to build further improvements.", "histories": [["v1", "Wed, 21 Sep 2016 18:12:04 GMT  (6458kb,D)", "http://arxiv.org/abs/1609.06657v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["andrew shin", "yoshitaka ushiku", "tatsuya harada"], "accepted": false, "id": "1609.06657"}, "pdf": {"name": "1609.06657.pdf", "metadata": {"source": "CRF", "title": "The Color of the Cat is Gray: 1 Million Full-Sentences Visual Question Answering (FSVQA)", "authors": ["Andrew Shin", "Yoshitaka Ushiku", "Tatsuya Harada"], "emails": [], "sections": [{"heading": "Introduction", "text": "The mentionlrceheaeVnlrsrtee\u00fcgr rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rf"}, {"heading": "Related Work", "text": "A number of data sets for answering visual questions have been introduced in recent years (Malinowski and Fritz 2014; Ren, Kiros and Zemel 2015), among which (Antol et al. 2015) in particular has attracted the most attention and has helped popularize the task. However, these data sets largely consist of a small set of answers that covers most questions, and most answers are a single word. Our FSVQA data set, derived from (Antol et al. 2015), minimizes such limitations by converting the answers into full sentences, broadening the set of answers far. (Fukui et al. 2016) proposed multimodal compact bilinear pooling (MCB) to combine multimodal features of visual and text representation, which won 1st place in the VQA challenge in the real image category. (Saito et al al al. 2016) Proposed multifunctional pooling (MCB) will be performed both in the dualplicate category and in the dualplicate category of the proposed QA."}, {"heading": "Dataset", "text": "We circumvent these financial costs by converting the answers in the original VQA dataset (Antol et al. 2015) to get complete answers by applying a set of rules for processing natural language. In addition, we also offer an extended version of the dataset by converting the descriptions contained in MS COCO. We ask questions with a set of rules for which the title itself is the answer. Both versions of the FSVQA datasets, along with the features used in our experiment as described in the Experiment section, are publicly available."}, {"heading": "Converting Captions", "text": "We also offer an extended version of the dataset by converting the human-written captions into questions and answers. Apart from yes / no questions, the answers to the generated questions are the captions themselves, eliminating the burden of generating reliable answers. Most images in MS COCO contain 5 captions, and we have generated different questions for each caption, the conversion rule of which is in Table 2.We have assigned at least two yes / no questions with a yes and a no to all images, roughly balancing the number of answers with \"yes\" and \"no.\" Questions with positive answers that contain \"yes\" were generated by simply reformulating the caption in such a way that the question confirms the content in the caption, for which the answer is an affirmative statement of the question (which is the caption itself), accompanied by \"yes,\" questions with affirmative answers that contain \"yes\" and \"yes\" is not an affirmative answer in the affirmative answer (which is \"yes\")."}, {"heading": "Statistics", "text": "Table 3 shows statistics for the original VQA dataset and two versions of the FSVQA dataset. Both versions of the FSVQA contain, on average, much longer answers, and the number of unique answers is more than ten times greater in the regular version and about 38 times larger in the extended version than the VQA dataset. The extended version is longer, because captions in MS COCO tend to be longer than questions in VQA. Also, the vocabulary size is much larger in both versions. Note that, consistent with the conversion process, only the most common answer of 10 answers per question was considered for the statistics of the VQA dataset. Comparing the coverage of 1,000 common answers in the datasets, this shows an even stronger contrast. In the VQA datasets, 1,000 common answers covering over 86.5% of the entire dataset are relatively easy to learn from a small group of frequent answers."}, {"heading": "Setting", "text": "We used 4096-dimensional characteristics from the second fully connected layer (fc7) of VGG (Simonyan and Zisserman 2014) with 19 layers trained on ImageNet (Deng et al. 2009) as our image characteristics. Words in the question were entered into the LSTM (Hochreiter and Schmidhuber 1997) individually as a hot vector, with the dictionary containing only those words that appear more than once. Image characteristics and question characteristics are then mapped to the common embedding space as a 1024-dimensional vector, the batch size was 500 and the training was performed for 300 epochs. We trained only with the answers that appear twice or more in the train distribution, as all the unique answers in the data set are not performed, with the memory required far exceeding the capacity of most contemporary GPUs, NVIDIA Tesla 40m in our case. 20,130 answers appear more than once in the regular version of the 940 questions covered."}, {"heading": "Evaluation", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "Conclusion", "text": "We introduced FSVQA, a publicly available dataset consisting of nearly one million pairs of questions and integral answers for images, built on existing datasets by applying linguistic rules. As we push the VQA task to a more human level, it presents many additional complexities. We explored basic approaches to this new task. Applying some of the successful approaches from the original VQA task, such as attention mechanisms, will be an intriguing and important work in the future. Whether a generative approach can play a greater role in the future as the number of answers increases and the classification approach becomes less efficient is also of interest. We invite the research community to develop an innovative and efficient method to improve the performance of FSVQA."}, {"heading": "Acknowledgement", "text": "This work was funded by the ImPACT programme of the Council for Science, Technology and Innovation (Cabinet Office, Japanese Government)."}], "references": [{"title": "Vqa: Visual question answering", "author": ["Antol"], "venue": null, "citeRegEx": "Antol,? \\Q2015\\E", "shortCiteRegEx": "Antol", "year": 2015}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["Deng"], "venue": null, "citeRegEx": "Deng,? \\Q2009\\E", "shortCiteRegEx": "Deng", "year": 2009}, {"title": "and Lavie", "author": ["M. Denkowski"], "venue": "A.", "citeRegEx": "Denkowski and Lavie 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding", "author": ["Fukui"], "venue": null, "citeRegEx": "Fukui,? \\Q2016\\E", "shortCiteRegEx": "Fukui", "year": 2016}, {"title": "and Schmidhuber", "author": ["S. Hochreiter"], "venue": "J.", "citeRegEx": "Hochreiter and Schmidhuber 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "and Li", "author": ["A. Karpathy"], "venue": "F.", "citeRegEx": "Karpathy and Li 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multimodal Residual Learning for Visual QA", "author": ["Kim"], "venue": null, "citeRegEx": "Kim,? \\Q2016\\E", "shortCiteRegEx": "Kim", "year": 2016}, {"title": "C", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Dollr", "Zitnick"], "venue": "L.", "citeRegEx": "Lin et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Hierarchical Question-Image Co-Attention for Visual Question Answering", "author": ["Lu"], "venue": null, "citeRegEx": "Lu,? \\Q2016\\E", "shortCiteRegEx": "Lu", "year": 2016}, {"title": "and Fritz", "author": ["M. Malinowski"], "venue": "M.", "citeRegEx": "Malinowski and Fritz 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "The Penn Treebank: annotating predicate argument structure", "author": ["Marcus"], "venue": "In HLT", "citeRegEx": "Marcus,? \\Q1994\\E", "shortCiteRegEx": "Marcus", "year": 1994}, {"title": "Coherent Image Annotation by Learning Semantic Distance", "author": ["Mei"], "venue": null, "citeRegEx": "Mei,? \\Q2008\\E", "shortCiteRegEx": "Mei", "year": 2008}, {"title": "and Han", "author": ["H. Noh"], "venue": "B.", "citeRegEx": "Noh and Han 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Recognizing activities of daily living with a wrist-mounted camera", "author": ["Ohnishi"], "venue": null, "citeRegEx": "Ohnishi,? \\Q2016\\E", "shortCiteRegEx": "Ohnishi", "year": 2016}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["Papineni"], "venue": null, "citeRegEx": "Papineni,? \\Q2002\\E", "shortCiteRegEx": "Papineni", "year": 2002}, {"title": "Exploring Models and Data for Image Question Answering", "author": ["Kiros Ren", "M. Zemel 2015] Ren", "R. Kiros", "R. Zemel"], "venue": null, "citeRegEx": "Ren et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ren et al\\.", "year": 2015}, {"title": "Data-Driven Response Generation in Social Media", "author": ["Cherry Ritter", "A. Dolan 2011] Ritter", "C. Cherry", "B. Dolan"], "venue": null, "citeRegEx": "Ritter et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ritter et al\\.", "year": 2011}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["Saito"], "venue": null, "citeRegEx": "Saito,? \\Q2016\\E", "shortCiteRegEx": "Saito", "year": 2016}, {"title": "and Zisserman", "author": ["K. Simonyan"], "venue": "A.", "citeRegEx": "Simonyan and Zisserman 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Ucf101: A dataset of 101 human actions classes from videos in the wild. arXiv:1212.0402", "author": ["Zamir Soomro", "K. Shah 2012] Soomro", "A. Zamir", "M. Shah"], "venue": null, "citeRegEx": "Soomro et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Soomro et al\\.", "year": 2012}, {"title": "C", "author": ["Vedantam, R.", "Zitnick"], "venue": "L.; and Parikh, D.", "citeRegEx": "Vedantam. Zitnick. and Parikh 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Vinyals"], "venue": null, "citeRegEx": "Vinyals,? \\Q2015\\E", "shortCiteRegEx": "Vinyals", "year": 2015}, {"title": "Multi-Label Sparse Coding for Automatic Image Annotation", "author": ["Wang"], "venue": null, "citeRegEx": "Wang,? \\Q2009\\E", "shortCiteRegEx": "Wang", "year": 2009}, {"title": "Show, Attend and Tell: Neural Image Caption Generation with Visual Attention", "author": ["Xu"], "venue": "In ICML", "citeRegEx": "Xu,? \\Q2015\\E", "shortCiteRegEx": "Xu", "year": 2015}, {"title": "Stacked Attention Networks for Image Question Answering", "author": ["Yang"], "venue": null, "citeRegEx": "Yang,? \\Q2016\\E", "shortCiteRegEx": "Yang", "year": 2016}], "referenceMentions": [], "year": 2016, "abstractText": "Visual Question Answering (VQA) task has showcased a new stage of interaction between language and vision, two of the most pivotal components of artificial intelligence. However, it has mostly focused on generating short and repetitive answers, mostly single words, which fall short of rich linguistic capabilities of humans. We introduce Full-Sentence Visual Question Answering (FSVQA) dataset (www.mi.t.u-tokyo.ac. jp/static/projects/fsvqa), consisting of nearly 1 million pairs of questions and full-sentence answers for images, built by applying a number of rule-based natural language processing techniques to original VQA dataset and captions in the MS COCO dataset. This poses many additional complexities to conventional VQA task, and we provide a baseline for approaching and evaluating the task, on top of which we invite the research community to build further improvements.", "creator": "LaTeX with hyperref package"}}}