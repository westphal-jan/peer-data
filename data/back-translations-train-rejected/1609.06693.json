{"id": "1609.06693", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "SoftTarget Regularization: An Effective Technique to Reduce Over-Fitting in Neural Networks", "abstract": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropCon- nect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh &amp; Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "histories": [["v1", "Wed, 21 Sep 2016 19:31:07 GMT  (801kb,D)", "https://arxiv.org/abs/1609.06693v1", null], ["v2", "Thu, 22 Sep 2016 00:23:30 GMT  (801kb,D)", "http://arxiv.org/abs/1609.06693v2", null], ["v3", "Sat, 3 Dec 2016 08:08:53 GMT  (805kb,D)", "http://arxiv.org/abs/1609.06693v3", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["armen aghajanyan"], "accepted": false, "id": "1609.06693"}, "pdf": {"name": "1609.06693.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Armen Aghajanyan"], "emails": ["armen.aghajanyan@dimensionalmechanics.com"], "sections": [{"heading": null, "text": "Many regulatory techniques, such as Dropout, DropConnect, and Weight Loss, all attempt to solve the problem of overadjustment by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992).In this paper, we present a new form of regulation that directs the learning problem in a way that reduces overadjustment without sacrificing the capacity of the model.The mistakes that models make in the early stages of training carry information about the learning problem. By adjusting the labels of the current era of training by a weighted average of real labels and an exponential average of past soft targets, we have achieved a regulation program as powerful as Dropout without necessarily reducing the capacity of the model, and simplifying the complexity of the learning problem."}, {"heading": "1 INTRODUCTION", "text": "Many regulatory techniques have been developed to address the problem of overfitting into deep neural networks, but most of these methods reduce the capacities of models to force them to learn sufficiently general characteristics. For example, Dropout reduces the number of learnable parameters by dropping random activations, and DropConnect expands this idea by randomly dropping weights (Srivastava et al., 2014), (Wan et al., 2013)."}, {"heading": "1.1 MOTIVATION", "text": "Hinton has shown that soft labels or labels predicted by a model contain more information than binary hard labels due to the fact that they encode similarities between classes (Hinton et al., 2015). False labels identified by the model describe similarities between the two labels, and these similarities should be evident in future learning stages, even if the effect is less. Imagine, for example, forming a deep neural network on a dataset for classifying different breeds of dogs. In the initial stages of learning, the model will not make exact differences between similar breeds of dogs such as a Belgian shepherd and a German shepherd. The same effect, though not so exaggerated, should occur in later training stages. If the model predicts the German shepherd class with high accuracy in the light of an image of a German shepherd dog, the nearest predicted dog should still be a Belgian shepherd dog."}, {"heading": "1.2 METHOD", "text": "Consider the standard overriding learning problem. In view of a dataset with inputs and outputs, X and Y, a regularization function R and a model prediction function F, we have tried to minimize the loss function L, that of: L (X, Y) = 1 N + 0 Li (F (Xi, W), Yi) + 2 (W) + 1), where we modify the optimization problem by adding a time dimension (t) to the loss function: Lt (X, Y) = 0 Lti (F), Yi) + R (W) (2), which we have to adjust in two steps: first, we have an exponential moving average of past labels Y, and second, we update the current epochs."}, {"heading": "1.3 SIMILARITIES TO OTHER METHODS", "text": "Other similar methods are specific to the case where the \u03b2 hyperparameter is set to zero without baking time. \u2022 Reed et al. investigate the specific case of the SoftTarget method described above with the \u03b2 parameter to zero (Reed et al., 2014), focusing on the network's ability to be noise-resistant rather than on the method's regulatory capabilities. \u2022 Grandvalet and Bengio have proposed minimal entropy regulation in determining semi-supervised learning (Grandvalet & Bengio, 2005). This algorithm alters the categorical cross-entropy loss to force the network to make predictions with a high degree of reliability about the unmarked part of the dataset."}, {"heading": "2 EXPERIMENTS", "text": "We performed Python experiments with the Theano and Keras libraries (The Theano Development Team, 2016), (Chollet, 2015), all of our code ran on a single Nvidia Titan X GPU using the CnMEM and cuDNN (5.103) extensions, and we visualized our results with Matplotlib (Hunter, 2007), using the same seed in all of our calculations to ensure that the initial weights were the same in each set of experiments, and the only source of randomness was the non-deterministic behavior of the cuDNN libraries."}, {"heading": "2.1 MNIST", "text": "We first looked at the famous MNIST dataset (LeCun et al., 1998), and for each of the experiments described below, we performed a random network search using the hyperparameters of the optimization algorithm, and for the hyperparameters of SoftTarget regularization, a very small brute force network search was performed. We compared our results with the cases where the hyperparameters resulted in the best performance of the vanilla neural network without SoftTarget regularization, and all of our reported values were calculated using the standardized test part of the MNIST dataset, as provided by the Keras library. Networks were rigorously trained on the training part of the dataset. We tested eight different architectures with four combinations of each architecture, and the four combinations came from testing each architecture using a combination of: no regulation, dropout, soft camouflage, soft camouflage, and soft camouflage."}, {"heading": "2.2 CIFAR-10", "text": "We then looked at the CIFAR-10 dataset (Krizhevsky & Hinton, 2009) by comparing different combinations of SoftTarget, Dropout and BatchNormalization (BN) (Ioffe & Szegedy, 2015).BatchNormalization has shown that there is a regulatory effect on neural networks due to the noise inherent in the mini-batch statistics.We have gone through each configuration of the network through sixty iterations throughout the training series, using the full architecture: Input \u2192 Convolution (64,3,3) \u2192 BN \u2192 Convolution (64,3,3) \u2192 Convolution (BN) \u2192 BN \u2192 RedatPooling (3,3)."}, {"heading": "2.3 SVHN", "text": "Finally, we looked at the Street View house number dataset (SVHN), which consists of different images mapped to one of ten digits (Netzer et al., 2011), which is similar to the MNIST dataset, but is much more organic as these images contain much more natural noise, such as lighting conditions and camera orientation. We tested residual networks in four configurations: No regulation, Batch Normalization (BN), SoftTarget and BN + SoftTarget (?). Our architecture consisted of the same building blocks as his et al., consisting of identity and conversion blocks (BN et al., 2015). Identity blocks are blocks that contain no conversion layer at the link, while conversion blocks do. In our notation I (3, [16,16,32], BN), an identity block with an average square conversion size of 3, three of 32, 16 and 16.1 means the conversion blocks."}, {"heading": "2.4 CO-LABEL SIMILARITIES", "text": "To test this hypothesis, we compared the covariant matrices of an excessively adapted network, early training networks, and regularized networks. We compared four configurations: Early (10 epochs), Overfit (100 epochs), Dropout (p = 0.2, 100 epochs), and SoftTarget (nb = 2, \u03b2 = 0.7, 100 epochs). After training each configuration for its respected amount, we predicted the labels of the training, and then calculated a covariance matrix scaled to a range of [0, 1]."}, {"heading": "3 CONCLUSION AND FUTURE WORK", "text": "Finally, we presented a new regulation method based on the observation that similarities that are visible at the beginning of the training disappear as soon as a network begins to fit. SoftTargetgularization reduced overfit and dropout without adding complexity to the network, reducing computing time, and we provided new insights into the problem of overfitting. Future work will focus on methods to reduce the number of hyperparameters introduced by SoftTarget regulation and create a formal mathematical framework for the phenomenon of co-label similarities."}], "references": [{"title": "Random search for hyper-parameter optimization", "author": ["James Bergstra", "Yoshua Bengio"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "Bergstra and Bengio.,? \\Q2012\\E", "shortCiteRegEx": "Bergstra and Bengio.", "year": 2012}, {"title": "URL https://github.com/ fchollet/keras", "author": ["Fran\u00e7ois Chollet"], "venue": "Keras Deep Learning Library,", "citeRegEx": "Chollet.,? \\Q2015\\E", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Categorization, prototype theory and neural dynamics", "author": ["W. Duch"], "venue": "Proceedings of the 4th International Conference on Soft Computing,", "citeRegEx": "Duch.,? \\Q1996\\E", "shortCiteRegEx": "Duch.", "year": 1996}, {"title": "Semi-supervised Learning by Entropy", "author": ["Yves Grandvalet", "Yoshua Bengio"], "venue": "Minimization. Network,", "citeRegEx": "Grandvalet and Bengio.,? \\Q2005\\E", "shortCiteRegEx": "Grandvalet and Bengio.", "year": 2005}, {"title": "Deep Residual Learning for Image Recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv, pp", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Distilling the Knowledge in a Neural Network", "author": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean"], "venue": "arXiv, pp", "citeRegEx": "Hinton et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2015}, {"title": "Matplotlib: A 2D Graphics Environment", "author": ["John D Hunter"], "venue": "Computing in Science and Engineering,", "citeRegEx": "Hunter.,? \\Q2007\\E", "shortCiteRegEx": "Hunter.", "year": 2007}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv, pp", "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Learning Multiple Layers of Features from Tiny Images", "author": ["Alex Krizhevsky", "Geoffrey Hinton"], "venue": "Technical report,", "citeRegEx": "Krizhevsky and Hinton.,? \\Q2009\\E", "shortCiteRegEx": "Krizhevsky and Hinton.", "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": "Advances In Neural Information Processing Systems,", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "A Simple Weight Decay Can Improve Generalization", "author": ["A. Krogh", "J. a. Hertz"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "Krogh and Hertz.,? \\Q1992\\E", "shortCiteRegEx": "Krogh and Hertz.", "year": 1992}, {"title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks", "author": ["Dong-Hyun Lee"], "venue": "In ICML 2013 Workshop: Challenges in Representation Learning (WREPL),", "citeRegEx": "Lee.,? \\Q2013\\E", "shortCiteRegEx": "Lee.", "year": 2013}, {"title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "author": ["Yuval Netzer", "Tao Wang", "Adam Coates", "Alessandro Bissacco", "Bo Wu", "Andrew Y Ng"], "venue": "In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,", "citeRegEx": "Netzer et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Netzer et al\\.", "year": 2011}, {"title": "On the adequacy of prototype theory as a theory of concepts", "author": ["Daniel N. Osherson", "Edward E. Smith"], "venue": "Cognition,", "citeRegEx": "Osherson and Smith.,? \\Q1981\\E", "shortCiteRegEx": "Osherson and Smith.", "year": 1981}, {"title": "Training Deep Neural Networks on Noisy Labels with Bootstrapping", "author": ["Scott Reed", "Honglak Lee", "Dragomir Anguelov", "Christian Szegedy", "Dumitru Erhan", "Andrew Rabinovich"], "venue": "arXiv, pp", "citeRegEx": "Reed et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Reed et al\\.", "year": 2014}, {"title": "Principles of Categorization", "author": ["Eleanor Rosch"], "venue": "Cognition and categorization,", "citeRegEx": "Rosch.,? \\Q1978\\E", "shortCiteRegEx": "Rosch.", "year": 1978}, {"title": "Evaluation of Pooling Operations in Convolutional Architectures for Object Recognition, pp. 92\u2013101", "author": ["Dominik Scherer", "Andreas M\u00fcller", "Sven Behnke"], "venue": null, "citeRegEx": "Scherer et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Scherer et al\\.", "year": 2010}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Development Team. Theano: A Python framework for fast computation of mathematical expressions", "author": ["The Theano"], "venue": "arXiv, pp", "citeRegEx": "Theano,? \\Q2016\\E", "shortCiteRegEx": "Theano", "year": 2016}, {"title": "Regularization of Neural Networks using DropConnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler"], "venue": "arXiv, pp", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 17, "context": "Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al.", "startOffset": 179, "endOffset": 204}, {"referenceID": 19, "context": ", 2014), (Wan et al., 2013), (Krogh & Hertz, 1992).", "startOffset": 9, "endOffset": 27}, {"referenceID": 17, "context": "For example, Dropout reduces the amount of learn-able parameters by randomly dropping activations, and DropConnect extends this idea by randomly dropping weights (Srivastava et al., 2014), (Wan et al.", "startOffset": 162, "endOffset": 187}, {"referenceID": 19, "context": ", 2014), (Wan et al., 2013).", "startOffset": 9, "endOffset": 27}, {"referenceID": 5, "context": "Hinton has shown that soft-labels, or labels predicted from a model contain more information that binary hard labels due to the fact that they encode similarity measures between the classes (Hinton et al., 2015).", "startOffset": 190, "endOffset": 211}, {"referenceID": 14, "context": "study the specific case of the SoftTarget method described above with the \u03b2 parameter set to zero (Reed et al., 2014).", "startOffset": 98, "endOffset": 117}, {"referenceID": 11, "context": "Pseudo-labeling tags unlabeled data with the class predicted highest by a learning model (Lee, 2013).", "startOffset": 89, "endOffset": 100}, {"referenceID": 5, "context": "\u2022 Hinton et al described the power of soft targets in the use of transferring knowledge from one model to another, usually to a model that contains less parameters (Hinton et al., 2015).", "startOffset": 164, "endOffset": 185}, {"referenceID": 1, "context": "We conducted experiments in python using the Theano and Keras libraries (The Theano Development Team, 2016), (Chollet, 2015).", "startOffset": 109, "endOffset": 124}, {"referenceID": 6, "context": "103) extensions, and we visualized our results using matplotlib (Hunter, 2007).", "startOffset": 64, "endOffset": 78}, {"referenceID": 20, "context": "(Zeiler, 2012).", "startOffset": 0, "endOffset": 14}, {"referenceID": 9, "context": "where: Convolution (64,3,3) signifies the convolution operator with 64 filters, and a kernel size of 3 by 3, MaxPooling ((3,3), (2,2)) represents the max-pooling operation with a kernel size of 3 by 3, and a stride of 2 by 2, AveragePooling ((6,6)) represents the average pooling operator with a kernel size of 6 by 6, Flatten represents a flattening of the tensor into a matrix, and Dense (256) a fully-connected layer (Krizhevsky et al., 2012), (Scherer et al.", "startOffset": 420, "endOffset": 445}, {"referenceID": 16, "context": ", 2012), (Scherer et al., 2010).", "startOffset": 9, "endOffset": 31}, {"referenceID": 12, "context": "3 SVHN Finally, we considered the Street View House Numbers (SVHN) dataset, consisting of various images mapping to one of ten digits (Netzer et al., 2011).", "startOffset": 134, "endOffset": 155}, {"referenceID": 4, "context": ", consisting of identity and convolution blocks (He et al., 2015).", "startOffset": 48, "endOffset": 65}, {"referenceID": 2, "context": "It is interesting to note, that this is the core principle behind prototype theory in human psychology (Osherson & Smith, 1981), (Duch, 1996), (Rosch, 1978).", "startOffset": 129, "endOffset": 141}, {"referenceID": 15, "context": "It is interesting to note, that this is the core principle behind prototype theory in human psychology (Osherson & Smith, 1981), (Duch, 1996), (Rosch, 1978).", "startOffset": 143, "endOffset": 156}], "year": 2016, "abstractText": "Deep neural networks are learning models with a very high capacity and therefore prone to over-fitting. Many regularization techniques such as Dropout, DropConnect, and weight decay all attempt to solve the problem of over-fitting by reducing the capacity of their respective models (Srivastava et al., 2014), (Wan et al., 2013), (Krogh & Hertz, 1992). In this paper we introduce a new form of regularization that guides the learning problem in a way that reduces over-fitting without sacrificing the capacity of the model. The mistakes that models make in early stages of training carry information about the learning problem. By adjusting the labels of the current epoch of training through a weighted average of the real labels, and an exponential average of the past soft-targets we achieved a regularization scheme as powerful as Dropout without necessarily reducing the capacity of the model, and simplified the complexity of the learning problem. SoftTarget regularization proved to be an effective tool in various neural network architectures.", "creator": "LaTeX with hyperref package"}}}