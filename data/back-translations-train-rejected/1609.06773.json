{"id": "1609.06773", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "21-Sep-2016", "title": "Joint CTC-Attention based End-to-End Speech Recognition using Multi-task Learning", "abstract": "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoder-decoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the attention model has shown poor results especially in noisy condition and is hard to be trained in the initial training stage with long input sequences, as compared with CTC. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-to-right constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 6.6-10.3% relative improvements in Character Error Rate (CER).", "histories": [["v1", "Wed, 21 Sep 2016 22:48:53 GMT  (5155kb,D)", "http://arxiv.org/abs/1609.06773v1", null], ["v2", "Tue, 31 Jan 2017 21:00:01 GMT  (5466kb,D)", "http://arxiv.org/abs/1609.06773v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["suyoun kim", "takaaki hori", "shinji watanabe"], "accepted": false, "id": "1609.06773"}, "pdf": {"name": "1609.06773.pdf", "metadata": {"source": "CRF", "title": "JOINT CTC-ATTENTION BASED END-TO-END SPEECH RECOGNITION USING MULTI-TASK LEARNING", "authors": ["Suyoun Kim", "Takaaki Hori", "Shinji Watanabe"], "emails": [], "sections": [{"heading": null, "text": "Index terms - end-to-end, speech recognition, connectionist temporal classification, attention, multi-task learning"}, {"heading": "1. INTRODUCTION", "text": "In this context, it should be noted that this project is a project, which is primarily a project."}, {"heading": "2. JOINT CTC-ATTENTION MECHANISM", "text": "In this section, we review the CTC in Section 2.1 and the attention-based encoder decoder in Section 2.2, which deals with input boxes of variable (T) length, x = (x1, \u00b7 \u00b7, xT) and U-length output characters y = (y1, \u00b7 \u00b7 \u00b7, yU), where yu value {1, \u00b7 \u00b7 \u00b7, K} is the number of different labels. Then, our common end-to-end frame based on CTC attention is described in Section 2.3.ar Xiv: 160 9.06 773v 1 [cs.C L] 21 Sep 2016."}, {"heading": "2.1. Connectionist temporal classification (CTC)", "text": "The key idea of CTC [12] is to use the middle label representation \u03c0 = (\u03c01, \u00b7 \u00b7 \u00b7, \u03c0T), which allows repetitions of labels and occurrence of an empty label (\u2212), which represents special emission without labels, i.e.: P (y | x) = total labeling (y \u2032). CTC constructs the model for maximizing P (y | x), whereby the probability distribution can take place across all possible sequences of labels. (y \u2032): P (y \u2032) = total independence (y \u2032) P (\u03c0 | x), (1) where \u2032 is a modified label sequence produced by inserting empty symbols between labels, in order to allow blanks in the edition, i.e., yu \u04321, K} a modified label sequence (y \u2032), which is produced by inserting empty symbols, in order to allow blanks in the process, i.e."}, {"heading": "2.2. Attention-based encoder-decoder", "text": "In contrast to the CTC approach, the attention model directly predicts any objective without requiring an intermediate representation or any assumptions, and improves the CER compared to CTC when no external language model is used. [7] The model emits any label distribution based on the previous designations according to the following recursive equations: P (y-x) = 1 P (yu-x, y1: u \u2212 1) (5) h = Encoder (x) (6) yu-1: u \u2212 1). (7) The framework consists of two RNs: Encoder and AttentionDecoder, so that it is able to learn two different lengths of sequences based on the cross-entropy criteria. Encoder transforms x, to high-level representation h = (h1, \u00b7 \u00b7 hL) in Eq. (6), then AttentionDecoder produces the probability distribution across characters, yu, conditional and all characters seen before."}, {"heading": "2.3. Proposed model: Joint CTC-attention (MTL)", "text": "The idea of our model is to use a CTC objective function as an auxiliary task to train the encoder of the attention model within the Multitask Learning (MTL) framework. Figure 1 illustrates the general ar architecture of our framework, in which the encoder network is shared with CTC and attention models. In contrast to the attention model, the CTC forward-backward algorithm can force a monotonous alignment between speech and label sequences. Therefore, we expect our framework to be more robust when it comes to achieving appropriate alignments under loud conditions. Another advantage of the CTC as an auxiliary task is that the network is learned quickly. In our experiments, we do not rely exclusively on data-driven attention methods to estimate the desired alignments in long sequences, but the forward-backward algorithm in the CTC helps to accelerate the process of estimating the desired alignment manually (the objective is suggested by CTC, which requires the alignment of both \u2212 1)."}, {"heading": "3. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Data", "text": "We conducted three experiments: two on a pure speech corpus, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a loud speech corpus, CHiME-4 (18 hours) [16]. The CHiME-4 corpus was recorded with a tablet device in everyday environments - a cafe, an intersection, public transportation, and a pedestrian zone. As input characteristics, we used 40 first and second-order filter bank coefficients with their temporal derivatives to obtain a total of 120 characteristic values per image. Evaluation was done on (1) \"eval92\" for WSJ and (2) \"et05\" true isolated 1ch-lane \"for CHiME-4. Hyperparameter selection was made on (1)\" dev93 \"for WSJ and (2)\" dt05 multiisolated 1ch-lane \"for CHiME-4. None of our experiments used any language model or the Icon, the Econ, the space, and the keywords only."}, {"heading": "3.2. Training and Decoding", "text": "Our model used four layers of 320 bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder and a layer of 320 LSTM in the decoder. The top two layers read every second of hidden states in the network below, which increases the length of the utterance by a factor of 4, L = T / 4. In the site-based attention model, ten centred folding filters of width 100 were used to extract the features from the previous step alignment. We use the sharpening factor \u03b3 = 2. Each linear projection layer follows the BLSTM layer. For optimization, the AdaDelta algorithm [19] with gradient cutout [20] was used. All weights are initialized with the range [-0.1, 0.1] of the uniform distribution. For our MTL, we tested three different task weights, namely: 0.2, 0.5 and 0.8. For decoding, we used a search algorithm similar to the one of the size of the hyper [21-1]."}, {"heading": "3.3. Results", "text": "The results in Table 1 show that our proposed model MTL performs significantly better in both the CTC and the attention model in CER for both the loud CHiME-4 and the clean WSJ tasks. Our model showed 7.0 - 9.5% and 6.6 - 10.3% relative improvements in validation and evaluation, respectively. As we expected, the attention model performed relatively poorly in the loud CHiME-4 tasks compared to the clean WSJ tasks. We observed that the benefit from our shared CTC attention increased in the noisy state, and when greater weights in the CTC losses (i.e. = 0.8) achieved the best performance in CHiME-4, while \u03bb = 0.5 showed the best performance in clean WSJ. It is noteworthy that our framework used both the CTC model and the attention model even in the clean companies WSJ1 and WSJ0 performed significantly better in the CHiME-4 tasks, while \u03bb = 0.5 showed the best performance in clean WSJ. It is also noticeable that the CTC model as well as the attention model even in the clean companies WSJ1 and WSJ0 did not perform well in the CTC Gold."}, {"heading": "4. CONCLUSIONS", "text": "We have introduced a novel, generic end-to-end speech recognition method based on the multi-task learning approach with the CTC and the Attention Encoder Decoder. Our method improves performance by training a common encoder that uses an additional CTC lens function. In addition, it significantly speeds up the process of learning the desired alignment without limiting the range of input manually, even in longer sequences. Our method has outperformed both CTC and an attention model in terms of a speech recognition task in both real loud conditions and clean conditions, and this work can potentially be applied to any sequence-to-sequence learning framework."}, {"heading": "5. REFERENCES", "text": "[1] Alex Graves and Navdeep Jaitly and Understanding (ASRU), \"Towards end-to-end speech recognition with recurrent neural networks,\" in Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764-1772. [2] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \"Deep speech: Scaling up endto-end speech recognition,\" arXiv preprint arXiv: 1412.5567, 2014. [3] Yajie Miao, Mohammad Gowayyed, and Florian Metze \"Eesen: End-to-end speech recognition using deep rnrnn models and wfst-based decoding,\" in 2015 IEEneE Workshop on Automatic Speech Recognition and Understanding (ASRU)."}], "references": [{"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764\u20131772.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech: Scaling up endto-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Eesen: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Yajie Miao", "Mohammad Gowayyed", "Florian Metze"], "venue": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 167\u2013174.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: First results", "author": ["Jan Chorowski", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.1602, 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Attention-based models for speech recognition", "author": ["Jan K Chorowski", "Dzmitry Bahdanau", "Dmitriy Serdyuk", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 577\u2013585.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Listen, attend and spell", "author": ["William Chan", "Navdeep Jaitly", "Quoc V Le", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1508.01211, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "End-to-end attentionbased large vocabulary speech recognition", "author": ["Dzmitry Bahdanau", "Jan Chorowski", "Dmitriy Serdyuk", "Philemon Brakel", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1508.04395, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "On training the recurrent neural network encoder-decoder for large vocabulary end-to-end speech recognition", "author": ["Liang Lu", "Xingxing Zhang", "Steve Renais"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5060\u20135064.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "On online attention-based speech recognition and joint mandarin character-pinyin training", "author": ["William Chan", "Ian Lane"], "venue": "Interspeech 2016, pp. 3404\u20133408, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Acoustic modeling using deep belief networks", "author": ["Abdel-rahman Mohamed", "George E Dahl", "Geoffrey Hinton"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 14\u201322, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["Geoffrey Hinton", "Li Deng", "Dong Yu", "George E Dahl", "Abdelrahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara N Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks", "author": ["Alex Graves", "Santiago Fern\u00e1ndez", "Faustino Gomez", "J\u00fcrgen Schmidhuber"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 369\u2013376.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473, 2014.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Csr-ii (wsj1) complete", "author": ["Linguistic Data Consortium"], "venue": "Linguistic Data Consortium, Philadelphia, vol. LDC94S13A, 1994.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1994}, {"title": "Csr-i (wsj0) complete", "author": ["John Garofalo", "David Graff", "Doug Paul", "David Pallett"], "venue": "Linguistic Data Consortium, Philadelphia, vol. LDC93S6A, 2007.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "An analysis of environment, microphone and data simulation mismatches in robust speech recognition", "author": ["Emmanuel Vincent", "Shinji Watanabe", "Aditya Arie Nugraha", "Jon Barker", "Ricard Marxer"], "venue": "Computer Speech and Language, to appear.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1997}, {"title": "Hybrid speech recognition with deep bidirectional lstm", "author": ["Alan Graves", "Navdeep Jaitly", "Abdel-rahman Mohamed"], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 2013, pp. 273\u2013278.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1211.5063, 2012.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc VV Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u20133112.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Chainer: a next-generation open source framework for deep learning", "author": ["Seiya Tokui", "Kenta Oono", "Shohei Hido", "Justin Clayton"], "venue": "Proceedings of Workshop on Machine Learning Systems (LearningSys) in The Twenty-ninth Annual Conference on Neural Information Processing Systems (NIPS), 2015.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Chainer", "author": ["Preferred Networks"], "venue": "\u201dhttp://chainer.org/\u201d.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 1, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 2, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 3, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 4, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 5, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 6, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 7, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 8, "context": "End-to-end speech recognition is a recently proposed approach that directly transcribes speech to text without requiring predefined alignment between acoustic frames and characters [1, 2, 3, 4, 5, 6, 7, 8, 9].", "startOffset": 181, "endOffset": 208}, {"referenceID": 9, "context": "acoustic model, contextdependent phone model, pronunciation model, and language model) based on conditional independence assumptions (including Markov assumptions) and approximations [10, 11].", "startOffset": 183, "endOffset": 191}, {"referenceID": 10, "context": "acoustic model, contextdependent phone model, pronunciation model, and language model) based on conditional independence assumptions (including Markov assumptions) and approximations [10, 11].", "startOffset": 183, "endOffset": 191}, {"referenceID": 11, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 134, "endOffset": 147}, {"referenceID": 0, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 134, "endOffset": 147}, {"referenceID": 1, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 134, "endOffset": 147}, {"referenceID": 2, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 134, "endOffset": 147}, {"referenceID": 12, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 184, "endOffset": 197}, {"referenceID": 3, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 184, "endOffset": 197}, {"referenceID": 4, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 184, "endOffset": 197}, {"referenceID": 5, "context": "Recent work on end-to-end speech recognition can be categorized into two main approaches: Connectionist Temporal Classification (CTC) [12, 1, 2, 3] and attention-based encoder-decoder [13, 4, 5, 6].", "startOffset": 184, "endOffset": 197}, {"referenceID": 6, "context": "Since the attention model does not use any conditional independence assumption, it has often shown to improve Character Error Rate (CER) than CTC when no external language model is used [7].", "startOffset": 186, "endOffset": 189}, {"referenceID": 6, "context": "Another issue is that the model is hard to be learned from scratch due to the misalignment on longer input sequences, and therefore a windowing technique is commonly used to limit the area explored by the attention mechanism [7], but several parameters for windowing need to be determined manually depending on the training data.", "startOffset": 225, "endOffset": 228}, {"referenceID": 11, "context": "The key idea of CTC [12] is to use intermediate label representation \u03c0 = (\u03c01, \u00b7 \u00b7 \u00b7 , \u03c0T ), allowing repetitions of labels and occurrences of a blank label (\u2212), which represents the special emission without labels, i.", "startOffset": 20, "endOffset": 24}, {"referenceID": 1, "context": "Therefore, lexicon or language models are commonly incorporated, like the hybrid framework [2, 3].", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "Therefore, lexicon or language models are commonly incorporated, like the hybrid framework [2, 3].", "startOffset": 91, "endOffset": 97}, {"referenceID": 6, "context": "Unlike the CTC approach, the attention model directly predicts each target without requiring intermediate representation or any assumptions, improving CER as compared to CTC when no external language model is used [7].", "startOffset": 214, "endOffset": 217}, {"referenceID": 4, "context": "where w,W, V, F, U, b are trainable parameters, su\u22121 is the decoder state, \u03b3 is the sharpening factor [5], and * denotes convolution.", "startOffset": 102, "endOffset": 105}, {"referenceID": 4, "context": "au can be computed by the softmax of energy eu,l from two types of attention mechanisms: content-based and location-based [5] in Eq.", "startOffset": 122, "endOffset": 125}, {"referenceID": 4, "context": "To make training faster, the author [5, 7] constrains the attention mechanism to only consider inputs within a narrow range.", "startOffset": 36, "endOffset": 42}, {"referenceID": 6, "context": "To make training faster, the author [5, 7] constrains the attention mechanism to only consider inputs within a narrow range.", "startOffset": 36, "endOffset": 42}, {"referenceID": 13, "context": "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].", "startOffset": 105, "endOffset": 113}, {"referenceID": 14, "context": "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].", "startOffset": 105, "endOffset": 113}, {"referenceID": 15, "context": "We performed three sets of experiments: two on clean speech corpora, WSJ1 (81 hours) and WSJ0 (15 hours) [14, 15], and one on a noisy speech corpus, CHiME-4 (18 hours) [16].", "startOffset": 168, "endOffset": 172}, {"referenceID": 16, "context": "Our model used four layers of 320 Bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder, and one layer of 320 LSTM in the decoder.", "startOffset": 88, "endOffset": 96}, {"referenceID": 17, "context": "Our model used four layers of 320 Bidirectional Long Short-Term Memory Networks (BLSTM) [17, 18] in the encoder, and one layer of 320 LSTM in the decoder.", "startOffset": 88, "endOffset": 96}, {"referenceID": 18, "context": "The AdaDelta algorithm [19] with gradient clipping [20] was used for optimization.", "startOffset": 23, "endOffset": 27}, {"referenceID": 19, "context": "The AdaDelta algorithm [19] with gradient clipping [20] was used for optimization.", "startOffset": 51, "endOffset": 55}, {"referenceID": 20, "context": "For decoding, we used a beam search algorithm similar to [21] with the beam size 20 to reduce the computation cost.", "startOffset": 57, "endOffset": 61}, {"referenceID": 21, "context": "Our framework is implemented with the Chainer library [22, 23].", "startOffset": 54, "endOffset": 62}, {"referenceID": 22, "context": "Our framework is implemented with the Chainer library [22, 23].", "startOffset": 54, "endOffset": 62}], "year": 2016, "abstractText": "Recently, there has been an increasing interest in end-to-end speech recognition that directly transcribes speech to text without any predefined alignments. One approach is the attention-based encoderdecoder framework that learns a mapping between variable-length input and output sequences in one step using a purely data-driven method. The attention model has often been shown to improve the performance over another end-to-end approach, the Connectionist Temporal Classification (CTC), mainly because it explicitly uses the history of the target character without any conditional independence assumptions. However, we observed that the attention model has shown poor results especially in noisy condition and is hard to be trained in the initial training stage with long input sequences, as compared with CTC. This is because the attention model is too flexible to predict proper alignments in such cases due to the lack of left-toright constraints as used in CTC. This paper presents a novel method for end-to-end speech recognition to improve robustness and achieve fast convergence by using a joint CTC-attention model within the multi-task learning framework, thereby mitigating the alignment issue. An experiment on the WSJ and CHiME-4 tasks demonstrates its advantages over both the CTC and attention-based encoder-decoder baselines, showing 6.6-10.3% relative improvements in Character Error Rate (CER).", "creator": "LaTeX with hyperref package"}}}