{"id": "1609.06791", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling", "abstract": "Twitter data is extremely noisy -- each tweet is short, unstructured and with informal language, a challenge for current topic modeling. On the other hand, tweets are accompanied by extra information such as authorship, hashtags and the user-follower network. Exploiting this additional information, we propose the Twitter-Network (TN) topic model to jointly model the text and the social network in a full Bayesian nonparametric way. The TN topic model employs the hierarchical Poisson-Dirichlet processes (PDP) for text modeling and a Gaussian process random function model for social network modeling. We show that the TN topic model significantly outperforms several existing nonparametric models due to its flexibility. Moreover, the TN topic model enables additional informative inference such as authors' interests, hashtag analysis, as well as leading to further applications such as author recommendation, automatic topic labeling and hashtag suggestion. Note our general inference framework can readily be applied to other topic models with embedded PDP nodes.", "histories": [["v1", "Thu, 22 Sep 2016 01:08:31 GMT  (397kb,D)", "http://arxiv.org/abs/1609.06791v1", "NIPS workshop paper"]], "COMMENTS": "NIPS workshop paper", "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.SI", "authors": ["kar wai lim", "changyou chen", "wray buntine"], "accepted": false, "id": "1609.06791"}, "pdf": {"name": "1609.06791.pdf", "metadata": {"source": "CRF", "title": "Twitter-Network Topic Model: A Full Bayesian Treatment for Social Network and Text Modeling", "authors": ["Kar Wai Lim", "Changyou Chen"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This user-generated information is generally more personal, informal and often contains personal opinions. Overall, it can be useful for reputation analysis of companies and products, detection of natural disasters, receiving first-hand news, or even demographic analysis. Twitter, an easily accessible source of information, allows users to express their opinions and thoughts in short texts known as tweets. Latent Dirichlet topic (Blei et al., 2003) is a popular form of the topic model. Unfortunately, applying LDA directly to tweets leads to poor results, as tweets are short and often noisy (Zhao et al., 2011), i.e. tweets are unstructured and often contain grammatical and spelling errors, as well as informal words such as user-defined abbreviations used due to the 140-character limit."}, {"heading": "2 Background and Related Work", "text": "s DP theme (Rosen-Zvi et al., 2004), the tag-theme model (Tsai, 2011), and the topic-link LDA (Liu et al., 2009). However, these models only handle one type of additional information and do not work well with tweets as they are designed for other types of text data. Note that the tag-theme model treats tags as hard labels and uses them to group text documents, which is not suitable for tweets due to the noisy nature of hashtags. Twitter-LDA (Zhao et al., 2011) and the behavior-theme model (Qiu et al., 2013) were designed for explicitly model tweets. Both models are not blending models as they restrict one topic per document."}, {"heading": "3 Model Summary", "text": "The TN topic model consists of two main components: an HPDP topic0 500 1000 1500 2000- 2400 000 -220 0000 -200 0000iterationlo g-lik elih oodTN ATMHDP-LDAFigure 2: Log likelihood vs. iteration model for the text and the hashtags, and a GP-based random function model for the follower topic. Author information is used to link the two. We design our HPDP topic model for text as follows: First, we create the global topic distribution \u00b50, which serves as the previous one; then, we create the topic distributions for each author and a mixed topic distribution model \u00b51 to capture topics that differ from the usual topics of the authors."}, {"heading": "4 Posterior Inference", "text": "We derive a collapsed Gibbs sampler for the theme model and a Metropolis-Hastings (MH) algorithm for the network model. We develop a framework for performing collapsing Gibbs sampling in general on any Bavarian network of PDPs, building on the work of (Buntine et al., 2010; Chen et al., 2011), which allows rapid prototyping and the development of new variants of the theme model. For technical details, we refer readers to complementary materials."}, {"heading": "5 Experiments and Applications", "text": "We evaluate the TN topic model quantitatively using standard topic models such as test set perplexity, probability convergence, and clustering metrics. We evaluate the model qualitatively by evaluating the topic summaries, authors \"topic distributions, and by performing an automatic labeling task. We compare our model with HDP-LDA, a non-parametric variant of the author theme (ATM), and the original random functional network model. We also conduct ablation studies to show the importance of each component in the model. Results of the comparison and ablation studies are shown in Table 1. We use two tweets corpus for experiments, first of all is a subset of Twitter7 dataset2 (Yang and Leskovec, 2011), which is achieved by queries with specific key words (e.g. finance, sports, politics). We remove tweets that are not English with langid.py (Lutter7 and Baldwin, 2012, and Baldwin, and have no filters)."}, {"heading": "6 Conclusion and Future Work", "text": "We propose a full Bayesian non-parametric Twitter Network (TN) theme model that models tweets and related social network information together; our model uses a non-parametric Bayesian approach, using the PDP and GP; and achieves flexible modeling by drawing conclusions about the network of PDPs; our experiments with Twitter data sets show that the TN theme model achieves significant improvements over existing baselines; in addition, our ablation study shows the usefulness of each component of the TN model; our model also shows interesting applications such as author recommendations and additional informative conclusions; we also developed a framework for rapid theme model development, which is important because of the complexity of the model; while we could have used Adaptor Grammars (Johnson et al., 2007), our framework provides more efficient calculations for theme models, which includes speeding up the post-linking theme, particularly for the use of other social media algorithms, as well as the augmentation algorithms."}, {"heading": "Acknowledgement", "text": "We would like to thank the anonymous reviewers for their helpful feedback and comments. NICTA is funded by the Australian government through the Ministry of Communications and the Australian Research Council through the ICT Centre of Excellence Program."}], "references": [{"title": "How noisy social media text, how diffrnt social media sources? IJCNLP", "author": ["T. Baldwin", "P. Cook", "M. Lui", "A. MacKinlay", "L. Wang"], "venue": null, "citeRegEx": "Baldwin et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Baldwin et al\\.", "year": 2013}, {"title": "Latent Dirichlet Allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "the Journal of machine Learning research, 3:993\u20131022.", "citeRegEx": "Blei et al\\.,? 2003", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Bayesian networks on dirichlet distributed vectors", "author": ["W. Buntine", "L. Du", "P. Nurmi"], "venue": "pages 33\u201340.", "citeRegEx": "Buntine et al\\.,? 2010", "shortCiteRegEx": "Buntine et al\\.", "year": 2010}, {"title": "Hierarchical relational models for document networks", "author": ["J. Chang", "D.M. Blei"], "venue": "The Annals of Applied Statistics, 4(1):124\u2013150.", "citeRegEx": "Chang and Blei,? 2010", "shortCiteRegEx": "Chang and Blei", "year": 2010}, {"title": "Sampling table configurations for the hierarchical Poisson-Dirichlet process", "author": ["C. Chen", "L. Du", "W. Buntine"], "venue": "Machine Learning and Knowledge Discovery in Databases, pages 296\u2013311. Springer.", "citeRegEx": "Chen et al\\.,? 2011", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "What to do about bad language on the internet", "author": ["J. Eisenstein"], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Atlanta, Georgia. Association for Computational Linguistics.", "citeRegEx": "Eisenstein,? 2013", "shortCiteRegEx": "Eisenstein", "year": 2013}, {"title": "Interpolating between types and tokens by estimating power-law generators", "author": ["S. Goldwater", "T. Griffiths", "M. Johnson"], "venue": "Advances in neural information processing systems, 18:459.", "citeRegEx": "Goldwater et al\\.,? 2006", "shortCiteRegEx": "Goldwater et al\\.", "year": 2006}, {"title": "Adaptor grammars: A framework for specifying compositional nonparametric Bayesian models", "author": ["M. Johnson", "T.L. Griffiths", "S. Goldwater"], "venue": "Advances in neural information processing systems, 19:641.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Topic-link LDA: joint models of topic and author community", "author": ["Y. Liu", "A. Niculescu-Mizil", "W. Gryc"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning, pages 665\u2013672. ACM.", "citeRegEx": "Liu et al\\.,? 2009", "shortCiteRegEx": "Liu et al\\.", "year": 2009}, {"title": "Random function priors for exchangeable arrays with applications to graphs and relational data", "author": ["J. Lloyd", "P. Orbanz", "Z. Ghahramani", "D. Roy"], "venue": "Advances in Neural Information Processing Systems 25, pages 1007\u20131015.", "citeRegEx": "Lloyd et al\\.,? 2012", "shortCiteRegEx": "Lloyd et al\\.", "year": 2012}, {"title": "langid.py: An off-the-shelf language identification tool", "author": ["M. Lui", "T. Baldwin"], "venue": "In Proceedings of the ACL 2012 System Demonstrations,", "citeRegEx": "Lui and Baldwin,? \\Q2012\\E", "shortCiteRegEx": "Lui and Baldwin", "year": 2012}, {"title": "Improving LDA topic models for microblogs via tweet pooling and automatic labeling", "author": ["R. Mehrotra", "S. Sanner", "W. Buntine", "L. Xie"], "venue": "The 36th Annual ACM SIGIR Conference, page 4, Dublin/Ireland.", "citeRegEx": "Mehrotra et al\\.,? 2013", "shortCiteRegEx": "Mehrotra et al\\.", "year": 2013}, {"title": "Nonparametric latent feature models for link prediction", "author": ["K. Miller", "M.I. Jordan", "T.L. Griffiths"], "venue": "Advances in neural information processing systems, pages 1276\u20131284.", "citeRegEx": "Miller et al\\.,? 2009", "shortCiteRegEx": "Miller et al\\.", "year": 2009}, {"title": "Joint latent topic models for text and citations", "author": ["R. Nallapati", "A. Ahmed", "E.P. Xing"], "venue": "KDD.", "citeRegEx": "Nallapati et al\\.,? 2008", "shortCiteRegEx": "Nallapati et al\\.", "year": 2008}, {"title": "It is not just what we say, but how we say them: Lda-based behavior-topic model", "author": ["M. Qiu", "F. Zhu", "J. Jiang"], "venue": "2013 SIAM International Conference on Data Mining (SDM\u201913).", "citeRegEx": "Qiu et al\\.,? 2013", "shortCiteRegEx": "Qiu et al\\.", "year": 2013}, {"title": "The author-topic model for authors and documents", "author": ["M. Rosen-Zvi", "T. Griffiths", "M. Steyvers", "P. Smyth"], "venue": "Proceedings of the 20th conference on Uncertainty in artificial intelligence, pages 487\u2013494. AUAI Press.", "citeRegEx": "Rosen.Zvi et al\\.,? 2004", "shortCiteRegEx": "Rosen.Zvi et al\\.", "year": 2004}, {"title": "Topic models with power-law using pitman-yor process", "author": ["I. Sato", "H. Nakagawa"], "venue": "Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD \u201910, pages 673\u2013682, New York, NY, USA. ACM.", "citeRegEx": "Sato and Nakagawa,? 2010", "shortCiteRegEx": "Sato and Nakagawa", "year": 2010}, {"title": "Hierarchical Bayesian nonparametric models with applications", "author": ["Y.W. Teh", "M.I. Jordan"], "venue": "Bayesian Nonparametrics: Principles and Practice. Cambridge University Press.", "citeRegEx": "Teh and Jordan,? 2010", "shortCiteRegEx": "Teh and Jordan", "year": 2010}, {"title": "A tag-topic model for blog mining", "author": ["F.S. Tsai"], "venue": "Expert Systems with Applications, 38(5):5330\u20135335.", "citeRegEx": "Tsai,? 2011", "shortCiteRegEx": "Tsai", "year": 2011}, {"title": "A biterm topic model for short texts", "author": ["X. Yan", "J. Guo", "Y. Lan", "X. Cheng"], "venue": "Proceedings of the 22nd international conference on World Wide Web, WWW \u201913, pages 1445\u20131456, Republic and Canton of Geneva, Switzerland. International World Wide Web Conferences Steering Committee.", "citeRegEx": "Yan et al\\.,? 2013", "shortCiteRegEx": "Yan et al\\.", "year": 2013}, {"title": "Patterns of temporal variation in online media", "author": ["J. Yang", "J. Leskovec"], "venue": "Proceedings of the fourth ACM international conference on Web search and data mining, pages 177\u2013186. ACM.", "citeRegEx": "Yang and Leskovec,? 2011", "shortCiteRegEx": "Yang and Leskovec", "year": 2011}, {"title": "Comparing twitter and traditional media using topic models", "author": ["W.X. Zhao", "J. Jiang", "J. Weng", "J. He", "Lim", "E.-P.", "H. Yan", "X. Li"], "venue": "Proceedings of the 33rd European conference on Advances in information retrieval, ECIR\u201911, pages 338\u2013349, Berlin, Heidelberg. Springer-Verlag.", "citeRegEx": "Zhao et al\\.,? 2011", "shortCiteRegEx": "Zhao et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 1, "context": "Latent Dirichlet allocation (LDA) (Blei et al., 2003) is a popular form of topic model.", "startOffset": 34, "endOffset": 53}, {"referenceID": 21, "context": "Unfortunately, a direct application of LDA on tweets yields poor result as tweets are short and often noisy (Zhao et al., 2011), i.", "startOffset": 108, "endOffset": 127}, {"referenceID": 0, "context": "In other text analysis applications, tweets are often \u2018cleansed\u2019 by NLP methods such as lexical normalization (Baldwin et al., 2013).", "startOffset": 110, "endOffset": 132}, {"referenceID": 5, "context": "However, the use of normalization is also criticized (Eisenstein, 2013).", "startOffset": 53, "endOffset": 71}, {"referenceID": 15, "context": "LDA is often extended for different types of data, some notable examples that use auxiliary information are the author-topic model (Rosen-Zvi et al., 2004), the tag-topic model (Tsai, 2011), and Topic-Link LDA (Liu et al.", "startOffset": 131, "endOffset": 155}, {"referenceID": 18, "context": ", 2004), the tag-topic model (Tsai, 2011), and Topic-Link LDA (Liu et al.", "startOffset": 29, "endOffset": 41}, {"referenceID": 8, "context": ", 2004), the tag-topic model (Tsai, 2011), and Topic-Link LDA (Liu et al., 2009).", "startOffset": 62, "endOffset": 80}, {"referenceID": 21, "context": "Twitter-LDA (Zhao et al., 2011) and the behavior-topic model (Qiu et al.", "startOffset": 12, "endOffset": 31}, {"referenceID": 14, "context": ", 2011) and the behavior-topic model (Qiu et al., 2013) were designed to explicitly model tweets.", "startOffset": 37, "endOffset": 55}, {"referenceID": 19, "context": "On the other hand, the biterm topic model (Yan et al., 2013) uses only the biterm co-occurrence to model tweets, discarding document level information.", "startOffset": 42, "endOffset": 60}, {"referenceID": 17, "context": "To sidestep the need of choosing the number of topics, (Teh and Jordan, 2010) proposed Hierarchical Dirichlet process (HDP) LDA, which utilizes the Dirichlet process (DP) as nonparametric prior.", "startOffset": 55, "endOffset": 77}, {"referenceID": 6, "context": "In natural languages, the distribution of word frequencies exhibits a power-law (Goldwater et al., 2006).", "startOffset": 80, "endOffset": 104}, {"referenceID": 16, "context": "For topic models, replacing the Dirichlet distribution with the PDP can yield great improvement (Sato and Nakagawa, 2010).", "startOffset": 96, "endOffset": 121}, {"referenceID": 8, "context": "Some recent work models text data with network information ((Liu et al., 2009; Chang and Blei, 2010; Nallapati et al., 2008)), however, these models are parametric in nature and can be restrictive.", "startOffset": 60, "endOffset": 124}, {"referenceID": 3, "context": "Some recent work models text data with network information ((Liu et al., 2009; Chang and Blei, 2010; Nallapati et al., 2008)), however, these models are parametric in nature and can be restrictive.", "startOffset": 60, "endOffset": 124}, {"referenceID": 13, "context": "Some recent work models text data with network information ((Liu et al., 2009; Chang and Blei, 2010; Nallapati et al., 2008)), however, these models are parametric in nature and can be restrictive.", "startOffset": 60, "endOffset": 124}, {"referenceID": 12, "context": "(Miller et al., 2009) and Lloyd et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "(Lloyd et al., 2012) model network data directly with nonparametric priors, i.", "startOffset": 0, "endOffset": 20}, {"referenceID": 9, "context": "We emphasize that our treatment of the network model is different to that of (Lloyd et al., 2012).", "startOffset": 77, "endOffset": 97}, {"referenceID": 2, "context": "We develop a framework to perform collapse Gibbs sampling generally on any Bayesian network of PDPs, built upon the work of (Buntine et al., 2010; Chen et al., 2011), which allows quick prototyping and development of new variants of topic model.", "startOffset": 124, "endOffset": 165}, {"referenceID": 4, "context": "We develop a framework to perform collapse Gibbs sampling generally on any Bayesian network of PDPs, built upon the work of (Buntine et al., 2010; Chen et al., 2011), which allows quick prototyping and development of new variants of topic model.", "startOffset": 124, "endOffset": 165}, {"referenceID": 20, "context": "We use two tweets corpus for experiments, first is a subset of Twitter7 dataset2 (Yang and Leskovec, 2011), obtained by querying with certain keywords (e.", "startOffset": 81, "endOffset": 106}, {"referenceID": 10, "context": "py (Lui and Baldwin, 2012) and filter authors who do not have network information and who authored less than 100 tweets.", "startOffset": 3, "endOffset": 26}, {"referenceID": 11, "context": "Second tweets corpus is obtained from (Mehrotra et al., 2013), which contains a total of 781186 tweets.", "startOffset": 38, "endOffset": 61}, {"referenceID": 9, "context": "We compare our new kernel function with the original kernel function (denoted as original) used in (Lloyd et al., 2012).", "startOffset": 99, "endOffset": 119}, {"referenceID": 11, "context": "Clustering and Topic Coherence We also evaluate the TN topic model against state-of-the-art LDA-based clustering techniques (Mehrotra et al., 2013).", "startOffset": 124, "endOffset": 147}, {"referenceID": 7, "context": "While we could have used Adaptor Grammars (Johnson et al., 2007), our framework yields more efficient computation for topic models.", "startOffset": 42, "endOffset": 64}], "year": 2016, "abstractText": "Twitter data is extremely noisy \u2013 each tweet is short, unstructured and with informal language, a challenge for current topic modeling. On the other hand, tweets are accompanied by extra information such as authorship, hashtags and the user-follower network. Exploiting this additional information, we propose the Twitter-Network (TN) topic model to jointly model the text and the social network in a full Bayesian nonparametric way. The TN topic model employs the hierarchical Poisson-Dirichlet processes (PDP) for text modeling and a Gaussian process random function model for social network modeling. We show that the TN topic model significantly outperforms several existing nonparametric models due to its flexibility. Moreover, the TN topic model enables additional informative inference such as authors\u2019 interests, hashtag analysis, as well as leading to further applications such as author recommendation, automatic topic labeling and hashtag suggestion. Note our general inference framework can readily be applied to other topic models with embedded PDP nodes.", "creator": "LaTeX with hyperref package"}}}