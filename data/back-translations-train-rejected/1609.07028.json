{"id": "1609.07028", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Image-embodied Knowledge Representation Learning", "abstract": "Entity images provide significant visual information that helps the construction of knowledge representations. Most conventional methods learn knowledge representations solely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model, where knowledge representations are learned with both triples and images. More specifically, for each image of an entity, we construct image-based representations via a neural image encoder, and these representations with respect to multiple image instances are then integrated via an attention-based method. We evaluate our models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.", "histories": [["v1", "Thu, 22 Sep 2016 15:37:45 GMT  (348kb,D)", "http://arxiv.org/abs/1609.07028v1", "7 pages"], ["v2", "Mon, 22 May 2017 08:14:27 GMT  (343kb,D)", "http://arxiv.org/abs/1609.07028v2", "7 pages; Accepted by IJCAI-2017"]], "COMMENTS": "7 pages", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["ruobing xie", "zhiyuan liu", "huanbo luan", "maosong sun"], "accepted": false, "id": "1609.07028"}, "pdf": {"name": "1609.07028.pdf", "metadata": {"source": "CRF", "title": "Image-embodied Knowledge Representation Learning", "authors": ["Ruobing Xie", "Zhiyuan Liu", "Tat-seng Chua", "Huanbo Luan", "Maosong Sun"], "emails": ["(liuzy@tsinghua.edu.cn)"], "sections": [{"heading": "1 Introduction", "text": "Knowledge diagrams (KGs), which provide an enormous amount of structured information for entities and relationships, focus only on several structural information that have been successfully used in various areas such as knowledge reasoning [Yang et al., 2014] and question answering [Yin et al., 2016]. To model knowledge diagrams, translation-based methods are proposed that project both entities and relationships into a continuously low semantic space, with relationships considered as translation operations between head and tail entities [Bordes et al., 2013]. These methods could leverage both effectiveness and efficiency in knowledge representation by representing learning (CRL) and thus attracting much attention."}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Translation-based Methods", "text": "Translation-based methods have been great success on knowledge representation learning in recent years. TransE [Bordes et al., 2013] models both entities and relations into the same low-dimensional continuous vector space, with relations as translating operations between head and tail entities. TransE's basic assumption is that the embedding of tail entity t is the neighbor of h + r. TransE's energy function is defined as follows: E (h, r, t) = | h + r \u2212 t |. (1) TransE is both effective and efficient, while simple adoption in modeling complicated entities and relationships can lead to conflicts. To solve this problem, TransH [Wang et al., 2014b] proposes relationship-specific hyperplanes for translations between entities. TransR [Lin et al., 2015] models entities and relations in different vector spaces, projecting entities from entity space to relation specific matrices."}, {"heading": "2.2 Multi-source information Learning", "text": "To use rich textual information, [Wang et al., 2014a] projects both entities and words into a common vector space with alignment models. [Xie et al., 2016] directly constructs entity representations from entity descriptions that are able to model new entities. In terms of visual information, multimodal representations based on words and images are widely used for various tasks such as image-sentence ranking [Kiros et al., 2014] and metaphor identification [Shutova et al., 2016]."}, {"heading": "3 Methodology", "text": "We will first present the notations used in this essay. Given to a triple (h, r, t), it consists of two units h, t, E and a relation r, R. T stands for the entire training group of the triples, E stands for the entirety of the units and R for the entirety of the relationships. Each unit and relation embedding receives a value in Rds. To use the image information of the units, we propose for each unit two types of representations inspired by [Xie et al., 2016]. We use hS, tS as structure-based representations of head and tail units, which are the distributed representations learned through conventional knowledge models. We also propose a novel type of knowledge representations hI, tI as image-based representations constructed from the corresponding images of both units."}, {"heading": "3.1 Overall Architecture", "text": "Using translation-based methods, we define the total energy function as follows: E (h, r, t) = ESS + ESI + EIS + EII. (2) The total energy function is determined jointly by the two types of entity representations. ESS = | hS + r \u2212 tS | | is the same energy function as TransE, which only depends on the structure-based representations. EII = | | hI + r \u2212 tI | | is the energy function in which both head and tail entities are image-based representations learned from their respective images. We also have ESI = | | hS + r \u2212 tI | | and EIS = | hI + r \u2212 tS | | to ensure that both structure-based representations and image-based representations are incorporated into the same vector space."}, {"heading": "3.2 Image Encoder", "text": "Images provide informative visual information that can intuitively describe the appearance and behavior of units considered basic input data of the ICRL model. For each unit ek, there are several image instances represented as Ik = {img (k) 1, img (k) 2, \u00b7 \u00b7, img (k) n}. To effectively encode image information in knowledge representations, we propose an image encoder consisting of an image representation module and an image projection module. The image representation module uses neural networks to extract discriminatory features in images and constructs image attributes for each image. Next, the image projection module attempts to project these image attributes from the image space into the entity space. Fig. 3 shows the entire pipeline of the image encoder."}, {"heading": "Image Representation Module", "text": "We use AlexNet, a deep neural network that contains five folding layers, two fully connected layers, and a softmax layer to extract image features [Krizhevsky et al., 2012]. During pre-processing, all images from the center, corners, and their horizontal reflections are recast to 224 \u00d7 224. Inspired by [Shutova et al., 2016], we use the 4096-dimensional embedding exits of the second fully connected layer (also known as fc7) as image feature representations."}, {"heading": "Image Projection Module", "text": "After obtaining the compressed feature representations for each image, the next step is to build bridges between images and entities using the image projection module. Specifically, we transfer the image feature representations from the image space to the entity space using a common projection matrix. The image-based representation pi in the entity space for the i-th image is defined as: pi = M \u00b7 f (imgi), (3) in which M-Rdi \u00d7 ds is the projection matrix, di represents the dimension of the image characteristics, while ds represents the dimension of the entities. f (imgi) stands for the i-th image feature representation in the image space constructed by the image representation module."}, {"heading": "3.3 Attention-based Multi-instance Leaning", "text": "The image encoder takes images as inputs and then constructs image-based representations for each individual image. However, most entities have more than one image from different aspects in different scenarios. It is important, but also difficult, to determine which images are better to represent their respective entities. A simple summary of all image representations can suffer from either noise or loss of information. Instead, to construct the aggregated image-based representation for each entity from multiple instances, we propose an attention-based learning method. Attention-based methods prove to be intelligent for automatically selecting informative instances from multiple candidates. It has been used in various areas such as the Asimage classification [Mnih et al., 2014], machine translation [Bahdanau et al., 2015] and abstract sentence summary [Rush et al., 2015] to automatically select informative instances from multiple candidates."}, {"heading": "3.4 Objective Formalization", "text": "As a training target, we use a margin-based score function defined as L = \u2211 (h, r, t), (6), where \u03b3 is a margin hyperparameter. E (h, r, t) is the total energy function specified above, where both head and tail units have two types of representation, including structure-based representations and image-based representations. T \"stands for the negative sample set of T, which we define as: T\" = (h, r, t) | h \"E\" (h, r, t) | t \"E\" (h, r, t), (h, r, t), (h, t), (h, r, t), meaning that one unit or relationship of a triple has been randomly replaced by another."}, {"heading": "3.5 Optimization and Implementation Details", "text": "The ICRL model can be formalized as a parameter set \u03b8 = (E, R, W, M), where E stands for the structure-based embedding set of units, R for the embedding set of relationships. W represents the weights of the neural networks used in the image display module, while M stands for the projection matrix used in the image projection module. We use stochastic gradient descent (SGD) in the minibatch to optimize our model, using chain rule to update the parameters. M is randomly initialized, while E and R could be either randomly initialized or pre-trained by previous translation-based methods. With regard to the image display module, we use a deep learning framework Caffe [Jia et al., 2014] to implement AlexNet, which will be implemented on ILSVRC 2012 with a slight deviation from the one described in [Krizheval, 2012]."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "In this paper, we propose a new dataset of knowledge graphs combined with images called WN9-IMG for evaluation tasks, including completion of knowledge graphs and triple classification. WN9-IMG is a subgraph of WN18 used in [Bordes et al., 2014], originally extracted from WordNet [Miller, 1995]. To view image quality, we provide 63,225 images extracted from ImageNet, a huge image database containing over 21,000 synsets organized according to the WordNet hierarchy [Deng et al., 2009]."}, {"heading": "4.2 Experiment Settings", "text": "We train the ICRL model via mini-batch SGD, whereby the batch size B between {7, 14, 28} and the margin \u03b3 between {0.5, 1.0, 2.0} could be specified. The optimal configuration of the ICRL model is: B = 7, \u03b3 = 1.0, whereby the learning rate is defined on the basis of a linear decimated strategy in which \u03bb values are between 0.001 and 0.0002, because the frame number n for each unit is up to 10. We also specify the dimension of the image feature embedding di = 4096 and the dimension of the entity and relationship embedding = 50. We implement TransE [Bordes et al., 2013] and TransR [Lin et al., 2015] as our foundations, with the same experimental embedding indicated in their papers."}, {"heading": "4.3 Knowledge Graph Completion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Evaluation Protocal", "text": "The completion of the knowledge chart aims to complete a triple (h, r, t) when one of h, r, t is missing. This task has been widely used to evaluate the quality of the knowledge representations [Bordes et al., 2012; Bordes et al., 2013]. The prediction is determined by the dissimilarity function. Since the ICRL model has two types of representations, we will report three prediction results based on our models: ICRL (SBR) uses structure-only representations for all units when predicting the missing ones, while ICRL (IBR) only uses image-based representations for predicting. ICRL (UNION) is a simple common method that takes into account the weighted concatenation of the two essentials representations.Following the same settings in [Bordes et al., 2013] we consider two measures as our evaluation metrics in the prediction of the entity (1) ratio of the units (we will also mean the corrects in the 10)."}, {"heading": "Entity Prediction", "text": "The results of the entity prediction are presented in Table 2. From the results, we can deduce the following: (1) All ICRL models outperform all baselines on both Mean Rank and Hits @ 10 evaluation metrics, of which ICRL (UNION) achieves the best performance. It shows that the visual information of the images has been successfully encoded in entity representations, which is of great importance for the construction of knowledge representations. (2) Both ICRL (SBR) and ICRL (IBR) perform better than baselines, suggesting that visual information could not only guide the construction of image-based representations, but also improve the performance of structure-based representations. (3) The ICRL models perform significantly and consistently better than baselines on Mean Rank. This is because the Mean Rank depends on the overall quality of knowledge representations and is therefore sensitive to the performance of structure-based representations. (3) The ICRL models perform significantly and consistently better than baselines on Mean Rank. This is because the Mean Rank may not be based on the overall quality of knowledge representations, and may therefore be sensitive to the performance of structure-based representations."}, {"heading": "Further Discussion on Attention", "text": "In order to further demonstrate the performance of the attention-based method, we implement three combination strategies to look at multiple instances of images together. ICRL (ATT) represents the basic model with attention when creating the aggregated image-based representations, while ICRL (MAX) represents the combination strategy that only takes into account the image instance that receives the most attention, and ICRL (AVG) represents the strategy that considers the average embedding of all instances of images as a unit. Assessment results in predicting entities with both image-based representations and structure-based representations are shown in Table 3.From Table 3, we note that: (1) all ICRL models are still better than baselines on Mean Rank and Hits @ 10, regardless of what the combination strategy is. It confirms the improvements introduced by images, as visual information has been successfully encoded in knowledge representations (2) KRL is the best of all three (the performance model KRL is the best of all)."}, {"heading": "4.4 Triple Classification", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Evaluation Protocol", "text": "The triple classification aims to predict whether or not a triple fact (h, r, t) is correct according to the deviation function. Since WN9-IMG has no explicit negative instances, we generate the negative instances by accidentally replacing head or tail units with another unit that follows the same protocol used in [Socher et al., 2013]. We also assure that the number of positive triples is equal to those of negative triples. In the classification, we set different relationship-specific thresholds for each relationship, which are optimized by maximizing the classification accuracy on the validation set with its corresponding relationships. To better demonstrate the advantages in the ICRL models, only the triple function of representation is predicted when the triple deviation is exceeded."}, {"heading": "Experimental Results", "text": "Table 4 shows that (1) all ICRL models exceed both baselines, demonstrating the effectiveness and robustness of our models, which combine structured information in triple execution with visual information in images. Note that ICRL is based on the framework of TransE, but performs better even compared to the enhanced TransR model, confirming the improvements achieved by images. (2) The ICRL model (ATT) performs best compared to other combination strategies, showing that the attention-based method can consider multiple cases together and select smarter informational images of all candidates."}, {"heading": "4.5 Case study", "text": "In this section we will analyze two cases: the first is to introduce the semantic laws of images, and the second is to demonstrate the ability to pay attention. To allow for better demonstrations, the images shown in the case study can be hacked while the most important objects are included."}, {"heading": "Semantic Regularities of Images", "text": "[Mikolov et al., 2013] shows that word representations exhibit some interesting semantic regularities, such as v (king) \u2212 v (man) \u2248 v (queen) \u2212 v (woman), and similar regularities have also been found on visual-semantic embeddings [Kiros et al., 2014]. We examine these semantic regularities on images shown in Fig. 4. In contrast to earlier work, the subtraction results of two images are more concrete and significant, since they are considered as corresponding relationship embeddings."}, {"heading": "Capability of Attention", "text": "Fig. 5 shows some pairs of images with different attention, which aim to confirm the ability of attention when selecting more informative images from multiple instances. In the first example of a portable computer, the attention-based method successfully recognizes the inferior instance, which is actually a phone, by giving little attention. In golf, the image with little attention only shows an overview of the lawn without a person or detailed sporting goods, and is therefore viewed less in combination. In the watering pot, the image focuses with little attention on the spout of a watering pot, which is confusing to represent the whole unit. By means of attention, we can automatically learn knowledge representations from better images, thereby reducing the noise in several cases."}, {"heading": "5 Conclusion and Future Work", "text": "We use neural networks and a projection module to model each image, and then construct the aggregated image-based representations by combining multiple instances of images based on attention. Experimental results confirm that our models are capable of encoding image information in knowledge representations. In the future, we will explore the following areas of research: (1) The quality of image representations is critical; we will use more complex models to extract better image properties; and we will further investigate the performance in specific areas. (2) Current ICRL models are based on TransE. We will examine the effectiveness of our models, which will be extended to other improved translation-based methods."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Proceedings of ICLR,", "citeRegEx": "Bahdanau et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "In Proceedings of AISTATS", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. Joint learning of words", "meaning representations for open-text semantic parsing"], "venue": "pages 127\u2013135,", "citeRegEx": "Bordes et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "In Proceedings of NIPS", "author": ["Antoine Bordes", "Nicolas Usunier", "Alberto Garcia-Duran", "Jason Weston", "Oksana Yakhnenko. Translating embeddings for modeling multirelational data"], "venue": "pages 2787\u20132795,", "citeRegEx": "Bordes et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Machine Learning", "author": ["Antoine Bordes", "Xavier Glorot", "Jason Weston", "Yoshua Bengio. A semantic matching energy function for learning with multi-relational data"], "venue": "94(2):233\u2013259,", "citeRegEx": "Bordes et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "LiJia Li", "Kai Li", "Li Fei-Fei"], "venue": "Proceedings of CVPR, pages 248\u2013255,", "citeRegEx": "Deng et al.. 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "In Proceedings of ACL", "author": ["Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao. Knowledge graph embedding via dynamic mapping matrix"], "venue": "pages 687\u2013696,", "citeRegEx": "Ji et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "Proceedings of ACM MM, pages 675\u2013678,", "citeRegEx": "Jia et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["Ryan Kiros", "Ruslan Salakhutdinov", "Richard S Zemel"], "venue": "Proceedings of NIPS,", "citeRegEx": "Kiros et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of NIPS", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks"], "venue": "pages 1097\u20131105,", "citeRegEx": "Krizhevsky et al.. 2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI,", "citeRegEx": "Lin et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig"], "venue": "Proceedings of HLTNAACL,", "citeRegEx": "Mikolov et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "Wordnet: a lexical database for english", "author": ["George A Miller"], "venue": "Communications of the ACM,", "citeRegEx": "Miller. 1995", "shortCiteRegEx": null, "year": 1995}, {"title": "et al", "author": ["Volodymyr Mnih", "Nicolas Heess", "Alex Graves"], "venue": "Recurrent models of visual attention. In Proceedings of NIPS, pages 2204\u20132212,", "citeRegEx": "Mnih et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Alexander M Rush", "Sumit Chopra", "Jason Weston"], "venue": "Proceedings of EMNLP,", "citeRegEx": "Rush et al.. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Black holes and white rabbits: Metaphor identification with visual features", "author": ["Ekaterina Shutova", "Douwe Kiela", "Jean Maillard"], "venue": "Proceedings of NAACL,", "citeRegEx": "Shutova et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "In Proceedings of NIPS", "author": ["Richard Socher", "Danqi Chen", "Christopher D Manning", "Andrew Ng. Reasoning with neural tensor networks for knowledge base completion"], "venue": "pages 926\u2013934,", "citeRegEx": "Socher et al.. 2013", "shortCiteRegEx": null, "year": 2013}, {"title": "In Proceedings of EMNLP", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen. Knowledge graph", "text jointly embedding"], "venue": "pages 1591\u20131601,", "citeRegEx": "Wang et al.. 2014a", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of AAAI", "author": ["Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen. Knowledge graph embedding by translating on hyperplanes"], "venue": "pages 1112\u20131119,", "citeRegEx": "Wang et al.. 2014b", "shortCiteRegEx": null, "year": 2014}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun"], "venue": "Proceedings of AAAI,", "citeRegEx": "Xie et al.. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "Proceedings of ICLR,", "citeRegEx": "Yang et al.. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural generative question answering", "author": ["Jun Yin", "Xin Jiang", "Zhengdong Lu", "Lifeng Shang", "Hang Li", "Xiaoming Li"], "venue": "Proceedings of IJCAI,", "citeRegEx": "Yin et al.. 2016", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 19, "context": "Knowledge graphs (KGs), which provide huge amount of structured information for entities and relations, have been successfully utilized in various fields such as knowledge inference [Yang et al., 2014] and question answering [Yin et al.", "startOffset": 182, "endOffset": 201}, {"referenceID": 20, "context": ", 2014] and question answering [Yin et al., 2016].", "startOffset": 31, "endOffset": 49}, {"referenceID": 2, "context": "To model knowledge graphs, translation-based methods are proposed, which project both entities and relations into a continuous low-dimensional semantic space, with relations considered to be translating operations between head and tail entities [Bordes et al., 2013].", "startOffset": 245, "endOffset": 266}, {"referenceID": 2, "context": "TransE [Bordes et al., 2013] models both entities and relations into the same low-dimensional continuous vector space, with relations considered to be translating operations between head and tail entities.", "startOffset": 7, "endOffset": 28}, {"referenceID": 17, "context": "To address this problem, TransH [Wang et al., 2014b] proposes relation-specific hyperplanes for translations between entities.", "startOffset": 32, "endOffset": 52}, {"referenceID": 9, "context": "TransR [Lin et al., 2015] models entities and relations in different vector spaces, projecting entities from entity space to relation spaces with relation-specific matrices.", "startOffset": 7, "endOffset": 25}, {"referenceID": 5, "context": "TransD [Ji et al., 2015] further ar X iv :1 60 9.", "startOffset": 7, "endOffset": 24}, {"referenceID": 16, "context": "To utilize rich textual information, [Wang et al., 2014a] projects both entities and words into a joint vector space with alignment models.", "startOffset": 37, "endOffset": 57}, {"referenceID": 18, "context": "[Xie et al., 2016] directly constructs entity representations from entity descriptions, which is capable of modeling new entities.", "startOffset": 0, "endOffset": 18}, {"referenceID": 7, "context": "As for visual information, multimodal representations based on words and images are widely used on various tasks like image-sentence ranking [Kiros et al., 2014] and metaphor identification [Shutova et al.", "startOffset": 141, "endOffset": 161}, {"referenceID": 14, "context": ", 2014] and metaphor identification [Shutova et al., 2016].", "startOffset": 36, "endOffset": 58}, {"referenceID": 18, "context": "To utilize entity image information, we propose two kinds of representations for each entity inspired by [Xie et al., 2016].", "startOffset": 105, "endOffset": 123}, {"referenceID": 8, "context": "We utilize AlexNet, a deep neural network that contains five convolution layers, two fullyconnected layers and a softmax layer, to extract image features [Krizhevsky et al., 2012].", "startOffset": 154, "endOffset": 179}, {"referenceID": 14, "context": "Inspired by [Shutova et al., 2016], we take the 4096-dimensional embeddings which are outputs", "startOffset": 12, "endOffset": 34}, {"referenceID": 12, "context": "It has been widely utilized in various fields such as image classification [Mnih et al., 2014], machine translation [Bahdanau et al.", "startOffset": 75, "endOffset": 94}, {"referenceID": 0, "context": ", 2014], machine translation [Bahdanau et al., 2015] and abstractive sentence summarization [Rush et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 13, "context": ", 2015] and abstractive sentence summarization [Rush et al., 2015].", "startOffset": 47, "endOffset": 66}, {"referenceID": 6, "context": "As for the image representation module, we utilize a deep learning framework Caffe [Jia et al., 2014] to implement AlexNet, which is pre-trained on ILSVRC 2012 with a minor variation from the version described in [Krizhevsky et al.", "startOffset": 83, "endOffset": 101}, {"referenceID": 8, "context": ", 2014] to implement AlexNet, which is pre-trained on ILSVRC 2012 with a minor variation from the version described in [Krizhevsky et al., 2012].", "startOffset": 119, "endOffset": 144}, {"referenceID": 3, "context": "WN9-IMG is a sub-graph of WN18 utilized in [Bordes et al., 2014], which is originally extracted from WordNet [Miller, 1995].", "startOffset": 43, "endOffset": 64}, {"referenceID": 11, "context": ", 2014], which is originally extracted from WordNet [Miller, 1995].", "startOffset": 52, "endOffset": 66}, {"referenceID": 4, "context": "For the consideration of image quality, we provide 63,225 images extracted from ImageNet, which is a huge image database containing over 21,000 synsets organized according to the WordNet hierarchy [Deng et al., 2009].", "startOffset": 197, "endOffset": 216}, {"referenceID": 2, "context": "We implement TransE [Bordes et al., 2013] and TransR [Lin et al.", "startOffset": 20, "endOffset": 41}, {"referenceID": 9, "context": ", 2013] and TransR [Lin et al., 2015] as our baselines, with the same experimental settings reported in their papers.", "startOffset": 19, "endOffset": 37}, {"referenceID": 1, "context": "This task has been widely used to evaluate the quality of knowledge representations [Bordes et al., 2012; Bordes et al., 2013].", "startOffset": 84, "endOffset": 126}, {"referenceID": 2, "context": "This task has been widely used to evaluate the quality of knowledge representations [Bordes et al., 2012; Bordes et al., 2013].", "startOffset": 84, "endOffset": 126}, {"referenceID": 2, "context": "Following the same settings in [Bordes et al., 2013], we consider two measures as our evaluation metrics in entity prediction: (1) mean rank of correct entities (Mean Rank); (2) proportion of correct entity results ranked in top 10 (Hits@10).", "startOffset": 31, "endOffset": 52}, {"referenceID": 2, "context": "We also follow the two evaluation settings named \u201cRaw\u201d and \u201cFilter\u201d used in [Bordes et al., 2013].", "startOffset": 76, "endOffset": 97}, {"referenceID": 15, "context": "Evaluation Protocol Triple classification aims to predict whether a triple fact (h, r, t) is correct or not according to the dissimilarity function [Socher et al., 2013].", "startOffset": 148, "endOffset": 169}, {"referenceID": 15, "context": "Since WN9-IMG has no explicit negative instances, we generate the negative instances by randomly replacing head or tail entities with another entity following the same protocol utilized in [Socher et al., 2013].", "startOffset": 189, "endOffset": 210}, {"referenceID": 10, "context": "[Mikolov et al., 2013] shows that word representations have some interesting semantic regularities such as v(king) \u2212 v(man) \u2248 v(queen) \u2212 v(woman), and the similar regularities have also been found on visual-semantic embeddings [Kiros et al.", "startOffset": 0, "endOffset": 22}, {"referenceID": 7, "context": ", 2013] shows that word representations have some interesting semantic regularities such as v(king) \u2212 v(man) \u2248 v(queen) \u2212 v(woman), and the similar regularities have also been found on visual-semantic embeddings [Kiros et al., 2014].", "startOffset": 212, "endOffset": 232}], "year": 2016, "abstractText": "Entity images provide significant visual information that helps the construction of knowledge representations. Most conventional methods learn knowledge representations solely from structured triples, ignoring rich visual information extracted from entity images. In this paper, we propose a novel Image-embodied Knowledge Representation Learning model, where knowledge representations are learned with both triples and images. More specifically, for each image of an entity, we construct image-based representations via a neural image encoder, and these representations with respect to multiple image instances are then integrated via an attention-based method. We evaluate our models on knowledge graph completion and triple classification. Experimental results demonstrate that our models outperform all baselines on both tasks, which indicates the significance of visual information for knowledge representations and the capability of our models in learning knowledge representations with images.", "creator": "LaTeX with hyperref package"}}}