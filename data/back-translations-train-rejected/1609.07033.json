{"id": "1609.07033", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Generating Abstractive Summaries from Meeting Transcripts", "abstract": "Summaries of meetings are very important as they convey the essential content of discussions in a concise form. Generally, it is time consuming to read and understand the whole documents. Therefore, summaries play an important role as the readers are interested in only the important context of discussions. In this work, we address the task of meeting document summarization. Automatic summarization systems on meeting conversations developed so far have been primarily extractive, resulting in unacceptable summaries that are hard to read. The extracted utterances contain disfluencies that affect the quality of the extractive summaries. To make summaries much more readable, we propose an approach to generating abstractive summaries by fusing important content from several utterances. We first separate meeting transcripts into various topic segments, and then identify the important utterances in each segment using a supervised learning approach. The important utterances are then combined together to generate a one-sentence summary. In the text generation step, the dependency parses of the utterances in each segment are combined together to create a directed graph. The most informative and well-formed sub-graph obtained by integer linear programming (ILP) is selected to generate a one-sentence summary for each topic segment. The ILP formulation reduces disfluencies by leveraging grammatical relations that are more prominent in non-conversational style of text, and therefore generates summaries that is comparable to human-written abstractive summaries. Experimental results show that our method can generate more informative summaries than the baselines. In addition, readability assessments by human judges as well as log-likelihood estimates obtained from the dependency parser show that our generated summaries are significantly readable and well-formed.", "histories": [["v1", "Thu, 22 Sep 2016 15:50:50 GMT  (1226kb,D)", "http://arxiv.org/abs/1609.07033v1", "10 pages, Proceedings of the 2015 ACM Symposium on Document Engineering, DocEng' 2015"]], "COMMENTS": "10 pages, Proceedings of the 2015 ACM Symposium on Document Engineering, DocEng' 2015", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["siddhartha banerjee", "prasenjit mitra", "kazunari sugiyama"], "accepted": false, "id": "1609.07033"}, "pdf": {"name": "1609.07033.pdf", "metadata": {"source": "CRF", "title": "Generating Abstractive Summaries from Meeting Transcripts", "authors": ["Siddhartha Banerjee", "Prasenjit Mitra", "Kazunari Sugiyama"], "emails": ["sub253@ist.psu.edu", "pmitra@qf.org.qa", "sugiyama@comp.nus.edu.sg", "Permissions@acm.org."], "sections": [{"heading": null, "text": "Permission to make digital or printed copies of this work for personal or commercial use is granted free of charge, provided that no copies are made or distributed for profit or commercial purposes, and that copies bear this notice and the complete quotation on the first page. Copyrights to components of this work owned by others other than ACM must be respected. Credit abstraction is permitted. Copying or republishing otherwise, posting on servers, or distributing on lists requires prior express permission and / or a fee. Request permissions from Permissions @ acm.org, DocEng '15 September 2015, Lausanne, Switzerland. c \u00a9 2015 ACM. ISBN 978-1-4503-3307-8 / 15 / 09... $15.00. DOI: http: / / dx.doi.org / 10.1145 / 2682571.7271.701.Keyword Categories and Subject Item Item ENTELTIC.IX: NATIONAL INFORMATIONS."}, {"heading": "1. INTRODUCTION", "text": "This year is the highest in the history of the country."}, {"heading": "2. RELATED WORK", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "3. PROPOSED APPROACH", "text": "As explained in Section 1, our proposed approach consists of three steps: First, we segment an entire conversation between participants into multiple text segments; second, we use an extractive summarizer that extracts important utterances from each segment; and finally, using an ILP-based approach, we merge all utterances in a segment into a summary sentence; and all generated sentences are appended to create the final summary."}, {"heading": "3.1 Text Segmentation", "text": "In general, lexical cohesion-based metrics lend themselves well to topic segmentation [27]. Since the main focus of our work is on compiling summaries, we are experimenting with two different text segmentation algorithms: LCSeg and Bayesian unattended topic segmentation. LCSeg: Galley et al. [13] developed a topic segmentor, LCSeg, based on lexical cohesion, which is considered a good indicator of the discourse structure of the text. The intuition behind this algorithm is that large term repetitions occur when the underlying topics begin or end in the text. It takes into account multiplex features such as discourses and overlaps. LCSeg is used at meetings of companies and achieves promising results. Therefore, this approach lends itself to our segmentation step.Bayesian unattended topic segmentation: This topic is also promising for an approach to a defining 8 iron stone and a delicacy."}, {"heading": "3.2 Selection of Important Utterances", "text": "As shown in Table 2, we use several attributes to identify the important set of statements in a meeting. We adopt basic and substantive attributes from previous work [12, 37]. In addition to the above attributes, we introduce two segment-based attributes: (i) The most important speaker in a segment. (ii) Kosine similarity between the utterance and all other utterances in a segment. We construct classifiers that use all characteristics of the training. We conduct experiments to evaluate the impact of our introduced segment-based attributes in addition to the basic and substantive attributes. In addition, the construction of a model using the meeting summary data also suffers from the unbalanced data problem, as few utterances are considered important to generate the final summary. To address this problem, we apply the following techniques to trick the minority data: Weight: Leave npRatio # negative # positive."}, {"heading": "3.3 Fusion of Utterances for Summary Generation", "text": "This year it is more than ever before."}, {"heading": "4. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset and Evaluation Metrics", "text": "The AMI Meeting Corpus [5] includes 139 session minutes and associated extractive and abstract summaries. The standard test set for this corpus includes 20 sessions. Our extractive summaries are trained on the basis of the training set, i.e. the remaining 119 sessions. We evaluate the accuracy of the 4http: / / www.gurobi.com / classifiers using standard metrics: Precision, Recall and F-Measure. We also evaluate the impact of introducing segment-based features. To evaluate the quality of the summaries, we evaluate the effectiveness of content selection using ROUGE, widely used as a standard method for evaluating information content in summary tasks, comparing system-generated summaries with humanely generated abstract summaries. We also evaluate the linguistic quality of the summaries produced on the basis of human assessments."}, {"heading": "4.2 Classifier Evaluation", "text": "In this context, it should be noted that these two are the best minds who have ever lived in a country."}, {"heading": "4.3 Content Selection", "text": "This year, more than ever before in the history of a country in which it is a country in which it is a country in which it is not a country in which it is not a country, but a country in which it is a country, a country, a country, a country, a country, a country, a city, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a country, a"}, {"heading": "4.4 Readability Analysis", "text": "In fact, it is the case that most of them are in a position to move into another world, in which they are in a position, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "5. CONCLUSIONS AND FUTURE WORK", "text": "We have proposed a method for dividing a conversation into several topic segments. We used an extractive summary to identify the important set of statements, and then applied ILP-based summaries to generate a sentence summary from each topic segment. We used the grammatical relationships in fusion technology that are more prevalent in non-conversational text styles.The experiments on content selection and readability suggest that our method can generate relevant abstract summaries from session protocols without templates. However, as we have already pointed out, not all generated summaries are usable due to the lack of coherence between multiple units discussed within the same summary sentence. We plan to improve the generation of units by using knowledge of units and also refine readability using a language model. In future work, we plan to develop better linearization techniques. We also plan to improve our algorithm per segment by enabling ILs to define only one sentence per sentence, rather than one sentence per optimal algorithm."}, {"heading": "6. ACKNOWLEDGMENTS", "text": "This material is based on research funded by the National Science Foundation under grant number 0845487."}, {"heading": "7. REFERENCES", "text": "In fact: it is not that people are able to save themselves, and that they are not able to save themselves, and that they are not able to save themselves. (...) It is also not that they are able to save themselves. (...) It is not that they are able to save themselves. (...) It is not that they are able to save themselves. (...) It is that they are not able to save themselves. (...) It is as if they are not able to save themselves. (...). (...). (...). (...). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.). (.).). (.). (.).)."}], "references": [{"title": "Automated Learning of Decision Rules for Text Categorization", "author": ["C. Apt\u00e9", "F. Damerau", "S.M. Weiss"], "venue": "ACM Transactions on Information Systems (TOIS), 12(3):233\u2013251", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "Abstractive Meeting Summarization Using Dependency Graph Fusion", "author": ["S. Banerjee", "P. Mitra", "K. Sugiyama"], "venue": "Proc. of the 24th International Conference on World Wide Web Companion (WWW \u201915 Companion), pages 5\u20136", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Sentence Fusion for Multidocument News Summarization", "author": ["R. Barzilay", "K.R. McKeown"], "venue": "Computational Linguistics, 31(3):297\u2013328", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Discourse Segmentation in Aid of Document Summarization", "author": ["B.K. Boguraev", "M.S. Neff"], "venue": "Proc. of the 33rd Annual Hawaii International Conference on System Sciences (HICSS-33), pages 1\u201310", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "et al", "author": ["J. Carletta", "S. Ashby", "S. Bourban", "M. Flynn", "M. Guillemot", "T. Hain", "J. Kadlec", "V. Karaiskos", "W. Kraaij", "M. Kronenthal"], "venue": "The AMI Meeting Corpus: A Pre-announcement. In Proc. of the 2nd International Workshop on Machine Learning for Multimodal Interaction ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2005}, {"title": "SMOTE: Synthetic Minority Over-sampling  Technique", "author": ["N.V. Chawla", "K.W. Bowyer", "L.O. Hall", "W.P. Kegelmeyer"], "venue": "Journal of Artificial Intelligence Research (JAIR), 16", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Global Inference for Sentence Compression: An Integer Linear Programming Approach", "author": ["J. Clarke", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research (JAIR), 31", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian Unsupervised Topic Segmentation", "author": ["J. Eisenstein", "R. Barzilay"], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-sentence Compression: Finding Shortest Paths in Word Graphs", "author": ["K. Filippova"], "venue": "Proc. of the 23rd International Conference on Computational Linguistics ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Sentence Fusion via Dependency Graph Compression", "author": ["K. Filippova", "M. Strube"], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing ", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}, {"title": "Tree Linearization in English: Improving Language Model Based Approaches", "author": ["K. Filippova", "M. Strube"], "venue": "Proc. of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A Skip-Chain Conditional Random Field for Ranking Meeting Utterances by Importance", "author": ["M. Galley"], "venue": "Proc. of the 2006 Conference on Empirical Methods in Natural Language Processing ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Discourse Segmentation of Multi-party Conversation", "author": ["M. Galley", "K. McKeown", "E. Fosler-Lussier", "H. Jing"], "venue": "Proc. of the 41st Annual Meeting on Association for Computational Linguistics (ACL \u201903), pages 562\u2013569", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "ClusterRank: A Graph Based Method for Meeting Summarization", "author": ["N. Garg", "B. Favre", "K. Riedhammer", "D. Hakkani-T\u00fcr"], "venue": "Proc. of the 10th Annual Conference of the International Speech Communication ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2009}, {"title": "A Global Optimization Framework for Meeting Summarization", "author": ["D. Gillick", "K. Riedhammer", "B. Favre", "D. Hakkani-Tur"], "venue": "Proc. of the IEEE International Conference on Acoustics, Speech, and Signal Processing ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "The WEKA data mining software: an update", "author": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "venue": "ACM SIGKDD Explorations Newsletter, 11(1):10\u201318", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "A New Approach to Automatic Speech Summarization", "author": ["C. Hori", "S. Furui"], "venue": "IEEE Transactions on Multimedia, 5(3):368\u2013378", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2003}, {"title": "Automatic Topic Segmentation and Labeling in Multiparty Dialogue", "author": ["P.-Y. Hsueh", "J.D. Moore"], "venue": "Spoken Language Technology Workshop, pages 98\u2013101", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "author": ["C.-Y. Lin"], "venue": "Proc. of the ACL-04 Workshop on Text Summarization Branches Out, pages 74\u201381", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "The Automated Acquisition of Topic Signatures for Text Summarization", "author": ["C.-Y. Lin", "E. Hovy"], "venue": "Proc. of the 18th Conference on Computational Linguistics (COLING \u201900), pages 495\u2013501", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Graph-based Submodular Selection for Extractive Summarization", "author": ["H. Lin", "J. Bilmes", "S. Xie"], "venue": "Proc. of the IEEE Workshop on Automatic Speech Recognition & Understanding ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "From Extractive to Abstractive Meeting Summaries: Can It Be Done by Sentence Compression? In Proc", "author": ["F. Liu", "Y. Liu"], "venue": "of the 47th Annual Meeting of the Association for Computational Linguistics ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2009}, {"title": "Using N-best Recognition Output for Extractive Summarization and Keyword Extraction in Meeting Speech", "author": ["Y. Liu", "S. Xie", "F. Liu"], "venue": "Proc. of IEEE International Conference on Acoustics Speech and Signal Processing ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "The Stanford CoreNLP Natural Language Processing Toolkit", "author": ["C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky"], "venue": "Proc. of 52nd Annual Meeting of the Association for Computational Linguistics ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Abstractive Meeting Summarization with Entailment and Fusion", "author": ["Y. Mehdad", "G. Carenini", "F.W. Tompa", "R.T. NG"], "venue": "Proc. of the 14th European Workshop on Natural Language Generation, pages 136\u2013146", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2013}, {"title": "Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text", "author": ["J. Morris", "G. Hirst"], "venue": "Computational Linguistics, 17(1):21\u201348", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1991}, {"title": "Summarizing Spoken and Written Conversations", "author": ["G. Murray", "G. Carenini"], "venue": "Proc. of the Conference on Empirical Methods in Natural Language Processing ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "Generating and Validating Abstracts of Meeting Conversations: a User Study", "author": ["G. Murray", "G. Carenini", "R. Ng"], "venue": "Proc. of the 6th International Natural Language Generation Conference ", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Interpretation and Transformation for Abstracting Conversations", "author": ["G. Murray", "G. Carenini", "R. Ng"], "venue": "Proc. of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics ", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "A Survey of Text Summarization Techniques", "author": ["A. Nenkova", "K. McKeown"], "venue": "Mining Text Data, pages 43\u201376", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "A Template-based Abstractive Meeting Summarization: Leveraging Summary and Source Text Relationships", "author": ["T. Oya", "Y. Mehdad", "G. Carenini", "R. Ng"], "venue": "Proc. of the 8th International Natural Language Generation Conference ", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "The Reuters Corpus Volume 1-from Yesterday\u2019s News to Tomorrow\u2019s Language Resources", "author": ["T. Rose", "M. Stevenson", "M. Whitehead"], "venue": "Proc. of the 3rd International Conference on Language Resources and Evaluation Conference (LREC\u201902), pages 827\u2013832", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "A Linear Programming Formulation for Global Inference in Natural Language Tasks", "author": ["D. Roth", "W.-T. Yih"], "venue": "Proc. of the 8th Conference on Natural Language Learning ", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Dijkstra\u2019s algorithm", "author": ["S. Skiena"], "venue": "Implementing Discrete Mathematics: Combinatorics and Graph Theory with Mathematica, Reading, pages 225\u2013227", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}, {"title": "Domain-Independent Abstract Generation for Focused Meeting Summarization", "author": ["L. Wang", "C. Cardie"], "venue": "Proc. of the 51st Annual Meeting of the Association for Computational Linguistics ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Using Corpus and Knowledge-based Similarity Measure in Maximum Marginal Relevance for Meeting Summarization", "author": ["S. Xie", "Y. Liu"], "venue": "Proc. of IEEE International Conference on Acoustics, Speech and Signal Processing ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 4, "context": "Table 1: Two sets of extractive summaries along with the corresponding gold standard human generated abstractive summaries from a meeting in the AMI corpus [5].", "startOffset": 156, "endOffset": 159}, {"referenceID": 13, "context": "While previous work on meeting summarization was primarily extractive [14, 15], a recent study showed that people generally prefer abstractive summaries [29].", "startOffset": 70, "endOffset": 78}, {"referenceID": 14, "context": "While previous work on meeting summarization was primarily extractive [14, 15], a recent study showed that people generally prefer abstractive summaries [29].", "startOffset": 70, "endOffset": 78}, {"referenceID": 27, "context": "While previous work on meeting summarization was primarily extractive [14, 15], a recent study showed that people generally prefer abstractive summaries [29].", "startOffset": 153, "endOffset": 157}, {"referenceID": 4, "context": "pus [5].", "startOffset": 4, "endOffset": 7}, {"referenceID": 34, "context": "Previous approaches to abstractive meeting summarization have relied on template-based [36] or word-graph fusion-based [26] methods.", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "Previous approaches to abstractive meeting summarization have relied on template-based [36] or word-graph fusion-based [26] methods.", "startOffset": 119, "endOffset": 123}, {"referenceID": 17, "context": "Every meeting is usually comprised of several sub-topics [19].", "startOffset": 57, "endOffset": 61}, {"referenceID": 26, "context": "Previous work on meeting summarization [28] has shown that lexical cohesion is an important indicator in topic identification in meetings.", "startOffset": 39, "endOffset": 43}, {"referenceID": 12, "context": "We experiment with two different lexicalcohesion based text segmentation algorithms: LCSeg [13] and unsupervised Bayesian topic segmentation [8].", "startOffset": 91, "endOffset": 95}, {"referenceID": 7, "context": "We experiment with two different lexicalcohesion based text segmentation algorithms: LCSeg [13] and unsupervised Bayesian topic segmentation [8].", "startOffset": 141, "endOffset": 144}, {"referenceID": 9, "context": "We formulate the sub-graph generation problem as an Integer Linear Programming (ILP) problem by adapting an existing sentence fusion technique [10].", "startOffset": 143, "endOffset": 147}, {"referenceID": 16, "context": "Information content is measured using Hori and Furui\u2019s word informativeness formula [18] while the linguistic quality is estimated using probabilities of grammatical relations from the Reuter\u2019s corpus [1].", "startOffset": 84, "endOffset": 88}, {"referenceID": 0, "context": "Information content is measured using Hori and Furui\u2019s word informativeness formula [18] while the linguistic quality is estimated using probabilities of grammatical relations from the Reuter\u2019s corpus [1].", "startOffset": 201, "endOffset": 204}, {"referenceID": 10, "context": "The sub-graph generated from each segment is linearized [11] using a bottom-up approach to generate a one-sentence summary.", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "ROUGE-2 and ROUGE-SU4 [20] scores from our abstractive model (0.", "startOffset": 22, "endOffset": 26}, {"referenceID": 24, "context": "044) as well as the word-graph based abstractive summarization method [26] (0.", "startOffset": 70, "endOffset": 74}, {"referenceID": 20, "context": "In the field of meeting summarization, while extractive techniques have been widely employed so far [22, 24], abstractive techniques, including sentence compression, template and graph-based approaches, have been focused on recently.", "startOffset": 100, "endOffset": 108}, {"referenceID": 22, "context": "In the field of meeting summarization, while extractive techniques have been widely employed so far [22, 24], abstractive techniques, including sentence compression, template and graph-based approaches, have been focused on recently.", "startOffset": 100, "endOffset": 108}, {"referenceID": 21, "context": "Liu and Liu [23] used sentence compression to generate summaries of meetings.", "startOffset": 12, "endOffset": 16}, {"referenceID": 28, "context": "[30] mapped conversations to an ontology that was complemented with a Natural", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "The corresponding full summarization system was later presented in [29], where a user study was conducted on the abstractive summaries that were generated.", "startOffset": 67, "endOffset": 71}, {"referenceID": 34, "context": "Lu and Cardie [36] proposed a method that learns templates from the human written summaries and generates the summaries of decisions and actions of meetings by using the best set of templates for a particular summary ranked using a greedy approach.", "startOffset": 14, "endOffset": 18}, {"referenceID": 24, "context": "[26] developed a method that first over-generates multiple fused utterances in an entailment graph, and then chooses one based on the final path ranking.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[32] used the same fusion technique to generate summaries of meetings.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "mentioned above relied on using multi-sentence compression (MSC) [9] that combines information from sentences that are similar or connected using some common entity.", "startOffset": 65, "endOffset": 68}, {"referenceID": 1, "context": "Our previous work [2] has briefly described the effectiveness of the fusion-based technique, which is also employed in this work.", "startOffset": 18, "endOffset": 21}, {"referenceID": 19, "context": "Topic segmentation has been used in summarization of news articles [21, 4].", "startOffset": 67, "endOffset": 74}, {"referenceID": 3, "context": "Topic segmentation has been used in summarization of news articles [21, 4].", "startOffset": 67, "endOffset": 74}, {"referenceID": 25, "context": "Generally, lexical cohesion-based measures work well for topic segmentation [27].", "startOffset": 76, "endOffset": 80}, {"referenceID": 12, "context": "[13] developed a topic segmentor, LCSeg, based on lexical cohesion, which is considered to be a good indicator of the discourse structure of the text.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Most of them are adopted from previous works [12, 37].", "startOffset": 45, "endOffset": 53}, {"referenceID": 35, "context": "Most of them are adopted from previous works [12, 37].", "startOffset": 45, "endOffset": 53}, {"referenceID": 7, "context": "Eisenstein and Barzilay [8] proposed an unsupervised approach to topic segmentation based on lexical cohesion modeled by a Bayesian framework.", "startOffset": 24, "endOffset": 27}, {"referenceID": 11, "context": "We adopt basic and content features from previous works [12, 37].", "startOffset": 56, "endOffset": 64}, {"referenceID": 35, "context": "We adopt basic and content features from previous works [12, 37].", "startOffset": 56, "endOffset": 64}, {"referenceID": 5, "context": "SMOTE: In synthetic minority oversampling technique (SMOTE) [6], the minority class is randomly oversampled.", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "Several techniques have been proposed for sentence fusion tasks [3].", "startOffset": 64, "endOffset": 67}, {"referenceID": 9, "context": "We adapt a sentence fusion technique [10] to meeting utterances.", "startOffset": 37, "endOffset": 41}, {"referenceID": 6, "context": "ILP has been applied successfully to many natural language processing tasks [7, 34].", "startOffset": 76, "endOffset": 83}, {"referenceID": 32, "context": "ILP has been applied successfully to many natural language processing tasks [7, 34].", "startOffset": 76, "endOffset": 83}, {"referenceID": 23, "context": "We use the publicly available Stanford CoreNLP package [25] that has a co-reference resolution module.", "startOffset": 55, "endOffset": 59}, {"referenceID": 9, "context": "Similar to the fusion technique by Fillipova and Strube [10], we model the problem as an integer linear programming (ILP) formulation.", "startOffset": 56, "endOffset": 60}, {"referenceID": 31, "context": "In this work, we calculate these values using Reuters corpora [33] in order to obtain dominant relations from non-conversational style of text.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "In order to compute I(d), we improve the word significance score [18] as follows:", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "Some of the constraints have been directly adapted from the original ILP formulation [10].", "startOffset": 85, "endOffset": 89}, {"referenceID": 33, "context": "ply Dijkstra\u2019s algorithm [35] to calculate the path length.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "The AMI Meeting corpus [5] contains 139 meeting transcripts along with their corresponding extractive and abstractive summaries.", "startOffset": 23, "endOffset": 26}, {"referenceID": 15, "context": "We used Weka [17] for all the classification tasks with the default set of parameters.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "071 MSC model [9] 0.", "startOffset": 14, "endOffset": 17}, {"referenceID": 8, "context": "We also compare the summaries to the MSC method proposed by Fillipova [9] that has been adapted for abstractive meeting summarization [26] develped earlier.", "startOffset": 70, "endOffset": 73}, {"referenceID": 24, "context": "We also compare the summaries to the MSC method proposed by Fillipova [9] that has been adapted for abstractive meeting summarization [26] develped earlier.", "startOffset": 134, "endOffset": 138}, {"referenceID": 8, "context": "73 MSC model [9] 0.", "startOffset": 13, "endOffset": 16}, {"referenceID": 29, "context": "Both the ROUGE scores have been found to correlate well with human judgments [31].", "startOffset": 77, "endOffset": 81}], "year": 2016, "abstractText": "Summaries of meetings are very important as they convey the essential content of discussions in a concise form. Both participants and non-participants are interested in the summaries of meetings to plan for their future work. Generally, it is time consuming to read and understand the whole documents. Therefore, summaries play an important role as the readers are interested in only the important context of discussions. In this work, we address the task of meeting document summarization. Automatic summarization systems on meeting conversations developed so far have been primarily extractive, resulting in unacceptable summaries that are hard to read. The extracted utterances contain disfluencies that affect the quality of the extractive summaries. To make summaries much more readable, we propose an approach to generating abstractive summaries by fusing important content from several utterances. We first separate meeting transcripts into various topic segments, and then identify the important utterances in each segment using a supervised learning approach. The important utterances are then combined together to generate a one-sentence summary. In the text generation step, the dependency parses of the utterances in each segment are combined together to create a directed graph. The most informative and well-formed sub-graph obtained by integer linear programming (ILP) is selected to generate a one-sentence summary for each topic segment. The ILP formulation reduces disfluencies by leveraging grammatical relations that are more prominent in nonconversational style of text, and therefore generates summaries that is comparable to human-written abstractive summaries. Experimental results show that our method can generate more informative summaries than the baselines. In addition, readability assessments by human judges as well as log-likelihood estimates obtained from the dependency parser show that our generated summaries are significantly readable and well-formed. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from Permissions@acm.org. DocEng\u201915, September 8-11, 2015, Lausanne, Switzerland. c \u00a9 2015 ACM. ISBN 978-1-4503-3307-8/15/09 ...$15.00. DOI: http://dx.doi.org/10.1145/2682571.2797061. Table 1: Two sets of extractive summaries along with the corresponding gold standard human generated abstractive summaries from a meeting in the AMI corpus [5]. Set 2 follows Set 1 in the actual meeting transcript. \u201cA,\u201d \u201cB\u201d and \u201cD\u201d refer to three distinct speakers in the meeting. Set 1: Human-generated extractive summary D: um as well as uh characters. D: um different uh keypad styles and s symbols. D: Well right away I\u2019m wondering if there\u2019s um th th uh, like with DVD players, if there are zones. A: Cause you have more complicated characters like European languages, then you need more buttons. D: I\u2019m thinking the price might might appeal to a certain market in one region, whereas in another it\u2019ll be different, so D: kay trendy probably means something other than just basic Abstractive summary: The team then discussed various features to consider in making the remote.ive summary: The team then discussed various features to consider in making the remote. Set 2: Human-generated extractive summary B: Like how much does, you know, a remote control cost. B: Well twenty five Euro, I mean that\u2019s um that\u2019s about like eighteen pounds or something. D: This is this gonna to be like the premium product kinda thing or B: So I don\u2019t know how how good a remote control that would get you. Um. Abstractive summary: The project manager talked about the project finances and selling prices.ive summary: The project manager talked about the project finances and selling prices.", "creator": "LaTeX with hyperref package"}}}