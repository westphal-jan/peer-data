{"id": "1609.07053", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Semantic Tagging with Deep Residual Networks", "abstract": "We propose a novel semantic tagging task, semtagging, tailored for the purpose of multilingual semantic parsing, and present the first tagger using deep residual networks (ResNets). Our tagger uses both word and character representations and includes a novel residual bypass architecture. We evaluate the tagset both intrinsically on the new task of semantic tagging, as well as on Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an auxiliary loss function predicting our semantic tags, significantly outperforms prior results on English Universal Dependencies POS tagging (95.71% accuracy on UD v1.2 and 95.67% accuracy on UD v1.3).", "histories": [["v1", "Thu, 22 Sep 2016 16:34:00 GMT  (184kb,D)", "http://arxiv.org/abs/1609.07053v1", "10 pages, to appear at COLING 2016"], ["v2", "Mon, 31 Oct 2016 18:33:13 GMT  (118kb,D)", "http://arxiv.org/abs/1609.07053v2", "COLING 2016, camera ready version"]], "COMMENTS": "10 pages, to appear at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["johannes bjerva", "barbara plank", "johan bos"], "accepted": false, "id": "1609.07053"}, "pdf": {"name": "1609.07053.pdf", "metadata": {"source": "CRF", "title": "Semantic Tagging with Deep Residual Networks", "authors": ["Johannes Bjerva", "Barbara Plank", "Johan Bos"], "emails": ["j.bjerva@rug.nl", "b.plank@rug.nl", "johan.bos@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "In fact, most of them will be able to move to another world, in which they will be able to integrate, and in which they will be able to change and change the world."}, {"heading": "2 Semantic Tagging", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Background", "text": "We refer to the semantic session, or conference, which is about assigning semantic class categories to the smallest units in a sentence. In the context of this paper, these units may be morphemes, words, or multiple terms. However, the linguistic information obtained for semantic processing is insufficient."}, {"heading": "2.2 Annotated Data", "text": "We use two semtag records. Groningen Meaning Bank (GMB) uses English texts (1.4 million words) containing silver standard semantic tags obtained by executing a simple rule-based semantic tagger (Bos et al., Forthcoming), which uses POS and named entity tags available in GMB (which are automatically corrected with the C & C tools (Curran et al., 2007) and then manually corrected), as well as a set of manually created rules for issuing semantic tags. Some tags referring to specific phenomena have been corrected manually in a second stage. Our second set of data is smaller but equipped with gold-standard semantic tags and is used for testing (PMB, the parallel meaning bank). It includes a selection of 400 records from the English part of a parallel corpus. It has no overlap with the GMB corpus. For this set we use the elephant, the multiword and the token element."}, {"heading": "3 Method", "text": "Our tagger is a hierarchical deep neural network consisting of a bidirectional gated recurrent unit (GRU) network at the upper level and a convolutionary neural network (CNN) and / or deep residual network (ResNet) at the lower level, including an optional novel residual bypass function (see Figure 1)."}, {"heading": "3.1 Gated Recurrent Unit networks", "text": "GRUs (Cho et al., 2014) are a recently introduced variant of RNNs and are designed to prevent disappearing gradients and thus handle longer input sequences than vanilla RNNs. GRUs are similar to the more commonly used Long Short-Term Memory Networks (LSTMs), both in purpose and in implementation (Chung et al., 2014). A bi-directional GRU is a GRU that makes both forward and backward transitions across sequences and can therefore use both previous and subsequent contexts to predict a day (Graves et al., 2005; Goldberg et al., 2015). Bidirectional GRUs and LSTMs have been shown to perform well in multiple NLP tasks, such as POS tagging, entity tagging and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016)."}, {"heading": "3.2 Deep Residual Networks", "text": "A residual unit can be expressed as: yl = h (xl) + F (xl, Wl), xl + 1 = f (yl), (3) where xl and xl + 1 are the input and output of the l-th layer, Wl is the weight for the l-th layer, and F is a residual function (He et al., 2016), such as the identity function (He et al., 2015), which we also use in our experiments. ResNets can be understood intuitively by considering residual functions as ways through which information can easily spread. This means that a ResNet learns more complex functional combinations in each layer, which it combines with the flatter representation from the previous layer. This architecture enables the construction of much deeper networks. ResNets have recently performed impressive performance in image recognition tasks, with networks as deep as 1001 layers (He et al 2016) and Nymal layers (2016 He)."}, {"heading": "3.3 Modelling character information and residual bypass", "text": "This means that we will be able to address the public in different ways. (2016) The use of sub-token representations can be done in multiple ways. (2016) and Yang et al. (2016) Use a hierarchical bi-directional RNN to create word representations. (2015) Similarly, we apply an LSTM-based model that directly uses byte information. Dos Santos and Zadrozny construct character-based representations by running a revolutionary network over the representation of each word. All of these approaches have in common that the character-based representation is passed through the rest of the network."}, {"heading": "3.4 System description", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "3.4.1 Hyperparameters", "text": "We use linear units (ReLUs) for all activation functions (Nair and Hinton, 2010) and apply dropouts with p = 0.1 to both input weights and recurring weights in the bi-GRU (Srivastava et al., 2014). In the CNNs we apply batch normalization (Ioffe and Szegedy, 2015) followed by dropouts with p = 0.5 after each shift. In our basic CNN we apply a 4 x 8 folding, followed by 2 x 2-maximum pooling, followed by 4 x 4 folding and another 2 x-2 maximum pooling. Our ResNet has the same structure, with the addition of a residual connection. We also experimented with using average pooling units instead of maximum pooling, but this yielded lower validation data performance on the day-semantic task."}, {"heading": "4 Evaluation", "text": "We evaluate our tagger based on two tasks: semantic tagging and POS tagging. Note that the tagger is developed exclusively on the semantic tagging task, using GMB silver training and validation data. Therefore, no further fine-tuning of the hyperparameters for the POS tagging task takes place. We calculate the significance using bootstrap resampling (Efron and Tibshirani, 1994). In our experiments, we manipulate the following independent variables: 1. Character and word representations (~ w, ~ c); 2. Remaining bypass for character representations (~ cbp); 3. Convolutionary representations (Basic CNN and ResNets); 4. Auxiliary loss (using coarse semtags on ST and fine semtags on UD). We compare our results with four baseline: 1. the most common baseline per word (MFC), assigning the most frequent day for a word in the training data."}, {"heading": "4.1 Experiments on semantic tagging", "text": "We evaluate our system using two semantic tagging datasets (ST): our silver semtag dataset and our gold semtag dataset. For the + AUX condition, we use coarse semags as auxiliary loss. The results of these experiments are shown in Table 3."}, {"heading": "4.2 Experiments on Part-of-Speech tagging", "text": "We evaluate our system using v1.2 and v1.3 of the English part of the Universal Dependencies (UD) data. We output the results solely for POS marking, compare with commonly used baselines and previous work using LSTMs, and use the fine-grained semantic tags as auxiliary information. For the + AUX condition, we train a single common model with a multi-task target, with POS and ST being our two tasks. This model is trained on concatenating the ST silver data with the UD data and updates the loss of the respective task of an instance in each iteration. Therefore, the weights leading to the UD Softmax layer are not updated on the ST silver part of the data and vice versa on the ST Softmax layer on the UD part of the data. The results of these experiments are shown in Table 4."}, {"heading": "5 Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Performance on semantic tagging", "text": "The overall best system is ResNet, which combines both word and character representations, exceeding all baselines, including the recently proposed RNN-based Bi-LSTM. There is a significant difference in ST silver data (p < 0.01) when comparing our best system with the strongest baseline (BI-LSTM). In ST gold data, we observe significant differences in alpha values recommended by S\u00f8gaard et al. (2014), with p < 0.0025. The residual bypass effectively helps improve the performance of the basic CNN. However, CNN's tagging accuracy falls short of baseline values. Furthermore, the large gap between gold and silver data for CNN shows that the CNN model is more susceptible to mismatches, thereby favoring the use of ResNet."}, {"heading": "5.2 Performance on POS tagging", "text": "Our system has been tuned exclusively for semtag data, which is reflected, for example, in the fact that while our system outperforms Plank et al. (2016) in terms of semtag, it performs significantly better in this setup with UD 1.2 and 1.3 than our system Plank et al. (2016). However, adding an auxiliary loss based on our semags significantly improves POS tagging performance. In this setup, our tagger outperforms the BI-LSTM system and yields new state-of-the-art results for both UD 1.2 (95.71% accuracy) and 1.3 (95.67% accuracy).The difference between the BI-LSTM system and our best system is significant with p < 0.0025. The fact that semantic tags improve POS tagging performance reflects two properties of semantic tags. First, it shows that the semantic tags contain important information for predicting POS tags."}, {"heading": "5.3 ResNets for sequence tagging", "text": "This work is the first to apply ResNets to NLP tagging tasks. Our experiments show that ResNets perform significantly better than conventional conventional conventional networks in both POS tagging and semtagging. ResNets allow for better signal propagation and carry a lower risk of overmatch, allowing the model to capture more elaborate feature representations than in a standard CNN."}, {"heading": "5.4 Pre-trained embeddings", "text": "In our main experiments, we initialized the word embedding layer with pre-trained polyglot embedding. We also compared this with the initialization of this layer from an even distribution over the interval [\u2212 0.05, 0.05). In the case of semantic embedding, the difference to random embedding is negligible, whereby pre-trained embedding resulted in an increase in accuracy of about 0.04%. In the case of POS tagging, however, the use of pre-trained embedding increased the accuracy in ResNet by almost 3 percentage points."}, {"heading": "6 Conclusions", "text": "We introduce a semantic tagset tailored to multilingual semantic parsing. We evaluate tagging performance using standard CNNs and the recently released ResNets. ResNets are more robust and make our best model. The combination of word and ResNet-based character representations helps to outperform cutting-edge taggers in semantic tagging. Coupling this with an additional loss from our semantic tagset results in state-of-the-art performance on the UD 1.2 and 1.3 POS datasets."}, {"heading": "Acknowledgements", "text": "The authors thank Robert O \ufffd stling for tips on ResNets and Calle Bo \ufffd rstell and Johan Sjons for feedback on earlier versions of this manuscript. We thank the Centre for Information Technology of the University of Groningen for their support and access to the Peregrine supercomputer cluster. This work was partly funded by the NWO-VICI scholarship \"Lost in Translation - Found in Meaning\" (288-89-003)."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "author": ["nanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vi\u00e9gas et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vi\u00e9gas et al\\.", "year": 2016}, {"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena."], "venue": "CoNLL-2013.", "citeRegEx": "Al.Rfou et al\\.,? 2013", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Semantic parsing via paraphrasing", "author": ["Jonathan Berant", "Percy Liang."], "venue": "ACL, pages 1415\u20131425.", "citeRegEx": "Berant and Liang.,? 2014", "shortCiteRegEx": "Berant and Liang.", "year": 2014}, {"title": "Wide-Coverage Semantic Analysis with Boxer", "author": ["Johan Bos."], "venue": "J. Bos and R. Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 277\u2013286. College Publications.", "citeRegEx": "Bos.,? 2008", "shortCiteRegEx": "Bos.", "year": 2008}, {"title": "Tnt: a statistical part-of-speech tagger", "author": ["Thorsten Brants."], "venue": "Proceedings of the sixth conference on Applied natural language processing, pages 224\u2013231. Association for Computational Linguistics.", "citeRegEx": "Brants.,? 2000", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "The Semantics of Grammatical Dependencies, volume 23", "author": ["Alastair Butler."], "venue": "Emerald Group Publishing Limited.", "citeRegEx": "Butler.,? 2010", "shortCiteRegEx": "Butler.", "year": 2010}, {"title": "Open-domain name error detection using a multi-task rnn", "author": ["Hao Cheng", "Hao Fang", "Mari Ostendorf."], "venue": "EMNLP.", "citeRegEx": "Cheng et al\\.,? 2015", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio."], "venue": "EMNLP.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Keras", "author": ["Fran\u00e7ois Chollet."], "venue": "https://github.com/fchollet/keras.", "citeRegEx": "Chollet.,? 2015", "shortCiteRegEx": "Chollet.", "year": 2015}, {"title": "Text segmentation with character-level text embeddings", "author": ["Grzegorz Chrupa\u0142a."], "venue": "Workshop on Deep Learning for Audio, Speech and Language Processing, ICML.", "citeRegEx": "Chrupa\u0142a.,? 2013", "shortCiteRegEx": "Chrupa\u0142a.", "year": 2013}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio."], "venue": "arXiv preprint arXiv:1412.3555.", "citeRegEx": "Chung et al\\.,? 2014", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["Junyoung Chung", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "Procedings of ACL 2016, arXiv preprint arXiv:1603.06147.", "citeRegEx": "Chung et al\\.,? 2016", "shortCiteRegEx": "Chung et al\\.", "year": 2016}, {"title": "Very deep convolutional networks for natural language processing", "author": ["Alexis Conneau", "Holger Schwenk", "Lo\u0131\u0308c Barrault", "Yann Lecun"], "venue": "arXiv preprint arXiv:1606.01781", "citeRegEx": "Conneau et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Conneau et al\\.", "year": 2016}, {"title": "Minimal recursion semantics: An introduction", "author": ["Ann Copestake", "Dan Flickinger", "Ivan Sag", "Carl Pollard."], "venue": "Journal of Research on Language and Computation, 3(2\u20133):281\u2013332.", "citeRegEx": "Copestake et al\\.,? 2005", "shortCiteRegEx": "Copestake et al\\.", "year": 2005}, {"title": "Linguistically Motivated Large-Scale NLP with C&C and Boxer", "author": ["James Curran", "Stephen Clark", "Johan Bos."], "venue": "Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 33\u201336, Prague, Czech Republic.", "citeRegEx": "Curran et al\\.,? 2007", "shortCiteRegEx": "Curran et al\\.", "year": 2007}, {"title": "Learning character-level representations for part-ofspeech tagging", "author": ["C\u0131\u0301cero Nogueira dos Santos", "Bianca Zadrozny"], "venue": "In ICML,", "citeRegEx": "Santos and Zadrozny.,? \\Q2014\\E", "shortCiteRegEx": "Santos and Zadrozny.", "year": 2014}, {"title": "An introduction to the bootstrap", "author": ["Bradley Efron", "Robert J Tibshirani."], "venue": "CRC press.", "citeRegEx": "Efron and Tibshirani.,? 1994", "shortCiteRegEx": "Efron and Tibshirani.", "year": 1994}, {"title": "Elephant: Sequence labeling for word and sentence segmentation", "author": ["Kilian Evang", "Valerio Basile", "Grzegorz Chrupa\u0142a", "Johan Bos."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1422\u20131426.", "citeRegEx": "Evang et al\\.,? 2013", "shortCiteRegEx": "Evang et al\\.", "year": 2013}, {"title": "Multilingual language processing", "author": ["Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Identity mappings in deep residual networks", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1603.05027.", "citeRegEx": "He et al\\.,? 2016", "shortCiteRegEx": "He et al\\.", "year": 2016}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy."], "venue": "arXiv preprint arXiv:1502.03167.", "citeRegEx": "Ioffe and Szegedy.,? 2015", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "L\u00e9on Bottou", "Yoshua Bengio", "Patrick Haffner."], "venue": "Proceedings of the IEEE, 86(11):2278\u20132324.", "citeRegEx": "LeCun et al\\.,? 1998", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Building a Large Annotated Corpus of English: The Penn Treebank", "author": ["M.P. Marcus", "B. Santorini", "M.A. Marcinkiewicz."], "venue": "Computational Linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton."], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807\u2013814.", "citeRegEx": "Nair and Hinton.,? 2010", "shortCiteRegEx": "Nair and Hinton.", "year": 2010}, {"title": "Universal dependencies v1: A multilingual treebank collection", "author": ["Joakim Nivre", "Marie-Catherine de Marneffe", "Filip Ginter", "Yoav Goldberg", "Jan Hajic", "Christopher D Manning", "Ryan McDonald", "Slav Petrov", "Sampo Pyysalo", "Natalia Silveira"], "venue": "In Proceedings of the 10th International Conference on Language Resources and Evaluation", "citeRegEx": "Nivre et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2016}, {"title": "Morphological reinflection with convolutional neural networks", "author": ["Robert \u00d6stling."], "venue": "Proceedings of the 2016 Meeting of SIGMORPHON, Berlin, Germany. Association for Computational Linguistics.", "citeRegEx": "\u00d6stling.,? 2016", "shortCiteRegEx": "\u00d6stling.", "year": 2016}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg."], "venue": "Proceedings of ACL 2016, arXiv preprint arXiv:1604.05529.", "citeRegEx": "Plank et al\\.,? 2016", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "Shallow semantic parsing using support vector machines", "author": ["Sameer S Pradhan", "Wayne Ward", "Kadri Hacioglu", "James H Martin", "Daniel Jurafsky."], "venue": "HLT-NAACL, pages 233\u2013240.", "citeRegEx": "Pradhan et al\\.,? 2004", "shortCiteRegEx": "Pradhan et al\\.", "year": 2004}, {"title": "Whats in a p-value in nlp? In CoNLL-2014", "author": ["Anders S\u00f8gaard", "Anders Johannsen", "Barbara Plank", "Dirk Hovy", "Hector Martinez"], "venue": null, "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2014}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u2013 1958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Generating text with recurrent neural networks", "author": ["Ilya Sutskever", "James Martens", "Geoffrey E Hinton."], "venue": "Proceedings of the 28th International Conference on Machine Learning (ICML-11), pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich."], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20139.", "citeRegEx": "Szegedy et al\\.,? 2015", "shortCiteRegEx": "Szegedy et al\\.", "year": 2015}, {"title": "A unified tagging solution: Bidirectional lstm recurrent neural network with word embedding", "author": ["Peilu Wang", "Yao Qian", "Frank K Soong", "Lei He", "Hai Zhao."], "venue": "arXiv preprint arXiv:1511.00215.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Multi-task cross-lingual sequence tagging from scratch", "author": ["Zhilin Yang", "Ruslan Salakhutdinov", "William Cohen."], "venue": "arXiv preprint arXiv:1603.06270.", "citeRegEx": "Yang et al\\.,? 2016", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Character-level convolutional networks for text classification", "author": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "venue": "Advances in Neural Information Processing Systems, pages 649\u2013657.", "citeRegEx": "Zhang et al\\.,? 2015", "shortCiteRegEx": "Zhang et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 30, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 13, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 3, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 5, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 2, "context": "Many semantic parsing systems depend on sources of information such as POS tags (Pradhan et al., 2004; Copestake et al., 2005; Bos, 2008; Butler, 2010; Berant and Liang, 2014).", "startOffset": 80, "endOffset": 175}, {"referenceID": 25, "context": "However, these tags are often customised for the language at hand (Marcus et al., 1993) or massively abstracted, such as the Universal Dependencies tagset (Nivre et al.", "startOffset": 66, "endOffset": 87}, {"referenceID": 27, "context": ", 1993) or massively abstracted, such as the Universal Dependencies tagset (Nivre et al., 2016).", "startOffset": 75, "endOffset": 95}, {"referenceID": 21, "context": "This allows for the construction of much deeper networks, since keeping a \u2018clean\u2019 information path in the network facilitates optimisation (He et al., 2016).", "startOffset": 139, "endOffset": 156}, {"referenceID": 20, "context": "ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have", "startOffset": 88, "endOffset": 122}, {"referenceID": 21, "context": "ResNets have recently shown state-of-the-art performance for image classification tasks (He et al., 2015; He et al., 2016), and have", "startOffset": 88, "endOffset": 122}, {"referenceID": 28, "context": "also seen some recent use in NLP (\u00d6stling, 2016; Conneau et al., 2016).", "startOffset": 33, "endOffset": 70}, {"referenceID": 12, "context": "also seen some recent use in NLP (\u00d6stling, 2016; Conneau et al., 2016).", "startOffset": 33, "endOffset": 70}, {"referenceID": 25, "context": "The widely used Penn Treebank (PTB) Partof-Speech tagset (Marcus et al., 1993) does not make the necessary semantic distinctions, in addition to containing redundant information for semantic processing.", "startOffset": 57, "endOffset": 78}, {"referenceID": 3, "context": "Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010).", "startOffset": 76, "endOffset": 101}, {"referenceID": 5, "context": "Indeed some recent implementations of semantic parsing follow this strategy (Bos, 2008; Butler, 2010).", "startOffset": 76, "endOffset": 101}, {"referenceID": 14, "context": "This tagger uses POS and named entity tags available in the GMB (automatically obtained with the C&C tools (Curran et al., 2007) and then manually corrected), as well as a set of manually crafted rules to output semantic tags.", "startOffset": 107, "endOffset": 128}, {"referenceID": 17, "context": "For this dataset, we used the Elephant tokeniser, which performs word, multi-word and sentence segmentation (Evang et al., 2013).", "startOffset": 108, "endOffset": 128}, {"referenceID": 27, "context": "3 (Nivre et al., 2016).", "startOffset": 2, "endOffset": 22}, {"referenceID": 7, "context": "GRUs (Cho et al., 2014) are a recently introduced variant of RNNs, and are designed to prevent vanishing gradients, thus being able to cope with longer input sequences than vanilla RNNs.", "startOffset": 5, "endOffset": 23}, {"referenceID": 10, "context": "GRUs are similar to the more commonly-used Long Short-Term Memory networks (LSTMs), both in purpose and implementation (Chung et al., 2014).", "startOffset": 119, "endOffset": 139}, {"referenceID": 19, "context": "A bi-directional GRU is a GRU which makes both forward and backward passes over sequences, and can therefore use both preceding and succeeding contexts to predict a tag (Graves and Schmidhuber, 2005; Goldberg, 2015).", "startOffset": 169, "endOffset": 215}, {"referenceID": 35, "context": "Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016).", "startOffset": 150, "endOffset": 208}, {"referenceID": 36, "context": "Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016).", "startOffset": 150, "endOffset": 208}, {"referenceID": 29, "context": "Bi-directional GRUs and LSTMs have been shown to yield high performance on several NLP tasks, such as POS tagging, named entity tagging, and chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016).", "startOffset": 150, "endOffset": 208}, {"referenceID": 21, "context": "where xl and xl+1 are the input and output of the l-th layer,Wl is the weights for the l-th layer, and F is a residual function (He et al., 2016), e.", "startOffset": 128, "endOffset": 145}, {"referenceID": 20, "context": ", the identity function (He et al., 2015), which we also use in our experiments.", "startOffset": 24, "endOffset": 41}, {"referenceID": 20, "context": "ResNets have recently been found to yield impressive performance in image recognition tasks, with networks as deep as 1001 layers (He et al., 2015; He et al., 2016), and are thus an interesting and effective alternative to simply stacking layers.", "startOffset": 130, "endOffset": 164}, {"referenceID": 21, "context": "ResNets have recently been found to yield impressive performance in image recognition tasks, with networks as deep as 1001 layers (He et al., 2015; He et al., 2016), and are thus an interesting and effective alternative to simply stacking layers.", "startOffset": 130, "endOffset": 164}, {"referenceID": 20, "context": "where xl and xl+1 are the input and output of the l-th layer,Wl is the weights for the l-th layer, and F is a residual function (He et al., 2016), e.g., the identity function (He et al., 2015), which we also use in our experiments. ResNets can be intuitively understood by thinking of residual functions as paths through which information can propagate easily. This means that, in every layer, a ResNet learns more complex feature combinations, which it combines with the shallower representation from the previous layer. This architecture allows for the construction of much deeper networks. ResNets have recently been found to yield impressive performance in image recognition tasks, with networks as deep as 1001 layers (He et al., 2015; He et al., 2016), and are thus an interesting and effective alternative to simply stacking layers. In this paper we use the assymetric variant of ResNets as described in Equation 9 in He et al. (2016):", "startOffset": 129, "endOffset": 942}, {"referenceID": 28, "context": "ResNets have been very recently applied in NLP to morphological reinflection (\u00d6stling, 2016) and tasks such as sentiment analysis and text categorisation (Conneau et al.", "startOffset": 77, "endOffset": 92}, {"referenceID": 12, "context": "ResNets have been very recently applied in NLP to morphological reinflection (\u00d6stling, 2016) and tasks such as sentiment analysis and text categorisation (Conneau et al., 2016).", "startOffset": 154, "endOffset": 176}, {"referenceID": 33, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 9, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 37, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 11, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 18, "context": "Using sub-token representations instead of, or in combination with, word-level representations has recently obtained a lot of attention due to their effectiveness (Sutskever et al., 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015).", "startOffset": 163, "endOffset": 265}, {"referenceID": 9, "context": ", 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al.", "startOffset": 8, "endOffset": 179}, {"referenceID": 9, "context": ", 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations.", "startOffset": 8, "endOffset": 202}, {"referenceID": 9, "context": ", 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations. Gillick et al. (2015) similarly apply an LSTM-based model using byte-level information directly.", "startOffset": 8, "endOffset": 339}, {"referenceID": 9, "context": ", 2011; Chrupa\u0142a, 2013; Zhang et al., 2015; Chung et al., 2016; Gillick et al., 2015). The use of sub-token representations can be approached in several ways. Plank et al. (2016) and Yang et al. (2016) use a hierarchical bidirectional RNN, first passing over characters in order to create word-level representations. Gillick et al. (2015) similarly apply an LSTM-based model using byte-level information directly. Dos Santos and Zadrozny (2014) construct character-based word-level representations by running a convolutional network over the character representations of each word.", "startOffset": 8, "endOffset": 445}, {"referenceID": 24, "context": "A core intuition behind CNNs is the processing of an input signal in a hierarchical manner (LeCun et al., 1998; Goodfellow et al., 2016).", "startOffset": 91, "endOffset": 136}, {"referenceID": 34, "context": "We also implemented a variant of the Inception model (Szegedy et al., 2015), but found this to be outperformed by ResNets.", "startOffset": 53, "endOffset": 75}, {"referenceID": 8, "context": "Our system is implemented in Keras using the Tensorflow backend (Chollet, 2015; Abadi et al., 2016), and the code is available at https://github.", "startOffset": 64, "endOffset": 99}, {"referenceID": 1, "context": "We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings.", "startOffset": 39, "endOffset": 61}, {"referenceID": 10, "context": "Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014).", "startOffset": 60, "endOffset": 80}, {"referenceID": 1, "context": "We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings. Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014). We also experimented using a bi-LSTM. However, we found GRUs to yield comparatively better validation data performance on semtags. We also observe better validation data performance when running two consecutive forward and backward passes before concatenating the GRU layers, rather than concatenating after each forward/backward pass as is commonplace in NLP literature. We use CNNs for character-level modelling. Our basic CNN is inspired by dos Santos and Zadrozny (2014), who use character-representations to produce local features around each character of a word, and combine these with a maximum pooling operation in order to create fixed-size character-level word embeddings.", "startOffset": 40, "endOffset": 731}, {"referenceID": 1, "context": "We use the English Polyglot embeddings (Al-Rfou et al., 2013) in order to initialise the word embedding layer, but also experiment with randomly initialised word embeddings. Word embeddings are passed directly into a two-layer bi-GRU (Chung et al., 2014). We also experimented using a bi-LSTM. However, we found GRUs to yield comparatively better validation data performance on semtags. We also observe better validation data performance when running two consecutive forward and backward passes before concatenating the GRU layers, rather than concatenating after each forward/backward pass as is commonplace in NLP literature. We use CNNs for character-level modelling. Our basic CNN is inspired by dos Santos and Zadrozny (2014), who use character-representations to produce local features around each character of a word, and combine these with a maximum pooling operation in order to create fixed-size character-level word embeddings. The convolutions used in this manner cover a few neighbouring letters at a time, as well as the entire character vector dimension (dc). In contrast to dos Santos and Zadrozny (2014), we treat a word analogously to an image.", "startOffset": 40, "endOffset": 1121}, {"referenceID": 6, "context": "Cheng et al. (2015) use a language modelling task as an auxiliary loss, as they attempt to predict the next token while performing named entity recognition.", "startOffset": 0, "endOffset": 20}, {"referenceID": 6, "context": "Cheng et al. (2015) use a language modelling task as an auxiliary loss, as they attempt to predict the next token while performing named entity recognition. Plank et al. (2016) use the log frequency of the current token as an auxiliary loss function, and find this to improve POS tagging accuracy.", "startOffset": 0, "endOffset": 177}, {"referenceID": 26, "context": "We use rectified linear units (ReLUs) for all activation functions (Nair and Hinton, 2010), and apply dropout with p = 0.", "startOffset": 67, "endOffset": 90}, {"referenceID": 32, "context": "1 to both input weights and recurrent weights in the bi-GRU (Srivastava et al., 2014).", "startOffset": 60, "endOffset": 85}, {"referenceID": 22, "context": "In the CNNs, we apply batch normalisation (Ioffe and Szegedy, 2015) followed by dropout with p = 0.", "startOffset": 42, "endOffset": 67}, {"referenceID": 23, "context": "Optimisation is done using the ADAM algorithm (Kingma and Ba, 2014), with the categorical cross-entropy loss function as training objective.", "startOffset": 46, "endOffset": 67}, {"referenceID": 16, "context": "We calculate significance using bootstrap resampling (Efron and Tibshirani, 1994).", "startOffset": 53, "endOffset": 81}, {"referenceID": 4, "context": "the trigram statistic based TNT tagger offers a slightly tougher baseline (Brants, 2000);", "startOffset": 74, "endOffset": 88}, {"referenceID": 29, "context": "the BI-LSTM baseline, running the off-the-shelf state-of-the-art POS tagger for the UD dataset (Plank et al., 2016) (using default parameters with pre-trained Polyglot embeddings);", "startOffset": 95, "endOffset": 115}, {"referenceID": 29, "context": "MFC indicates the per-word most frequent class baseline, TNT indicates the TNT tagger, and BI-LSTM indicates the system by Plank et al. (2016). BI-GRU indicates the ~ w only baseline.", "startOffset": 123, "endOffset": 143}, {"referenceID": 31, "context": "On the ST gold data, we observe significant differences at the alpha values recommended by S\u00f8gaard et al. (2014), with p < 0.", "startOffset": 91, "endOffset": 113}, {"referenceID": 29, "context": ", the fact that even though our ~c\u2227 ~ w ResNet system outperforms the Plank et al. (2016) system on semtags, we are substantially outperformed on UD 1.", "startOffset": 70, "endOffset": 90}], "year": 2016, "abstractText": "We propose a novel semantic tagging task, semtagging, tailored for the purpose of multilingual semantic parsing, and present the first tagger using deep residual networks (ResNets). Our tagger uses both word and character representations and includes a novel residual bypass architecture. We evaluate the tagset both intrinsically on the new task of semantic tagging, as well as on Part-of-Speech (POS) tagging. Our system, consisting of a ResNet and an auxiliary loss function predicting our semantic tags, significantly outperforms prior results on English Universal Dependencies POS tagging (95.71% accuracy on UD v1.2 and 95.67% accuracy on UD v1.3).", "creator": "TeX"}}}