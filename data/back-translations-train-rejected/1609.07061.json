{"id": "1609.07061", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations", "abstract": "We introduce a method to train Quantized Neural Networks (QNNs) --- neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At train-time the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves $51\\%$ top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online.", "histories": [["v1", "Thu, 22 Sep 2016 16:48:03 GMT  (95kb,D)", "http://arxiv.org/abs/1609.07061v1", "arXiv admin note: text overlap witharXiv:1602.02830"]], "COMMENTS": "arXiv admin note: text overlap witharXiv:1602.02830", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["itay hubara", "matthieu courbariaux", "daniel soudry", "ran el-yaniv", "yoshua bengio"], "accepted": false, "id": "1609.07061"}, "pdf": {"name": "1609.07061.pdf", "metadata": {"source": "CRF", "title": "Quantized Neural Networks Quantized Neural Networks: Training Neural Networks with Low Precision Weights and Activations", "authors": ["Itay Hubara", "Daniel Soudry", "Yoshua Bengio"], "emails": ["itayh@campuse.technion.ac.il", "matthieu.courbariaux@gmail.com", "daniel.soudry@gmail.com", "rani@cs.technion.ac.il", "yoshua.umontreal@gmail.com"], "sections": [{"heading": null, "text": "ar Xiv: 1Keywords: Deep Learning, Neural Networks Compression, Energy Efficient Neural Networks, Computer Vision, Language Models."}, {"heading": "1. Introduction", "text": "This year, it has reached the point where it will be able to take the lead at a time when it is not as far away as it has been in the past."}, {"heading": "2. Binarized Neural Networks", "text": "In this section, we explain our binarization function, how we use it to calculate the parameter histories and how we propagate through them backward.1https: / / github.com / MatthieuCourbariaux / BinaryNet 2https: / / github.com / itayhubara / BinaryNet"}, {"heading": "2.1 Deterministic vs Stochastic Binarization", "text": "When creating a BNN, we limit both the weights and the activations to either + 1 or \u2212 1. These two values are very beneficial from a hardware perspective, as we explain in Section 6. To transform the real variables into these two values, we use two different binarization functions, as suggested by Courbariaux et al. (2015a). The first binarization function is deterministic: xb = characters (x) = {+ 1 if x \u2265 0, \u2212 1 otherwise, (1) where xb is the binarized variable (weight or activation) and x is the real variable. It is very easy to implement and works quite well in practice. The second binarization function is stochastic: xb = {+ 1 with probability p = Celsius (x), \u2212 1 with probability 1 \u2212 p, (2) with binarization functions is the \"hard sigmoid\" function."}, {"heading": "2.2 Gradient Computation and Accumulation", "text": "Although our BNN training method uses binary weights and activations to calculate the parameter gradients, the real gradients of the weights are accumulated in real variables according to algorithm 1. Real weights are probably required for Stochasic Gradient Descent (SGD) to function at all. SGD explores the space of parameters in small and loud steps, and the noise is averaged by the stochastic gradient contributions of each weight. Therefore, it is important to maintain sufficient resolution for these accumulators, which at first glance suggests that high precision is absolutely necessary. Furthermore, adding noise to weights and activations in the calculation of parameter gradients provides a form of regulation that can help achieve better generalization, as previously demonstrated with variable weight noise (Graves, 2011), dropout (Srivastava et al., 2014 and Dropan, 2013)."}, {"heading": "2.3 Propagating Gradients Through Discretization", "text": "The derivative of the drawing function is almost the same everywhere, which apparently makes it incompatible with backward propagation = > q = | since the exact gradients of the cost in relation to the quantities prior to discretization (pre-activation or weights) are zero. Note that this constraint remains even when stochastic quantization is used (Hinton, 2012). We take a similar approach, but use the continuous estimator version, which takes into account the saturation effect, and use deterministic rather than stochastic scanning of the bit. Consider the drawing function quantizationq = sign (r) and assume that an estimator gq of the gradient Q."}, {"heading": "2.4 Shift-based Batch Normalization", "text": "The BN standardization (BN) (Ioffe and Szegedy, 2015) speeds up training and reduces the overall impact of the weight scale (Courbariaux et al., 2015a). The normalization process can also help regulate the model. Although the number of scales is identical to the number of neurons (in the case of ConvNets, this number is quite large, namely to be divided by the running variance, which is the weighted mean of the activation variance).Although the number of scales is the same as the number of neurons, the number is quite large in the case of ConvNets. For example, in the CIFAR-10 Dataset (using our architecture), the first convolution layer, consisting of only 128 x 3 filter masks, converts an image of the size 3 x."}, {"heading": "2.5 Shift Based AdaMax", "text": "The ADAM learning method (Kingma and Ba, 2014b) also reduces the impact of the weight scale (each using Normalize). Since ADAM requires many multiplications, we suggest using the shift-based AdaMax factor instead, which we outlined in Algorithm 3. In the experiment, we observed no loss of accuracy when using the shift-based AdaMax algorithm instead of the vanilla-based ADAM algorithm."}, {"heading": "2.6 First Layer", "text": "In a BNN, only the binary values of weights and activations are used in all calculations. As the output of one layer is the input of the next, the inputs of all layers are binary, except the first layer. Consequently, the first layer of a ConvNet is often the smallest folding layer, both in terms of parameters and calculations (Szegedy et al., 2014). Second, it is relatively easy to handle continuous inputs as fixed point numbers, with m bits of precision. For example, the first layer of a ConvNet is often the smallest folding layer, both in terms of parameters and calculations (Szegedy et al., 2014)."}, {"heading": "3. Qunatized Neural network - More than 1-bit", "text": "If we look at Equation (8), we can see that the use of 2-bit activations simply doubles the number of times we need to run our XnorPopCount kernel (i.e. directly proportional to the activation bit width). However, this idea was recently proposed by Zhou et al. (2016) (DoReFa net) and Miyashita et al. (2016) (shortly after our preliminary technical report was published there on arXive). However, unlike Zhou et al., we did not find it useful to initialize the network with weights obtained by training the network with full precision. Furthermore, the Zhou et al network did not quantify the weights of the first revolutionary layer and the last fully connected layer, while binarizing both. We followed the quantization schemes proposed by Miyashita et al al al al al al al al. (2016), namely linear quantization: Quantization (Quantification, Quantification, Minimum, Maximum Width, Bitwidth, Clip) (quantification) (quantification) V (quantification)."}, {"heading": "4. Benchmark Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Results on MNIST,SVHN, and CIFAR-10", "text": "We conducted two sets of experiments, each based on a different frame, namely Torch7 and Theano. Besides the frame, the two sets of experiments are very similar: MNIST MNIST is a benchmark image classification dataset (LeCun et al., 1998). It consists of a training set of 60K and a test set of 10K 28 x 28 grayscale images, the digits from 0 to 9. The Multi Layer Perceptron (MLP) we train on MNIST consists of 3 hidden layers. In our Theano implementation, we used hidden layers of size 4096, while in our Torch implementation we used much smaller size 2048. This difference explains the accuracy gap between the two implementations.CIFAR-10 CIFAR-10 is an image classification benchmark for data sets. It consists of a training set of size 50K and a test set of size 10 trucks representing 10K for aircrafts, and an SVAR for aircrafts."}, {"heading": "4.2 Results on ImageNet", "text": "To test the strength of our method, we applied it to the challenging ImageNet classification task, which is probably the most important benchmark for classification. It consists of a training set of 1.2 M samples and a test set of 50K size. Each instance is labeled with one of 1,000 categories, including objects, animals, scenes, and even some abstract shapes. On ImageNet, it is common to report two error rates: top 1 and top 5, with the top x error rate being the fraction of test images where the correct label is not among the X labels considered most likely by the model. Major research results deal with compressing ImageNet architectures while maintaining high accuracy. Previous approaches include pruning near-zero weights (Gong et al., 2014; Han et al., 2015a) using matrix factor techniques while maintaining high accuracy. Previous approaches include pruning near-zero weights (Gong et al., 2014; Han et al., 2015a) using matrix factor techniques while maintaining high accuracy."}, {"heading": "4.3 Relaxing \u201chard tanh\u201d boundaries", "text": "As explained in Section 2.3, the straight-through estimator (which can be written as \"hard tanh\") removes gradients originating from neurons with absolute values greater than 1. Therefore, most gradients are zero during the last training sessions and the weight values do not update. By loosening the \"hard tanh\" boundaries, we allow more gradients to flow during the backpropagation phase and improve top-1 accuracy on AlexNet topology by 1.5% using vanilla BNN."}, {"heading": "4.4 2-bit activations", "text": "While we were training the BNNs on the ImageNet dataset, we noticed that we could not force the error rate during the training to converge to zero. In fact, the error rate during the training remained pretty close to the validation error rate. This observation prompted us to investigate a more relaxed activation quantification (more than 1-bit), but as can be seen in Table 2, the results are quite impressive, illustrating an approximately 5.6% drop in performance (top-1 accuracy) compared to floating-point representation, using only 1-bit weights and 2-bit activation. Following Miyashita et al. (2016), we also attempted to quantify the gradients and discovered that only logarithmic quantification works. With 6-bit gradients, we achieved a degradation of 46.8%. These results are state-of-the-art under-pressure and exceed those achieved by the Doschou network (in contrast to Zhou 6), which is the highest value achieved in 2016."}, {"heading": "4.5 Language Models", "text": "Recursive neural networks (RNNs) are very demanding in memory and computing power compared to forward networks. However, there is a wide variety of recursive models using the Long Short Term Memory Networks (LSTMs) introduced by Hochreiter and Schmidhuber (1997). LSTMs are a special type of RNN that are able to learn long-term dependencies using unique gating mechanisms. Recently, Ott et al. (2016) attempted to quantify the RNNs weight matrices using techniques similar to those described in Section 2. They observed that the weight binding methods do not work with RNNNs. However, using 2-bits (i.e. \u2212 1, 0, 1), they were able to achieve similar and even higher accuracy on several data sets. Here, we report the first attempt to quantify both weight and activation modes."}, {"heading": "5. High Power Efficiency during the Forward Pass", "text": "It is only a matter of time before it will happen, until it will happen."}, {"heading": "6. Seven Times Faster on GPU at Run-Time", "text": "The basic idea of SWAR is to concatenate groups of 32 binary variables in 32-bit registers to achieve a 32-fold acceleration of bitwise operations (e.g. XNOR). With SWAR it is possible to evaluate 32 connections with only 3 statements: a1 + = Popcount (xnor (a 32b 0, w 32b 1), (11) where a1 is the resulting weighted sum, and a 32 b 0 and w 32b 1 are the concatenated inputs and weights. These 3 statements (accumulation, popcount, xnor) take 1 + 4 = 6 clock cycles on the most recent Nvidia GPUs (and if they become a fused statement, there will be only one clock cycle)."}, {"heading": "7. Discussion and Related Work", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "Acknowledgments", "text": "We thank Elad Hoffer for his technical support and constructive comments. We thank our other MILA lab members who took the time to read the article and give us feedback. We thank the developers of Torch (Collobert et al., 2011), a Lua-based environment, and Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library that enabled us to easily develop fast and optimized code for GPU. We also thank the developers of Pylearn2 (Goodfellow et al., 2013a) and Lasagne (Dieleman et al., 2015), two Theano-based deep learning libraries. We thank Yuxin Wu for helping us compare our GPU cores with cuBLAS. We are also grateful for funding from NSERC, the Canada Research Chairs, Compute Canada, and CIFAR. We are also grateful for the support of The IBC, The NSC, This IAR, This Science."}, {"heading": "Appendix A. Implementation Details", "text": "In this year we are dealing with a number of countries in which people are able to put themselves in another world, in which they put themselves in another world, in which they put themselves in another world, in which they find themselves in another world, in which they find their way in another world, in which they find their way in another world, in which they live in another world, in which they live in another world, in which they live themselves in another world, in which they find their way in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they live, that they are, that they live, that they are living, that they live, that they live, that they live, that they live, that they are, that they are living, that they are, that they are, that they are, that they are, that they are, that they are, that they are, that they are, that they are, that they are, that they are, that they are, that they are, that they are in this year, that year, that year, that year, that year."}], "references": [{"title": "Yodann: An ultra-low power convolutional neural network accelerator based on binary weights", "author": ["Renzo Andri", "Lukas Cavigelli", "Davide Rossi", "Luca Benini"], "venue": "arXiv preprint arXiv:1606.05487,", "citeRegEx": "Andri et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Andri et al\\.", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In ICLR\u20192015,", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Subdominant Dense Clusters Allow for Simple Learning and High Computational Performance in Neural Networks with Discrete Synapses", "author": ["Carlo Baldassi", "Alessandro Ingrosso", "Carlo Lucibello", "Luca Saglietti", "Riccardo Zecchina"], "venue": "Physical Review Letters,", "citeRegEx": "Baldassi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Baldassi et al\\.", "year": 2015}, {"title": "Embedded floating-point units in FPGAs", "author": ["Michael J Beauchamp", "Scott Hauck", "Keith D Underwood", "K Scott Hemmert"], "venue": "In Proceedings of the 2006 ACM/SIGDA 14th international symposium on Field programmable gate arrays,", "citeRegEx": "Beauchamp et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Beauchamp et al\\.", "year": 2006}, {"title": "Estimating or propagating gradients through stochastic neurons", "author": ["Yoshua Bengio"], "venue": "Technical Report arXiv:1305.2982, Universite de Montreal,", "citeRegEx": "Bengio.,? \\Q2013\\E", "shortCiteRegEx": "Bengio.", "year": 2013}, {"title": "Theano: a CPU and GPU math expression compiler", "author": ["James Bergstra", "Olivier Breuleux", "Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "Guillaume Desjardins", "Joseph Turian", "David Warde-Farley", "Yoshua Bengio"], "venue": "In Proceedings of the Python for Scientific Computing Conference (SciPy),", "citeRegEx": "Bergstra et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Bergstra et al\\.", "year": 2010}, {"title": "Diannao: A small-footprint high-throughput accelerator for ubiquitous machinelearning", "author": ["Tianshi Chen", "Zidong Du", "Ninghui Sun", "Jia Wang", "Chengyong Wu", "Yunji Chen", "Olivier Temam"], "venue": "In Proceedings of the 19th international conference on Architectural support for programming languages and operating systems,", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Compressing neural networks with the hashing trick", "author": ["Wenlin Chen", "James T Wilson", "Stephen Tyree", "Kilian Q Weinberger", "Yixin Chen"], "venue": "arXiv preprint arXiv:1504.04788,", "citeRegEx": "Chen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2015}, {"title": "Dadiannao: A machine-learning supercomputer", "author": ["Yunji Chen", "Tao Luo", "Shaoli Liu", "Shijin Zhang", "Liqiang He", "Jia Wang", "Ling Li", "Tianshi Chen", "Zhiwei Xu", "Ninghui Sun"], "venue": "In Microarchitecture (MICRO),", "citeRegEx": "Chen et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2014}, {"title": "Training binary multilayer neural networks for image classification using expectation backpropgation", "author": ["Zhiyong Cheng", "Daniel Soudry", "Zexi Mao", "Zhenzhong Lan"], "venue": "arXiv preprint arXiv:1503.03562,", "citeRegEx": "Cheng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cheng et al\\.", "year": 2015}, {"title": "Deep learning with COTS HPC systems", "author": ["Adam Coates", "Brody Huval", "Tao Wang", "David Wu", "Bryan Catanzaro", "Ng Andrew"], "venue": "In Proceedings of the 30th international conference on machine learning,", "citeRegEx": "Coates et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Coates et al\\.", "year": 2013}, {"title": "Torch7: A matlab-like environment for machine learning", "author": ["Ronan Collobert", "Koray Kavukcuoglu", "Cl\u00e9ment Farabet"], "venue": "In BigLearn, NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Training deep neural networks with low precision multiplications", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "ArXiv e-prints,", "citeRegEx": "Courbariaux et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2014}, {"title": "Binaryconnect: Training deep neural networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "ArXiv e-prints,", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "BinaryConnect: Training Deep Neural Networks with binary weights during propagations", "author": ["Matthieu Courbariaux", "Yoshua Bengio", "Jean-Pierre David"], "venue": "Nips, pages", "citeRegEx": "Courbariaux et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Courbariaux et al\\.", "year": 2015}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["Jacob Devlin", "Rabih Zbib", "Zhongqiang Huang", "Thomas Lamar", "Richard Schwartz", "John Makhoul"], "venue": "In Proc. ACL\u20192014,", "citeRegEx": "Devlin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Devlin et al\\.", "year": 2014}, {"title": "Lasagne: First release", "author": ["Sander Dieleman", "Jan Schlter", "Colin Raffel", "Eben Olson", "Sren Kaae Snderby", "Daniel Nouri", "Daniel Maturana", "Martin Thoma", "Eric Battenberg", "Jack Kelly", "Jeffrey De Fauw", "Michael Heilman", "diogo", "Brian McFee", "Hendrik Weideman", "takacsg", "peterderivaz", "Jon", "instagibbs", "Dr. Kashif Rasul", "CongLiu", "Britefury", "Jonas Degrave"], "venue": "URL http://dx.doi.org/10.5281/zenodo.27878", "citeRegEx": "Dieleman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dieleman et al\\.", "year": 2015}, {"title": "Backpropagation for energy-efficient neuromorphic computing", "author": ["Steve K Esser", "Rathinakumar Appuswamy", "Paul Merolla", "John V Arthur", "Dharmendra S Modha"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Esser et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Esser et al\\.", "year": 2015}, {"title": "Large-scale FPGA-based convolutional networks", "author": ["Cl\u00e9ment Farabet", "Yann LeCun", "Koray Kavukcuoglu", "Eugenio Culurciello", "Berin Martini", "Polina Akselrod", "Selcuk Talay"], "venue": "Machine Learning on Very Large Data Sets,", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Neuflow: A runtime reconfigurable dataflow processor for vision", "author": ["Cl\u00e9ment Farabet", "Berin Martini", "Benoit Corda", "Polina Akselrod", "Eugenio Culurciello", "Yann LeCun"], "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "citeRegEx": "Farabet et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Farabet et al\\.", "year": 2011}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "In AISTATS\u20192010,", "citeRegEx": "Glorot and Bengio.,? \\Q2010\\E", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Compressing deep convolutional networks using vector quantization", "author": ["Yunchao Gong", "Liu Liu", "Ming Yang", "Lubomir Bourdev"], "venue": "arXiv preprint arXiv:1412.6115,", "citeRegEx": "Gong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Gong et al\\.", "year": 2014}, {"title": "Pylearn2: a machine learning research", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Pascal Lamblin", "Vincent Dumoulin", "Mehdi Mirza", "Razvan Pascanu", "James Bergstra", "Fr\u00e9d\u00e9ric Bastien", "Yoshua Bengio"], "venue": "library. arXiv preprint arXiv:1308.4214,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "URL http://arxiv.org/ abs/1302.4389", "author": ["Ian J. Goodfellow", "David Warde-Farley", "Mehdi Mirza", "Aaron Courville", "Yoshua Bengio"], "venue": "Maxout Networks. arXiv preprint,", "citeRegEx": "Goodfellow et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Goodfellow et al\\.", "year": 2013}, {"title": "Analysis of high-performance floating-point arithmetic on FPGAs", "author": ["Gokul Govindu", "Ling Zhuo", "Seonil Choi", "Viktor Prasanna"], "venue": "In Parallel and Distributed Processing Symposium,", "citeRegEx": "Govindu et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Govindu et al\\.", "year": 2004}, {"title": "Spatially-sparse convolutional neural networks", "author": ["Benjamin Graham"], "venue": "arXiv preprint arXiv:1409.6070,", "citeRegEx": "Graham.,? \\Q2014\\E", "shortCiteRegEx": "Graham.", "year": 2014}, {"title": "Practical variational inference for neural networks", "author": ["Alex Graves"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Graves.,? \\Q2011\\E", "shortCiteRegEx": "Graves.", "year": 2011}, {"title": "Deep learning with limited numerical precision", "author": ["Suyog Gupta", "Ankur Agrawal", "Kailash Gopalakrishnan", "Pritish Narayanan"], "venue": "CoRR, abs/1502.02551,", "citeRegEx": "Gupta et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gupta et al\\.", "year": 2015}, {"title": "Hardware-oriented approximation of convolutional neural networks", "author": ["Philipp Gysel", "Mohammad Motamedi", "Soheil Ghiasi"], "venue": "arXiv preprint arXiv:1604.03168,", "citeRegEx": "Gysel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Gysel et al\\.", "year": 2016}, {"title": "Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding", "author": ["Huizi Mao Han", "Song", "William J. Dally"], "venue": "arXiv preprint,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding", "author": ["Song Han", "Huizi Mao", "William J Dally"], "venue": "arXiv preprint arXiv:1510.00149,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Learning both weights and connections for efficient neural network", "author": ["Song Han", "Jeff Pool", "John Tran", "William Dally"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Han et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Han et al\\.", "year": 2015}, {"title": "Neural networks for machine learning", "author": ["Geoffrey Hinton"], "venue": "Coursera, video lectures,", "citeRegEx": "Hinton.,? \\Q2012\\E", "shortCiteRegEx": "Hinton.", "year": 2012}, {"title": "Deep neural networks for acoustic modeling in speech recognition", "author": ["Geoffrey Hinton", "Li Deng", "George E. Dahl", "Abdel-rahman Mohamed", "Navdeep Jaitly", "Andrew Senior", "Vincent Vanhoucke", "Patrick Nguyen", "Tara Sainath", "Brian Kingsbury"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Computing\u2019s Energy Problem (and what we can do about it)", "author": ["Mark Horowitz"], "venue": "IEEE Interational Solid State Circuits Conference,", "citeRegEx": "Horowitz.,? \\Q2014\\E", "shortCiteRegEx": "Horowitz.", "year": 2014}, {"title": "Fixed-point feedforward deep neural network design using weights+", "author": ["Kyuyeon Hwang", "Wonyong Sung"], "venue": "In Signal Processing Systems (SiPS),", "citeRegEx": "Hwang and Sung.,? \\Q2014\\E", "shortCiteRegEx": "Hwang and Sung.", "year": 2014}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Bitwise Neural Networks", "author": ["M. Kim", "P. Smaragdis"], "venue": "ArXiv e-prints,", "citeRegEx": "Kim and Smaragdis.,? \\Q2016\\E", "shortCiteRegEx": "Kim and Smaragdis.", "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "[cs], pages 1\u201313,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "ImageNet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "In NIPS\u20192012", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Yann LeCun", "Leon Bottou", "Yoshua Bengio", "Patrick Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}, {"title": "Generalizing pooling functions in convolutional neural networks: Mixed", "author": ["Chen-Yu Lee", "Patrick W Gallagher", "Zhuowen Tu"], "venue": "gated, and tree. arXiv preprint arXiv:1509.08985,", "citeRegEx": "Lee et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lee et al\\.", "year": 2015}, {"title": "Neural networks with few multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "ArXiv e-prints,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Neural Networks with Few Multiplications", "author": ["Zhouhan Lin", "Matthieu Courbariaux", "Roland Memisevic", "Yoshua Bengio"], "venue": "Iclr, pages 1\u20138,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Fast inverse square root", "author": ["Chris Lomont"], "venue": "Tech-315 nical Report,", "citeRegEx": "Lomont.,? \\Q2003\\E", "shortCiteRegEx": "Lomont.", "year": 2003}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini"], "venue": "Computational linguistics,", "citeRegEx": "Marcus et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Deep neural networks are robust to weight binarization and other non-linear distortions", "author": ["Paul Merolla", "Rathinakumar Appuswamy", "John Arthur", "Steve K Esser", "Dharmendra Modha"], "venue": null, "citeRegEx": "Merolla et al\\.,? \\Q1981\\E", "shortCiteRegEx": "Merolla et al\\.", "year": 1981}, {"title": "Context dependent recurrent neural network language model", "author": ["Tomas Mikolov", "Geoffrey Zweig"], "venue": "In SLT,", "citeRegEx": "Mikolov and Zweig.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov and Zweig.", "year": 2012}, {"title": "Convolutional neural networks using logarithmic data representation", "author": ["Daisuke Miyashita", "Edward H Lee", "Boris Murmann"], "venue": "arXiv preprint arXiv:1603.01025,", "citeRegEx": "Miyashita et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Miyashita et al\\.", "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["Volodymyr Mnih", "Koray Kavukcuoglo", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidgeland", "Georg Ostrovski", "Stig Petersen", "Charles Beattie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharsan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "venue": "Nature, 518:529\u2013533,", "citeRegEx": "Mnih et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2015}, {"title": "Recurrent neural networks with limited numerical precision", "author": ["Joachim Ott", "Zhouhan Lin", "Ying Zhang", "Shih-Chii Liu", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1608.06902,", "citeRegEx": "Ott et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ott et al\\.", "year": 2016}, {"title": "Neuflow: Dataflow vision processing system-on-a-chip", "author": ["Phi-Hung Pham", "Darko Jelaca", "Clement Farabet", "Berin Martini", "Yann LeCun", "Eugenio Culurciello"], "venue": "In Circuits and Systems (MWSCAS),", "citeRegEx": "Pham et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2012}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "author": ["Mohammad Rastegari", "Vicente Ordonez", "Joseph Redmon", "Ali Farhadi"], "venue": "arXiv preprint arXiv:1603.05279,", "citeRegEx": "Rastegari et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rastegari et al\\.", "year": 2016}, {"title": "Fitnets: Hints for thin deep nets", "author": ["Adriana Romero", "Nicolas Ballas", "Samira Ebrahimi Kahou", "Antoine Chassang", "Carlo Gatta", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.6550,", "citeRegEx": "Romero et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Romero et al\\.", "year": 2014}, {"title": "Deep convolutional neural networks for LVCSR", "author": ["Tara Sainath", "Abdel rahman Mohamed", "Brian Kingsbury", "Bhuvana Ramabhadran"], "venue": null, "citeRegEx": "Sainath et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sainath et al\\.", "year": 2013}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "In ICLR,", "citeRegEx": "Simonyan and Zisserman.,? \\Q2015\\E", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2015}, {"title": "Expectation backpropagation: Parameter-free training of multilayer neural networks with continuous or discrete weights", "author": ["Daniel Soudry", "Itay Hubara", "Ron Meir"], "venue": "In NIPS\u20192014,", "citeRegEx": "Soudry et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Soudry et al\\.", "year": 2014}, {"title": "Reduction of quantizing noise by use of feedback", "author": ["H Spang", "P Schultheiss"], "venue": "IRE Transactions on Communications Systems,", "citeRegEx": "Spang and Schultheiss.,? \\Q1962\\E", "shortCiteRegEx": "Spang and Schultheiss.", "year": 1962}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Srivastava et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["Ilya Sutskever", "Oriol Vinyals", "Quoc V. Le"], "venue": "In NIPS\u20192014,", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "Technical report,", "citeRegEx": "Szegedy et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Szegedy et al\\.", "year": 2014}, {"title": "Deep learning using linear support vector machines", "author": ["Yichuan Tang"], "venue": "Workshop on Challenges in Representation Learning,", "citeRegEx": "Tang.,? \\Q2013\\E", "shortCiteRegEx": "Tang.", "year": 2013}, {"title": "Asic implementation of random number generators using sr latches and its evaluation", "author": ["Naoya Torii", "Hirotaka Kokubo", "Dai Yamamoto", "Kouichi Itoh", "Masahiko Takenaka", "Tsutomu Matsumoto"], "venue": "EURASIP Journal on Information Security,", "citeRegEx": "Torii et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Torii et al\\.", "year": 2016}, {"title": "Improving the speed of neural networks on CPUs", "author": ["Vincent Vanhoucke", "Andrew Senior", "Mark Z Mao"], "venue": "In Proc. Deep Learning and Unsupervised Feature Learning NIPS Workshop,", "citeRegEx": "Vanhoucke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Vanhoucke et al\\.", "year": 2011}, {"title": "Regularization of neural networks using dropconnect", "author": ["Li Wan", "Matthew Zeiler", "Sixin Zhang", "Yann LeCun", "Rob Fergus"], "venue": "In ICML\u20192013,", "citeRegEx": "Wan et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Wan et al\\.", "year": 2013}, {"title": "Efficient and accurate approximations of nonlinear convolutional networks", "author": ["Xiangyu Zhang", "Jianhua Zou", "Xiang Ming", "Kaiming He", "Jian Sun"], "venue": null, "citeRegEx": "Zhang et al\\.,? \\Q1984\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 1984}, {"title": "Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients", "author": ["Shuchang Zhou", "Zekun Ni", "Xinyu Zhou", "He Wen", "Yuxin Wu", "Yuheng Zou"], "venue": "arXiv preprint arXiv:1606.06160,", "citeRegEx": "Zhou et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 41, "context": "Deep Neural Networks (DNNs) have substantially pushed Artificial Intelligence (AI) limits in a wide range of tasks, including but not limited to object recognition from images (Krizhevsky et al., 2012; Szegedy et al., 2014), speech recognition (Hinton et al.", "startOffset": 176, "endOffset": 223}, {"referenceID": 62, "context": "Deep Neural Networks (DNNs) have substantially pushed Artificial Intelligence (AI) limits in a wide range of tasks, including but not limited to object recognition from images (Krizhevsky et al., 2012; Szegedy et al., 2014), speech recognition (Hinton et al.", "startOffset": 176, "endOffset": 223}, {"referenceID": 33, "context": ", 2014), speech recognition (Hinton et al., 2012; Sainath et al., 2013), statistical machine translation (Devlin et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 56, "context": ", 2014), speech recognition (Hinton et al., 2012; Sainath et al., 2013), statistical machine translation (Devlin et al.", "startOffset": 28, "endOffset": 71}, {"referenceID": 15, "context": ", 2013), statistical machine translation (Devlin et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 61, "context": ", 2013), statistical machine translation (Devlin et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 1, "context": ", 2013), statistical machine translation (Devlin et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al.", "startOffset": 41, "endOffset": 109}, {"referenceID": 51, "context": ", 2015), Atari and Go games (Mnih et al., 2015; Silver et al., 2016), and even computer generation of abstract art (Mordvintsev et al.", "startOffset": 28, "endOffset": 68}, {"referenceID": 10, "context": "Today, DNNs are almost exclusively trained on one or many very fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013).", "startOffset": 117, "endOffset": 138}, {"referenceID": 65, "context": "As a result, it is often a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in speeding up DNNs at run-time on both generalpurpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al.", "startOffset": 179, "endOffset": 262}, {"referenceID": 21, "context": "As a result, it is often a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in speeding up DNNs at run-time on both generalpurpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al.", "startOffset": 179, "endOffset": 262}, {"referenceID": 55, "context": "As a result, it is often a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in speeding up DNNs at run-time on both generalpurpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al.", "startOffset": 179, "endOffset": 262}, {"referenceID": 53, "context": ", 2015b) and specialized computer hardware (Farabet et al., 2011a,b; Pham et al., 2012; Chen et al., 2014a,b; Esser et al., 2015).", "startOffset": 43, "endOffset": 129}, {"referenceID": 17, "context": ", 2015b) and specialized computer hardware (Farabet et al., 2011a,b; Pham et al., 2012; Chen et al., 2014a,b; Esser et al., 2015).", "startOffset": 43, "endOffset": 129}, {"referenceID": 7, "context": "HashedNets (Chen et al., 2015) reduce model sizes by using a hash function to randomly group connection weights and force them to share a single parameter value.", "startOffset": 11, "endOffset": 30}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al., 2015; Silver et al., 2016), and even computer generation of abstract art (Mordvintsev et al., 2015). Training or even just using neural network (NN) algorithms on conventional generalpurpose digital hardware (Von Neumann architecture) has been found highly inefficient due to the massive amount of multiply-accumulate operations (MACs) required to compute the weighted sums of the neurons\u2019 inputs. Today, DNNs are almost exclusively trained on one or many very fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013). As a result, it is often a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in speeding up DNNs at run-time on both generalpurpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al., 2011a,b; Pham et al., 2012; Chen et al., 2014a,b; Esser et al., 2015). The most common approach is to compress a trained (full precision) network. HashedNets (Chen et al., 2015) reduce model sizes by using a hash function to randomly group connection weights and force them to share a single parameter value. Gong et al. (2014) compressed deep convnets using vector quantization, which resulteds in only a 1% accuracy loss.", "startOffset": 8, "endOffset": 1245}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al., 2015; Silver et al., 2016), and even computer generation of abstract art (Mordvintsev et al., 2015). Training or even just using neural network (NN) algorithms on conventional generalpurpose digital hardware (Von Neumann architecture) has been found highly inefficient due to the massive amount of multiply-accumulate operations (MACs) required to compute the weighted sums of the neurons\u2019 inputs. Today, DNNs are almost exclusively trained on one or many very fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013). As a result, it is often a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in speeding up DNNs at run-time on both generalpurpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al., 2011a,b; Pham et al., 2012; Chen et al., 2014a,b; Esser et al., 2015). The most common approach is to compress a trained (full precision) network. HashedNets (Chen et al., 2015) reduce model sizes by using a hash function to randomly group connection weights and force them to share a single parameter value. Gong et al. (2014) compressed deep convnets using vector quantization, which resulteds in only a 1% accuracy loss. However, both methods focused only on the fully connected layers. A recent work by Han and Dally (2015) successfully pruned several state-of-the-art large scale networks and showed that the number of parameters could be reduced by an order of magnitude.", "startOffset": 8, "endOffset": 1445}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al., 2015; Silver et al., 2016), and even computer generation of abstract art (Mordvintsev et al., 2015). Training or even just using neural network (NN) algorithms on conventional generalpurpose digital hardware (Von Neumann architecture) has been found highly inefficient due to the massive amount of multiply-accumulate operations (MACs) required to compute the weighted sums of the neurons\u2019 inputs. Today, DNNs are almost exclusively trained on one or many very fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013). As a result, it is often a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in speeding up DNNs at run-time on both generalpurpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al., 2011a,b; Pham et al., 2012; Chen et al., 2014a,b; Esser et al., 2015). The most common approach is to compress a trained (full precision) network. HashedNets (Chen et al., 2015) reduce model sizes by using a hash function to randomly group connection weights and force them to share a single parameter value. Gong et al. (2014) compressed deep convnets using vector quantization, which resulteds in only a 1% accuracy loss. However, both methods focused only on the fully connected layers. A recent work by Han and Dally (2015) successfully pruned several state-of-the-art large scale networks and showed that the number of parameters could be reduced by an order of magnitude. Recent works have shown that more computationally efficient DNNs can be constructed by quantizing some of the parameters during the training phase. In most cases, DNNs are trained by minimizing some error function using Back-Propagation (BP) or related gradient descent methods. However, such an approach cannot be directly applied if the weights are restricted to binary values. Soudry et al. (2014) used a variational Bayesian approach with Mean-Field and Central Limit approximation to calculate the posterior distribution of the weights (the probability of each weight to be +1 or -1).", "startOffset": 8, "endOffset": 1996}, {"referenceID": 1, "context": ", 2014; Bahdanau et al., 2015), Atari and Go games (Mnih et al., 2015; Silver et al., 2016), and even computer generation of abstract art (Mordvintsev et al., 2015). Training or even just using neural network (NN) algorithms on conventional generalpurpose digital hardware (Von Neumann architecture) has been found highly inefficient due to the massive amount of multiply-accumulate operations (MACs) required to compute the weighted sums of the neurons\u2019 inputs. Today, DNNs are almost exclusively trained on one or many very fast and power-hungry Graphic Processing Units (GPUs) (Coates et al., 2013). As a result, it is often a challenge to run DNNs on target low-power devices, and substantial research efforts are invested in speeding up DNNs at run-time on both generalpurpose (Vanhoucke et al., 2011; Gong et al., 2014; Romero et al., 2014; Han et al., 2015b) and specialized computer hardware (Farabet et al., 2011a,b; Pham et al., 2012; Chen et al., 2014a,b; Esser et al., 2015). The most common approach is to compress a trained (full precision) network. HashedNets (Chen et al., 2015) reduce model sizes by using a hash function to randomly group connection weights and force them to share a single parameter value. Gong et al. (2014) compressed deep convnets using vector quantization, which resulteds in only a 1% accuracy loss. However, both methods focused only on the fully connected layers. A recent work by Han and Dally (2015) successfully pruned several state-of-the-art large scale networks and showed that the number of parameters could be reduced by an order of magnitude. Recent works have shown that more computationally efficient DNNs can be constructed by quantizing some of the parameters during the training phase. In most cases, DNNs are trained by minimizing some error function using Back-Propagation (BP) or related gradient descent methods. However, such an approach cannot be directly applied if the weights are restricted to binary values. Soudry et al. (2014) used a variational Bayesian approach with Mean-Field and Central Limit approximation to calculate the posterior distribution of the weights (the probability of each weight to be +1 or -1). During the inference stage (test phase), their method samples from this distribution one binary network and used it to predict the targets of the test set (More than one binary network can also be used). Courbariaux et al. (2015b) similarly used two sets of weights, real-valued and binary.", "startOffset": 8, "endOffset": 2416}, {"referenceID": 12, "context": "In order to transform the real-valued variables into those two values, we use two different binarization functions, as proposed by Courbariaux et al. (2015a). The first binarization function is deterministic:", "startOffset": 131, "endOffset": 158}, {"referenceID": 64, "context": "This stochastic binarization is more appealing theoretically (see Section 4) than the sign function, but somewhat harder to implement as it requires the hardware to generate random bits when quantizing (Torii et al., 2016).", "startOffset": 202, "endOffset": 222}, {"referenceID": 26, "context": "Moreover, adding noise to weights and activations when computing the parameter gradients provide a form of regularization that can help to generalize better, as previously shown with variational weight noise (Graves, 2011), Dropout (Srivastava et al.", "startOffset": 208, "endOffset": 222}, {"referenceID": 60, "context": "Moreover, adding noise to weights and activations when computing the parameter gradients provide a form of regularization that can help to generalize better, as previously shown with variational weight noise (Graves, 2011), Dropout (Srivastava et al., 2014) and DropConnect (Wan et al.", "startOffset": 232, "endOffset": 257}, {"referenceID": 66, "context": ", 2014) and DropConnect (Wan et al., 2013).", "startOffset": 24, "endOffset": 42}, {"referenceID": 32, "context": "He found that the fastest training was obtained when using the \u201cstraight-through estimator,\u201d previously introduced in Hinton\u2019s lectures (Hinton, 2012).", "startOffset": 136, "endOffset": 150}, {"referenceID": 4, "context": "Bengio (2013) studied the question of estimating or propagating gradients through stochastic discrete neurons.", "startOffset": 0, "endOffset": 14}, {"referenceID": 37, "context": "4 Shift-based Batch Normalization Batch Normalization (BN) (Ioffe and Szegedy, 2015) accelerates the training and reduces the overall impact of the weight scale (Courbariaux et al.", "startOffset": 59, "endOffset": 84}, {"referenceID": 44, "context": "From the early work of Lomont (2003) we know that the inverse-square operation could be applied with approximately the same complexity as multiplication.", "startOffset": 23, "endOffset": 37}, {"referenceID": 44, "context": ", after averaging (for a more precise calculation, see the BN analysis in Lin et al. (2015b). Furthermore, the size of the standard deviation vectors is relatively small.", "startOffset": 74, "endOffset": 93}, {"referenceID": 37, "context": "BatchNorm() specifies how to batch-normalize the activations, using either batch normalization (Ioffe and Szegedy, 2015) or its shift-based variant we describe in Algorithm 2.", "startOffset": 95, "endOffset": 120}, {"referenceID": 20, "context": "Require: a minibatch of inputs and targets (a0, a \u2217), previous weights W , previous BatchNorm parameters \u03b8, weight initialization coefficients from (Glorot and Bengio, 2010) \u03b3, and previous learning rate \u03b7.", "startOffset": 148, "endOffset": 173}, {"referenceID": 62, "context": "Consequently, the first layer of a ConvNet is often the smallest convolution layer, both in terms of parameters and computations (Szegedy et al., 2014).", "startOffset": 129, "endOffset": 151}, {"referenceID": 67, "context": "This idea was recently proposed by Zhou et al. (2016) (DoReFa net) and Miyashita et al.", "startOffset": 35, "endOffset": 54}, {"referenceID": 50, "context": "(2016) (DoReFa net) and Miyashita et al. (2016) (published on arXive shortly after our preliminary technical report was published there).", "startOffset": 24, "endOffset": 48}, {"referenceID": 50, "context": "(2016) (DoReFa net) and Miyashita et al. (2016) (published on arXive shortly after our preliminary technical report was published there). However, in contrast to to Zhou et al., we did not find it useful to initialize the network with weights obtained by training the network with full precision weights. Moreover, the Zhou et al. network did not quantize the weights of the first convolutional layer and the last fully-connected layer, whereas we binarized both. We followed the quantization schemes suggested by Miyashita et al. (2016), namely, linear quantization:", "startOffset": 24, "endOffset": 538}, {"referenceID": 2, "context": "40% Committee Machines\u2019 Array Baldassi et al. (2015) 1.", "startOffset": 30, "endOffset": 53}, {"referenceID": 2, "context": "40% Committee Machines\u2019 Array Baldassi et al. (2015) 1.35% Binarized weights, during training and test BinaryConnect Courbariaux et al. (2015a) 1.", "startOffset": 30, "endOffset": 144}, {"referenceID": 2, "context": "40% Committee Machines\u2019 Array Baldassi et al. (2015) 1.35% Binarized weights, during training and test BinaryConnect Courbariaux et al. (2015a) 1.29\u00b1 0.08% 2.30% 9.90% Binarized activations+weights, during test EBP Cheng et al. (2015) 2.", "startOffset": 30, "endOffset": 235}, {"referenceID": 2, "context": "40% Committee Machines\u2019 Array Baldassi et al. (2015) 1.35% Binarized weights, during training and test BinaryConnect Courbariaux et al. (2015a) 1.29\u00b1 0.08% 2.30% 9.90% Binarized activations+weights, during test EBP Cheng et al. (2015) 2.2\u00b1 0.1% Bitwise DNNs Kim and Smaragdis (2016) 1.", "startOffset": 30, "endOffset": 283}, {"referenceID": 2, "context": "40% Committee Machines\u2019 Array Baldassi et al. (2015) 1.35% Binarized weights, during training and test BinaryConnect Courbariaux et al. (2015a) 1.29\u00b1 0.08% 2.30% 9.90% Binarized activations+weights, during test EBP Cheng et al. (2015) 2.2\u00b1 0.1% Bitwise DNNs Kim and Smaragdis (2016) 1.33% Ternary weights, binary activations, during test Hwang and Sung (2014) 1.", "startOffset": 30, "endOffset": 360}, {"referenceID": 2, "context": "40% Committee Machines\u2019 Array Baldassi et al. (2015) 1.35% Binarized weights, during training and test BinaryConnect Courbariaux et al. (2015a) 1.29\u00b1 0.08% 2.30% 9.90% Binarized activations+weights, during test EBP Cheng et al. (2015) 2.2\u00b1 0.1% Bitwise DNNs Kim and Smaragdis (2016) 1.33% Ternary weights, binary activations, during test Hwang and Sung (2014) 1.45% No binarization (standard results) No reg 1.3\u00b1 0.2% 2.44% 10.94% Maxout Networks Goodfellow et al. (2013b) 0.", "startOffset": 30, "endOffset": 473}, {"referenceID": 2, "context": "40% Committee Machines\u2019 Array Baldassi et al. (2015) 1.35% Binarized weights, during training and test BinaryConnect Courbariaux et al. (2015a) 1.29\u00b1 0.08% 2.30% 9.90% Binarized activations+weights, during test EBP Cheng et al. (2015) 2.2\u00b1 0.1% Bitwise DNNs Kim and Smaragdis (2016) 1.33% Ternary weights, binary activations, during test Hwang and Sung (2014) 1.45% No binarization (standard results) No reg 1.3\u00b1 0.2% 2.44% 10.94% Maxout Networks Goodfellow et al. (2013b) 0.94% 2.47% 11.68% Gated pooling Lee et al. (2015) 1.", "startOffset": 30, "endOffset": 524}, {"referenceID": 42, "context": "MNIST MNIST is an image classification benchmark dataset (LeCun et al., 1998).", "startOffset": 57, "endOffset": 77}, {"referenceID": 21, "context": "Previous approaches include pruning near zero weights (Gong et al., 2014; Han et al., 2015a) using matrix factorization techniques (Zhang et al.", "startOffset": 54, "endOffset": 92}, {"referenceID": 27, "context": ", 2015), quantizing the weights (Gupta et al., 2015), using shared weights (Chen et al.", "startOffset": 32, "endOffset": 52}, {"referenceID": 7, "context": ", 2015), using shared weights (Chen et al., 2015) and applying Huffman codes (Han et al.", "startOffset": 30, "endOffset": 49}, {"referenceID": 68, "context": "Those results are presently state-of-the-art, surpassing those obtained by the DoReFa net (Zhou et al., 2016).", "startOffset": 90, "endOffset": 109}, {"referenceID": 50, "context": "Following Miyashita et al. (2016), we also tried quantizing the gradients and discovered that only logarithmic quantization works.", "startOffset": 10, "endOffset": 34}, {"referenceID": 54, "context": "1% Xnor-Nets4 (Rastegari et al., 2016) 44.", "startOffset": 14, "endOffset": 38}, {"referenceID": 68, "context": "67% DoReFaNet 2-bit activation4 (Zhou et al., 2016) 50.", "startOffset": 32, "endOffset": 51}, {"referenceID": 28, "context": "67% (Gysel et al., 2016) - 2-bit 0.", "startOffset": 4, "endOffset": 24}, {"referenceID": 47, "context": "The Penn Treebank Corpus (Marcus et al., 1993) contains 10K unique words.", "startOffset": 25, "endOffset": 46}, {"referenceID": 49, "context": "We followed the same setting as in (Mikolov and Zweig, 2012) which resulted in 18.", "startOffset": 35, "endOffset": 60}, {"referenceID": 34, "context": "the Long Short Term Memory networks (LSTMs) introduced by Hochreiter and Schmidhuber (1997) are being the most popular model.", "startOffset": 58, "endOffset": 92}, {"referenceID": 34, "context": "the Long Short Term Memory networks (LSTMs) introduced by Hochreiter and Schmidhuber (1997) are being the most popular model. LSTMs are a special kind of RNN, capable of learning long-term dependencies using unique gating mechanisms. Recently, Ott et al. (2016) tried to quantize the RNNs weight matrices using similar techniques as described in Section 2.", "startOffset": 58, "endOffset": 262}, {"referenceID": 52, "context": "Similar to (Ott et al., 2016), our preliminary results indicate that binarization of weight matrices lead to large accuracy degradation.", "startOffset": 11, "endOffset": 29}, {"referenceID": 35, "context": "Table 5: Energy consumption of multiply- accumulations; see Horowitz (2014) Operation MUL ADD 8-bit Integer 0.", "startOffset": 60, "endOffset": 76}, {"referenceID": 35, "context": "Table 6: Energy consumption of memory accesses; see Horowitz (2014) Memory size 64-bit Cache 8K 10pJ 32K 20pJ 1M 100pJ DRAM 1.", "startOffset": 52, "endOffset": 68}, {"referenceID": 35, "context": "Over the last decade, power has been the main constraint on performance (Horowitz, 2014).", "startOffset": 72, "endOffset": 88}, {"referenceID": 35, "context": "Over the last decade, power has been the main constraint on performance (Horowitz, 2014). This is why considerable research efforts have been devoted to reducing the energy consumption of neural networks. Horowitz (2014) provides rough numbers for the energy consumed by the computation (the given numbers are for 45nm technology), as summarized in Tables 5 and 6.", "startOffset": 73, "endOffset": 221}, {"referenceID": 24, "context": "For instance, a 32-bit floating point multiplier costs about 200 Xilinx FPGA slices (Govindu et al., 2004; Beauchamp et al., 2006), whereas a 1-bit XNOR gate only costs a single slice.", "startOffset": 84, "endOffset": 130}, {"referenceID": 3, "context": "For instance, a 32-bit floating point multiplier costs about 200 Xilinx FPGA slices (Govindu et al., 2004; Beauchamp et al., 2006), whereas a 1-bit XNOR gate only costs a single slice.", "startOffset": 84, "endOffset": 130}, {"referenceID": 12, "context": "Until recently, the use of extremely low-precision networks (binary in the extreme case) was believed to substantially degrade the network performance (Courbariaux et al., 2014).", "startOffset": 151, "endOffset": 177}, {"referenceID": 11, "context": "Until recently, the use of extremely low-precision networks (binary in the extreme case) was believed to substantially degrade the network performance (Courbariaux et al., 2014). Soudry et al. (2014) and Cheng et al.", "startOffset": 152, "endOffset": 200}, {"referenceID": 9, "context": "(2014) and Cheng et al. (2015) proved the contrary by showing that good performance could be achieved even if all neurons and weights are binarized to \u00b11 .", "startOffset": 11, "endOffset": 31}, {"referenceID": 9, "context": "(2014) and Cheng et al. (2015) proved the contrary by showing that good performance could be achieved even if all neurons and weights are binarized to \u00b11 . This was done using Expectation BackPropagation (EBP), a variational Bayesian approach, which infers networks with binary weights and neurons by updating the posterior distributions over the weights. These distributions are updated by differentiating their parameters (e.g., mean values) via the back propagation (BP) algorithm. Esser et al. (2015) implemented a fully binary network at run time using a very similar approach to EBP, showing significant", "startOffset": 11, "endOffset": 505}, {"referenceID": 2, "context": "Other research (Baldassi et al., 2015) showed that full binary training and testing is possible in an array of committee machines with randomized input, where only one weight layer is being adjusted.", "startOffset": 15, "endOffset": 38}, {"referenceID": 11, "context": "The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process.", "startOffset": 81, "endOffset": 108}, {"referenceID": 11, "context": "The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification; see Spang and Schultheiss, 1962). The noise would have little effect on the next neuron\u2019s input because the input is a summation over many weighted neurons. Thus, the real-valued version could be updated using the back propagated error by simply ignoring the binarization noise in the update. With this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets.", "startOffset": 81, "endOffset": 737}, {"referenceID": 11, "context": "The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification; see Spang and Schultheiss, 1962). The noise would have little effect on the next neuron\u2019s input because the input is a summation over many weighted neurons. Thus, the real-valued version could be updated using the back propagated error by simply ignoring the binarization noise in the update. With this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. They also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown by Wan et al. (2013). This method binarized weights while still maintaining full precision neurons.", "startOffset": 81, "endOffset": 1003}, {"referenceID": 11, "context": "The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification; see Spang and Schultheiss, 1962). The noise would have little effect on the next neuron\u2019s input because the input is a summation over many weighted neurons. Thus, the real-valued version could be updated using the back propagated error by simply ignoring the binarization noise in the update. With this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. They also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown by Wan et al. (2013). This method binarized weights while still maintaining full precision neurons. Lin et al. (2015a) carried over the work of Courbariaux et al.", "startOffset": 81, "endOffset": 1101}, {"referenceID": 11, "context": "The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification; see Spang and Schultheiss, 1962). The noise would have little effect on the next neuron\u2019s input because the input is a summation over many weighted neurons. Thus, the real-valued version could be updated using the back propagated error by simply ignoring the binarization noise in the update. With this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. They also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown by Wan et al. (2013). This method binarized weights while still maintaining full precision neurons. Lin et al. (2015a) carried over the work of Courbariaux et al. (2015a) to the backpropagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons\u2019 values to be power-of-two integers.", "startOffset": 81, "endOffset": 1153}, {"referenceID": 11, "context": "The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification; see Spang and Schultheiss, 1962). The noise would have little effect on the next neuron\u2019s input because the input is a summation over many weighted neurons. Thus, the real-valued version could be updated using the back propagated error by simply ignoring the binarization noise in the update. With this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. They also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown by Wan et al. (2013). This method binarized weights while still maintaining full precision neurons. Lin et al. (2015a) carried over the work of Courbariaux et al. (2015a) to the backpropagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons\u2019 values to be power-of-two integers. Lin et al. (2015a)\u2019s work and ours seem to share similar characteristics .", "startOffset": 81, "endOffset": 1399}, {"referenceID": 11, "context": "The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of Courbariaux et al. (2015a). In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification; see Spang and Schultheiss, 1962). The noise would have little effect on the next neuron\u2019s input because the input is a summation over many weighted neurons. Thus, the real-valued version could be updated using the back propagated error by simply ignoring the binarization noise in the update. With this method, Courbariaux et al. (2015a) were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. They also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown by Wan et al. (2013). This method binarized weights while still maintaining full precision neurons. Lin et al. (2015a) carried over the work of Courbariaux et al. (2015a) to the backpropagation process by quantizing the representations at each layer of the network, to convert some of the remaining multiplications into binary shifts by restricting the neurons\u2019 values to be power-of-two integers. Lin et al. (2015a)\u2019s work and ours seem to share similar characteristics .However, their approach continues to use full precision weights during the test phase. Moreover, Lin et al. (2015a) quantize the neurons only during the back propagation process, and not during forward propagation.", "startOffset": 81, "endOffset": 1570}, {"referenceID": 2, "context": "Other research (Baldassi et al., 2015) showed that full binary training and testing is possible in an array of committee machines with randomized input, where only one weight layer is being adjusted. Gong et al. (2014) aimed to compress a fully trained high precision network by using quantization or matrix factorization methods.", "startOffset": 16, "endOffset": 219}, {"referenceID": 2, "context": "Other research (Baldassi et al., 2015) showed that full binary training and testing is possible in an array of committee machines with randomized input, where only one weight layer is being adjusted. Gong et al. (2014) aimed to compress a fully trained high precision network by using quantization or matrix factorization methods. These methods required training the network with full precision weights and neurons, thus requiring numerous MAC operations (which the proposed QNN algorithm avoids). Hwang and Sung (2014) focused on a fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture.", "startOffset": 16, "endOffset": 520}, {"referenceID": 2, "context": "Other research (Baldassi et al., 2015) showed that full binary training and testing is possible in an array of committee machines with randomized input, where only one weight layer is being adjusted. Gong et al. (2014) aimed to compress a fully trained high precision network by using quantization or matrix factorization methods. These methods required training the network with full precision weights and neurons, thus requiring numerous MAC operations (which the proposed QNN algorithm avoids). Hwang and Sung (2014) focused on a fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture. Kim and Smaragdis (2016) retrained neural networks with binary weights and activations.", "startOffset": 16, "endOffset": 678}, {"referenceID": 2, "context": "Other research (Baldassi et al., 2015) showed that full binary training and testing is possible in an array of committee machines with randomized input, where only one weight layer is being adjusted. Gong et al. (2014) aimed to compress a fully trained high precision network by using quantization or matrix factorization methods. These methods required training the network with full precision weights and neurons, thus requiring numerous MAC operations (which the proposed QNN algorithm avoids). Hwang and Sung (2014) focused on a fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture. Kim and Smaragdis (2016) retrained neural networks with binary weights and activations. As far as we know, before the first revision of this paper was published on arXive, no work succeeded in binarizing weights and neurons, at the inference phase and the entire training phase of a deep network. This was achieved in the present work. We relied on the idea that binarization can be done stochastically, or be approximated as random noise. This was previously done for the weights by Courbariaux et al. (2015a), but our BNNs extend this to the activations.", "startOffset": 16, "endOffset": 1164}, {"referenceID": 2, "context": "Other research (Baldassi et al., 2015) showed that full binary training and testing is possible in an array of committee machines with randomized input, where only one weight layer is being adjusted. Gong et al. (2014) aimed to compress a fully trained high precision network by using quantization or matrix factorization methods. These methods required training the network with full precision weights and neurons, thus requiring numerous MAC operations (which the proposed QNN algorithm avoids). Hwang and Sung (2014) focused on a fixed-point neural network design and achieved performance almost identical to that of the floating-point architecture. Kim and Smaragdis (2016) retrained neural networks with binary weights and activations. As far as we know, before the first revision of this paper was published on arXive, no work succeeded in binarizing weights and neurons, at the inference phase and the entire training phase of a deep network. This was achieved in the present work. We relied on the idea that binarization can be done stochastically, or be approximated as random noise. This was previously done for the weights by Courbariaux et al. (2015a), but our BNNs extend this to the activations. Note that the binary activations are especially important for ConvNets, where there are typically many more neurons than free weights. This allows highly efficient operation of the binarized DNN at run time, and at the forward-propagation phase during training. Moreover, our training method has almost no multiplications, and therefore might be implemented efficiently in dedicated hardware. However, we have to save the value of the full precision weights. This is a remaining computational bottleneck during training, since it is an energy-consuming operation. Shortly after the first version of this paper was posted on arXiv, several papers tried to improve and extend it. Rastegari et al. (2016) made a small modification to our algo-", "startOffset": 16, "endOffset": 1912}, {"referenceID": 54, "context": "Note that their method, named Xnor-Net, requires additional multiplication by a different scaling factor for each patch in each sample (Rastegari et al., 2016) Section 3.", "startOffset": 135, "endOffset": 159}, {"referenceID": 54, "context": "Moreover, (Rastegari et al., 2016) didn\u2019t quantize first and last layers, therefore XNOR-Net are only partially binarized NNs.", "startOffset": 10, "endOffset": 34}, {"referenceID": 48, "context": "Miyashita et al. (2016) suggested a more relaxed quantization (more than 1-bit) for both the weights and activation.", "startOffset": 0, "endOffset": 24}, {"referenceID": 48, "context": "Miyashita et al. (2016) suggested a more relaxed quantization (more than 1-bit) for both the weights and activation. Their idea was to quantize both and use shift operations as in our Eq. (4). They proposed to quantize the parameters in their non-uniform, base-2 logarithmic representation. This idea was inspired by the fact that the weights and activations in a trained network naturally have non-uniform distributions. They moreover showed that they can quantize the gradients as well to 6-bit without significant losses in performance (on the Cifar-10 dataset). Zhou et al. (2016) applied similar ideas to the ImageNet dataset and showed that by using 1-bit weights, 2-bit activations and 6-bit gradients they can achieve 46.", "startOffset": 0, "endOffset": 585}, {"referenceID": 47, "context": "Merolla et al. (2016) showed that DNN can be robust to more than just weight binarization.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Andri et al. (2016) even created a hardware implementation to speed up BNNs.", "startOffset": 0, "endOffset": 20}, {"referenceID": 11, "context": "We thank the developers of Torch, (Collobert et al., 2011) a Lua based environment, and Theano (Bergstra et al.", "startOffset": 34, "endOffset": 58}, {"referenceID": 5, "context": ", 2011) a Lua based environment, and Theano (Bergstra et al., 2010; Bastien et al., 2012), a Python library that allowed us to easily develop fast and optimized code for GPU.", "startOffset": 44, "endOffset": 89}, {"referenceID": 16, "context": ", 2013a) and Lasagne (Dieleman et al., 2015), two deep learning libraries built on the top of Theano.", "startOffset": 21, "endOffset": 44}, {"referenceID": 42, "context": "MNIST is an image classification benchmark dataset (LeCun et al., 1998).", "startOffset": 51, "endOffset": 71}, {"referenceID": 63, "context": "The Multi-LayerPerceptron (MLP) we train on MNIST consists of 3 hidden layers of 4096 binary units and a L2-SVM output layer; L2-SVM has been shown to perform better than Softmax on several classification benchmarks (Tang, 2013; Lee et al., 2014).", "startOffset": 216, "endOffset": 246}, {"referenceID": 60, "context": "We regularize the model with Dropout (Srivastava et al., 2014).", "startOffset": 37, "endOffset": 62}, {"referenceID": 20, "context": "We use an exponentially decaying global learning rate, as per Algorithm 1, and also scale the learning rates of the weights with their initialization coefficients from (Glorot and Bengio, 2010), as suggested by Courbariaux et al.", "startOffset": 168, "endOffset": 193}, {"referenceID": 4, "context": "We use an exponentially decaying global learning rate, as per Algorithm 1, and also scale the learning rates of the weights with their initialization coefficients from (Glorot and Bengio, 2010), as suggested by Courbariaux et al. (2015a). We use Batch Normalization with a minibatch of size 100 to speed up the training.", "startOffset": 180, "endOffset": 238}, {"referenceID": 57, "context": "(2015a) architecture is itself mainly inspired by VGG (Simonyan and Zisserman, 2015).", "startOffset": 54, "endOffset": 84}, {"referenceID": 20, "context": "We scale the learning rates of the weights with their initialization coefficients from (Glorot and Bengio, 2010).", "startOffset": 87, "endOffset": 112}, {"referenceID": 11, "context": "The architecture of our ConvNet is identical to that used by Courbariaux et al. (2015b) except for the binarization of the activations.", "startOffset": 61, "endOffset": 88}, {"referenceID": 11, "context": "The architecture of our ConvNet is identical to that used by Courbariaux et al. (2015b) except for the binarization of the activations. The Courbariaux et al. (2015a) architecture is itself mainly inspired by VGG (Simonyan and Zisserman, 2015).", "startOffset": 61, "endOffset": 167}, {"referenceID": 57, "context": "We only use \u201dsame\u201d convolutions as in VGG (Simonyan and Zisserman, 2015).", "startOffset": 42, "endOffset": 72}], "year": 2016, "abstractText": "We introduce a method to train Quantized Neural Networks (QNNs) \u2014 neural networks with extremely low precision (e.g., 1-bit) weights and activations, at run-time. At traintime the quantized weights and activations are used for computing the parameter gradients. During the forward pass, QNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations. As a result, power consumption is expected to be drastically reduced. We trained QNNs over the MNIST, CIFAR-10, SVHN and ImageNet datasets. The resulting QNNs achieve prediction accuracy comparable to their 32-bit counterparts. For example, our quantized version of AlexNet with 1-bit weights and 2-bit activations achieves 51% top-1 accuracy. Moreover, we quantize the parameter gradients to 6-bits as well which enables gradients computation using only bit-wise operation. Quantized recurrent neural networks were tested over the Penn Treebank dataset, and achieved comparable accuracy as their 32-bit counterparts using only 4-bits. Last but not least, we programmed a binary matrix multiplication GPU kernel with which it is possible to run our MNIST QNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The QNN code is available online. 1 ar X iv :1 60 9. 07 06 1v 1 [ cs .N E ] 2 2 Se p 20 16 Hubara, Courbariaux, Soudry, El-Yaniv and Bengio", "creator": "LaTeX with hyperref package"}}}