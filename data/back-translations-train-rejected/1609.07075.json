{"id": "1609.07075", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Knowledge Representation via Joint Learning of Sequential Text and Knowledge Graphs", "abstract": "Textual information is considered as significant supplement to knowledge representation learning (KRL). There are two main challenges for constructing knowledge representations from plain texts: (1) How to take full advantages of sequential contexts of entities in plain texts for KRL. (2) How to dynamically select those informative sentences of the corresponding entities for KRL. In this paper, we propose the Sequential Text-embodied Knowledge Representation Learning to build knowledge representations from multiple sentences. Given each reference sentence of an entity, we first utilize recurrent neural network with pooling or long short-term memory network to encode the semantic information of the sentence with respect to the entity. Then we further design an attention model to measure the informativeness of each sentence, and build text-based representations of entities. We evaluate our method on two tasks, including triple classification and link prediction. Experimental results demonstrate that our method outperforms other baselines on both tasks, which indicates that our method is capable of selecting informative sentences and encoding the textual information well into knowledge representations.", "histories": [["v1", "Thu, 22 Sep 2016 17:16:43 GMT  (145kb,D)", "http://arxiv.org/abs/1609.07075v1", "10 pages, 3 figures"]], "COMMENTS": "10 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["jiawei wu", "ruobing xie", "zhiyuan liu", "maosong sun"], "accepted": false, "id": "1609.07075"}, "pdf": {"name": "1609.07075.pdf", "metadata": {"source": "CRF", "title": "Knowledge Representation via Joint Learning of Sequential Text and Knowledge Graphs", "authors": ["Jiawei Wu", "Ruobing Xie", "Zhiyuan Liu", "Maosong Sun"], "emails": ["(liuzy@tsinghua.edu.cn)."], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcnlrVo rf\u00fc eid eerwdnei rf\u00fc eid eerwtlrteeoiKe rf\u00fc eid eerwtlrteeeirsrVnlrteeoiietlcnlhsrteeoiuiueaeBngn"}, {"heading": "2 Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Translation-based Methods", "text": "Translation-based methods have achieved great success in representation learning in knowledge graphs. TransE (Bordes et al., 2013) interprets relationships as translation operations between head and tail units and projects both units and relationships into the same continuous low-dimensional vector space. The energy function is defined as: E (h, r, t) = | | h + r \u2212 t | |, (1), assuming that the tail embedding t is in the vicinity of h + r. TransE is simple and effective, while this simple translation operation may have problems in modeling 1-to-N, N-to-1 and N-to-N relationships. Furthermore, the translation process focuses only on a single step, notwithstanding abundant information located in distant relational paths. To address the first problem, TransH (Wang et al., 2014b) models the translation of operations between units on relationship-specific hypersec, in multiple units (L5b) and transspatial."}, {"heading": "2.2 Representation Learning of Knowledge Graphs with Textual Information", "text": "Multi-source information, especially text information, is important for the representation of knowledge graphs that have recently attracted a lot of attention. They can provide useful information from various aspects, which is helpful in modelling knowledge graphs. (Wang et al., 2014a) encode entities and words in a common low-dimensional vector space using entity name alignment models and Wikipedia anchors. (Zhong et al., 2015) expands the alignment model to include entity descriptions. However, the methods for modeling simple texts in the two above models are quite simple and ignore important information encoded in word order. (Xie et al., 2016) proposes a new way of representing entities constructed directly from entity descriptions using CNN and thus able to model new entities. However, their description-based representation is limited by the completeness and quality of the entity descriptions."}, {"heading": "2.3 Multi-instance Learning", "text": "Multi-instance learning, originally proposed in (Dietterich et al., 1997), arises in tasks where a single object may have several alternative examples or representations that describe it. Multi-instance learning aims to determine the reliability of examples in each object. (Bunescu and Mooney, 2007) expands a method of relationship extraction by minimal supervision with multi-instance learning. (Zhou et al., 2012) proposes multi-instance multi-label learning for multiple tasks such as scene classification and text categorization. (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) also apply multi-instance learning in remote supervisions to relationship extraction. (Zeng et al., 2015) also combine multi-instance learning with revolutionary neural network for relationship extraction using remote surveillance data. In order to fully exploit the rich textual information contained in multiple sentences of each sentence, we suggest using multiple sentences of information for a single sentence."}, {"heading": "3 Methodology", "text": "We first describe the notations used in this essay. For each triple (h, r, t), T consists of two units h, t, E and a relation r, R. E stands for the entirety of the units, while R stands for the entirety of the relationships. T is the training group of the triple units, and the embedding of units and relationships takes values in Rk. We have two representations for each unit. We use hK, tK as structure-based representations of head and tail units that correspond to those in previous knowledge models, and hS, tS as text-based representations learned from simple texts through sentence coding. For each unit, we first scan through the corpus to extract all sentences that contain the corresponding name of the unit. These sentences are considered as reference sets of the corresponding unit."}, {"heading": "3.1 Overall Architecture", "text": "First, we present the general architecture of the STKRL model. Inspired by translation-based methods, we define the energy function as follows: E (h, r, t) = EKK + ESS + ESK + EKS, (2) where EKK is the same energy function as TransE, ESS, ESK and EKS, which are added jointly by the two representations. ESS = \u0394hS + r \u2212 tS, in which both head and tail are text-based representations learned from reference sets. Similarly, we have ESK = \u0394hS + r \u2212 tK and EKS = \u03c6hK + r \u2212 tS, in which the two types of entity representations are considered together. The overall architecture of the STKRL model is demonstrated in Figure 2."}, {"heading": "3.2 Word Representations", "text": "In our framework, we consider the embedding of each word in reference sentences as input, and each entity name is also considered as a word. Inspired by (Zeng et al., 2014), the word representations consist of two parts, including word characteristics and position characteristics."}, {"heading": "3.2.1 Word Features", "text": "Word characteristics could be learned by negatively scanning Skip-gram models, since these models could encode contextual information found in large solids, and the learned word embeddings are then viewed directly as word characteristics."}, {"heading": "3.2.2 Position Features", "text": "Suppose each sentence is represented as sequence s = (x1, x2, \u00b7 \u00b7, xn), where xi represents the i-th word. In a sentence, the position characteristic of its entity name is marked as 0, and the positions of other words are marked according to the relevant integer distance to the entity name. Left words have negative position values, while right ones have positive position values. Position characteristics are marked as \u2212 d or d if their relevant spacing is greater than d (d is a hyperparameter)."}, {"heading": "3.3 Sentence Encoder", "text": "We assume that the meaning of an entity could be extracted from its reference sentences. There are tons of algorithms to represent sentence information using word order, such as neural networks (RNN) and long-term short-term memories (LSTM). These models are widely used in various natural language processing tasks such as machine translation. We use RNN with pooling and LSTM as sentence encoders to learn sentence representations of entities, intending to extract meanings of entities from reference sentences."}, {"heading": "3.3.1 Recurrent Neural Network", "text": "The recursive neural network uses a reference set as input. It operates on a sequence and gets a hidden state over time. At each time step t, the hidden state vector ht is updated as follows: ht = tanh (Wxt + Uht \u2212 1 + b), (3) where the transition function is an affine transformation followed by a nonlinear function such as the hyperbolic angular one. More precisely, RNN reads each word sequentially, with the hidden state of the RNN changing according to Equation (3). After completing the entire sequence marked by an end-of-of-the-sequence symbol, we can get the final hidden state vector hn as output. hn is then considered as a sentence level."}, {"heading": "3.3.2 RNN with Pooling", "text": "Recurrent neural networks are powerful and widely used in various areas, whereas they usually suffer from a disappearance of the gradient. Therefore, it is difficult for the last hidden state of RNN to capture the early local information if the sentence is too long. (Collobert et al., 2011) suggests an approach that solves the disappearance of the gradient to some extent through medium pooling. To take advantage of efficiency and effectiveness, we add a medium pooling layer to encode the aggregate information of a sentence into its sentence plane. We have: c = \u2211 i = 1, \u00b7, nhi n, (4), in which all hidden intermediate states containing different local information should contribute to the final sentence level and could therefore be updated during the retransmission. The structure is shown in Fig. 3."}, {"heading": "3.3.3 Long Short-Term Memory Network", "text": "LSTM (Hochreiter and Schmidhuber, 1997) is an advanced neural network based on RNN that could address the disappearance of the gradient when learning long-term dependencies. LSTM introduces memory cells capable of maintaining the state over long periods of time. At any time, the LSTM unit consists of: an input gate it, a forget gate ft, an output gate ot, a memory cell ct and a hidden state ht. The entries of the gating vectors it, ft and ot are in [0, 1]. The LSTM transition equations are: it = \u03c3 (W (i) xt + U (i) ht \u2212 1 + b (i))))), (5) ft = \u03c3 (W (f) xt + U (f) ht \u2212 1 + b (f) ht \u2212 b (f)))))), (6) ot = \u03c3 (W (o) xt (o) ht \u2212 o (h), o), (t (t), t (t) (t) and ct (t)."}, {"heading": "3.4 Attention over Reference Sentences", "text": "We use sentence encoders to build sentence representations from reference sentences, next we want to integrate these sentence representations into the text-based representation for each unit. Simply looking at the mean or maximum number of sentence embeddings will suffer from noise or lose rich information. Instead of taking the mean / maximum sentence embeddings, we suggest an attention method to automatically select sentences that can explicitly explain the meaning of the unit. The attention-based model is powerful and has been widely applied to machine translation (Bahdanau et al., 2015), abstract sentence summary (Rush et al., 2015) and speech recognition (Chorowski et al., 2014). It can automatically highlight more informative examples from multiple applications. We implement an attention-based multi-level learning method to select informative reference sentences for the corresponding unit from all candidates. Specifically, each unit has a structure-based representation K."}, {"heading": "3.5 Objective Formalization", "text": "We use a margin-based score function as our training goal. The total score function is defined as follows: L = \u2211 (h, r, t), (13), where \u03b3 is a margin hyper parameter. E (h, r, t) is the energy function specified above, which can be either L1 or L2 standard. T \u2032 is the negative sampling set of T, defined as T \u2032 = {(h \u2032, r, t) | h \u2032 E}, (14) in which head and tail are randomly replaced by a different unit. Note that for each unit there are two types of entity representations, including text-based representation and structure-based representation. Therefore, we can replace these two types of entities with a different unit."}, {"heading": "3.6 Model Initialization and Optimization", "text": "The STKRL model can be initialized either randomly or with pre-formed TransE embeddings. Word representations are pre-trained by Word2Vec with Wikipedia corpus. Optimization is a standard back-propagation using stochastic gradient descent (SGD) in the minibatch. For efficiency, we also use GPU to speed up the training process. Note that there is only one RNN / LSTM, which means that all RNN / LSTM in the figure share the same parameters, and this RNN / LSTM is trained to encode reference sets for all units. Our model consists of two parts: (1) text-based RNN / LSTM with attention to the coding of sentences and (2) structure-based TransE (Bordes et al., 2013) for the coding of knowledge. We train the two parts at the same time. The training process is described as follows: a triple STKnowledge-based texture and Darttail."}, {"heading": "4 Experiment", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets", "text": "FB15K is a subset of Freebase with 14,951 entities and 1,345 relationships. For each entity, we use Wikipedia as a corpus to extract reference sets. Note that the Wikipedia article on the entity itself has the highest priority in providing reference sets. Each entity has approximately 40 sets. Statistics of the FB15K dataset are listed in Table 1."}, {"heading": "4.2 Parameter Settings", "text": "For STKRL, we implement 4 sentence encoders for evaluation. \"RNN without ATT\" represents the selection of RNN as sentence encoders and the use of the middle vector of sentence representations instead of attention, while \"RNN + ATT,\" \"RNN + P + ATT\" and \"LSTM + ATT\" represent the selection of the corresponding sentence encoders of RNN, RNN + Pooling and LSTM with the help of attention. We implement TransE and jointly implement (name) the model in (Wang et al., 2014a) as baselines according to their experimental settings. For fair comparisons, all baselines have the same dimension of units and relationships."}, {"heading": "4.3 Triple Classification", "text": "The purpose of the triple classification is to test whether a triple classification (h, r, t) is true or false, which could be considered a binary classification test."}, {"heading": "4.3.1 Evaluation Protocol", "text": "Since each unit has two types of representations, a triple unit (h, r, t) has four representations (hK, r, tK), (hS, r, tS), (hS, r, tK) and (hK, r, tS). In accordance with the similar protocol in (Socher et al., 2013), we construct a negative example for each triple representation by randomly replacing the head or tail unit with another unit. Furthermore, we ensure that the number of true triples is equal to the wrong one. The evaluation strategy is described as follows: If the dissimilarity of a testing triple unit (h, r, tK) is below the relationship-specific threshold (h, r, t), it is predicted to be positive, otherwise the negative threshold can be determined by maximizing the classification."}, {"heading": "4.3.2 Results", "text": "The results of the triple classification are shown in Table 3. Results show that (1) our attention-based models significantly exceed all baselines, indicating the ability of our methods to model knowledge representations. (2) STKRL (RNN w / o ATT) significantly exceeds TransE, which implies the importance of text information. In addition, the attention-based STKRL models exceed Wang's method, which also encodes textual information in knowledge representations. It confirms that our sentence coders understand the meaning of sentences better and can therefore achieve better performance. (3) Attention is the key component in our model. It can automatically select the more informative reference sets to represent a unit, thereby mitigating the noises caused by low-quality reference sets. (4) STKRL (LSTM + ATT) achieves the best performance and successfully captures the rich information in STRL (NATT), also demonstrating the long-range strategy (NATT)."}, {"heading": "4.4 Link Prediction", "text": "The task of the link prediction is to complete a triple fact (h, r, t) if h or t is missing."}, {"heading": "4.4.1 Evaluation Protocol", "text": "This whole procedure is repeated by removing the tail instead of the head, and the same rule applies to the tail. Note that in Wang's and our model, each entity has two representations. As a result, the predicted rank for a particular entity is the mean of two representations. However, as with the Hits @ 10 test, one of the two representations appearing in the top 10 could be considered a successful hit."}, {"heading": "4.4.2 Results", "text": "From Table 6, we can see the following: (1) Our model outperforms all baselines in both Mean Rank and Hits @ 10, demonstrating the effectiveness and robustness of our model. (2) STKRL (RNN + P + ATT) and STKRL (LSTM + ATT) significantly outperform Wang's method, which is because we use sentence encoders to model text information using orders of words rather than simply considering individual words used in Wang's methodology. In addition, the attention-based STKRL models may pay more attention to these informative reference sets when creating text-based representations. (3) STKRL (LSTM + ATT) achieves the best performance, indicating the performance when using better sentence encoders. Table 2 shows that the linkage prediction achieves results with different categories of relationships. Relationships are divided into four categories that follow the same settings (from Table 2), which we can observe STN (from Table 1)."}, {"heading": "4.5 Case Study", "text": "In this section, we cite two cases and analyses to prove that we can successfully select informative reference sets to represent entities."}, {"heading": "4.5.1 Effectiveness of Attention", "text": "In order to show that attention has reference rates with a reasonable ranking, we give four examples of entity economics with their ranks. Results in Table 4 indicate that STKRL can successfully select the more informative sentences of the corresponding entity. These best placed sentences are usually the definitions or descriptions of entities. Moreover, the lowest placed sentences are generally less relevant to the targeted entity, which also indicates the ability of attention to filter noise."}, {"heading": "4.5.2 Definition Extraction", "text": "Definition extraction is an important task in text mining (Espinosa-Anke et al., 2015). We want to show that the reference sets selected by our model are generally definitions of the corresponding entities in the first place. By randomly selecting 100 entities from entity group E, we find that 61 reference sets of entities are entities based on manual annotations. In Table 5, we give 4 reference sets. The first three sets are exactly entity definitions, the last one not. Nevertheless, all sentences are still informative to learn the meaning of entities. Results show that our model has rational selectivity to extract reference sets, and is potentially useful for definition extraction."}, {"heading": "5 Conclusion", "text": "In this paper, we propose the STKRL model, a novel model for learning knowledge representation that collectively takes into account triple facts and sequential text information; we also examine how we can extract informative sentences from many candidates using an attention-based method; we evaluate our models based on two tasks, triple classification and link prediction; and we also give some examples to demonstrate the ability to extract useful information efficiently. Experimental results show that our models achieve significant improvements over other baselines; code and data set are released if accepted; we will examine the following further work: (1) We assume that there are, of course, three types of representations for entity representations, including word, sentence and knowledge representations; we mainly use knowledge and sentence representations in the STKRL model; and we will examine to integrate all three representations in the future. (2) The STKRL model can further develop definition sets of future subject matter by using this by-product extraction method."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of ICLR", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of KDD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Translating embeddings for modeling multi-relational data", "author": ["Yakhnenko."], "venue": "Proceedings of NIPS, pages 2787\u20132795.", "citeRegEx": "Yakhnenko.,? 2013", "shortCiteRegEx": "Yakhnenko.", "year": 2013}, {"title": "Learning to extract relations from the web using minimal supervision", "author": ["Bunescu", "Mooney2007] Razvan Bunescu", "Raymond Mooney"], "venue": "In Proceedings of ACL", "citeRegEx": "Bunescu et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bunescu et al\\.", "year": 2007}, {"title": "End-to-end continuous speech recognition using attention-based recurrent nn: first results", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": null, "citeRegEx": "Chorowski et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chorowski et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": null, "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Solving the multiple instance problem with axis-parallel rectangles", "author": ["Richard H Lathrop", "Tom\u00e1s Lozano-P\u00e9rez"], "venue": "Artificial intelligence,", "citeRegEx": "Dietterich et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Dietterich et al\\.", "year": 1997}, {"title": "Knowledge vault: A web-scale approach to probabilistic knowledge fusion", "author": ["Dong et al.2014] Xin Dong", "Evgeniy Gabrilovich", "Geremy Heitz", "Wilko Horn", "Ni Lao", "Kevin Murphy", "Thomas Strohmann", "Shaohua Sun", "Wei Zhang"], "venue": null, "citeRegEx": "Dong et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dong et al\\.", "year": 2014}, {"title": "Weakly supervised definition extraction", "author": ["Espinosa-Anke", "Francesco Ronzano", "Horacio Saggion."], "venue": "Proceedings of Recent Advances in Natural Language Processing, page 176.", "citeRegEx": "Espinosa.Anke et al\\.,? 2015", "shortCiteRegEx": "Espinosa.Anke et al\\.", "year": 2015}, {"title": "Traversing knowledge graphs in vector space", "author": ["Gu et al.2015] Kelvin Gu", "John Miller", "Percy Liang"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Gu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gu et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Knowledge-based weak supervision for information extraction of overlapping relations", "author": ["Congle Zhang", "Xiao Ling", "Luke Zettlemoyer", "Daniel S Weld"], "venue": "In Proceedings of ACL,", "citeRegEx": "Hoffmann et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Hoffmann et al\\.", "year": 2011}, {"title": "A latent factor model for highly multi-relational data", "author": ["Nicolas L Roux", "Antoine Bordes", "Guillaume R Obozinski"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Jenatton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Jenatton et al\\.", "year": 2012}, {"title": "Knowledge graph embedding via dynamic mapping matrix", "author": ["Ji et al.2015] Guoliang Ji", "Shizhu He", "Liheng Xu", "Kang Liu", "Jun Zhao"], "venue": "In Proceedings of ACL,", "citeRegEx": "Ji et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2015}, {"title": "Modeling relation paths for representation learning of knowledge bases", "author": ["Lin et al.2015a] Yankai Lin", "Zhiyuan Liu", "Huanbo Luan", "Maosong Sun", "Siwei Rao", "Song Liu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning entity and relation embeddings for knowledge graph completion", "author": ["Lin et al.2015b] Yankai Lin", "Zhiyuan Liu", "Maosong Sun", "Yang Liu", "Xuan Zhu"], "venue": "Proceedings of AAAI", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Learning word meanings from context during normal reading", "author": ["Nagy et al.1987] William E Nagy", "Richard C Anderson", "Patricia A Herman"], "venue": "American educational research journal,", "citeRegEx": "Nagy et al\\.,? \\Q1987\\E", "shortCiteRegEx": "Nagy et al\\.", "year": 1987}, {"title": "Compositional vector space models for knowledge base completion", "author": ["Benjamin Roth", "Andrew McCallum"], "venue": "Proceedings of EMNLP", "citeRegEx": "Neelakantan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Neelakantan et al\\.", "year": 2015}, {"title": "A three-way model for collective learning on multi-relational data", "author": ["Volker Tresp", "Hans-Peter Kriegel"], "venue": "In Proceedings of ICML,", "citeRegEx": "Nickel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Nickel et al\\.", "year": 2011}, {"title": "Modeling relations and their mentions without labeled text", "author": ["Limin Yao", "Andrew McCallum"], "venue": "In Machine Learning and Knowledge Discovery in Databases,", "citeRegEx": "Riedel et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Riedel et al\\.", "year": 2010}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Reasoning with neural tensor networks for knowledge base completion", "author": ["Danqi Chen", "Christopher D Manning", "Andrew Ng"], "venue": "In Proceedings of NIPS,", "citeRegEx": "Socher et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2013}, {"title": "Multi-instance multi-label learning for relation extraction", "author": ["Julie Tibshirani", "Ramesh Nallapati", "Christopher D Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Surdeanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Surdeanu et al\\.", "year": 2012}, {"title": "Knowledge graph and text jointly embedding", "author": ["Wang et al.2014a] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Knowledge graph embedding by translating on hyperplanes", "author": ["Wang et al.2014b] Zhen Wang", "Jianwen Zhang", "Jianlin Feng", "Zheng Chen"], "venue": "In Proceedings of AAAI,", "citeRegEx": "Wang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2014}, {"title": "Representation learning of knowledge graphs with entity descriptions", "author": ["Xie et al.2016] Ruobing Xie", "Zhiyuan Liu", "Jia Jia", "Huanbo Luan", "Maosong Sun"], "venue": "In Proceedings of AAAI", "citeRegEx": "Xie et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xie et al\\.", "year": 2016}, {"title": "Embedding entities and relations for learning and inference in knowledge bases", "author": ["Yang et al.2014] Bishan Yang", "Wen-tau Yih", "Xiaodong He", "Jianfeng Gao", "Li Deng"], "venue": "Proceedings of ICLR", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of COLING,", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}, {"title": "Distant supervision for relation extraction via piecewise convolutional neural networks", "author": ["Zeng et al.2015] Daojian Zeng", "Kang Liu", "Yubo Chen", "Jun Zhao"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Zeng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2015}, {"title": "Aligning knowledge and text embeddings by entity descriptions", "author": ["Zhong et al.2015] Huaping Zhong", "Jianwen Zhang", "Zhen Wang", "Hai Wan", "Zheng Chen"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Zhong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zhong et al\\.", "year": 2015}, {"title": "Multiinstance multi-label learning", "author": ["Zhou et al.2012] Zhi-Hua Zhou", "Min-Ling Zhang", "Sheng-Jun Huang", "Yu-Feng Li"], "venue": "Artificial Intelligence", "citeRegEx": "Zhou et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Zhou et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 1, "context": "There are large numbers of KGs like Freebase, YAGO and DBpedia that are widely utilized in nature language processing applications such as question answering and web search (Bollacker et al., 2008).", "startOffset": 173, "endOffset": 197}, {"referenceID": 26, "context": "To alleviate these problems, representation learning (RL) is proposed and widely used, significantly improving the capability of knowledge representations in knowledge inference, fusion and completion (Yang et al., 2014; Dong et al., 2014; Neelakantan et al., 2015).", "startOffset": 201, "endOffset": 265}, {"referenceID": 7, "context": "To alleviate these problems, representation learning (RL) is proposed and widely used, significantly improving the capability of knowledge representations in knowledge inference, fusion and completion (Yang et al., 2014; Dong et al., 2014; Neelakantan et al., 2015).", "startOffset": 201, "endOffset": 265}, {"referenceID": 17, "context": "To alleviate these problems, representation learning (RL) is proposed and widely used, significantly improving the capability of knowledge representations in knowledge inference, fusion and completion (Yang et al., 2014; Dong et al., 2014; Neelakantan et al., 2015).", "startOffset": 201, "endOffset": 265}, {"referenceID": 18, "context": "Many methods have introduced representation learning to KGs, projecting both entities and relations into a continuous low-dimensional vector space (Nickel et al., 2011; Jenatton et al., 2012; Bordes et al., 2013).", "startOffset": 147, "endOffset": 212}, {"referenceID": 12, "context": "Many methods have introduced representation learning to KGs, projecting both entities and relations into a continuous low-dimensional vector space (Nickel et al., 2011; Jenatton et al., 2012; Bordes et al., 2013).", "startOffset": 147, "endOffset": 212}, {"referenceID": 29, "context": "(Wang et al., 2014a; Zhong et al., 2015) propose a joint model projecting both entities and words into the same vector space with alignment models.", "startOffset": 0, "endOffset": 40}, {"referenceID": 25, "context": "(Xie et al., 2016) directly builds entity representations from entity descriptions, while their model is restricted by the completeness and quality of entity descriptions.", "startOffset": 0, "endOffset": 18}, {"referenceID": 16, "context": "As shown in (Nagy et al., 1987), humans learn meanings of new words from their contextual information.", "startOffset": 12, "endOffset": 31}, {"referenceID": 13, "context": "TransD (Ji et al., 2015) proposes dynamic mapping matrix constructed by both entities and relations for multiple representations of entities.", "startOffset": 7, "endOffset": 24}, {"referenceID": 9, "context": "To extend the single-step translating operation, (Gu et al., 2015; Lin et al., 2015a) encode multiple-step relation paths into representation learning of knowledge graphs and achieve significant improvements.", "startOffset": 49, "endOffset": 85}, {"referenceID": 29, "context": "(Zhong et al., 2015) extends the alignment model by considering entity descriptions.", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "(Xie et al., 2016) proposes a new kind of representation for entities, which is directly constructed from entity descriptions using CNN and thus is capable of modeling new entities.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "Multi-instance learning, which was originally proposed in (Dietterich et al., 1997), arises in the tasks where a single object may possess multiple alternative examples or representations that describe it.", "startOffset": 58, "endOffset": 83}, {"referenceID": 30, "context": "(Zhou et al., 2012) proposes multi-instance multi-label learning on multiple tasks such as scene classification and text categorization.", "startOffset": 0, "endOffset": 19}, {"referenceID": 19, "context": "(Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning in distant supervision for relation extraction.", "startOffset": 0, "endOffset": 67}, {"referenceID": 11, "context": "(Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning in distant supervision for relation extraction.", "startOffset": 0, "endOffset": 67}, {"referenceID": 22, "context": "(Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) adopt multi-instance learning in distant supervision for relation extraction.", "startOffset": 0, "endOffset": 67}, {"referenceID": 28, "context": "(Zeng et al., 2015) further combines multiinstance learning with convolutional neural network for relation extraction on distant supervision data.", "startOffset": 0, "endOffset": 19}, {"referenceID": 27, "context": "Inspired by (Zeng et al., 2014), the word representations consist of two parts, including word features and position features.", "startOffset": 12, "endOffset": 31}, {"referenceID": 5, "context": "(Collobert et al., 2011) proposes a mean-pooling approach to solve gradient vanishment to some degree.", "startOffset": 0, "endOffset": 24}, {"referenceID": 0, "context": "The attention-based model is powerful and has been widely applied to machine translation (Bahdanau et al., 2015), abstractive sentence summarization (Rush et al.", "startOffset": 89, "endOffset": 112}, {"referenceID": 20, "context": ", 2015), abstractive sentence summarization (Rush et al., 2015) and speech recognition (Chorowski et al.", "startOffset": 44, "endOffset": 63}, {"referenceID": 4, "context": ", 2015) and speech recognition (Chorowski et al., 2014).", "startOffset": 31, "endOffset": 55}, {"referenceID": 21, "context": "Following the similar protocol in (Socher et al., 2013), for each triple representation, we construct a negative example by randomly replace the head or tail entity with another entity.", "startOffset": 34, "endOffset": 55}, {"referenceID": 8, "context": "Definition extraction is an important task in text mining (Espinosa-Anke et al., 2015).", "startOffset": 58, "endOffset": 86}], "year": 2016, "abstractText": "Textual information is considered as significant supplement to knowledge representation learning (KRL). There are two main challenges for constructing knowledge representations from plain texts: (1) How to take full advantages of sequential contexts of entities in plain texts for KRL. (2) How to dynamically select those informative sentences of the corresponding entities for KRL. In this paper, we propose the Sequential Text-embodied Knowledge Representation Learning to build knowledge representations from multiple sentences. Given each reference sentence of an entity, we first utilize recurrent neural network with pooling or long short-term memory network to encode the semantic information of the sentence with respect to the entity. Then we further design an attention model to measure the informativeness of each sentence, and build text-based representations of entities. We evaluate our method on two tasks, including triple classification and link prediction. Experimental results demonstrate that our method outperforms other baselines on both tasks, which indicates that our method is capable of selecting informative sentences and encoding the textual information well into knowledge representations.", "creator": "LaTeX with hyperref package"}}}