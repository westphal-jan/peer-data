{"id": "1609.07082", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "Large Margin Nearest Neighbor Classification using Curved Mahalanobis Distances", "abstract": "We consider the supervised classification problem of machine learning in Cayley-Klein projective geometries: We show how to learn a curved Mahalanobis metric distance corresponding to either the hyperbolic geometry or the elliptic geometry using the Large Margin Nearest Neighbor (LMNN) framework. We report on our experimental results, and further consider the case of learning a mixed curved Mahalanobis distance. Besides, we show that the Cayley-Klein Voronoi diagrams are affine, and can be built from an equivalent (clipped) power diagrams, and that Cayley-Klein balls have Mahalanobis shapes with displaced centers.", "histories": [["v1", "Thu, 22 Sep 2016 17:41:03 GMT  (1027kb,D)", "https://arxiv.org/abs/1609.07082v1", "21 pages, 8 figures, 5 tables, extend ICIP 2016 paper entitled \"CLASSIFICATION WITH MIXTURES OF CURVED MAHALANOBIS METRICS\""], ["v2", "Mon, 26 Sep 2016 18:34:42 GMT  (1026kb,D)", "http://arxiv.org/abs/1609.07082v2", "21 pages, 8 figures, 5 tables, extend ICIP 2016 paper entitled \"classification With Mixtures of Curved Mahalanobis Metrics\""]], "COMMENTS": "21 pages, 8 figures, 5 tables, extend ICIP 2016 paper entitled \"CLASSIFICATION WITH MIXTURES OF CURVED MAHALANOBIS METRICS\"", "reviews": [], "SUBJECTS": "cs.LG cs.CG cs.CV", "authors": ["frank nielsen", "boris muzellec", "richard nock"], "accepted": false, "id": "1609.07082"}, "pdf": {"name": "1609.07082.pdf", "metadata": {"source": "CRF", "title": "Large Margin Nearest Neighbor Classification using Curved Mahalanobis Distances\u2217", "authors": ["Frank Nielsen", "Boris Muzellec", "Richard Nock"], "emails": ["Frank.Nielsen@acm.org"], "sections": [{"heading": null, "text": "Keywords: Classification; Metric Learning; Cayley Klein Metrics; LMNN; Voronoi Diagrams."}, {"heading": "1 Introduction", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Metric learning", "text": "The distance between point p and q of Rd is defined for a symmetric positive definitive matrix Q 0 by: dM (p, q) = \u221a (p \u2212 q) > Q (p \u2212 q). (1) It is a metric distance that meets the three metric axioms: indiscernibility (dM (p, q) = 0 ff. p = q), symmetry (dM (p, q), and triangular inequality (dM (p, q) + dM (q, r)."}, {"heading": "1.2 Contributions and outline", "text": "We summarize our main contributions as follows: \u2022 We extend the LMNN to hyperbolic Cayley-Klein geometries (\u00a7 4.3), \u2022 We introduce a linearly mixed Cayley-Klein distance and examine its experimental performance (\u00a7 5.2), \u2022 We show that Cayley-Klein-Voronoi diagrams are affin and equivalent to performance diagrams (\u00a7 3.1), and \u2022 We prove that Cayley-Klein-Balls have Mahalanobis shapes with offset centers (\u00a7 3.2). The work is organized as follows: Section 2 introduces the basic concepts of Cayley-Klein geometries and the current formula for elliptic / hyperbolic Cayley-Klein spacings. These elliptic / hyperbolic Cayley-Klein spacings are reinterpreted as curved Mahalanobis spacings (Section 2), the distances in Section 2.4, Section 3 examines some facts that are useful for geometry."}, {"heading": "2 Cayley-Klein geometry", "text": "The real projection space [22] RPd can be understood as the series of lines passing through the origin of the Q-Q space Rd + 1. Projected spaces differ from spherical geometry because antipodal points of the Unit Sphere are identified (they result in the same line passing through the origin).Let RPd = (Rd + 1\\ {0} / p look at the real projection space with the equivalence class relation: (2) x, 1) for \u03bb 6 = 0. A point x in Rd is dehomogenized to a point x (RPd) using homogeneous coordinates x 7 \u2212 p = (x, w = 1) by adding an extra coordinate w. Conversely, a projective point x Rd + 1 = (x, w) is dehomogenized point x RPd with the \"perspective division\" x 7 \u2192 xw Rd."}, {"heading": "2.1 Cayley-Klein distances from cross-ratio measures", "text": "A Cayley Klein geometry is a triple K = (F, cdist, q), where: 1. F is a fundamental cone shape, 2. cdist, c is a constant unit for measuring distances, and 3. cangle, c is a constant unit for measuring angles. The distance in Cayley Klein geometries (see Figure 2) is defined by: dist (p, q) = cdist Log (p, P, Q)), (5) where P and Q are the intersections of the line l = (pq) with the fundamental cone shape F. Historically, the fundamental cone shape has been called the \"absolute\" formula."}, {"heading": "2.2 Dual conics and taxonomy of Cayley-Klein geometries", "text": "In fact, it is as if most people are able to know and understand themselves. (...) In fact, it is as if most people are able to know themselves. (...) It is as if they see themselves able to know themselves. (...) It is as if they were able to change the world. (...) It is as if they were able to know themselves. (...) It is as if they were able to see themselves able to know themselves. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they were able to change the world. (...) It is as if they are able to change the world of the world. (...) It is as if they are in the world of the world. (...) It is as if they are in the world of the world. (...) It is as if they are in the world of the world. (...) It is as if they are in the world of the world. (... It is as if they are in the world of the world.) It is as if they are in the world of the world of the world."}, {"heading": "2.3 Bilinear form and formula for the hyperbolic/elliptic Cayley-Klein distances", "text": "To get the real value of the Cayley Klein distances, we select the constants as follows: (with \u043c q q q q = q q q = = curvature): \u2022 Elliptical (\u0445 > 0): Elliptical = \u03ba2i, \u2022 Hyperbolic (\u043c < 0): cdist = \u2212 \u03ba2. By using the bilinear form for a (d + 1) \u00b7 (d + 1) matrix S: (p >, 1) > S (q, 1) = p > Sq, (7) we free the intersection ratio expression in the distance / angle formula of Eq. 5 and Eq. 6 using [22]: (p, q; P, Q) = Spq \u2212 S2pSqqSpayq \u2212 Spayq \u2212 Spayq \u2212 Sq."}, {"heading": "2.4 Cayley-Klein elliptic/hyperbolic distances: Curved Malahanobis distances", "text": "Bi et. al [4] has rewritten the two-dimensional form as follows: LetS = [q = q a > b] = S\u03a3, a, b, (14) with \u03a3 0 a \u00b7 d \u00b7 d \u00b7 d-dimensional matrix and a, b \u00b2 Rd, so that: Sp, q = p > q + p > a > q + b. (15) Let \u00b5 = \u2212 v = \u2212 p < p > (16) Then the two-dimensional form can be rewritten as follows: S (p, q) = S (p) = p > p \u00b2 2, so that: \u0432 = (b \u2212 \u00b5 > \u00b5) \u2212 12 b > p < \u00b5 > p (16) Then the two-dimensional form can be rewritten as follows: S (p, q) = S (p, q) = spacing (p, q) = (p, p) > p (p \u2212 p) > p (p) > p \u2212 grinding p & ltp &ltp < \u00b5 > (16)."}, {"heading": "3 Computational geometry in Cayley-Klein geometries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Cayley-Klein Voronoi diagrams", "text": "Define the bi-sector Bi (p, q) of points p and q as follows: Bi (p, q) = {x-DS: distS (p, x) = distS (x, q)}. (21) Then the bi-sector is a hyperplane (finally truncated to domain D) with the equation: x, \u221a | S (p, p) | 0 (22) Figure 4 shows two examples of the bisectors of the two points in planar hyperbolic Cayley-Klein geometry. Thus, the Cayley-Klein-Voronoi diagram is an affinagram (q, q) | (a > (p + x) + b) = 0 (22) Voronoi-Voronoi diagram in which Voronoi-Voronoi-Voronoi diagram can be calculated as equivalent (truncated)."}, {"heading": "3.2 Cayley-Klein balls have Mahalanobis shapes with displaced centers", "text": "A Cayley Small Sphere B of center c and radius r is defined by: BCK (c, r) = > B > > Klein = > x: dCK (x, c) \u2264 r. \"(26) The Cayley Small Sphere S = \u2202 BCK has the equation dCK (x, c) = r. For comparison, the spheres of Cayley-Klein are represented at different center positions in the elliptical case (red) and in the hyperbolic case (green) (but for fixed elliptical and hyperbolic geometries). (For comparison: The spheres of Mahalanobis are shown here (blue): This drawing allows us to visualize the anisotropy of Mahalanobis Small Spheres, which have a shape depending on the middle position, while Mahalanobis spheres have identical shapes (isotropy) everywhere. (c) It is noted in Figure 6 that the spheres of Cayley-Klein have the shape from Mahalanox to Mahalanox."}, {"heading": "4 Learning curved Mahalanobis metrics", "text": "In the technique called Mahalanobis Metric for Clustering (MMC) [27], Xing and al. use pairwise information to learn a global Mahalanobis metric. [27] We learn a matrix M 0 by gradient descent so that the total pairwise distance in D is maximized, while the pairs that are unequal (e.g., have different labels), Xing and al. We learn a matrix M 0 by gradient descent so that the pairwise distance in D is maximized while keeping the total pairwise distance in S constant. While good performances are achieved experimentally, this MMC method tends to cluster similar points together and can therefore be poorly performed in the multimodal data."}, {"heading": "4.1 Large Margin Nearest Neighbors (LMNN)", "text": "In view of the fact that we have one unit in relation to points that have different functions to define the cost, we take the number of points we are oriented to. (x1, y1), (x1, y1), (x1, y1), (x2), (x1), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x3), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2), (x2, (x2), (x2), (x2), (x2, (x2), (x2), (x2), (x2), (x2, (x2), (x2), (x2), (x2, (x2), (x2), (x2), (x2, (x2), (x2), (x2, (x2), (x2), (x2, (x2), (x2), (x2, (x2), (x2, (x2), (x2), (x2, (x2), (x2), (x2, (x2), (x2), (x2, (x2), (x2, (x2), (x2), (x2, (x2), (x2, (x2), (x2, (x2, (x2), (x2), (x2), (x2), (x2), (x2, (x2), (x2), (x2), (x2, (x2), (x2), (x2, (x2), ("}, {"heading": "4.2 Elliptic Cayley-Klein LMNN", "text": "Bi et al. [4] consider the extension of LMNN to the case of the elliptical Cayley Klein geometry. The cost function is defined as: (L) = (1 \u2212 \u00b5) \u2211 i, i \u2192 j dE (xi, xj) + (1 \u2212 yil) \u0433ijl (39) with [1 + dE (xi, xj) \u2212 dE (xi, xl)] +. (40) Gradation 5 in relation to the lower triangular matrix L is calculated as: (L) axy L = (1 \u2212 \u00b5) \u2211 i, i \u2192 j - rixi dE (xi, xj). (1 \u2212 rixi). (1 \u2212 m). Gradation 5 in relation to the lower triangular matrix. (41). (There are minor errors in the gradation of L = (L). (L)."}, {"heading": "4.3 Hyperbolic Cayley-Klein LMNN", "text": "To ensure that the (d + 1) \u00b7 (d + 1) dimensional matrix S maintains the correct signature (1, 0, d) during the LMNN gradient descent, we disassemble S = L > DL (with L 0) and perform a gradient descent to L with the following gradient. (45) We initialize L = (L \u2032 1) and D so that P \u00b2 DS is represented as follows: Leave \u2212 1 = L \u2212 > Vorkjj DL (e.g. by selecting the precision matrix of P \u2212 (Cij + Cji) and then select the diagonal matrix as: D = \u2212 1.. \u2212 1\u043c maxx \u2012 L \u00b2 x \u00b2, (46) with the hyperfix zone of P \u2212 n (e.g. by selecting the precision matrix of P \u2212 1 of P \u2212 (Cij + Cji)."}, {"heading": "5 Experimental results", "text": "We report on our experimental results on some UCI datasets. [6] The descriptions of these marked datasets are briefly summarized in Table 3. We performed k = 3 classification of the closest neighbors. As in [4], we performed a leave-one-out cross-validation for the wine dataset, while we trained the model for balance, pica and vowel datasets on random subsets of size 250, tested them on the remaining data and repeated this procedure 10 times. We observed that the elliptical CK-LMNN performs significantly better than the Mahalanobis-LMNN and the hyperbolic CK-LMNN."}, {"heading": "5.1 Spectral decomposition and proximity queries in Cayley-Klein geometry", "text": "In order to avoid that dE or dH for the arbitrary Matrix S = > \u00b2 \u00b2 = > x \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2 \u00b2."}, {"heading": "5.2 Mixed curved Mahalanobis distance", "text": "We consider the mixed elliptical / hyperbolic Cayley-Klein distance: d (x, y) = \u03b1dE (x, y) + (1 \u2212 \u03b1) dH (x, y). (51) Since the sum of (belt) metric distances is a (belt) metric distance, we conclude that d (x, y) is a (belt) metric distance. Although the metric tensors mix locally, this \"fusion\" of positive with negative constant curvature geometries (belt) does not solve a constant curvature geometry. In fact, the Ordinary Differential Equation (ODE) characterizing geodesy solves geodesy differently. Note that we have a limited distance (elliptical CK) with an unlimited distance (hyperbolic CK) above the hyperparticle one that needs to be tuned."}, {"heading": "6 Conclusion and perspectives", "text": "First, we examined some nice properties of Voronoi diagrams and balls in Cayley-Klein geometry: We proved that the Cayley Klein Voronoi diagram is affinity, and reported that it was constructed using a formula as an equivalent (truncated) performance diagram. Then, we showed that Cayley Klein Voronoi balls have Mahalanobis shapes with offset centers, and gave the explicit conversion formula. Second, we expanded the LMNN framework to include hyperbolic Cayley Klein geometries not included in [4], and proposed to learn a mixed elliptical / hyperbolic distance, which experimentally shows a good improvement over constantly curved Cayley-Klein geometries. The fact that the Cayley Klein bisectors are hyperplanes provides nice computational perspectives in machine learning, and small-geometry would be interesting to study."}], "references": [{"title": "Information Geometry and Its Applications. Applied Mathematical Sciences", "author": ["S. Amari"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Clustering with Bregman divergences", "author": ["Arindam Banerjee", "Srujana Merugu", "Inderjit S Dhillon", "Joydeep Ghosh"], "venue": "Journal of machine learning research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2005}, {"title": "Beyond Mahalanobis metric: Cayley-Klein metric learning", "author": ["Yanhong Bi", "Bin Fan", "Fuchao Wu"], "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Anisotropic Delaunay mesh generation", "author": ["Jean-Daniel Boissonnat", "Camille Wormser", "Mariette Yvinec"], "venue": "SIAM Journal on Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Hilbert geometry for convex polygonal domains", "author": ["Bruno Colbois", "Constantin Vernicos", "Patrick Verovic"], "venue": "Journal of Geometry,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2011}, {"title": "Hilbert geometry for strictly convex domains", "author": ["Bruno Colbois", "Patrick Verovic"], "venue": "Geometriae Dedicata,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Foundations of multi-dimensional metric scaling in Cayley-Klein geometries", "author": ["Jan Dr\u00f6sler"], "venue": "British Journal of Mathematical and Statistical Psychology,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1979}, {"title": "Learning local invariant mahalanobis distances", "author": ["E. Fetaya", "S. Ullman"], "venue": "International Conference on Machine Learning (ICML),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Convex Polytopes, volume 221", "author": ["Branko Gr\u00fcnbaum"], "venue": "Springer Science & Business Media,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Geometry, Kinematics, and Rigid Body Mechanics in Cayley-Klein Geometries", "author": ["C. Gunn"], "venue": "PhD thesis, Technische Universita\u0308t Berlin,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "On Bregman Voronoi diagrams", "author": ["Frank Nielsen", "Jean-Daniel Boissonnat", "Richard Nock"], "venue": "In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "Classification with mixtures of curved Mahalanobis metrics", "author": ["Frank Nielsen", "Boris Muzellec", "Richard Nock"], "venue": "In IEEE International Conference on Image Processing (ICIP),", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Hyperbolic Voronoi diagrams made easy", "author": ["Frank Nielsen", "Richard Nock"], "venue": "In IEEE International Conference on Computational Science and Its Applications (ICCSA),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2010}, {"title": "Further results on the hyperbolic Voronoi diagrams", "author": ["Frank Nielsen", "Richard Nock"], "venue": "CoRR, abs/1410.1036,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Bregman vantage point trees for efficient nearest neighbor queries", "author": ["Frank Nielsen", "Paolo Piro", "Michel Barlaud"], "venue": "In IEEE International Conference on Multimedia and Expo,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Projective and Cayley-Klein Geometries", "author": ["Arkadij L Onishchik", "Rolf Sulanke"], "venue": "Springer Science & Business Media,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2006}, {"title": "Local distance functions: A taxonomy, new algorithms, and an evaluation", "author": ["D. Ramanan", "S. Baker"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Perspectives on Projective Geometry: A Guided Tour Through Real and Complex", "author": ["J\u00fcrgen Richter-Gebert"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Sparse compositional metric learning", "author": ["Yuan Shi", "Aur\u00e9lien Bellet", "Fei Sha"], "venue": "In Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2014}, {"title": "Projective spaces with Cayley-Klein metrics", "author": ["Horst Struve", "Rolf Struve"], "venue": "Journal of Geometry,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2004}, {"title": "Non-euclidean geometries: the Cayley-Klein approach", "author": ["Horst Struve", "Rolf Struve"], "venue": "Journal of Geometry,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["Kilian Q. Weinberger", "John Blitzer", "Lawrence K. Saul"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2006}, {"title": "Distance metric learning, with application to clustering with side-information", "author": ["Eric P. Xing", "Andrew Y. Ng", "Michael I. Jordan", "Stuart Russell"], "venue": "In Advances in neural information processing", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2003}, {"title": "Data structures and algorithms for nearest neighbor search in general metric spaces", "author": ["Peter N. Yianilos"], "venue": "In Proceedings of the Fourth Annual ACM-SIAM Symposium on Discrete Algorithms,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1993}], "referenceMentions": [{"referenceID": 11, "context": "\u2217A preliminary work appeared at IEEE International Conference on Image Processing (ICIP) 2016 [16].", "startOffset": 94, "endOffset": 98}, {"referenceID": 21, "context": "[26] proposed an efficient method to learn a Mahalanobis distance: The Large Margin Nearest Neighbor (LMNN) algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The LMNN algorithm was further extended to elliptic Cayley-Klein geometries in [4].", "startOffset": 79, "endOffset": 82}, {"referenceID": 2, "context": "[4] to elliptic Cayley-Klein geometries, and describe our novel extension to hyperbolic Cayley-Klein geometries in \u00a7 4.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "2 Cayley-Klein geometry The real projective space [22] RP can be understood as the set of lines passing through the origin of the vector space Rd+1.", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "In projective geometry [22], the cross-ratio (Figure 1) of four collinear points p, q, P,Q on a line is defined by:", "startOffset": 23, "endOffset": 27}, {"referenceID": 17, "context": "The cross-ratio is a measure that is invariant by projectivities [22] (see Figure 1 (a)), also called collineations or homographies.", "startOffset": 65, "endOffset": 69}, {"referenceID": 17, "context": "A gentle introduction to projective geometry and Cayley-Klein geometries can be found in [22, 24, 25].", "startOffset": 89, "endOffset": 101}, {"referenceID": 19, "context": "A gentle introduction to projective geometry and Cayley-Klein geometries can be found in [22, 24, 25].", "startOffset": 89, "endOffset": 101}, {"referenceID": 20, "context": "A gentle introduction to projective geometry and Cayley-Klein geometries can be found in [22, 24, 25].", "startOffset": 89, "endOffset": 101}, {"referenceID": 15, "context": "We also refer the reader to a more advanced textbook [20] handling invariance and isometries, and to the historical seminal paper [7] of Cayley (1859).", "startOffset": 53, "endOffset": 57}, {"referenceID": 17, "context": "This formula generalizes the Laguerre formula that calculates the acute angle between two distinct real lines [22].", "startOffset": 110, "endOffset": 114}, {"referenceID": 5, "context": "The Cayley-Klein geometries can further be extended to Hilbert projective geometries [9] by replacing the conic object F with a bounded convex subset of Rd.", "startOffset": 85, "endOffset": 88}, {"referenceID": 4, "context": "Interestingly, the convex objects delimiting the Hilbert geometry domain do not need to be strictly convex [8].", "startOffset": 107, "endOffset": 110}, {"referenceID": 17, "context": "For example, Pascal\u2019s theorem is dual to Brianchon\u2019s theorem [22].", "startOffset": 61, "endOffset": 65}, {"referenceID": 8, "context": "This is similar to the dual H-representation and V -representation of finite convex polytopes [13] (\u2019H\u2019 standing for Halfspaces, and \u2019V\u2019 for Vertex).", "startOffset": 94, "endOffset": 98}, {"referenceID": 17, "context": "All degenerate cases can be obtained as the limit of non-degenerate cases, see [22].", "startOffset": 79, "endOffset": 83}, {"referenceID": 17, "context": "Each type of measurement is of three kinds [22]: elliptic or hyperbolic for non-degenerate geometries or parabolic for degenerate cases.", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "With the following choice cdist = \u2212 2 and cangle = i 2 , we obtain [22] (Chapter 20):", "startOffset": 67, "endOffset": 71}, {"referenceID": 9, "context": "In higher dimensions [14, 22], Cayley-Klein geometries unify common space geometries (euclidean, elliptical, and hyperbolic) with other space-time geometries (Minkowskian, Galilean, de Sitter, etc.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "In higher dimensions [14, 22], Cayley-Klein geometries unify common space geometries (euclidean, elliptical, and hyperbolic) with other space-time geometries (Minkowskian, Galilean, de Sitter, etc.", "startOffset": 21, "endOffset": 29}, {"referenceID": 17, "context": "6 using [22]:", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "Those elliptic/hyperbolic distances can be interpreted from projections [22, 18], as depicted in Figure 3.", "startOffset": 72, "endOffset": 80}, {"referenceID": 13, "context": "Those elliptic/hyperbolic distances can be interpreted from projections [22, 18], as depicted in Figure 3.", "startOffset": 72, "endOffset": 80}, {"referenceID": 2, "context": "al [4] rewrote the bilinear form as follows: Let", "startOffset": 3, "endOffset": 6}, {"referenceID": 2, "context": "Furthermore, it is proved in [4] that:", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "Indeed, we choose to term those hyperbolic/elliptic Cayley-Klein distances \u201ccurved Mahalanobis distances\u201d to constrast with the fact that (squared) Mahalanobis distances are symmetric Bregman divergences that induce a (self-dual) flat geometry in information geometry [1].", "startOffset": 268, "endOffset": 271}, {"referenceID": 12, "context": ", 1,\u22121), we recover the canonical hyperbolic distance [17] in Cayley-Klein model:", "startOffset": 54, "endOffset": 58}, {"referenceID": 10, "context": "Therefore the Cayley-Klein Voronoi diagram can be computed as an equivalent (clipped) power diagram [15, 5, 17], using the following conversion formula:", "startOffset": 100, "endOffset": 111}, {"referenceID": 12, "context": "Therefore the Cayley-Klein Voronoi diagram can be computed as an equivalent (clipped) power diagram [15, 5, 17], using the following conversion formula:", "startOffset": 100, "endOffset": 111}, {"referenceID": 22, "context": "In the technique called Mahalanobis Metric for Clustering (MMC) [27], Xing and al.", "startOffset": 64, "endOffset": 68}, {"referenceID": 22, "context": "[27] learn a matrix M 0 by gradient descent such that the total pairwise distance in D in maximized, while keeping the total pairwise distance in S constant.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "LMNN [26] on the other hand is a projection-free metric learning method.", "startOffset": 5, "endOffset": 9}, {"referenceID": 7, "context": "For example, in [12], Fetaya and Ullman learn one Mahalanobis metric per data point using only negative examples (eg.", "startOffset": 16, "endOffset": 20}, {"referenceID": 18, "context": "In [23], Shi et.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "For a comprehensive survey on local metric learning, we refer the reader to [21].", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": ", xn of Rd, the Large Margin Nearest Neighbors4 (LMNN) [26] learns a Mahalanobis distance (ie.", "startOffset": 55, "endOffset": 59}, {"referenceID": 1, "context": "(It is a Bregman divergence [3, 15, 5].", "startOffset": 28, "endOffset": 38}, {"referenceID": 10, "context": "(It is a Bregman divergence [3, 15, 5].", "startOffset": 28, "endOffset": 38}, {"referenceID": 21, "context": "To define the objective cost function [26] in LMNN, we consider two sets S and R, or target neighbors and impostors:", "startOffset": 38, "endOffset": 42}, {"referenceID": 21, "context": "Using Cholesky decomposition M = L>L 0, the LMNN cost function [26] is then defined as: pull(L) = \u03a3i,i\u2192j\u2016L(xi \u2212 xj)\u2016, (34) push(L) = \u03a3i,i\u2192j\u03a3j(1\u2212 yil) [ 1 + \u2016L(xi \u2212 xj)\u2016 \u2212 \u2016L(xi \u2212 xl)\u2016 ] + , (35) (L) = (1\u2212 \u03bc) pull(L) + \u03bc push(L), (36) where [x]+ = max(0, x) and \u03bc is a trade-off parameter for tuning target/impostor relative importance, and i\u2192 j indicates that xj is a target neighbor of xi.", "startOffset": 63, "endOffset": 67}, {"referenceID": 21, "context": "The LMNN cost function is convex and piecewise linear [26].", "startOffset": 54, "endOffset": 58}, {"referenceID": 21, "context": "Instead, Weinberger and Saul [26] propose a gradient descent where the set of impostors is re-computed every 10 to 20 iterations.", "startOffset": 29, "endOffset": 33}, {"referenceID": 22, "context": "There is no projection mechanism like for the Mahalanobis Metric for Clustering (MMC) [27] method.", "startOffset": 86, "endOffset": 90}, {"referenceID": 2, "context": "We shall now consider extensions of the LMNNmethod to Cayley-Klein elliptic [4] and hyperbolic geometries.", "startOffset": 76, "endOffset": 79}, {"referenceID": 2, "context": "[4] consider the extension of LMNN to the case of elliptic Cayley-Klein geometry.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4], as Cij +Cji was replaced by 2Cij , which cannot be the distance gradient that must be symmetric with respect to xi and xj .", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Such a matrix is called a generalized Mahalanobis matrix in [4].", "startOffset": 60, "endOffset": 63}, {"referenceID": 2, "context": "As in [4], we performed leave-one-out cross validation for the wine data-set, whereas for balance, pima and vowel data-sets, we trained the model on random subsets of size 250, testing it on the remaining data and repeating this procedure 10 times.", "startOffset": 6, "endOffset": 9}, {"referenceID": 23, "context": "For example, we may consider the Vantage Point Tree data-structures [28, 19].", "startOffset": 68, "endOffset": 76}, {"referenceID": 14, "context": "For example, we may consider the Vantage Point Tree data-structures [28, 19].", "startOffset": 68, "endOffset": 76}, {"referenceID": 2, "context": "Second, we extended the LMNN framework to hyperbolic Cayley-Klein geometries that were not considered in [4], and proposed learning a mixed elliptic/hyperbolic distance that experimentally shows good improvement over constant-curvature Cayley-Klein geometries.", "startOffset": 105, "endOffset": 108}, {"referenceID": 6, "context": "For example, it would be interesting to study Multi-Dimensional Scaling [11] or Support Vector Machines (SVMs) in Cayley-Klein geometries, or to mesh anisotropically [6] in Cayley-Klein geometries.", "startOffset": 72, "endOffset": 76}, {"referenceID": 3, "context": "For example, it would be interesting to study Multi-Dimensional Scaling [11] or Support Vector Machines (SVMs) in Cayley-Klein geometries, or to mesh anisotropically [6] in Cayley-Klein geometries.", "startOffset": 166, "endOffset": 169}], "year": 2016, "abstractText": "We consider the supervised classification problem of machine learning in Cayley-Klein projective geometries: We show how to learn a curved Mahalanobis metric distance corresponding to either the hyperbolic geometry or the elliptic geometry using the Large Margin Nearest Neighbor (LMNN) framework. We report on our experimental results, and further consider the case of learning a mixed curved Mahalanobis distance. Besides, we show that the Cayley-Klein Voronoi diagrams are affine, and can be built from an equivalent (clipped) power diagrams, and that Cayley-Klein balls have Mahalanobis shapes with displaced centers.", "creator": "LaTeX with hyperref package"}}}