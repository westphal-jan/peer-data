{"id": "1609.07132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "22-Sep-2016", "title": "A Fully Convolutional Neural Network for Speech Enhancement", "abstract": "In hearing aids, the presence of babble noise degrades hearing intelligibility of human speech greatly. However, removing the babble without creating artifacts in human speech is a challenging task in a low SNR environment. Here, we sought to solve the problem by finding a `mapping' between noisy speech spectra and clean speech spectra via supervised learning. Specifically, we propose using fully Convolutional Neural Networks, which consist of lesser number of parameters than fully connected networks. The proposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates that a convolutional network can be 12 times smaller than a recurrent network and yet achieves better performance, which shows its applicability for an embedded system: the hearing aids.", "histories": [["v1", "Thu, 22 Sep 2016 19:57:08 GMT  (5020kb,D)", "http://arxiv.org/abs/1609.07132v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["se rim park", "jinwon lee"], "accepted": false, "id": "1609.07132"}, "pdf": {"name": "1609.07132.pdf", "metadata": {"source": "CRF", "title": "A FULLY CONVOLUTIONAL NEURAL NETWORK FOR SPEECH ENHANCEMENT", "authors": ["Se Rim Park", "Jin Won Lee"], "emails": [], "sections": [{"heading": null, "text": "Index terms - speech enhancement, speech denosis, babble noise, fully revolutionary neural network, revolutionary encoder decoder network, redundant revolutionary encoder decoder network"}, {"heading": "1. INTRODUCTION", "text": "In the last few years it has been shown that the number of people who are able to move, and that they are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance"}, {"heading": "2. PROBLEM STATEMENT", "text": "In view of a segment of noisy spectra {xt} Tt = 1 and clean spectra {yt} Tt = 1, our goal is to learn a figure f that generates a segment of \"denoted\" spectra {f (xt) Tt = 1 that comes closer to the recursive spectra in the \"2 standard,\" e.g. min T \u2211 t = 1 | | yt \u2212 f (xt) | 22. (1) Specifically, we formulate f using a neural network (see Fig.1). If f is a recursive network, the temporal behavior of input spectra is already addressed by the network, and therefore objective (1) is sufficient. On the other hand, for a revolutionary network, the past nT noisy spectra {xi} ti = t \u2212 nT + 1 are considered to denode the current spectrum behavior, i.e. for a conventional network, the past nT noisy spectra {xi} t = nT + 1 are denoted as the current spectrum (see below)."}, {"heading": "3. CONVOLUTIONAL NETWORK ARCHITECTURES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Convolutional Encoder-Decoder Network (CED)", "text": "The Convolutional Encoder Decoder (CED) network proposed in [16] consists of symmetrical coding layers and decoding layers (see Fig.2, each block represents a feature).Encoder consists of repetitions of a folding, batch normalization [17], max pooling and a ReLU [18] activation layer. Decoder consists of repetitions of a folding, batch normalization and an up-sampling layer. Typically, CED compresses the features along the encoder and then reconstructs the features along the decoder. In our problem, the original Softmax layer on the last layer is modified to a folding layer to make CED a fully revolutionary network."}, {"heading": "3.2. Redundant CED Network (R-CED)", "text": "Here we propose an alternative revolutionary network architecture, namely the Redundant Convolutional Encoder Decoder (R-CED) network. R-CED consists of repetitions of a folding, batch normalization and a ReLU activation layer (see Fig.3, each block represents a feature). There is no pooling layer, and therefore no upsampling layer is required. Unlike CED, R-CED encodes the features along the encoder into a higher dimension and achieves compression along the decoder. The number of filters remains symmetrical: the number of filters is gradually increased on the encoder, and the number of filters gradually decreases on the decoder. The last layer is a conversion that makes R-CED a fully revolutionary network."}, {"heading": "3.3. Bypass Connections", "text": "In CED, R-CED and CR-CED, bypass connections are added to the network to facilitate optimization in the training phase and improve performance. Between two different bypass schemes - skip connections in [14] and residual connections in [15] - we opted for skip connections in [14], which are more suitable for symmetrical encoder decoder design. Bypass connections are displayed as an \"additional\" operating symbol with arrow in Fig.2 and Fig.3. Bypass connections are added every two layers."}, {"heading": "3.4. 1-Dim Convolution Operation for Convolution Layers", "text": "In Fig.4, the input (3 x 3 white matrix) and the filter (2 x 3 blue matrix) have the same dimension in the time axis, and the folding occurs in the frequency axis. We found this to be more efficient for our input spectra (129 x 8) than 2 x weak folding (see Fig.5)."}, {"heading": "4. EXPERIMENTAL METHODS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Preprocessing", "text": "Dataset: The experiment was conducted on the TIMIT database [19] and 27 different types of noise clips were collected from freely available online resources [20]. Noise is mostly babble, but includes different types of noise such as instrumental noises. However, both data in the training set (4620 utterances) and in the test set (200 utterances) were added with one of 27 noise clips at 0dB SNR. After all feature transformation steps were completed, 20% of the training features were used as validation set.Feature Transformation: The audio signals were sampled down to 8kHz, and the silent frames were removed from the signal. Spectral vectors were calculated using a 256-point short-time Fourier transformer (32 ms Hamming window) with a 64-point phase (8ms) window shift in the frequency resolution of 31.25 Hz (= 4kz per H8 frequency range)."}, {"heading": "4.2. Optimization", "text": "Fully connected and recurrent layer weights were initialized as in [22] and recurrent layer weights as in [23]. Fully connected and recurrent layer weights were pre-trained by networks of smaller depth with the same number of nodes. Folding layers were trained from the ground up, with an additional layer added after each folding layer [17]. All networks were re-propagated using gradient descend optimization using Adam [24] with a mini-batch size of 64. Learning rate started from lr = 0.0015 with \u03b21 = 0.9, \u03b22 = 0.999 and = 1.0e \u2212 8. When the validation loss did not decrease for more than 4 epochs, the learning rate decreased to lr / 2, lr / 3, lr / 4, and the training was repeated once again for FNN and RNN with '2 regulation, which slightly improved performance."}, {"heading": "4.3. Evaluation Metric", "text": "Signal to Distortion Ration (SDR) [25] was used to measure the amount of \"2 errors\" between clean and denoted speech: SDR: = 10 log10 | | y | | 2 | | f (x) \u2212 y | | 2. SDR is inversely associated with the objective function represented in (1). Furthermore, Short Time Objective Intelligibility (STOI) [26] and Perceptual Evaluation of Speech Distortion (PESQ) [27] - both of which assume that human perception has a short-term memory and therefore the error is not measured linearly in the time of interest - were used to measure the subjective quality of listening."}, {"heading": "5. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1. Test 1: FNN vs. RNN vs. CNN", "text": "The first experiment compared CNN with FNN and RNN to show how practicable it is to use CNN for speech enhancement. Network configurations (e.g. number of nodes, number of layers) that delivered the best performance for each network are shown in Table.2. The best FNN and RNN architectures have 4 fully connected (FC) layers, whereas CNN has 16 twisted layers."}, {"heading": "5.2. Test 2: CED vs. R-CED", "text": "In the second experiment, R-CED was compared to CED. To make a fair comparison, the total number of parameters was set at 33,000 (approximately 132MB of memory), while the depth of the network was set to 10 folding layers. Accordingly, the filter width per layer is determined to i) maintain the symmetrical encoder decoder structure, ii) gradually increase and decrease the number of parameters, iii) the \"frequency coverage\" for both networks is the same. \"Frequency coverage\" refers here to how many nearby frequency bins are used at the input to reconstruct a single frequency bin the output. We have ensured that both networks use the same amount of frequency bins to reconstruct a single frequency bin. Test 2 configurations are summarized in the first two lines of the table. 1."}, {"heading": "5.3. Test 3: Finding the Best R-CED Performance", "text": "In the third experiment, we tested the extent to which performance can be improved with the R-CED network by comparing the R-CED and CR-CED networks with connections of different network sizes and depths. Considered network size (number of parameters) is 33K (132MB of memory) and 100K (400MB of memory), the network depth considered is 10, 16 and 20 conversion layers. Table 1 below 3 summarizes the network configurations for Test 3."}, {"heading": "6. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1. Test 1: FNN vs. RNN vs. CNN", "text": "Fig.9 illustrates the denocializing performance of FNN, RNN, and CNN (left) and the corresponding network size (= number of parameters, right).All networks had similar performance based on both subjective (STOI, PESQ) and objective quality (SDR).On the other hand, CNN's model size was about 68 times smaller than FNN's and about 12 times smaller than RNN's. We note that FNN and RNN were optimized for the smallest network architecture.This experiment confirms that CNN requires much less parameters per layer due to its weight distribution property and can still achieve similar or better performance than FNN and RNN. 33,000 parameters for CNN are approximately 132MB of memory that can be implemented in an embedded system. See Fig.6, Fig.7, Fig.8 for examples of loud spectrograms, clean spectrograms, or CNN spectrograms."}, {"heading": "6.2. Test 2: CED vs. R-CED", "text": "The R-CED with skip connections showed the best performance, while the CED without skip connections showed the worst performance. Regardless of the presence of skip connections, R-CED delivered better results than CED. The effect of the skip connection was pronounced in CED (5.96 to 7.92).This implies that the decoder itself could not reconstruct the \"lost\" information compressed on the encoder unless the \"lost\" information was provided by the skip connections.In addition, the resulting language from CED sounded artificial and mechanical, confirming that the decoder could not reconstruct what is needed for audio to sound like human language.On the other hand, the effect of the skip connection in R-CED (8.07 to 8.19) was not as noticeable because R-CED extends the coder input rather than compressing, which can be effectively represented by removing an important spectrum in decrypting."}, {"heading": "6.3. Test 3: Finding the Best R-CED Network", "text": "Some interesting observations are that i) network size was the most dominant factor associated with network performance, ii) network depth was secondary, iii) CR-CED with skip connection performed best when other conditions remained the same (16 folding layers, 33K parameters)."}, {"heading": "7. CONCLUSION", "text": "Inspired by the successes of FNN and RNN in the past, we hypothesized that CNN can effectively denocialize speech with a smaller network size according to its weight distribution property. We conducted an experiment to denocialize human speech against noise levels, which is a major discomfort for hearing aid users. Through experiments, we demonstrated that CNN can perform similarly or better with a much lower number of model parameters compared to FNN and RNN. We also proposed a new fully revolutionary network architecture R-CED and demonstrated its effectiveness in speech enhancement. We observed that the success of R-CED is associated with the increasing dimension of featurespace along the encoder and the decreasing dimension along the decoder. We expect that R-CED can also be applied to other interesting areas."}, {"heading": "8. REFERENCES", "text": "[1] Steven Boll, \"Suppression of acoustic noise in speech using spectral subtraction,\" IEEE Transactions on acoustics, speech, and signal processing, vol. 27, no. 2, pp. 113-120, 1979. [2] Jae S Lim and Alan V Oppenheim, \"Enhancement and bandwidth IES 32,\" Speech enhancement of noisy speech, \"Proceedings of the IEEE, vol. 67, no. 12, pp. 1586-1604, 1979. [3] Yariv Ephraim and David Malah,\" Speech enhancement \"using a minimum-mean square error short-time spectral estimator,\" IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 1109-1121, 1984. [4] Pascal Scalart et al."}], "references": [{"title": "Suppression of acoustic noise in speech using spectral subtraction", "author": ["Steven Boll"], "venue": "IEEE Transactions on acoustics, speech, and signal processing, vol. 27, no. 2, pp. 113\u2013120, 1979.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1979}, {"title": "Enhancement and bandwidth compression of noisy speech", "author": ["Jae S Lim", "Alan V Oppenheim"], "venue": "Proceedings of the IEEE, vol. 67, no. 12, pp. 1586\u20131604, 1979.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1979}, {"title": "Speech enhancement using a minimum-mean square error short-time spectral amplitude estimator", "author": ["Yariv Ephraim", "David Malah"], "venue": "IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 6, pp. 1109\u20131121, 1984.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1984}, {"title": "Speech enhancement based on a priori signal to noise estimation", "author": ["Pascal Scalart"], "venue": "Acoustics, Speech, and Signal Processing, 1996. ICASSP-96. Conference Proceedings., 1996 IEEE International Conference on. IEEE, 1996, vol. 2, pp. 629\u2013632.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1996}, {"title": "A signal subspace approach for speech enhancement", "author": ["Yariv Ephraim", "Harry L Van Trees"], "venue": "IEEE Transactions on speech and audio processing, vol. 3, no. 4, pp. 251\u2013266, 1995.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1995}, {"title": "Babble noise: modeling, analysis, and applications", "author": ["Nitish Krishnamurthy", "John HL Hansen"], "venue": "IEEE transactions on audio, speech, and language processing, vol. 17, no. 7, pp. 1394\u20131407, 2009.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2009}, {"title": "Why do people fitted with hearing aids not wear them", "author": ["Abby McCormack", "Heather Fortnum"], "venue": "International Journal of Audiology, vol. 52, no. 5, pp. 360\u2013368, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning spectral mapping for speech dereverberation", "author": ["Kun Han", "Yuxuan Wang", "DeLiang Wang"], "venue": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 4628\u20134632.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "A regression approach to speech enhancement based on deep neural networks", "author": ["Yong Xu", "Jun Du", "Li-Rong Dai", "Chin-Hui Lee"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 1, pp. 7\u201319, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Speech enhancement with weighted denoising auto-encoder", "author": ["Bingyin Xia", "Changchun Bao"], "venue": "INTERSPEECH, 2013, pp. 3444\u20133448.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Complex recurrent neural networks for denoising speech signals", "author": ["Keiichi Osako", "Rita Singh", "Bhiksha Raj"], "venue": "Applications of Signal Processing to Audio and Acoustics (WASPAA), 2015 IEEE Workshop on. IEEE, 2015, pp. 1\u20135.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for speech recognition", "author": ["Ossama Abdel-Hamid", "Abdel-Rahman Mohamed", "Hui Jiang", "Li Deng", "Gerald Penn", "Dong Yu"], "venue": "IEEE/ACM Transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533\u20131545, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Image denoising using very deep fully convolutional encoder-decoder networks with symmetric skip connections", "author": ["Xiao-Jiao Mao", "Chunhua Shen", "Yu-Bin Yang"], "venue": "arXiv preprint arXiv:1603.09056, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["Pascal Vincent", "Hugo Larochelle", "Isabelle Lajoie", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "venue": "Journal of Machine Learning Research, vol. 11, no. Dec, pp. 3371\u20133408, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Vinod Nair", "Geoffrey E Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML- 10), 2010, pp. 807\u2013814.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Darpa timit acoustic-phonetic continous speech corpus cd-rom. nist speech disc 1-1.1", "author": ["John S Garofolo", "Lori F Lamel", "William M Fisher", "Jonathon G Fiscus", "David S Pallett"], "venue": "NASA STI/Recon technical report n, vol. 93, 1993.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1993}, {"title": "Iterative closed-loop phase-aware single-channel speech enhancement", "author": ["Pejman Mowlaee", "Rahim Saeidi"], "venue": "IEEE Signal Processing Letters, vol. 20, no. 12, pp. 1235\u20131239, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Aistats, 2010, vol. 9, pp. 249\u2013256.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "A simple way to initialize recurrent networks of rectified linear units", "author": ["Quoc V Le", "Navdeep Jaitly", "Geoffrey E Hinton"], "venue": "arXiv preprint arXiv:1504.00941, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, vol. abs/1412.6980, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Performance measurement in blind audio source separation", "author": ["Emmanuel Vincent", "R\u00e9mi Gribonval", "C\u00e9dric F\u00e9votte"], "venue": "IEEE transactions on audio, speech, and language processing, vol. 14, no. 4, pp. 1462\u20131469, 2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "A short-time objective intelligibility measure for time-frequency weighted noisy speech", "author": ["Cees H Taal", "Richard C Hendriks", "Richard Heusdens", "Jesper Jensen"], "venue": "ICASSP, 2010, pp. 4214\u20134217.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 1, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 2, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 3, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 4, "context": "Decades of works showed feasible solutions which estimated the noise model and used it to recover noise-deducted speech [1, 2, 3, 4, 5].", "startOffset": 120, "endOffset": 135}, {"referenceID": 5, "context": "When babble noise dominates over speech, aforementioned methods often times will fail to find the correct noise model [6].", "startOffset": 118, "endOffset": 121}, {"referenceID": 6, "context": "If so, the noise-deduction will render distortion in speech, which creates discomforts to the users of hearing aids [7].", "startOffset": 116, "endOffset": 119}, {"referenceID": 7, "context": "Here, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11].", "startOffset": 212, "endOffset": 226}, {"referenceID": 8, "context": "Here, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11].", "startOffset": 212, "endOffset": 226}, {"referenceID": 9, "context": "Here, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11].", "startOffset": 212, "endOffset": 226}, {"referenceID": 10, "context": "Here, instead of explicitly modeling the babble noise, we focus on learning a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra, inspired by recent works on speech enhancement using neural networks [8, 9, 10, 11].", "startOffset": 212, "endOffset": 226}, {"referenceID": 11, "context": "CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15].", "startOffset": 78, "endOffset": 86}, {"referenceID": 12, "context": "CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15].", "startOffset": 78, "endOffset": 86}, {"referenceID": 13, "context": "CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15].", "startOffset": 122, "endOffset": 130}, {"referenceID": 14, "context": "CNNs already proved its efficacy on extracting features in speech recognition [12, 13] or on eliminating noises in images [14, 15].", "startOffset": 122, "endOffset": 130}, {"referenceID": 15, "context": "Convolutional Encoder-Decoder (CED) network proposed in [16] consists of symmetric encoding layers and decoding layers (see Fig.", "startOffset": 56, "endOffset": 60}, {"referenceID": 16, "context": "Encoder consists of repetitions of a convolution, batch-normalization [17], max-pooling, and an ReLU [18] activation layer.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "Encoder consists of repetitions of a convolution, batch-normalization [17], max-pooling, and an ReLU [18] activation layer.", "startOffset": 101, "endOffset": 105}, {"referenceID": 13, "context": "Between two different bypass schemes \u2014 skip connections in [14] and residual connections in [15] \u2014 we chose to use skip connections in [14] which is more suitable for symmetric encoder-decoder design.", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "Between two different bypass schemes \u2014 skip connections in [14] and residual connections in [15] \u2014 we chose to use skip connections in [14] which is more suitable for symmetric encoder-decoder design.", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "Between two different bypass schemes \u2014 skip connections in [14] and residual connections in [15] \u2014 we chose to use skip connections in [14] which is more suitable for symmetric encoder-decoder design.", "startOffset": 135, "endOffset": 139}, {"referenceID": 18, "context": "Dataset: The experiment was conducted on the TIMIT database [19] and 27 different types of noise clips were collected from freely available online resource [20].", "startOffset": 60, "endOffset": 64}, {"referenceID": 19, "context": "Phase Aware Scaling: To avoid extreme differences (more than 45 degree) between the noisy and clean phase, the clean spectral magnitude was encoded as similar to [21]:", "startOffset": 162, "endOffset": 166}, {"referenceID": 20, "context": "Fully connected and convolution layer weights were initialized as in [22] and recurrent layer weights were initialized as in [23].", "startOffset": 69, "endOffset": 73}, {"referenceID": 21, "context": "Fully connected and convolution layer weights were initialized as in [22] and recurrent layer weights were initialized as in [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 16, "context": "Convolution layers were trained from scratch, with the aid of batch normalization layer [17] added after each convolution layer .", "startOffset": 88, "endOffset": 92}, {"referenceID": 22, "context": "All networks were trained using back propagation with gradient descent optimization using Adam [24] with a mini-batch size of 64.", "startOffset": 95, "endOffset": 99}, {"referenceID": 23, "context": "Signal to Distortion Ration (SDR) [25] was used to measure the amount of `2 error present between clean and denoised speech:", "startOffset": 34, "endOffset": 38}, {"referenceID": 24, "context": "In addition, Short time Objective Intelligibility (STOI) [26] and Perceptual Evaluation of Speech Distortion (PESQ) [27] \u2014 both assume that human perception has short term memory and hence the error is measured nonlinearly in time of interest\u2014 were used to measure the subjective quality of listening.", "startOffset": 57, "endOffset": 61}], "year": 2016, "abstractText": "In hearing aids, the presence of babble noise degrades hearing intelligibility of human speech greatly. However, removing the babble without creating artifacts in human speech is a challenging task in a low SNR environment. Here, we sought to solve the problem by finding a \u2018mapping\u2019 between noisy speech spectra and clean speech spectra via supervised learning. Specifically, we propose using fully Convolutional Neural Networks, which consist of lesser number of parameters than fully connected networks. The proposed network, Redundant Convolutional Encoder Decoder (R-CED), demonstrates that a convolutional network can be 12 times smaller than a recurrent network and yet achieves better performance, which shows its applicability for an embedded system: the hearing aids.", "creator": "LaTeX with hyperref package"}}}