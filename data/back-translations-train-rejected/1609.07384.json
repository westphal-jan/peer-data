{"id": "1609.07384", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "Discovering Sound Concepts and Acoustic Relations In Text", "abstract": "In this paper we describe approaches for discovering acoustic concepts and relations in text. The first major goal is to be able to identify text phrases which contain a notion of audibility and can be termed as a sound or an acoustic concept. We also propose a method to define an acoustic scene through a set of sound concepts. We use pattern matching and parts of speech tags to generate sound concepts from large scale text corpora. We use dependency parsing and LSTM recurrent neural network to predict a set of sound concepts for a given acoustic scene. These methods are not only helpful in creating an acoustic knowledge base but also directly help in acoustic event and scene detection research in a variety of ways.", "histories": [["v1", "Fri, 23 Sep 2016 14:35:17 GMT  (21kb)", "http://arxiv.org/abs/1609.07384v1", "5 pages"], ["v2", "Mon, 13 Feb 2017 01:09:27 GMT  (21kb)", "http://arxiv.org/abs/1609.07384v2", "ICASSP 2017"]], "COMMENTS": "5 pages", "reviews": [], "SUBJECTS": "cs.SD cs.AI cs.LG", "authors": ["anurag kumar", "bhiksha raj", "ndapandula nakashole"], "accepted": false, "id": "1609.07384"}, "pdf": {"name": "1609.07384.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["alnu@andrew.cmu.edu,"], "sections": [{"heading": null, "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2.1. Unsupervised Sound Concept Discovery", "text": "Our method is based on the idea that there are patterns in certain shapes that are primarily used to express sound concepts in speech. These patterns can help in identifying sound concepts. We start with a single pattern: \"sound (s) of < Y >,\" where Y is any phrase, we allow Y to be up to 4 words long. We then look for occurrences of the pattern in a large body. In our experiments, we used the English part of ClueWeb092, which contains about 500 million web pages. This gives us a large collection of occurrences such as: \"sound of honking cars,\" \"sound of gunshots.\" However, this step generates a significant amount of noise. Therefore, we treat its output as a candidate sound concept and introduce a minimally supervised method to cut out sounds from this collection."}, {"heading": "2.2. Supervised Classification", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "3.1. Training Data", "text": "To obtain the training data, we proceed as follows: We sort the paths by frequency, i.e. how often we have seen that the path occurs with different scene-sound pairs. Among the most common paths are the paths yes or no, depending on whether they express the relationship of interest. This gives us an opportunity to generate positive and negative examples from the marked paths. Examples of paths that generate positive training data are shown in Table 3. Examples of paths that generate negative training data are shown in Table 4."}, {"heading": "3.2. Classification", "text": "We use an LSTM recurrent neural network to learn the scenery-sound relationship that relies on controlling the cell. (It is) We have the ability to communicate in a way that we do it in a way that we do it in a way that we do it in a way that we do it in a way that we learn them during training. (It is) The shortest paths between a sound concept and an acoustic scene that we use an LSTM recurrent neural network (RNN) that is able to learn long range dependencies. (While regular RNs can also learn long dependencies, they tend to use newer inputs in the sequence. (LSTMs) Learn this limitation with a memory and adaptation to a cell."}], "references": [{"title": "Informedia@ trecvid 2014 med and mer", "author": ["Shoou-I Yu", "Lu Jiang", "Zexi Mao", "Xiaojun Chang", "Xingzhong Du", "Chuang Gan", "Zhenzhong Lan", "Zhongwen Xu", "Xuanchong Li", "Yang Cai"], "venue": "NIST TRECVID Video Retrieval Evaluation Workshop, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Content-based multimedia retrieval", "author": ["Flora Amato", "Luca Greco", "Fabio Persia", "Silvestro Roberto Poccia", "Aniello De Santo"], "venue": "Data Management in Pervasive Systems, pp. 291\u2013 310. Springer, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio based event detection for multimedia surveillance", "author": ["Pradeep K Atrey", "Namunu C Maddage", "Mohan S Kankanhalli"], "venue": "2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings. IEEE, 2006, vol. 5.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Sound-event recognition with a companion humanoid", "author": ["Maxime Janvier", "Xavier Alameda-Pineda", "Laurent Girinz", "Radu Horaud"], "venue": "2012 12th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2012). IEEE, 2012, pp. 104\u2013 111.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Bird species recognition using support vector machines", "author": ["Seppo Fagerlund"], "venue": "EURASIP Journal on Applied Signal Processing, vol. 2007, no. 1, pp. 64\u201364, 2007.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2007}, {"title": "Audio-based context recognition", "author": ["Antti J Eronen", "Vesa T Peltonen", "Juha T Tuomi", "Anssi P Klapuri", "Seppo Fagerlund", "Timo Sorsa", "Ga\u00ebtan Lorho", "Jyri Huopaniemi"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 14, pp. 321\u2013329, 2006.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2006}, {"title": "Real-world acoustic event detection", "author": ["Xiaodan Zhuang", "Xi Zhou", "Mark A Hasegawa-Johnson", "Thomas S Huang"], "venue": "Pattern Recognition Letters, vol. 31, no. 12, pp. 1543\u20131551, 2010.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Audio event detection from acoustic unit occurrence patterns", "author": ["A Kumar", "P Dighe", "R Singh", "S Chaudhuri", "B Raj"], "venue": "IEEE ICASSP, 2012, pp. 489\u2013492.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Sound event detection in multisource environments using source separation", "author": ["Toni Heittola", "Annamaria Mesaros", "Tuomas Virtanen", "Antti Eronen"], "venue": "Workshop on machine listening in Multisource Environments, 2011.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Reliable detection of audio events in highly noisy environments", "author": ["Pasquale Foggia", "Nicolai Petkov", "Alessia Saggese", "Nicola Strisciuglio", "Mario Vento"], "venue": "Pattern Recognition Letters, vol. 65, pp. 22\u201328, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio event classification using deep neural networks", "author": ["Zvi Kons", "Orith Toledo-Ronen"], "venue": "Interspeech, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Polyphonic sound event detection using multi label deep neural networks", "author": ["Emre Cakir", "Toni Heittola", "Heikki Huttunen", "Tuomas Virtanen"], "venue": "2015 international joint conference on neural networks (IJCNN). IEEE, 2015, pp. 1\u20137.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Audio event detection using weakly labeled data", "author": ["Anurag Kumar", "Bhiksha Raj"], "venue": "24th ACM International Conference on Multimedia. ACM Multimedia, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Weakly supervised scalable audio content analysis", "author": ["Anurag Kumar", "Bhiksha Raj"], "venue": "2016 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on. IEEE, 2009, pp. 248\u2013255.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "NEIL: Extracting Visual Knowledge from Web Data", "author": ["Xinlei Chen", "Abhinav Shrivastava", "Abhinav Gupta"], "venue": "International Conference on Computer Vision (ICCV), 2013, http://www.neil-kb.com/.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Toward an architecture for never-ending language learning", "author": ["Andrew Carlson", "Justin Betteridge", "Bryan Kisiel", "Burr Settles", "Estevam R Hruschka Jr", "Tom M Mitchell"], "venue": "AAAI, 2010, vol. 5, p. 3.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}, {"title": "Eventnet: A large scale structured concept library for complex event detection in video", "author": ["Guangnan Ye", "Yitong Li", "Hongliang Xu", "Dong Liu", "Shih- Fu Chang"], "venue": "Proceedings of the 23rd ACM international conference on Multimedia. ACM, 2015, pp. 471\u2013480.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A dataset and taxonomy for urban sound research", "author": ["Justin Salamon", "Christopher Jacoby", "Juan Pablo Bello"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 1041\u20131044.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "The soundscape: Our sonic environment and the tuning of the world", "author": ["R Murray Schafer"], "venue": "Inner Traditions/Bear & Co,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1993}, {"title": "Towards standardization in soundscape preference assessment", "author": ["AL Brown", "Jian Kang", "Truls Gjestland"], "venue": "Applied Acoustics, vol. 72, no. 6, pp. 387\u2013392, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Urban soundscapes: Experiences and knowledge", "author": ["Manon Raimbault", "Daniele Dubois"], "venue": "Cities, vol. 22, no. 5, pp. 339\u2013 350, 2005.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1997}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T Mikolov", "J Dean"], "venue": "Advances in neural information processing systems, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "venue": "EMNLP, 2014, vol. 14, pp. 1532\u201343.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Task-oriented learning of word embeddings for semantic relation classification", "author": ["Kazuma Hashimoto", "Pontus Stenetorp", "Makoto Miwa", "Yoshimasa Tsuruoka"], "venue": "CoNLL 2015, p. 268, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Don\u2019t count, predict! a systematic comparison of contextcounting vs. context-predicting semantic vectors", "author": ["Marco Baroni", "Georgiana Dinu", "Germ\u00e1n Kruszewski"], "venue": "ACL (1), 2014, pp. 238\u2013247.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Classifying relations via long short term memory networks along shortest dependency paths", "author": ["Yan Xu", "Lili Mou", "Ge Li", "Yunchuan Chen", "Hao Peng", "Zhi Jin"], "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP 2015, Lisbon, Portugal, September 17-21, 2015, 2015, pp. 1785\u20131794.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Discovering semantic relations from the web and organizing them with PATTY", "author": ["Ndapandula Nakashole", "Gerhard Weikum", "Fabian M. Suchanek"], "venue": "SIGMOD Record, vol. 42, no. 2, pp. 29\u201334, 2013.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Sound concepts and relations", "author": ["A Kumar"], "venue": "http://www.cs.cmu.edu/%7Ealnu/SOExpt.htm Copy and Paste in browser if clicking does not work.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "One of the most prominent applications is content based retrieval of multimedia recordings [1] [2], where the audio component of multimedia carries a significant amount of information.", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "One of the most prominent applications is content based retrieval of multimedia recordings [1] [2], where the audio component of multimedia carries a significant amount of information.", "startOffset": 95, "endOffset": 98}, {"referenceID": 2, "context": "Other well known applications of automated analysis of audio data are: audio based surveillance [3], human computer interaction [4], classification of bird species [5], context recognition system [6].", "startOffset": 96, "endOffset": 99}, {"referenceID": 3, "context": "Other well known applications of automated analysis of audio data are: audio based surveillance [3], human computer interaction [4], classification of bird species [5], context recognition system [6].", "startOffset": 128, "endOffset": 131}, {"referenceID": 4, "context": "Other well known applications of automated analysis of audio data are: audio based surveillance [3], human computer interaction [4], classification of bird species [5], context recognition system [6].", "startOffset": 164, "endOffset": 167}, {"referenceID": 5, "context": "Other well known applications of automated analysis of audio data are: audio based surveillance [3], human computer interaction [4], classification of bird species [5], context recognition system [6].", "startOffset": 196, "endOffset": 199}, {"referenceID": 6, "context": "Several methods have been proposed for audio event and scene detection [7\u201312].", "startOffset": 71, "endOffset": 77}, {"referenceID": 7, "context": "Several methods have been proposed for audio event and scene detection [7\u201312].", "startOffset": 71, "endOffset": 77}, {"referenceID": 8, "context": "Several methods have been proposed for audio event and scene detection [7\u201312].", "startOffset": 71, "endOffset": 77}, {"referenceID": 9, "context": "Several methods have been proposed for audio event and scene detection [7\u201312].", "startOffset": 71, "endOffset": 77}, {"referenceID": 10, "context": "Several methods have been proposed for audio event and scene detection [7\u201312].", "startOffset": 71, "endOffset": 77}, {"referenceID": 11, "context": "Several methods have been proposed for audio event and scene detection [7\u201312].", "startOffset": 71, "endOffset": 77}, {"referenceID": 12, "context": "Moreover, due to the limited availability of labeled data and the time consuming and expensive process of manual annotations, there have been attempts to learn event detectors from weakly labeled data as well [13] [14].", "startOffset": 209, "endOffset": 213}, {"referenceID": 13, "context": "Moreover, due to the limited availability of labeled data and the time consuming and expensive process of manual annotations, there have been attempts to learn event detectors from weakly labeled data as well [13] [14].", "startOffset": 214, "endOffset": 218}, {"referenceID": 14, "context": "If we look at the analogous problem in the field of computer vision, one can notice that object detection in images has been scaled to thousands of visual object categories [15].", "startOffset": 173, "endOffset": 177}, {"referenceID": 15, "context": "Never Ending Image Learner (NEIL) [17] not only detects thousands of visual objects and scenes in images but has also learned a variety of commonsense knowledge visual relationships such as Umbrella looks similar to Ferris wheel or scene-object relation such as Monitor is found in Control room.", "startOffset": 34, "endOffset": 38}, {"referenceID": 16, "context": "This allows it to provide visual knowledge to knowledge bases such as Never Ending Language Learner (NELL) [18].", "startOffset": 107, "endOffset": 111}, {"referenceID": 17, "context": "Another architecture EventNet [19] is tailored towards multimedia events.", "startOffset": 30, "endOffset": 34}, {"referenceID": 18, "context": "Some works have tried to relate sounds through hand crafted sound taxonomies [20] [21] [22] [23].", "startOffset": 77, "endOffset": 81}, {"referenceID": 19, "context": "Some works have tried to relate sounds through hand crafted sound taxonomies [20] [21] [22] [23].", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "Some works have tried to relate sounds through hand crafted sound taxonomies [20] [21] [22] [23].", "startOffset": 87, "endOffset": 91}, {"referenceID": 21, "context": "Some works have tried to relate sounds through hand crafted sound taxonomies [20] [21] [22] [23].", "startOffset": 92, "endOffset": 96}, {"referenceID": 18, "context": "Even in the specific context of environmental sounds there is no clear consensus on building such taxonomies [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "Sound concepts and acoustic scenes are related through dependency paths and then an LSTM neural network [24] is used to predict whether a sound concept is found in an acoustic scene or not.", "startOffset": 104, "endOffset": 108}, {"referenceID": 23, "context": "Word Embeddings have been found to be very effective in capturing syntactic and semantic similarity between words [25\u201327] and have shown remarkable success in a variety of semantic tasks [28].", "startOffset": 114, "endOffset": 121}, {"referenceID": 24, "context": "Word Embeddings have been found to be very effective in capturing syntactic and semantic similarity between words [25\u201327] and have shown remarkable success in a variety of semantic tasks [28].", "startOffset": 114, "endOffset": 121}, {"referenceID": 25, "context": "Word Embeddings have been found to be very effective in capturing syntactic and semantic similarity between words [25\u201327] and have shown remarkable success in a variety of semantic tasks [28].", "startOffset": 114, "endOffset": 121}, {"referenceID": 26, "context": "Word Embeddings have been found to be very effective in capturing syntactic and semantic similarity between words [25\u201327] and have shown remarkable success in a variety of semantic tasks [28].", "startOffset": 187, "endOffset": 191}, {"referenceID": 27, "context": "Shortest paths between entities have been found to be a good indicator of relationships between entities [29, 30].", "startOffset": 105, "endOffset": 113}, {"referenceID": 28, "context": "Shortest paths between entities have been found to be a good indicator of relationships between entities [29, 30].", "startOffset": 105, "endOffset": 113}, {"referenceID": 22, "context": "LSTMs tackle this limitation with a memory cell and an adaptive gating mechanism that controls how much of the input to give to the memory cell, and the how much of the previous state to forget [24].", "startOffset": 194, "endOffset": 198}, {"referenceID": 29, "context": "The complete list of sound concepts discovered by our method is available on this [31] webpage.", "startOffset": 82, "endOffset": 86}, {"referenceID": 29, "context": "The full list of sound concepts discovered for each acoustic scene or environment is available on this [31] webpage.", "startOffset": 103, "endOffset": 107}], "year": 2017, "abstractText": "In this paper we describe approaches for discovering acoustic concepts and relations in text. The first major goal is to be able to identify text phrases which contain a notion of audibility and can be termed as a sound or an acoustic concept. We also propose a method to define an acoustic scene through a set of sound concepts. We use pattern matching and parts of speech tags to generate sound concepts from large scale text corpora. We use dependency parsing and LSTM recurrent neural network to predict a set of sound concepts for a given acoustic scene. These methods are not only helpful in creating an acoustic knowledge base but also directly help in acoustic event and scene detection research in a variety of ways.", "creator": "LaTeX with hyperref package"}}}