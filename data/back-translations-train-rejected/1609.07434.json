{"id": "1609.07434", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "23-Sep-2016", "title": "Regulating Reward Training by Means of Certainty Prediction in a Neural Network-Implemented Pong Game", "abstract": "We present the first reinforcement-learning model to self-improve its reward-modulated training implemented through a continuously improving \"intuition\" neural network. An agent was trained how to play the arcade video game Pong with two reward-based alternatives, one where the paddle was placed randomly during training, and a second where the paddle was simultaneously trained on three additional neural networks such that it could develop a sense of \"certainty\" as to how probable its own predicted paddle position will be to return the ball. If the agent was less than 95% certain to return the ball, the policy used an intuition neural network to place the paddle. We trained both architectures for an equivalent number of epochs and tested learning performance by letting the trained programs play against a near-perfect opponent. Through this, we found that the reinforcement learning model that uses an intuition neural network for placing the paddle during reward training quickly overtakes the simple architecture in its ability to outplay the near-perfect opponent, additionally outscoring that opponent by an increasingly wide margin after additional epochs of training.", "histories": [["v1", "Fri, 23 Sep 2016 17:11:53 GMT  (634kb)", "http://arxiv.org/abs/1609.07434v1", "7 pages, 3 figures"]], "COMMENTS": "7 pages, 3 figures", "reviews": [], "SUBJECTS": "cs.AI cs.NE", "authors": ["matt oberdorfer", "matt abuzalaf"], "accepted": false, "id": "1609.07434"}, "pdf": {"name": "1609.07434.pdf", "metadata": {"source": "CRF", "title": "Regulating Reward Training by Means of Certainty Prediction in a Neural Network-Implemented Pong Game", "authors": ["Matt Oberdorfer", "Matt Abuzalaf"], "emails": ["matt@frostdatacapital.com", "matthew.abuzalaf@yale.edu"], "sections": [{"heading": "1. Introduction", "text": "In fact, most of them are able to move to another world, in which they are able to move, and in which they are able to move."}, {"heading": "2. Related Work", "text": "Reinforcement learning is an area of machine learning where a machine learns how to execute its own trial-and-error processes optimally, rather than through external human corrections. Reinforcement learning selects the right decisions so that predetermined \"rewards\" are maximized in the long run, and the network is strengthened when receiving these rewards, so that it proceeds similarly in future cases. [1] Generally, this process works as follows: through a series of stages or events, the learning agent is assigned an arbitrary number of decisions or interactions, and the RMLA's policy directs its behavior at a given point in the process (which can be random or deterministic). Positive actions or stages in the process are assigned to a numerical representation of desirability through a reward function, and the RMLA's ultimate goal is to maximize its reward in the long term, and thus positive behaviors, along the way."}, {"heading": "3. Regulating Reward Training by Means of Certainty Prediction", "text": "In fact, most of them will be able to go in search of a solution."}, {"heading": "4. Results of Experiments", "text": "One of the two goals was simply to verify whether the reward modulated learning was successful or not, in order to train the agent to play Pong. The second goal was to compare the two programs, the Simple Reward and FourNetwork architectures, to see if the introduction of trust regulated reward training could lead to a much more efficient training process. To test, the two games were performed with reward training enabled and the agent's performance checked at specific intervals between training periods: 500K, 1M, 2M and 5M. In these many eras, reward training was turned off and Agent Figure 3 was allowed to work alone. After the trained agent reached 10,000 points with the training, the \"human\" score was recorded and the reward training continued, and the relative success and performance of the two systems could then be recorded and compared. As shown in Figure 3 and Table 1, both reward networks could trump the near-perfect \"human\" player."}, {"heading": "5. Analysis and Evaluation", "text": "If success as the reliability of the RMLA is qualified to trump a particularly good \"human player,\" then the result is an astonishing success both for the simple reward and for four-network programs. Both begin to trump the \"human\" sometime between 500K and 1M epochs of training, which can be achieved in less than five minutes with the acceleration function programmed in the script. At this point, a truly imperfect human game against the agent would always be solidly defeated. With additional training, both programs begin to win with larger margins and become essentially perfect within a relatively short period of time. This is well reflected in the choice of variable inputs and architectures for the two networks, as these are the key elements in the success and efficiency of the program."}, {"heading": "6. Conclusion", "text": "The reward modulation model and reward policy consisted of four sequenced neural networks, with the second and third networks predicting the certainty of receiving a reward predicted by the first network. A fourth neural network was trained to produce an action when the certainty was low. Even with a simple reward network that involved nothing more than random paddle placement to train the agent to successfully return, the program was able to learn how to trump an opponent in less than a million training periods, which is accomplished in real time in less than five minutes of training. Moreover, performance only increased with additional training, and it appears that the simple reward network would have trained a near-perfect agent in a relatively short time."}, {"heading": "7. Suggestions for Further Research", "text": "A comparison between this and other methods of training an agent to play arcade games, such as Deep Q-Learning [8], will help validate which model has better training convergence and better task performance. In future experiments, we will investigate whether the four-network method developed here could be generalized and implemented for other challenges, including other arcade games. If our approach of a reward modulated program could outperform Qlearning-based algorithms in a comparable number of training periods, then the four-network method developed here would be an alternative method of machine learning for use in broader general applications than the one presented in this experiment."}, {"heading": "8. References", "text": "[1] Sutton, Richard S., and Andrew G. Barto. Reinforcement Learning: An Introduction, MIT Press 2012 [2] M. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling. The arcade learning environment: An evaluation platform for general agents. Journal of Artificial Intelligence Research, pp. 253-279, 2013 [3] Christopher JCH Watkins and Peter Dayan. Qlearning. Machine learning, pp. 279-292, 1992. [4] Poole, David, and Alan Mackworth, \"Q-learning\" Artificial Intelligence, 2010 [5] C. Watkins, \"Learning From Delayed Rewards,\" (Ph.D. Thesis) Cambridge University, England, 1989 [6] Aswolinskiy, Witali, and Gordon Pipa, \"RMSORN: a reward-modulated self-organizing recurrent neural network,\" Frontiers, Computational Neuroscience, 2015 [7]."}], "references": [{"title": "Reinforcement Learning: An Introduction, MIT", "author": ["Sutton", "Richard S", "Andrew G. Barto"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research, p.p", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Qlearning. Machine learning, p.p", "author": ["Christopher JCH Watkins", "Peter Dayan"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1992}, {"title": "Learning From Delayed Rewards\u201d, (Ph.D", "author": ["C. Watkins"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1989}, {"title": "RM- SORN: a reward-modulated self-organizing recurrent neural network\u201d, Frontiers", "author": ["Aswolinskiy", "Witali", "Gordon Pipa"], "venue": "Computational Neuroscience,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Confidence Estimation Methods for Neural Networks: A Practical Comparison\u201d, ESANN'2000 proceedings, p.p", "author": ["G. Papadopoulos", "P.J. Edwards", "A.F. Murray"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2000}, {"title": "Learning multiple layers of representation", "author": ["Geoffrey E Hinton"], "venue": "Trends in cognitive sciences, p.p", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Generating Sequences With Recurrent Neural Networks", "author": ["Alex Graves"], "venue": "[cs.NE],", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "\u201d One method by which an agent can regulate its learning is through reward modulation, wherein the agent is trained as it receives rewards for desirable actions in order to form an optimal policy [1].", "startOffset": 196, "endOffset": 199}, {"referenceID": 1, "context": "It has been shown [2] that arcade games can represent both a challenge problem and provide a platform and methodology for evaluating the development of", "startOffset": 18, "endOffset": 21}, {"referenceID": 2, "context": "to assess the state and derive actions similar results as Q-learning [3] while entirely based on", "startOffset": 69, "endOffset": 72}, {"referenceID": 0, "context": "[1] In general, this process works as follows: over a series of stages or events, the learning agent will make some arbitrary number of decisions or interactions.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "an optimal policy [5].", "startOffset": 18, "endOffset": 21}, {"referenceID": 4, "context": "when the agent carries out a desirable action, which, is pre-designated by the reward function [6].", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "methods for neural network architectures examined their effectiveness [7].", "startOffset": 70, "endOffset": 73}, {"referenceID": 6, "context": "In our approach, we used multiple layers of representation [9] combined with sequenced neural networks [10] to develop a policy that consists of backpropagation networks.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "In our approach, we used multiple layers of representation [9] combined with sequenced neural networks [10] to develop a policy that consists of backpropagation networks.", "startOffset": 103, "endOffset": 107}], "year": 2016, "abstractText": "We present the first reinforcement-learning model to self-improve its reward-modulated training implemented through a continuously improving \u201cintuition\u201d neural network. An agent was trained how to play the arcade video game Pong with two reward-based alternatives, one where the paddle was placed randomly during training, and a second where the paddle was simultaneously trained on three additional neural networks such that it could develop a sense of \u201ccertainty\u201d as to how probable its own predicted paddle position will be to return the ball. If the agent was less than 95% certain to return the ball, the policy used an intuition neural network to place the paddle. We trained both architectures for an equivalent number of epochs and tested learning performance by letting the trained programs play against a near-perfect opponent. Through this, we found that the reinforcement learning model that uses an intuition neural network for placing the paddle during reward training quickly overtakes the simple architecture in its ability to outplay the near-perfect opponent, additionally outscoring that opponent by an increasingly wide margin after additional epochs of training.", "creator": "Word"}}}