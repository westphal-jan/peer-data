{"id": "1609.07568", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2016", "title": "A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects", "abstract": "Discriminating between closely-related language varieties is considered a challenging and important task. This paper describes our submission to the DSL 2016 shared-task, which included two sub-tasks: one on discriminating similar languages and one on identifying Arabic dialects. We developed a character-level neural network for this task. Given a sequence of characters, our model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. We primarily focused on the Arabic dialect identification task and obtained an F1 score of 0.4834, ranking 6th out of 18 participants. We also analyze errors made by our system on the Arabic data in some detail, and point to challenges such an approach is faced with.", "histories": [["v1", "Sat, 24 Sep 2016 04:02:13 GMT  (49kb,D)", "http://arxiv.org/abs/1609.07568v1", "DSL 2016"]], "COMMENTS": "DSL 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yonatan belinkov", "james glass"], "accepted": false, "id": "1609.07568"}, "pdf": {"name": "1609.07568.pdf", "metadata": {"source": "CRF", "title": "A Character-level Convolutional Neural Network for Distinguishing Similar Languages and Dialects", "authors": ["Yonatan Belinkov"], "emails": ["glass}@mit.edu"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is as if most of them will be able to abide by the rules they have imposed on themselves. (...) It is not as if they are able to abide by the rules. (...) It is as if they are able to change the rules. (...) It is not as if they are able to change the rules. (...) It is not as if they are able to change the rules. (...) It is as if they are able to change the rules. (...) It is not as if they are able to break the rules. (...) It is as if they are able to abide by the rules. (...) It is as if they are able to break the rules. (...)"}, {"heading": "2 Related Work", "text": "Previous contests have proven that despite very good performance (over 95% accuracy), it is still a non-trivial task, which is considerably more difficult than identifying unrelated languages. Granted, even people have a difficult time identifying the correct language in certain cases, as reported by Goutte et al. (2016). Previous shared task reports contain detailed information about the task, related work and participating systems. Here, we have only elicited a few relevant trends. In terms of characteristics, the majority of groups used sequences of characters and words in previous years. A notable exception is the use of word-white lists by Porta and Sancho (2014). Different learning algorithms were used for this task, most commonly linear SVMs or maximum entropy models."}, {"heading": "3 Methodology", "text": "We formulate the task as a multi-class classification problem in which each language (or dialect) is a separate class. We do not consider a two-stage classification, although this has proven useful in previous work (Zampieri et al., 2015). Formally, our predictor is a neural network over strings. Let's leave t: = c = c1, \u00b7 \u00b7 \u00b7 cL denote a sequence of characters, where L is a maximum length that we determine empirically. Longer texts are shortened and shorter ones are padded with a special PAD symbol. Each character c in the alphabet is represented as a real vector xc. This character embedding is learned during the training. Our neural network has the following structure: \u2022 Input layer: mapping the string c to a vector sequence."}, {"heading": "3.1 Training details and submitted runs", "text": "We train the entire network together, including the embedding layer. We use Adam (Kingma and Ba, 2014) with the original default parameters to minimize loss of cross-entropy. Training is performed with mixed mini-batches of size 16 and stopped as soon as the loss on the dev set stops improving; we allow a patience of 10 eras. Our implementation is based on Keras (Chollet, 2015) with the TensorFlow backend (Abadi et al., 2015).We have largely experimented with the sub-task 2 dataset of Arabic dialects. Since the official shared task did not include a dedicated dev set, we randomly allocated 90% of the training for development. We used the following hyperparameters using this split (selected parameters are bold): embedding 2 Arabic dialects (0.2, 0.5), fully connected 200-density (2-400 density, 0.5-density, 0.2 full, 0.2-density)."}, {"heading": "4 Results", "text": "Table 1 shows the results of our submitted runs, along with two baselines: a random baseline for Sub-Task 1, Test Set A (a balanced test set) and a majority baseline for Sub-Task 2, Test Set C (a slightly unbalanced test set). We also report the results of the most powerful systems in the common task set. In Sub-Task 2, Test Set C, we notice a fairly large difference between our runs. Our best result, with Run 3, used plurality in matching between 10 different models trained on 90% of the training data. We chose this strategy in an effort to avoid matching. However, during development, we obtained accuracy results of about 57-60% on a separate move / development test, so we suspect that there was still match of the training set. In this run, our team ranked 6th out of 18 teams according to the official rating. In Sub-Task 1, slightly better training set A, due to larger models, will yield better results."}, {"heading": "5 Discussion", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "6 Conclusion", "text": "In this paper, we investigated character-level Convolutionary Neural Networks to differentiate between similar languages and dialects. We demonstrated that such a model can work quite well in identifying Arabic dialects, even though it does not produce current results. We also performed a brief analysis of the errors our system has made in the Arabic dataset, pointing out the challenges such a model faces. A natural extension of this work is the combination of word-like features in the neural network. White lists of typical words of certain dialects may also be helpful, although in preliminary experiments we could not achieve performance gains by adding such features. We are also curious about how our model would evolve in accessing voice recordings, for example by running it on recognized telephone sequences or directly incorporating acoustic features. However, this would require a different preparation of the dataset, which we hope would be made available in future DSL tasks."}, {"heading": "Acknowledgments", "text": "The authors thank Sergiu Nisioi and Noam Ordan for their helpful discussions, which were supported by the Qatar Computing Research Institute (QCRI). Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding organisations."}], "references": [{"title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. Software available from tensorflow.org", "author": ["gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "gas et al\\.,? \\Q2015\\E", "shortCiteRegEx": "gas et al\\.", "year": 2015}, {"title": "Peter Bell", "author": ["Ahmed Ali", "Najim Dehak", "Patrick Cardinal", "Sameer Khurana", "Sree Harsha Yella", "James Glass"], "venue": "and Steve Renals.", "citeRegEx": "Ali et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Julia Hirschberg", "author": ["Fadi Biadsy"], "venue": "and Nizar Habash.", "citeRegEx": "Biadsy et al.2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Paolo Rosso", "author": ["Marc Franco-Salvador"], "venue": "and Francisco Rangel.", "citeRegEx": "Franco.Salvador et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Antoine Bordes", "author": ["Xavier Glorot"], "venue": "and Yoshua Bengio.", "citeRegEx": "Glorot et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Serge L\u00c3l\u2019ger", "author": ["Cyril Goutte"], "venue": "Shervin Malmasi, and Marcos Zampieri.", "citeRegEx": "Goutte et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "David Sontag", "author": ["Yoon Kim", "Yacine Jernite"], "venue": "and Alexander Rush.", "citeRegEx": "Kim et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models. arXiv preprint arXiv:1604.00788", "author": ["Luong", "Manning2016] Minh-Thang Luong", "Christopher D Manning"], "venue": null, "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Eshrag Refaee", "author": ["Shervin Malmasi"], "venue": "and Mark Dras.", "citeRegEx": "Malmasi et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ahmed Ali", "author": ["Shervin Malmasi", "Marcos Zampieri", "Nikola Ljube\u0161i\u0107", "Preslav Nakov"], "venue": "and J\u00f6rg Tiedemann.", "citeRegEx": "Malmasi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Using Maximum Entropy Models to Discriminate between Similar Languages and Varieties", "author": ["Porta", "Sancho2014] Jordi Porta", "Jos\u00e9-Luis Sancho"], "venue": "In Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial),", "citeRegEx": "Porta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Porta et al\\.", "year": 2014}, {"title": "Learning character-level representations for part-of-speech tagging", "author": ["Santos", "Zadrozny2014] Cicero D Santos", "Bianca Zadrozny"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "citeRegEx": "Santos et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Santos et al\\.", "year": 2014}, {"title": "2014", "author": ["Marcos Zampieri", "Liling Tan", "Nikola Ljube\u0161i\u0107", "J\u00f6rg Tiedemann"], "venue": "A Report on the DSL Shared Task", "citeRegEx": "Zampieri et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2015", "author": ["Marcos Zampieri", "Liling Tan", "Nikola Ljube\u0161i\u0107", "J\u00f6rg Tiedemann", "Preslav Nakov"], "venue": "Overview of the DSL Shared Task", "citeRegEx": "Zampieri et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Junbo Zhao", "author": ["Xiang Zhang"], "venue": "and Yann LeCun.", "citeRegEx": "Zhang et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "Discriminating between closely-related language varieties is considered a challenging and important task. This paper describes our submission to the DSL 2016 shared-task, which included two sub-tasks: one on discriminating similar languages and one on identifying Arabic dialects. We developed a character-level neural network for this task. Given a sequence of characters, our model embeds each character in vector space, runs the sequence through multiple convolutions with different filter widths, and pools the convolutional representations to obtain a hidden vector representation of the text that is used for predicting the language or dialect. We primarily focused on the Arabic dialect identification task and obtained an F1 score of 0.4834, ranking 6th out of 18 participants. We also analyze errors made by our system on the Arabic data in some detail, and point to challenges such an approach is faced with.1", "creator": "LaTeX with hyperref package"}}}