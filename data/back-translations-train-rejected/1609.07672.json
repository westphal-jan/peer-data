{"id": "1609.07672", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "24-Sep-2016", "title": "Information-Theoretic Methods for Planning and Learning in Partially Observable Markov Decision Processes", "abstract": "Bounded agents are limited by intrinsic constraints on their ability to process information that is available in their sensors and memory and choose actions and memory updates. In this dissertation, we model these constraints as information-rate constraints on communication channels connecting these various internal components of the agent.", "histories": [["v1", "Sat, 24 Sep 2016 20:45:37 GMT  (9274kb,D)", "http://arxiv.org/abs/1609.07672v1", "PhD thesis, Hebrew University of Jerusalem, 9/2016"], ["v2", "Thu, 30 Mar 2017 04:57:49 GMT  (9077kb,D)", "http://arxiv.org/abs/1609.07672v2", "PhD thesis, Hebrew University of Jerusalem, 9/2016"]], "COMMENTS": "PhD thesis, Hebrew University of Jerusalem, 9/2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["roy fox"], "accepted": false, "id": "1609.07672"}, "pdf": {"name": "1609.07672.pdf", "metadata": {"source": "CRF", "title": "Information-Theoretic Methods for Planning and Learning in Partially Observable Markov Decision Processes", "authors": ["Naftali Tishby"], "emails": [], "sections": [{"heading": null, "text": "This year, it has come to the point where you feel you can go to the top without being able to go to the top."}, {"heading": "RF and AP jointly wrote the manuscript, with RF taking the lead on some parts, and making an equal contribution to other parts. All other sections of the thesis were authored by Roy Fox jointly with", "text": "Consultant Naftali Tishby."}, {"heading": "Contents", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1 Introduction 6", "text": "1.1 Partially observable Markov decision-making processes..................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2 Minimum-Information POMDP Planning 32", "text": "2.1 Limited planning for passive POMDPs.......... 32 2.2 Optimal selective attention for reactive agents...... 41"}, {"heading": "3 Minimum-Information LQG Control 53", "text": "3.1 Part I: Memoryless controllers............................................................................................................."}, {"heading": "4 Minimum-KL Reinforcement Learning 75", "text": "4.1 Taming Noise in Reinforcement Learning through Soft Updates 75"}, {"heading": "5 Discussion 86", "text": "Glossary 915Chapter 1"}, {"heading": "Introduction", "text": "In this chapter we present the conceptual framework that forms the basis for the results presented in this paper. Although both reinforcement learning and information theory have been studied intensively for many decades, some aspects of our approach in these areas are new. Preparatory work is contained here in somewhat unusual notation and is accompanied by several new organizational principles and insights."}, {"heading": "1.1 Partially Observable Markov Decision", "text": "Processes A partially observable Markov decision-making process (POMDP) is a dynamic system whose outputs partially reveal the state of the system and inputs partially control state dynamics. This expressive model has numerous and diverse applications, from autonomous vehicles to display displays [1]. POMDPs, and in particular the reinforced learning paradigm for their optimization and learning, have therefore received increasing attention from the research community in recent years.6"}, {"heading": "1.1.1 Setting", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dynamics", "text": "A discrete time dynamic system has a time-dependent state, and possibly stochastic dynamics that determine the distribution of each next state underlying each current state. A closed system has no input, and the dynamics is simply a Markov chain of states tstu, induced by the conditional probability distribution ppst '1 | stq of the state transition. In open systems that we consider in this thesis, an input control signal at P A, also referred to as action, can influence the dynamics of the state wt P W, which is now transmitted by the distribution ppwt' 1 | stq, atq.The system also sends an output signal at P O, also called an observation, based on its state. The observation dynamics is given by the distribution pot."}, {"heading": "Extrinsic Constraints", "text": "Without these limitations, the agent could closely observe the current state of the world and fully determine its next state wt '1. In POMDPs, the observability is partial, since the only information about wt that the agent can use is the one considered by above. Dual, the controllability is also partial, since the only future path of the states that the agent can effect are those induced by a \u2022 t. In other words, the state dynamic p limits how the agent can control the global P'pwt '1."}, {"heading": "Stationary Processes", "text": "\"It's as if he's been able to get to the top,\" he says."}, {"heading": "Finite-Horizon Processes", "text": "In many attachment learning areas, the process ends when a terminating state is reached. A terminating state can be modelled to persist with probability 1 and cost 0, making it a closed communicating class, and the value of any well-behaved policy 0. However, a special case of this terminating horizon setting is the popular discounted setting, although it is often mistakenly considered an infinite horizon [2]. In this setting, each transition has a fixed probability 0 \u2020 1 '\u00a7 1 of termination, regardless of the current state or action. The horizon Tf is distributed geometrically with parameter 1', and we have \"an infinite horizon.\""}, {"heading": "Symmetries", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1.2 Structure", "text": "This symmetry highlights the fundamental distinction between the agent and the world: the dynamics p and one remain fixed as those of the other, q and, are optimized. This suggests that in a closed system, the component whose dynamics are adaptive on shorter time scales can be thought of as an agent in relation to the rest of the system. Second, there is a vertical (left / right) symmetry between past and future, which in turn maps the inputs to the outputs and vice versa. This symmetry emphasizes the role of causality in the process: while the outputs of wt, namely ot and wt '1, are independent in view of the state st \"pwt, mtqot K wt' 1 | wt, mt, the inputs of wt '1, namely wt' and at, depend on causality in the process."}, {"heading": "Reactive Agents", "text": "The most reactionary measures in this area are: \"It is as if it were a reactionary policy that is able to contemplate a reactionary policy.\" (mti) \"It is a reactionary policy that is able to establish a reactionary policy.\" (mti) \"It is a reactionary policy that is able to initiate a reactionary policy.\" (mti)"}, {"heading": "1.1.3 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Optimization Problem", "text": "We are working on optimizing the value of the policy p ', qqV', q \"lim suq\" T \"81TT\" 1P \"0 Ercpwt,\" atqs \"Epwt,\" mtq \"p\" at \"1,\" mt \"1q\" E pwt, \"mtq\" p \"p\", p \",\" p, \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p,\" \"p,\" \"p,\" \"p,\" \"p,\" p, \"\" p, \"\" p, \"\" p, \"\" \"p,\" \"\" p, \"\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" \"p,\" p, \"\" p, \"p,\" p, \"\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" p, \"p,\" \"p,\" p, \"p,\" p, \"\" p, \""}, {"heading": "Backward Operator", "text": "The Gradient in relation to p-isBp-pwt, mtq lp, q-pwt, atq-pwt, atq-pwt-1, mt-pwt, mts-pwt, mtq-t, mtqs-pwt, atqs-pwt-E-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-p-"}, {"heading": "Optimal Policy", "text": "In the eeisrmtPnreeu, \"he says,\" it is able to be in a position. \"iSe\" nvo eeisn, tS \"t tasgt he is in a position in which he is able to be in a position.\" iSe \"iSe taht it be in a position in which he is able to be in a position.\" iSe \"iSe tsi rf\u00fc die eeisn,\" tS taht tmi rf\u00fc die eeisn. \"iSe\" iSe"}, {"heading": "Model-Based vs. Sample-Based Learning", "text": "Our approach so far has been based on the need for the dynamics of the world pp, q to calculate the advance (3) and backwards (4) updates, as well as the inference policy (6). In fact, general POMDPs are usually solved using model-based methods [7]. If the model is known at the time of design, the task of policy optimization is described as uncertain, the task is called learning, because the actor needs to learn about the world before good behavior can be determined, and the actor cannot simply exploit his partial knowledge of the world before it is sufficiently explored."}, {"heading": "1.1.4 Challenges", "text": "The POMDP planning problem is proven to be mathematically difficult [14], and the optimization problem is highly non-convex. Nevertheless, the problem is important enough to deserve the attention it has received. Inspired by the fact that natural agents regularly solve cases of the problem, the research community has developed useful insights and increasingly effective approaches, but also faces significant challenges."}, {"heading": "Memory Space Identification", "text": "Naively considered, since states of memory represent states of faith, we might be tempted to look at the entire space of distributions across the states of the world. Unfortunately, this space is continuous, and its discretization requires an exponential state space in the number of states, an explosion called the Curse of Dimensionality. To specify this, the number of p | W | '1q-dimensional beliefs with edge length is required to tile the simplex, which represents the distributions over W, is p? 2 {q | W |' 1. Not all Bayesian beliefs are attainable in a given POMDP. Bayesian inference politics is a deterrent, which makes the state of faith a function of observable history. Therefore, the number of achievable beliefs is limited by the number of different stories, | O | T, which is unfortunately exponentially on the horizon T, an explosion that is clear as the curse of history."}, {"heading": "Forward-Backward Coupling", "text": "This brings us to the last aspect of the challenge, which is the predictive nature of the algorithms involved. Calculation of beliefs and information costs requires marginal distributions, which must be found by means of a forward-looking process, while value functions are calculated by means of a backward-looking process. If the forward-looking and backward-looking processes are separable, the prob-21lem will be much easier to solve. This is the case when the observability is full, making the forward-looking processes processable. In areas with linear-gaussian dynamics and quadratic costs (LQG), the Gaussian distribution and the square function make the processes separable, and the problem is easily solvable, despite partial observability and controllability. This depends on the important property that the achievable beliefs are even Gaussian distributions over the space of world state, with fixed covariances."}, {"heading": "Source Coding and Channel Coding", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.2.1 Rate-Distortion Theory", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.2 Information Theory", "text": "Since the expected costs depend only on the distribution of x, this indicates a separation between source encoding and channel encoding. In the distortion rate problem, we determine the distribution qS-Sps-square induced by the source encoding, so that it achieves low expected distortions and at the same time compresses the signal to keep the information rate Irs; s low. In the capacity cost problem, we determine the distribution qXpxq induced by the channel encoding, so that it achieves low expected costs and at the same time enables greater information capacity Irx; ys on the channel."}, {"heading": "Source-Channel Separation", "text": "A solution to the common source-channel coding problem is also feasible separately for each of the rate-distortion and capacity-cost problems. Therefore, the optimum of the sub-problems results in a lower limit for the common optimum. Classical coding theory shows that if we allow the coders and decoders to map a large block of inputs as a unit into a large block of outputs, then asymptotically for large blocks the lower limit achieved by the separation is narrow. [16] This separation principle allows coders and practitioners to focus their efforts on one of the sub-problems without worrying about combining the solutions in a common solution, at least in this simple framework. If we apply this theory to enhancing learning in Section 1.2.2, we find that the coding blocks of inputs are not an option."}, {"heading": "Optimal Lossy Source Coding", "text": "In order to balance the expected distortion of earth ps, s \u00b2 s \u00b2 and the information rate of Irs \u00b2 q \u00b2 s against each other, we can specify one of these terms as an optimization target, with the caveat that the other is not too high. Lagrange of this optimization problem is Fq, q \u00b2; \"E s\" p \u00b2 s \"qp \u00b2 | sq\" 1log \u00b2 | sq \"1log \u00b2\" dps, \"s \u00b2 q,\" plus terms that limit the distributions q and q \u00b2 to be normalized. Here is a Lagrange multiplier that specifies the relative marginal costs of the distortion and the information rate. This storage range is also referred to as free energy, due to the similarities with the quantity of this name in statistical physics, with the inverse temperature ure.Note that we do not restrict the distribution of q \u00b2 s \u00b2 s \u00b2 and the optimal Fqs \u00b2 s \u00b2 convergence. \""}, {"heading": "1.2.2 Sequential Rate-Distortion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Problem Formulation", "text": "Distortion theory gives us a way to model the intrinsic constraints of limited agents. Agents often operate under limited capacity of their internal storage and communication channels. Such constraints can also be used as proxies for computational constraints caused by the lack of information processing resources. Intrinsic costs of information rates between the various components of an agent limit the space of strategies that can be practicably implemented. The agent may not be able to pay attention to all the observation available in his sensors, record the entire observable history (or provide sufficient statistics on 26World wtObservation otSensor encoderActuator decoderAction atwith patq \"E wt\" p \"p\" p \"p\" p \"p\" p \"p\" p \"ptq\" ptq \"pat.\" The free energy of this sequential distortion problem is Fp, \",\", \",\", \"\" \"\" wq, \"\" wt \"pp.\""}, {"heading": "Optimality Principle", "text": "Lagrangian's sequence distortion is not linear, but convex in the political parameters, and her global optimum given the other parameters is that she exceeds expectations in terms of the forward-looking postorP-1-2-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-1-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-1-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4-4"}, {"heading": "Extensions", "text": "One way to avoid the complication of forward-backward coupling is to eliminate the optimization via the parameter that produces the marginal distribution and replace it with a fixed before-after coupling. Generally, we can also have this fixed before-after coupling performed by a backward algorithm. We use a sample-based version of this approach in Section 4.1. We can generalize attention to fully observable policies by considering the reduction in Section 1.2, and the entire optimization can be performed by a backward algorithm. We use a sample-based version of this approach in Section 4.1."}, {"heading": "1.3 Organization of the Dissertation", "text": "This dissertation is organized as follows: In Section 2.1, we present our approach to POMDP planning under informational constraints and introduce the forward-backward algorithm for sequential distortion of interest rates; the setting in this section is limited to passive POMDPs, where actions incur costs but do not affect the state of the world; the same algorithm can generally be implemented in POMDPs, but the convergence characteristics are poorer; and Section 2.2 examines a telling aspect of these convergence challenges, namely that the optimal solution can be either a boundary cycle or an unstable fix point of the update operator."}, {"heading": "Minimum-Information POMDP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Planning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Bounded Planning in Passive POMDPs", "text": "In fact, it is the case that most people are able to abide by the rules that they have imposed on themselves. (...) In fact, it is the case that most people are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to abide by the rules. (...) In fact, it is the case that they are able to break the rules. \"(...)"}, {"heading": "2.2 Optimal Selective Attention in Reactive Agents", "text": "Unpublished: Roy Fox and Naftali Tishby, Optimal Selective Attention in Reactive Agents, Technical Report, 2015.41Optimal Selective Attention in Reactive AgentsTechnical ReportRoy Fox Naftali TishbySchool of Computer Science and Engineering The Hebrew UniversityAbstractIn POMDPs, information about the hidden state conveyed through observations is valuable both to the agent, as it allows him to base his actions on more informed internal states, and a \"curse\" that exploits the size and diversity of the internal state space. An attempt to deal with this is to focus on reactive strategies that base their actions only on recent observations, but even reactive strategies may require resources, and agents need to pay selective attention to only a portion of the information available to them in observations. In this report, we present the minimum informational principle for selective attention in reactive agents."}, {"heading": "1 Introduction", "text": "In fact, the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move."}, {"heading": "2 Preliminaries", "text": "This year is the highest in the history of the country."}, {"heading": "3 Reduction from retentive to reactive policies", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "4 Minimum-information principle", "text": "Our guiding principle in formalizing selective attention is to reduce information complexity as measured by the Shannon information between observation and action. We first present the principle and then justify it by linking it to source coding. We note that there are numerous other justifications and correlations, some of which have been discussed previously [9] [24] [16] [8], and some of which should be examined further, especially in connection with POMDP planning. (2) The meaningful mutual information between ot and time step t is given by it (ot, at) = log t (at) t (at) t (at), with t (at) = Xot t (at) t (at). (2) This can be thought of as the internal information costs if one chooses measures in response to observation ot. The long-term average expectation of these internal costs, similar to the external costs, is I = lim T! 11TT 1Xt = 0ot []."}, {"heading": "If the policy has period T and the process is at its periodic marginal distribution then", "text": "I = 1T T 1Xt = 0Xot, for T (ot) t (at | ot) it (ot, at) = 1 T 1Xt = 0DKL [tk't] = 1 T 1Xt = 0I [ot; at].Here, DKL [tk't] is the Kullback-Leibler divergence from t to t, and I [ot; at] is the Shannon mutual information between ot and at.DKL [tk't] is a measure of the cognitive efforts required for the agent to divert from a passive, uncontrolled policy t to an active, controlled policy. Unlike [9] [24] [25], we allow passive policy and active policy to be designed or developed. Uncontrolled policy, which minimizes the cost of information, is the appropriate boundary distribution of the action (2) [5].Among the external agents that cause C, the internal costs are the lowest, the internal costs, and the lowest internal costs."}, {"heading": "F is called the free energy, due to its similarity to the quantity of the same name in statistical physics, with taking the part of the inverse temperature.", "text": "T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T-T T"}, {"heading": "4.1 Necessary conditions for optimality", "text": "t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (t) t (st) t (st) t (st) t (t) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t st st st st st st st st st st t st st st st st st st st st st st t (t) t (t) t (t) t (t) t (st) t (t) t (st) t (st) t (t) t (st) t) t (st) t (t) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st st st st st st st st st st st st st st st st st st st st st st st st st st st t (t) t (t) t (t) st st st st st st st st st st st st st st st t (t) t (t) t (t) t (st) st st st st st st st st st st t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t (st) t) t (st) t (st) t) t (st) t) t (st) t (st) t (st) t) t (st) t (st) t (st st st) t (st st st st) t (st) t (st) t (st) t (st) t) t (st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st st t t (t (t) t"}, {"heading": "As tends to infinity, the optimal policy in (4) becomes deterministic. Together with (3), it becomes a Bellman equation [3].", "text": "The best deterministic reactive policies are generally arbitrarily inferior to optimal stochastic policies [20]. Optimism in reactive strategies requires stochasticity. Unfortunately, many planning algorithms rely on the smaller space of deterministic strategies (e.g. [14]), and others lack a principle by which the optimal level of uncertainty in the agent's actions can be measured (e.g. [2]). We propose a minimum of information as a principle. Furthermore, the model used for planning is unsafe in practice itself. Deterministic strategies may overlap with available data and impede further learning [22]. Reflections on information offer a principled way to adapt policy uncertainty to the uncertainty of the model [18]. In enhanced learning, it is common to use Softmax to obtain stochastic planning and exploration policies [22]."}, {"heading": "4.2 Sequential rate-distortion", "text": "The form of (4) and (2) may be known as a solution to the rate-distortion problem of lossy source encoding. (5) In fact, the minimum information can be interpreted as a sequential rate-distortion problem. (5) The reactive agent selects a channel from its sensor to its actor. (5) The decoder then reconstructs the intended at. (5) For a given time step t (4) and a distortion function dt (5)."}, {"heading": "4.3 Optimization algorithm", "text": "Forward recursion (1), backward recursion (3), optimal policy (4) and its marginal solution (2) are necessary prerequisites for an optimal solution. They also provide an algorithm to search for a good solution: Insert the current solution iteratively into the right side of one of the equations to obtain a better solution until (asymptotically) no such improvement is possible. Many existing algorithms use a similar scheme. For example, in the Generalized Policy Iteration algorithm for planning in the MDPs [22] there is a timetable for switching between 3-policy assessment, a variant of (3) and policy improvement, a variant of (4) with! 1. An ingenious timetable can guarantee that the solution will improve monotonously with each iteration [6]. Here we propose the following simpler timetable, for which such a guarantee does not apply, but which is empirically consistent with good solutions in practice."}, {"heading": "Repeat until convergence:", "text": "1. Calculate the margin solution t given the current solution for by applying (2).2. Calculate the value function given the current solution for p-t, t and. This can be done by iterative application (3) until it converges, or by solving it as a system of linear equations.3A forward equation is not required for fully observable problems if the attention is not selective. 473. In a forward algorithm to convergence to a boundary cycle: (a) Calculate the margin solution p-t given the current solution for p-t 1 and t 1 by applying (1). b) Calculate the optimal policy t given the current solution for p-t, t and t by applying (4)."}, {"heading": "5 Periodicity in reactive policies", "text": "This year it has come to the point where we are in a phase where we are in a phase where we are in a phase where we have never been in a position to be in a phase where we are in a position to be in a phase where we are in a position to be in a phase where we are in a position to be in a time where we are going to be in a time where we are going to be in a time where we are going to be in a time where we are going to be in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are a time where we are in a time where we are in a time where we are in a time where we are in a time where we are."}, {"heading": "5.1 Robot example", "text": "Another example is the POMDP, shown in Figure 9. Here, a robot moves from the left end of a corridor to the right. It can be in one of 4 states: it can be at both ends of the corridor, and it can carry an object or not. It has 4 actions: to move to the left end of the corridor, or to the right, or to pick up or drop an object. However, one action may fail with a probability of 0.2, leaving the robot in the same state. It can pick up only one object at the left end of the corridor, and it receives a reward for picking up or dropping an object at the right end. It has 4 possible observations of two binary sensors that tell it its position and whether it is carrying an object or not. The location sensor is more reliable and shows the correct position with a probability of 0.88. The load sensor only shows the correct load state with a probability of 0.7. The parameters have been selected for visual clarity of the problem."}, {"heading": "6 Discussion", "text": "In this paper, we presented three novel results in which reactive agents interact with partially observable systems. We motivated the focus on reactive strategies by reducing retentive strategies, introduced a principle and algorithm for optimizing reactive strategies, and examined a surprising aspect of their phenomenology. We conclude with a few comments on the implications of each contribution."}, {"heading": "6.1 Selective attention as clustering", "text": "Information-limited clustering can also be interpreted as source coding [16], so that the data to be clustered are considered a source and cluster centering as a reconstruction. Following the relationship we show between selective attention and source coding, we can imagine a reactive policy as soft clustering of observations into actions. If the information restriction is lifted, clustering becomes difficult, with each data point assigned to its closest center. In our case, the policy becomes more deterministic as it grows until it always selects the optimal measures for each observation at! 1. The implication of considering reactive strategies as clusters is that actions should generally be simpler and never more complex than the observations on which they are based. In fact, there is a duality between observations and actions, and between selective attention (the retained part of observation) and selective measures (the intended part of action as a deviation from the preceding section, the information cannot be retained), and the items of information cannot be retained from the preceding section."}, {"heading": "6.2 Implications of selective attention for retentive agents", "text": "In this paper, we have focused on reactive agents and introduced the minimum information principle for optimal selective attention. However, as Reduction 50in Section 3 shows, this also has an impact on reactive agents. The effect of selective attention is to make internal states less complex than their inputs by discarding information that is not useful enough. Applied to the inference policy, this leads to an approximate conclusion that offsets the external value of information in the conduct of actions against its internal cost in information complexity. In fact, an inference process in POMDPs corresponds to sequential clustering. With each new observation, the pair (mt 1, ot) is clustered into a new internal state. The biggest challenge in planning in POMDPs is to approach the Bayesian belief in a way that allows efficient planning and execution without losing too much value. Selective attention and in this case retention is precisely such a principle."}, {"heading": "6.3 Policy bifurcations and chaos theory", "text": "We have discovered the occurrence of branching in the optimization process of reactive strategies, which represent many characteristics of chaos theory of iterated functions, such as period doubling and slow convergence near the branching points. We expect that there will be many more of these characteristics in other, more complex systems. We suspect that systems with more states, perhaps infinitely more, can represent a cascade of branching that leads to aperidicity and chaos. A full investigation of the orientation of the branching and chaos theories toward optimal control in dynamic systems goes beyond the scope of this report. To the extent that such a connection exists, it could have profound philosophical implications, since it could indicate that intelligent actors interacting with complex environments must choose between the following alternatives: \u2022 Plan with very little regard to their input factors \u2022 Plan for very short horizons \u2022 Determine their own value function with some degree of inability to predict their own future actions."}, {"heading": "Minimum-Information LQG", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Control", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Part I: Memoryless Controllers", "text": "Approved for publication: Roy Fox and Naftali Tishby, Minimum-Information LQG Control - Part I: Memoryless Controllers, In Proceedings of the 55th"}, {"heading": "IEEE Conference on Decision and Control (CDC), 2016.", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is not a country, but a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is not a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which is a country, in which is a country,"}, {"heading": "3.2 Part II: Retentive Controllers", "text": "Approved for publication: Roy Fox and Naftali Tishby, Minimum-Information LQG Control - Part II: Retentive Controllers, In Proceedings of the 55th"}, {"heading": "IEEE Conference on Decision and Control (CDC), 2016.", "text": "This year is the highest in the history of the country."}, {"heading": "3.3 Supplementary Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Supplementary material for Part I and Part II.", "text": "In fact, most of us are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves, \"he told the German Press Agency in an interview with\" Welt am Sonntag. \""}, {"heading": "Minimum-KL Reinforcement", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Taming the Noise in Reinforcement", "text": "Learning via Soft UpdatesPublished: Roy Fox \u02da, Ari Pakman \u02da and Naftali Tishby, Baming the Noise in Reinforcement Learning via Soft UpdatesRoy Fox Hebrew UniversityAri Pakman Columbia UniversityNaftali Tishby Hebrew UniversityAbstractModel-free reinforcement learning algorithms, such as Q-learning, perform poorly in the early stages of learning in noisy environments because much effort is devoted to neglecting distorted estimates of the state-action-value function. The distortion results from the fact that, among several noisy estimates, the apparent optimum of learning, which can actually be suboptimal, is selected. We propose G-Learning, a new non-political learning algorithm that regulates value by reducing the previous deterrence of politics to the costs of this approach."}, {"heading": "1 INTRODUCTION", "text": "In this context, it should be noted that the measures in question are measures that have been taken in recent years."}, {"heading": "2 LEARNING IN NOISY ENVIRONMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 NOTATION AND BACKGROUND", "text": "We consider the usual setting of a Markov Decision Process (MDP), in which an agent interacts with its environment by repeatedly observing its state s 2 S and finally performing an action 2A with A and S at cost c 2 R. This results in a stochastic process s0, a0, c0, s1,.., where s0 is fixed, and where for t 0 we have the Markov properties derived from the conditional distributions at t (at), ct (ct, at), and st + 1p (st + 1, at).The objective of the agent is to find a time-invariant policy that refers to the total discounted costs V (s) = Xt 0 t E [ct | s0 = s], (1) that we simultaneously apply for each s 2 S, for a given discontractive discontractive factor 0 & lt.For each t, the expectation above is related to all trajectories of length from the beginning."}, {"heading": "2.2 BIAS AND EARLY COMMITMENT", "text": "Despite the success of Q-Learning in many situations, learning can proceed extremely slowly when there is noise in the distribution, since st and at, one of the terms of (2), namely the cost ct and the value of the next state st + 1. The source of this problem is a negative bias introduced by the min operator into the estimator mina0 Q (st + 1, a0) when (5) is inserted into (4). To illustrate this bias, assume that Q (s, a) is an unbiased but loud estimate of the optimal Q (s, a). Then Jensen's inequality for the concave min operator implies that E [min a Q (s, a) min a (s, a), (7) with equality only when Q already indicates the optimal course of action by showing arg mina (s, a) the course of action mina (s, a) is not necessary in the course of action mina (a)."}, {"heading": "2.3 THE INTERPLAY OF VALUE BIAS AND POLICY SUBOPTIMALITY", "text": "It is instructive to consider the effects of the distortion not only on the estimation function, but also on the real value V of greedy politics (5), since in many cases the latter is the actual performance of the learning process. Here, the central quantity of interest is the gap Q (s, a0) V (s), in a certain state s, between the value of a non-optimal action a0 and the value of the optimal action. First, consider the case where the gap in the estimation of the Q (s, a) values is large compared to noise. In this case, a0 does indeed appear to be highly likely to be suboptimal, as desired. Interestingly, even the learning agent should not worry if the confusion of such an a0 for the optimal action has a limited effect on the value of greedy politics, as the choice of a0 is almost optimal. We conclude that the real value V of greedy politics (5) is suboptimal only when the effect of the large gap between the optimal levels of noise cannot appear to be the cause and the optimal order itself."}, {"heading": "2.4 A DYNAMIC OPTIMISM-UNCERTAINTY LOOP", "text": "The above considerations were agnostic towards the exploration policy, but the reduction of the distortions can be accelerated by an exploration policy that is almost greedy. In this case, an estimate with high variance corrects itself: an estimated value with optimistic distortion pulls the exploration towards this state, which leads to a reduction in variance, which in turn reduces the optimistic distortion. This is a dynamic form of optimism under uncertainty. Whereas in the usual case, optimism is externally imposed as a starting condition [19], here it is spontaneously generated by the noise and corrected by the exploration itself. The approach we propose below to reduce the distortion is motivated to express the uncertainty explicitly and not indirectly by an optimistic distortion. We note that, although at the end of the learning process, the deterministic greedy policy obtained from Q (a, s) as in (5) is obtained, during the learning process itself we can consider the distortion of the policy to be improved by forgoing the distortion in Q, the next phase of the distortion of the policy."}, {"heading": "3 LEARNING WITH SOFT UPDATES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 THE FREE-ENERGY FUNCTION G AND G-LEARNING", "text": "In fact, it is the case that it is a matter of a way in which one sees oneself in a position to abide by the rules. (...) In fact, it is the case that one sees oneself in a position to abide by the rules. (...) In fact, it is the case that one has to abide by the rules. (...) In fact, it is the case that one abides by the rules. (...) In fact, it is the case that one abides by the rules. (...) It is as if one abides by the rules. (...) In fact, it is as if one abides by the rules. (...) It is as if one abides by the rules. (...) It is as if one abides by the rules. (...) It is as if one abides by the rules. (...) It is as if one abides by the rules. (...) It is as if one abides by the rules. (...) It is as if one abides by the rules."}, {"heading": "3.2 THE ROLE OF THE PRIOR", "text": "The previous policy can therefore choose a previous policy that represents all our prior knowledge, but no more. This previous policy has maximum entropy in light of prior knowledge. [20] In our examples in Section 6, we use the uniform previous policy that does not represent prior knowledge. In both Q-Learning and G-Learning, we could use the prior knowledge that stepping into a wall is never a good measure by eliminating these measures. An advantage of G-Learning is that it can be based on softer prior knowledge. For example, an earlier policy that provides a lower probability of stepping into a wall represents the prior knowledge that such a measure is normally (but not always) harmful, a type of knowledge that cannot be used in Q-Learning."}, {"heading": "3.3 CONVERGENCE", "text": "In this section we examine the convergence of G according to the updating rule (18). Remember that the uppermost norm is defined as | x | 1 = maxi | xi |. We need the following term, which is in the Supplementary Material.Lemma 1. (19) The operator B [G] (s, a) is a contraction in the uppermost norm, B [G1] B [G2] 1 G1 G2 1. (19) The updating equation (18) of the algorithm can be written as stochastic iteration equation equation Gt + 1 (st, at) = (1) Gt (st, at) (20) + 1 (B t] (st, at) + zt (ct, st + 1)), whereby the random variable zt zt (ct, st + 1) = (st, at) = B [Gt] (st, at) + < Convergence Xa0 (0 | st) + 1, in particular, is equal to a pair (G)."}, {"heading": "4 SCHEDULING", "text": "In the previous section, we have shown that G-Learning with a fixed convergence, with probability 1, leads to the optimal G-Learning for this, since it degenerates to the equations for G 'and F' by recursion in (12) - (14).79When = 1 and G-Learning becomes Q-Learning. When = 0, the update policy in (14) corresponds to the previous one. This case, referred to as Q-Learning, converges to Q '. In an early learning phase, Q-Learning has an advantage over Q-Learning because it avoids a deterministic policy based on a loud Q function. In a later learning phase, when Q'Learning is a more precise estimate of Q', Q-Learning gains the advantage by being updated with a better policy than the previous one. This will therefore be demonstrated in Section 6.1.We would like to show that G-Learning makes a smooth transition from Q-Learning, just at the right pace, in order to enjoy the advantage of the latter, as we always argue in Section 6.1."}, {"heading": "4.1 ORACLE SCHEDULING", "text": "To consider the effects of scheduling on correcting bias (7), we assume that during learning we achieve a G that is an unbiased estimate of G. G (st, at) would remain impartial if we update the update rule (st + 1, a) (22) with a = arg min a0 G (st + 1, a 0), (23), but we do not have access to this optimal action. If we update the update rule (18) with = 0, we update G (st, at) towardsct + Xa0 (a0 | st + 1, a 0) G (st + 1, a0), (24) which is always at least as large as (22), which creates a positive bias. If we update G (st, at), we update G (st) towardsct + min a0G (st + 1, a 0), (24) which creates at least as large a positive bias as (22)."}, {"heading": "4.2 PRACTICAL SCHEDULING", "text": "In the examples in Section 6, we have adopted the linear Schedulet = kt, (26) with a constant k > 0. Another possibility we investigated was to make the variance of G inversely proportional to the current average of the Bellman error, which decreases as learning progresses, and the results were similar to the linear Schedulet. The optimal parameter k can be achieved by performing initial runs with different k values and selecting the value whose learned policies empirically yield the lower cost part. Although this research appears costly compared to other algorithms that do not require parameter adjustment, these initial runs do not need to be performed for many iterations. Furthermore, in many situations, the agent is confronted with a class of similar ranges, and matching k in some initial ranges results in improved learning for the entire class. This is the case in the domain generator example in Section 6.1."}, {"heading": "5 RELATED WORK", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "6 EXAMPLES", "text": "This section demonstrates how G-Learning improves existing model-free learning algorithms in multiple settings, and the areas we use are clean and simple to show that the benefits of G-Learning are inherent in the algorithm itself. We plan the learning rate so that it does not relate to (st, at)!, (29), where nt (st, at) is the number of visits by the couple (st, at). This scheme is widely applied and is consistent with (6) for! 2 (1 / 2, 1]. We choose! = 0.8, which is within the range proposed in [37]. We plan linearly, as discussed in Section 4.2. In any case, we start with 5 preliminary runs of G-Learning with different linear coefficients, and choose the coefficient with the lowest empirical cost. This coefficient is used in subsequent test runs, the results of which are presented in Figure 2.2. In all cases, we use a uniform G-Learning policy with different linear coefficients and select the coefficients with the lowest empirical cost."}, {"heading": "6.1 GRIDWORLD", "text": "In fact, most of them are able to trump themselves, and they are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...) Most of them are able to trump themselves. (...)"}, {"heading": "6.2 CLIFF WALKING", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "7 CONCLUSIONS", "text": "The algorithm we have introduced successfully mitigates the slow learning problem of Q-Learning in the early stage in Figure 4: Cliff Domain. The agent can select a horizontally or vertically adjacent square and move there deterministically. The color scale and arrow lengths each indicate the frequency of visit in each state and transition, in the first 250,000 iterations of Q-Learning, Expected-SARSA and G-Learning. The short-term exploration policy of Q-Learning has a higher chance of taking the shortest path near the cliff at the bottom than that of G-Learning. As a non-political algorithm, Q-Learning cannot be optimized for exploration policy, while G-Learning is successful.Noisy environments caused by the bias caused by the hard optimization of policy allow such bias to be applied to us, which have an optimal repetition of the information model we believe is also possible in Q-Learning policy."}, {"heading": "Acknowledgments", "text": "AP is supported by ONR funding N00014-14-1-0243 and IARPA through DoI / IBC contract number D16PC00003. RF and NT are supported by the DARPA MSEE Program, the Gatsby Charitable Foundation, the Israel Science Foundation and the Intel ICRI-CI Institute.84"}, {"heading": "Discussion", "text": "In this paper, we examined bound agents operating in dynamic systems under intrinsic information constraints, which can be modelled as a sequential distortion problem and solved with a forward-backward algorithm. We examined the convergence properties of the algorithm, used the structure of the specific LQG case, and simplified the setting to make it usable for learning. In this section, we summarize and discuss some of the insights we have gained in our work."}, {"heading": "A Principle for the Tradeoff of Informational Resources and Costs", "text": "Our approach, introduced in Section 2.1, is to identify different components within the agent, such as sensors, memories and actuators, and to take into account the information rates on the communication channels between these components. Reducing extrinsic costs is commonly assumed to be an optimization goal, with the latter becoming intrinsic information costs, to which we add intrinsic constraints on the rates at which information can be communicated between sensors, memories and actuators. In the storage form of this optimization problem, the latter become intrinsic information costs traded at extrinsic expectation costs."}, {"heading": "Periodicity and Instability of the Optimization Principle", "text": "The algorithm presented in Section 2.1 is applicable to general POMDPs, but is only demonstrated in passive POMDPs where actions have a cost but do not affect the state of the world. Experiments with other types of examples showed low convergence, which appeared to be the result of periodicity or instability of the solution among the update provider, especially after phase transitions that increase support for agent policy. Therefore, we have taken the first steps in investigating the bifurcation structure of learning dynamics around critical values of the target parameter 87eter. Section 2.2 analyzes examples that illustrate this structure. Phenomenology of planning in partial observation under informational constraints includes time duplication by supercritical pitchfork forks. The optimal solution at these critical points becomes periodic, forcing the agent to pay attention to a clock signal."}, {"heading": "The Linear-Quadratic-Gaussian Case", "text": "The general algorithm presented in Section 2.1 is polynomial in relation to the quantities of the world state involved, the state of memory, the spaces of observation and action. If these spaces are very large or continuous, we can no longer apply the algorithm in this tabular form. Instead, the solution must be parameterized in a tractable manner, and the gradient must be taken into account in relation to these parameters. A particularly important and revealing parametric family examined in Chapter 3 is the Gaussian distributions (for p), the linear-Gaussian conditional distributions (for p, q and), and the quadratic functions (for c and b). This family has special properties when considering unlimited agents. It is self-conjugated, meaning that under linear-Gaussian dynamics an optimal dynamic is created, a Gaussian marginal form remains Gaussian, and a quadratic function remains quadratic order."}, {"heading": "Learning and Scheduling", "text": "An actor interacting with an environment whose state is partially observable must learn whether or not he has a model of dynamics. The setting in which no such model is available is particularly interesting because it illustrates how the trade-off between cost and simplicity changes over the course of the algorithm, as shown in Section 4.1. The Maximum Relative Entropy Principle states that a solution should minimize KL divergence to a simple prior one, provided that it matches all the additional information we have about the solution. In the MDP learning environment, this additional information is represented by the value function, which is iteratively enhanced by random sample-based updates. An imperfect value function generally cannot be used to select an optimal policy, and we must settle for a suboptimal value guarantee. Therefore, the policy used in any update should be the simplest, in terms of the KL divergence that is achieved under the condition that this value guarantee is met."}, {"heading": "Glossary", "text": "\"It's not the first time we've had to deal with the question of to what extent we're able to establish ourselves in the world,\" he said. \"It's the first time we're trying to establish ourselves in the world.\" \"It's the second time we're trying to establish ourselves in the world.\" \"It's the second time we're trying to establish ourselves in the world.\" \"It's the second time we're trying to establish ourselves in the world.\" \"It's the second time we're trying to establish ourselves in the world.\" \"It's the first time we're trying to establish ourselves in the world.\" \"It's the second time we're trying to establish ourselves in the world.\" \"It's the second time we're trying to establish ourselves in the world.\" \"It's the second time we're trying to err in the world.\""}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "We model the interaction of an intelligent agent with its environment as<lb>a Partially Observable Markov Decision Process (POMDP), where the joint<lb>dynamics of the internal state of the agent and the external state of the<lb>world are subject to extrinsic and intrinsic constraints. Extrinsic constraints<lb>of partial observability and partial controllability specify how the agent\u2019s<lb>input observation depends on the world state and how the latter depends on<lb>the agent\u2019s output action. The agent also incurs an extrinsic cost, based on<lb>the world states reached and the actions taken in them.<lb>Bounded agents are also limited by intrinsic constraints on their ability to<lb>process information that is available in their sensors and memory and choose<lb>actions and memory updates. In this dissertation, we model these constraints<lb>as information-rate constraints on communication channels connecting these<lb>various internal components of the agent.<lb>The simplest is to first consider reactive (memoryless) agents, with a chan-<lb>nel connecting their sensors to their actuators. The problem of optimizing<lb>such an agent, under a constraint on the information rate between the input<lb>and the output, is a sequential rate-distortion problem. The marginal distri-<lb>bution of the observation can be computed by a forward inference process,<lb>whereas the expected cost-to-go of an action can be computed by a backward<lb>control process. Given this source distribution and this effective distortion,<lb>respectively, each step can be optimized by solving a rate-distortion problem<lb>that trades off the extrinsic cost with the intrinsic information rate. Retentive (memory-utilizing) agents can be reduced to reactive agents<lb>by interpreting the state of the memory component as part of the external<lb>world state. The memory reader can then be thought of as another sensor<lb>and the memory writer as another actuator and they are limited by the same<lb>informational constraint between inputs and outputs.<lb>In this dissertation we make four major contributions detailed below and<lb>many smaller contributions detailed in each section.<lb>First, we formulate the problem of optimizing the agent under both ex-<lb>trinsic and intrinsic constraints and develop the main tools for solving it.<lb>This optimization problem is highly non-convex, with many local optima.<lb>Its difficulty is mostly due to the coupling of the forward inference process<lb>and the backward control process. The inference policy and the control pol-<lb>icy can be optimal given each other but still jointly suboptimal as a pair.<lb>For example, if some information is not attended to it cannot be used and if<lb>it is not used it should optimally not be attended to.<lb>Second, we identify another reason for the challenging convergence prop-<lb>erties of the optimization algorithm, which is the bifurcation structure of the<lb>update operator near phase transitions. We show that the update operator<lb>may undergo period doubling, after which the optimal policy is periodic and<lb>the optimal stationary policy is unstable. Any algorithm for planning in such<lb>domains must therefore allow for periodic policies, which may themselves be<lb>subject to an informational constraint on the clock signal.<lb>Third, we study the special case of linear-Gaussian dynamics and quadratic<lb>cost (LQG), where the optimal solution has a particularly simple and solv-<lb>able form. Under informational constraints, the forward and the backward<lb>processes are not separable. However, we show that they do have a more ex-<lb>plicitly solvable structure; namely, a sequential semidefinite program. This<lb>also allows us to analyze the structure of the retentive solution under the<lb>reduction to the reactive setting.<lb>Fourth, we explore the learning task, where the model of the world dy-", "creator": "LaTeX with hyperref package"}}}