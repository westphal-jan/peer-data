{"id": "1609.07706", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2016", "title": "Learning by Stimulation Avoidance: A Principle to Control Spiking Neural Networks Dynamics", "abstract": "Learning based on networks of real neurons, and by extension biologically inspired models of neural networks, has yet to find general learning rules leading to widespread applications. In this paper, we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network. Using carefully timed external stimulation, the network can be driven towards a desired dynamical state. We term this principle \"Learning by Stimulation Avoidance\" (LSA). We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1]. We examine the mechanism's basic dynamics in a reduced network, and demonstrate how it scales up to a network of 100 neurons. We show that LSA has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation, and can be used as a learning rule for an embodied application: learning of wall avoidance by a simulated robot. The surge in popularity of artificial neural networks is mostly directed to disembodied models of neurons with biologically irrelevant dynamics: to the authors' knowledge, this is the first work demonstrating sensory-motor learning with random spiking networks through pure Hebbian learning.", "histories": [["v1", "Sun, 25 Sep 2016 06:44:42 GMT  (653kb,D)", "http://arxiv.org/abs/1609.07706v1", "17 pages, 11 figures"]], "COMMENTS": "17 pages, 11 figures", "reviews": [], "SUBJECTS": "cs.NE cs.AI cs.LG", "authors": ["lana sinapayen", "atsushi masumori", "takashi ikegami"], "accepted": false, "id": "1609.07706"}, "pdf": {"name": "1609.07706.pdf", "metadata": {"source": "CRF", "title": "Learning by Stimulation Avoidance: A Principle to Control Spiking Neural Networks Dynamics", "authors": ["Lana Sinapayen", "Atsushi Masumori", "Takashi Ikegami"], "emails": ["lana@sacral.c.u-tokyo.ac.jp"], "sections": [{"heading": null, "text": "In this paper, we argue for the existence of a principle that makes it possible to control the dynamics of a biologically inspired neural network. By means of carefully timed external stimulation, the network can be brought into a desired dynamic state. We call this principle \"Learning by Stimulation Avoidance\" (LSA). We show by simulation that the minimal sufficient conditions that lead to LSA in artificial networks are also sufficient to reproduce learning outcomes similar to those that Shahaf and Marom have achieved in biological neurons. We investigate the fundamental dynamics of the mechanism in a reduced network and show how it scales to a network of 100 neurons. We show that LSA has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation and can be used as a learning rule for an embodied application: Learning this wall of dynamically simulated models is initially entangled by the popularity of the robotic dance."}, {"heading": "Author Summary", "text": "Theoretical approaches have shown that spiking models have enormous potential for learning and processing information [3], but efforts to harness this potential in mainstream applications are still ongoing. By finding general learning rules that govern the dynamics of spiking networks, we move closer to this goal. In this paper, we propose such a biologically plausible rule (LSA), explain how it affects the dynamics of the network, show how it can be used in practice to obtain the desired dynamics from initially random networks, and finally show that it can be used to achieve the desired behavior in a simulated robot."}, {"heading": "Introduction", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "Output", "text": "The structure of the paper is as follows: The model is presented in Section 1; we show that the conditions necessary to obtain LSA are sufficient to reproduce biological results in Section 2.1 and to study the dynamics of LSA in a minimal network of 3 neurons in Section 2.2. We then show that LSA supports hypotheses to explain biological mechanisms not covered by the SRP (Section 2.3) and implements a simple embodied application using LSA as the only learning mechanism in Section 2.4. In Section 2.5 we examine the effects of parameter changes on the learning performance of the network."}, {"heading": "1 Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Network model", "text": "We use the model of spiking neurons developed by Izhikevich [17] to simulate excitatory neurons (regular spiking neurons) and inhibitory neurons (fast spiking neurons) with a simulation time step of 1 ms. Equations of the neuronal model and the resulting dynamics are shown in Figure 2. In the experiments of Section 2.1 and Section 2.4, we simulate fully connected networks of 100 neurons (self-connections are forbidden) with 80 excitatory and 20 inhibitory neurons. This ratio of 20% inhibitory neurons is standard in simulations [2, 17] and close to real biological values (15%, [18]). Initial weights are random (even distribution: 0 < w < 5 for excitatory neurons, \u2212 5 < w < 0 for inhibitory neurons, < < 0 for inhibitory neurons, < < < for neurons < <)."}, {"heading": "Plasticity model", "text": "We add synaptic plasticity to all networks in the form of STDP, as suggested in [19]. STDP is applied only between excitatory neurons; other connections retain their initial weight throughout the simulation. We use additive STDP: Fig. 3 shows the weight variation \u2206 w for a synapse that goes from neuron a to neuron b. As shown in the figure, \u2206 w is negative when b fires first, and positive a fire first. Total weight w varies as follows: wt = wt \u2212 1 + \u2206 w. (3) We set a maximum value for the weight: if w > wmax, w is reset to wmax. In experiments with 100-neuron networks, we also apply a decay function to all weights in the network. The decay function is applied to each iteration as: 1variations in the number of connections and the strength variance are fracted as {1} in Section 2.5\\ w (A - 1s}."}, {"heading": "Robot", "text": "In Section 2.4, we simulate a simple robot that moves within an arena surrounded by walls, the arena is a square of 1000 px (pixels).The robot has a radius of 25 px, which constantly moves at 1 px / ms, except in the event of a collision with a wall. The robot has two distance sensors, each oriented to \u03c0 / 4 and \u2212 \u03c0 / 4 from the front direction of the robot.The sensors have a range of 80 px; they are activated when the robot is less than 80 px away from a wall, in the direction supported by the orientation of each sensor. Two input zones in the network (10 neurons each) receive input signals in mV from the sensors as follows: output = sensitivity / distance (10) The sensitivity of the sensors is set to a constant value for the duration of each experiment. For simplicity, the control of the robot is non-neural, it is controlled by two peaks each in the network (10)."}, {"heading": "2 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 LSA is Sufficient to Explain Biological Results", "text": "In this context, it is also worth mentioning the fact that these two are a group of people who are able to survive on their own."}, {"heading": "2.2 Dynamics of LSA in a Minimal Network", "text": "In [7] we showed that a minimal network of 2 neurons consistently follows the principle of LSA; we also showed that a single neuron is capable of cutting one synapse and amplifying another synapse simultaneously, depending on the stimulation received by the two pre-synaptic neurons. In this experiment, we investigate the weight dynamics in a chain of 3 excitatory neurons, all connected to each other, as a simplification of what can happen in a fully networked network: one neuron is used as input, one as output, and they are separated from a \"hidden neuron.\" Neurons are called 0 (input neurons), 1 (hidden neurons), and 2 (output neurons). Figure 5 shows the results of experiments with different learning conditions and different starting states can be summarized as follows: 1. In positive LSA, direct connections between input and output neurons are privileged."}, {"heading": "2.3 LSA has More Explanatory Power than the SRP", "text": "We have seen that our simple model can reproduce the results of the Shahaf experiments, and that LSA can potentially explain this behavior. In this experiment, we change the parameters of the network to have a sparsely connected network with strong noise (see Methods), which are less susceptible to global eruptions and exhibit strong desynchronous activity, as shown in Fig."}, {"heading": "2.4 Embodied Application: Wall Avoidance with a Robot", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2.5 Parameter Exploration", "text": "Finally, we perform a parameter search to explore the working conditions of the \"simple learning task.\" In the previous experiments, the network was fully networked and the initial connection weights followed a uniform distribution between 0 and 5 for the excitatory neurons and -5 and 0 for the inhibitory neurons. In this section, we vary the number of connections in the network and the variance v of the weights. For each neuron, an initial connection is randomly selected and the weight is initialized at w = 5 + \u03c9 (w = \u2212 5 + \u03c9), repeating this process at n = 20 experiments (\"simple learning\" experiment) of length T = 500 seconds. The average difference between the firing rate of the output zone during the first 100 seconds and the last 100 seconds may be lower than M. For each group (ig, M), we perform N = 20 experiments (\"simple learning\" experiment) of length T = 500 seconds."}, {"heading": "Discussion", "text": "In this paper, we present a new principle that explains the dynamics of spiking neural networks under the influence of external stimulation. The model presented in this paper is very simplified compared to biological neurons, since it uses only two types of neurons, one type of STDP, no homeostatic mechanism, etc. Nevertheless, the model is able to reproduce key features of experiments performed on biological neurons and to explain the results obtained in vitro with neurons subject to external stimulation. LSA also provides an explanation for a biological mechanism that is ignored by the theory of the SRP, namely the circumcision of synapses. LSA has direct practical applications: by establishing causal relationships between neuronal dynamics and external stimulations, we can induce learning and change the dynamics of neurons from the outside. LSA relies on the mechanism of the STDP, and we show that the conditions we obtain to obtain the network (externally) are (1) by stimulating the network (externally)."}, {"heading": "Acknowledgments", "text": "This work was supported by the Grant-in-Aid for Scientific Research (Studies on Homeo-Dynamics with Cultivated Neural Circuits and Embodied Artificial Neural Networks; 24300080)."}], "references": [{"title": "Learning in networks of cortical neurons", "author": ["G Shahaf", "S. Marom"], "venue": "The Journal of Neuroscience", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Which model to use for cortical spiking neurons", "author": ["Izhikevich EM"], "venue": "IEEE transactions on neural networks", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2004}, {"title": "Networks of spiking neurons: the third generation of neural network models", "author": ["W. Maass"], "venue": "Neural networks", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1997}, {"title": "Development, learning and memory in large random networks of cortical neurons: lessons beyond anatomy", "author": ["S Marom", "G. Shahaf"], "venue": "Quarterly reviews of biophysics", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Principles of behavior: an introduction to behavior theory", "author": ["Hull CL"], "venue": "Appleton-Century;", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1943}, {"title": "Psychological facts and psychological theory", "author": ["Guthrie ER"], "venue": "Psychological Bulletin", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1946}, {"title": "Learning by Stimulation Avoidance as a primary principle of spiking neural networks dynamics", "author": ["L Sinapayen", "A Masumori", "N Virgo", "T. Ikegami"], "venue": "European Conference on Artificial Life (ECAL", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Experiments on a robot behavior controlled by cultured neuronal cells", "author": ["A Masumori", "N Maruyama", "L Sinapayen", "T Mita", "U Frey", "D Bakkum", "Principle et al. Emergence of sense-making behavior by the Stimulus Avoidance"], "venue": "13th European Conference on Artificial Life", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "The organization of behavior", "author": ["Hebb DO"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1949}, {"title": "Spike timing-dependent plasticity: a Hebbian learning rule", "author": ["N Caporale", "Y. Dan"], "venue": "Annu Rev Neurosci", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Spike timing-dependent plasticity: from synapse to perception", "author": ["Dan Y", "Poo MM"], "venue": "Physiological reviews", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Competitive Hebbian learning through spike-timing-dependent synaptic plasticity", "author": ["S Song", "KD Miller", "LF. Abbott"], "venue": "Nature neuroscience", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Hebbian STDP in mushroom bodies facilitates the synchronous flow of olfactory information in locusts", "author": ["S Cassenaer", "G. Laurent"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2007}, {"title": "Spike timing-dependent synaptic depression in the in vivo barrel cortex of the rat", "author": ["V Jacob", "DJ Brasier", "I Erchova", "D Feldman", "DE. Shulz"], "venue": "The Journal of Neuroscience", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Temporal specificity in the cortical plasticity of visual space representation", "author": ["YX Fu", "K Djupsund", "H Gao", "B Hayden", "K Shen", "Y. Dan"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2003}, {"title": "Pairing-induced changes of orientation maps in cat visual cortex", "author": ["S Schuett", "T Bonhoeffer", "M. H\u00fcbener"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Simple model of spiking neurons", "author": ["EM Izhikevich"], "venue": "IEEE Transactions on neural networks", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "GABA immunoreactive neurons in rat visual cortex", "author": ["DL Meinecke", "A. Peters"], "venue": "Journal of Comparative Neurology", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1987}, {"title": "Reconciling the STDP and BCM models of synaptic plasticity in a spiking recurrent neural network", "author": ["D Bush", "A Philippides", "P Husbands", "M. O\u2019Shea"], "venue": "Neural computation", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2010}, {"title": "Simulation of networks of spiking neurons: a review of tools and strategies", "author": ["R Brette", "M Rudolph", "T Carnevale", "M Hines", "D Beeman", "JM Bower"], "venue": "Journal of computational neuroscience", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "Morphological characterization of in vitro neuronal networks", "author": ["O Shefi", "I Golding", "R Segev", "E Ben-Jacob", "A. Ayali"], "venue": "Physical Review E", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2002}, {"title": "Controlling bursting in cortical cultures with closed-loop multi-electrode stimulation", "author": ["DA Wagenaar", "R Madhavan", "J Pine", "SM. Potter"], "venue": "The Journal of neuroscience", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Dynamics of sparsely connected networks of excitatory and inhibitory spiking neurons", "author": ["N. Brunel"], "venue": "Journal of computational neuroscience", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "The free-energy principle: a unified brain theory", "author": ["K. Friston"], "venue": "Nature Reviews Neuroscience", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}], "referenceMentions": [{"referenceID": 0, "context": "We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1].", "startOffset": 229, "endOffset": 232}, {"referenceID": 1, "context": "Networks of spiking neurons are currently the model that most closely reproduces real neuron\u2019s dynamics [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 2, "context": "Theoretical approaches have shown that spiking models have a tremendous potential for learning and processing information [3], but efforts are still ongoing to use this potential in mainstream applications.", "startOffset": 122, "endOffset": 125}, {"referenceID": 0, "context": "In two papers published in 2001 and 2002, Shahaf and Marom conduct experiments with a training method that drives rats\u2019 cortical neurons cultivated in vitro to learn given tasks [1, 4].", "startOffset": 178, "endOffset": 184}, {"referenceID": 3, "context": "In two papers published in 2001 and 2002, Shahaf and Marom conduct experiments with a training method that drives rats\u2019 cortical neurons cultivated in vitro to learn given tasks [1, 4].", "startOffset": 178, "endOffset": 184}, {"referenceID": 4, "context": "Marom explains these results by invoking the Stimulus Regulation Principle (SRP, from [5, 6]).", "startOffset": 86, "endOffset": 92}, {"referenceID": 5, "context": "Marom explains these results by invoking the Stimulus Regulation Principle (SRP, from [5, 6]).", "startOffset": 86, "endOffset": 92}, {"referenceID": 6, "context": "Why are several training cycles necessary if \u201cstability\u201d guarantees that the configuration of the network is preserved after stopping the stimulation? How does \u201cmodifiability\u201d not conflict with the idea of learning, if we cannot prevent the \u201cgood\u201d topology to be modified by the stimulation at each new training cycle? We propose a different explanatory mechanism: the principle of Learning By Stimulation Avoidance (LSA, [7, 8]).", "startOffset": 422, "endOffset": 428}, {"referenceID": 7, "context": "Why are several training cycles necessary if \u201cstability\u201d guarantees that the configuration of the network is preserved after stopping the stimulation? How does \u201cmodifiability\u201d not conflict with the idea of learning, if we cannot prevent the \u201cgood\u201d topology to be modified by the stimulation at each new training cycle? We propose a different explanatory mechanism: the principle of Learning By Stimulation Avoidance (LSA, [7, 8]).", "startOffset": 422, "endOffset": 428}, {"referenceID": 8, "context": "LSA is an emergent property of spiking networks coupled to Hebbian rule [9] and external stimulation.", "startOffset": 72, "endOffset": 75}, {"referenceID": 9, "context": "STDP is a Hebbian learning rule so fundamental that it has been consistently found in the brains of a wide range of species, from insects to humans [10\u201312].", "startOffset": 148, "endOffset": 155}, {"referenceID": 10, "context": "STDP is a Hebbian learning rule so fundamental that it has been consistently found in the brains of a wide range of species, from insects to humans [10\u201312].", "startOffset": 148, "endOffset": 155}, {"referenceID": 11, "context": "STDP is a Hebbian learning rule so fundamental that it has been consistently found in the brains of a wide range of species, from insects to humans [10\u201312].", "startOffset": 148, "endOffset": 155}, {"referenceID": 12, "context": "In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].", "startOffset": 85, "endOffset": 89}, {"referenceID": 13, "context": "In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].", "startOffset": 208, "endOffset": 215}, {"referenceID": 15, "context": "In vitro and in vivo experiments based on STDP can reliably enhance sensory coupling [13], decrease it [14], and these bidirectional changes can even be combined to create receptive fields in sensory neurons [15,16].", "startOffset": 208, "endOffset": 215}, {"referenceID": 16, "context": "We use the model of spiking neuron devised by Izhikevich [17] to simulate excitatory neurons (regular spiking neurons) and inhibitory neurons (fast spiking neurons) with a simulation time step of 1 ms.", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "This ratio of 20% of inhibitory neurons is standard in simulations [2, 17] and close to real biological values (15%, [18]).", "startOffset": 67, "endOffset": 74}, {"referenceID": 16, "context": "This ratio of 20% of inhibitory neurons is standard in simulations [2, 17] and close to real biological values (15%, [18]).", "startOffset": 67, "endOffset": 74}, {"referenceID": 17, "context": "This ratio of 20% of inhibitory neurons is standard in simulations [2, 17] and close to real biological values (15%, [18]).", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "We add synaptic plasticity to all networks in the form of STDP as proposed in [19].", "startOffset": 78, "endOffset": 82}, {"referenceID": 6, "context": "1 LSA is Sufficient to Explain Biological Results In [7] we showed that a simulated random spiking network built from [2, 21] combined to STDP could be driven to learn desired output patterns using a training method similar to that of Shahaf et al.", "startOffset": 53, "endOffset": 56}, {"referenceID": 1, "context": "1 LSA is Sufficient to Explain Biological Results In [7] we showed that a simulated random spiking network built from [2, 21] combined to STDP could be driven to learn desired output patterns using a training method similar to that of Shahaf et al.", "startOffset": 118, "endOffset": 125}, {"referenceID": 19, "context": "1 LSA is Sufficient to Explain Biological Results In [7] we showed that a simulated random spiking network built from [2, 21] combined to STDP could be driven to learn desired output patterns using a training method similar to that of Shahaf et al.", "startOffset": 118, "endOffset": 125}, {"referenceID": 6, "context": "This is the experiment we reproduced in [7], demonstrating that this learning behaviour is a direct effect of STDP and is captured by the principle of LSA: firing patterns leading to the removal of external stimulation are strengthened, firing patterns that lead to the application of an external stimulation are avoided.", "startOffset": 40, "endOffset": 43}, {"referenceID": 20, "context": "We also use a fully connected network, while the biological network grown in vitro is likely to be sparsely connected [22].", "startOffset": 118, "endOffset": 122}, {"referenceID": 21, "context": "Our hypothesis is that bursts are detrimental to learning [23] and explain the difficulty of obtaining selective learning.", "startOffset": 58, "endOffset": 62}, {"referenceID": 6, "context": "2 Dynamics of LSA in a Minimal Network In [7] we showed that a minimal network of 2 neurons consistently follows the principle of LSA; we also showed that a single neuron is able to prune one synapse and enhance another synapse simultaneously depending on the stimulation received by the two presynaptic neurons.", "startOffset": 42, "endOffset": 45}, {"referenceID": 22, "context": "But introducing inhibitory neurons in the network can improve network stability [24].", "startOffset": 80, "endOffset": 84}, {"referenceID": 21, "context": "Global bursts are also considered to be a pathologic behaviour for in vitro networks, and do not occur with healthy in vivo networks [23].", "startOffset": 133, "endOffset": 137}, {"referenceID": 7, "context": "These results could potentially be reproduced in a network in vitro: the Izhikevich model of spiking network that we use has been found to exhibit the same dynamics as real neurons, and our experiments with STDP can reproduce some results of biological experiments; therefore there is a probability that this results predicted by LSA still holds in biological networks with suppressed bursts, especially since we have shown that LSA gives promising results on biological networks embodied in simple robots (see [8] and following section).", "startOffset": 511, "endOffset": 514}, {"referenceID": 23, "context": "Minimisation introduced by Friston [25].", "startOffset": 35, "endOffset": 39}], "year": 2016, "abstractText": "Learning based on networks of real neurons, and by extension biologically inspired models of neural networks, has yet to find general learning rules leading to widespread applications. In this paper, we argue for the existence of a principle allowing to steer the dynamics of a biologically inspired neural network. Using carefully timed external stimulation, the network can be driven towards a desired dynamical state. We term this principle \u201cLearning by Stimulation Avoidance\u201d (LSA). We demonstrate through simulation that the minimal sufficient conditions leading to LSA in artificial networks are also sufficient to reproduce learning results similar to those obtained in biological neurons by Shahaf and Marom [1]. We examine the mechanism\u2019s basic dynamics in a reduced network, and demonstrate how it scales up to a network of 100 neurons. We show that LSA has a higher explanatory power than existing hypotheses about the response of biological neural networks to external simulation, and can be used as a learning rule for an embodied application: learning of wall avoidance by a simulated robot. The surge in popularity of artificial neural networks is mostly directed to disembodied models of neurons with biologically irrelevant dynamics: to the authors\u2019 knowledge, this is the first work demonstrating sensory-motor learning with random spiking networks through pure Hebbian learning.", "creator": "LaTeX with hyperref package"}}}