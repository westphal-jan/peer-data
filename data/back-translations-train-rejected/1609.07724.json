{"id": "1609.07724", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2016", "title": "The RNN-ELM Classifier", "abstract": "In this paper we examine learning methods combining the Random Neural Network, a biologically inspired neural network and the Extreme Learning Machine that achieve state of the art classification performance while requiring much shorter training time. The Random Neural Network is a integrate and fire computational model of a neural network whose mathematical structure permits the efficient analysis of large ensembles of neurons. An activation function is derived from the RNN and used in an Extreme Learning Machine. We compare the performance of this combination against the ELM with various activation functions, we reduce the input dimensionality via PCA and compare its performance vs. autoencoder based versions of the RNN-ELM.", "histories": [["v1", "Sun, 25 Sep 2016 10:18:19 GMT  (44kb,D)", "http://arxiv.org/abs/1609.07724v1", null]], "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["athanasios vlontzos"], "accepted": false, "id": "1609.07724"}, "pdf": {"name": "1609.07724.pdf", "metadata": {"source": "CRF", "title": "The RNN-ELM Classifier", "authors": ["Athanasios Vlontzos"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "In this paper, we combine the Random Neural Network (RNN) [6], [12], [13] and the Extreme Learning Machine (ELM) [16] in flat and deep classifiers and compare their performance. The RNN is a stochastic integral state in which integrate and fire system [14], initially developed into model biological neurons [7] and expanded to model soma-soma-soma interactions. It consists of M interconnected neurons, each of which can receive positive (exemplary) or negative (inhibitory) signals from external sources such as sensory sources or other cells. The RNN can be described by equations that are possible to provide mathematically useful properties."}, {"heading": "2 The RNN-ELM and the PCA-RNN-ELM", "text": "RNN-ELM: Inspired by the fact that in mammalian brains, among other communication mechanisms, cells exhibit a quasi-simultaneous firing pattern (1), which through soma-soma interactions [15], in [14] an extension of the RNN = q = q = q result (1). A special network was considered containing n identical connected neurons, each of which has a firing rate of r and external excitatory and inhibitory spikes, so that each cell i within the cluster has an inhibitory weight of w \u2212 u. \u2212 u The state of each neuron was indicated by q and each neuron receives an inhibitory input from some external neurons that are not part of the cluster. Also, the internal firing rate was set to zero w + i, j = 0."}, {"heading": "3 Simulation Results", "text": "Before we set out, we must note that most algorithms perform better (e.g., the number of algorithms in this area is very high) or that the number of algorithms in this area is very high (e.g., the number of algorithms is very high) and that the number of algorithms in this area is very high (e.g., the number of algorithms is very high)."}, {"heading": "4 Conclusion", "text": "In this work, we compared the RNN-ELM with the ELM with various activation functions and found that the RNN-ELM achieves much better results with far less hidden neurons. We also demonstrated that the RNN-ELM network with PCA pre-processing is a viable alternative to other image classification algorithms by comparing three versions of the RNN-ELM network and a deep RNN architecture on the standard data sets MNIST and NORB. Results were obtained without prior extraction or image processing outside the PCA algorithm to focus on the raw performance of the tested algorithms. We observed that the relatively simple PCA-RNN-ELM can provide high accuracy and very fast training and test times, while the deep auto-code ELM algorithm can achieve similar results on the MNIST dataset."}], "references": [{"title": "A laser intensity image based automatic vehicle classification system", "author": ["H. Abdelbaki", "K. Hussain", "E. Gelenbe"], "venue": "IEEE Proc. on Intelligent Transportation Systems pp. 460\u2013465", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "Flexible, high performance convolutional neural networks for image classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "L.M. Gambardella", "J. Schmidhuber"], "venue": "International Joint Conference on Artificial Intelligence (IJCAI-2011, Barcelona)", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-column deep neural network for traffic sign classification", "author": ["D.C. Ciresan", "U. Meier", "J. Masci", "J. Schmidhuber"], "venue": "Neural Networks", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Video quality and traffic qos in learning based subsampled and receiver interpolated video sequences", "author": ["C. Cramer", "E. Gelenbe"], "venue": "IEEE J. on Selected Areas in Communications 18(2), 150\u2013167", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2000}, {"title": "Low bit-rate video compression with neural networks and temporal subsampling", "author": ["C. Cramer", "E. Gelenbe", "H. Bakircloglu"], "venue": "Proceedings of the IEEE 84(10), 1529\u20131543", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1996}, {"title": "Random neural networks with negative and positive signalsand product form solution", "author": ["E. Gelenbe"], "venue": "Neural Computation 1(4), 502\u2013510", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1989}, {"title": "Oscillatory corticothalamic response to somatosensory input", "author": ["E. Gelenbe", "C. Cramer"], "venue": "Biosystems 48(1), 67\u201375", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1998}, {"title": "Cognitive packet network for bilateral asymmetric connections", "author": ["E. Gelenbe", "Z. Kazhmaganbetova"], "venue": "IEEE Trans. Industrial Informatics 10(3), 1717\u20131725", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Area-based results for mine detection", "author": ["E. Gelenbe", "T. Kocak"], "venue": "IEEE Trans. Geoscience and Remote Sensing 38(1), 12\u201314", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Dynamical random neural network approach to the traveling salesman problem", "author": ["E. Gelenbe", "V. Koubi", "F. Pekegrin"], "venue": "IEEE Trans Sys. Man, Cybernetics pp. 630\u2013635", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1993}, {"title": "Deep learning with random neural networks", "author": ["E. Gelenbe", "Y. Yin"], "venue": "IEEE World Conference on Computational Intelligence, IJCNN", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "G-networks with triggered customer movement", "author": ["E. Gelenbe"], "venue": "Journal of Applied Probability pp. 742\u2013748", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1993}, {"title": "Random neural networks with synchronized interactions", "author": ["E. Gelenbe", "S. Timotheou"], "venue": "Neural Computation vol. 20(no. 9), pp. 2308\u20132324,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep learning with random neural networks", "author": ["E. Gelenbe", "Y. Yin"], "venue": "SAI Intelligent Systems Conference 2016", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Neuronal soma-satellite glial cell interactions in sensory ganglia and the participation of purigenic receptors", "author": ["Y. Gu", "Y. Chen", "X. Zhang", "G. Li", "C. Wang", "L. Huang"], "venue": "Neuron Glia Biology 6(1), 53\u201362", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2010}, {"title": "Extreme learning machine: theory and applications", "author": ["G.B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing 70(1), 489 \u2013 501", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Representational learning with elms for big data", "author": ["L. Kasun", "H. Zhou", "G. Huang", "C. Vong"], "venue": "IEEE Intelligent Systems pp. 31\u201334", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G. Hinton"], "venue": "NIPS 2012: Neural Information Processing Systems, Lake Tahoe, Nevada", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2012}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE 86(5), 755 \u2013 824", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Autonomous search for mines", "author": ["W.Gelenbe", "Y. Cao"], "venue": "European J. Oper. Research 108(2), 319\u2013333", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1998}], "referenceMentions": [{"referenceID": 1, "context": "Deep Learning, using convolutional neural networks with multiple layers of hidden units has in recent years achieved human-competitive or even better than human performance in image classification tasks [2],[18] at the expense of long training times and specialised hardware [3].", "startOffset": 203, "endOffset": 206}, {"referenceID": 17, "context": "Deep Learning, using convolutional neural networks with multiple layers of hidden units has in recent years achieved human-competitive or even better than human performance in image classification tasks [2],[18] at the expense of long training times and specialised hardware [3].", "startOffset": 207, "endOffset": 211}, {"referenceID": 2, "context": "Deep Learning, using convolutional neural networks with multiple layers of hidden units has in recent years achieved human-competitive or even better than human performance in image classification tasks [2],[18] at the expense of long training times and specialised hardware [3].", "startOffset": 275, "endOffset": 278}, {"referenceID": 5, "context": "In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.", "startOffset": 56, "endOffset": 59}, {"referenceID": 11, "context": "In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.", "startOffset": 60, "endOffset": 64}, {"referenceID": 12, "context": "In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.", "startOffset": 65, "endOffset": 69}, {"referenceID": 15, "context": "In this paper we combine the Random Neural Network (RNN)[6],[12],[13] and the Extreme Learning Machine (ELM)[16] in shallow and deep classifiers and compare their performance.", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "The RNN: The RNN is a stochastic integer state, integrate and fire system [14], initially developed to model biological neurons [7] and extended to model soma-to-soma interactions [15].", "startOffset": 74, "endOffset": 78}, {"referenceID": 6, "context": "The RNN: The RNN is a stochastic integer state, integrate and fire system [14], initially developed to model biological neurons [7] and extended to model soma-to-soma interactions [15].", "startOffset": 128, "endOffset": 131}, {"referenceID": 14, "context": "The RNN: The RNN is a stochastic integer state, integrate and fire system [14], initially developed to model biological neurons [7] and extended to model soma-to-soma interactions [15].", "startOffset": 180, "endOffset": 184}, {"referenceID": 13, "context": "It provides useful mathematical properties and algorithmic efficiency as seen in [14] : \u2013 The state of each neuron i is represented at a given time t by a integer ki \u2265 0 which can describe the neuron\u2019s level of excitation.", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": ", kn)] and it satisfies a coupled system of Chapman-Kolmogorov equations \u2013 The RNN has a \u201cproduct form\u201d solution [6], meaning that in steady state, the joint probability distribution of network state is equal to the product of marginal probabilities", "startOffset": 113, "endOffset": 116}, {"referenceID": 6, "context": "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.", "startOffset": 60, "endOffset": 63}, {"referenceID": 19, "context": "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.", "startOffset": 104, "endOffset": 108}, {"referenceID": 8, "context": "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.", "startOffset": 109, "endOffset": 112}, {"referenceID": 4, "context": "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.", "startOffset": 141, "endOffset": 144}, {"referenceID": 3, "context": "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.", "startOffset": 145, "endOffset": 148}, {"referenceID": 0, "context": "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.", "startOffset": 181, "endOffset": 185}, {"referenceID": 7, "context": "The RNN was initially developed to model biological neurons [7] and has been used for landmine detection[20],[9], video and image processing [5],[4],[1], combinatorial optimisation [10], network routing[8] and emergency management citeemergency.", "startOffset": 202, "endOffset": 205}, {"referenceID": 15, "context": "The ELM: The Extreme Learning Machine [16] is a Single Layer Feedforward Network (SLFN) with one layer of hidden neurons.", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "Input weights W1 to the hidden neurons are assigned randomly in the range [0,1] and never changed while the output weights W2 are estimated in one step by observing that its output is calculated as in eq.", "startOffset": 74, "endOffset": 79}, {"referenceID": 14, "context": "RNN-ELM: Inspired by the fact that in mammalian brains, among other communication mechanisms, cells exhibit a quasi-simultaneous firing pattern through soma-to-soma interactions[15], in [14] an extension of the RNN was presented.", "startOffset": 177, "endOffset": 181}, {"referenceID": 13, "context": "RNN-ELM: Inspired by the fact that in mammalian brains, among other communication mechanisms, cells exhibit a quasi-simultaneous firing pattern through soma-to-soma interactions[15], in [14] an extension of the RNN was presented.", "startOffset": 186, "endOffset": 190}, {"referenceID": 13, "context": "The result that [14] has reached is :", "startOffset": 16, "endOffset": 20}, {"referenceID": 13, "context": "An Update Rule for ELM Output: In [14] and [11] to achieve better accuracies in classification tasks an update rule was introduced.", "startOffset": 34, "endOffset": 38}, {"referenceID": 10, "context": "An Update Rule for ELM Output: In [14] and [11] to achieve better accuracies in classification tasks an update rule was introduced.", "startOffset": 43, "endOffset": 47}, {"referenceID": 18, "context": "[19]) or require much larger computational resources (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2],[18],[17]).", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[2],[18],[17]).", "startOffset": 4, "endOffset": 8}, {"referenceID": 16, "context": "[2],[18],[17]).", "startOffset": 9, "endOffset": 13}, {"referenceID": 16, "context": "In [17] an ELM structure 784-15000-10 was used to achieve 97% testing accuracy with a sigmoid activation function.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Also its important to note that the time needed to achieve 30 iterations of the simulation seems to be constant and independent of the number of neurons used, in contrast to the method used in [14] where we observe an increase in the time needed as the number of neurons increases, as seen in table 2.", "startOffset": 193, "endOffset": 197}, {"referenceID": 10, "context": "Autoencoder-ELM: We compared PCA-RNN-ELM with Autoencoder-ELM[11] using the same number of PCs and autoencoder neurons while varying the ELM size.", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "Table 2: MNIST Simulation results: Classifier of [14] with 500-500-X structure", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "pair of images had 2048 features (pixel values), similarly to [14].", "startOffset": 62, "endOffset": 66}, {"referenceID": 13, "context": "Finally we ran the Deep RNN-ELM network of [14] and obtained a training time of 34.", "startOffset": 43, "endOffset": 47}], "year": 2016, "abstractText": "In this paper we examine learning methods combining the Random Neural Network, a biologically inspired neural network and the Extreme Learning Machine that achieve state of the art classification performance while requiring much shorter training time. The Random Neural Network is a integrate and fire computational model of a neural network whose mathematical structure permits the efficient analysis of large ensembles of neurons. An activation function is derived from the RNN and used in an Extreme Learning Machine. We compare the performance of this combination against the ELM with various activation functions, we reduce the input dimensionality via PCA and compare its performance vs. autoencoder based versions of the RNN-ELM.", "creator": "LaTeX with hyperref package"}}}