{"id": "1609.07756", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2016", "title": "A Factorized Model for Transitive Verbs in Compositional Distributional Semantics", "abstract": "We present a factorized compositional distributional semantics model for the representation of transitive verb constructions. Our model first produces (subject, verb) and (verb, object) vector representations based on the similarity of the nouns in the construction to each of the nouns in the vocabulary and the tendency of these nouns to take the subject and object roles of the verb. These vectors are then combined into a final (subject,verb,object) representation through simple vector operations. On two established tasks for the transitive verb construction our model outperforms recent previous work.", "histories": [["v1", "Sun, 25 Sep 2016 15:10:16 GMT  (16kb)", "http://arxiv.org/abs/1609.07756v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lilach edelstein", "roi reichart"], "accepted": false, "id": "1609.07756"}, "pdf": {"name": "1609.07756.pdf", "metadata": {"source": "CRF", "title": "A Factorized Model for Transitive Verbs in Compositional Distributional Semantics", "authors": ["Lilach Edelstein"], "emails": ["lilach.edelstein@mail.huji.ac.il", "roiri@ie.technion.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.07 756v 1 [cs.C L] 25 SeWe present a factorized compositional distribution model for the representation of transitive verbal constructions. Our model initially produces (subject, verb) and (verb, object) vector representations based on the similarity of nouns in construction to each of the nouns in vocabulary and the tendency of these nouns to occupy the subject and object roles of the verb. These vectors are then combined into a definitive (subject, verb, object) representation by simple vector operations. In two established tasks for transitive verb construction, our model surpasses previous work."}, {"heading": "1 Introduction", "text": "In recent years, vector-space models deriving representations of word coccurence patterns in text have established themselves in lexical semantics research (Turney et al., 2010; Clark, 2012). Following this success, we have turned to compositional semantics (CDS): the combination of distributional word representations, often in a syntactic manner, to produce representations of phrases and sentences. Some tasks and techniques have been proposed for CDS. Some work aims to represent sentences that vary in length and structure (Socher et al., 2012; Marelli et al., 2014; Mikolov, 2014; Pham et al."}, {"heading": "2 Model", "text": "The goal of our model is to create vector representations (embedding) for subject-verb object (s, v, o) constructions, where s is subject noun, v is verb and o is object noun. The model consists of two steps: (1) embedding the (s, v) and (v, o) pairs based on coexistence statistics of the members of each pair with all other nouns in the vocabulary; and (2) combine the (s, v) and (v, o) representations to create a definitive (s, v, o) embedding. Our model is a factorized model that generates (s, v, o) representations from its pair components (s, v) and (v, o) that often appear as subjects and (v, o)."}, {"heading": "2.1 Pair Representations", "text": "We represent the (s, v) and (v, o) pairs through the relations between the verb (v) and the subject (s, o). (v, o) The object (v, o) is considered as the subject (for (s, o) or the object (for (v, o) of the subject (for s) or the object (for s). (n) The object (for s, o) and the object (for o) is described as the subject. (n) It follows the representation of the subjects in the vocabulary. (s, o) This is a very similar process for (s, o). (s) We become the subjectives (for the subjectivists) and the subjectivists (for the subjectivists) and the subjectivists (for the subjectivists) and the subjectivists (for the subjectivists) and (for the) for the (for the) and the subjectivists (for the) and (for the) for the subjectivists and (for the) for the subjectivists."}, {"heading": "3 Experiments", "text": "\"We have our models on the purified and tokenized Polyglot-Corpus (Al-Rfou et al., 2013), 1, 2, 3, 4, 5, 5, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7"}, {"heading": "4 Results", "text": "The results are presented in Table 1. Our models are superior to the GS11 task: The distances between our best model and the best starting position are 13.2 and 4.2 points respectively for the averaged and non-averaged conditions, compared to the best previous results, the deviation is 13.5 and 9.7 points respectively. In the KS14 task, it is the simple w2v sum basis that performs best (\u03c1 = 0.76 for averaged values, \u03c1 = 0.6 for non-averaged values), but in both conditions, it is one of our models that performs best (w2v-PPMI concat p = 0.743 for averaged values, w2v-PPMI-Coor-Mult with p = 0.567 for non-averaged values). However, in this task, our gap between the fundamentals and the SS11 models is smaller compared to the superiority of a simple additive model for KS14, which is consistent with previously reported results."}, {"heading": "5 Conclusions", "text": "We presented a factorized model for (s, v, o) embedding, which is a simple and compact alternative to existing methods, and showed that it excels in two current CDS tasks. In future work, we intend to expand our model to include more complex syntactic constructions, such as ditransitive verb constructions and constructions containing adjectives and adverbs."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual NLP", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "In Proc. of CoNLL", "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space", "author": ["Baroni", "Zamparelli2010] Marco Baroni", "Roberto Zamparelli"], "venue": "In Proc. of EMNLP", "citeRegEx": "Baroni et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2010}, {"title": "Frege in space: A program of compositional distributional semantics", "author": ["Baroni et al.2014] Marco Baroni", "Raffaela Bernardi", "Roberto Zamparelli"], "venue": "Linguistic Issues in Language Technology,", "citeRegEx": "Baroni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Baroni et al\\.", "year": 2014}, {"title": "Top accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet"], "venue": "In Proc. of COLING", "citeRegEx": "Bohnet.,? \\Q2010\\E", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Vector space models of lexical meaning. Handbook of Contemporary Semantics, Wiley-Blackwell, \u00e0 para\u0131\u0302tre", "author": ["Stephen Clark"], "venue": null, "citeRegEx": "Clark.,? \\Q2012\\E", "shortCiteRegEx": "Clark.", "year": 2012}, {"title": "Mathematical foundations for a compositional distributional model of meaning", "author": ["Coecke et al.2011] Bob Coecke", "Mehrnoosh Sadrzadeh", "Stephen Clark"], "venue": "Linguistic Analysis,", "citeRegEx": "Coecke et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Coecke et al\\.", "year": 2011}, {"title": "Low-rank tensors for verbs in compositional distributional semantics", "author": ["Fried et al.2015] Daniel Fried", "Tamara Polajnar", "Stephen Clark"], "venue": "In Proc. of ACL (short papers)", "citeRegEx": "Fried et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fried et al\\.", "year": 2015}, {"title": "Experimental support for a categorical compositional distributional model of meaning", "author": ["Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proc. of EMNLP", "citeRegEx": "Grefenstette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Experimenting with transitive verbs in a discocat", "author": ["Grefenstette", "Mehrnoosh Sadrzadeh"], "venue": "In Proceedings of the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics", "citeRegEx": "Grefenstette et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2011}, {"title": "Multi-step regression learning for compositional distributional semantics", "author": ["Georgiana Dinu", "Yao-Zhong Zhang", "Mehrnoosh Sadrzadeh", "Marco Baroni"], "venue": "In Proc. of IWCS", "citeRegEx": "Grefenstette et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Grefenstette et al\\.", "year": 2013}, {"title": "A unified sentence space for categorical distributionalcompositional semantics: Theory and experiments", "author": ["Mehrnoosh Sadrzadeh", "Stephen Pulman"], "venue": "In Proc. of COLING", "citeRegEx": "Kartsaklis et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2012}, {"title": "A study of entanglement in a categorical framework of natural language", "author": ["Mehrnoosh Sadrzadeh", "Chris Heunen", "Manuel L Reyes", "Ravi Kunjwal", "Tobias Fritz"], "venue": "In Proceedings of the 11th Workshop on Quantum", "citeRegEx": "Kartsaklis et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kartsaklis et al\\.", "year": 2014}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Mikolov2014] Quoc Le", "Tomas Mikolov"], "venue": "In Proc. of ICML", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "A sick cure for the evaluation of compositional distributional semantic models", "author": ["Stefano Menini", "Marco Baroni", "Luisa Bentivogli", "Raffaella Bernardi", "Roberto Zamparelli"], "venue": "In Proc. of LREC", "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Turning on the turbo: Fast third-order non-projective turbo parsers", "author": ["Miguel B. Almeida", "Noah A. Smith"], "venue": "In Proc. of ACL (short papers)", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Evaluating neural word representations in tensor-based compositional settings", "author": ["Dimitri Kartsaklis", "Mehrnoosh Sadrzadeh", "Matthew Purver"], "venue": "In Proc. of EMNLP", "citeRegEx": "Milajevs et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Milajevs et al\\.", "year": 2014}, {"title": "Vector-based models of semantic composition", "author": ["Mitchell", "Lapata2008] Jeff Mitchell", "Mirella Lapata"], "venue": "In Proc. of ACL", "citeRegEx": "Mitchell et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2008}, {"title": "A practical and linguistically-motivated approach to compositional distributional semantics", "author": ["Nghia The Pham", "Marco Baroni"], "venue": "In Proc. of ACL", "citeRegEx": "Paperno et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2014}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan T. McDonald"], "venue": "In In Proc. of LREC", "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Jointly optimizing word representations for lexical and sentential tasks with the c-phrase model", "author": ["Pham et al.2015] Nghia The Pham", "Germ\u00e1n Kruszewski", "Angeliki Lazaridou", "Marco Baroni"], "venue": "In Proc. of ACL", "citeRegEx": "Pham et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Reducing dimensions of tensors in type-driven distributional semantics", "author": ["Luana Fagarasan", "Stephen Clark"], "venue": "In Proc. of EMNLP", "citeRegEx": "Polajnar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Polajnar et al\\.", "year": 2014}, {"title": "Semantic compositionality through recursive matrixvector spaces", "author": ["Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proc. of EMNLP-CoNLL", "citeRegEx": "Socher et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2012}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}], "referenceMentions": [{"referenceID": 4, "context": "In recent years, vector space models, deriving word meaning representations from word cooccurrence patterns in text, have become prominent in lexical semantics research (Turney et al., 2010; Clark, 2012).", "startOffset": 169, "endOffset": 203}, {"referenceID": 22, "context": "Some work aims at representing sentences that vary in length and structure mostly with neural network models (Socher et al., 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015).", "startOffset": 109, "endOffset": 193}, {"referenceID": 13, "context": "Some work aims at representing sentences that vary in length and structure mostly with neural network models (Socher et al., 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015).", "startOffset": 109, "endOffset": 193}, {"referenceID": 20, "context": "Some work aims at representing sentences that vary in length and structure mostly with neural network models (Socher et al., 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015).", "startOffset": 109, "endOffset": 193}, {"referenceID": 5, "context": "Recently, the categorical framework (Coecke et al., 2011; Baroni et al., 2014) has been proposed, where each word is represented by a tensor whose order is determined by the categorical grammar type of the word.", "startOffset": 36, "endOffset": 78}, {"referenceID": 2, "context": "Recently, the categorical framework (Coecke et al., 2011; Baroni et al., 2014) has been proposed, where each word is represented by a tensor whose order is determined by the categorical grammar type of the word.", "startOffset": 36, "endOffset": 78}, {"referenceID": 10, "context": ", 2012; Marelli et al., 2014; Le and Mikolov, 2014; Pham et al., 2015). Another approach, which we take in this paper, is to focus on specific syntactic constructions. At the expense of generality, this approach enables an in-depth investigation of a specific linguistic phenomenon. Mitchell and Lapata (2008) proposed various additive and multiplicative operators for the combinations of word vectors, and applied them to intransitive verbs and their subjects.", "startOffset": 8, "endOffset": 310}, {"referenceID": 1, "context": ", 2011; Baroni et al., 2014) has been proposed, where each word is represented by a tensor whose order is determined by the categorical grammar type of the word. For example, Baroni and Zamparelli (2010) represent nouns by a vector, and adjectives by matrices transforming one noun vector into another.", "startOffset": 8, "endOffset": 204}, {"referenceID": 21, "context": "Indeed, several recent works have tried to reduce the size of these models (Polajnar et al., 2014; Fried et al., 2015) while others proposed matrix based representations (Polajnar et al.", "startOffset": 75, "endOffset": 118}, {"referenceID": 6, "context": "Indeed, several recent works have tried to reduce the size of these models (Polajnar et al., 2014; Fried et al., 2015) while others proposed matrix based representations (Polajnar et al.", "startOffset": 75, "endOffset": 118}, {"referenceID": 21, "context": ", 2015) while others proposed matrix based representations (Polajnar et al., 2014; Milajevs et al., 2014; Paperno et al., 2014).", "startOffset": 59, "endOffset": 127}, {"referenceID": 16, "context": ", 2015) while others proposed matrix based representations (Polajnar et al., 2014; Milajevs et al., 2014; Paperno et al., 2014).", "startOffset": 59, "endOffset": 127}, {"referenceID": 18, "context": ", 2015) while others proposed matrix based representations (Polajnar et al., 2014; Milajevs et al., 2014; Paperno et al., 2014).", "startOffset": 59, "endOffset": 127}, {"referenceID": 11, "context": "Our model outperforms recent previous work on two established tasks for the transitive verb construction (Grefenstette and Sadrzadeh, 2011b; Kartsaklis et al., 2014).", "startOffset": 105, "endOffset": 165}, {"referenceID": 0, "context": "Data Preprocessing and Training We trained our models on the cleaned and tokenized Polyglot Wikipedia corpus (Al-Rfou et al., 2013),1 consisting of approximately 75M sentences and 1.", "startOffset": 109, "endOffset": 131}, {"referenceID": 19, "context": "The corpora were POS-tagged with universal POS (UPOS) tags (Petrov et al., 2012) using the TurboTagger (Martins et al.", "startOffset": 59, "endOffset": 80}, {"referenceID": 14, "context": ", 2012) using the TurboTagger (Martins et al., 2013),2 trained with default settings (SVM MIRA with 20 iterations) without any further parameter fine-tuning, on the TRAIN+DEV portion of the UD treebank.", "startOffset": 30, "endOffset": 52}, {"referenceID": 3, "context": "61 (Bohnet, 2010),4 trained on the same UD treebank portion as the tagger and with default settings.", "startOffset": 3, "endOffset": 17}, {"referenceID": 15, "context": "To compute the similarity between two nouns with NNSim, we trained the word2vec skip-gram model with negative sampling (Mikolov et al., 2013) on our (unparsed) training corpus; context-window size was set to 5 and vector dimensionality to 200.", "startOffset": 119, "endOffset": 141}, {"referenceID": 11, "context": "The second task uses the transitive sentence similarity dataset (KS14, (Kartsaklis et al., 2014)).", "startOffset": 71, "endOffset": 96}, {"referenceID": 10, "context": "The second task uses the transitive sentence similarity dataset (KS14, (Kartsaklis et al., 2014)). This dataset consists of 108 subject-verb-object pairs, derived from 72 subject-verb-object triplets arranged into pairs. Unlike GS11, here each pair is composed of two triplets that differ in all three words. For example, the pair \u201d(programme, offer, support), (service, provide, help)\u201d is expected to get a higher similarity score compared to \u201d(school, encourage, child), (employee, leave, company)\u201d. In both tasks, we consider two different aggregation methods over the annotator scores of a pair: (a) the human scores of each annotator are paired with the model scores without averaging, and a \u03c1 score is computed between the two vectors; and (b) the human scores are first averaged, a human ranking is derived from the averaged scores and compared to the model ranking. While the second method may seem more robust, it was not used in most previous works (see discussion in Mitchell and Lapata (2008)).", "startOffset": 72, "endOffset": 1005}, {"referenceID": 6, "context": "Finally, we compare to state-of-the-art previous work: (a) the most recent study on our tasks ((Fried et al., 2015), their table 1, 2015best-tensor: their best model; 2015-best-simple: best result with additive or multiplicative combination as in Mitchell and Lapata (2008)); and (b) Milajevs et al.", "startOffset": 95, "endOffset": 115}, {"referenceID": 6, "context": "Finally, we compare to state-of-the-art previous work: (a) the most recent study on our tasks ((Fried et al., 2015), their table 1, 2015best-tensor: their best model; 2015-best-simple: best result with additive or multiplicative combination as in Mitchell and Lapata (2008)); and (b) Milajevs et al.", "startOffset": 96, "endOffset": 274}, {"referenceID": 6, "context": "Finally, we compare to state-of-the-art previous work: (a) the most recent study on our tasks ((Fried et al., 2015), their table 1, 2015best-tensor: their best model; 2015-best-simple: best result with additive or multiplicative combination as in Mitchell and Lapata (2008)); and (b) Milajevs et al. (2014) which performed exhaustive comparison of models and vector representations for our tasks (see their table 2, 2014-best-simple: best result with additive or", "startOffset": 96, "endOffset": 307}, {"referenceID": 10, "context": "multiplicative combination; 2014-best-non-simple: best result with tensor and matrix combinations based on (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b; Kartsaklis et al., 2012; Kartsaklis et al., 2014)).", "startOffset": 107, "endOffset": 227}, {"referenceID": 11, "context": "multiplicative combination; 2014-best-non-simple: best result with tensor and matrix combinations based on (Grefenstette and Sadrzadeh, 2011a; Grefenstette and Sadrzadeh, 2011b; Kartsaklis et al., 2012; Kartsaklis et al., 2014)).", "startOffset": 107, "endOffset": 227}], "year": 2016, "abstractText": "We present a factorized compositional distributional semantics model for the representation of transitive verb constructions. Our model first produces (subject, verb) and (verb, object) vector representations based on the similarity of the nouns in the construction to each of the nouns in the vocabulary and the tendency of these nouns to take the subject and object roles of the verb. These vectors are then combined into a final (subject,verb,object) representation through simple vector operations. On two established tasks for the transitive verb construction our model outperforms recent previous work.", "creator": "LaTeX with hyperref package"}}}