{"id": "1609.07876", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation", "abstract": "We study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semi-Markov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.", "histories": [["v1", "Mon, 26 Sep 2016 07:34:24 GMT  (4941kb,D)", "http://arxiv.org/abs/1609.07876v1", "arXiv admin note: substantial text overlap witharXiv:1608.08339"]], "COMMENTS": "arXiv admin note: substantial text overlap witharXiv:1608.08339", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["taehwan kim", "jonathan keane", "weiran wang", "hao tang", "jason riggle", "gregory shakhnarovich", "diane brentari", "karen livescu"], "accepted": false, "id": "1609.07876"}, "pdf": {"name": "1609.07876.pdf", "metadata": {"source": "CRF", "title": "Lexicon-Free Fingerspelling Recognition from Video: Data, Models, and Signer Adaptation", "authors": ["Taehwan Kima", "Jonathan Keaneb", "Weiran Wanga", "Hao Tanga", "Jason Riggleb", "Gregory Shakhnarovicha", "Diane Brentarib", "Karen Livescua"], "emails": [], "sections": [{"heading": null, "text": "We are investigating the problem of finger letter recognition in American Sign Language (ASL). Finger accounting is a significant but relatively under-studied part of ASL. Finger letter recognition is challenging for a number of reasons: it involves fast, small movements that are often strongly coarticulated; it exhibits significant differences between signatories; and a lack of continuous finger spelling data has been identified. In this work, we are collecting and commenting on a new set of continuous finger spelling videos, comparing several types of identifiers, and investigating the problem of signatory variation. Our most powerful models are segmental (semiMarkov) induced random fields based on deep neural network functionality. In the signature-dependent setting, our recognition devices achieve up to 92% letter accuracy. Setting with multiple signatures is much more difficult, but with neural network adaptation we achieve up to 83% letter accuracy in this setting."}, {"heading": "1. Introduction", "text": "In fact, the fact is that most of them are able to move to another world in which they are able to find themselves."}, {"heading": "2.1 Recognizers", "text": "When developing recognition systems, we consider several considerations: firstly, although the data set is large by standards of research on sign language, it is still quite small compared to typical language datasets, which means that large models with many context-dependent units cannot be trained with our data (as our initial experiments confirmed), so we limit our attention here to \"monobook\" models, i.e. models in which each unit is a context-independent letter. Secondly, we consider the use of articulatory (phonological and phonetic) characteristic units, as there is evidence from speech recognition research that they can be useful in low-data settings [41, 66, 10]. Secondly, we want our models to be able to capture rich sign-language-specific information, such as the above-discussed dynamic aspects of finger letters; this suggests the segmental models we are looking at below. Finally, we want our models to be able to easily adapt our array of differentiators to each other, so that our networks can be formed independently of each other."}, {"heading": "2.1.1 Tandem model", "text": "The aforementioned rf\u00fc ide nlrf\u00fc eerdGne\u00fcn nvo nde nlrf\u00fc nlrf\u00fc nlrf\u00fc rf\u00fc ide rf\u00fc ide nlrf\u00fc eeirf\u00fc nlrf\u00fc hsci-eaJnlrrgne\u00fceaeFnlrh-rf\u00fc-eaeSrrf\u00fc-eSrgne\u00fce ni rf\u00fc eid nlrf\u00fc-eaJnlrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrrrrf\u00fc-eSrrlc-eSrlllc-eF\u00fc-eSrllllc-eF\u00fc-eSref\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc-eSrf\u00fc"}, {"heading": "2. Related work", "text": "In fact, it is such that most of them will be able to move to another world, in which they are able to move to another world, in which they are able to move to another world, in which they are able to move, in which they are able to move, in which they are able to move, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are able to live, in which they are living, in which they are able to live, in which they are living, in which they are living, in which they"}, {"heading": "3. Data collection and annotation", "text": "We recorded and analyzed videos from three native ASL signatories and an early learner and recorded a total of 3,684 word instances with the finger. We commented on the video by identifying the point in time of peak articulation (also known as holding, posture, or goal) for each finger letter. In total, there were 21,453 peaks. The following sections describe in detail the data collection and annotation process. The data and annotations are published to make it easier for others to study fingerspelling more comprehensively and replicate our results and compare them with our results."}, {"heading": "3.1. Video recording", "text": "Our use of \"words\" includes both real words and nonsense letter sequences, and the signatories were presented with an isolated word on a computer screen. Each session lasted 25-40 minutes, with a self-timed pause in the middle of each session. Three word lists were created and used to collect data, with the first list containing 300 names, 100 nouns and 100 non-English words. These words were selected to provide examples of many letters in the middle of each session."}, {"heading": "3.2. Annotation", "text": "Our annotation method is divided into two main parts: 1. a simple task to determine the approximate times of each peak (peak detection) and 2. a verification task to determine the exact time for each peak (peak verification); the first is designed to be extremely fast so that multiple annotator judgments can be summarized; the second is more time-consuming and is performed by a single annotator."}, {"heading": "3.2.1. Peak detection", "text": "To this end, we defined a peak as the point at which the articulators change direction to get to the next peak (i.e., where the current speed of the articulators is zero or minimal), which is typically the point at which the hand is most similar to the canonical hand shape, although the tip hand shape is often very different from the canonical one at normal speed. Two FS letters, -J- and -Z-, are not well represented in terms of spikes because they have movement. In these two FS letters, the commentators were asked to mark a peak at the point at which they could determine that it was one of these two FS letters. Peak recognition is simple and requires minimal training; commentators reported that this task was very intuitive."}, {"heading": "3.2.2. Peak verification", "text": "Finally, a more experienced bilingual learner from ASL, or an annotator specially trained in the fingerprint combination, checked the location and identity of each peak from the peak recognition level. For the verification level, we further refined the definition of the peak to the point where the hand shape configuration comes closest to the canonical hand shape for a particular FS letter. If a tip hand shape remained stable for more than one frame, each stable frame was marked. However, for detection attempts, only the original single tip frame was used. When using the recognition training data, the tip comments are used to segment each word into FS letters: The boundary between consecutive FS letters is defined as the center point between their tip frames. Appendix A provides additional details on the annotation and analysis conventions, as well as detailed definitions of the finger-spelled letters."}, {"heading": "4. Recognition methods", "text": "Our task is to take as input a video (a sequence of images) that corresponds to a finger-spelled word, as in Fig. 2, and predict the signed FS letters. This is a sequence prediction task analogous to the connected phone or word recognition, but there are some interesting sign language specific properties for the data area. As described in Fig. 3, each FS letter is represented by a short \"culmination of articulation,\" during which the movement of the hand is limited to a minimum and the hand shape comes closest to the target hand shape of the FS letter. This peak is surrounded by longer periods of movement between the current FS letter and the previous / next FS letter. Another striking feature of the sign language is the large variation between signatories."}, {"heading": "4.1. Recognizers", "text": "When developing recognition systems, we take several aspects into account: First, although the data set is large compared to previous fingerspelling data sets, it is still quite small compared to typical language data sets, which means that large models with many context-dependent units cannot be trained with our data (as our initial experiments confirmed). Therefore, we limit our attention here to \"mono-letter\" models, i.e. models where each unit is a context-independent FS letter. Second, we want our models to be able to capture detailed sign language-specific information, such as the dynamic aspects of FS letters discussed above; this suggests that the segmental models we are looking at below may be useful. Finally, we want our models to simply adapt to new strings. To enable this, all of our recognition systems use independently concerted FS letters, as described above; this suggests that we are segmenting the models below."}, {"heading": "4.1.1. Tandem model", "text": "The phonological characteristics are defined in Tab. 2, and sample frames for the values of a feature are shown in Fig. 5. The output of the classifier and the image characteristics of the image are reduced in dimensionality using PCA and then concatenated with each other. The concatenated characteristics form the observations in an HMM-based recognition system with Gaussian mixing density. We use a three-stage HMM for each (context-independent) FS letter plus one HMM each for initial and final \"silence\" (non-significant segments). In addition, we use a Bigram letter language model with Gaussian mixing density. Generally, the language model of the FS letter sequences is difficult to define or estimate as the finger letter sequence does not follow the same distribution."}, {"heading": "4.1.2. Segmental CRF", "text": "The second recognition factor is a segmental CRF (SCRF). SCRFs [76, 71] are conditional loglinear models with feature functions that can be based on segments of input images of different lengths, allowing great flexibility in defining feature functions. Formally, a sequence of images o1, o2, oT means a sequence of size k means a sequence of points 0 = q0, q1,.., qk \u2212 1, qk = T used to define time limits of segments. In other words, the i-th segment starts at a time qi \u2212 1 and ends at qi. The label of sequence q is a sequence s1, s2,..., sk. We will denote the length of the label sequences qk and segments qs: (s), the sequencing qs (s) and segments p."}, {"heading": "4.1.3. Rescoring segmental CRF", "text": "A common way to use SCRFs is to rescore a DNN classification in multiple places, and that's one way we use SCRFs here. We first use the basic detection, in this case the tandem feature HMM, to generate lattices of high-quality segmentation and labeling, and then resurrect them with our SCRFs. We use the same functions as in [12], which are described here for completeness. Some of the functions are generally tailored to sequence detection tasks, while some are specifically tailored to fingerspelling detection functions. The first set of functions measures how well the frames within a segment match the hypothetical label. To do this, we use the same DNN classifiers as in the tandem model. Let's use an FS letter and v be the value of an FS letter or linguistic feature."}, {"heading": "4.1.4. First-pass segmental CRF", "text": "One of the disadvantages of a Rescoring approach is that the quality of final editions q = q q q = q = q = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7"}, {"heading": "4.2. DNN adaptation", "text": "In the experiments, we will consider both signature-dependent and signature-independent recognition. In the latter case, the test signer is not seen in training. As we can see, there is a very large gap between signature-dependent and signature-independent recognition of our data. We will also consider the case where we only adapt the DNN classifiers, but not a sufficient amount for the formation of signature-dependent models. In this case, we will consider the adaptation of a signature-independent model to the test signature classifiers, and then use the adapted signature-independent models."}, {"heading": "5. Experimental Results", "text": "We begin by describing some of the details of hand segmentation and extraction, followed by experiments with frame-level DNN classifiers (Sec. 5.1) and sequence detectors (Sec. 5.2). Hand localization and segmentation of the hand pioneers is able to use a simple, hand-dependent model for hand capture. We have a mixture of Gaussians, phrases and the color of the hand pixels in our hand."}, {"heading": "5.1. DNN frame classification performance", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "5.2. FS-letter recognition experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.2.1. Signer-dependent recognition", "text": "Our first continuous FS letter recognition experiments are signature-dependent; that is, we train and test on the same signature, for each of four signatures. For each signature, we use a 10-fold setup: In each fold, 80% of the data is used as a training set, 10% as a development set for tuning parameters, and the remaining 10% as a final test set. We match the parameters in each fold. To make the results comparable to later adaptation results, we use 8 out of 10 folds to calculate the final test results and to determine the average letter error rate (LER) in these 8 folds, we train letter bigram models from large online dictionaries that include both English words and names. We use HTK [101] to implement the base letter error rate."}, {"heading": "5.2.2. Signer-independent recognition", "text": "In the signatory-independent environment, we would like to see FS letter sequences of a new signatory because a model is trained only on data from other signatories. For each of the four test signatories, we train models on the remaining three signatories and report on the performance of each signatory and the average across the four test signatories. For direct comparison with the signatory-dependent experiments, the performance of each test signatory is itself an average over the 8 test folds for that signatory. As shown in the first line of Tab. 3, the signatory-independent performance of the three types of detectors is quite poor, with the rescoring SCRF slightly outperforming the tandem HMM and the first-pass SCRF. Perhaps the poor performance is to be expected with such a small number of signatories."}, {"heading": "5.2.3. Signer-adapted recognition", "text": "The rest of Tab. 3 (second and third lines) reflects the combined performance of FS letter recognition achieved with the three types of models that use DNNs that have been fine-tuned using different types of match data (Ground-Truth, GT, vs. Forcedaligned, FA). On all models, the models are not re-trained with the customized DNNs, but Hyperparameter6 is tuned to 10% of the test signer's data, and the tuned models are evaluated against an invisible 10% of the test signer's remaining data; finally, we repeat this for eight ways of match and test kits, covering the 80% of the test signer's data that we do not use for the fit, and reporting the mean accuracy of the FS letters across the test sets. As shown in Tab. 3, the fit improves performance to up to 30.3% letter error rate with FS and 17.3 labels matching."}, {"heading": "5.3. Extensions and analysis", "text": "6See [11, 12, 72] for details on the tuning parameters. 5.3.1. Analysis: Could we do better if we train entirely on fit data? So far, we have considered adapting the DNNs while using sequence models (HMMs / SCRFs) that are only based on sign-independent data. In this section, we will consider alternatives to this fit setting. We will set the model to a first-pass SCRF and the adaptation data to 20% of the test signer data that is labeled with Ground Truth. In this setting, we will consider two alternative ways to use the fit data: (1) by using the adaptation data of the test signer to train both the DNNs and the sequence model from the ground up, and (2) by ignoring these sign-independent training sets (independent of the DNs) and RF-independent (but with the fit data)."}, {"heading": "5.3.2. Analysis: FS-letter vs. feature DNNs", "text": "Next, we compare the FS letter DNN classifiers and the phonological feature DNN classifiers in the context of the First Pass SCRF recognition functions. We also look at an alternative sub-letter set, in particular a number of phonetic features introduced by Keane [2] and their characteristic values in Section A, Tab. B.9. We use the First Pass SCRF either with FS letter classifiers only, phonetic feature classifiers only, FS letters + phonological feature classifiers and FS letters + phonetic feature classifiers. We do not consider the case of phonological features alone because they are not discrimatic for some FS characters. Fig. 9 shows the results of the FS letter recognition for the sign-dependent and sign-customized settings. We find that for some FS characters alone, we achieve the use of FS characters only by using FS options other than the two subcharacters (7.S)."}, {"heading": "5.3.3. Analysis: DNNs vs. CNNs", "text": "In all previous experiments, we used HOG image descriptors fed into fully networked feedback DNNs. However, it has recently become common to use Convolutionary Neural Networks (CNNs) on raw image pixels without hand-picked image descriptors, resulting in improved performance for certain visual recognition tasks (e.g., [103, 104]). In our case, we have relatively little training data compared to typical benchmark visual recognition tasks, which prepare our choice for image descriptors rather than relying on networks learning the properties of raw pixels. To test this assumption, we compare the performance of FS letters and phonological features based on DNNs classifications."}, {"heading": "5.3.4. Improving performance in the force-aligned adaptation case", "text": "This is an important setting, as it can be very difficult in practice to obtain ground truth labels at the frame level. If we only use the FS letter label sequence for the fit data, we use the signature-independent tandem recognition mechanism to obtain force-oriented frame labels. Then, we adjust the DNNs using force-oriented fit data (as previously in the FA case). Then, we adjust the match data of the test label with the adapted recognition mechanism. Finally, we adjust the DNNs again to the reoriented data. During this experiment, we do not change the detection mechanism, but only update the DNNs. With this iterative reorientation approach, we are able to improve detection accuracy by about 1.3% in FA case, as shown in Tab. 5."}, {"heading": "5.3.5. Improving performance with segmental cascades", "text": "Finally, we will consider whether we can improve the performance of our best models, the first SCRFs, by rescording their results in a second run with more powerful features. We will follow the discriminatory Segment Cascades (DSC) approach of [72], which uses a simpler first SCRF pass for grid generation and a second SCRF with more mathematically sophisticated features for rescoring. For these experiments, we will start with the most successful first SCRF pass in the above experiments, which uses FS letter DNNs and is adapted with 20% of the signatory's data with ground-level hand markings. For the second SCRF pass, we will use the first SCRF value as a feature and add two more complex features: a segmental DNN that takes an entire hypothesized segment as input and generates posterior probabilities for all FS letters."}, {"heading": "6. Conclusion", "text": "This paper addresses the problem of unrestricted letter recognition in ASL, where the FS letter sequences are not limited to a closed vocabulary, which is characterized by both the small amount of available training data and significant deviations from the values mentioned."}, {"heading": "Acknowledgements", "text": "We are grateful for the work of several assistants to comment on the data. This research was funded by a Google Faculty Award and NSF grants NSF-1433485 and NSF / BCS-1251807. Opinions expressed in this work are those of the authors and do not necessarily reflect the views of the funding agency."}, {"heading": "Appendix A. Fingerspelled handshape definitions and annotation conventions", "text": "In fact, most people who are able are able to move, to move and to move, to move, to move, to move, to move, to move, to move, to move and to move."}, {"heading": "Appendix B. Phonetic feature definitions", "text": "7These are the FS characters traditionally described as being differently oriented; there are other possibilities we have found: -X- and -Y-.8This is often abbreviated to a horizontal line representing the uppermost bar of the z."}], "references": [{"title": "How many people use ASL in the United States? Why estimates need updating", "author": ["R.E. Mitchell", "T.A. Young", "B. Bachleda", "M.A. Karchmer"], "venue": "Sign Language Studies 6 (3) ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2006}, {"title": "Towards an articulatory model of handshape: What fingerspelling tells us about the phonetics and phonology of handshape in American Sign Language", "author": ["J. Keane"], "venue": "Ph.D. thesis, University of Chicago ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "A Prosodic Model of Sign Language Phonology", "author": ["D. Brentari"], "venue": "MIT Press", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1998}, {"title": "Modelling and recognition of the linguistic components in American Sign Language", "author": ["L. Ding", "A.M. Martinez"], "venue": "Image and Vision Computing 27 (12) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Real-time articulated hand pose estimation using semi-supervised transductive regression forests", "author": ["D. Tang", "T.-H. Yu", "T.-K. Kim"], "venue": "in: ICCV", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "How the alphabet came to be used in a sign language", "author": ["C. Padden", "D.C. Gunsauls"], "venue": "Sign Language Studies 4 (1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2003}, {"title": "Native and foreign vocabulary in American Sign Language: A lexicon with multiple origins", "author": ["D. Brentari", "C. Padden"], "venue": "in: Foreign vocabulary in sign languages: A cross-linguistic investigation of word formation, Lawrence Erlbaum, Mahwah, NJ", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2001}, {"title": "Dynamic fingerspelling recognition using geometric and motion features", "author": ["P. Goh", "E.-J. Holden"], "venue": "in: ICIP", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Automatic recognition of fingerspelled words in British Sign Language", "author": ["S. Liwicki", "M. Everingham"], "venue": "in: 2nd IEEE Workshop on CVPR for Human Communicative Behavior Analysis", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Fingerspelling recognition through classification of letterto-letter transitions", "author": ["S. Ricco", "C. Tomasi"], "venue": "in: ACCV", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "American Sign Language fingerspelling recognition with phonological feature-based tandem models", "author": ["T. Kim", "K. Livescu", "G. Shakhnarovich"], "venue": "in: Proc. IEEE Workshop on Spoken Language Technology (SLT)", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Fingerspelling recognition with semi- Markov conditional random fields", "author": ["T. Kim", "G. Shakhnarovich", "K. Livescu"], "venue": "in: ICCV", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Signer-independent fingerspelling recognition with deep neural network adaptation", "author": ["T. Kim", "W. Wang", "H. Tang", "K. Livescu"], "venue": "in: ICASSP", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Continuous sign language recognition: Towards large vocabulary statistical recognition systems handling multiple signers", "author": ["O. Koller", "J. Forster", "H. Ney"], "venue": "Computer Vision and Image Understanding 141 ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic sign language analysis: A survey and the future beyond lexical meaning", "author": ["S.C. Ong", "S. Ranganath"], "venue": "IEEE transactions on pattern analysis and machine intelligence 27 (6) ", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Recognition of finger spelling of American Sign Language with artificial neural network using position/orientation sensors and data glove", "author": ["C. Oz", "M.C. Leu"], "venue": "in: 2nd International Conference on Advances in Neural Networks", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2005}, {"title": "Real-time hand-tracking with a color glove", "author": ["R. Wang", "J. Popovic"], "venue": "in: SIG- GRAPH", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2009}, {"title": "Spelling it out: Real-time ASL fingerspelling recognition", "author": ["N. Pugeault", "R. Bowden"], "venue": "in: ICCV Workshops", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Hand pose estimation and hand shape classification using multi-layered randomized decision forests", "author": ["C. Keskin", "F. K\u0131ra\u00e7", "Y.E. Kara", "L. Akarun"], "venue": "in: ECCV", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Collecting and evaluating the cuny asl corpus for research on american sign language animation", "author": ["P. Lu", "M. Huenerfauth"], "venue": "Computer Speech and Language 28 (3) ", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Histogram of 3d facets: a depth descriptor for human action and hand gesture recognition", "author": ["C. Zhang", "Y. Tian"], "venue": "Computer Vision and Image Understanding 139 ", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "American Sign Language alphabet recognition using Microsoft Kinect", "author": ["C. Dong", "M.C. Lieu", "Z. Yin"], "venue": "in: CVPR Workshops", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Purdue ASL database for the recognition of American sign language", "author": ["A.M. Martinez", "R.B. Wilbur", "R. Shay", "A.C. Kak"], "venue": "in: ICMI", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2002}, {"title": "H", "author": ["P. Dreuw", "C. Neidle", "V. Athitsos", "S. Sclaroff"], "venue": "Ney, Benchmark databases for video-based automatic sign language recognition., in: International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2008}, {"title": "The significance of facial features for automatic sign language recognition", "author": ["U. Von Agris", "M. Knorr", "K.-F. Kraiss"], "venue": "in: Proc. IEEE International Conference on Automatic Face and Gesture Recognition", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "SignSpeak - understanding", "author": ["P. Dreuw", "J. Forster", "Y. Gweth", "D. Stein", "H. Ney", "G. Martinez", "J.V. Llahi", "O. Crasborn", "E. Ormel", "W. Du", "T. Hoyoux", "J. Piater", "J.M.M. Lazaro", "M. Wheatley"], "venue": "recognition, and translation of sign languages, in: Proc. Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT)", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Large lexicon project: American sign language video corpus and sign language indexing/retrieval algorithms", "author": ["V. Athitsos", "C. Neidle", "S. Sclaroff", "J. Nash", "A. Stefan", "A. Thangali", "H. Wang", "Q. Yuan"], "venue": "in: Workshop on the Representation and Processing of Sign Languages: Corpora and Sign Language Technologies (CSLT)", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A new web interface to facilitate access to corpora: Development of the ASLLRP data access interface (DAI)", "author": ["C. Neidle", "C. Vogler"], "venue": "in: LREC Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2012}, {"title": "Sign language technologies and resources of the Dicta-Sign project", "author": ["E. Efthimiou", "S.-E. Fotinea", "T. Hanke", "J. Glauert", "R. Bowden", "A. Braffort", "C. Collet", "P. Maragos", "F. Lefebvre-Albaret"], "venue": "in: LREC Workshop on the Representation and Processing of Sign Languages: Interactions between Corpus and Lexicon", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2012}, {"title": "H", "author": ["J. Forster", "C. Schmidt", "T. Hoyoux", "O. Koller", "U. Zelle", "J.H. Piater"], "venue": "Ney, RWTH-PHOENIX-Weather: A large vocabulary sign language recognition and translation corpus., in: International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2012}, {"title": "SignStream: A tool for linguistic and computer vision research on visual-gestural language data", "author": ["C. Neidle", "S. Sclaroff", "V. Athitsos"], "venue": "Behavior Research Methods, Instruments, & Computers 33 (3) ", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2001}, {"title": "A linguistic feature vector for the visual interpretation of sign language", "author": ["R. Bowden", "D. Windridge", "T. Kadir", "A. Zisserman", "M. Brady"], "venue": "in: ECCV", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2004}, {"title": "Detecting coarticulation in sign language using conditional random fields", "author": ["R. Yang", "S. Sarkar"], "venue": "in: Proc. Intl. Conf. on Pattern Recognition (ICPR)", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2006}, {"title": "Geometric features for improving continuous appearance-based sign language recognition", "author": ["M. Zahedi", "P. Dreuw", "D. Rybach", "T. Deselaers", "H. Ney"], "venue": "in: BMVC", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2006}, {"title": "Sign language recognition using a combination of new vision based features", "author": ["M.M. Zaki", "S.I. Shaheen"], "venue": "Pattern Recognition Letters 32 (4) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2011}, {"title": "Transfer learning in sign language", "author": ["A. Farhadi", "D. Forsyth", "R. White"], "venue": "in: CVPR", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2007}, {"title": "M", "author": ["P. Dreuw", "D. Rybach", "T. Deselaers"], "venue": "Zahedi, , H. Ney, Speech recognition techniques for a sign language recognition system, in: Proc. Interspeech", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Automated extraction of signs from continuous sign language sentences using iterated conditional modes", "author": ["S. Nayak", "S. Sarkar", "B. Loeding"], "venue": "in: CVPR", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2009}, {"title": "Upper body detection and tracking in extended signing sequences", "author": ["P. Buehler", "M. Everingham", "D.P. Huttenlocher", "A. Zisserman"], "venue": "International Journal of Computer Vision 95 (2) ", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2011}, {"title": "Automatic and efficient long term arm and hand tracking for continuous sign language TV broadcasts", "author": ["T. Pfister", "J. Charles", "M. Everingham", "A. Zisserman"], "venue": "in: BMVC", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2012}, {"title": "Histograms of oriented gradients for human detection", "author": ["N. Dalal", "B. Triggs"], "venue": "in: CVPR", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2005}, {"title": "Sign language recognition using dynamic time warping and hand shape distance based on histogram of oriented gradient features", "author": ["P. Jangyodsuk", "C. Conly", "V. Athitsos"], "venue": "in: Proc. 7th International Conference on PErvasive Technologies Related to Assistive Environments", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Real-time american sign language recognition using desk and wearable computer based video", "author": ["T. Starner", "J. Weaver", "A. Pentland"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 20 (12) ", "citeRegEx": "45", "shortCiteRegEx": null, "year": 1998}, {"title": "Parallel hidden Markov models for American Sign Language recognition", "author": ["C. Vogler", "D. Metaxas"], "venue": "in: ICCV", "citeRegEx": "46", "shortCiteRegEx": null, "year": 1999}, {"title": "Exploiting phonological constraints for handshape inference in ASL video", "author": ["A. Thangali", "J.P. Nash", "S. Sclaroff", "C. Neidle"], "venue": "in: CVPR", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2011}, {"title": "Handshapes and movements: Multiple-channel ASL recognition", "author": ["C. Vogler", "D. Metaxas"], "venue": "in: Gesture Workshop", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2003}, {"title": "Product-HMMs for automatic sign language recognition", "author": ["S. Theodorakis", "A. Katsamanis", "P. Maragos"], "venue": "in: ICASSP", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2009}, {"title": "The use of context in large vocabulary speech recognition", "author": ["J. Odell"], "venue": "Ph.D. thesis, University of Cambridge ", "citeRegEx": "50", "shortCiteRegEx": null, "year": 1995}, {"title": "Moving beyond the \u2018beads-on-a-string\u2019 model of speech", "author": ["M. Ostendorf"], "venue": "in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "citeRegEx": "51", "shortCiteRegEx": null, "year": 1999}, {"title": "Subword modeling for automatic speech recognition: Past", "author": ["K. Livescu", "E. Fosler-Lussier", "F. Metze"], "venue": "present, and emerging approaches, IEEE Signal Processing Magazine 29 (6) ", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2012}, {"title": "Audiovisual Speech Processing", "author": ["G. Potamianos", "C. Neti", "J. Luettin", "I. Matthews"], "venue": "Cambridge University Press", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2015}, {"title": "A segment-based audio-visual speech recognizer: Data collection", "author": ["T.J. Hazen", "K. Saenko", "C.-H. La", "J.R. Glass"], "venue": "development, and initial experiments, in: ICMI, ACM", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2004}, {"title": "Multistream articulatory featurebased models for visual speech recognition", "author": ["K. Saenko", "K. Livescu", "J. Glass", "T. Darrell"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 31 (9) ", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2009}, {"title": "A framework for recognizing the simultaneous aspects of American Sign Language", "author": ["C. Vogler", "D. Metaxas"], "venue": "Computer Vision and Image Understanding 81 ", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2001}, {"title": "Toward scalability in ASL recognition: Breaking down signs into phonemes", "author": ["C. Vogler", "D. Metaxas"], "venue": "in: Gesture Workshop", "citeRegEx": "57", "shortCiteRegEx": null, "year": 1999}, {"title": "Advances in phoneticsbased sub-unit modeling for transcription alignment and sign language recognition", "author": ["V. Pitsikalis", "S. Theodorakis", "C. Vogler", "P. Maragos"], "venue": "in: IEEE CVPR Workshop on Gesture Recognition", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2011}, {"title": "Model-level data-driven sub-units for signs in videos of continuous sign language", "author": ["S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "in: ICASSP", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2010}, {"title": "A prosodic model of sign language phonology", "author": ["D. Brentari"], "venue": "The MIT Press", "citeRegEx": "60", "shortCiteRegEx": null, "year": 1998}, {"title": "Adapting hidden Markov models for ASL recognition by using three-dimensional computer vision methods", "author": ["C. Vogler", "D. Metaxas"], "venue": "in: IEEE International Conference on Systems, Man and Cybernetics", "citeRegEx": "61", "shortCiteRegEx": null, "year": 1997}, {"title": "Handling movement epenthesis and hand segmentation ambiguities in continuous sign language recognition using nested dynamic programming", "author": ["R. Yang", "S. Sarkar", "B. Loeding"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 32 (3) ", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2010}, {"title": "Advances in dynamic-static integration of manual cues for sign language recognition", "author": ["S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "in: The 9th International Gesture Workshop: Gesture in Embodied Communication and Human-Computer Interaction (GW)", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2011}, {"title": "Sign transition modeling and a scalable solution to continuous sign language recognition for real-world applications", "author": ["K. Li", "Z. Zhou", "C.-H. Lee"], "venue": "ACM Transactions on Accessible Computing (TACCESS) 8 (2) ", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "BoostMap: A method for efficient approximate similarity rankings", "author": ["V. Athitsos", "J. Alon", "S. Sclaroff", "G. Kollios"], "venue": "in: CVPR", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2004}, {"title": "Affine-invariant modeling of shape-appearance images applied on sign language handshape classification", "author": ["A. Roussos", "S. Theodorakis", "V. Pitsikalis", "P. Maragos"], "venue": "in: ICIP", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2010}, {"title": "Rapid signer adaptation for continuous sign language recognition using a combined approach of eigenvoices", "author": ["U. Von Agris", "C. Blomer", "K.-F. Kraiss"], "venue": "MLLR, and MAP, in: Proc. Intl. Conf. on Pattern Recognition (ICPR)", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2008}, {"title": "Deciphering gestures with layered meanings and signer adaptation", "author": ["S.C. Ong", "S. Ranganath"], "venue": "in: Proc. IEEE International Conference on Automatic Face and Gesture Recognition", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2004}, {"title": "Tandem connectionist feature extraction for conventional HMM systems", "author": ["H. Hermansky", "D.P.W. Ellis", "S. Sharma"], "venue": "in: ICASSP", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2000}, {"title": "Probabilistic and bottle-neck features for LVCSR of meetings", "author": ["F. Gr\u00e9zl", "M. Karafi\u00e1t", "S. Kont\u00e1r", "J. Cernocky"], "venue": "in: ICASSP", "citeRegEx": "70", "shortCiteRegEx": null, "year": 2007}, {"title": "Segmental CRF approach to large vocabulary continuous speech recognition", "author": ["G. Zweig", "P. Nguyen"], "venue": "in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2009}, {"title": "Discriminative segmental cascades for feature-rich phone recognition", "author": ["H. Tang", "W. Wang", "K. Gimpel", "K. Livescu"], "venue": "in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2015}, {"title": "H", "author": ["G. Zweig", "P. Nguyen", "D. Van Compernolle", "K. Demuynck", "L. Atlas", "P. Clark", "G. Sell", "M. Wang", "F. Sha"], "venue": "Hermansky, et al., Speech recognition with segmental conditional random fields: A summary of the jhu clsp 2010 summer workshop, in: ICASSP", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2011}, {"title": "Efficient segmental conditional random fields for phone recognition", "author": ["Y. He", "E. Fosler-Lussier"], "venue": "in: Proc. Interspeech", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2012}, {"title": "Semi-Markov conditional random fields for information extraction", "author": ["S. Sarawagi", "W.W. Cohen"], "venue": "in: NIPS", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2004}, {"title": "Activity recognition and abnormality detection with the switching hidden semi-Markov model", "author": ["T.V. Duong", "H.H. Bui", "D.Q. Phung", "S. Venkatesh"], "venue": "in: CVPR", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2005}, {"title": "Sign language spotting based on semi- Markov conditional random field", "author": ["S.-S. Cho", "H.-D. Yang", "S.-W. Lee"], "venue": "in: Workshop on the Applications of Computer Vision", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2009}, {"title": "P", "author": ["H. Sloetjes"], "venue": "Wittenburg, Annotation by category: ELAN and ISO DCR., in: International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2008}, {"title": "Coarticulation in ASL fingerspelling", "author": ["J. Keane", "D. Brentari", "J. Riggle"], "venue": "in: Annual Meeting of the North East Linguistic Society", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2012}, {"title": "Handshape and coarticulation in ASL fingerspelling", "author": ["J. Keane", "D. Brentari", "J. Riggle"], "venue": "conference presentation, linguistic Society of America 2012 Annual Meeting ", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2012}, {"title": "Dispelling prescriptive rules in ASL fingerspelling: the case of -E", "author": ["J. Keane", "D. Brentari", "J. Riggle"], "venue": "poster, theoretical Issues in Sign Language Research 11; London, UK ", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2013}, {"title": "A", "author": ["S. St\u00fcker", "F. Metze", "T. Schultz"], "venue": "Waibel, Integrating multilingual articulatory features into speech recognition., in: Proc. Interspeech", "citeRegEx": "85", "shortCiteRegEx": null, "year": 2003}, {"title": "An articulatory feature-based tandem approach and factored observation modeling", "author": ["O. \u00c7etin", "A. Kantor", "S. King", "C. Bartels", "M. Magimai-doss", "J. Frankel", "K. Livescu"], "venue": "in: ICASSP", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2007}, {"title": "CSR-III text", "author": ["D. Graff", "R. Rosenfeld", "D. Paul"], "venue": "http://http://www.ldc. upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC95T6 ", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1995}, {"title": "Deep segmental neural networks for speech recognition", "author": ["O. Abdel-Hamid", "L. Deng", "D. Yu", "H. Jiang"], "venue": "in: Proc. Interspeech", "citeRegEx": "88", "shortCiteRegEx": null, "year": 2013}, {"title": "Segmental conditional random fields with deep neural networks as acoustic models for first-pass word recognition", "author": ["Y. He", "E. Fosler-Lussier"], "venue": "in: Proc. Interspeech", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2015}, {"title": "Speaker adaptation of context dependent deep neural networks", "author": ["H. Liao"], "venue": "in: ICASSP", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2013}, {"title": "Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code", "author": ["O. Abdel-Hamid", "H. Jiang"], "venue": "in: ICASSP", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning hidden unit contributions for unsupervised speaker adaptation of neural network acoustic models", "author": ["P. Swietojanski", "S. Renals"], "venue": "in: Proc. IEEE Workshop on Spoken Language Technology (SLT)", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker dependent bottleneck layer training for speaker adaptation in automatic speech recognition", "author": ["R. Doddipatla", "M. Hasan", "T. Hain"], "venue": "in: Proc. Interspeech", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker-adaptation for hybrid HMM-ANN continuous speech recognition system", "author": ["J. Neto", "L. Almeida", "M. Hochberg", "C. Martins", "L. Nunes", "S. Renals", "T. Robinson"], "venue": "in: Proc. Eurospeech", "citeRegEx": "94", "shortCiteRegEx": null, "year": 1995}, {"title": "Adaptation of context-dependent deep neural networks for automatic speech recognition", "author": ["K. Yao", "D. Yu", "F. Seide", "H. Su", "L. Deng", "Y. Gong"], "venue": "in: Proc. IEEE Workshop on Spoken Language Technology (SLT)", "citeRegEx": "95", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparison of discriminative input and output transformations for speaker adaptation in the hybrid NN/HMM systems", "author": ["B. Li", "K.C. Sim"], "venue": "in: Proc. Interspeech", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2010}, {"title": "Rapid object detection using a boosted cascade of simple features", "author": ["P. Viola", "M.J. Jones"], "venue": "in: CVPR", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2001}, {"title": "On rectified linear units for speech processing", "author": ["M.D. Zeiler", "M. Ranzato", "R. Monga", "M. Mao", "K. Yang", "Q.V. Le", "P. Nguyen", "A. Senior", "V. Vanhoucke", "J. Dean", "G.E. Hinton"], "venue": "in: ICASSP", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2013}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R.R. Salakhutdinov"], "venue": "Journal of Machine Learing Research 15 ", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2014}, {"title": "SRILM at sixteen: update and outlook", "author": ["A. Stolcke", "J. Zheng", "W. Wang", "V. Abrash"], "venue": "in: Proc. IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2011}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in: Advances in neural information processing systems", "citeRegEx": "103", "shortCiteRegEx": null, "year": 2012}, {"title": "keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras ", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In the US, there are about 350,000\u2013500,000 people for whom American Sign Language (ASL) is the primary language [1].", "startOffset": 112, "endOffset": 115}, {"referenceID": 1, "context": "Reproduced from [2].", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "Sign language handshape has its own phonology that has been studied and enjoys a broadly agreed-upon understanding relative to the other manual parameters of movement and place of articulation [3].", "startOffset": 193, "endOffset": 196}, {"referenceID": 3, "context": "Recent linguistic work on sign language phonology has developed approaches based on articulatory features, related to motions of parts of the hand [4, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 1, "context": "Recent linguistic work on sign language phonology has developed approaches based on articulatory features, related to motions of parts of the hand [4, 2].", "startOffset": 147, "endOffset": 153}, {"referenceID": 4, "context": "At the same time, computer vision research has studied pose estimation and tracking of hands [5], but usually not in the context of a grammar that constrains the motion.", "startOffset": 93, "endOffset": 96}, {"referenceID": 5, "context": "Overall, fingerspelling comprises 12-35% of ASL [6], depending on the context, and includes 72% of the handshapes used in ASL [7].", "startOffset": 48, "endOffset": 51}, {"referenceID": 6, "context": "Overall, fingerspelling comprises 12-35% of ASL [6], depending on the context, and includes 72% of the handshapes used in ASL [7].", "startOffset": 126, "endOffset": 129}, {"referenceID": 38, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [41, 66, 10].", "startOffset": 183, "endOffset": 195}, {"referenceID": 63, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [41, 66, 10].", "startOffset": 183, "endOffset": 195}, {"referenceID": 9, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, as there is evidence from speech recognition research that these may be useful in lowdata settings [41, 66, 10].", "startOffset": 183, "endOffset": 195}, {"referenceID": 20, "context": "The first recognizer we consider is based on the popular tandem approach to speech recognition [21].", "startOffset": 95, "endOffset": 99}, {"referenceID": 7, "context": "ditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies, or signer-dependent applications [8, 9, 10] (see Section 2 for more on related work).", "startOffset": 126, "endOffset": 136}, {"referenceID": 8, "context": "ditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies, or signer-dependent applications [8, 9, 10] (see Section 2 for more on related work).", "startOffset": 126, "endOffset": 136}, {"referenceID": 9, "context": "ditions such as careful articulation, isolated signs, restricted (20-100 word) vocabularies, or signer-dependent applications [8, 9, 10] (see Section 2 for more on related work).", "startOffset": 126, "endOffset": 136}, {"referenceID": 13, "context": "[14] and Ong and Ranganath [15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[14] and Ong and Ranganath [15].", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 16, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 17, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 18, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 19, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 20, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 21, "context": "A great deal of effort has been aimed at exploiting specialized equipment such as depth sensors, motion capture, data gloves, or colored gloves in video [16, 17, 18, 19, 20, 21, 22].", "startOffset": 153, "endOffset": 181}, {"referenceID": 22, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 23, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 24, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 25, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 26, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 27, "context": "A number of sign language video corpora have been collected [23, 24, 25, 26, 27, 28].", "startOffset": 60, "endOffset": 84}, {"referenceID": 28, "context": "Some of the largest data collection efforts have been for European languages, such as the Dicta-Sign [29] and SignSpeak [30] projects.", "startOffset": 101, "endOffset": 105}, {"referenceID": 29, "context": "Some of the largest data collection efforts have been for European languages, such as the Dicta-Sign [29] and SignSpeak [30] projects.", "startOffset": 120, "endOffset": 124}, {"referenceID": 26, "context": "For American Sign Language, the American Sign Language Lexicon Video Dataset (ASLLVD) [27, 28, 31] includes recordings of almost 3,000 isolated signs and can be searched via a queryby-example interface.", "startOffset": 86, "endOffset": 98}, {"referenceID": 27, "context": "For American Sign Language, the American Sign Language Lexicon Video Dataset (ASLLVD) [27, 28, 31] includes recordings of almost 3,000 isolated signs and can be searched via a queryby-example interface.", "startOffset": 86, "endOffset": 98}, {"referenceID": 30, "context": "The National Center for Sign Language and Gesture Resources (NCSLGR) Corpus includes videos of continuous ASL signing, with over 10,000 sign tokens including about 1,500 fingerspelling sequences, annotated using the SignStream linguistic annotation tool [32, 33].", "startOffset": 254, "endOffset": 262}, {"referenceID": 31, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 135, "endOffset": 151}, {"referenceID": 32, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 135, "endOffset": 151}, {"referenceID": 33, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 135, "endOffset": 151}, {"referenceID": 34, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 135, "endOffset": 151}, {"referenceID": 35, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 200, "endOffset": 215}, {"referenceID": 36, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 200, "endOffset": 215}, {"referenceID": 37, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 200, "endOffset": 215}, {"referenceID": 3, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 200, "endOffset": 215}, {"referenceID": 38, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 233, "endOffset": 241}, {"referenceID": 39, "context": "Prior work has used a variety of visual features, including ones based on estimated position, shape and movement of the hands and head [34, 35, 36, 37], sometimes combined with appearance descriptors [38, 39, 40, 4] and color models [41, 42].", "startOffset": 233, "endOffset": 241}, {"referenceID": 40, "context": "In this work, we are aiming at relatively small motions that are difficult to track a priori, and therefore begin with general image appearance features based on histograms of oriented gradients (HOG) [43], which have also been used in other recent sign language recognition", "startOffset": 201, "endOffset": 205}, {"referenceID": 10, "context": "2Parts of this work have appeared in our conference papers [11, 12, 13].", "startOffset": 59, "endOffset": 71}, {"referenceID": 11, "context": "2Parts of this work have appeared in our conference papers [11, 12, 13].", "startOffset": 59, "endOffset": 71}, {"referenceID": 12, "context": "2Parts of this work have appeared in our conference papers [11, 12, 13].", "startOffset": 59, "endOffset": 71}, {"referenceID": 13, "context": "work [14, 44].", "startOffset": 5, "endOffset": 13}, {"referenceID": 41, "context": "work [14, 44].", "startOffset": 5, "endOffset": 13}, {"referenceID": 42, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 43, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 36, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 13, "context": "Much prior work has used hidden Markov model (HMM)-based approaches [45, 46, 39, 14], and this is the starting point for our work as well.", "startOffset": 68, "endOffset": 84}, {"referenceID": 36, "context": "Ney and colleagues have shown that it is possible to borrow many of the standard HMM-based techniques from automatic speech recognition to obtain good performance on naturalistic German Sign Language videos [39, 14].", "startOffset": 207, "endOffset": 215}, {"referenceID": 13, "context": "Ney and colleagues have shown that it is possible to borrow many of the standard HMM-based techniques from automatic speech recognition to obtain good performance on naturalistic German Sign Language videos [39, 14].", "startOffset": 207, "endOffset": 215}, {"referenceID": 32, "context": "In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].", "startOffset": 66, "endOffset": 70}, {"referenceID": 44, "context": "In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].", "startOffset": 114, "endOffset": 126}, {"referenceID": 45, "context": "In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].", "startOffset": 114, "endOffset": 126}, {"referenceID": 46, "context": "In addition, there have been efforts involving conditional models [35] and more complex (non-linear-chain) models [47, 48, 49].", "startOffset": 114, "endOffset": 126}, {"referenceID": 47, "context": "For acoustic speech, the most commonly used unit is the context-dependent phoneme [50], although other choices such as syllables, articulatory features, and automatically learned units have also been considered [51, 52].", "startOffset": 82, "endOffset": 86}, {"referenceID": 48, "context": "For acoustic speech, the most commonly used unit is the context-dependent phoneme [50], although other choices such as syllables, articulatory features, and automatically learned units have also been considered [51, 52].", "startOffset": 211, "endOffset": 219}, {"referenceID": 49, "context": "For acoustic speech, the most commonly used unit is the context-dependent phoneme [50], although other choices such as syllables, articulatory features, and automatically learned units have also been considered [51, 52].", "startOffset": 211, "endOffset": 219}, {"referenceID": 50, "context": "As the research community started to consider visual speech recognition (\u201clipreading\u201d), analogous units have been explored: visemes, articulatory features, and automatic clusters [53, 54, 55].", "startOffset": 179, "endOffset": 191}, {"referenceID": 51, "context": "As the research community started to consider visual speech recognition (\u201clipreading\u201d), analogous units have been explored: visemes, articulatory features, and automatic clusters [53, 54, 55].", "startOffset": 179, "endOffset": 191}, {"referenceID": 52, "context": "As the research community started to consider visual speech recognition (\u201clipreading\u201d), analogous units have been explored: visemes, articulatory features, and automatic clusters [53, 54, 55].", "startOffset": 179, "endOffset": 191}, {"referenceID": 31, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 3, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 44, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 45, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 53, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 54, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 43, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 55, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 56, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 2, "endOffset": 37}, {"referenceID": 57, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 1, "context": ", [34, 4, 47, 48, 56, 57, 46, 58, 59], and multiple systems of phonological and phonetic features have been developed by linguists [60, 2].", "startOffset": 131, "endOffset": 138}, {"referenceID": 58, "context": "One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].", "startOffset": 228, "endOffset": 244}, {"referenceID": 59, "context": "One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].", "startOffset": 228, "endOffset": 244}, {"referenceID": 60, "context": "One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].", "startOffset": 228, "endOffset": 244}, {"referenceID": 61, "context": "One of the unique aspects of sign language is that transitional movements occupy a larger portion of the signal than steady states, and some researchers have developed approaches for explicitly modeling the transitions as units [61, 62, 63, 64].", "startOffset": 228, "endOffset": 244}, {"referenceID": 62, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 108, "endOffset": 124}, {"referenceID": 63, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 108, "endOffset": 124}, {"referenceID": 44, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 108, "endOffset": 124}, {"referenceID": 17, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 108, "endOffset": 124}, {"referenceID": 7, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 165, "endOffset": 175}, {"referenceID": 8, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 165, "endOffset": 175}, {"referenceID": 9, "context": "A subset of ASL recognition work has focused specifically on fingerspelling and/or handshape classification [65, 66, 47, 18] and fingerspelling sequence recognition [8, 9, 10].", "startOffset": 165, "endOffset": 175}, {"referenceID": 13, "context": "The problem of signer adaptation has been addressed in prior work, using techniques borrowed from speaker adaptation for speech recognition, such as maximum likelihood linear regression and maximum a posteriori estimation [14, 67, 68], and to explicitly study the contrast between signer-dependent and multi-signer fingerspelling recognition.", "startOffset": 222, "endOffset": 234}, {"referenceID": 64, "context": "The problem of signer adaptation has been addressed in prior work, using techniques borrowed from speaker adaptation for speech recognition, such as maximum likelihood linear regression and maximum a posteriori estimation [14, 67, 68], and to explicitly study the contrast between signer-dependent and multi-signer fingerspelling recognition.", "startOffset": 222, "endOffset": 234}, {"referenceID": 65, "context": "The problem of signer adaptation has been addressed in prior work, using techniques borrowed from speaker adaptation for speech recognition, such as maximum likelihood linear regression and maximum a posteriori estimation [14, 67, 68], and to explicitly study the contrast between signer-dependent and multi-signer fingerspelling recognition.", "startOffset": 222, "endOffset": 234}, {"referenceID": 66, "context": "Our HMM baselines are tandem models, analogous to ones developed for speech recognition [69, 70].", "startOffset": 88, "endOffset": 96}, {"referenceID": 67, "context": "Our HMM baselines are tandem models, analogous to ones developed for speech recognition [69, 70].", "startOffset": 88, "endOffset": 96}, {"referenceID": 68, "context": "The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].", "startOffset": 176, "endOffset": 196}, {"referenceID": 69, "context": "The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].", "startOffset": 176, "endOffset": 196}, {"referenceID": 70, "context": "The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].", "startOffset": 176, "endOffset": 196}, {"referenceID": 71, "context": "The segmental models we propose are related to segmental conditional random fields (SCRFs) and their variants, which have now been applied fairly widely for speech recognition [71, 72, 73, 74, 75].", "startOffset": 176, "endOffset": 196}, {"referenceID": 72, "context": "In natural language processing, semi-Markov CRFs have been used for named entity recognition [76], where the labeling is binary.", "startOffset": 93, "endOffset": 97}, {"referenceID": 73, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [77, 78] with a small set of possible activities to choose from, including work on spotting and recognition of a small vocabulary of (non-fingerspelled) signs in sign language video [79] or instrumented data capture [80].", "startOffset": 120, "endOffset": 128}, {"referenceID": 74, "context": "Finally, segmental models have been applied to vision tasks such as classification and segmentation of action sequences [77, 78] with a small set of possible activities to choose from, including work on spotting and recognition of a small vocabulary of (non-fingerspelled) signs in sign language video [79] or instrumented data capture [80].", "startOffset": 302, "endOffset": 306}, {"referenceID": 68, "context": "In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [71, 74].", "startOffset": 245, "endOffset": 253}, {"referenceID": 70, "context": "In prior speech recognition work, this computational difficulty has often been addressed by adopting a lattice rescoring approach, where a frame-based model such as an HMM system generates first-pass lattices and a segmental model rescores them [71, 74].", "startOffset": 245, "endOffset": 253}, {"referenceID": 69, "context": "We compare this approach to an efficient first-pass segmental model [72].", "startOffset": 68, "endOffset": 72}, {"referenceID": 1, "context": "A full listing of the word lists can be found in [2].", "startOffset": 49, "endOffset": 52}, {"referenceID": 75, "context": "For purposes of annotation, the video files were processed with FFMPEG to deinterlace, crop, resize, and reencode them for compatibility with the ELAN annotation software [81].", "startOffset": 171, "endOffset": 175}, {"referenceID": 76, "context": "Several studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.", "startOffset": 16, "endOffset": 31}, {"referenceID": 77, "context": "Several studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.", "startOffset": 16, "endOffset": 31}, {"referenceID": 78, "context": "Several studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.", "startOffset": 16, "endOffset": 31}, {"referenceID": 1, "context": "Several studies [82, 83, 84, 2] have been conducted looking at how frequently FSletters are realized canonically (i.", "startOffset": 16, "endOffset": 31}, {"referenceID": 77, "context": ", -C-), some almost never (-D-), while others have 75%\u2013 85% ( -E- or -O- respectively) canonical peak handshape realizations [83].", "startOffset": 125, "endOffset": 129}, {"referenceID": 1, "context": "For example, the pinky extension property of FS-letters has been shown to spread from peak handshapes that have an extended pinky to the ones before and after [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 79, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, and there is evidence from speech recognition research that these may be useful in low-data settings [85, 86].", "startOffset": 185, "endOffset": 193}, {"referenceID": 80, "context": "We also consider the use of articulatory (phonological and phonetic) feature units, and there is evidence from speech recognition research that these may be useful in low-data settings [85, 86].", "startOffset": 185, "endOffset": 193}, {"referenceID": 66, "context": "Tandem model The first recognizer we consider is a fairly typical tandem model [69, 70].", "startOffset": 79, "endOffset": 87}, {"referenceID": 67, "context": "Tandem model The first recognizer we consider is a fairly typical tandem model [69, 70].", "startOffset": 79, "endOffset": 87}, {"referenceID": 81, "context": "For this work, the language model is trained using ARPA CSR-III text, which includes English words and names [87].", "startOffset": 109, "endOffset": 113}, {"referenceID": 2, "context": "Table 2: Definition and possible values for phonological features based on [3].", "startOffset": 75, "endOffset": 78}, {"referenceID": 2, "context": "For detailed descriptions, see [3].", "startOffset": 31, "endOffset": 34}, {"referenceID": 72, "context": "SCRFs [76, 71] are conditional log-linear models with feature functions that can be based on variable-length segments", "startOffset": 6, "endOffset": 14}, {"referenceID": 68, "context": "SCRFs [76, 71] are conditional log-linear models with feature functions that can be based on variable-length segments", "startOffset": 6, "endOffset": 14}, {"referenceID": 11, "context": "We use the same feature functions as in [12], described here for completeness.", "startOffset": 40, "endOffset": 44}, {"referenceID": 82, "context": "These are similar to features used in prior work on SCRFs for ASR with DNN-based feature functions [88, 89, 72].", "startOffset": 99, "endOffset": 111}, {"referenceID": 83, "context": "These are similar to features used in prior work on SCRFs for ASR with DNN-based feature functions [88, 89, 72].", "startOffset": 99, "endOffset": 111}, {"referenceID": 69, "context": "These are similar to features used in prior work on SCRFs for ASR with DNN-based feature functions [88, 89, 72].", "startOffset": 99, "endOffset": 111}, {"referenceID": 68, "context": "To take advantage of the already high-quality baseline that generated the lattices, we use a baseline feature like the one in [71], which is based on the 1-best output from the baseline tandem recognizer.", "startOffset": 126, "endOffset": 130}, {"referenceID": 69, "context": "[72], and the same feature functions as in [72], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias.", "startOffset": 0, "endOffset": 4}, {"referenceID": 69, "context": "[72], and the same feature functions as in [72], namely average DNN outputs over each segment, samples of DNN outputs within the segment, boundaries of DNN outputs in each segment, duration and bias.", "startOffset": 43, "endOffset": 47}, {"referenceID": 84, "context": ", [90, 91, 92, 93]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 85, "context": ", [90, 91, 92, 93]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 86, "context": ", [90, 91, 92, 93]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 87, "context": ", [90, 91, 92, 93]).", "startOffset": 2, "endOffset": 18}, {"referenceID": 88, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [94, 95, 96].", "startOffset": 96, "endOffset": 108}, {"referenceID": 89, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [94, 95, 96].", "startOffset": 96, "endOffset": 108}, {"referenceID": 90, "context": "Two of the approaches are based on linear input networks (LIN) and linear output networks (LON) [94, 95, 96].", "startOffset": 96, "endOffset": 108}, {"referenceID": 8, "context": "Hand localization and segmentation As in prior work [9, 11, 12], we used a simple signer-dependent model for hand detection.", "startOffset": 52, "endOffset": 63}, {"referenceID": 10, "context": "Hand localization and segmentation As in prior work [9, 11, 12], we used a simple signer-dependent model for hand detection.", "startOffset": 52, "endOffset": 63}, {"referenceID": 11, "context": "Hand localization and segmentation As in prior work [9, 11, 12], we used a simple signer-dependent model for hand detection.", "startOffset": 52, "endOffset": 63}, {"referenceID": 91, "context": "First, we suppress pixels that fall within regions detected as faces by the Viola-Jones face detector [97], since these tend to be false positives.", "startOffset": 102, "endOffset": 106}, {"referenceID": 40, "context": "Handshape descriptors We use histograms of oriented gradients (HOG [43]) as the visual descriptor (feature vector) for a given hand region.", "startOffset": 67, "endOffset": 71}, {"referenceID": 92, "context": "The DNNs have three hidden layers, each with 3000 ReLUs [98].", "startOffset": 56, "endOffset": 60}, {"referenceID": 93, "context": "Network learning is done with cross-entropy training with a weight decay penalty of 10\u22125, via stochastic gradient descent (SGD) over 100-sample minibatches for up to 30 epochs, with dropout [99] at a rate of 0.", "startOffset": 190, "endOffset": 194}, {"referenceID": 94, "context": "We use HTK [101] to implement the baseline HMM-based recognizers and SRILM [102] to train the language models.", "startOffset": 75, "endOffset": 80}, {"referenceID": 10, "context": "6See [11, 12, 72] for details of the tuning parameters.", "startOffset": 5, "endOffset": 17}, {"referenceID": 11, "context": "6See [11, 12, 72] for details of the tuning parameters.", "startOffset": 5, "endOffset": 17}, {"referenceID": 69, "context": "6See [11, 12, 72] for details of the tuning parameters.", "startOffset": 5, "endOffset": 17}, {"referenceID": 1, "context": "We also consider an alternative sub-letter feature set, in particular a set of phonetic features introduced by Keane [2], whose feature values are listed in Section Appendix A, Tab.", "startOffset": 117, "endOffset": 120}, {"referenceID": 10, "context": "In contrast, in earlier work [11] we found that phonological features outperform FSletters in the tandem HMM.", "startOffset": 29, "endOffset": 33}, {"referenceID": 95, "context": ", [103, 104]).", "startOffset": 2, "endOffset": 12}, {"referenceID": 96, "context": "We implemented the CNNs using Keras [105] with Theano [106].", "startOffset": 36, "endOffset": 41}, {"referenceID": 2, "context": "We compare: FS-letter only, phonetic features only, FS-letters + phonological features [3] and FS-letters + phonetic features [2].", "startOffset": 87, "endOffset": 90}, {"referenceID": 1, "context": "We compare: FS-letter only, phonetic features only, FS-letters + phonological features [3] and FS-letters + phonetic features [2].", "startOffset": 126, "endOffset": 129}, {"referenceID": 69, "context": "We follow the discriminative segmental cascades (DSC) approach of [72], where a simpler first-pass SCRF is used for lattice generation and a second SCRF, with more computationally demanding features, is used for rescoring.", "startOffset": 66, "endOffset": 70}, {"referenceID": 1, "context": "9: Phonetic features [2].", "startOffset": 21, "endOffset": 24}], "year": 2016, "abstractText": "We study the problem of recognizing video sequences of fingerspelled letters in American Sign Language (ASL). Fingerspelling comprises a significant but relatively understudied part of ASL. Recognizing fingerspelling is challenging for a number of reasons: It involves quick, small motions that are often highly coarticulated; it exhibits significant variation between signers; and there has been a dearth of continuous fingerspelling data collected. In this work we collect and annotate a new data set of continuous fingerspelling videos, compare several types of recognizers, and explore the problem of signer variation. Our best-performing models are segmental (semiMarkov) conditional random fields using deep neural network-based features. In the signer-dependent setting, our recognizers achieve up to about 92% letter accuracy. The multi-signer setting is much more challenging, but with neural network adaptation we achieve up to 83% letter accuracies in this setting.", "creator": "LaTeX with hyperref package"}}}