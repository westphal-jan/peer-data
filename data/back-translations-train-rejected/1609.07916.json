{"id": "1609.07916", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Deep Structured Features for Semantic Segmentation", "abstract": "We propose a highly structured neural network architecture for semantic segmentation of images that combines i) a Haar wavelet-based tree-like convolutional neural network (CNN), ii) a random layer realizing a radial basis function kernel approximation, and iii) a linear classifier. While stages i) and ii) are completely pre-specified, only the linear classifier is learned from data. Thanks to its high degree of structure, our architecture has a very small memory footprint and thus fits onto low-power embedded and mobile platforms. We apply the proposed architecture to outdoor scene and aerial image semantic segmentation and show that the accuracy of our architecture is competitive with conventional pixel classification CNNs. Furthermore, we demonstrate that the proposed architecture is data efficient in the sense of matching the accuracy of pixel classification CNNs when trained on a much smaller data set.", "histories": [["v1", "Mon, 26 Sep 2016 10:33:13 GMT  (1925kb,D)", "http://arxiv.org/abs/1609.07916v1", "10 pages, 2 figures"], ["v2", "Mon, 13 Mar 2017 16:23:14 GMT  (3513kb,D)", "http://arxiv.org/abs/1609.07916v2", "5 pages, 2 figures"], ["v3", "Fri, 16 Jun 2017 15:12:49 GMT  (3513kb,D)", "http://arxiv.org/abs/1609.07916v3", "EUSIPCO 2017, 5 pages, 2 figures"]], "COMMENTS": "10 pages, 2 figures", "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["michael tschannen", "lukas cavigelli", "fabian mentzer", "thomas wiatowski", "luca benini"], "accepted": false, "id": "1609.07916"}, "pdf": {"name": "1609.07916.pdf", "metadata": {"source": "CRF", "title": "Deep Structured Features for Semantic Segmentation", "authors": ["Michael Tschannen", "Lukas Cavigelli", "Fabian Mentzer", "Thomas Wiatowski", "Luca Benini"], "emails": ["michaelt@nari.ee.ethz.ch", "cavigelli@iis.ee.ethz.ch", "mentzerf@student.ethz.ch", "withomas@nari.ee.ethz.ch", "lbenini@iis.ee.ethz.ch"], "sections": [{"heading": "1 Introduction", "text": "The semantic segmentation of images is an important step in many areas of computer visualization and relates to the task of identifying the semantic category of each individual pixel of an image. In recent years, it has become a popular method of semantic segmentation. In order to derive a compact label from this, most of the corresponding CNN architectures have to resort to either pixel classification (1-3) or so-called deconvolution layers (4-10), which combine the interpolation with a (possibly learned) kernel."}, {"heading": "2 Network architecture", "text": "We have set the stage by introducing our CNN architecture for semantic segmentation: the CNN we are looking at has a total depth of D + 2, with the first D layers corresponding to a tree-like CNN-based feature extractor with predefined (i.e. handmade) frame filters [12], followed by a non-linear classifier consisting of a single fully connected RBF kernel approximation layer with predefined random filters [13] and a single fully connected linear classification layer based on learned filters [14]."}, {"heading": "2.1 CNN-based feature extractor", "text": "world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-the-world-of-of-the-world-of-the-world-of-of-the-world-of-the-world-of-of-the-world-of-of-the-world-of-of-the-world-of-of-the-world-of-of-the-world-of-the-world-of-of-the-world-of-the-world-of-world-of-of-the-world-of-the-world-of-world-of-the-world-of-world-of-of-of-the-world-of-of-the-world-of-the-world-of-of-the-world-of-of-of-the-world-world-of-of-of-the-world-of-the-world-of-of-the-world-of-world-of-of-the-world-of-of-of-the-world-of-of-the-world-of-the-world-of-the-world-of-the-world-of-of-of-of-the-the-world-of-the-world-of-the-world-of-the-world-of-the-the-of-the-world-of-the-world-of-the-world-of-world-of-of-the-world-of-the-world-of-the-world-of-of-the-of-world-of-the-of-of-the-world-of-of-the-of-of-of-the-of-the-world-world-of-of-the-of-of-of-the-"}, {"heading": "2.2 RBF kernel approximation layer", "text": "In the (D + 1) -th layer of our CNN, we form the feature vectors Fi, j \u00b2 Rm to random feature vectors \u03c6 (Fi, j), where k (Fi, j, Fi, j): = exp (\u2212 \u03b3 Fi, j \u2212 Fi, j \u00b2 22) is an RBF kernel with parameter \u03b3. In large-scale classification tasks such as the one we are looking at here, such randomized feature vectors in combination with a linear classifier (see Section 2.3) typically enable much faster training and faster conclusions than non-linear kernel-based classifiers [13]. We follow the construction given in [13]: Let G, Rm \u00b2 m \u00b2 m have a predefined matrix with i.i.d. standard entries and refer to a uniform feature vector."}, {"heading": "2.3 Linear classification layer", "text": "In the last, i.e., (D + 2) nd layer of our CNN, we dealt with a linear classifier distributed over all pixels, which means that we apply the same classifier to all \u03c6 (Fi, j). Formally, we apply a matrix W + RK, add a bias vector v + RK to toyi, j: = W\u03c6 (Fi, j) + RK, (5) and determine the class name using a one-on-one-the-rest scheme as arg maxk (1,... K) (yi, j) k. \"Matrix W and vector b become by minimizing the hinge loss using a\" 2-regulation scheme using SGD [14], i.e. we learn the last layer (see Fig. 1) using a support vector machine (SVM).We proceed with some comments on the proposed architecture."}, {"heading": "3 Experiments", "text": "We evaluate the performance of the proposed CNN architecture in semantic segmentation of outdoor and aerial photos. For both segmentation tasks, we set the output dimension of the RBF approximation layer to m = 5000. For the training, we first randomly draw a subset containing 2% of all pixels in the training set, collect the corresponding feature vectors, and then perform SGD transitions over randomly mixed versions of the feature vector set obtained in this way. In addition, we adjust the RBF kernel parameter \u03b3 and the parameter \u03bb to compensate for the hinge loss and the regulation term in the SVM target (see [14, Tab. 1]). We evaluate the segmentation performance for the feature extraction in a single scale (i.e. the original image size) and in scales s's s s s s s-1, 2, 4}. Preliminary experiments have shown that the use of the full training for the GRAQ Extraction was not exactly increased with the CRD Extraction Size 48 or the CRD Extraction Size."}, {"heading": "3.1 Outdoor scene semantic segmentation", "text": "We use the Stanford Background dataset [15], which contains 715 RGB images of outdoor scenes with a size of approximately 320 x 240 pixels. Each pixel is labeled with one of eight semantic categories (\"sky,\" \"tree,\" \"road,\" \"grass,\" \"water,\" \"building,\" \"mountain,\" and \"foreground object\").An image is processed by first transforming it into the YUV color space, applying the Feature Extraction Network (possibly in multiple scales) to each color channel, and then linking the extracted feature vectors. This results in feature vectors Fi, j of dimension m = 309 for 1 scale and m = 927 for 3 scales. Running time per image for segmentation was 9.6s and 23.4s for 1 scale and 3 scales, according to accuracy. Following [1, 3], we estimate the accuracy of our cross-validation method using all pixels (1)."}, {"heading": "3.2 Aerial images semantic segmentation", "text": "We evaluate our architecture on the Vaihinger data set1 of the ISPRS 2D Semantic Labeling Contest [16], which contains 33 aerial photographs of varying sizes (average size 2494 x 2064 pixels [10]). Semantic labels at pixel level (categories: \"impermeable surface,\" \"buildings,\" \"low vegetation,\" \"car\" and \"background\") are available for 16 images, the labels of the remaining images serve as a private test set for the competition. The images have three channels (near-infrared, red and green) and come with a (correlated) digital surface model (DSM). Following [10,24] we retain images 11, 15, 28, 30 and 34 for testing and use the remaining labeled images in the dataset for training. Similar to semantic labeling of scenes, we apply the feature Extraction Network to each channel and to the standardized version of the DSM."}, {"heading": "4 Conclusion", "text": "We proposed a simple highly structured hair-wavelet-based CNN architecture for semantic segmentation and demonstrated that its accuracy competes with traditional CNNs for pixel classification in two different benchmark tasks. Replacing the pixel classification network with deconvolution layers could improve segmentation accuracy and is an interesting direction to explore in the future."}, {"heading": "Acknowledgements", "text": "The authors thank J. Ku \ufffd hne for the preparatory work for the experiments in section 3.1 and M. Lerjen for the help with computational questions. L. Cavigelli and L. Benini thank them for the support by armasuisse Science & Technology."}], "references": [{"title": "Learning hierarchical features for scene labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "IEEE Trans. Pattern Anal. Machine Intell., vol. 35, no. 8, pp. 1915\u20131929, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1915}, {"title": "Effective semantic pixel labelling with convolutional networks and conditional random fields", "author": ["S. Paisitkriangkrai", "J. Sherrah", "P. Janney", "V.-D. Hengel"], "venue": "Proc. of IEEE Conf. on Computer Vision and Pattern Recognition Workshops, 2015, pp. 36\u201343.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Recurrent convolutional neural networks for scene labeling", "author": ["P.H. Pinheiro", "R. Collobert"], "venue": "Proc. of Int. Conf. on Machine Learning, 2014, pp. 82\u201390.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proc. of IEEE Conf. on Computer Vision and Pattern Recognition, 2015, pp. 3431\u20133440.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture", "author": ["D. Eigen", "R. Fergus"], "venue": "Proc. of IEEE Int. Conf. on Computer Vision, 2015, pp. 2650\u20132658.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic image segmentation with deep convolutional nets and fully connected crfs", "author": ["C. Liang-Chieh", "G. Papandreou", "I. Kokkinos", "K. Murphy", "A. Yuille"], "venue": "Int. Conf. on Learning Representations, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Segnet: A deep convolutional encoderdecoder architecture for image segmentation", "author": ["V. Badrinarayanan", "A. Kendall", "R. Cipolla"], "venue": "arXiv:1511.00561, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Accelerating real-time embedded scene labeling with convolutional networks", "author": ["L. Cavigelli", "M. Magno", "L. Benini"], "venue": "Proc. of 52nd Annual Design Automation Conference. ACM, 2015, p. 108.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Semantic segmentation of aerial images with an ensemble of cnns", "author": ["D. Marmanis", "J.D. Wegner", "S. Galliani", "K. Schindler", "M. Datcu", "U. Stilla"], "venue": "ISPRS Annals of Photogrammetry, Remote Sensing and Spatial Information Sciences, pp. 473\u2013480, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Dense semantic labeling of sub-decimeter resolution images with convolutional neural networks", "author": ["M. Volpi", "D. Tuia"], "venue": "arXiv:1608.00775, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Energy-efficient machine learning in silicon: A communications-inspired approach", "author": ["N. Shanbhag"], "venue": "ICML 2016 Workshop on On-Device Intelligence, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Discrete deep feature extraction: A theory and new architectures", "author": ["T. Wiatowski", "M. Tschannen", "A. Stani\u0107", "P. Grohs", "H. B\u00f6lcskei"], "venue": "Proc. of Int. Conf. on Machine Learning, June 2016, pp. 2149\u20132158.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Random features for large-scale kernel machines", "author": ["A. Rahimi", "B. Recht"], "venue": "Advances in Neural Information Processing Systems, 2007, pp. 1177\u20131184.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "Proc. of COMPSTAT\u20192010, pp. 177\u2013186. Springer, 2010. 9", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Decomposing a scene into geometric and semantically consistent regions", "author": ["S. Gould", "R. Fulton", "D. Koller"], "venue": "Proc. IEEE ICCV, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "A mathematical theory of deep convolutional neural networks for feature extraction", "author": ["T. Wiatowski", "H. B\u00f6lcskei"], "venue": "arXiv:1512.06293, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Invariant scattering convolution networks", "author": ["J. Bruna", "S. Mallat"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8, pp. 1872\u20131886, 2013.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 1872}, {"title": "Group invariant scattering", "author": ["S. Mallat"], "venue": "Comm. Pure Appl. Math., vol. 65, no. 10, pp. 1331\u2013 1398, 2012.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "A wavelet tour of signal processing: The sparse way", "author": ["S. Mallat"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "author": ["B. Hariharan", "P. Arbel\u00e1ez", "R. Girshick", "J. Malik"], "venue": "Proc. IEEE CVPR, 2015, pp. 447\u2013456.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Fastfood-approximating kernel expansions in loglinear time", "author": ["Q. Le", "T. Sarl\u00f3s", "A. Smola"], "venue": "Proc. of Int. Conf. on Machine Learning, 2013.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2013}, {"title": "Recycling randomness with structure for sublinear time kernel expansions", "author": ["K. Choromanski", "V. Sindhwani"], "venue": "Proc. of Int. Conf. on Machine Learning, 2016, pp. 2502\u20132510.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2016}, {"title": "Fully convolutional networks for dense semantic labelling of high-resolution aerial imagery", "author": ["J. Sherrah"], "venue": "arXiv:1606.02585, 2016.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning both weights and connections for efficient neural network", "author": ["S. Han", "J. Pool", "J. Tran", "W. Dally"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1135\u2013 1143.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "The DGPF-test on digital airborne camera evaluation\u2013overview and test design", "author": ["M. Cramer"], "venue": "Photogrammetrie-Fernerkundung-Geoinformation, vol. 2010, no. 2, pp. 73\u201382, 2010.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2010}, {"title": "Use of the stair vision library within the ISPRS 2D semantic labeling benchmark (Vaihingen)", "author": ["M. Gerke"], "venue": "Tech. Rep., University of Twente, 2015.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional neural networks for semantic labeling", "author": ["A. Lagrange", "B. Le Saux"], "venue": "Tech. Rep., Onera \u2013 The French Aerospace Lab, 2015. 10", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 107, "endOffset": 112}, {"referenceID": 1, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 107, "endOffset": 112}, {"referenceID": 2, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 107, "endOffset": 112}, {"referenceID": 3, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 4, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 5, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 6, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 7, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 8, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 9, "context": "To infer a dense labeling, most of the corresponding CNN architectures either rely on pixel classification [1\u20133] or on so-called deconvolution layers [4\u201310], which combine up-sampling, interpolation with a (possibly learned) kernel, and a non-linearity.", "startOffset": 150, "endOffset": 156}, {"referenceID": 6, "context": "layers (with the output size being equal to the size of the original image) to one (in the case of encoder-decoder architectures [7, 10]) or multiple intermediate layers of a feed-forward CNN, the output of these deconvolution layers being finally combined.", "startOffset": 129, "endOffset": 136}, {"referenceID": 9, "context": "layers (with the output size being equal to the size of the original image) to one (in the case of encoder-decoder architectures [7, 10]) or multiple intermediate layers of a feed-forward CNN, the output of these deconvolution layers being finally combined.", "startOffset": 129, "endOffset": 136}, {"referenceID": 3, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 43, "endOffset": 51}, {"referenceID": 4, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 43, "endOffset": 51}, {"referenceID": 5, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 43, "endOffset": 51}, {"referenceID": 8, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 43, "endOffset": 51}, {"referenceID": 10, "context": "Furthermore, the necessity of pre-training [4\u20136, 9], training on large labeled data sets, and parameter optimization requiring gradient back-propagation through the entire network, may hinder on-device learning [11] and applications where only a small set of labeled images is available.", "startOffset": 211, "endOffset": 215}, {"referenceID": 11, "context": "Specifically, this architecture combines a tree-like CNN-based feature extractor [12], a random layer realizing a radial basis function (RBF) kernel approximation [13], and a linear classifier [14].", "startOffset": 81, "endOffset": 85}, {"referenceID": 12, "context": "Specifically, this architecture combines a tree-like CNN-based feature extractor [12], a random layer realizing a radial basis function (RBF) kernel approximation [13], and a linear classifier [14].", "startOffset": 163, "endOffset": 167}, {"referenceID": 13, "context": "Specifically, this architecture combines a tree-like CNN-based feature extractor [12], a random layer realizing a radial basis function (RBF) kernel approximation [13], and a linear classifier [14].", "startOffset": 193, "endOffset": 197}, {"referenceID": 14, "context": "Using Haar wavelets as convolutional filters, we evaluate the architecture for semantic segmentation of two different image types, namely outdoor scene images (from the Stanford Background data set [15]) and aerial images (form the Vaihingen data set of the ISPRS 2D semantic labeling contest [16]), using identical values for almost all hyper-parameters of the architecture in both cases.", "startOffset": 198, "endOffset": 202}, {"referenceID": 0, "context": "Further experiments show that our architecture is data efficient and matches the accuracy of the CNN in [1] using a much smaller training set.", "startOffset": 104, "endOffset": 107}, {"referenceID": 11, "context": ", hand-crafted) frame filters [12], followed by a non-linear classifier composed of a single fully connected RBF kernel approximation layer with pre-specified random filters [13], and a single fully connected linear classification layer based on learned filters [14].", "startOffset": 30, "endOffset": 34}, {"referenceID": 12, "context": ", hand-crafted) frame filters [12], followed by a non-linear classifier composed of a single fully connected RBF kernel approximation layer with pre-specified random filters [13], and a single fully connected linear classification layer based on learned filters [14].", "startOffset": 174, "endOffset": 178}, {"referenceID": 13, "context": ", hand-crafted) frame filters [12], followed by a non-linear classifier composed of a single fully connected RBF kernel approximation layer with pre-specified random filters [13], and a single fully connected linear classification layer based on learned filters [14].", "startOffset": 262, "endOffset": 266}, {"referenceID": 11, "context": "1 CNN-based feature extractor We briefly review the tree-like CNN-based feature extractor presented in [12], the basis of which is a convolutional transform followed by a non-linearity and a pooling operation.", "startOffset": 103, "endOffset": 107}, {"referenceID": 15, "context": "We refer the reader to [17\u201319] for similar translation-invariant and deformation-insensitive tree-like CNNs.", "startOffset": 23, "endOffset": 30}, {"referenceID": 16, "context": "We refer the reader to [17\u201319] for similar translation-invariant and deformation-insensitive tree-like CNNs.", "startOffset": 23, "endOffset": 30}, {"referenceID": 17, "context": "We refer the reader to [17\u201319] for similar translation-invariant and deformation-insensitive tree-like CNNs.", "startOffset": 23, "endOffset": 30}, {"referenceID": 18, "context": ", separable) Haar wavelets {g\u03bb}\u03bb\u2208\u039b, sensitive to 3 directions (horizontal, vertical, and diagonal) and\u2014for each direction\u2014sensitive to 4 scales, with corresponding wavelet low-pass filter \u03c7, together satisfying the Bessel condition (1) with Bd = 1, see [20].", "startOffset": 253, "endOffset": 257}, {"referenceID": 11, "context": "non-linear deformations [12] and hence allows our architecture to deal with variation in appearance of the semantic categories.", "startOffset": 24, "endOffset": 28}, {"referenceID": 0, "context": "Finally, similarly to [1], we also employ a variant of our network that extracts features at multiple image scales {s`}`=1 \u2282 N by applying the feature extractor to multiple sub-sampled versions {f }`=1 of the input image f , i.", "startOffset": 22, "endOffset": 25}, {"referenceID": 12, "context": "3) typically allow for much faster training and inference than non-linear kernel-based classifiers [13].", "startOffset": 99, "endOffset": 103}, {"referenceID": 12, "context": "We follow the construction given in [13]: Let G \u2208 Rm\u0303\u00d7m be a prespecified matrix with i.", "startOffset": 36, "endOffset": 40}, {"referenceID": 13, "context": "The matrix W and the vector b are learned by minimizing the hinge loss with an `2-regularization term using SGD [14], i.", "startOffset": 112, "endOffset": 116}, {"referenceID": 19, "context": "First, we note that a similar concept of aggregating pixel-wise features across feature maps was proposed in [21] for simultaneous detection and segmentation.", "startOffset": 109, "endOffset": 113}, {"referenceID": 19, "context": "However, [21] requires a pre-trained detection network and end-toend training, while here we rely on pre-specified wavelet filters as well as pre-specified random filters and train the last classification layer only.", "startOffset": 9, "endOffset": 13}, {"referenceID": 7, "context": "6G operations [8].", "startOffset": 14, "endOffset": 17}, {"referenceID": 20, "context": "Furthermore, note that the evaluation of the RBF approximation layer can be accelerated from O(m\u0303m) operations toO(m\u0303 logm) operations by using a fast RBF kernel approximation such as [22,23].", "startOffset": 184, "endOffset": 191}, {"referenceID": 21, "context": "Furthermore, note that the evaluation of the RBF approximation layer can be accelerated from O(m\u0303m) operations toO(m\u0303 logm) operations by using a fast RBF kernel approximation such as [22,23].", "startOffset": 184, "endOffset": 191}, {"referenceID": 3, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 4, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 5, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 8, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 22, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 134, "endOffset": 144}, {"referenceID": 23, "context": "In contrast, many popular (pre-trained) CNN architectures such as AlexNet, VGG, or GoogLeNet, which the semantic segmentation CNNs in [4\u20136,9,24] build upon, have millions of parameters, even when truncated [25].", "startOffset": 206, "endOffset": 210}, {"referenceID": 14, "context": "1 Outdoor scene semantic segmentation We use the Stanford Background data set [15] containing 715 RGB images of outdoor scenes of size approximately 320 \u00d7 240 pixels.", "startOffset": 78, "endOffset": 82}, {"referenceID": 0, "context": "Following [1, 3], we estimate the accuracy of our method using 5-fold cross-validation (CV).", "startOffset": 10, "endOffset": 16}, {"referenceID": 2, "context": "Following [1, 3], we estimate the accuracy of our method using 5-fold cross-validation (CV).", "startOffset": 10, "endOffset": 16}, {"referenceID": 0, "context": ", the average class precision) of our CNN architecture, along with the three end-to-end trained architectures [1, 3, 8] (we refer to [1, Tab.", "startOffset": 110, "endOffset": 119}, {"referenceID": 2, "context": ", the average class precision) of our CNN architecture, along with the three end-to-end trained architectures [1, 3, 8] (we refer to [1, Tab.", "startOffset": 110, "endOffset": 119}, {"referenceID": 7, "context": ", the average class precision) of our CNN architecture, along with the three end-to-end trained architectures [1, 3, 8] (we refer to [1, Tab.", "startOffset": 110, "endOffset": 119}, {"referenceID": 0, "context": "For 1 scale, our network outperforms the pixel classification CNN from [1] but yields lower pixel and class accuracies than the CNN from [8] and the recurrent CNN from [3].", "startOffset": 71, "endOffset": 74}, {"referenceID": 7, "context": "For 1 scale, our network outperforms the pixel classification CNN from [1] but yields lower pixel and class accuracies than the CNN from [8] and the recurrent CNN from [3].", "startOffset": 137, "endOffset": 140}, {"referenceID": 2, "context": "For 1 scale, our network outperforms the pixel classification CNN from [1] but yields lower pixel and class accuracies than the CNN from [8] and the recurrent CNN from [3].", "startOffset": 168, "endOffset": 171}, {"referenceID": 0, "context": ", the gain from using multiple scales is smaller than for the CNNs from [1, 8].", "startOffset": 72, "endOffset": 78}, {"referenceID": 7, "context": ", the gain from using multiple scales is smaller than for the CNNs from [1, 8].", "startOffset": 72, "endOffset": 78}, {"referenceID": 3, "context": "We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets).", "startOffset": 37, "endOffset": 42}, {"referenceID": 4, "context": "We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets).", "startOffset": 37, "endOffset": 42}, {"referenceID": 5, "context": "We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets).", "startOffset": 37, "endOffset": 42}, {"referenceID": 6, "context": "We note that other CNN architectures [4\u20137] achieve higher accuracies in semantic segmentation of outdoor scenes (on other data sets).", "startOffset": 37, "endOffset": 42}, {"referenceID": 0, "context": "In particular, in the single scale case, it matches the pixel accuracy of the CNN from [1] using 5 times less training images.", "startOffset": 87, "endOffset": 90}, {"referenceID": 0, "context": "CNN [1] 66.", "startOffset": 4, "endOffset": 7}, {"referenceID": 7, "context": "CNN [8] 74.", "startOffset": 4, "endOffset": 7}, {"referenceID": 2, "context": "CNN (rCNN3 (\u25e6)) [3] 80.", "startOffset": 16, "endOffset": 19}, {"referenceID": 9, "context": "2 Aerial images semantic segmentation We evaluate our architecture on the Vaihingen data set1 of the ISPRS 2D semantic labeling contest [16], which contains 33 aerial images of varying size (average size 2494 \u00d7 2064 pixels [10]).", "startOffset": 223, "endOffset": 227}, {"referenceID": 9, "context": "Following [10,24], we retain images 11, 15, 28, 30, and 34 for testing and use the remaining labeled images in the data set for training.", "startOffset": 10, "endOffset": 17}, {"referenceID": 22, "context": "Following [10,24], we retain images 11, 15, 28, 30, and 34 for testing and use the remaining labeled images in the data set for training.", "startOffset": 10, "endOffset": 17}, {"referenceID": 25, "context": "Similarly as for outdoor scene semantic labeling, we apply the feature extraction network to each channel and to the normalized version of the DSM provided by [27].", "startOffset": 159, "endOffset": 163}, {"referenceID": 24, "context": "In Table 2 we report the pixel accuracy and the F1 score (averaged over classes) of our CNN architecture as well as that of two end-to-end 1The Vaihingen data set was provided by the German Society for Photogrammetry, Remote Sensing and Geoinformation (DGPF) [26]: http://www.", "startOffset": 259, "endOffset": 263}, {"referenceID": 1, "context": "CNN [2] 83.", "startOffset": 4, "endOffset": 7}, {"referenceID": 9, "context": "74 CNN-PC [10] 86.", "startOffset": 10, "endOffset": 14}, {"referenceID": 1, "context": "trained pixel classification CNNs [2, 10].", "startOffset": 34, "endOffset": 41}, {"referenceID": 9, "context": "trained pixel classification CNNs [2, 10].", "startOffset": 34, "endOffset": 41}, {"referenceID": 1, "context": "As in [2, 10] (and following the rules of the ISPRS 2D semantic labeling contest) labeling errors within a 3 pixel radius of the (true) category boundaries are excluded for the computation of the accuracy and the F1 score.", "startOffset": 6, "endOffset": 13}, {"referenceID": 9, "context": "As in [2, 10] (and following the rules of the ISPRS 2D semantic labeling contest) labeling errors within a 3 pixel radius of the (true) category boundaries are excluded for the computation of the accuracy and the F1 score.", "startOffset": 6, "endOffset": 13}, {"referenceID": 1, "context": "It can be seen that the accuracy and the average F1 score of our architecture is competitive with the pixel classification CNNs proposed in [2, 10].", "startOffset": 140, "endOffset": 147}, {"referenceID": 9, "context": "It can be seen that the accuracy and the average F1 score of our architecture is competitive with the pixel classification CNNs proposed in [2, 10].", "startOffset": 140, "endOffset": 147}, {"referenceID": 26, "context": "9 %, thus outperforming the algorithms from [28], each of which combines a pre-trained CNN with a SVM.", "startOffset": 44, "endOffset": 48}, {"referenceID": 1, "context": "We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set.", "startOffset": 37, "endOffset": 51}, {"referenceID": 8, "context": "We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set.", "startOffset": 37, "endOffset": 51}, {"referenceID": 9, "context": "We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set.", "startOffset": 37, "endOffset": 51}, {"referenceID": 22, "context": "We note that other CNN architectures [2, 9, 10, 24] achieve higher accuracies and F1 scores on the Vaihingen data set.", "startOffset": 37, "endOffset": 51}, {"referenceID": 8, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 86, "endOffset": 92}, {"referenceID": 9, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 86, "endOffset": 92}, {"referenceID": 8, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 115, "endOffset": 121}, {"referenceID": 22, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 115, "endOffset": 121}, {"referenceID": 1, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 144, "endOffset": 151}, {"referenceID": 22, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 144, "endOffset": 151}, {"referenceID": 1, "context": "Again, these architectures are all trained endto-end and rely on deconvolution layers [9,10], pre-trained networks [9,24], CRF-based refinement [2, 24], and/or additional hand-crafted features [2].", "startOffset": 193, "endOffset": 196}], "year": 2016, "abstractText": "We propose a highly structured neural network architecture for semantic segmentation of images that combines i) a Haar wavelet-based tree-like convolutional neural network (CNN), ii) a random layer realizing a radial basis function kernel approximation, and iii) a linear classifier. While stages i) and ii) are completely pre-specified, only the linear classifier is learned from data. Thanks to its high degree of structure, our architecture has a very small memory footprint and thus fits onto low-power embedded and mobile platforms. We apply the proposed architecture to outdoor scene and aerial image semantic segmentation and show that the accuracy of our architecture is competitive with conventional pixel classification CNNs. Furthermore, we demonstrate that the proposed architecture is data efficient in the sense of matching the accuracy of pixel classification CNNs when trained on a much smaller data set.", "creator": "LaTeX with hyperref package"}}}