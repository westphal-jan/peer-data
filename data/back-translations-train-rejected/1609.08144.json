{"id": "1609.08144", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "26-Sep-2016", "title": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "abstract": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system.", "histories": [["v1", "Mon, 26 Sep 2016 19:59:55 GMT  (969kb,D)", "http://arxiv.org/abs/1609.08144v1", null], ["v2", "Sat, 8 Oct 2016 19:10:41 GMT  (968kb,D)", "http://arxiv.org/abs/1609.08144v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG", "authors": ["yonghui wu", "mike schuster", "zhifeng chen", "quoc v le", "mohammad norouzi", "wolfgang macherey", "maxim krikun", "yuan cao", "qin gao", "klaus macherey", "jeff klingner", "apurva shah", "melvin johnson", "xiaobing liu", "{\\l}ukasz kaiser", "stephan gouws", "yoshikiyo kato", "taku kudo", "hideto kazawa", "keith stevens", "george kurian", "nishant patil", "wei wang", "cliff young", "jason smith", "jason riesa", "alex rudnick", "oriol vinyals", "greg corrado", "macduff hughes", "jeffrey dean"], "accepted": false, "id": "1609.08144"}, "pdf": {"name": "1609.08144.pdf", "metadata": {"source": "CRF", "title": "Google\u2019s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation", "authors": ["Yonghui Wu", "Mike Schuster", "Zhifeng Chen", "Quoc V. Le", "Mohammad Norouzi", "Wolfgang Macherey", "Maxim Krikun", "Yuan Cao", "Qin Gao", "Klaus Macherey", "Jeff Klingner", "Apurva Shah", "Melvin Johnson", "Xiaobing Liu", "\u0141ukasz Kaiser", "Stephan Gouws", "Yoshikiyo Kato", "Taku Kudo", "Hideto Kazawa", "Keith Stevens", "George Kurian", "Nishant Patil", "Wei Wang", "Cliff Young", "Jason Smith", "Jason Riesa", "Alex Rudnick", "Oriol Vinyals", "Greg Corrado", "Macduff Hughes", "Jeffrey Dean"], "emails": ["yonghui@google.com", "schuster@google.com", "zhifengc@google.com", "qvl@google.com", "mnorouzi@google.com"], "sections": [{"heading": null, "text": "Unfortunately, NMT systems are known to be expensive in terms of both training and translation conclusions - sometimes prohibitively so in the case of very large data sets and large models. Several authors have also argued that NMT systems lack robustness, especially when input sentences contain rare words. These problems have hampered the use of NMT in practical deployments and services where both accuracy and speed are critical. In this paper, we present GNMT, Google's neural machine translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoders and 8 decoder layers, using residual connections and attention connections from the decoder network to the encoder. To improve parallelism and thus reduce training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder."}, {"heading": "1 Introduction", "text": "This year, it has come to the point that it has never come as far as it has this year."}, {"heading": "2 Related Work", "text": "Statistical Machine Translation (SMT) has been the dominant translation paradigm for machine translation [3, 4, 5] for decades. Practical implementations of SMT are generally phrase-based systems (PBMT) that translate sequences of words or phrases where lengths may vary [25]. Even before the advent of direct Neural Machine Translation, neural networks were used as a component within SMT systems with some success. One of the most notable attempts was the use of a common language model for learning phrase representations [13], which, in combination with phrase-based translation, led to impressive improvements. However, this approach still uses phrase-based translation systems at its core, and therefore inherits shortcomings. Other suggested approaches to learning phrase representation [7] or learning end-to-end-end-to-end-end translation compared with neural networks ultimately provided more encouraging [23] clues to more accurate overall systems."}, {"heading": "3 Model Architecture", "text": "Our model (see Figure 1) follows the common sequence-to-sequence learning framework [39] with attention (2]. It has three components: an encoder network, a decoder network, and an attention network. The encoder transforms a source set into a list of vectors, one vector per input symbol. In light of this list of vectors, the decoder produces one symbol each at a time until the special source set symbol (EOS) is produced. The encoder and decoder are connected by an attention module that allows the decoder to focus on different regions of the source set during the decoding course.For notation, we use bold lower cases to decode vectors (e.g. v), bold uppercase letters to represent matrices (e.g. U, W), italic letters to represent sentences (e.g. V, T, sequences, Y, and lowercase letters)."}, {"heading": "3.1 Residual Connections", "text": "As mentioned above, deep stacked LSTMs often offer better accuracy compared to flatter models. Simply stacking more layers of LSTM, however, only works up to a certain number of layers beyond which the network is too slow and difficult to train, probably due to exploding and disappearing gradient problems [32, 21]. In our experience with large-scale translation tasks, simply stacked LSTM layers work well up to 4 layers, hardly with 6 layers and very poorly beyond 8 layers. Motivated by [20], we introduce residual connections between the LSTM layers in a stack (see Figure 2). Specifically, we leave LSTMi and LSTMi + 1 the i-th and (i + 1) -th LSTM layers in a stack whose parameters are Wi and Wi + 1. In the t-th step, for stacked LSTM without residual connections, we have: cit, with LSTMxi = 1mi \u2212 1 (1 \u2212 mi \u2212 xi = 1mi \u2212 mi)."}, {"heading": "3.2 Bi-directional Encoder for First Layer", "text": "For translation systems, the information required to translate certain words on the output page may appear anywhere on the source page. Often, the source side information is roughly from left to right, similar to the destination page, but depending on the language pair, the information can be distributed for a particular output word and even split into specific areas of the input page. To have the best possible context at any point in the encoder network, it makes sense to use a bidirectional RNN [35] for the encoder, which was also used in [2]. To allow for the greatest possible parallelization during the calculation (which is explained in Section 3.3), bidirectional connections are used only for the lower encoder layer - all other encoder layers are unidirectional. Figure 3 illustrates our use of bidirectional LSTMs on the bottom encoder layer. The LSTMf layer processes the source set from left to right, while the LSTMb layer works from the source right (and STMb to the output first)."}, {"heading": "3.3 Model Parallelism", "text": "Due to the complexity of our model, we use both the parallelism of the model and the parallelism of the data to speed up the training. Parallelism of the data is straightforward: we train n model replications simultaneously with a downpour SGD algorithm [12]. The n replicas all share a copy of the model parameters, with each replica updating the parameters asynchronously using a combination of Adam [24] and SGD algorithms. In our experiments, n is often at 10 o'clock. Each replica works on a mini-batch of m-sets, which are often 128 in our experiments."}, {"heading": "4 Segmentation Approaches", "text": "Neural machine translation models often operate with fixed vocabulary, even though translation is basically an open vocabulary problem (names, numbers, dates, etc.) There are two broad categories of approaches to translating words outside the vocabulary (OOV): one approach is to simply copy rare words from the source to the destination (since most rare words are names or numbers where the correct translation is only a copy), either based on the attention model [36], using an external orientation model [30], or even using a more complicated network of purposes [17]. Another broad category of approaches is the use of subordinate units, such as characters [10], mixed words / signs [27] or smarter subverbs [37]."}, {"heading": "4.1 Wordpiece Model", "text": "In the second category (sub-word units) and in the third category (sub-word units) we have the possibility to find the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence in the sequence."}, {"heading": "4.2 Mixed Word/Character Model", "text": "A second approach is the mixed-word / character model. As in a word model, we keep a fixed-size vocabulary. Unlike a traditional word model, in which OOV words are broken down into a single UNK symbol, we convert OOV words into the order of their components. The letters are preceded by special prefixes that 1) show the position of the letters in a word and 2) to distinguish them from normal in-vocabulary characters. There are three prefixes: < B >, < M > and < E > that indicate the beginning, middle, and end of the word. Suppose the word Miki is not in the vocabulary. It is pre-processed in a sequence of special characters: < B > M < M > i < M > k < E > i < E > i. The process takes place both at the source and on the target."}, {"heading": "5 Training Criteria", "text": "Faced with a dataset of parallel text containing N-input-output sequence pairs. Several recent papers that we are considering as a reward [38], we have considered a reward. (X (i), Y (i), Y (i)), Y (i)), N (1), Standard Highest Probability Training aims to maximize the sum of the reward probabilities of the ground truth output in light of the corresponding inputs, OML (\u03b8) = N (N) = 1 logPledge (Y) output sequences (I) | X (i)). The main problem with this goal is that it does not reflect the task reward function measured by the BLEU score in the translation. Moreover, this goal does not explicitly promote a ranking among erroneous output sequences - where outputs with higher BLEU scores are even more likely to achieve deficiencies within the framework of the model - as we should not make errors during training, as the other words are never observed while using the maximum code."}, {"heading": "6 Quantizable Model and Quantized Inference", "text": "One of the most important challenges in providing our Neural Translation Model to our interactive production translation service is that it is more computationally intensive in inferences, making low latency translations difficult and high volume requirements computationally expensive. Quantified conclusions using a factor of 4-6 with minimal losses in classification accuracy on the ILSVRC-12 benchmarks can be significantly reduced. [26] shows that neural network weights can only be quantified in three states, -1, 0 and + 1. However, many of these previous studies [18, 41, 26] largely focus on CNN models with relatively few levels. Deep LSTMs with long sequences represent a novel challenge in which quantification errors can be significantly increased after many rolled or deep steps."}, {"heading": "7 Decoder", "text": "We use the beam search during decoding \u03b2 to find the sequence Y that maximizes a score function s (Y, X) to fully cover the Y results (Y, X) in the face of a trained model. We introduce two important refinements to the pure maximum probability based on the beam search algorithm: a coverage penalty [40] and the length of the penalty score is added at each step to take into account the fact that we have to compare hypotheses of different lengths. We first tried to divide the results simply by length to normalize this original heuristic method by dividing them by length differentiation, with 0 < \u03b1 < 1, where the results are optimized on a development (\u03b1 = 0.6 \u2212 0.7] to be best."}, {"heading": "8 Experiments and Results", "text": "In this section, we present our experimental results on two publicly available corpora widely used as benchmarks for Neural Machine Translation Systems: WMT '14 English-French (WMT En \u2192 Fr) and English-German (WMT En \u2192 De). On these two sets of data, we evaluate GNMT models with word, character, and piece-based vocabularies. We also present the improved accuracy of our models after fine-tuning them with RL and model similarity. Our main goal with these data sets is to show the contributions of various components in our implementation, in particular the word-piece model, the refinement of RL models, and model embedding. In addition to testing on publicly available corpora, we test GNMT for Google's translation production corpora that is two to three decimal sizes larger than the WMT corpora for a given language network best comparing with the one of our 10."}, {"heading": "8.1 Datasets", "text": "We evaluate our model using the WMT En \u2192 Fr dataset, the WMT En \u2192 De dataset and many of Google's internal production datasets. On WMT En \u2192 Fr, the training set contains 36M pairs of sentences. On WMT En \u2192 De, the training set contains 5M pairs of sentences. In both cases, we use the 2014 Newstest2014 as a test set to compare with previous work [30, 36, 43]. As a development set.In addition to the WMT, we evaluate our model using some of Google's internal datasets representing a wider range of languages with distinct linguistic characteristics: English, French, English, Spanish and English and Chinese."}, {"heading": "8.2 Evaluation Metrics", "text": "In order to be comparable to previous work [39, 30, 43], we report on the symbolized BLEU score, which is calculated by the multi-bleu.pl script downloaded from the public implementation of Moses (on Github) and also used in [30]. It is well known that the BLEU score does not fully capture the quality of a translation. For this reason, we also perform side-by-side (SxS) evaluations, in which human raters evaluate and compare the quality of two translations presented side by side for a particular set of sources. Side-by-side scores range from 0 to 6, where a score of 0 means \"completely nonsensical translation\" and a score of 6 means \"perfect translation\": the meaning of the translation is perfectly consistent with the source, and the grammar is correct. \"A translation receives a score of 4 if\" the sentence retains most of the meaning of the source set, but some grammar errors in the translation may be more significant than both of the human ones. \""}, {"heading": "8.3 Training Procedure", "text": "In fact, it is that we are able to hold our own, that we are able to put ourselves at the top, and that we are able to hold our own, that we are able to hold our own, that we are able to hold our own, that we are able to put ourselves at the top, \"he said."}, {"heading": "8.4 Evaluation after Maximum Likelihood Training", "text": "The models in our house are word-based, character-based and earthquake-proof."}, {"heading": "8.5 Evaluation of RL-refined Models", "text": "The models trained in the previous section are optimized for the log probability of the next step prediction, which may not correlate well with the translation quality, as in Section 5. The results of fine-tuning the RL to the best En \u2192 Fr and En \u2192 De models are presented in Table 6, which shows that fine-tuning the models with RL can improve the BLEU values. In WMT En \u2192 Fr, model refinement improves the BLEU values by almost 1 point. In de-refinement, RL refinement slightly impairs test performance, although we note an improvement of about 0.4 BLEU points over the development table. The results shown in Table 6 are the average of 8 independent models. We also note that there is an overlap between the gains from RL refinement and decoder fine-tuning (i.e. the introduction of length normalization and the capping penalty)."}, {"heading": "8.6 Model Ensemble and Human Evaluation", "text": "We group 8 RL-refined models to obtain a state-of-the-art result of 41.16 BLEU points on the WMT En \u2192 Fr dataset. Our results are presented in Table 7.We group 8 RL-refined models to obtain a state-of-the-art result of 26.30 BLEU points on the WMT En \u2192 De dataset. Our results are presented in Table 8.Finally, in order to better understand the quality of our models and the effect of RL refinement, we performed a four-page human evaluation to compare our NMT translations with reference translations and the best phrase-based statistical machine translations. To better understand the quality of our models and the effect of RL refinement, people are asked to rate four translations with a source set. The four translations are: 1) the best phrase-based translations as downloaded from http: / / matrix.statmt.org / systems / show / 2065, 2) a combination of BLL-M3 models, which are presented as a relative set of human factors."}, {"heading": "8.7 Results on Production Data", "text": "Since the above experiments cast doubt on whether RL improves real-world translation quality or simply the BLEU metric, RL-based model refinement is not used in these experiments. Given the larger volume of training data available in the Google corpora, no drop-out is required in these experiments either. In this section, we describe our experiments with human perception of translation quality. We asked human guides to rate translations in a three-way side-by-side comparison, the three pages being: 1) translations from the production-language statistical translation system used by Google, 2) translations from our GNMT system, and 3) translations by people who are fluent in both languages. Here, Table 10 average ratings for English, French, English, Spanish and English, and Chinese are reported. All GNMT models are word game models, without modeling, and use a common source and target vocabulary with 32K."}, {"heading": "9 Conclusion", "text": "In this paper, we describe in detail the implementation of Google's Neural Machine Translation (GNMT) system, including all techniques critical to its accuracy, speed, and robustness. In the public WMT '14 translation benchmark, the translation quality of our system approaches, or even exceeds, all currently released results. More importantly, our approach spreads to much larger production datasets that contain several orders of magnitude more data to deliver high-quality translations.Our key findings are: 1) that word-to-sequence modeling effectively handles open vocabulary and the challenge of morphologically rich languages for translation quality and inference velocity; 2) that a combination of model and data parallelism can be used to effectively learn advanced sequence-to-sequence NMT models in about a week; 3) that the quantization of translation inferences is dramatically accelerated by comparing the additional length of these systems with that of standard MT systems are comparable to that of large scale MT."}, {"heading": "Acknowledgements", "text": "We would like to thank the entire Google Brain team and the Google Translate team for their fundamental contributions to this project."}], "references": [{"title": "Tensorflow: A system for large-scale machine learning", "author": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving", "M. Isard", "M. Kudlur", "J. Levenberg", "R. Monga", "S. Moore", "D.G. Murray", "B. Steiner", "P. Tucker", "V. Vasudevan", "P. Warden", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "Tech. rep., Google Brain,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "In International Conference on Learning Representations", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "A statistical approach to language translation", "author": ["P. Brown", "J. Cocke", "S.D. Pietra", "V.D. Pietra", "F. Jelinek", "R. Mercer", "P. Roossin"], "venue": "In Proceedings of the 12th Conference on Computational Linguistics - Volume 1 (Stroudsburg, PA,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1988}, {"title": "A statistical approach to machine translation", "author": ["P.F. Brown", "J. Cocke", "S.A.D. Pietra", "V.J.D. Pietra", "F. Jelinek", "J.D. Lafferty", "R.L. Mercer", "P.S. Roossin"], "venue": "Computational linguistics 16,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1990}, {"title": "The mathematics of statistical machine translation: Parameter estimation", "author": ["P.F. Brown", "V.J.D. Pietra", "S.A.D. Pietra", "R.L. Mercer"], "venue": "Comput. Linguist. 19,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1993}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Learning recursive distributed representations for holistic computation", "author": ["L. Chrisman"], "venue": "Connection Science 3,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1991}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["J. Chung", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1603.06147", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "A character-level decoder without explicit segmentation for neural machine translation", "author": ["J. Chung", "K. Cho", "Y. Bengio"], "venue": "CoRR abs/1603.06147", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Character-based neural machine translation", "author": ["M.R. Costa-Juss\u00e0", "J.A.R. Fonollosa"], "venue": "CoRR abs/1603.00810", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G.S. Corrado", "R. Monga", "K. Chen", "M. Devin", "Q.V. Le", "M.Z. Mao", "M. Ranzato", "A. Senior", "P. Tucker", "K. Yang", "A.Y. Ng"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Fast and robust neural network joint models for statistical machine translation", "author": ["J. Devlin", "R. Zbib", "Z. Huang", "T. Lamar", "R.M. Schwartz", "J. Makhoul"], "venue": "Citeseer, pp. 1370\u20131380", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Multi-task learning for multiple language translation", "author": ["D. Dong", "H. Wu", "W. He", "D. Yu", "H. Wang"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Edinburgh\u2019s phrase-based machine translation systems for WMT-14", "author": ["N. Durrani", "B. Haddow", "P. Koehn", "K. Heafield"], "venue": "In Proceedings of the Ninth Workshop on Statistical Machine Translation", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2014}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["F.A. Gers", "J. Schmidhuber", "F. Cummins"], "venue": "Neural computation 12,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2000}, {"title": "Deep learning with limited numerical precision", "author": ["S. Gupta", "A. Agrawal", "K. Gopalakrishnan", "P. Narayanan"], "venue": "CoRR abs/1502.02551", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding", "author": ["S. Han", "H. Mao", "W.J. Dally"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In IEEE Conference on Computer Vision and Pattern Recognition", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Gradient flow in recurrent nets: the difficulty of learning", "author": ["S. Hochreiter", "Y. Bengio", "P. Frasconi", "J. Schmidhuber"], "venue": "long-term dependencies,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2001}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation 9,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1997}, {"title": "Recurrent continuous translation models", "author": ["N. Kalchbrenner", "P. Blunsom"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR abs/1412.6980", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Statistical phrase-based translation", "author": ["P. Koehn", "F.J. Och", "D. Marcu"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association for Computational Linguistics", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2003}, {"title": "Ternary weight networks", "author": ["F. Li", "B. Liu"], "venue": "CoRR abs/1605.04711", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Achieving open vocabulary neural machine translation with hybrid word-character models", "author": ["M. Luong", "C.D. Manning"], "venue": "CoRR abs/1604.00788", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Multi-task sequence to sequence learning", "author": ["Luong", "M.-T", "Q.V. Le", "I. Sutskever", "O. Vinyals", "L. Kaiser"], "venue": "In International Conference on Learning Representations", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Luong", "M.-T", "H. Pham", "C.D. Manning"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2015}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Luong", "M.-T", "I. Sutskever", "Q.V. Le", "O. Vinyals", "W. Zaremba"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Reward augmented maximum likelihood for neural structured prediction", "author": ["M. Norouzi", "S. Bengio", "Z. Chen", "N. Jaitly", "M. Schuster", "Y. Wu", "D. Schuurmans"], "venue": "In Neural Information Processing Systems", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Understanding the exploding gradient problem", "author": ["R. Pascanu", "T. Mikolov", "Y. Bengio"], "venue": "CoRR abs/1211.5063", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2012}, {"title": "Sequence level training with recurrent neural networks", "author": ["M. Ranzato", "S. Chopra", "M. Auli", "W. Zaremba"], "venue": "In International Conference on Learning Representations", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Japanese and Korean voice", "author": ["M. Schuster", "K. Nakajima"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Bidirectional recurrent neural networks", "author": ["M. Schuster", "K. Paliwal"], "venue": "IEEE Transactions on Signal Processing 45,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 1997}, {"title": "On using very large target vocabulary for neural machine translation", "author": ["J. S\u00e9bastien", "C. Kyunghyun", "R. Memisevic", "Y. Bengio"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Neural machine translation of rare words with subword units", "author": ["R. Sennrich", "B. Haddow", "A. Birch"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2016}, {"title": "Minimum risk training for neural machine translation", "author": ["S. Shen", "Y. Cheng", "Z. He", "W. He", "H. Wu", "M. Sun", "Y. Liu"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "In Advances in Neural Information Processing Systems", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}, {"title": "Coverage-based neural machine translation", "author": ["Z. Tu", "Z. Lu", "Y. Liu", "X. Liu", "H. Li"], "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2016}, {"title": "Quantized convolutional neural networks for mobile devices", "author": ["J. Wu", "C. Leng", "Y. Wang", "Q. Hu", "J. Cheng"], "venue": "CoRR abs/1512.06473", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}], "referenceMentions": [{"referenceID": 36, "context": "Neural Machine Translation (NMT) [39, 2] has recently been introduced as a promising approach with the potential of addressing many shortcomings of traditional machine translation systems.", "startOffset": 33, "endOffset": 40}, {"referenceID": 1, "context": "Neural Machine Translation (NMT) [39, 2] has recently been introduced as a promising approach with the potential of addressing many shortcomings of traditional machine translation systems.", "startOffset": 33, "endOffset": 40}, {"referenceID": 1, "context": "NMT is often accompanied by an attention mechanism [2] which helps it cope effectively with long input sequences.", "startOffset": 51, "endOffset": 54}, {"referenceID": 22, "context": "An advantage of Neural Machine Translation is that it sidesteps many brittle design choices in traditional phrase-based machine translation [25].", "startOffset": 140, "endOffset": 144}, {"referenceID": 27, "context": "Though this can be addressed in principle by training a \u201ccopy model\u201d to mimic a traditional alignment model [30], or by using the attention mechanism to copy rare words [36], these approaches are both unreliable at scale, since the quality of the alignments varies across languages, and the latent alignments produced by the attention mechanism are unstable when the network is deep.", "startOffset": 108, "endOffset": 112}, {"referenceID": 33, "context": "Though this can be addressed in principle by training a \u201ccopy model\u201d to mimic a traditional alignment model [30], or by using the attention mechanism to copy rare words [36], these approaches are both unreliable at scale, since the quality of the alignments varies across languages, and the latent alignments produced by the attention mechanism are unstable when the network is deep.", "startOffset": 169, "endOffset": 173}, {"referenceID": 19, "context": "In our implementation, the recurrent networks are Long Short-Term Memory (LSTM) RNNs [22, 16].", "startOffset": 85, "endOffset": 93}, {"referenceID": 14, "context": "In our implementation, the recurrent networks are Long Short-Term Memory (LSTM) RNNs [22, 16].", "startOffset": 85, "endOffset": 93}, {"referenceID": 17, "context": "Our LSTM RNNs have 8 layers, with residual connections between layers to encourage gradient flow [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 31, "context": "To effectively deal with rare words, we use sub-word units (also known as \u201cwordpieces\u201d) [34] for inputs and outputs in our system.", "startOffset": 88, "endOffset": 92}, {"referenceID": 27, "context": "5 BLEU from a single model without an external alignment model reported in [30] and an improvement of 1.", "startOffset": 75, "endOffset": 79}, {"referenceID": 2, "context": "Statistical Machine Translation (SMT) has been the dominant translation paradigm for decades [3, 4, 5].", "startOffset": 93, "endOffset": 102}, {"referenceID": 3, "context": "Statistical Machine Translation (SMT) has been the dominant translation paradigm for decades [3, 4, 5].", "startOffset": 93, "endOffset": 102}, {"referenceID": 4, "context": "Statistical Machine Translation (SMT) has been the dominant translation paradigm for decades [3, 4, 5].", "startOffset": 93, "endOffset": 102}, {"referenceID": 22, "context": "Practical implementations of SMT are generally phrase-based systems (PBMT) which translate sequences of words or phrases where the lengths may differ [25].", "startOffset": 150, "endOffset": 154}, {"referenceID": 11, "context": "Perhaps one of the most notable attempts involved the use of a joint language model to learn phrase representations [13] which yielded an impressive improvement when combined with phrase-based translation.", "startOffset": 116, "endOffset": 120}, {"referenceID": 5, "context": "Other proposed approaches for learning phrase representations [7] or learning end-to-end translation with neural networks [23] offered encouraging hints, but ultimately delivered worse overall accuracy compared to standard phrase-based systems.", "startOffset": 62, "endOffset": 65}, {"referenceID": 20, "context": "Other proposed approaches for learning phrase representations [7] or learning end-to-end translation with neural networks [23] offered encouraging hints, but ultimately delivered worse overall accuracy compared to standard phrase-based systems.", "startOffset": 122, "endOffset": 126}, {"referenceID": 6, "context": ", [8]) with", "startOffset": 2, "endOffset": 5}, {"referenceID": 36, "context": "Following seminal papers in the area [39, 2], NMT translation quality has crept closer to the level of phrase-based translation systems for common research benchmarks.", "startOffset": 37, "endOffset": 44}, {"referenceID": 1, "context": "Following seminal papers in the area [39, 2], NMT translation quality has crept closer to the level of phrase-based translation systems for common research benchmarks.", "startOffset": 37, "endOffset": 44}, {"referenceID": 27, "context": "Perhaps the first successful attempt at surpassing phrase-based translation was described in [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 33, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 130, "endOffset": 134}, {"referenceID": 37, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 178, "endOffset": 182}, {"referenceID": 12, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 249, "endOffset": 257}, {"referenceID": 25, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 249, "endOffset": 257}, {"referenceID": 7, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 279, "endOffset": 282}, {"referenceID": 9, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 304, "endOffset": 308}, {"referenceID": 34, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 324, "endOffset": 328}, {"referenceID": 26, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 406, "endOffset": 410}, {"referenceID": 35, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 449, "endOffset": 457}, {"referenceID": 30, "context": "Since then, many novel techniques have been proposed to further improve NMT: using an attention mechanism to deal with rare words [36], a mechanism to model translation coverage [40], multi-task and semi-supervised training to incorporate more data [14, 28], a character decoder [9], a character encoder [11], subword units [37] also to deal with rare word outputs, different kinds of attention mechanisms [29], and sentence-level loss minimization [38, 33].", "startOffset": 449, "endOffset": 457}, {"referenceID": 36, "context": "Our model (see Figure 1) follows the common sequence-to-sequence learning framework [39] with attention [2].", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "Our model (see Figure 1) follows the common sequence-to-sequence learning framework [39] with attention [2].", "startOffset": 104, "endOffset": 107}, {"referenceID": 36, "context": "This observation is similar to previous observations that deep LSTMs significantly outperform shallow LSTMs [39].", "startOffset": 108, "endOffset": 112}, {"referenceID": 27, "context": "Similar to [30], we use a deep stacked Long Short Term Memory (LSTM) [22] network for both the encoder RNN and the decoder RNN.", "startOffset": 11, "endOffset": 15}, {"referenceID": 19, "context": "Similar to [30], we use a deep stacked Long Short Term Memory (LSTM) [22] network for both the encoder RNN and the decoder RNN.", "startOffset": 69, "endOffset": 73}, {"referenceID": 1, "context": "Our attention module is similar to [2].", "startOffset": 35, "endOffset": 38}, {"referenceID": 29, "context": "However, simply stacking more layers of LSTM works only to a certain number of layers, beyond which the network becomes too slow and difficult to train, likely due to exploding and vanishing gradient problems [32, 21].", "startOffset": 209, "endOffset": 217}, {"referenceID": 18, "context": "However, simply stacking more layers of LSTM works only to a certain number of layers, beyond which the network becomes too slow and difficult to train, likely due to exploding and vanishing gradient problems [32, 21].", "startOffset": 209, "endOffset": 217}, {"referenceID": 36, "context": "On the left: simple stacked LSTM layers [39].", "startOffset": 40, "endOffset": 44}, {"referenceID": 17, "context": "Motivated by [20], we introduce residual connections among the LSTM layers in a stack (see Figure 2).", "startOffset": 13, "endOffset": 17}, {"referenceID": 32, "context": "To have the best possible context at each point in the encoder network it makes sense to use a bi-directional RNN [35] for the encoder, which was also used in [2].", "startOffset": 114, "endOffset": 118}, {"referenceID": 1, "context": "To have the best possible context at each point in the encoder network it makes sense to use a bi-directional RNN [35] for the encoder, which was also used in [2].", "startOffset": 159, "endOffset": 162}, {"referenceID": 10, "context": "Data parallelism is straightforward: we train n model replicas concurrently using a Downpour SGD algorithm [12].", "startOffset": 107, "endOffset": 111}, {"referenceID": 21, "context": "The n replicas all share one copy of model parameters, with each replica asynchronously updating the parameters using a combination of Adam [24] and SGD algorithms.", "startOffset": 140, "endOffset": 144}, {"referenceID": 33, "context": "One approach is to simply copy rare words from source to target (as most rare words are names or numbers where the correct translation is just a copy), either based on the attention model [36], using an external alignment model [30], or even using a more complicated special purpose pointing network [17].", "startOffset": 188, "endOffset": 192}, {"referenceID": 27, "context": "One approach is to simply copy rare words from source to target (as most rare words are names or numbers where the correct translation is just a copy), either based on the attention model [36], using an external alignment model [30], or even using a more complicated special purpose pointing network [17].", "startOffset": 228, "endOffset": 232}, {"referenceID": 8, "context": ", chararacters [10], mixed word/characters [27], or more intelligent sub-words [37].", "startOffset": 15, "endOffset": 19}, {"referenceID": 24, "context": ", chararacters [10], mixed word/characters [27], or more intelligent sub-words [37].", "startOffset": 43, "endOffset": 47}, {"referenceID": 34, "context": ", chararacters [10], mixed word/characters [27], or more intelligent sub-words [37].", "startOffset": 79, "endOffset": 83}, {"referenceID": 31, "context": "Our most successful approach falls into the second category (sub-word units), and we adopt the wordpiece model (WPM) implementation initially developed to solve a Japanese/Korean segmentation problem for the Google speech recognition system [34].", "startOffset": 241, "endOffset": 245}, {"referenceID": 34, "context": "It is similar to the method used in [37] to deal with rare words in Neural Machine Translation.", "startOffset": 36, "endOffset": 40}, {"referenceID": 34, "context": "Our greedy algorithm to this optimization problem is similar to [37] and is described in more detail in [34].", "startOffset": 64, "endOffset": 68}, {"referenceID": 31, "context": "Our greedy algorithm to this optimization problem is similar to [37] and is described in more detail in [34].", "startOffset": 104, "endOffset": 108}, {"referenceID": 31, "context": "Compared to the original implementation used in [34], we use a special symbol only at the beginning of the words and not at both ends.", "startOffset": 48, "endOffset": 52}, {"referenceID": 30, "context": "Several recent papers [33, 38, 31] have considered different ways of incorporating the task reward into optimization of neural sequence-to-sequence models.", "startOffset": 22, "endOffset": 34}, {"referenceID": 35, "context": "Several recent papers [33, 38, 31] have considered different ways of incorporating the task reward into optimization of neural sequence-to-sequence models.", "startOffset": 22, "endOffset": 34}, {"referenceID": 28, "context": "Several recent papers [33, 38, 31] have considered different ways of incorporating the task reward into optimization of neural sequence-to-sequence models.", "startOffset": 22, "endOffset": 34}, {"referenceID": 30, "context": "We consider model refinement using the expected reward objective (also used in [33]), which can be expressed as", "startOffset": 79, "endOffset": 83}, {"referenceID": 38, "context": "For example, in [41], it is demonstrated that a convolutional neural network model can be sped up by a factor of 4-6 with minimal loss on classification accuracy on the ILSVRC-12 benchmark.", "startOffset": 16, "endOffset": 20}, {"referenceID": 23, "context": "In [26], it is demonstrated that neural network model weights can be quantized to only three states, -1, 0, and +1.", "startOffset": 3, "endOffset": 7}, {"referenceID": 15, "context": "Many of those previous studies [18, 19, 41, 26] however mostly focus on CNN models with relatively few layers.", "startOffset": 31, "endOffset": 47}, {"referenceID": 16, "context": "Many of those previous studies [18, 19, 41, 26] however mostly focus on CNN models with relatively few layers.", "startOffset": 31, "endOffset": 47}, {"referenceID": 38, "context": "Many of those previous studies [18, 19, 41, 26] however mostly focus on CNN models with relatively few layers.", "startOffset": 31, "endOffset": 47}, {"referenceID": 23, "context": "Many of those previous studies [18, 19, 41, 26] however mostly focus on CNN models with relatively few layers.", "startOffset": 31, "endOffset": 47}, {"referenceID": 37, "context": "We introduce two important refinements to the pure max-probability based beam search algorithm: a coverage penalty [40] and length normalization.", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "In both cases, we use newstest2014 as the test sets to compare against previous work [30, 36, 43].", "startOffset": 85, "endOffset": 97}, {"referenceID": 33, "context": "In both cases, we use newstest2014 as the test sets to compare against previous work [30, 36, 43].", "startOffset": 85, "endOffset": 97}, {"referenceID": 36, "context": "To be comparable to previous work [39, 30, 43], we report tokenized BLEU score as computed by the multi-bleu.", "startOffset": 34, "endOffset": 46}, {"referenceID": 27, "context": "To be comparable to previous work [39, 30, 43], we report tokenized BLEU score as computed by the multi-bleu.", "startOffset": 34, "endOffset": 46}, {"referenceID": 27, "context": "pl script, downloaded from the public implementation of Moses (on Github), which is also used in [30].", "startOffset": 97, "endOffset": 101}, {"referenceID": 0, "context": "The models are trained by a system we implemented using TensorFlow[1].", "startOffset": 66, "endOffset": 69}, {"referenceID": 36, "context": "As is common wisdom in training RNN models, we apply gradient clipping (similar to [39]): all gradients are uniformly scaled down such that the norm of the modified gradients is no larger than a fixed constant, which is 5.", "startOffset": 83, "endOffset": 87}, {"referenceID": 21, "context": "For the first stage of maximum likelihood training (that is, to optimize for objective function 7), we use a combination of Adam [24] and simple SGD learning algorithms provided by the TensorFlow runtime system.", "startOffset": 129, "endOffset": 133}, {"referenceID": 33, "context": "We then use the attention mechanism to copy a corresponding word from the source to replace these unknown words during decoding [36].", "startOffset": 128, "endOffset": 132}, {"referenceID": 13, "context": "2774 PBMT [15] 37.", "startOffset": 10, "endOffset": 14}, {"referenceID": 27, "context": "0 LSTM (6 layers) [30] 31.", "startOffset": 18, "endOffset": 22}, {"referenceID": 27, "context": "5 LSTM (6 layers + PosUnk) [30] 33.", "startOffset": 27, "endOffset": 31}, {"referenceID": 33, "context": "7 RNNSearch [36] 16.", "startOffset": 12, "endOffset": 16}, {"referenceID": 33, "context": "5 RNNSearch-LV [36] 16.", "startOffset": 15, "endOffset": 19}, {"referenceID": 33, "context": "9 RNNSearch-LV [36] 16.", "startOffset": 15, "endOffset": 19}, {"referenceID": 27, "context": "16 LSTM (6 layers) [30] 35.", "startOffset": 19, "endOffset": 23}, {"referenceID": 27, "context": "6 LSTM (6 layers + PosUnk) [30] 37.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "Model BLEU Side-by-side averaged score PBMT [15] 37.", "startOffset": 44, "endOffset": 48}], "year": 2016, "abstractText": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference \u2013 sometimes prohibitively so in the case of very large data sets and large models. Several authors have also charged that NMT systems lack robustness, particularly when input sentences contain rare words. These issues have hindered NMT\u2019s use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google\u2019s Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\u201cwordpieces\u201d) for both input and output. This method provides a good balance between the flexibility of \u201ccharacter\u201d-delimited models and the efficiency of \u201cword\u201d-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. To directly optimize the translation BLEU scores, we consider refining the models by using reinforcement learning, but we found that the improvement in the BLEU scores did not reflect in the human evaluation. On the WMT\u201914 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google\u2019s phrase-based production system.", "creator": "LaTeX with hyperref package"}}}