{"id": "1609.08151", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "25-Sep-2016", "title": "Nonnegative autoencoder with simplified random neural network", "abstract": "This paper proposes new nonnegative (shallow and multi-layer) autoencoders by combining the model of spiking Random Neural Network (RNN), the network architecture in the deep-learning area and the training technique in the nonnegative matrix factorization (NMF) area. The shallow autoencoder is a simplified RNN model, which is then stacked into a multi-layer architecture. The learning algorithms are based on the weight update rules in the NMF area, subjecting to the nonnegative and probability constraints in a RNN model. The autoencoders equipped with the learning algorithms are tested on both typical image datasets including the MNIST, Yale face and CIFAR-10 datesets and 16 real-world datasets from different areas, and the results verify its efficacy. Simulation results of the stochastic spiking behaviors of this RNN autoencoder demonstrate that it can be implemented in a highly-distributed manner.", "histories": [["v1", "Sun, 25 Sep 2016 13:47:08 GMT  (603kb,D)", "https://arxiv.org/abs/1609.08151v1", "10 pages"], ["v2", "Thu, 29 Sep 2016 11:02:29 GMT  (603kb,D)", "http://arxiv.org/abs/1609.08151v2", "10 pages (a small edit to the abstract)"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["yonghua yin", "erol gelenbe"], "accepted": false, "id": "1609.08151"}, "pdf": {"name": "1609.08151.pdf", "metadata": {"source": "CRF", "title": "Nonnegative autoencoder with simplified random neural network", "authors": ["Yonghua Yin", "Erol Gelenbe"], "emails": ["y.yin14@imperial.ac.uk,", "e.gelenbe@imperial.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it is closer than ever before in the history of the country."}, {"heading": "2 A quasi-linear simplified random neural network", "text": "It is as if it is a matter of a way in which the neurons of other cells are not able to say goodbye to the previous assumptions that it is necessary to recur for an arbitrary N (3), for an arbitrary N (4), for an arbitrary N (4), for an arbitrary N (5), which may not be recurrent. (5) It is as if the neurons depend for an arbitrary N (5), which may not be recurrent. (5) It is as if the neurons depend for other cells. (5) It is as if the neurons depend for other cells. (5) It is as if the neurons depend for an arbitrary N (5), which may not be recurrent. (5) It is as if the neurons are derived from other cells. (5) It is as if the neurons depend for other cells."}, {"heading": "3 Shallow non-negative LRNN autoencoder", "text": "We add an output layer of O neurons on the top level of the LRNN shown in Figure 1 (Q = 1) to construct a flat, non-negative LRNN (V = 1). Let us specify the probability that the output neurons interact with the LRNN in the following way, in which o = 1, \u00b7 \u00b7 \u00b7 \u00b7 LRNN = 1, \u00b7 LRNN = 1, \u00b7 LRNN = 1, \u2022 LRNN = 1, \u2022 LRNN = 1, \u00b7 LRNO = 1, \u00b7 LRNO = 1, \u00b7 LRNO = 1, \u00b7 LRNO = o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o, o"}, {"heading": "4 Multi-layer non-negative LRNN autoencoder", "text": ", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\" \",\" \",\" \",\" \"\", \"\" \",\" \"\", \"\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\", \"\", \"\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \",\" \"\", \"\" \"\", \"\" \",\" \"\" \",\" \"\", \"\" \"\", \"\" \"\" \",\" \"\", \",\" \"\", \"\" \"\", \"\" \",\" \"\" \",\" \"\", \"\" \",\" \"\" \",\" \"\" \",\" \",\" \"\" \",\" \"\" \",\" \"\", \"\" \"\" \",\" \"\", \"\" \"\", \"\" \",\" \"\" \"\", \"\" \",\", \"\" \"\", \"\" \"\" \",\" \",\" \"\" \"\", \"\" \"\", \"\" \"\" \",\" \"\" \"\", \"\" \",\" \"\" \"\" \",\" \",\", \"\" \"\" \"\", \"\" \"\", \"\" \",\" \"\" \"\", \"\" \",\" \"\" \",\", \"\" \"\", \"\" \",\" \"\" \"\", \"\" \",\", \"\" \"\" \",\" \"\" \",\" \"\" \"\", \"\" \"\", \"\" \"\", \"\" \"\" \",\" \",\" \"\" \"\", \"\", \",\" \"\" \""}, {"heading": "5 Numerical Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Datasets", "text": "MNIST: The MNIST dataset with handwritten digits [29] contains 60,000 and 10,000 images in the training and test dataset. The number of input attributes is 784 (28 x 28 images) located in [0, 1]. Yale Face: This database (http: / / vision.ucsd.edu / content / yale-face database) contains 165 grayscale images of 15 individuals. Here we use the pre-processed dataset from [30], where each image is resized to 32 x 32 (1024 pixels). CIFAR-10: The CIFAR-10 dataset consists of 60,000 32 x 32 color images [31]. Each image has 3072 attributes. It contains 50,000 and 10,000 images in the training and test datasets. UCI datasets from the real world: In addition to the image datasets, we perform numerical experiments on different real datasets in different areas of the UCI that differ from the ones."}, {"heading": "5.2 Convergence and reconstruction performance", "text": "The results of the study are, in fact, as if it were able to remedy and remedy the deficiencies mentioned, without being able to identify the causes."}, {"heading": "6 Simulating the spiking random neural network", "text": "The advantage of a spiking model, such as the LRNN autoencoder, lies in its highly distributed nature (v = 1 layer of fire).In this section, instead of a numerical calculation, we simulate the stochastic spiking behavior of the LRNN autoencoder. The simulation in this section is based on the numerical experiment of sub-section 5.2. Specifically, in sub-section 5.2, we construct an LRNN autoencoder of the structure 784 \u2192 100 (found with appropriate weights), which has three layers: the visual layer (784 neurons), the hidden layer (100 neurons) and the output layer (784 neurons).First, an image with 28 \u00d7 28 = 784 attributes is taken from the MNIST dataset. Each visual neuron receives exciting spikes from outside the network in a Poisson stream, with the rate being the corresponding value in the image."}, {"heading": "7 Conclusions", "text": "New non-negative autoencoders (the flat and multi-layered LRNN autoencoders) have been proposed based on the Spiking RNN model, which adopts the multi-layered network architecture of the feed word in the deep learning area. In order to meet the RNN limitations of non-negativity and the sum of probabilities is no greater than 1, learning algorithms have been developed by adapting the weight update rules from the NMF area. Numerical results based on typical image data sets such as the MNIST, Yale and CIFAR-10 datasets, as well as 16 data sets from different areas of the real world have well confirmed the robust convergence and reconstruction performance of the LRNN autoencoder. In addition to numerical experiments, we have performed simulations of the autoencoder in which the stochastic spiking behavior is simulated. Simulation results correspond well with the corresponding autocoder results that can be superimposed on the RN highly."}], "references": [{"title": "Random neural networks with negative and positive signals and product form solution", "author": ["E. Gelenbe"], "venue": "Neural computation, vol. 1, no. 4, pp. 502\u2013510, 1989.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1989}, {"title": "Stability of the random neural network model", "author": ["\u2014\u2014"], "venue": "Neural computation, vol. 2, no. 2, pp. 239\u2013247, 1990.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning in the recurrent random neural network", "author": ["\u2014\u2014"], "venue": "Neural Computation, vol. 5, pp. 154\u2013164, 1993.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1993}, {"title": "Dynamical random neural network approach to the traveling salesman problem", "author": ["E. Gelenbe", "V. Koubi", "F. Pekergin"], "venue": "Proceedings IEEE Symp. Systems, Man and Cybernetics. IEEE, 1993, p. 630\u2013635.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1993}, {"title": "Video quality and traffic qos in learning-based subsampled and receiver-interpolated video sequences", "author": ["C.E. Cramer", "E. Gelenbe"], "venue": "Selected Areas in Communications, IEEE Journal on, vol. 18, no. 2, pp. 150\u2013167, 2000.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning in the multiple class random neural network", "author": ["E. Gelenbe", "K.F. Hussain"], "venue": "Neural Networks, IEEE Transactions on, vol. 13, no. 6, pp. 1257\u20131267, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Steps toward self-aware networks", "author": ["E. Gelenbe"], "venue": "Communications of the ACM, vol. 52, no. 7, pp. 66\u201375, 2009.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2009}, {"title": "Large scale simulation for human evacuation and rescue", "author": ["E. Gelenbe", "F.-J. Wu"], "venue": "Computers & Mathematics with Applications, vol. 64, no. 12, pp. 3869\u20133880, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "A new approach for the prediction of end-to-end performance of multimedia streams", "author": ["G. Rubino", "M. Varela"], "venue": "Quantitative Evaluation of Systems, 2004. QEST 2004. Proceedings. First International Conference on the. IEEE, 2004, pp. 110\u2013119.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "A study of real-time packet video quality using random neural networks", "author": ["S. Mohamed", "G. Rubino"], "venue": "IEEE Transactions on Circuits and Systems for Video Technology, vol. 12, no. 12, pp. 1071\u20131083, 2002.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2002}, {"title": "Evaluating perceived voice quality on packet networks using different random neural network architectures", "author": ["K. Radhakrishnan", "H. Larijani"], "venue": "Performance Evaluation, vol. 68, no. 4, pp. 347\u2013360, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-intrusive method for video quality prediction over lte using random neural networks (rnn)", "author": ["T. Ghalut", "H. Larijani"], "venue": "Communication Systems, Networks & Digital Signal Processing (CSNDSP), 2014 9th International Symposium on. IEEE, 2014, pp. 519\u2013524.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Pictorial information retrieval using the random neural network", "author": ["A. Stafylopatis", "A. Likas"], "venue": "IEEE Transactions on Software Engineering, vol. 18, no. 7, pp. 590\u2013600, 1992.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1992}, {"title": "Voice quality in voip networks based on random neural networks", "author": ["H. Larijani", "K. Radhakrishnan"], "venue": "Networks (ICN), 2010 Ninth International Conference on. IEEE, 2010, pp. 89\u201392.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "A grasp algorithm using rnn for solving dynamics in a p2p live video streaming network", "author": ["M. Mart\u00ednez", "A. Mor\u00f3n", "F. Robledo", "P. Rodr\u00edguez-Bocca", "H. Cancela", "G. Rubino"], "venue": "Hybrid Intelligent Systems, 2008. HIS\u201908. Eighth International Conference on. IEEE, 2008, pp. 447\u2013452.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Reducing the dimensionality of data with neural networks", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Science, vol. 313, no. 5786, pp. 504\u2013507, 2006.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Understanding the difficulty of training deep feedforward neural networks.", "author": ["X. Glorot", "Y. Bengio"], "venue": "in Aistats, vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation, vol. 18, no. 7, pp. 1527\u20131554, 2006.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep sparse rectifier neural networks.", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "in Aistats,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2011}, {"title": "The tradeoffs of large scale learning", "author": ["O. Bousquet", "L. Bottou"], "venue": "Advances in neural information processing systems, 2008, pp. 161\u2013168. 9", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Learning the parts of objects by non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "Nature, vol. 401, no. 6755, pp. 788\u2013791, 1999.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1999}, {"title": "Projective nonnegative graph embedding", "author": ["X. Liu", "S. Yan", "H. Jin"], "venue": "IEEE Transactions on Image Processing, vol. 19, no. 5, pp. 1126\u20131137, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "Non-negative sparse coding", "author": ["P.O. Hoyer"], "venue": "Neural Networks for Signal Processing, 2002. Proceedings of the 2002 12th IEEE Workshop on. IEEE, 2002, pp. 557\u2013565.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2002}, {"title": "Nonnegative matrix factorization: A comprehensive review", "author": ["Y.-X. Wang", "Y.-J. Zhang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 6, pp. 1336\u20131353, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, 2006, pp. 126\u2013135.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Recognition of objects and their component parts: responses of single units in the temporal cortex of the macaque", "author": ["E. Wachsmuth", "M. Oram", "D. Perrett"], "venue": "Cerebral Cortex, vol. 4, no. 5, pp. 509\u2013522, 1994.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1994}, {"title": "On the equivalence of nonnegative matrix factorization and spectral clustering.", "author": ["C.H. Ding", "X. He", "H.D. Simon"], "venue": "in SDM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1998}, {"title": "Learning a spatially smooth subspace for face recognition", "author": ["D. Cai", "X. He", "Y. Hu", "J. Han", "T. Huang"], "venue": "2007 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 2007, pp. 1\u20137.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning multiple layers of features from tiny images", "author": ["A. Krizhevsky", "G. Hinton"], "venue": "2009.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "UCI machine learning repository", "author": ["M. Lichman"], "venue": "2013. [Online]. Available: http: //archive.ics.uci.edu/ml", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Projective nonnegative graph embedding", "author": ["X. Liu", "S. Yan", "H. Jin"], "venue": "Image Processing, IEEE Transactions on, vol. 19, no. 5, pp. 1126\u20131137, 2010.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2010}, {"title": "Pattern recognition via linear programming: Theory and application to medical diagnosis", "author": ["O.L. Mangasarian", "R. Setiono", "W. Wolberg"], "venue": "Large-scale numerical optimization, pp. 22\u201331, 1990.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 1990}, {"title": "Robust linear programming discrimination of two linearly inseparable sets", "author": ["K.P. Bennett", "O.L. Mangasarian"], "venue": "Optimization methods and software, vol. 1, no. 1, pp. 23\u201334, 1992.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1992}, {"title": "Exploiting nonlinear recurrence and fractal scaling properties for voice disorder detection", "author": ["M.A. Little", "P.E. McSharry", "S.J. Roberts", "D.A. Costello", "I.M. Moroz"], "venue": "BioMedical Engineering OnLine, vol. 6, no. 1, p. 1, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Short-term memory mechanisms in neural network learning of robot navigation tasks: A case study", "author": ["A.L. Freire", "G.A. Barreto", "M. Veloso", "A.T. Varela"], "venue": "Robotics Symposium (LARS), 2009 6th Latin American. IEEE, 2009, pp. 1\u20136.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Classification of radar returns from the ionosphere using neural networks", "author": ["V.G. Sigillito", "S.P. Wing", "L.V. Hutton", "K.B. Baker"], "venue": "Johns Hopkins APL Technical Digest, vol. 10, no. 3, pp. 262\u2013266, 1989.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 1989}, {"title": "Machine learning for first-order theorem proving", "author": ["J.P. Bridge", "S.B. Holden", "L.C. Paulson"], "venue": "Journal of automated reasoning, vol. 53, no. 2, pp. 141\u2013172, 2014.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2014}, {"title": "Analysis of hidden units in a layered network trained to classify sonar targets", "author": ["R.P. Gorman", "T.J. Sejnowski"], "venue": "Neural networks, vol. 1, no. 1, pp. 75\u201389, 1988.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 1988}, {"title": "A supervised machine learning algorithm for arrhythmia analysis", "author": ["H.A. Guvenir", "B. Acar", "G. Demiroz", "A. Cekin"], "venue": "Computers in Cardiology 1997. IEEE, 1997, pp. 433\u2013436. 10", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1997}], "referenceMentions": [{"referenceID": 0, "context": "A mathematical tool that has existed since 1989 [1\u20133], but is less well known in the machine-learning community, is the Random Neural Network (RNN), which is a stochastic integer-state \u201cintegrate and fire\u201d system and developed to mimic the behaviour of biological neurons in the brain.", "startOffset": 48, "endOffset": 53}, {"referenceID": 1, "context": "A mathematical tool that has existed since 1989 [1\u20133], but is less well known in the machine-learning community, is the Random Neural Network (RNN), which is a stochastic integer-state \u201cintegrate and fire\u201d system and developed to mimic the behaviour of biological neurons in the brain.", "startOffset": 48, "endOffset": 53}, {"referenceID": 2, "context": "A mathematical tool that has existed since 1989 [1\u20133], but is less well known in the machine-learning community, is the Random Neural Network (RNN), which is a stochastic integer-state \u201cintegrate and fire\u201d system and developed to mimic the behaviour of biological neurons in the brain.", "startOffset": 48, "endOffset": 53}, {"referenceID": 3, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 4, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 5, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 6, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 8, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 9, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 10, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 11, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 12, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 13, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 14, "context": "The RNN has been used for numerous applications [4\u201315] that exploit its recurrent structure.", "startOffset": 48, "endOffset": 54}, {"referenceID": 15, "context": "Deep learning has achieved great success in machine learning [16,17].", "startOffset": 61, "endOffset": 68}, {"referenceID": 16, "context": "Deep learning has achieved great success in machine learning [16,17].", "startOffset": 61, "endOffset": 68}, {"referenceID": 17, "context": "The feed-forward fully-connected multi-layer neural network could be difficult to train [18].", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "Pre-training the network layer by layer is a great advance [16, 19] and useful due to its wide adaptability, though recent literature shows that utilizing the rectified linear unit (ReLU) could train a deep neural network without pre-training [20].", "startOffset": 59, "endOffset": 67}, {"referenceID": 18, "context": "Pre-training the network layer by layer is a great advance [16, 19] and useful due to its wide adaptability, though recent literature shows that utilizing the rectified linear unit (ReLU) could train a deep neural network without pre-training [20].", "startOffset": 59, "endOffset": 67}, {"referenceID": 19, "context": "Pre-training the network layer by layer is a great advance [16, 19] and useful due to its wide adaptability, though recent literature shows that utilizing the rectified linear unit (ReLU) could train a deep neural network without pre-training [20].", "startOffset": 243, "endOffset": 247}, {"referenceID": 20, "context": "The typical training procedure called stochastic gradient descent (SGD) provides a practical choice for handling large datasets [21].", "startOffset": 128, "endOffset": 132}, {"referenceID": 21, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 22, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 23, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 24, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 25, "context": "Nonnegative matrix factorization (NMF) is also a popular topic in machine learning [22\u201326], which learns part-based representations of raw data.", "startOffset": 83, "endOffset": 90}, {"referenceID": 21, "context": "Lee [22] suggested that the perception of the whole in the brain may be based on these part-based representations (based on the physiological evidence [27]) and proposed simple yet effective update rules.", "startOffset": 4, "endOffset": 8}, {"referenceID": 26, "context": "Lee [22] suggested that the perception of the whole in the brain may be based on these part-based representations (based on the physiological evidence [27]) and proposed simple yet effective update rules.", "startOffset": 151, "endOffset": 155}, {"referenceID": 23, "context": "Hoyer [24] combined sparse coding and NMF that allows control over sparseness.", "startOffset": 6, "endOffset": 10}, {"referenceID": 25, "context": "clustering in [26, 28] and presented simple update rules for orthogonal NMF.", "startOffset": 14, "endOffset": 22}, {"referenceID": 27, "context": "clustering in [26, 28] and presented simple update rules for orthogonal NMF.", "startOffset": 14, "endOffset": 22}, {"referenceID": 24, "context": "Wang [25] provided a comprehensive review on recent processes in the NMF area.", "startOffset": 5, "endOffset": 9}, {"referenceID": 15, "context": "Then, this shallow autoencoder is stacked into a multi-layer feed-forward autoencoder following the network architecture in the deep learning area [16, 17, 19].", "startOffset": 147, "endOffset": 159}, {"referenceID": 16, "context": "Then, this shallow autoencoder is stacked into a multi-layer feed-forward autoencoder following the network architecture in the deep learning area [16, 17, 19].", "startOffset": 147, "endOffset": 159}, {"referenceID": 18, "context": "Then, this shallow autoencoder is stacked into a multi-layer feed-forward autoencoder following the network architecture in the deep learning area [16, 17, 19].", "startOffset": 147, "endOffset": 159}, {"referenceID": 28, "context": "The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 177, "endOffset": 181}, {"referenceID": 29, "context": "The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 193, "endOffset": 197}, {"referenceID": 30, "context": "The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 211, "endOffset": 215}, {"referenceID": 31, "context": "The efficacy of the nonnegative autoencoders equipped with the learning algorithms is well verified via numerical experiments on both typical image datasets including the MNIST [29], Yale face [30] and CIFAR-10 [31] datesets and 16 real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 312, "endOffset": 316}, {"referenceID": 2, "context": "From the preceding assumptions it was proved in [3] that for an arbitrary N neuron RNN, which may or may not be recurrent (i.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "In [3], it was shown that the system of N non-linear equations (1) have a solution which is unique.", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "According to [1] and (1), they are given by q\u0302v = min(\u039b\u0302v /r\u0302v, 1), and qh = min(\u039b + h /rh, 1), where the quantities \u039b\u0302 + v and \u039b + h represent the total average arrival rates of excitatory spikes, r\u0302v and rh represent the firing rates of the neurons.", "startOffset": 13, "endOffset": 16}, {"referenceID": 32, "context": "We use the following update rules to solve this problem, which are simplified from Liu\u2019s work [33]:", "startOffset": 94, "endOffset": 98}, {"referenceID": 28, "context": "MNIST: The MNIST dataset of handwritten digits [29] contains 60,000 and 10,000 images in the training and test dataset.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "The number of input attributes is 784 (28\u00d7 28 images), which are in [0, 1].", "startOffset": 68, "endOffset": 74}, {"referenceID": 29, "context": "Here we use the pre-processed dataset from [30], where each image is resized as 32\u00d7 32 (1024 pixels).", "startOffset": 43, "endOffset": 47}, {"referenceID": 30, "context": "CIFAR-10: The CIFAR-10 dataset consists of 60,000 32\u00d7 32 colour images [31].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "UCI real-world datasets: In addition to image datasets, we also conduct numerical experiments on different real-world datasets in different areas from the UCI machine learning repository [32].", "startOffset": 187, "endOffset": 191}, {"referenceID": 33, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 155, "endOffset": 162}, {"referenceID": 34, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 155, "endOffset": 162}, {"referenceID": 35, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 215, "endOffset": 219}, {"referenceID": 36, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 266, "endOffset": 270}, {"referenceID": 37, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 290, "endOffset": 294}, {"referenceID": 38, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 363, "endOffset": 367}, {"referenceID": 39, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 382, "endOffset": 386}, {"referenceID": 40, "context": "Iris 4 150 Teaching Assistant Evaluation (TAE) 5 151 Liver Disorders (LD) 5 345 Seeds 7 210 Pima Indians Diabetes (PID) 8 768 Breast Cancer Wisconsin (BC) [34\u201336] 9 699 Glass 9 214 Wine 13 178 Zoo 16 100 Parkinsons [37] 22 195 Wall-Following Robot Navigation (WFRN) [38] 24 5456 Ionosphere [39] 34 351 Soybean Large (SL) 35 186 First-Order Theorem Proving (FOTP) [40] 51 6118 Sonar [41] 60 208 Cardiac Arrhythmia (CA) [42] 279 452", "startOffset": 418, "endOffset": 422}, {"referenceID": 0, "context": "Results of Yale face: Attribute values are normalized into [0, 1] (by dividing by 255).", "startOffset": 59, "endOffset": 65}, {"referenceID": 0, "context": "Results of CIFAR-10: Attribute values of the dataset are also divided by 255 for normalization in range [0, 1].", "startOffset": 104, "endOffset": 110}, {"referenceID": 0, "context": "The attribute values are linear normalized in range [0, 1].", "startOffset": 52, "endOffset": 58}], "year": 2016, "abstractText": "This paper proposes new nonnegative (shallow and multi-layer) autoencoders by combining the spiking Random Neural Network (RNN) model, the network architecture typical used in deep-learning area and the training technique inspired from nonnegative matrix factorization (NMF). The shallow autoencoder is a simplified RNN model, which is then stacked into a multi-layer architecture. The learning algorithm is based on the weight update rules in NMF, subject to the nonnegative probability constraints of the RNN. The autoencoders equipped with this learning algorithm are tested on typical image datasets including the MNIST, Yale face and CIFAR-10 datasets, and also using 16 real-world datasets from different areas. The results obtained through these tests yield the desired high learning and recognition accuracy. Also, numerical simulations of the stochastic spiking behavior of this RNN auto encoder, show that it can be implemented in a highly-distributed manner.", "creator": "LaTeX with hyperref package"}}}