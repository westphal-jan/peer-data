{"id": "1609.08237", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Aligning Coordinated Text Streams through Burst Information Network Construction and Decipherment", "abstract": "Aligning coordinated text streams from multiple sources and multiple languages has opened many new research venues on cross-lingual knowledge discovery. In this paper we aim to advance state-of-the-art by: (1). extending coarse-grained topic-level knowledge mining to fine-grained information units such as entities and events; (2). following a novel Data-to-Network-to-Knowledge (D2N2K) paradigm to construct and utilize network structures to capture and propagate reliable evidence. We introduce a novel Burst Information Network (BINet) representation that can display the most important information and illustrate the connections among bursty entities, events and keywords in the corpus. We propose an effective approach to construct and decipher BINets, incorporating novel criteria based on multi-dimensional clues from pronunciation, translation, burst, neighbor and graph topological structure. The experimental results on Chinese and English coordinated text streams show that our approach can accurately decipher the nodes with high confidence in the BINets and that the algorithm can be efficiently run in parallel, which makes it possible to apply it to huge amounts of streaming data for never-ending language and information decipherment.", "histories": [["v1", "Tue, 27 Sep 2016 01:19:41 GMT  (2821kb,D)", "http://arxiv.org/abs/1609.08237v1", "10 pages"]], "COMMENTS": "10 pages", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["tao ge", "qing dou", "xiaoman pan", "heng ji", "lei cui", "baobao chang", "zhifang sui", "ming zhou"], "accepted": false, "id": "1609.08237"}, "pdf": {"name": "1609.08237.pdf", "metadata": {"source": "CRF", "title": "Aligning Coordinated Text Streams through Burst Information Network Construction and Decipherment", "authors": ["Tao Ge", "Qing Dou", "Xiaoman Pan", "Heng Ji", "Lei Cui", "Baobao Chang", "Zhifang Sui", "Ming Zhou"], "emails": ["getao@pku.edu.cn", "qdou@isi.edu", "panx2@rpi.edu", "jih@rpi.edu", "lecu@microsoft.com", "chbb@pku.edu.cn", "szf@pku.edu.cn", "mingzhou@microsoft.com", "permissions@acm.org."], "sections": [{"heading": null, "text": "Categories and Subject Lines H.2.8 [Database Management]: Database Applications - Data Mining; I.2.7 [Artificial Intelligence]: Natural Lan permission to make digital or printed copies of all or part of this work for personal or commercial use is granted free of charge, provided that copies are not made or distributed for profit or commercial purposes, and that copies bear this notice and the full quote on the first page. Copyrights for components of this work other than ACM must be respected. Credit card abstraction is permitted. Otherwise copying or republishing, posting on servers, or redistributing to lists requires prior express permission and / or a fee. Request permission from permissions @ acm.org.c \u00a9 2016 ACM. ISBN 123-4567-24-567 / 08 / 06.... $15.00 DOI: 10.475 Prophywords / Netguage 4TextWorks - Streaming Aligment Analysis"}, {"heading": "1. INTRODUCTION", "text": "This year, it is only a matter of time before there is an agreement, until there is an agreement."}, {"heading": "2. BURST INFORMATION NETWORK", "text": "We will start by defining some new terminologies and describing the detailed methods for building burst information networks."}, {"heading": "2.1 Burst Detection", "text": "The burst of a word refers to a remarkable increase in the occurrence of the word over a short period of time and could indicate important events or trending topics. For example, the word \"(Williams)\" bursts from January 27 to January 31, as shown in Figure 2, due to the wonderful performance of Serena Williams at the 2010 Australian Open held during this period. For a time stamped document collection C = {D1, D2,..., DT}, where DT} is the collection of documents during the period, we define the burst sequence of a word s = (s1, s2,..., sT) in which the probability of a word is either 1 or 0 to indicate whether it bursts or not at the time of the epoch. Most burst detection approaches (e.g. [6]) recognize the burst sequence of a word during the period, which is mainly based on the comparison of the probability of a word qt to a time, the burst in the epoch."}, {"heading": "2.2 Burst Information Network Construction", "text": "An information network is a graph showing connections between entities and events in which each community should be topically and temporally coherent, as shown in Figure 1. Most of the previous approaches (e.g. [5]) rely on a large number of burst linguistic resources and marked data to construct information networks. [8] An information network in which each node is a word type (i.e., term) and edges between nodes denotes the simultaneous occurrence of words in a document. The resulting information network can demonstrate the connections between words. It usually consists of several communities (i.e., a group of nodes that are strongly related to each other) and each community in the information network corresponds to a topic. However, such a type of information network does not contain any temporal information, and thus the communities are generally not time coherent. Furthermore, it has not addressed the problem of ambiguity, as a word has different perceptions in corus."}, {"heading": "3. DECIPHERMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Hypothesis and Overall Framework", "text": "In this work, we use Chinese as a foreign language and conduct experiments with as few language-specific resources as possible. Our basic hypothesis is that units of information (units, events, etc.) tend to occur in many different languages at the same time. Therefore, if we collect a collection of English documents created from a timeframe similar to the Chinese corpus, we can construct BINets from this English corpus and then consult them to validate candidate translations. Formally, we define Gc = < Vc, Ec > and Ge = < Ve, Ee > as Chinese BINet or English BINet. For those who cannot understand Chinese, Gc can be considered a network of ciphers. Based on this hypothesis, we design a novel BINet deciphering method to decipher Gc. Formally, we define the part of the Vnoc process as a decryption process."}, {"heading": "3.2 Starting Point", "text": "To begin deciphering the Chinese BINet, we need a few seeds based on previous knowledge as a starting point. Inspired by some previous work on bilingual dictionary induction (e.g. [9, 10]), deciphering (e.g. [11, 12, 13]) and naming (e.g. [14, 15]), we use a few linguistic resources - a bilingual lexicon and universal linguistic representations such as time / calendar date, number, website URL, currency and emoticons - to decipher a subset of Chinese nodes. For the example shown in Figure 1, we can decipher some nodes in the Chinese BINet, such as \"7-6\" (to \"7-6\") and... \""}, {"heading": "3.3 Candidate Generation", "text": "Based on the foreknowledge mentioned above, we can decipher a subset of nodes in the Chinese BINet. For the remaining nodes, we must first discover their possible candidate translations. For a node c in the Chinese BINet, its translation can be any node in the English BINet or does not exist in the English BINet. However, if we use all nodes in the English BINet as candidates, the candidate list will be extremely long, making it inefficient to validate each candidate. To solve this problem, we use time constraints to drastically restrict the search field. As defined in Section 2, a node in a BINet is a burst element with a burst period. For a node in the Chinese BINet, its candidate translation is likely to be a node with the same burst period in the English BINet. For example, the node in a BINet is a burst period with a burst period."}, {"heading": "3.4 Candidate Validation", "text": "Once we have received the list of candidates from c (i.e., Cand (c)), each node e (c) en (n) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) en (c) e (e) e (c) e (e) e (c) e (e) e (c) e (e) e (c) e (c) e (c) e (e) e (c) e (c) e (c) e (c) e (e) e) e (c) e) e (c) e (c) e (c) e) e c c c c c c c c c c c c c c c c (e) e (e) e (c) e) e (c c c c c c c c) e (e) e (e) e) e (e) e (e (e) e) e (e) e (e) e (e) e (e) e (c) e) e (c) e (e) e (c) e) e (c) e (e) e) e (c (c) e) e) e (e) e (c (e) e (e) e) e (c (e) e) e (c (c (e) e) e) e) e (c (c (e) e) e) e (c (c (c c c (c) c (e) c c c c c c c c c c c c c c (c c c (c) c) c c c c c c c c c c c c (c (c) c) c c c c"}, {"heading": "3.5 Graph-based Decipherment", "text": "Based on the notes presented above, we define the overall validation score (credibility score) by the following equation: Score (c, e) = f (Sp, St, Sn, Sb) (2), where f (\u00b7) is a function and Sp, St, Sn and Sb are validation scores based on pronunciation, translation, neighboring and co-burst clues. Specifically, we define: f (Sp, St, Sn, Sb) = \u03b7Sp + \u03bbSt + min (\u03b3Sn, S-max n) + \u03b4Sb (3), where \u03b7, \u03bb, \u03b3 and \u03b4 are parameters for adjusting the weights of the above indicators. Note that in equation (3) we limit the effect of the neighbor validation score (no higher than S-maxn) + \u03b4Sb (3), where \u03b7, \u03b3 and \u03b4 are parameters for adjusting the weights of the aforementioned indicators. Note that we in equation (3) restrict the effect of the neighbor validation score + \u03b4Sb (no higher than S-maxn) + \u03b4Sb (3) Score-Score-Score-Score-Score-Score-Score-is influenced by other Score-Score-validation candidates."}, {"heading": "1: For the determined pair \u3008c, e\u3009 based on the prior knowledge,", "text": "Score (c, e) \u2190 1 2: For other undermined pairs < c, e > initialize score (c, e) according to Eq (4); 3: for each iteration (until convergence) do 4: for each indefinite pair < c, e > do 5: new score \u2190 f (Sp, St, Sn, Sb) according to Eq (3); 6: update (c, e) = min (0.99, new score) 7: end for 8: for each indefinite pair < c, e > do 9: Score (c, e) \u2190 update (c, e) 10: end for 11: end forSince the score of a pair < c, e > is affected by the scores of other pairs, we use a label propagation algorithm to iteratively calculate and update the scores so that we can decode the entire Chinese BINet in a dispersion mode."}, {"heading": "4. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data", "text": "We use Agence France Presse's 2010 news articles in Chinese Gigaword [16] and English Gigaword [17] as a non-parallel news stream corpora. The Chinese corpus has 17,327 documents and the English corpus has 186,737 documents. We removed stopwords, lematized and name-tagged the English corpus, and used word segmentation and name-tagging for the Chinese corpus using the Stanford CoreNLP toolkit [18]. Note that we used name-tagging in the Chinese corpus only to avoid segmenting a designated unit to separate words, and did not use other information (e.g. designated entity type) of name-tagging results because we want to prove that our approach is language-independent and does not require much language-specific knowledge. Therefore, in addition to a bilingual lexicon, we have a pinyin (pronunciation) and tools for Chinese word segmentation that contain INThering-specific knowledge."}, {"heading": "4.2 Experimental Setting", "text": "We evaluate our approach in an end-to-end manner. For a node c in the Chinese BINet, we select the node e *, which has the highest score as its translation in the English BINet. \u2022 For a node c in the Chinese BINet, we select the node e *, which has the highest score as its translation in the English BINet. \u2022 Note that a pair < c, e > is noted as correct, if e is the translation of c or c correct, a mention referring to an entity e is made. Annotation is performed by 2 human judges with 92.2% overlapping. The main cause of comment discrepancies is the ambiguity of some Chinese words including entities. In the evaluation, we consider < c, e > correct if both human judges annotate them as correct."}, {"heading": "4.3 Results", "text": "In fact, most of them will be able to move to another world, in which they can move to another world."}, {"heading": "5. RELATED WORK", "text": "Burst patterns have been studied for decades in data mining and natural language processing communities. [24, 6, 25, 26, 7] In addition, burst recognition problems from text streams have been investigated. [24, 27, 2, 28, 29, 30, 7, 31] used burst information to find crisp topics and events. Extensive research has also been done on bilingual lexicon induction (e.g. [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.g. [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora. Some of these approaches also used temporal similarities and context information. We proposed new and richer clues, including burst and graph topological structure. Nodes in BINet are not limited to named entities."}, {"heading": "6. CONCLUSIONS AND FUTURE WORK", "text": "In this paper, we propose an approach to constructing and deciphering burst information networks constructed from foreign languages as a novel and unique method of aligning streaming data. For the first time, we propose to model translation pair mining of non-parallel corpora as a network decryption problem. Based on the characteristics of co-bursts across languages, we propose 4 new and effective clues for deciphering. Our approach is language-independent, efficient, effective, and easy to implement, so it is useful for the infinite acquisition of language knowledge, machine translation, and cross-language information retrieval systems. In the future, we plan to introduce richer graph-based features (e.g. metapaths [4]). This self-promoting framework can also improve the extraction and acquisition of translations mutually and demonstrate a successful common inference process across different knowledge sources from multiple languages."}, {"heading": "7. REFERENCES", "text": "[1] A. Klementiev and C. Callison-Burch. Predictingsalient updates for disaster summarization. In ACL, 2015. [2] X. Wang et al. Mining correlated bursty topic patterns from coordinated text streams. In KDD, 2007. [3] S. H. Zheng et al. Cross-lingual topic alignment in time series Japanese / chinese news. In PACLIC, 2012. [4] J. Han et al. Mining heterogeneous information networks. In KDD, 2010. [5] Q. Li et al. Constructing information networks using one single model. In EMNLP, 2014. Bursty and hierarchical structure in streams. Data Mining and Knowledge Discovery, 2003. [7] X. Zhao et al. A novel-burst-based text representation model for scalable event detection. In ACL, 2012."}], "references": [{"title": "Predicting salient updates for disaster summarization", "author": ["A. Klementiev", "C. Callison-Burch"], "venue": "In ACL,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Mining correlated bursty topic patterns from coordinated text streams", "author": ["X. Wang"], "venue": "In KDD,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2007}, {"title": "Cross-lingual topic alignment in time series japanese/chinese news", "author": ["S.H. Zheng"], "venue": "In PACLIC,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "Mining heterogeneous information networks", "author": ["J. Han"], "venue": "In KDD,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Constructing information networks using one single model", "author": ["Q. Li"], "venue": "In EMNLP,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "Bursty and hierarchical structure in streams", "author": ["J. Kleinberg"], "venue": "Data Mining and Knowledge Discovery,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2003}, {"title": "A novel burst-based text representation model for scalable event detection", "author": ["X. Zhao"], "venue": "In ACL,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "A graph analytical approach for topic detection", "author": ["H. Sayyadi", "L. Raschid"], "venue": "ACM Transactions on Internet Technology (TOIT),", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "Learning a translation lexicon from monolingual corpora", "author": ["P. Koehn", "K. Knight"], "venue": "In ACL workshop on Unsupervised lexical acquisition,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2002}, {"title": "Supervised bilingual lexicon induction with multiple monolingual signals", "author": ["A. Irvine", "C. Callison-Burch"], "venue": "In NAACL,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Deciphering foreign language", "author": ["S. Ravi et al", "K. Knight"], "venue": "In ACL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2011}, {"title": "Large scale decipherment for out-of-domain machine translation", "author": ["Q. Dou"], "venue": "In EMNLP,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2012}, {"title": "Beyond parallel data: Joint word alignment and decipherment improves machine translation", "author": ["Q. Dou"], "venue": "In EMNLP,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Mining name translations from comparable corpora by creating bilingual information networks. In the 2nd Workshop on Building and Using Comparable Corpora: from Parallel to Non-parallel Corpora", "author": ["H. Ji"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2009}, {"title": "Unsupervised language-independent name translation mining from wikipedia infoboxes", "author": ["W. Lin"], "venue": "EMNLP Workshop on Unsupervised Learning for NLP,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Chinese gigaword", "author": ["D. Graff", "K. Chen"], "venue": "LDC Catalog No.: LDC2003T09, ISBN,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2005}, {"title": "English gigaword", "author": ["D. Graff"], "venue": "Linguistic Data Consortium, Philadelphia,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2003}, {"title": "The Stanford CoreNLP natural language ssing toolkit", "author": ["C.D. et al. Manning"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Improvements in phrase-based statistical machine translation", "author": ["R. Zens", "H. Ney"], "venue": "In HLT-NAACL,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2004}, {"title": "Bilingual lexicon extraction from comparable corpora using label propagation", "author": ["A. Tamura"], "venue": "In EMNLP,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Applying many-to-many alignments and hidden markov models to letter-to-phoneme conversion", "author": ["S. Jiampojamarn"], "venue": "In NAACL,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2007}, {"title": "An ir approach for translating new words from nonparallel and comparable texts", "author": ["P. Fung", "L.Y. Yee"], "venue": "In COLING-ACL,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Inducing translation lexicons via diverse similarity measures and bridge languages", "author": ["C. Schafer", "D. Yarowsky"], "venue": "In CoNLL,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2002}, {"title": "Timemines: Constructing timelines with statistical models of word usage", "author": ["R. Swan", "D. Jensen"], "venue": "In KDD Workshop on Text Mining,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2000}, {"title": "Temporal and social context based burst detection from folksonomies", "author": ["J. Yao"], "venue": "In AAAI,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2010}, {"title": "Location-based burst detection algorithm in spatiotemporal document stream", "author": ["K. Tamura", "H. Kitakami"], "venue": "In DMIN,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Parameter free bursty events detection in text streams", "author": ["G.P.C. Fung"], "venue": "In VLDB,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2005}, {"title": "Using burstiness to improve clustering of topics in news streams", "author": ["Q. He"], "venue": "In ICDM,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Mining common topics from multiple asynchronous text streams", "author": ["X. Wang"], "venue": "In WSDM,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2009}, {"title": "Finding bursty topics from microblogs", "author": ["Q. Diao"], "venue": "In ACL,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "A probabilistic model for bursty topic discovery in microblogs", "author": ["X. Yan"], "venue": "In AAAI,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2015}, {"title": "Automatic identification of word translations from unrelated english and german corpora", "author": ["R. Rapp"], "venue": "In ACL,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Mining new word translations from comparable corpora", "author": ["L. Shao", "H.T. Ng"], "venue": "In COLING,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2004}, {"title": "Translation discovery using diverse similarity measures", "author": ["Charles F Schafer III"], "venue": "Johns Hopkins University,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2006}, {"title": "Improving named entity translation by exploiting comparable and parallel corpora", "author": ["A. Hassan"], "venue": "In RANLP,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2007}, {"title": "Learning bilingual lexicons from monolingual corpora", "author": ["A. Haghighi"], "venue": "In ACL,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2008}, {"title": "Mint: A method for effective and scalable mining of named entity transliterations from large comparable corpora", "author": ["R. Udupa"], "venue": "In EACL,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Bilingual lexicon induction for low-resource languages", "author": ["A. Klementiev", "C. Callison-Burch"], "venue": "In JHU Technical Report,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2010}, {"title": "Discriminative bilingual lexicon induction", "author": ["A. Irvine", "C. Callison-Burch"], "venue": "Computational Linguistics,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2015}, {"title": "Visual bilingual lexicon induction with transferred convnet features", "author": ["D. Kiela"], "venue": "In EMNLP,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Named entity transliteration with comparable corpora", "author": ["R. Sproat"], "venue": "In ACL,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Weakly supervised named entity transliteration and discovery from multilingual comparable corpora", "author": ["A. Klementiev", "D. Roth"], "venue": "In COLING-ACL,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2006}, {"title": "Mining named entity transliteration equivalents from comparable corpora", "author": ["R. Udupa"], "venue": "In CIKM,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2008}, {"title": "Mining name translations from entity graph mapping", "author": ["G. You"], "venue": "In EMNLP,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2010}, {"title": "Mining named entities with temporally correlated bursts from multilingual web news streams", "author": ["A. Kotov"], "venue": "In WSDM,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2011}, {"title": "Mining named entity translation from non parallel corpora", "author": ["R. Sellami"], "venue": "In FLAIRS,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 2014}, {"title": "Cross-lingual annotation projection for semantic roles", "author": ["S. Pado", "M. Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2009}, {"title": "Multi-source transfer of delexicalized dependency parsers", "author": ["R. McDonald"], "venue": "In EMNLP,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Streaming data is an important source for many applications such as disaster summarization [1].", "startOffset": 91, "endOffset": 94}, {"referenceID": 1, "context": "Nonetheless, there are many text streams that are topically related and indexed by the same set of time points, which are called coordinated text streams [2].", "startOffset": 154, "endOffset": 157}, {"referenceID": 1, "context": "For example, [2] and [3] tried to discover common topic patterns from coordinated text streams.", "startOffset": 13, "endOffset": 16}, {"referenceID": 2, "context": "For example, [2] and [3] tried to discover common topic patterns from coordinated text streams.", "startOffset": 21, "endOffset": 24}, {"referenceID": 1, "context": "Most previous work on coordinated text stream mining such as [2] analyzed the streams on the coarse-grained topic level, which inevitably leads to loss of some information.", "startOffset": 61, "endOffset": 64}, {"referenceID": 3, "context": "Similar to traditional Information Networks [4, 5], nodes within a community in BINets are semantically related (e.", "startOffset": 44, "endOffset": 50}, {"referenceID": 4, "context": "Similar to traditional Information Networks [4, 5], nodes within a community in BINets are semantically related (e.", "startOffset": 44, "endOffset": 50}, {"referenceID": 5, "context": ", [6]) detect a word\u2019s burst states mainly based on comparing a word\u2019s probability q at a time epoch t to its base probablity q0 that is the word\u2019s global probability throughout the stream.", "startOffset": 2, "endOffset": 5}, {"referenceID": 6, "context": "In this paper, we use the approach of [7] which is a variant of [6] for burst detection.", "startOffset": 38, "endOffset": 41}, {"referenceID": 5, "context": "In this paper, we use the approach of [7] which is a variant of [6] for burst detection.", "startOffset": 64, "endOffset": 67}, {"referenceID": 4, "context": ", [5]) relied on a large number of linguistic resources and labeled data to construct information networks.", "startOffset": 2, "endOffset": 5}, {"referenceID": 7, "context": "[8] proposed a co-occurrence based approach to build an information network where each node is a word type (i.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": ", [9, 10]), decipherment (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 9, "context": ", [9, 10]), decipherment (e.", "startOffset": 2, "endOffset": 9}, {"referenceID": 10, "context": ", [11, 12, 13]) and name translation mining (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": ", [11, 12, 13]) and name translation mining (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", [11, 12, 13]) and name translation mining (e.", "startOffset": 2, "endOffset": 14}, {"referenceID": 13, "context": ", [14, 15]), we utilize a few linguistic resources - a bi-lingual lexicon and language-universal representations such as time/calendar date, number, website URL, currency and emoticons to decipher a subset of Chinese nodes.", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [14, 15]), we utilize a few linguistic resources - a bi-lingual lexicon and language-universal representations such as time/calendar date, number, website URL, currency and emoticons to decipher a subset of Chinese nodes.", "startOffset": 2, "endOffset": 10}, {"referenceID": 15, "context": "We use the 2010 Agence France Presse news story articles in Chinese Gigaword [16] and English Gigaword [17] as our non-parallel news stream corpora.", "startOffset": 77, "endOffset": 81}, {"referenceID": 16, "context": "We use the 2010 Agence France Presse news story articles in Chinese Gigaword [16] and English Gigaword [17] as our non-parallel news stream corpora.", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "We removed stopwords, conducted lemmatization and name tagging for the English corpus and did word segmentation and name tagging for the Chinese corpus using the Stanford CoreNLP toolkit [18].", "startOffset": 187, "endOffset": 191}, {"referenceID": 18, "context": "The seed bi-lingual lexicon we used is released by [19] which contains 81,990 Chinese word entries, each of which has an English translation.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "This baseline is similar to the approach of [20].", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "\u2022 Co-burst validation: (cv): Use the co-burst clue only \u2022 pv+tv \u2022 pv+tv+nv \u2022 Bayesian Inference: The state-of-the-art algorithm [12] for language decipherment on non-parallel corpora, deciphering a language based on the alignment of its bi-", "startOffset": 128, "endOffset": 132}, {"referenceID": 11, "context": "After normalization, we run the decipherment algorithm of [12] which outputs the probability distribution of possible candidates given a cipher node (i.", "startOffset": 58, "endOffset": 62}, {"referenceID": 20, "context": "Among top 100 translation pairs, only 9% can be correctly transliterated by a transliteration model [21], demonstrating that our approach can discover large numbers of translation pairs that cannot be transliterated.", "startOffset": 100, "endOffset": 104}, {"referenceID": 19, "context": "Accuracy is used to measure the proportion of the words that are correctly translated, as [20] did.", "startOffset": 90, "endOffset": 94}, {"referenceID": 21, "context": "Context is proposed by [22] which extracts translation pairs based on context similarity of words.", "startOffset": 23, "endOffset": 27}, {"referenceID": 22, "context": "Diverse is proposed by [23], which is a variant of [22] by adding pinyin and temporal similarity.", "startOffset": 23, "endOffset": 27}, {"referenceID": 21, "context": "Diverse is proposed by [23], which is a variant of [22] by adding pinyin and temporal similarity.", "startOffset": 51, "endOffset": 55}, {"referenceID": 19, "context": "CoLP and SimLP are label propagation methods proposed by [20] on word co-occurrence and similarity graph respectively.", "startOffset": 57, "endOffset": 61}, {"referenceID": 11, "context": "Bayesian is a language decipherment model by [12].", "startOffset": 45, "endOffset": 49}, {"referenceID": 23, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 5, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 24, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 25, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 6, "context": "[24, 6, 25, 26, 7] studied burst detection problem from text streams.", "startOffset": 0, "endOffset": 18}, {"referenceID": 23, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 26, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 1, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 27, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 28, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 29, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 6, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 30, "context": "[24, 27, 2, 28, 29, 30, 7, 31] employed burst information to find bursty topics and events.", "startOffset": 0, "endOffset": 30}, {"referenceID": 21, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 31, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 8, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 22, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 32, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 33, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 34, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 35, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 36, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 37, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 19, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 9, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 38, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 39, "context": ", [22, 32, 9, 23, 33, 34, 35, 36, 37, 38, 20, 10, 39, 40]) and name translation mining (e.", "startOffset": 2, "endOffset": 57}, {"referenceID": 40, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 41, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 42, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 13, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 43, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 44, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 14, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 45, "context": ", [41, 42, 43, 14, 44, 45, 15, 46]) from non-parallel corpora.", "startOffset": 2, "endOffset": 34}, {"referenceID": 46, "context": "In contrast to previous cross-lingual projection work like data transfer [47] and model transfer [48], we do not require any parallel data.", "startOffset": 73, "endOffset": 77}, {"referenceID": 47, "context": "In contrast to previous cross-lingual projection work like data transfer [47] and model transfer [48], we do not require any parallel data.", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "Our BINet construction method was inspired by [7, 8].", "startOffset": 46, "endOffset": 52}, {"referenceID": 7, "context": "Our BINet construction method was inspired by [7, 8].", "startOffset": 46, "endOffset": 52}, {"referenceID": 10, "context": ", [11, 12, 13]) to graph structures instead of sequence data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 11, "context": ", [11, 12, 13]) to graph structures instead of sequence data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 12, "context": ", [11, 12, 13]) to graph structures instead of sequence data.", "startOffset": 2, "endOffset": 14}, {"referenceID": 40, "context": "Another work related to ours is [41, 45], which used phonetic transliteration and frequency correlation to discover transliteration of entities.", "startOffset": 32, "endOffset": 40}, {"referenceID": 44, "context": "Another work related to ours is [41, 45], which used phonetic transliteration and frequency correlation to discover transliteration of entities.", "startOffset": 32, "endOffset": 40}, {"referenceID": 3, "context": ", meta-paths [4]).", "startOffset": 13, "endOffset": 16}], "year": 2016, "abstractText": "Aligning coordinated text streams from multiple sources and multiple languages has opened many new research venues on cross-lingual knowledge discovery. In this paper we aim to advance state-of-the-art by: (1). extending coarse-grained topic-level knowledge mining to fine-grained information units such as entities and events; (2). following a novel \u201cDatato-Network-to-Knowledge (D2N2K)\u201d paradigm to construct and utilize network structures to capture and propagate reliable evidence. We introduce a novel Burst Information Network (BINet) representation that can display the most important information and illustrate the connections among bursty entities, events and keywords in the corpus. We propose an effective approach to construct and decipher BINets, incorporating novel criteria based on multi-dimensional clues from pronunciation, translation, burst, neighbor and graph topological structure. The experimental results on Chinese and English coordinated text streams show that our approach can accurately decipher the nodes with high confidence in the BINets and that the algorithm can be efficiently run in parallel, which makes it possible to apply it to huge amounts of streaming data for never-ending language and information decipherment.", "creator": "LaTeX with hyperref package"}}}