{"id": "1609.08281", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "An Efficient Method for Robust Projection Matrix Design", "abstract": "The aim of this brief is to design a robust projection matrix for the Compressive Sensing (CS) system when the signal is not exactly sparse. The optimal projection matrix design is obtained by minimizing the Frobenius norm of the difference between the identity matrix and the Gram matrix of the equivalent dictionary $\\Phi\\Psi$. A novel penalty $\\|\\Phi\\|_F$ is added to make the projection matrix robust when the sparse representation error (SRE) exists. Additionally, designing the projection matrix with a high dimensional dictionary improves the signal reconstruct accuracy when the compression rate is the same as in a low dimensional dictionary scenario. Simulation results demonstrate the effectiveness of the proposed approach comparing with the state-of-the-art methods.", "histories": [["v1", "Tue, 27 Sep 2016 06:59:11 GMT  (75kb)", "https://arxiv.org/abs/1609.08281v1", "1 figure, 4 tables"], ["v2", "Thu, 19 Jan 2017 17:01:06 GMT  (177kb)", "http://arxiv.org/abs/1609.08281v2", "3 figures, 4 tables"], ["v3", "Wed, 6 Sep 2017 17:53:44 GMT  (6540kb,D)", "http://arxiv.org/abs/1609.08281v3", "8 figures, 4 tables"]], "COMMENTS": "1 figure, 4 tables", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["tao hong", "zhihui zhu"], "accepted": false, "id": "1609.08281"}, "pdf": {"name": "1609.08281.pdf", "metadata": {"source": "CRF", "title": "An Efficient Method for Robust Projection Matrix Design", "authors": ["Tao Honga", "Zhihui Zhub"], "emails": ["hongtao@cs.technion.ac.il", "zzhu@mines.edu"], "sections": [{"heading": null, "text": "Our goal is to efficiently design a robust projection matrix for Compressive Sensing (CS) systems when we apply it to signals that are not particularly sparse. The optimal projection matrix is achieved mainly by minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a new streamlining function independent of a particular SRE matrix. Simulation results show the efficiency and effectiveness of the proposed approach compared to the most modern methods for image processing using a conventional waveword dictionary in which the SRE matrix is generally not available. Simulation results show the efficiency and effectiveness of the proposed approach compared to the proposed state-of-the-art methods for image processing, in addition, we can demonstrate highly natural compression compression formation with a highly compressive system in high dimensions."}, {"heading": "1. Introduction", "text": "Since the beginning of this century, we have had a great deal of attention (1) - [6] - [6] - (6] - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (8) - (6) - (8) - (8) - (6) - (8) - (8) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - (6) - - (6) - (6) - (6) - (6) - (6) - (6) - (6) (6) (6) (6) - (6 - (6) (6) - (6 - (6) - (6 - (6) - (6) - (6) - (6 - (6) - (6 - (6) - (6) - (6 - (6) - (6) - (6 - (6) - (6) - (6 - (6) - (6) - (6 - (6) - (6) - (6 - (6) - (6 - (6) - (6 - (6) - (6) - (6 - (6) - (6 - (6) - (6) - (6 - (6) - (6 - (6) - (6 - (6) - (6) - (6 - (6) - - (6 - (6) - (6 - (6) - (6 - (6) - (6 - (6) - (6 - (6) - (6) - (6 - (6) - (6) - (6 - (6) - (6 - (6) - (6 - (6) - (6) - (6) - (6 - (6 - (6) - (6) (6 - (6) - (6) - (6 - (6) - (6) - (6) - (6) - (6) - (6) - ("}, {"heading": "2. Preliminaries", "text": "It is not as if the SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-SRE-"}, {"heading": "3. A Novel Approach to Projection Matrix Design", "text": "In this section, we offer an efficient robust sensor matrix design approach that reduces the requirement for training signals and the associated SRE matrix E. In fact, our proposed framework is inspired by two aspects: (8) and (9)."}, {"heading": "3.1. A Novel Framework for Robust Projection Matrix Design", "text": "First, that the energy of SRE-E-2F is usually very small, since the learned frugal dictionary hardly represents a signal as in (2). Otherwise, if it is very large, it means that the dictionary is not well designed for this class of signals, and it is possible that this signal class cannot be recovered from the compressing metrics, no matter which projection matrix is used. It follows from the normative consistent property that the dictionary is not well designed for this class of signals, and it informally implies that a smaller sensory matrix (2) produces a smaller projected SRE matrix. Also, as illustrated before the amount of training data should be sufficient so that they can represent the class of desired signals, the energy in the corresponding SRE matrix E should spread across all elements. In other words, one can represent the expected SRE distribution as noise additives."}, {"heading": "4. Simulation Results", "text": "In this section, we will conduct a series of experiments with synthetic data and natural images to demonstrate the performance of the CS system using the projection matrix designed by the proposed methods. For simplicity, the corresponding CS systems will be designated by CSMT with the sampling matrix obtained via (15) and CSMT -ET F with the sampling matrix obtained via (16) and compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sampling matrix obtained via (8), CSLH -ET F with the sampling matrix obtained via (9) [15]. It was first proposed in [28] that the simultaneous optimization of E and E for a CS system will lead to better performance with respect to SRA. Subsequently, we will also examine this strategy in natural images and the associated CS system will be determined by CSS \u2212 Dk \u2212 the simplicity of the SRP signal for use \u2212 22."}, {"heading": "A. Synthetic Data Experiments", "text": "We also generate a random M \u00b7 N matrix \u03a60 (where each entry is the Gaussian distribution of zero mean and variance 1) as the starting condition for all the above projection matrices (where each coupling factor used in CSS \u2212 DCS is set to 0.5, which is the best value in our setting). In general, CSS \u2212 DCS should perform best in terms of SRA because it optimizes the projection matrix and dictionality. Thus, the performance of CSS \u2212 DCS serves as an indicator of the best performance of other CS systems that consider only optimizing the projection matrix."}, {"heading": "B. Natural Images Experiments", "text": "In this section, three experiments with higher dimensions and a projection are performed, through which we verify the effectiveness of the proposed frame for robust sensor matrix construction in Section 3 and show the reason for the lowering of the requirement for the SRE matrix E. As we have already explained, since the SRE is relatively large for natural images, we compare the performance of CSMT and CSLH when a set of training signals and the associated SRE matrix E are available. In the second series of experiments, we only show the results for CSMT and CSLH. The performance of CSMT and CSLH is compared when a set of training signals and the corresponding SRE matrix is available."}, {"heading": "5. Conclusion", "text": "This paper looks at the problem of designing a robust projection matrix for the signals that are not exactly sparse. A novel cost function is proposed to reduce the influence of SRE for the measurements and is at the same time independent of training data and the corresponding SRE matrix (independence of training data saves calculations for practical interpretation).As shown in Lemma 1, we find that discarding the SRE matrix when designing the procedure is reasonable, as it corresponds to the case when we have an infinite number of training samples. Conversely, we use the SRE-2F as a replacement for the projected SRE-2F to design the sensor matrices.The performance of designing projection matrices with dictionaries that we have either learned in large training data sets or of high dimension.The simulation results on synthetic data and natural images show the effectiveness and efficiency of the proposed approach."}, {"heading": "Acknowledgment", "text": "This research is supported in part by ERC Funding Agreement No. 320649 and in part by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).The code in this paper representing the experiments can be downloaded from https: / / github.com / happyhongt /."}], "references": [{"title": "Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,", "author": ["E.J. Cand\u00e8s", "J. Romberg", "T. Tao"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2006}, {"title": "Near optimal signal recovery from random projections: Universal encoding strategies,", "author": ["E.J. Cand\u00e8s", "T. Tao"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "Compressed sensing,", "author": ["D.L. Donoho"], "venue": "IEEE Trans. Inf. Theory,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2006}, {"title": "An introduction to compressive samping,", "author": ["E.J. Cand\u00e8s", "M.B. Wakin"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2008}, {"title": "Sparse and Redundant Representations: from theory to applications in signal and image processing", "author": ["M. Elad"], "venue": "Springer Science & Business Media", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "`1-`2 Optimization in Signal and Image Processing,", "author": ["M. Zibulevsky", "M. Elad"], "venue": "IEEE Signal Process. Mag", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "Learning incoherent dictionaries for sparse approximation using iterative projections and rotations,", "author": ["D. Barchiesi", "M.D. Plumbley"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "and A", "author": ["G. Li", "Z. Zhu", "H. Bai"], "venue": "Yu, \u201cA new framework for designing incoherent sparsifying dictionaries,\u201d in IEEE Conf. Acous., Speech, Signal Process.(ICASSP), pp. 4416-4420", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2017}, {"title": "Optimized projections for compressed sensing,", "author": ["M. Elad"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2007}, {"title": "A gradient-based alternating minimzation approach for optimization of the measurement matrix in compressive sensing,", "author": ["V. Abolghasemi", "S. Ferdowsi", "S. Sanei"], "venue": "Signal Process.,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Design of projection matrix for compressive sensing by nonsmooth optimization,", "author": ["W.-S. Lu", "T. Hinamoto"], "venue": "IEEE International Symposium Circuits and Systems (ISCAS),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Projection matrix optimization for block-sparse compressive sensing,", "author": ["S. Li", "Z. Zhu", "G. Li", "L. Chang", "Q. Li"], "venue": "IEEE Conf. Signal Process., Communicaton and Computation (ICSPCC),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "On projection matrix optimization for compressive sensing systems,", "author": ["G. Li", "Z.H. Zhu", "D.H. Yang", "L.P. Chang", "H. Bai"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Designint robust sensing matrix for image compression,", "author": ["G. Li", "X. Li", "S. Li", "H. Bai", "Q. Jiang", "X. He"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "An efficient algorithm for designing projection matrix in compressive sensing based on alternating optimization,", "author": ["T. Hong", "H. Bai", "S. Li", "Z. Zhu"], "venue": "Signal Process.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Approximating sampled sinusoids and multiband signals using multiband modulated DPSS dictionaries,", "author": ["Z. Zhu", "M.B. Wakin"], "venue": "J. Fourier Analysis Appl.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Dictionary Learning,", "author": ["I. Tosic", "P. Frossard"], "venue": "IEEE Signal Process. Mag.,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2011}, {"title": "K-SVD: An algorithm for designing overcomplete dictionaries for sparse representation,", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2006}, {"title": "Hakon-housoy, \u201cMethod of optimal direction for frame design,", "author": ["K. Engan", "J.H.S.O. Aase"], "venue": "Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.(ICASSP),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1999}, {"title": "Online algorithms and stochastic approximations,", "author": ["L. Botou"], "venue": "Online Learning and Neural Networks,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1998}, {"title": "J", "author": ["J. Mairal", "F. Bach"], "venue": "Ponce and G. Sapiro, \u201cOnline dictionary learning for sparse coding,\u201d Proceedings of the 26th annual international conference on machine learning ACM (ICML), pp. 689-696", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2009}, {"title": "Online learning for matrix factorization and sparse coding,", "author": ["J. Mairal", "F. Bach", "J. Ponce", "G. Sapiro"], "venue": "Journal of Machine Learning,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "and M", "author": ["Z. Zhu", "Q. Li", "G. Tang"], "venue": "B. Wakin, \u201cGlobal Optimality in Lowrank Matrix Optimization,\u201d arXiv preprint, arXiv:1702.07945", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "minFunc: unconstrained differentiale multivariate optimization in Matlab.", "author": ["M. Schmidt"], "venue": "https://www.cs.ubc.ca/ \u0303schmidtm/ Software/minFunc.html,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Trainlets: dictionary learning in high dimensions,", "author": ["J. Sulam", "B. Ophir", "M. Zibulevsky", "M. Elad"], "venue": "IEEE Trans. Signal Process.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2016}, {"title": "Large inpainting of face images with trainlets,", "author": ["J. Sulam", "M. Elad"], "venue": "IEEE Signal Processing Letter,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}, {"title": "Learning to sense sparse sig-  nals: simultaneous sensing matrix ans sparsifying dictionary optimization,", "author": ["J.M. Duarte-Carvajalino", "G. Sapiro"], "venue": "IEEE Trans. Image Process.,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2009}, {"title": "Statistical Inference", "author": ["G. Casella", "L.B. Roger"], "venue": "Vol. 2. Pacific Grove, CA: Duxbury", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "Numerical Optimization", "author": ["J. Nocedal", "S. Wright"], "venue": "Springer", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2006}, {"title": "LabelMe: A Database and Web-Based Tool for Image Annotation,", "author": ["B.C. Russell", "A. Torralba", "K.P. Murphy", "W.T. Freeman"], "venue": "International Journal of Computation Vision,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2008}, {"title": "M", "author": ["R. Rubinstein"], "venue": "Zibulevsky and M. Elad, \u201cEfficient implementation of the K-SVD algorithm and the Batch-OMP method,\u201d Department of Computer Science, Technion, Israel, Tech. Rep.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "B", "author": ["A. Mensch", "J. Mairal"], "venue": "Thirion and G. Varoquaux, \u201cDictionary learning for massive matrix factorization,\u201d Proceedings of the 33th annual international conference on machine learning ACM (ICML)", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 13, "context": "In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix.", "startOffset": 107, "endOffset": 111}, {"referenceID": 14, "context": "In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix.", "startOffset": 112, "endOffset": 116}, {"referenceID": 0, "context": "Since the beginning of this century, Compressive Sensing or Compressed Sensing (CS) has received a great deal of attention [1] - [6].", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "We refer the reader to [5] [6] and the references therein to find the related topics mentioned above.", "startOffset": 23, "endOffset": 26}, {"referenceID": 4, "context": "All of the methods can be found in [5] [7] and the references therein.", "startOffset": 35, "endOffset": 38}, {"referenceID": 5, "context": "All of the methods can be found in [5] [7] and the references therein.", "startOffset": 39, "endOffset": 42}, {"referenceID": 4, "context": "As shown in [5], when there is no projection noise (i.", "startOffset": 12, "endOffset": 15}, {"referenceID": 6, "context": "Thus, [8] [9] proposed methods to design a dictionary with small mutual coherence.", "startOffset": 6, "endOffset": 9}, {"referenceID": 7, "context": "Thus, [8] [9] proposed methods to design a dictionary with small mutual coherence.", "startOffset": 10, "endOffset": 13}, {"referenceID": 8, "context": "There has been much effort [10] - [14] devoted to designing an optimal \u03a6 that outperforms the widely used random matrix in terms of signal recovery accuracy (SRA).", "startOffset": 27, "endOffset": 31}, {"referenceID": 12, "context": "There has been much effort [10] - [14] devoted to designing an optimal \u03a6 that outperforms the widely used random matrix in terms of signal recovery accuracy (SRA).", "startOffset": 34, "endOffset": 38}, {"referenceID": 8, "context": "It is experimentally observed that the sensing matrix designed by [10] - [14] based on mutual coherence results", "startOffset": 66, "endOffset": 70}, {"referenceID": 12, "context": "It is experimentally observed that the sensing matrix designed by [10] - [14] based on mutual coherence results", "startOffset": 73, "endOffset": 77}, {"referenceID": 13, "context": "To address this issue, the recent work in [15] [16] proposed novel methods to design a robust projection matrix when the SRE exists.", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "To address this issue, the recent work in [15] [16] proposed novel methods to design a robust projection matrix when the SRE exists.", "startOffset": 47, "endOffset": 51}, {"referenceID": 13, "context": "1 Through this paper, similar to what is used in [15] [16], a robust projection (or sensing) matrix means it is designed with consideration of possible SRE and hence the corresponding CS system yields superior performance when the SRE e in (2) is not nil.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "1 Through this paper, similar to what is used in [15] [16], a robust projection (or sensing) matrix means it is designed with consideration of possible SRE and hence the corresponding CS system yields superior performance when the SRE e in (2) is not nil.", "startOffset": 54, "endOffset": 58}, {"referenceID": 13, "context": "However, the approaches in [15] [16] need the explicit value of the SRE on the training dataset, making them inefficient in several aspects.", "startOffset": 27, "endOffset": 31}, {"referenceID": 14, "context": "However, the approaches in [15] [16] need the explicit value of the SRE on the training dataset, making them inefficient in several aspects.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": ", the wavelet dictionary, and the modulated discrete prolate spheroidal sequences (DPSS) dictionary for sampled multiband signals [17]) actually do not involve any training dataset and hence no SRE available.", "startOffset": 130, "endOffset": 134}, {"referenceID": 13, "context": "1We note that the approaches considered in [15] [16] share the same framework.", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "1We note that the approaches considered in [15] [16] share the same framework.", "startOffset": 48, "endOffset": 52}, {"referenceID": 14, "context": "The difference is that in [16] the authors utilized an efficient iterative algorithm giving an approximate solution, while a closed form solution is derived in [15].", "startOffset": 26, "endOffset": 30}, {"referenceID": 13, "context": "The difference is that in [16] the authors utilized an efficient iterative algorithm giving an approximate solution, while a closed form solution is derived in [15].", "startOffset": 160, "endOffset": 164}, {"referenceID": 13, "context": "In order to design the robust projection matrix for these CS systems using the framework presented in [15] [16], one has to first construct plenty of extra representative dataset for the explicit SRE with the given dictionary, which limits the range of applications.", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "In order to design the robust projection matrix for these CS systems using the framework presented in [15] [16], one has to first construct plenty of extra representative dataset for the explicit SRE with the given dictionary, which limits the range of applications.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "Therefore, the requirement of the explicit value of SRE for the training dataset makes the methods in [15] [16] limited and inefficient for all the cases discussed above.", "startOffset": 102, "endOffset": 106}, {"referenceID": 14, "context": "Therefore, the requirement of the explicit value of SRE for the training dataset makes the methods in [15] [16] limited and inefficient for all the cases discussed above.", "startOffset": 107, "endOffset": 111}, {"referenceID": 13, "context": "We stress that by efficient method for robust projection matrix design (which is the title of this paper), we are not providing an efficient method for solving the problems in [15] [16]; instead we provide a new framework in which the training dataset and its corresponding SRE are not required any more.", "startOffset": 176, "endOffset": 180}, {"referenceID": 14, "context": "We stress that by efficient method for robust projection matrix design (which is the title of this paper), we are not providing an efficient method for solving the problems in [15] [16]; instead we provide a new framework in which the training dataset and its corresponding SRE are not required any more.", "startOffset": 181, "endOffset": 185}, {"referenceID": 13, "context": "Experiments on synthetic data and real images demonstrate the proposed sensing matrix yields a comparable performance in terms of SRA compared with the ones obtained by [15] [16].", "startOffset": 169, "endOffset": 173}, {"referenceID": 14, "context": "Experiments on synthetic data and real images demonstrate the proposed sensing matrix yields a comparable performance in terms of SRA compared with the ones obtained by [15] [16].", "startOffset": 174, "endOffset": 178}, {"referenceID": 16, "context": "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].", "startOffset": 52, "endOffset": 56}, {"referenceID": 17, "context": "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].", "startOffset": 150, "endOffset": 154}, {"referenceID": 18, "context": "which can be addressed by some practical algorithms [18], among which the popularly utilized are the K-singular value decomposition (K-SVD) algorithm [19] and the method of optimal direction (MOD) [20].", "startOffset": 197, "endOffset": 201}, {"referenceID": 13, "context": "The recent work in [15] [16] attempted to design a robust projection matrix with consideration of the SRE matrix E and proposed to solve", "startOffset": 19, "endOffset": 23}, {"referenceID": 14, "context": "The recent work in [15] [16] attempted to design a robust projection matrix with consideration of the SRE matrix E and proposed to solve", "startOffset": 24, "endOffset": 28}, {"referenceID": 13, "context": "See [15] [16] for more discussions on this issue.", "startOffset": 4, "endOffset": 8}, {"referenceID": 14, "context": "See [15] [16] for more discussions on this issue.", "startOffset": 9, "endOffset": 13}, {"referenceID": 13, "context": "As stated in [15] [16], these methods ((8) and (9)) can be applied naturally when the dictionary is learned by algorithms like K-SVD with plenty of training data {xk}, since the SRE E is available without any additional effort.", "startOffset": 13, "endOffset": 17}, {"referenceID": 14, "context": "As stated in [15] [16], these methods ((8) and (9)) can be applied naturally when the dictionary is learned by algorithms like K-SVD with plenty of training data {xk}, since the SRE E is available without any additional effort.", "startOffset": 18, "endOffset": 22}, {"referenceID": 19, "context": "To train such a dictionary, we have to conduct online algorithms [21] - [23] which typically apply stochastic gradient method where in each iteration a randomly selected tiny part of the training signals called mini-batch instead of the whole data is utilized for computing the expected gradient.", "startOffset": 65, "endOffset": 69}, {"referenceID": 21, "context": "To train such a dictionary, we have to conduct online algorithms [21] - [23] which typically apply stochastic gradient method where in each iteration a randomly selected tiny part of the training signals called mini-batch instead of the whole data is utilized for computing the expected gradient.", "startOffset": 72, "endOffset": 76}, {"referenceID": 13, "context": "All of these situations make the approach proposed in [15] [16] become limited.", "startOffset": 54, "endOffset": 58}, {"referenceID": 14, "context": "All of these situations make the approach proposed in [15] [16] become limited.", "startOffset": 59, "endOffset": 63}, {"referenceID": 24, "context": "The recent work in [26] [27] stated that a dictionary learned with larger patches (e.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "The recent work in [26] [27] stated that a dictionary learned with larger patches (e.", "startOffset": 24, "endOffset": 28}, {"referenceID": 17, "context": "2The dimension of a dictionary in such a case becomes high compared with the moderate dictionary size shown in [19].", "startOffset": 111, "endOffset": 115}, {"referenceID": 27, "context": "Thus, by the law of large numbers [29], the average \u2016\u03a6E\u2016 2 F P converges in probability and almost surely to the expected value \u03c3\u2016\u03a6\u2016F as P\u2192 \u221e.", "startOffset": 34, "endOffset": 38}, {"referenceID": 27, "context": "Finally, the central limit theorem [29] establishes that as P ap-", "startOffset": 35, "endOffset": 39}, {"referenceID": 22, "context": "The recent work [24] has shown that a number of iterative algorithms (including gradient descent) can provably solve the factored problem (i.", "startOffset": 16, "endOffset": 20}, {"referenceID": 28, "context": "the Conjugate-Gradient (CG) [30] method is utilized to solve (17).", "startOffset": 28, "endOffset": 32}, {"referenceID": 23, "context": "\u2207\u03a6\u0303 f (\u03a6\u0303,G) = 2\u03bb\u03a6\u0303\u22124\u03a6\u0303\u03a8G\u03a8 T +4\u03a6\u0303\u03a8\u03a8T \u03a6\u0303 \u03a6\u0303\u03a8\u03a8T (19) After obtaining the gradient of f (\u03a6\u0303,G), the toolbox minFunc4 [25] is utilized to solve (17) with CG method.", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].", "startOffset": 73, "endOffset": 77}, {"referenceID": 9, "context": "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 13, "context": "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].", "startOffset": 83, "endOffset": 87}, {"referenceID": 14, "context": "A widely used strategy for such problems is the alternating minimization [10] [11] [15] [16].", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "3We note that both of the methods shown in [15] [16] for solving (17) need to calculate the inversion of \u03a8\u03a8T .", "startOffset": 43, "endOffset": 47}, {"referenceID": 14, "context": "3We note that both of the methods shown in [15] [16] for solving (17) need to calculate the inversion of \u03a8\u03a8T .", "startOffset": 48, "endOffset": 52}, {"referenceID": 22, "context": "Thus, as global convergence of many local search algorithms for solving similar low-rank optimizations is guaranteed in [24], CG is chosen to solve (17).", "startOffset": 120, "endOffset": 124}, {"referenceID": 13, "context": "Obviously, if the aforementioned problem does not happen in practical cases, the method in [15] [16] can be used to address (15).", "startOffset": 91, "endOffset": 95}, {"referenceID": 14, "context": "Obviously, if the aforementioned problem does not happen in practical cases, the method in [15] [16] can be used to address (15).", "startOffset": 96, "endOffset": 100}, {"referenceID": 13, "context": "Moreover, we will show that CG and the methods shown in [15] [16] yield a similar solution in the following experiments.", "startOffset": 56, "endOffset": 60}, {"referenceID": 14, "context": "Moreover, we will show that CG and the methods shown in [15] [16] yield a similar solution in the following experiments.", "startOffset": 61, "endOffset": 65}, {"referenceID": 13, "context": "\u2022 Simulation results with synthetic data and natural images (where the SRE matrix E is available) show that the proposed method also yields a comparable performance to or outperforms the methods in [15] [16] in terms of SRA.", "startOffset": 198, "endOffset": 202}, {"referenceID": 14, "context": "\u2022 Simulation results with synthetic data and natural images (where the SRE matrix E is available) show that the proposed method also yields a comparable performance to or outperforms the methods in [15] [16] in terms of SRA.", "startOffset": 203, "endOffset": 207}, {"referenceID": 13, "context": "For convenience, the corresponding CS systems are denoted by CSMT with \u03a6\u0303 obtained via (15) and CSMT\u2212ET F with \u03a6\u0303 obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH\u2212ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].", "startOffset": 268, "endOffset": 272}, {"referenceID": 13, "context": "For convenience, the corresponding CS systems are denoted by CSMT with \u03a6\u0303 obtained via (15) and CSMT\u2212ET F with \u03a6\u0303 obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH\u2212ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].", "startOffset": 325, "endOffset": 329}, {"referenceID": 26, "context": "For convenience, the corresponding CS systems are denoted by CSMT with \u03a6\u0303 obtained via (15) and CSMT\u2212ET F with \u03a6\u0303 obtained via (16), and are compared with the following CS systems: CSrandn with a random projection matrix, CSLH with the sensing matrix obtained via (8) [15], CSLH\u2212ET F with the sensing matrix obtained via (9) [15], and CSDCS [28].", "startOffset": 341, "endOffset": 345}, {"referenceID": 26, "context": "It was first proposed in [28] that simultaneously optimizing \u03a6 and \u03a8 for a CS system results in better performance in terms of SRA.", "startOffset": 25, "endOffset": 29}, {"referenceID": 4, "context": "The SRA is evaluated in terms of the peak signal-to-noise ratio (PSNR) [5]", "startOffset": 71, "endOffset": 74}, {"referenceID": 13, "context": "We note that a random dictionary with well-conditioned is chosen and thus we also compute the closed-form solution shown in [15] for (15).", "startOffset": 124, "endOffset": 128}, {"referenceID": 13, "context": "This phenomenon is also observed for CSLH and CSLH\u2212ET F in [15] [16].", "startOffset": 59, "endOffset": 63}, {"referenceID": 14, "context": "This phenomenon is also observed for CSLH and CSLH\u2212ET F in [15] [16].", "startOffset": 64, "endOffset": 68}, {"referenceID": 13, "context": "\u2022 It is clear that the sensing matrices obtained via (15) and (16) have at least similar performance to the ones obtained via (8) and (9) [15, 16], though our proposed framework does not utilize the SRE matrix E .", "startOffset": 138, "endOffset": 146}, {"referenceID": 14, "context": "\u2022 It is clear that the sensing matrices obtained via (15) and (16) have at least similar performance to the ones obtained via (8) and (9) [15, 16], though our proposed framework does not utilize the SRE matrix E .", "startOffset": 138, "endOffset": 146}, {"referenceID": 29, "context": "Both training and testing datasets used in these three set of experiments are extracted as follows from the LabelMe database [31].", "startOffset": 125, "endOffset": 129}, {"referenceID": 13, "context": "We perform the same experiment as in [15] to demonstrate the effectiveness of the proposed CS system CSMT without using the SRE E .", "startOffset": 37, "endOffset": 41}, {"referenceID": 13, "context": "Similar to [15], the parameters M, N, L and K are set to 20, 64, 100 and 4, respectively.", "startOffset": 11, "endOffset": 15}, {"referenceID": 13, "context": "Therefore, it is inefficient to utilize the methods in [15] [16] as they require the SRE matrix E .", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "Therefore, it is inefficient to utilize the methods in [15] [16] as they require the SRE matrix E .", "startOffset": 60, "endOffset": 64}, {"referenceID": 20, "context": "The online dictionary learning algorithm in [22] [23] is chosen to train the sparsifying dictionary on the whole Training Data I.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "The online dictionary learning algorithm in [22] [23] is chosen to train the sparsifying dictionary on the whole Training Data I.", "startOffset": 49, "endOffset": 53}, {"referenceID": 13, "context": "\u2022 We compare the computational complexity of our proposed method with the one in [15] [16].", "startOffset": 81, "endOffset": 85}, {"referenceID": 14, "context": "\u2022 We compare the computational complexity of our proposed method with the one in [15] [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "Calculating E involves the OMP algorithm [32] with computational complexity of O ( PKNL(KL logL+K3) ) , where we repeat that P, N, L and K denote the number of samples, the dimension of signal, the number of atoms in dictionary and the sparsity level, respectively.", "startOffset": 41, "endOffset": 45}, {"referenceID": 24, "context": "Inspired by the work in [26], we attempt to design the projection matrix on a high dimensional dictionary in this set of experiments.", "startOffset": 24, "endOffset": 28}, {"referenceID": 24, "context": "As stated in [26], training the dictionary with larger patches results in smaller sparse representation errors for natural images.", "startOffset": 13, "endOffset": 17}, {"referenceID": 31, "context": "\u2022 The recent work in [33] states that it is possible to train the dictionary on millions of training signals whose dimension is also more than one million.", "startOffset": 21, "endOffset": 25}, {"referenceID": 13, "context": "Our proposed framework for designing robust sensing matrices\u2014which shares similar structure to that in [15] [16]\u2014 simultaneously minimizes the surrogate of sparse representation error (SRE) and the mutual coherence of the CS systems.", "startOffset": 103, "endOffset": 107}, {"referenceID": 14, "context": "Our proposed framework for designing robust sensing matrices\u2014which shares similar structure to that in [15] [16]\u2014 simultaneously minimizes the surrogate of sparse representation error (SRE) and the mutual coherence of the CS systems.", "startOffset": 108, "endOffset": 112}], "year": 2017, "abstractText": "Our objective is to efficiently design a robust projection matrix \u03a6 for the Compressive Sensing (CS) systems when applied to the signals that are not exactly sparse. The optimal projection matrix is obtained by mainly minimizing the average coherence of the equivalent dictionary. In order to drop the requirement of the sparse representation error (SRE) for a set of training data as in [15] [16], we introduce a novel penalty function independent of a particular SRE matrix. Without requiring of training data, we can efficiently design the robust projection matrix and apply it for most of CS systems, like a CS system for image processing with a conventional wavelet dictionary in which the SRE matrix is generally not available. Simulation results demonstrate the efficiency and effectiveness of the proposed approach compared with the state-of-the-art methods. In addition, we experimentally demonstrate with natural images that under similar compression rate, a CS system with a learned dictionary in high dimensions outperforms the one in low dimensions in terms of reconstruction accuracy. This together with the fact that our proposed method can efficiently work in high dimension suggests that a CS system can be potentially implemented beyond the small patches in sparsity-based image processing.", "creator": "LaTeX with hyperref package"}}}