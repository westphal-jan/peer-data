{"id": "1609.08286", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Online Unsupervised Multi-view Feature Selection", "abstract": "In the era of big data, it is becoming common to have data with multiple modalities or coming from multiple sources, known as \"multi-view data\". Multi-view data are usually unlabeled and come from high-dimensional spaces (such as language vocabularies), unsupervised multi-view feature selection is crucial to many applications. However, it is nontrivial due to the following challenges. First, there are too many instances or the feature dimensionality is too large. Thus, the data may not fit in memory. How to select useful features with limited memory space? Second, how to select features from streaming data and handles the concept drift? Third, how to leverage the consistent and complementary information from different views to improve the feature selection in the situation when the data are too big or come in as streams? To the best of our knowledge, none of the previous works can solve all the challenges simultaneously. In this paper, we propose an Online unsupervised Multi-View Feature Selection, OMVFS, which deals with large-scale/streaming multi-view data in an online fashion. OMVFS embeds unsupervised feature selection into a clustering algorithm via NMF with sparse learning. It further incorporates the graph regularization to preserve the local structure information and help select discriminative features. Instead of storing all the historical data, OMVFS processes the multi-view data chunk by chunk and aggregates all the necessary information into several small matrices. By using the buffering technique, the proposed OMVFS can reduce the computational and storage cost while taking advantage of the structure information. Furthermore, OMVFS can capture the concept drifts in the data streams. Extensive experiments on four real-world datasets show the effectiveness and efficiency of the proposed OMVFS method. More importantly, OMVFS is about 100 times faster than the off-line methods.", "histories": [["v1", "Tue, 27 Sep 2016 07:10:16 GMT  (1032kb)", "http://arxiv.org/abs/1609.08286v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["weixiang shao", "lifang he", "chun-ta lu", "xiaokai wei", "philip s yu"], "accepted": false, "id": "1609.08286"}, "pdf": {"name": "1609.08286.pdf", "metadata": {"source": "CRF", "title": "Online Unsupervised Multi-view Feature Selection", "authors": ["Weixiang Shao", "Lifang He", "Chun-Ta Lu", "Xiaokai Wei", "Philip S. Yu"], "emails": ["psyu}@uic.edu", "lifanghescut@gmail.com"], "sections": [{"heading": null, "text": "In fact, it is so that most of them are able to surpass themselves by focusing on themselves. (...) In fact, it is so that most of us are able to surpass ourselves. (...) \"It is so, as if.\" (...) \"It is not so, as if.\" (...) \"It is so, as if.\" (...) \"It is so.\" (...) \"\" It is so. \"(...)\" (...) \"(...)\" (...) \"(\") \"(()\" () \"()\" () \"() ()\" () () \"() ()\" () () \"() ()\" () () \"()\" () () \"() ()\" () () () \"() () () () () () () () () () () () () () ()) () () () () () () () () () () () () ()) () () () () () () () () ()) () () () () () () () ()) () () () ()) () () () () () () () ()) () () ()) () () () () () ()) () () ()) () () () () () ()) () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () () () () () () () () () ()"}, {"heading": "II. PRELIMINARIES", "text": "In this section, we briefly describe the problem of multiple unattended feature selection. Afterwards, we introduce some background knowledge about unattended feature selection."}, {"heading": "A. Problem Description", "text": "Before describing the formulation of the problem, we summarize some of the notations used in this paper in Table I. In the course of this work, matrices are written in bold uppercase letters (e.g. M, Rn, m) and vectors in bold lowercase letters (e.g. mi)."}, {"heading": "B. Unsupervised Feature Selection using NMF", "text": "In this thesis we embed the feature selection in an NMF-based cluster algorithm. Suppose we cluster the data in K-clusters, NMF will factor the data matrix X into two non-negative matrices. We denounce the two non-negative matrix factors as U-RN-K + and V-R-K +. The objective function for NMF can be formulated as below: min U, VL = X-UVT-2Fs.t. U-R can be standardized."}, {"heading": "III. METHOD", "text": "The proposed online selection of unattended multi-view functions is based on non-negative matrix factorization and processes the multi-view data piece by piece, aggregating all historical information into small matrices with low computing and memory complexity. We will first describe how the objective function can be derived."}, {"heading": "A. Objective of OMVFS", "text": "Considering the data in nv views {X (v) \u00b7 R N \u00b7 Dv +, v = 1, 2,..., nv (v), we strive to find a feature selection matrix for each view and a consensus cluster indicator matrix that integrates information from all views. (According to the limited NMF framework in Section II-B, we can form the objective function matrix {V (v) \u2212 V (V) nv \u00b2 n \u00b2 n \u00b2 n \u00b2 n (V) \u2212 V \u00b2 -UV (V) 2F (V) s.t. UTU = I, U \u00b2 0, V (V) \u2265 0, 2,..., nv \u00b2, 5), where U is the consensus cluster indicator matrix, V (v) is the feature selection matrix for the v-th view, and \u03b2v is the parameter that controls the thrift of V (v), we should not take advantage of the original structure of the X)."}, {"heading": "B. Optimization", "text": "In the previous section, we derived the objective function of OMVFS. To solve OMVFS, we must first rewrite the optimization problem (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (v), (t), (t, (t), (t, (t), (t, (t), (t, t, (t), (t, t, (t), (t, t, t, (t), (t), (t, (t), (t, t, (t), (t), (t, (t), (t, (t), (t), (t, (t), (t), (t, (t), (t), (t, (t), (t), (t), (t, (t), (t, (t), (t), (t, (t), (t, (t), (t), (t), (t), (t, (t), (t), (t, (t), (t, (t, (t), (t), (t), (t, (t), (t, (t, (t, (t), (t), (t, (t), (t, (t), (t), (t, (t), (t), (t, (t), (t), (t, (t, (t), (t, (t), (t), (t, (t), (t), (t), (t, ("}, {"heading": "C. Further Optimization Using Buffering", "text": "From the update rules (12), we observe that the updating process for U [t] still requires all data and all cluster indicator matrices up to the time t to reside in memory. So, memory usage will continue to grow as more data comes in. Even if we only need to calculate the similarity between the m new instances and all mt instances, the time complexity (v) [v) [t] [t] will still increase as the data is piled up. Worse, the size of L (v) [t] [t] (mt) mt) [t] [t] [t] grows square. To overcome this shortage, we adopt the buffer technique by holding a limited number of samples for each update step. Another advantage of adopting buffering data is that by looking only at the structural information in the buffer, we can capture the concept of data flow."}, {"heading": "IV. ONLINE UNSUPERVISED MULTI-VIEW FEATURE SELECTION", "text": "In this case, it is an 'unforeseen' situation, it is an 'unforeseen' situation, it is an 'unforeseen' situation, it is an 'unforeseen' situation, it is an 'unforeseen' situation, it is an 'unforeseen' situation."}, {"heading": "A. Convergence Analysis", "text": "Although the objective function of the proposed OMVFS at the time t is not jointly convex with U [s, t] and {V (v)}, the alternating optimization strategy used in OMVFS is guaranteed to converge to the local minimum [20]. The proof is similar to that of Theorem 1 in [21], and we omit it. It is important to note that our method essentially applies stochastic gradient lineage [22] to the newly incoming data. As we take a stochastic approach to minimize an objective function written as the sum of differentiable functions, this method is expected to reduce the objective function in each iteration of the new data [23]. Therefore, the same convergence evidence can be adapted to the problem and algorithm design considered here. In the experiments, we also find that the proposed OMVFS method converges within 200 iterations for all data sets used."}, {"heading": "B. Complexity Analysis", "text": "There are two subproblems for the OMVFS algorithm: the optimization of U [s, t] and the optimization of {V (v)}. The calculation costs for updating U [s, t] depend on the calculation in Eq. (20). However, after analyzing the equation, we can determine that the calculation costs for updating U [s, t] O (nvsmDK) are + O (nvs2m2K), where s is the buffer size, m is the size of the data piece, and D is the average feature dimension for all views. Since D is usually very large, we can assume that the calculation costs for updating U [s, t] O (nvsmDK). The calculation costs for updating V (v) depend on Eq. (19). The calculation for D (v) Vsmsline is O (DvK). Since D (v) tnvunk, it only takes time (Dv) to perform the OMK update."}, {"heading": "V. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Dataset", "text": "In this paper, two small real-world datasets and two large real-world datasets are used to evaluate the proposed OMVFS method.The summary of these four datasets is presented in Table II, and the details of the datasets are as follows: \u2022 CNN and FOX1: These two datasets were searched from the CNN and FOX web messages [10]. Category information contained in the RSS feeds for each news article can be considered a reliable class designation. Each instance can be presented in two views, the text and image view. Titles, summaries and text content are extracted as text views and the image associated with the article is saved as image view data. All text content is used as text functions by portStemmer and l2-normalized TF-IDF, resulting in 35, 719 features for CNN and 27, 072 features for FOX."}, {"heading": "B. Comparison Methods", "text": "The differences between these comparison methods are summarized in Table III, and the details of the comparison methods are as follows: \u2022 OMVFS: OMVFS is the online method proposed in this paper for unattended selection of functions in multiple views. \u2022 EUFS: Embedded Unsupervised Feature Selection [18] is one of the most recent methods for unsupervised selection of functions in a single view. It embeds the algorithm for selecting unsupervised functions directly into a sparse learning cluster algorithm. \u2022 FSDS: Unsupervised Feature Selection on Data Stream [14] is one of the most advanced methods for unsupervised selection of functions."}, {"heading": "C. Experiment Settings", "text": "In our experiments, two widely used evaluation methods are used, Accuracy (ACC) and Normalized Mutual Information (NMI), to measure clustering performance [28]. We apply different methods to the four data sets, then the multiview spherical K-mean algorithm [29] is applied to obtain the clustering solution. Since LapScore, EUFS and FSDS only work for single-view data, we apply these three methods in the experiments to each of the views to select characteristics. It is worth noting that MVUFS is an off-line multiview method for selecting characteristics that takes all the data into account and often achieves better performance than online methods. Furthermore, LapScore, EUFS, MVUFS are all off-line methods that cannot handle large data sets. Therefore, only the two online methods FSDS and OMVFS are applied to the two large data sets."}, {"heading": "D. Results on Small Datasets", "text": "In order to compare the proposed OMVFS method with other comparison methods, we first apply all methods to two small datasets, FOX and CNN. Fig. 3 shows the performance of all methods on FOX and CNN data with different numbers of selected characteristics. Fig. 3 shows that performance will increase in most cases as the number of selected characteristics increases. In both datasets, the two multi-view feature selection methods MVUFS (offline) and OMVFS (online) outperform the methods for selecting individual characteristics. This observation suggests that combining different views would benefit the feature selection for multi-view data. In the FOX data, Fig. 3a and Fig. 3b show that the performance of the proposed OMVFS characteristics is very close and even better than that of the best off-line method. Among the three single-view methods VUFS FSS and EUFS can achieve better performance than laptops."}, {"heading": "E. Result on Large Datasets", "text": "Since the proposed OMVFS is designed for large-format multi-view datasets, we are testing OMVFS on the two largest publicly available multi-view datasets. In addition, we are comparing OMVFS with the most recent online method for selecting single views, FSDS. The results of both datasets are in Fig. 4. However, due to the extremely high runtime and memory consumption, LapScore, EUFS and MVUFS cannot be applied to the two large datasets. In Fig. 4, it is easy to see that the proposed OMVFS methods perform better than FSDS in most cases. In particular, if the number of selected features is small (i.e. less than 300), OMVFS is much better than FSDS. For YouTube data, for example, where we select only 100 features, the NMI for OMVFS is about 0.58, while the NMI for FSDS is only 0.28. This observation suggests that unlike selective FSDS, the FSDS contains less discriminating features in the selected features, even in the OMS."}, {"heading": "F. Scalability Comparison", "text": "To show the scalability of the proposed OMVFS method under different data sizes, we run OMVFS on the Reuters and YouTube data and report runtime for different data sizes (number of instances) in Fig. 5. Also, to show the scalability of the OMVFS method under different feature dimensions, we randomly select different features from the two large datasets and run OMVFS on the sampled features. Runtime for different feature dimensions is shown in Fig. 6. We select MVUFS and FSDS as comparison methods, since MVUFS is the only one that deals with multi-view data and FSDS is the only one that handles bulk / streaming data. Since FSDS is a single view method, we run FSDS in each view and report the total runtime for all views. From Fig. 5 we can observe that the two online methods, FSOMOMFS and MFOMFOS, are much faster than MFOMFOS."}, {"heading": "G. Stability under Concept Drift", "text": "To test the performance of OMVFS in such scenarios, we use 12,000 instances of Reuters data. We generate randomly unbalanced data with two dominant classes and randomly change the dominant classes for 3,000 instances each in the data stream. OMVFS is then applied to the data stream with concept drifts. As the data was received, we report on the performance of OMVFS with different orders of magnitude of characteristics in Fig. 7. We also compare a scheme using a static feature subgroup (with 200 characteristics) without adapting to the concept drift. OMVFS determines this static feature subgroup only using the first 3,000 instances in the data stream. Figure 7 shows that the approach based on static feature subgroup (with 200 characteristics) comes relatively close to OMVFS. However, as new data comes to the fore and concept drift, the performance of VOMFS data decreases compared to other subgroups."}, {"heading": "H. Parameter Study", "text": "In the proposed methods there are two sets of parameters: {\u03b1v} and {\u03b2v}. Here we examine the effects of the two sets of parameters. For the sake of simplicity, we equate \u03b1v for different views and also equate \u03b2v. We have executed OMVFS with different values for {\u03b1v} and {\u03b2v} on FOX data. Basically, we specify one of the parameters and execute OMVFS with different values for the other. We report the performance with different sizes of the selected characteristics. The results in ACC and NMI are in Fig. 8.Fig. 8 shows that the proposed OMVFS method is not very sensitive to the parameters \u03b1v and \u03b2v in most cases. However, the performance increases with the number of selected characteristics. Another parameter in the proposed OMVFS method is the batch size m, which is a common parameter for streaming algorithms. In order to consider the performance of OMFS under different sizes of the results of the OMNS, we will clearly select the VNS characteristics."}, {"heading": "VI. RELATED WORK", "text": "Most of the unattended feature selection methods combine generated pseudo-selection methods with sparse learning [6], [7], but in terms of the number of views available, the selection of conventional features becomes more obvious. Multi-view selection has attracted more attention in recent years. Several effective methods have been proposed to solve the problem of unattended feature selection for individual views. The more multiple views are generated, the more obvious the limitation of conventional feature selection methods becomes."}, {"heading": "VII. CONCLUSIONS", "text": "In this paper, we may present the first attempt to solve the online problem of uncontrolled multiview feature selection. Based on NMF, the proposed method OMVFS integrates feature selection directly into the graph-regulated clustering algorithm. A common NMF is used to learn a consensus clustering indicator matrix, which allows OMVFS to integrate information from different views. OMVFS also takes over graph regulation to preserve local structural information and help in selecting more discriminatory features. By gradually solving the optimization problem, OMVFS will be able to process the data blocks one after the other without storing all historical data, greatly reducing memory requirements. In addition, OMVFS can reduce the computing and storage costs by using the buffer technique, while taking advantage of the structural information. Buffering will also help OMVFS to demonstrate the concept of drift in the data streams, which allows FOMS to capture large data sets more efficiently, and FOMS to perform two small data sets faster."}], "references": [{"title": "A survey of multi-view machine learning", "author": ["S. Sun"], "venue": "Neural Computing and Applications, vol. 23, no. 7-8, pp. 2031\u20132038, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Irrelevant features and the subset selection problem", "author": ["G.H. John", "R. Kohavi", "K. Pfleger"], "venue": "ICML, 1994.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1994}, {"title": "Spectral feature selection for supervised and unsupervised learning", "author": ["Z. Zhao", "H. Liu"], "venue": "ICML, 2007.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2007}, {"title": "Unsupervised feature selection for multicluster data", "author": ["D. Cai", "C. Zhang", "X. He"], "venue": "KDD, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Embedded unsupervised feature selection.", "author": ["S. Wang", "J. Tang", "H. Liu"], "venue": "in AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Unsupervised feature selection using nonnegative spectral analysis.", "author": ["Z. Li", "Y. Yang", "J. Liu", "X. Zhou", "H. Lu"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Robust unsupervised feature selection.", "author": ["M. Qian", "C. Zhai"], "venue": "in IJCAI,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Multi-view clustering and feature learning via structured sparsity", "author": ["H. Wang", "F. Nie", "H. Huang"], "venue": "ICML, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised feature selection for multi-view data in social media.", "author": ["J. Tang", "X. Hu", "H. Gao", "H. Liu"], "venue": "in SDM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Unsupervised feature selection for multi-view clustering on text-image web news data", "author": ["M. Qian", "C. Zhai"], "venue": "CIKM, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining concept-drifting data streams using ensemble classifiers", "author": ["H. Wang", "W. Fan", "P.S. Yu", "J. Han"], "venue": "KDD, 2003.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "A fast clustering-based feature subset selection algorithm for high-dimensional data", "author": ["Q. Song", "J. Ni", "G. Wang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 1, pp. 1\u201314, 2013.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Pass-efficient unsupervised feature selection", "author": ["C. Maung", "H. Schweitzer"], "venue": "NIPS, 2013.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Unsupervised feature selection on data streams", "author": ["H. Huang", "S. Yoo", "S.P. Kasiviswanathan"], "venue": "CIKM, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Orthogonal nonnegative matrix t-factorizations for clustering", "author": ["C. Ding", "T. Li", "W. Peng", "H. Park"], "venue": "KDD, 2006.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2006}, {"title": "Algorithms for orthogonal nonnegative matrix factorization", "author": ["S. Choi"], "venue": "IJCNN, 2008.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2008}, {"title": "Nonnegative matrix factorization with orthogonality constraints", "author": ["J.-H. Yoo", "S.-J. Choi"], "venue": "Journal of computing science and engineering, vol. 4, no. 2, pp. 97\u2013109, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Embedded unsupervised feature selection", "author": ["S. Wang", "J. Tang", "H. Liu"], "venue": "AAAI, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Algorithms for non-negative matrix factorization", "author": ["D.D. Lee", "H.S. Seung"], "venue": "NIPS, 2001.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2001}, {"title": "Graph regularized nonnegative matrix factorization for data representation", "author": ["D. Cai", "X. He", "J. Han", "T.S. Huang"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 33, no. 8, pp. 1548\u2013 1560, 2011.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Large-scale machine learning with stochastic gradient descent", "author": ["L. Bottou"], "venue": "COMPSTAT, 2010.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning and stochastic approximations", "author": ["\u2014\u2014"], "venue": "On-line learning in neural networks, vol. 17, no. 9, p. 142.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 0}, {"title": "On using nearly-independent feature families for high precision and confidence", "author": ["O. Madani", "M. Georg", "D.A. Ross"], "venue": "Machine Learning, vol. 92, pp. 457\u2013477, 2013.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Learning from multiple partially observed views-an application to multilingual text categorization", "author": ["M. Amini", "N. Usunier", "C. Goutte"], "venue": "NIPS, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Laplacian score for feature selection", "author": ["X. He", "D. Cai", "P. Niyogi"], "venue": "NIPS, 2005.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2005}, {"title": "Unsupervised feature selection for multi-view clustering on text-image web news data", "author": ["M. Qian", "C. Zhai"], "venue": "CIKM, 2014.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "Document clustering based on nonnegative matrix factorization", "author": ["W. Xu", "X. Liu", "Y. Gong"], "venue": "SIGIR, 2003.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2003}, {"title": "Multi-view clustering", "author": ["S. Bickel", "T. Scheffer"], "venue": "ICDM, 2004.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-view Clustering via Canonical Correlation Analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "ICML, 2009.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2009}, {"title": "Clustering on multiple incomplete datasets via collective kernel learning", "author": ["W. Shao", "X. Shi", "P. Yu"], "venue": "ICDM, 2013.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-View Clustering via Joint Nonnegative Matrix Factorization", "author": ["J. Liu", "C. Wang", "J. Gao", "J. Han"], "venue": "SDM, 2013.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiple Incomplete Views Clustering via Weighted Nonnegative Matrix Factorization with L2,1 Regularization", "author": ["W. Shao", "L. He", "P.S. Yu"], "venue": "ECML PKDD, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Multiview partitioning via tensor methods", "author": ["X. Liu", "S. Ji", "W. Gl\u00e4nzel", "B. De Moor"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 25, no. 5, pp. 1056\u20131069, 2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Clustering on multi-source incomplete data via tensor modeling and factorization", "author": ["W. Shao", "L. He", "S.Y. Philip"], "venue": "PAKDD, 2015.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient document clustering via online nonnegative matrix factorizations.", "author": ["F. Wang", "P. Li", "A.C. K\u00f6nig"], "venue": "in SDM,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2011}, {"title": "Online nonnegative matrix factorization with robust stochastic approximation", "author": ["N. Guan", "D. Tao", "Z. Luo", "B. Yuan"], "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 23, no. 7, pp. 1087\u2013 1099, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Multi-view learning was proposed to combine different views to obtain better performance than relying on just one single view [1].", "startOffset": 126, "endOffset": 129}, {"referenceID": 1, "context": "Feature selection has been studied for decades [2].", "startOffset": 47, "endOffset": 50}, {"referenceID": 2, "context": "Supervised feature selection [3] uses the class labels to effectively select discriminative features to distinguish samples from different classes.", "startOffset": 29, "endOffset": 32}, {"referenceID": 3, "context": "Recently, several approaches on unsupervised feature selection has been proposed [4], [5].", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "Recently, several approaches on unsupervised feature selection has been proposed [4], [5].", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "Without the label information, most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 6, "context": "Without the label information, most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7].", "startOffset": 140, "endOffset": 143}, {"referenceID": 7, "context": "Most recently, as complementary information can be obtained from different views, unsupervised multi-view feature selection has drawn lots of attention [8]\u2013[10].", "startOffset": 152, "endOffset": 155}, {"referenceID": 9, "context": "Most recently, as complementary information can be obtained from different views, unsupervised multi-view feature selection has drawn lots of attention [8]\u2013[10].", "startOffset": 156, "endOffset": 160}, {"referenceID": 8, "context": "For example, [9] is the first to use spectral clustering and l2,1-norm regression to multi-view data in social media.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "[8] integrates all features and learns the weights for every feature with respect to each cluster individually via a new joint structured sparsity-inducing norms.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "For image and text data, [10] uses image local learning regularized orthogonal nonnegative matrix factorization to learn pseudo labels and simultaneously perform robust joint l2,1-norm minimization to select discriminative features.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "2) In many real-world applications, the data may come in as streams and concept drift [11] may happen.", "startOffset": 86, "endOffset": 90}, {"referenceID": 7, "context": "For example, [8]\u2013[10] only solve unsupervised feature selection problem for multi-view data.", "startOffset": 13, "endOffset": 16}, {"referenceID": 9, "context": "For example, [8]\u2013[10] only solve unsupervised feature selection problem for multi-view data.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "[12]\u2013[14] solve the problem of feature selection on large-scale/streaming data in a single view.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[12]\u2013[14] solve the problem of feature selection on large-scale/streaming data in a single view.", "startOffset": 5, "endOffset": 9}, {"referenceID": 14, "context": "In practice, for clustering problem, an orthogonality constraint is usually added to U [15]\u2013[17].", "startOffset": 87, "endOffset": 91}, {"referenceID": 16, "context": "In practice, for clustering problem, an orthogonality constraint is usually added to U [15]\u2013[17].", "startOffset": 92, "endOffset": 96}, {"referenceID": 17, "context": "error [18].", "startOffset": 6, "endOffset": 10}, {"referenceID": 17, "context": "[18] proposed to relax Eq.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Using the Karush-Kuhn-Tucker (KKT) complementary condition for the nonnegativity constraint on U[t], we can get the update rule for U[t] [16], [19]:", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Although the objective function for the proposed OMVFS at time t is not jointly convex with U[s,t] and {V}, the alternating optimization strategy used in OMVFS is guaranteed to converge to the local minimum [20].", "startOffset": 207, "endOffset": 211}, {"referenceID": 19, "context": "The proof is similar to that of Theorem 1 in [21] and we omit it.", "startOffset": 45, "endOffset": 49}, {"referenceID": 20, "context": "It is important to note that our method essentially applies stochastic gradient descent [22] to the new coming data.", "startOffset": 88, "endOffset": 92}, {"referenceID": 21, "context": "This method is expected to decrease the objective function in every iteration on the new data [23].", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "The summary of these four datasets is shown in Table II, and the details of the datasets are as follows: \u2022 CNN and FOX1: These two datasets were crawled from CNN and FOX web news [10].", "startOffset": 179, "endOffset": 183}, {"referenceID": 9, "context": "For image features, seven groups of color features and five textural features are used [10], which results in 996 features for both datasets.", "startOffset": 87, "endOffset": 91}, {"referenceID": 22, "context": "\u2022 YouTube Multiview Video Games (YouTube)2: This dataset consists of feature values and class labels for about 120,000 videos (instances) [24].", "startOffset": 138, "endOffset": 142}, {"referenceID": 23, "context": "\u2022 Reuters Multilingual Text Data (Reuters)3: The text collection contains feature characteristics of documents originally written in five different languages (English, French, German, Spanish and Italian), and their translations, over a common set of 6 topic categories [25].", "startOffset": 270, "endOffset": 274}, {"referenceID": 24, "context": "\u2022 LapScore: Laplacian Score [26] is a single view unsupervised feature selection method, which evaluates the importance of a feature via its power of locality preservation.", "startOffset": 28, "endOffset": 32}, {"referenceID": 17, "context": "\u2022 EUFS: Embedded Unsupervised Feature Selection [18] is one of the most recent single view unsupervised feature selection method.", "startOffset": 48, "endOffset": 52}, {"referenceID": 13, "context": "\u2022 FSDS: Unsupervised Feature Selection on Data Stream [14] is one of the state-of-the-art unsupervised feature selection algorithms, which handles large volume single view data stream efficiently.", "startOffset": 54, "endOffset": 58}, {"referenceID": 25, "context": "\u2022 MVUFS Multi-View Unsupervised Feature Selection [27] is the most advanced off-line unsupervised feature", "startOffset": 50, "endOffset": 54}, {"referenceID": 26, "context": "Experiment Settings In our experiments, two widely used evaluation metrics, Accuracy (ACC) and Normalized Mutual Information (NMI), are used to measure the clustering performance [28].", "startOffset": 179, "endOffset": 183}, {"referenceID": 27, "context": "We apply different methods to the four data sets, then the multiview spherical K-means algorithm [29] is applied to get the clustering solution.", "startOffset": 97, "endOffset": 101}, {"referenceID": 10, "context": "Stability under Concept Drift It is well-known that online/streaming algorithms are generally sensitive to the order of data, or concept drift [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 3, "context": "Feature selection, especially unsupervised feature selection [4], [5], is the first area that is related to this work.", "startOffset": 61, "endOffset": 64}, {"referenceID": 4, "context": "Feature selection, especially unsupervised feature selection [4], [5], is the first area that is related to this work.", "startOffset": 66, "endOffset": 69}, {"referenceID": 5, "context": "Most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7].", "startOffset": 104, "endOffset": 107}, {"referenceID": 6, "context": "Most of the unsupervised feature selection methods combine generated pseudo labels with sparse learning [6], [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 7, "context": "Several effective methods have been proposed to solve unsupervised feature selection problem for various multi-view scenarios [8]\u2013[10].", "startOffset": 126, "endOffset": 129}, {"referenceID": 9, "context": "Several effective methods have been proposed to solve unsupervised feature selection problem for various multi-view scenarios [8]\u2013[10].", "startOffset": 130, "endOffset": 134}, {"referenceID": 28, "context": "For example, [30], [31] propose to use canonical correlation analysis to combine different views.", "startOffset": 13, "endOffset": 17}, {"referenceID": 29, "context": "For example, [30], [31] propose to use canonical correlation analysis to combine different views.", "startOffset": 19, "endOffset": 23}, {"referenceID": 30, "context": "[32], [33] explore the option to model", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "[32], [33] explore the option to model", "startOffset": 6, "endOffset": 10}, {"referenceID": 32, "context": "[34], [35] use tensor to model the multi-view data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "[34], [35] use tensor to model the multi-view data.", "startOffset": 6, "endOffset": 10}, {"referenceID": 18, "context": "Nonnegative matrix factorization [20], especially online NMF, is the third area that is related to our work.", "startOffset": 33, "endOffset": 37}, {"referenceID": 14, "context": "NMF has been successfully used in unsupervised learning [15], [21].", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "NMF has been successfully used in unsupervised learning [15], [21].", "startOffset": 62, "endOffset": 66}, {"referenceID": 34, "context": "For example, [36] proposed an online NMF algorithm for document clustering.", "startOffset": 13, "endOffset": 17}, {"referenceID": 35, "context": "[37] proposed an efficient online NMF algorithm (OR-NMF) that takes one sample or a chunk of samples per step and updates the bases via robust hastic approximation.", "startOffset": 0, "endOffset": 4}], "year": 2016, "abstractText": "In the era of big data, it is becoming common to have data with multiple modalities or coming from multiple sources, known as \u201cmulti-view data\u201d. Since multi-view data are usually unlabeled and come from high-dimensional spaces (such as language vocabularies), unsupervised multi-view feature selection is crucial to many applications such as model interpretation and storage reduction. However, it is nontrivial due to the following challenges. First, the data may not fit in memory, because there are too many instances or the feature dimensionality is too large. How to select useful features with limited memory space? Second, the data may come in as streams and concept drift may happen. How to select features from streaming data and handles the concept drift? Third, different views may share some consistent and complementary information. How to leverage the consistent and complementary information from different views to improve the feature selection in the situation when the data are too big or come in as streams? To the best of our knowledge, none of the previous works can solve all the challenges simultaneously. In this paper, we propose an Online unsupervised MultiView Feature Selection, OMVFS, which deals with largescale/streaming multi-view data in an online fashion. OMVFS embeds unsupervised feature selection into a clustering algorithm via nonnegative matrix factorizatio with sparse learning. It further incorporates the graph regularization to preserve the local structure information and help select discriminative features. Instead of storing all the historical data, OMVFS processes the multi-view data chunk by chunk and aggregates all the necessary information into several small matrices. By using the buffering technique, the proposed OMVFS can reduce the computational and storage cost while taking advantage of the structure information. Furthermore, OMVFS can capture the concept drifts in the data streams. Extensive experiments on four real-world datasets show the effectiveness and efficiency of the proposed OMVFS method. More importantly, OMVFS is about 100 times faster than the off-line methods.", "creator": "LaTeX with hyperref package"}}}