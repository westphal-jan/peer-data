{"id": "1609.08337", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Multi-task Recurrent Model for True Multilingual Speech Recognition", "abstract": "Research on multilingual speech recognition remains attractive yet challenging. Recent studies focus on learning shared structures under the multi-task paradigm, in particular a feature sharing structure. This approach has been found effective to improve performance on each individual language. However, this approach is only useful when the deployed system supports just one language. In a true multilingual scenario where multiple languages are allowed, performance will be significantly reduced due to the competition among languages in the decoding space. This paper presents a multi-task recurrent model that involves a multilingual speech recognition (ASR) component and a language recognition (LR) component, and the ASR component is informed of the language information by the LR component, leading to a language-aware recognition. We tested the approach on an English-Chinese bilingual recognition task. The results show that the proposed multi-task recurrent model can improve performance of multilingual recognition systems.", "histories": [["v1", "Tue, 27 Sep 2016 09:56:09 GMT  (171kb,D)", "http://arxiv.org/abs/1609.08337v1", "APSIPA 2016. arXiv admin note: text overlap witharXiv:1603.09643"]], "COMMENTS": "APSIPA 2016. arXiv admin note: text overlap witharXiv:1603.09643", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.NE", "authors": ["zhiyuan tang", "lantian li", "dong wang"], "accepted": false, "id": "1609.08337"}, "pdf": {"name": "1609.08337.pdf", "metadata": {"source": "CRF", "title": "Multi-task Recurrent Model for True Multilingual Speech Recognition", "authors": ["Zhiyuan Tang", "Lantian Li", "Dong Wang"], "emails": ["lilt}@cslt.riit.tsinghua.edu.cn", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "In fact, it is the case that most people are able to understand the language and understand why they do not understand the language. In fact, it is the case that they do not understand the language they speak. In fact, it is the case that they cannot understand the language. In fact, it is the case that they cannot understand the language. In fact, it is the case that they cannot understand the language. In fact, it is the case that they cannot understand the language. In fact, it is the case that they cannot understand the language. In fact, it is the case that they cannot understand the language."}, {"heading": "II. MODELS", "text": "Let us then look at the bilingual trait exchange model ASR. Let x represent the primary input characteristic, t1 and t2 represent the targets for each language, or c is the additional input obtained from other components (LR in our experiments). With the information c, the model estimates the probability P (t1 | x, c) or P (t2 | x, c), respectively, which makes the decoding of two languages absolutely separate from each other. P (t | x, c) is really required for multilingual decoding, where t denotes the targets for both languages. If we consider the additional input c as a language indicator, the model is language-aware. Note that the language-conscious model is a conditional model with the context c as a condition. In contrast, the trait exchange model, which can be formulated in relation to P (t1 | x) or P (t2 | x), is less effective in relation to a single (R) system, c (and) is a bilingual (and) model."}, {"heading": "A. Basic single-task model", "text": "The most promising architecture for ASR is the recursive neural network, in particular the long-term short-term memory (LSTM) [9], [10] for its ability to model temporal sequences and their far-reaching dependencies. It uses the modified LSTM structure proposed in [9]. The corresponding calculation is as follows: it = \u03c3 (Wixxt + Wirrt \u2212 1 + bi) ft = \u03c3 (Wfxxt + Wfrrt \u2212 1 + Wfcct) ct = ft \u2212 1 + it g (Wcxxt + Wcrrt \u2212 1 + bc) ot = \u03c3 (Woxxt + Worrt \u2212 1 + Wocct + bo) mt = ot h (ct) rt = Wrmmtpt \u2212 1 + bf) ct \u2212 1 + it (Wcxxt + Wcrrt \u2212 1 + bc) ot (Woxxt + Worrt \u2212 1 + bo) mt = ct (Wrt = 1 ccrt) rt = 1 ct (Wrt)"}, {"heading": "B. Multi-task recurrent model", "text": "The basic idea of the multi-purpose recurrent model is to use the output of a task on the current frame = b = b = b = b = b = b = b = b = b = l = l = l) l = l = l = l = l = l = l = l) l (l = l = l = l = l) l (l = l = l = l = l = l = l = l = l = l = l l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l = l ="}, {"heading": "III. EXPERIMENTS", "text": "The proposed method was tested with the Aurora4 and Thchs30 databases labeled with word transcripts. There are two language identities, one for English and the other for Chinese. First, we present the ASR baseline for a task and then report on the common multi-task training model. All experiments were performed with the Kaldi toolkit [11]."}, {"heading": "A. Data", "text": "\u2022 Study Set: This set contains the moves of Aurora4 and Thchs30. It consists of 17, 137 phrases. This set was used to train the bilingual LSTM-based single system and the proposed multiple system for recurring tasks. The two subsets were also used to train monolingual ASR. \u2022 Test Set: This set includes \"eval92\" of Aurora4 for English and \"Test\" of Thchs30 for Chinese. These two sets consist of 4, 620 and 2, 495 phrases and were used to evaluate the performance of ASR in English and Chinese respectively."}, {"heading": "B. ASR baseline", "text": "The ASR system was largely constructed according to the Kaldi WSJ s5 nnet3 recipe, except that for simplicity we used a single LSTM layer; the dimension of the cell was 1 024, and the dimensions of the recurring and non-recurring projections were set to 256; the target delay was 5 frames; the natural stochastic gradient descent (NSGD) algorithm was used to train the model [12]; the input function was the 40-dimensional fbanks with a symmetrical 2-frame window for splitting adjacent frames; the output layer consisted of 6 468 units, corresponding to the total number of pdfs in the conventional GMM system, which was trained to boot-strap the LSTM model; the baseline of the monolingual ASR is represented in Table I, where the two languages were trained and deciphered; then we present the base line of the bilingual GMR system where a GMR system can be unified into two languages."}, {"heading": "C. Multi-task joint training", "text": "Due to the flexibility of the recurring multi-task LSTM structure, it is also not possible to evaluate all Chinese results with additional configurations. We have examined some typical ones in [7] and report the results in Table III. Note that the last configuration in which the recurring information is fed to all gates and the non-linear activation g (\u00b7) increases the information on the input variable x. Of the results shown in Tables III and IV, which were decoded with Mono-LM or Bi-LM, we first observe that the recurring multi-task model improves the performance of English ASR more than that of Chinese. We attribute this to several reasons. First, the auxiliary component was designed to provide only additional language information, but since the English and Chinese databases do not come from the same source, the voice signal contains too much channel information, which reduces the effect of the auxiliary language information when the channel classification is performed simultaneously."}, {"heading": "IV. CONCLUSIONS", "text": "We report on a multi-level, recurring learning architecture for speech-conscious speech recognition. Primary results of the bilingual ASR experiments on the Aurora4 / Thchs30 database showed that the presented method can exploit both the similarities and the diversity of different languages between two languages to a certain extent by learning ASR and LR models simultaneously."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the National Science Foundation of China (NSFC) Project No. 61371136 and the MESTDC PhD Foundation Project No. 20130002120011."}], "references": [{"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "Signal Processing Magazine, IEEE, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic Speech Recognition - A Deep Learning Approach, ser. Signals and Communication", "author": ["D. Yu", "L. Deng"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Cross-language knowledge transfer using multilingual deep neural network with shared hidden layers", "author": ["J.-T. Huang", "J. Li", "D. Yu", "L. Deng", "Y. Gong"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 7304\u20137308.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2013}, {"title": "Multilingual training of deep neural networks", "author": ["A. Ghoshal", "P. Swietojanski", "S. Renals"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 7319\u20137323.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Multilingual acoustic models using distributed deep neural networks", "author": ["G. Heigold", "V. Vanhoucke", "A. Senior", "P. Nguyen", "M. Ranzato", "M. Devin", "J. Dean"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 8619\u20138623.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-task recurrent model for speech and speaker recognition", "author": ["Z. Tang", "L. Li", "D. Wang"], "venue": "arXiv preprint arXiv:1603.09643, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling speaker variability using long shortterm memory networks for speech recognition", "author": ["X. Li", "X. Wu"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition", "author": ["H. Sak", "A. Senior", "F. Beaufays"], "venue": "arXiv preprint arXiv:1402.1128, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014, pp. 338\u2013342.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "Proceedings of IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Speech recognition (ASR) technologies develop fast in recent years, partly due to the powerful deep learning approach [1], [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 1, "context": "Speech recognition (ASR) technologies develop fast in recent years, partly due to the powerful deep learning approach [1], [2].", "startOffset": 123, "endOffset": 126}, {"referenceID": 2, "context": "The mostly studied architecture is the feature-shared deep neural network (DNN), where the input and low-level hidden layers are shared across languages, while the top-level layers and the output layer are separated for each language [3], [4], [5].", "startOffset": 234, "endOffset": 237}, {"referenceID": 3, "context": "The mostly studied architecture is the feature-shared deep neural network (DNN), where the input and low-level hidden layers are shared across languages, while the top-level layers and the output layer are separated for each language [3], [4], [5].", "startOffset": 239, "endOffset": 242}, {"referenceID": 4, "context": "The mostly studied architecture is the feature-shared deep neural network (DNN), where the input and low-level hidden layers are shared across languages, while the top-level layers and the output layer are separated for each language [3], [4], [5].", "startOffset": 244, "endOffset": 247}, {"referenceID": 5, "context": "In Deepspeech2 [6], English and Chinese can be jointly decoded under the end-to-end learning framework.", "startOffset": 15, "endOffset": 18}, {"referenceID": 6, "context": "Note that the multi-task recurrent model was proposed in [7], where we found that it can learn speech and speaker recognition models in a collaborative way.", "startOffset": 57, "endOffset": 60}, {"referenceID": 7, "context": "by [8], though it focused on ASR only.", "startOffset": 3, "endOffset": 6}, {"referenceID": 6, "context": "We first describe the single-task baseline model and then multi-task recurrent model as in [7].", "startOffset": 91, "endOffset": 94}, {"referenceID": 8, "context": "The most promising architecture for ASR is the recurrent neural network, especially the long short-term memory (LSTM) [9], [10] for its ability of modeling temporal sequences and their long-range dependencies.", "startOffset": 118, "endOffset": 121}, {"referenceID": 9, "context": "The most promising architecture for ASR is the recurrent neural network, especially the long short-term memory (LSTM) [9], [10] for its ability of modeling temporal sequences and their long-range dependencies.", "startOffset": 123, "endOffset": 127}, {"referenceID": 8, "context": "The modified LSTM structure proposed in [9] is used.", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "The picture is reproduced from [9].", "startOffset": 31, "endOffset": 34}, {"referenceID": 6, "context": "In this study, we use the recurrent LSTM model following the setting of [7] to build the ASR component and the LR component, as shown in Fig.", "startOffset": 72, "endOffset": 75}, {"referenceID": 10, "context": "All the experiments were conducted with the Kaldi toolkit [11].", "startOffset": 58, "endOffset": 62}, {"referenceID": 11, "context": "The natural stochastic gradient descent (NSGD) algorithm was employed to train the model [12].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "We explored some typical ones in [7] and report the results in Table III.", "startOffset": 33, "endOffset": 36}], "year": 2016, "abstractText": "Research on multilingual speech recognition remains attractive yet challenging. Recent studies focus on learning shared structures under the multi-task paradigm, in particular a feature sharing structure. This approach has been found effective to improve performance on each individual language. However, this approach is only useful when the deployed system supports just one language. In a true multilingual scenario where multiple languages are allowed, performance will be significantly reduced due to the competition among languages in the decoding space. This paper presents a multi-task recurrent model that involves a multilingual speech recognition (ASR) component and a language recognition (LR) component, and the ASR component is informed of the language information by the LR component, leading to a language-aware recognition. We tested the approach on an English-Chinese bilingual recognition task. The results show that the proposed multi-task recurrent model can improve performance of multilingual recognition systems.", "creator": "LaTeX with hyperref package"}}}