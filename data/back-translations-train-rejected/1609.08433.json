{"id": "1609.08433", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Local Training for PLDA in Speaker Verification", "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labeled development data, which is highly expensive in most cases. A possible approach to mitigate the problem is various unsupervised adaptation methods, which use unlabeled data to adapt the PLDA scattering matrices to the target domain.", "histories": [["v1", "Tue, 27 Sep 2016 13:37:13 GMT  (757kb,D)", "http://arxiv.org/abs/1609.08433v1", "O-COCOSDA 2016"]], "COMMENTS": "O-COCOSDA 2016", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["chenghui zhao", "lantian li", "dong wang", "april pu"], "accepted": false, "id": "1609.08433"}, "pdf": {"name": "1609.08433.pdf", "metadata": {"source": "CRF", "title": "Local Training for PLDA in Speaker Verification", "authors": ["Chenghui Zhao", "Lantian Li", "Dong Wang"], "emails": ["pu}@pachiratech.com", "lilt@cslt.riit.tsinghua.edu.cn", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "In fact, most of the people who are working for the rights of women and men are working for the rights of men and women, most of them are women and men who are working for the rights of women and men, most of them are women and men who are working for the rights of men and women, most of them are women and men who are working for the rights of men and women, most of them are women and men who are working for the rights of men and women, most of them are women who are working for the rights of women and men, most of them are women who are working for the rights of men and women."}, {"heading": "II. LOCAL PLDA TRAINING", "text": "In this section, the conventional PLDA model is briefly reviewed and our proposed local PLDA training approach is then presented in detail."}, {"heading": "A. PLDA model", "text": "PLDA is an extension of Linear Discrimination Analysis (LDA) by introducing a Gaussian priority on the middle vector of the classes. In combination with length normalization, PLDA has provided state-of-the-art performance in loudspeaker verification. If we call wij the i-vector of the Jth utterance (session) of the ith loudspeaker, the PLDA model can be formulated as follows: wij = u + V yi + zijwhere u is the speaker-independent global factor, and yi and zij represent the factors speaker-level and enunciation-level, respectively. Matrix V encompasses the basics of the loudspeaker space. Note that both yi and zij follow a fully-fledged Gaussian primary factor. The model can be trained using an EM algorithm [13], and the similarity of two i-vectors can be calculated as a ratio of evidence (probability) of two hypotheses: whether the two loudspeakers belong to the same or not."}, {"heading": "B. Global training and local training", "text": "Training a PLDA model also requires a large amount of marked data, usually thousands of speakers with multiple sessions each. These labels distinguish speakers within multiple sessions and are therefore global labels. Global labels are very expensive because it is usually very difficult to tell whether a voice comes from a new utterance made by a speaker in a sentence that includes thousands of speakers who are already known, or from a new speaker. Most existing databases have been collected using a registration and recording approach that identifies speakers \"identities using meta information rather than manual labeling. This approach is cheap in data labeling, but costly in hiring speakers and managing the record. In addition, it is not applicable in many practical scenarios where some internal data is important and should therefore be collected in a real environment, but the meta information is not available.Some uncontrolled learning methods have been suggested to solve the problem, as discussed in the introduction."}, {"heading": "III. EXPERIMENT", "text": "The proposed local training approach is tested using a task to verify the speaker with a telephone call from a call center archive.The system is based on the GMM-iVector framework. We present the data profile and then report on the results. Some analyses are performed to show in which state the local training is most effective."}, {"heading": "A. Databases and experimental setting", "text": "The training data used to train the GMM-iVector system consists of 500 hours of talk signals. These data are used to train the UBMs and the T-matrix of the i-Vector model.The development data used to train the PLDA model is divided into two groups: the Global Set and the Local Set with global or local labels. Note that the ambient state of the Local Set is closer to the state of the assessment data, which means that the Local Set can be viewed as in-domain data and unattended learning would be helpful. Further details on the development data are given in Table I. The assessment set includes 1, 236 FCs and the enrollment language for each speaker is 15 seconds long. The test is performed under 3 conditions, with the length of test expressions increasing from 5 seconds to 15 seconds. The details of the assessment data are given in Table II.The acoustic characteristics used in our experiments are the UM, the second dimension frequency, and the first dimension frequency component (the first 60)."}, {"heading": "B. Basic results", "text": "We are testing three training strategies for PLDA, as shown below: \u2022 Global Training (GT): The conventional PLDA training with the Global Developer Set. It is considered the starting point in our experiment. \u2022 Local Training (LT): Local PLDA training with the Local Developer Set. \u2022 Pooled Training (Pool): PLDA training with both the Global Set and the Local Set.The results are reported in Table III. For comparison, the results with Cosine Scoring are also reported. Initially, we observe that all three PLDA training approaches achieved a significant improvement in performance compared to Cosine Scoring. This is particularly interesting for the LT approach, where only local labels are available. This confirms our assumption that cheap local labels can be used to train PLDA and achieve a performance improvement with little effort in data labeling. At the same time, global training (GT) is still the most effective and local training (LT) and pool training (T) training is not capable of beating the GT."}, {"heading": "C. Study on pooled training", "text": "The superior performance with GT over LT is expected due to closer monitoring with global labels. However, the lower performance with the pooled training compared to the GT is somewhat surprising. As we have argued, monitoring with local labels is loud but informative, as evidenced by the LT. One reason why the performance is so strong (6000 speakers in the Global Set) is that the noisy local training is not necessary. Further research is required to confirm the state of the local training."}, {"heading": "IV. CONCLUSION", "text": "This work suggested a local training approach for PLDA and verified its potential for verifying speakers. Based on the assumption that speakers tend to be different in different conversations, local labels have a high probability of being correct and can therefore be used as weak oversight to train PLDA. Experimental results showed that the local training approach can improve system performance when globally labeled data is limited. A particular problem with local training is conversion-independent assumption. Future work will investigate to what extent this assumption would lead to effective on-site training."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the National Natural Science Foundation of China under grant numbers 61271389 and 61371136 and the National Basic Research Programme (973 Program) of China under grant number 2013CB329302."}], "references": [{"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis of i-vector length normalization in speaker recognition systems.", "author": ["D. Garcia-Romero", "C.Y. Espy-Wilson"], "venue": "in Proc. INTER- SPEECH\u201911,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "L. Ferrer", "M. McLaren"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 1695\u20131699.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian speaker verification with heavy-tailed priors", "author": ["P. Kenny"], "venue": "Proc. Odyssey, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "The fisher corpus:a resource for the next generations of speech-to-text", "author": ["C. Cieri", "D. Miller", "K. Walker"], "venue": "The Fourth International Conference on Language Resources and Evaluation (LREC),2004, 2004, pp. 69\u201371.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Improving speaker recognition performance in the domain adaptation challenge using deep neural networks", "author": ["D. Garcia-Romero", "X. Zhang", "A. McCree", "D. Povey"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 378\u2013383.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised adaptation of plda by using variational bayes methods", "author": ["J. Villalba", "E. Lleida"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on. IEEE, 2014, pp. 744\u2013748.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Utilization of unlabeled development data for speaker verification", "author": ["G. Liu", "C. Yu", "N. Shokouhi", "A. Misra", "H. Xing", "J.H. Hansen"], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE. IEEE, 2014, pp. 418\u2013423.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Domain adaptation using maximum likelihood linear transformation for plda-based speaker verification", "author": ["Q. Wang", "H. Yamamoto", "T. Koshinaka"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on. IEEE, 2016, pp. 5110\u20135114.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Dataset-invariant covariance normalization for out-domain plda speaker verification", "author": ["M.H. Rahman", "A. Kanagasundaram", "D. Dean", "S. Sridharan"], "venue": "Proc. INTERSPEECH\u201915, 2015, pp. 1017\u20131021.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic linear discriminant analysis for inferences about identity", "author": ["S.J. Prince", "J.H. Elder"], "venue": "ICCV\u201907. IEEE, 2007, pp. 1\u20138.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse probabilistic linear discriminant analysis for speaker verification", "author": ["Y. Hai", "L. Yan", "X. Fei"], "venue": "Proc. INTERSPEECH\u201912, 2012, pp. 2658\u20132661.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2012}, {"title": "The 2012 nist speaker recognition evaluation", "author": ["C.S. Greenberg", "V.M. Stanford", "A.F. Martin", "M. Yadagiri", "G.R. Doddington", "J.J. Godfrey", "J. Hernandez-Cordero"], "venue": "Proc. INTERSPEECH\u201913, 2013, pp. 1971\u20131975.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4].", "startOffset": 127, "endOffset": 130}, {"referenceID": 1, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4].", "startOffset": 132, "endOffset": 135}, {"referenceID": 2, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4].", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification systems [1], [2], [3], [4].", "startOffset": 142, "endOffset": 145}, {"referenceID": 1, "context": "It assumes that i-vectors of utterances of a particular speaker form a Gaussian distribution, with the mean vector following a normal distribution [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 3, "context": "Combined with length normalization, PLDA delivers state-of-the-art performance in various test benchmarks [4].", "startOffset": 106, "endOffset": 109}, {"referenceID": 4, "context": "For example, in the two popular development databases Fisher [5] and Switchboard [6], there are 12,399 and 543 speakers, respectively.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "[8] used an outof-domain PLDA to cluster in-domain data into some pseudo speakers, based on which the PLDA covariance matrices were adapted.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Villalba and colleagues [9] proposed a variational Bayesian method where the unknown labels were treated as latent variables.", "startOffset": 24, "endOffset": 27}, {"referenceID": 7, "context": "[10] proposed an approach where unlabeled data (i-vectors) were treated as from a universal speaker.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[11] proposed a domain-adaptation approach based on maximum likelihood linear transformation (MLLT), and Rahman et al.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[12] proposed a dataset-invariant covariance normalization approach that normalized i-vectors by a global covariance matrix computed from both in-domain and outdomain data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The model can be trained via an EM algorithm [13], and the similarity of two i-vectors can be computed as the ratio of the evidence (likelihood) of two hypothesises: whether or not the two i-vectors belong to the same speaker [14].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "The model can be trained via an EM algorithm [13], and the similarity of two i-vectors can be computed as the ratio of the evidence (likelihood) of two hypothesises: whether or not the two i-vectors belong to the same speaker [14].", "startOffset": 226, "endOffset": 230}, {"referenceID": 5, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 117, "endOffset": 120}, {"referenceID": 6, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 122, "endOffset": 125}, {"referenceID": 7, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 127, "endOffset": 131}, {"referenceID": 8, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 133, "endOffset": 137}, {"referenceID": 9, "context": "Some unsupervised learning methods have been proposed to solve the problem, as discussed already in the introduction [8], [9], [10], [11], [12].", "startOffset": 139, "endOffset": 143}, {"referenceID": 12, "context": "The performance is evaluated in terms of Equal Error Rate (EER) [15].", "startOffset": 64, "endOffset": 68}], "year": 2016, "abstractText": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labeled development data, which is highly expensive in most cases. A possible approach to mitigate the problem is various unsupervised adaptation methods, which use unlabeled data to adapt the PLDA scattering matrices to the target domain. In this paper, we present a new \u2018local training\u2019 approach that utilizes inaccurate but much cheaper local labels to train the PLDA model. These local labels discriminate speakers within a single conversion only, and so are much easier to obtain compared to the normal \u2018global labels\u2019. Our experiments show that the proposed approach can deliver significant performance improvement, particularly with limited globally-labeled data.", "creator": "LaTeX with hyperref package"}}}