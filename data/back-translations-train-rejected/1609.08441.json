{"id": "1609.08441", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Weakly Supervised PLDA Training", "abstract": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labelled development data, which is highly expensive in most cases. We present a cheap PLDA training approach, which assumes that speakers in the same session can be easily separated, and speakers in different sessions are simply different. This results in `weak labels' which are not fully accurate but cheap, leading to a weak PLDA training.", "histories": [["v1", "Tue, 27 Sep 2016 13:46:55 GMT  (311kb,D)", "http://arxiv.org/abs/1609.08441v1", "Submitted to ICASSP 2017"], ["v2", "Tue, 23 May 2017 10:19:15 GMT  (693kb,D)", "http://arxiv.org/abs/1609.08441v2", null]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.CL cs.SD", "authors": ["lantian li", "yixiang chen", "dong wang", "chenghui zhao"], "accepted": false, "id": "1609.08441"}, "pdf": {"name": "1609.08441.pdf", "metadata": {"source": "CRF", "title": "WEAKLY SUPERVISED PLDA TRAINING", "authors": ["Lantian Li", "Yixiang Chen", "Dong Wang", "Chenghui Zhao"], "emails": ["lilt@cslt.riit.tsinghua.edu.cn;", "chenyx@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn", "zhaochenghui@pachiratech.com"], "sections": [{"heading": null, "text": "In other words, one must not only occupy oneself with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with others, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself, with oneself."}, {"heading": "2.1. PLDA model", "text": "PLDA is an extension of the Linear Discrimination Analysis (LDA) by introducing a Gaussian priority on the middle i-vector of each speaker. In combination with length normalization, PLDA has provided state-of-the-art performance in loudspeaker verification. If Wij is called the i-vector of the jth utterance (session) of the ith speaker, the PLDA model can be formulated as follows: wij = u + V yi + zij, where u is the loudspeaker independent global factor, yi and zij represent the factors at the speaker level or at the level of expression. Matrix V consists of the basis of the loudspeaker brewery. Note that both yi and zij follow a diagonal full-range Gaussian primary factor. The model can be trained using an EM algorithm [12], and the similarity of two i-vectors can be calculated as a ratio of the evidence (probability) of two hypotheses: whether the two do not belong to the same loudspeaker or both."}, {"heading": "2.2. Knowledge-based weak training", "text": "We propose a weak training approach based on some prior knowledge to obtain cheap labels for unlabeled data. For example, in the customer service area where the work focuses, we use two prior knowledge: (1) there are few (often two) participants in a session, and they can easily be separated; (2) the speakers in different sessions are likely to be different, especially for customers. Based on this knowledge, a statement can easily be assigned to a label that includes a session ID and a local speaker ID, i.e., an ID is only valid within the session. These labels are not completely correct (as weak labels), but in most cases they are. Figure 1 illustrates the difference between human labels and the weak labels derived from the above knowledge, with each speaker represented by a specific color."}, {"heading": "2.3. Relation to other methods", "text": "Both rely on weak and cheap labels, but the labels are produced in different ways: knowledge-based weak training is based on subject-specific prior knowledge, and performance is determined by the accuracy of knowledge; semi-supervised training is based on the existing PLDA model, and performance is determined by the quality of the existing model. From this perspective, semi-supervised training can also be considered model-based weak training. We argue that knowledge-based weak training is superior in scenarios where human labelled data is insufficient, that a strong primary PLDA is not available, and the difference is that knowledge-based weak training is also associated with unattended PLDA adjustment [9, 10, 11, 7, 8]. Both methods use the distribution information of unlabelled data and can therefore be used to perform model adjustments."}, {"heading": "3.1. Data and configurations", "text": "The training data used to train the GMM Ivector System consists of 500 hours of call signals sampled from a large telephony customer service archive. These data are used to train the UBMs and T-matrix of the i-vector model.The development data used to train the PLDA model.The development data is divided into two data sets: the STRONG set and the WEAK set labeled by humans and the knowledge described in the previous paragraph.Note that the acoustic state of the WEAK set is closer to that of the evaluation data, meaning that the WEAK set can be considered an in-domain and therefore any improvement with this set is partially due to the model adaptation.The STRONG set includes voice signals from 2,000 speakers, and the WEAK set consists of 2,000 dual channel sessions, each of two speakers."}, {"heading": "3.2. Strong and weak training", "text": "The first experiment examines the performance of the knowledge-based weak PLDA training and compares it with the strong training using the human-labeled data. EER results are shown in Table 2, where the results are reported with the STRONG set and three WEAK subgroups. For comparison, the results with the cosinal scoring (NO PLDA) are also presented. We first observe that most PLDA models exceed the cosinal scoring, which is particularly interesting for the weak training approach, in which only inaccurate labels are used. This confirms our assumption that it is possible to use weak labels derived from the previous knowledge to train PLDA, at least in scenarios where the previous knowledge is corrected. If you compare the results with the three WEAK subgroups, you can observe that the WEAK customer delivers the best performance, while the WEAK service shows the worst results (the performance is actually worse than with the Cosine Scoring 200 school group). This is also understandable because the number of AK speakers is more limited than the number of AK school staff."}, {"heading": "3.3. Pooled training", "text": "In this experiment, we assume that limited human-labeled data and the use of weakly labeled data are an interesting concept. AK 3 also suggests a differentiated data model to improve the PLDA model. Specifically, the weakly labeled data is augmented with human-labeled data to train the PLDA data, which we call \"pooled training.\" According to the experience of the last experiment, only the data in WEAK customers is used for data multiplication. Figure 3 shows the contour of the performance of the pooled training, with different sets of data from STRONG and WEAK customers. It can be seen that the human-labeled data is limited (the number of speakers is less than 500), augmenting weakly labeled data provides clear performance improvement, and the more data is improved that improves more performance. However, the effectiveness of augmentation augmentation is not unlimited: the additional contribution becomes marginal if the amount of weakly labeled data is improved."}], "references": [{"title": "Front-end factor analysis for speaker verification", "author": ["Najim Dehak", "Patrick Kenny", "R\u00e9da Dehak", "Pierre Dumouchel", "Pierre Ouellet"], "venue": "IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis of i-vector length normalization in speaker recognition systems", "author": ["Daniel Garcia-Romero", "Carol Y Espy-Wilson"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2011, pp. 249\u2013252.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2011}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Yun Lei", "Nicolas Scheffer", "Luciana Ferrer", "Moray McLaren"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 1695\u20131699.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Bayesian speaker verification with heavy-tailed priors", "author": ["Patrick Kenny"], "venue": "Proceedings of Odyssey, 2010.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "The fisher corpus:a resource for the next generations of speech-to-text", "author": ["Christopher Cieri", "David Miller", "Kevin Walker"], "venue": "The Fourth International Conference on Language Resources and Evaluation (LREC),2004, 2004, pp. 69\u201371.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Domain adaptation using maximum likelihood linear transformation for plda-based speaker verification", "author": ["Qiongqiong Wang", "Hitoshi Yamamoto", "Takafumi Koshinaka"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5110\u20135114.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Dataset-invariant covariance normalization for out-domain plda speaker verification", "author": ["Md Hafizur Rahman", "Ahilan Kanagasundaram", "David Dean", "Sridha Sridharan"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (IN- TERSPEECH), 2015, pp. 1017\u20131021.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Improving speaker recognition performance in the domain adaptation challenge using deep neural networks", "author": ["Daniel Garcia-Romero", "Xiaohui Zhang", "Alan McCree", "Daniel Povey"], "venue": "Spoken Language Technology Workshop (SLT). IEEE, 2014, pp. 378\u2013383.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Unsupervised adaptation of plda by using variational bayes methods", "author": ["Jes\u00fas Villalba", "Eduardo Lleida"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2014, pp. 744\u2013748.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Utilization of unlabeled development data for speaker verification", "author": ["Gang Liu", "Chengzhu Yu", "Navid Shokouhi", "Abhinav Misra", "Hua Xing", "John HL Hansen"], "venue": "Spoken Language Technology Workshop (SLT). IEEE, 2014, pp. 418\u2013423.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Probabilistic linear discriminant analysis for inferences about identity", "author": ["Simon JD Prince", "James H Elder"], "venue": "Proceedings of 11th International Conference on Computer Vision (ICCV). IEEE, 2007, pp. 1\u20138.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Sparse probabilistic linear discriminant analysis for speaker verification", "author": ["Yang Hai", "LiangChun Yan", "XuYun Fei"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2012, pp. 2658\u20132661.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2012}, {"title": "The 2012 nist speaker recognition evaluation", "author": ["Craig S Greenberg", "Vincent M Stanford", "Alvin F Martin", "Meghana Yadagiri", "George R Doddington", "John J Godfrey", "Jaime Hernandez-Cordero"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2013, pp. 1971\u2013 1975.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification [1, 2, 3, 4].", "startOffset": 119, "endOffset": 131}, {"referenceID": 1, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification [1, 2, 3, 4].", "startOffset": 119, "endOffset": 131}, {"referenceID": 2, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification [1, 2, 3, 4].", "startOffset": 119, "endOffset": 131}, {"referenceID": 3, "context": "The i-vector model plus various normalization approaches offers the standard framework for modern speaker verification [1, 2, 3, 4].", "startOffset": 119, "endOffset": 131}, {"referenceID": 1, "context": "It assumes that i-vectors of a particular speaker subject to a Gaussian distribution, with the mean vector following a normal distribution [2].", "startOffset": 139, "endOffset": 142}, {"referenceID": 3, "context": "Combined with length normalization, PLDA has delivered state-of-the-art performance in various test benchmarks [4].", "startOffset": 111, "endOffset": 114}, {"referenceID": 4, "context": "For example, in the two popular development databases Fisher [5] and Switchboard [6], there are 12, 399 and 543 speakers, respectively.", "startOffset": 61, "endOffset": 64}, {"referenceID": 5, "context": "[7] proposed a domain-adaptation approach based on maximum likelihood linear transformation (MLLT), and Rahman et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] proposed a dataset-invariant covariance normalization approach that normalized i-vectors by a global covariance matrix computed from both in-domain and out-domain data.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] proposed a semi-supervised learning approach that used an out-of-domain PLDA to cluster in-domain data, based on which the PLDA projection matrix was adapted.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "Villalba and colleagues [10] proposed a variational Bayesian method where the unknown label of an unlabelled utterance was treated as a latent variable.", "startOffset": 24, "endOffset": 28}, {"referenceID": 9, "context": "[11] proposed an approach that treated unlabelled data as from a special universal speaker, and the PLDA was trained with the universal speaker involved.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "The model can be trained via an EM algorithm [12], and the similarity of two i-vectors can be computed as the ratio of the evidence (likelihood) of two hypothesises: whether or not the two i-vectors belong to the same speaker [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 11, "context": "The model can be trained via an EM algorithm [12], and the similarity of two i-vectors can be computed as the ratio of the evidence (likelihood) of two hypothesises: whether or not the two i-vectors belong to the same speaker [13].", "startOffset": 226, "endOffset": 230}, {"referenceID": 7, "context": "The knowledge-based weak training proposed here is related to the semi-supervised PLDA training in [9].", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "The knowledge-based weak training is also related to unsupervised PLDA adaptation [9, 10, 11, 7, 8].", "startOffset": 82, "endOffset": 99}, {"referenceID": 8, "context": "The knowledge-based weak training is also related to unsupervised PLDA adaptation [9, 10, 11, 7, 8].", "startOffset": 82, "endOffset": 99}, {"referenceID": 9, "context": "The knowledge-based weak training is also related to unsupervised PLDA adaptation [9, 10, 11, 7, 8].", "startOffset": 82, "endOffset": 99}, {"referenceID": 5, "context": "The knowledge-based weak training is also related to unsupervised PLDA adaptation [9, 10, 11, 7, 8].", "startOffset": 82, "endOffset": 99}, {"referenceID": 6, "context": "The knowledge-based weak training is also related to unsupervised PLDA adaptation [9, 10, 11, 7, 8].", "startOffset": 82, "endOffset": 99}, {"referenceID": 12, "context": "The performance is evaluated in terms of Equal Error Rate (EER) [14].", "startOffset": 64, "endOffset": 68}], "year": 2017, "abstractText": "PLDA is a popular normalization approach for the i-vector model, and it has delivered state-of-the-art performance in speaker verification. However, PLDA training requires a large amount of labelled development data, which is highly expensive in most cases. We present a cheap PLDA training approach, which assumes that speakers in the same session can be easily separated, and speakers in different sessions are simply different. This results in \u2018weak labels\u2019 which are not fully accurate but cheap, leading to a weak PLDA training. Our experimental results on real-life large-scale telephony customer service achieves demonstrated that the weak training can offer good performance when human-labelled data are limited. More interestingly, the weak training can be employed as a discriminative adaptation approach, which is more efficient than the prevailing unsupervised method when human-labelled data are insufficient.", "creator": "LaTeX with hyperref package"}}}