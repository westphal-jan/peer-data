{"id": "1609.08442", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Collaborative Learning for Language and Speaker Recognition", "abstract": "This paper presents a unified model to perform language and speaker recognition simultaneously and altogether. The model is based on a multi-task recurrent neural network where the output of one task is fed as the input of the other, leading to a collaborative learning framework that can improve both language and speaker recognition by borrowing information from each other. Our experiments demonstrated that the multi-task model outperforms the task-specific models on both tasks.", "histories": [["v1", "Tue, 27 Sep 2016 13:48:01 GMT  (237kb,D)", "https://arxiv.org/abs/1609.08442v1", "Submitted to ICASSP 2017"], ["v2", "Tue, 23 May 2017 09:56:54 GMT  (622kb,D)", "http://arxiv.org/abs/1609.08442v2", null]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["lantian li", "zhiyuan tang", "dong wang", "rew abel", "yang feng", "shiyue zhang"], "accepted": false, "id": "1609.08442"}, "pdf": {"name": "1609.08442.pdf", "metadata": {"source": "CRF", "title": "Collaborative Learning for Language and Speaker Recognition", "authors": ["Lantian Li", "Zhiyuan Tang", "Dong Wang", "Andrew Abel", "Yang Feng", "Shiyue Zhang"], "emails": ["lilt@cslt.riit.tsinghua.edu.cn;", "tangzy@cslt.riit.tsinghua.edu.cn;", "fengyang@cslt.riit.tsinghua.edu.cn;", "zhangsy@cslt.riit.tsinghua.edu.cn;", "wangdong99@mails.tsinghua.edu.cn;", "andrew.abel@xjtlu.edu.cn"], "sections": [{"heading": null, "text": "This model is based on a multi-task recursive neural network, in which the results of one task are fed into the other, resulting in a collaborative learning framework that can improve both speech and speech recognition by exchanging information between tasks. Preliminary experiments presented in this paper show that the multi-task model outperforms similar task-specific models in both language and speech tasks. The improvement in speech recognition is particularly noteworthy, which we believe is due to the speaker's normalization effect caused by the use of information from the speech recognition component. Index terms: speech recognition, speech recognition, deep learning, recursive neural network."}, {"heading": "1. Introduction", "text": "In fact, it is such that most people who are in a position to be able to move, to be able to live without fear and terror and to live without fear, will be able to be able to feel, to be able to feel, to be able to feel, to be able to be able, to be able to be able, to be able to be able to be able, to be able to be able to be able to be able, to be able to be able to be able to be able, to be able to be able to be able, to be able to be able, to be able to be able, to be able to be able to be able to be able, to be able to be able to be able to be able, to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able to be able, to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be"}, {"heading": "2. Related work", "text": "This collaborative learning approach has been proposed by Tang et al. for addressing the close relationship between speech and loudspeaker recognition [27]. The idea of multi-task learning for speech signals has been extensively studied, e.g., [28, 29], and further research on this multi-task learning can be found in [30]. However, the key difference between collaborative learning and traditional multi-task learning is that the interdisciplinary parts of knowledge are on-line, i.e. the results of a task will influence other tasks, and these effects will multiply again through the collaborative link, leading to a collaborative and integrated information framework. The close correlation between speaker traits and languagear Xiv: 160 9.08 442v 2 [cs.S 7Identities is well known to both SRE researchers."}, {"heading": "3. Multi-task RNN and collaborative learning", "text": "This section first introduces the neural model structure for individual tasks and then extends it to the recurring multi-task model for collaborative learning."}, {"heading": "3.1. Basic single-task model", "text": "For the work in this thesis, we have chosen a certain RNN, the long-term short-term memory (LSTM) [36] approach, to build the basic single task systems for SRE and LRE. LSTM has been shown to provide good performance for both SRE [9] and LRE [15, 22, 25]. Specifically, here, the recurring LSTM structure proposed in [37] is used, as shown in Figure 2, and the associated calculation is as follows: it = \u03c3 (Wixxt + Wirrt \u2212 1 + Wicct \u2212 1 + bi) ft = \u03c3 (Wfxxt + Wfrrt \u2212 1 + Wfcct) ct = ft \u2212 1 + it g (Wcxxt + Wcrrt \u2212 1 + bc) ot = \u03c3 (Woxxt + Worrt \u2212 1 + bi) ft = \u03c3 (Wfxxt + Wfrrt \u2212 1 + bi) ft = \u03c3 (Wfxxt + Wfrrt \u2212 1 + bf), and input-input \u2212 1 + bf) ct = ft \u2212 1 + it g (Wcxxxt + Wcrrt \u2212 1 + bc), the input-input _ input _ input _ input _ input _ input _ is the output \u00b7 \u00b7 \u00b7 \u00b7 input _ input _ input _ input _ input _ input _ input _ input _ input _ input _ input _ are the given (Wxt + Wxt = _ _ _ input _ input _ input _ input _ input _ input _ input _ input _ input _ input _ are the input _ input _ input _ input _ input _ input _ input _ input _ are the given (Wyxxt + _ _ _ _ _ _ _ _ _ wxts = _ _ _ mnr _ string _ mnr _ input _ input _ input _ input _ input _ input _ are _ input _ input _ input _ input _ input _ input _ input _ are _ input _ m.wxit _ mglyd)."}, {"heading": "3.2. Multi-task recurrent model", "text": "The basic idea of the recursive model of the multiple task, as shown in Figure 1, is to use the output of a task in the current time step as an auxiliary input in the other task in the next step = b = b = b = b = b = l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l) l (l) l (l) l (l) l (l) l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) s) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l (l) l) l (l) l (l) l (l) l) l (l (l) s) l) l (l) l (l) l (l) l) l (l (l) l) l (l) l (l) l (l) l (l (l) l) l (l) l) l (l (l (l) l) l) l (l) l) l (l) l (l (l) l) l (l) l (l) l (l) l (l) l (l) l (l) l (l (l) l) l (l) l (l) l (l) l (l (l) l (l) l) l) l (l (l) l (l) l) l (l) l (l) l) l (l (l) l (l) l (l) l (l) l (l) l) l) l (l (l) l (l) l) l (l (l) l) l (l) l) l) l (l) l (l) l (l (l) l) l (l) l"}, {"heading": "3.3. Model training", "text": "Our research to date has shown that both cases are suitable [27]. In this preliminary study, we have focused on the application of \"complete\" training. To train the model, the algorithm Natural stochastic Gradient Descent (NSGD) is used [38]."}, {"heading": "4. Experiments", "text": "This section first describes the data profile and introduces the basic systems. Finally, experimental results of our collaborative learning approach are presented."}, {"heading": "4.1. Data", "text": "To conduct the experiment, two databases were used: the WSJ database in English and the CSLT-C300 database in Chinese. 1 All statements in both databases were marked with both language and speaker identities.The development set includes two databases collected by our Institute for Commercial Use, so we cannot publish the wave data, but the Fbanks and MFCCs in Kaldi format were published online. See http: / / data.cslt. org. The Kaldi recipe for reproducing the results is also available there. Subsets: WSJ-E200, which contains 200 speakers (24, 031 expressions) selected from WSJ, and CSLT-C200, which contains 200 speakers (20,000 expressions) selected from the CSLT-C300 database. The development set was used to train the i-vector, SVM and multi-purpose repeat modes for Chinese subsets, 110-110 subsets in English, 110-791 subsets in Chinese, contains SST."}, {"heading": "4.2. LRE and SRE baselines", "text": "Here we first present the LRE and SRE baselines. For each task, two baseline systems were constructed, one based on state-of-the-art Ivectors and the other based on LSTM. All experiments were carried out with the Kaldi toolkit [39]."}, {"heading": "4.2.1. i-vector baseline", "text": "The number of Gaussian components of the Universal Background Model (UBM) was 1, 024, and the dimension of the i-vectors was 200; the resulting i-vectors were used to perform both SRE and LRE with different evaluation methods.For SRE, we look at simple cosine distance as well as the popular discriminatory models LDA and PLDA; for LRE, we look at cosine distance and SVM. All discriminatory models were developed set.The results of the SRE base model are shown in Table 1 in terms of equal error rate (EER).We tested two scenarios, one is a full-length test using all enrolment and test expression; the other is a short-length test using only 1 second language (sample from the original data after the language activity test).In both scenarios, the Chinese language is assumed as a differentiation number (R) in comparison between the two tests."}, {"heading": "4.2.2. r-vector baseline", "text": "The r-vector baseline is based on the recursive LSTM structure shown in Figure 2. SRE and LRE systems use the same configurations: the dimensionality of the cell is set to 1, 024, and the dimensionality of both recursive and non-recursive projections is set to 100. For the SRE system, the output corresponds to the 400 speakers in the training environment; for LRE, the output corresponds to the two languages to be identified. The output of both projections was linked and averaged across all frames of an utterance, resulting in a 200-dimensional \"r-vector\" for this utterance. The r-vector, derived from the SRE system, represents speaker characters, and the r-vector, derived from the LRE system, represents the language identity and the i-vector system, which are measured in the coseline vector-based distance between the cosine vector and the cosine discriminators, or the distance between the cosine vector decisions are made."}, {"heading": "4.3. Collaborative learning", "text": "The results are presented in Tables 3 and 4 for SRE and LRE, where i, f, o denotes the input, forget- and output gates, and g denotes the nonlinear function. Results show that collaborative learning consistently improves performance for both SRE and LRE, regardless of which component the feedback is applied to. Results show that the output gate is a suitable component for SRE and LRE, regardless of which component the feedback is applied to. Results show that the output gate is a suitable component for SRE to receive feedback, while for LRE the forgetgate gate seems to be a more appropriate choice. However, these observations are based on relatively small databases, on which large amounts of data are required for SRE training."}, {"heading": "5. Conclusions", "text": "This paper proposes a novel collaborative learning architecture that performs speech and speaker recognition as a single and unified model, based on a multi-task recursive neural network. These preliminary experiments showed that the proposed approach can deliver consistent performance improvement over the individual tasks for both SRE and LRE. Performance gains in LRE are particularly impressive, which we believe could be due to the effect of loudspeaker normalization. Future work will include experimenting with large databases and analyzing the characteristics of the collaborative mechanism, such as traceability, stability, and expandability."}, {"heading": "6. References", "text": "[1] J. Navratil, \"Spoken Language-a step towards multilin-guality P. P. in speech processing,\" IEEE Transactions on Speech and Audio Processing, vol. 9, no. 6, pp. 678-685, 2001. [2] F. Bimbot, J.-F. Bonastre, C. Fredouille, G. Gravier, I. MagrinChagnolleau, S. Meignier, T. Merlin, J. Ortega-Garc\u0131 a, D. Petrovska-Delacre \u0301 taz, and D. A. Reynolds, \"A tutorial on textindependent speaker verification, I. MagrinChagnolleau, S. Rodriguez Rodriguez, S. Rodriguez S. S. S. S. Rodriguez, T. 430-451, 2004. [3] W. M. Campbell, J. A. Campbell, D. A. Reynolds, E. Singer, and P. A. Torres-Carrasquillo,\" Support vector machines for speaker and language recognition, \"vol."}], "references": [{"title": "Spoken language recognition-a step toward multilinguality in speech processing", "author": ["J. Navratil"], "venue": "IEEE Transactions on Speech and Audio Processing, vol. 9, no. 6, pp. 678\u2013685, 2001.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2001}, {"title": "A tutorial on textindependent speaker verification", "author": ["F. Bimbot", "J.-F. Bonastre", "C. Fredouille", "G. Gravier", "I. Magrin- Chagnolleau", "S. Meignier", "T. Merlin", "J. Ortega-Garc\u0131\u0301a", "D. Petrovska-Delacr\u00e9taz", "D.A. Reynolds"], "venue": "EURASIP Journal on Applied Signal Processing, vol. 2004, pp. 430\u2013451, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Support vector machines for speaker and language recognition", "author": ["W.M. Campbell", "J.P. Campbell", "D.A. Reynolds", "E. Singer", "P.A. Torres-Carrasquillo"], "venue": "Computer Speech & Language, vol. 20, no. 2, pp. 210\u2013229, 2006.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2006}, {"title": "Front-end factor analysis for speaker verification", "author": ["N. Dehak", "P.J. Kenny", "R. Dehak", "P. Dumouchel", "P. Ouellet"], "venue": "IEEE Transactions on Audio, Speech and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2011.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "A novel scheme for speaker recognition using a phonetically-aware deep neural network", "author": ["Y. Lei", "N. Scheffer", "L. Ferrer", "M. McLaren"], "venue": "ICASSP. IEEE, 2014, pp. 1695\u20131699.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Language recognition via ivectors and dimensionality reduction", "author": ["N. Dehak", "A.-C. Pedro", "D. Reynolds", "R. Dehak"], "venue": "Interspeech, 2011, pp. 857\u2013860.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Language recognition in ivectors space", "author": ["D. Mart\u0131nez", "O. Plchot", "L. Burget", "O. Glembek", "P. Matejka"], "venue": "Interspeech, 2011, pp. 861\u2013864.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Deep neural networks for small footprint text-dependent speaker verification", "author": ["V. Ehsan", "L. Xin", "M. Erik", "L.M. Ignacio", "G.-D. Javier"], "venue": "ICASSP, 2014, pp. 357\u2013366.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "End-to-end text-dependent speaker verification", "author": ["G. Heigold", "I. Moreno", "S. Bengio", "N. Shazeer"], "venue": "ICASSP. IEEE, 2016, pp. 5115\u20135119.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep neural network-based speaker embeddings for end-to-end speaker verification", "author": ["D. Snyder", "P. Ghahremani", "D. Povey", "D. Garcia-Romero", "Y. Carmiel", "S. Khudanpur"], "venue": "SLT, 2016.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Automatic language identification using deep neural networks", "author": ["I. Lopez-Moreno", "J. Gonzalez-Dominguez", "O. Plchot", "D. Martinez", "J. Gonzalez-Rodriguez", "P. Moreno"], "venue": "ICASSP. IEEE, 2014, pp. 5337\u20135341.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "An end-to-end approach to language identification in short utterances using convolutional neural networks", "author": ["A. Lozano-Diez", "R. Zazo Candil", "J. Gonz\u00e1lez Dom\u0131\u0301nguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "Interspeech, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked long-term tdnn for spoken language recognition", "author": ["D. Garcia-Romero", "A. McCree"], "venue": "Interspeech, 2016, pp. 3226\u2013 3230.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "LIDsenone extraction via deep neural networks for end-to-end language identification", "author": ["M. Jin", "Y. Song", "I. Mcloughlin", "L.-R. Dai", "Z.-F. Ye"], "venue": "Odyssey, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification in short utterances using long short-term memory (LSTM) recurrent neural networks", "author": ["R. Zazo", "A. Lozano-Diez", "J. Gonzalez-Dominguez", "D.T. Toledano", "J. Gonzalez-Rodriguez"], "venue": "PloS one, vol. 11, no. 1, p. e0146917, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Language identification using time delay neural network d-vector on short utterances", "author": ["M. Kotov", "M. Nastasenko"], "venue": "Speech and Computer: 18th International Conference, SPECOM 2016, vol. 9811. Springer, 2016, p. 443.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "English-Chinese bilingual text-independent speaker verification", "author": ["B. Ma", "H. Meng"], "venue": "ICASSP. IEEE, 2004, pp. V\u2013293.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2004}, {"title": "Language dependency in text-independent speaker verification", "author": ["R. Auckenthaler", "M.J. Carey", "J. Mason"], "venue": "ICASSP. IEEE, 2001, pp. 441\u2013444.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2001}, {"title": "Spoken language mismatch in speaker verification: An investigation with nist-sre and crss biling corpora", "author": ["A. Misra", "J.H.L. Hansen"], "venue": "IEEE Spoken Language Technology Workshop (SLT). IEEE, 2014, pp. 372\u2013377.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Language-aware plda for multilingual speaker recognition", "author": ["A. Rozi", "D. Wang", "L. Li", "T.F. Zheng"], "venue": "O-COCOSDA 2016, 2016.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2016}, {"title": "Brno university of technology system for nist 2005 language recognition evaluation", "author": ["P. Matejka", "L. Burget", "P. Schwarz", "J. Cernocky"], "venue": "Speaker and Language Recognition Workshop,IEEE Odyssey 2006. IEEE, 2006, pp. 1\u20137.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "A divide-andconquer approach for language identification based on recurrent neural networks", "author": ["G. Gelly", "J.-L. Gauvain", "V. Le", "A. Messaoudi"], "venue": "Interspeech, 2016, pp. 3231\u20133235.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved gmm-based language recognition using constrained mllr transforms", "author": ["W. Shen", "D. Reynolds"], "venue": "ICASSP. IEEE, 2008, pp. 4149\u20134152.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "End-to-end attention based text-dependent speaker verification", "author": ["S.-X. Zhang", "Z. Chen", "Y. Zhao", "J. Li", "Y. Gong"], "venue": "arXiv preprint arXiv:1701.00562, 2017.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2017}, {"title": "Automatic language identification using long short-term memory recurrent neural networks.", "author": ["J. Gonzalez-Dominguez", "I. Lopez-Moreno", "H. Sak", "J. Gonzalez- Rodriguez", "P.J. Moreno"], "venue": "in Interspeech,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "On the use of phone-gram units in recurrent neural networks for language identification", "author": ["C. Salamea", "L.F. D\u2019Haro", "R. de C\u00f3rdoba", "R. San-Segundo"], "venue": "Odyssey, 2016, pp. 117\u2013123.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Collaborative joint training with multi-task recurrent model for speech and speaker recognition", "author": ["Z. Tang", "L. Li", "D. Wang", "R.C. Vipperla"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Modeling speaker variability using long shortterm memory networks for speech recognition", "author": ["X. Li", "X. Wu"], "venue": "Interspeech, 2015, pp. 1086\u20131090.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Neural network based multi-factor aware joint training for robust speech recognition", "author": ["Y. Qian", "T. Tan", "D. Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 12, pp. 2231\u20132240, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Transfer learning for speech and language processing", "author": ["D. Wang", "T.F. Zheng"], "venue": "APSIPA, 2015, pp. 1225\u20131237.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2015}, {"title": "Language identification using phone-based acoustic likelihoods", "author": ["L.F. Lamel", "J.-L. Gauvain"], "venue": "ICASSP, vol. 1. IEEE, 1994, pp. I\u2013293.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1994}, {"title": "Comparison of four approaches to automatic language identification of telephone speech", "author": ["M.A. Zissman"], "venue": "IEEE Transactions on speech and audio processing, vol. 4, no. 1, p. 31, 1996.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 1996}, {"title": "I-vector representation based on bottleneck features for language identification", "author": ["Y. Song", "B. Jiang", "Y. Bao", "S. Wei", "L.-R. Dai"], "venue": "Electronics Letters, vol. 49, no. 24, pp. 1569\u20131570, 2013.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2013}, {"title": "Investigation of senone-based long-short term memory rnns for spoken language recognition", "author": ["Y. Tian", "L. He", "Y. Liu", "J. Liu"], "venue": "Odyssey, pp. 89\u201393, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "The effect of language factors for robust speaker recognition", "author": ["L. Lu", "Y. Dong", "Z. Xianyu", "L. Jiqing", "W. Haila"], "venue": "ICASSP. IEEE, 2009, pp. 4217\u20134220.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2009}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "S. J\u00fcrgen"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 1997}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["H. Sak", "A.W. Senior", "F. Beaufays"], "venue": "Interspeech, 2014, pp. 338\u2013342.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallel training of deep neural networks with natural gradient and parameter averaging", "author": ["D. Povey", "X. Zhang", "S. Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "The kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding, no. EPFL- CONF-192584. IEEE Signal Processing Society, 2011.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical language identification based on automatic language clustering", "author": ["B. Yin", "A. Eliathamby", "C. Fang"], "venue": "Interspeech, 2007, pp. 178\u2013181.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2007}], "referenceMentions": [{"referenceID": 0, "context": "Language recognition (LRE) [1] and speaker recognition (SRE) [2] are two important tasks in speech processing.", "startOffset": 27, "endOffset": 30}, {"referenceID": 1, "context": "Language recognition (LRE) [1] and speaker recognition (SRE) [2] are two important tasks in speech processing.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 153, "endOffset": 156}, {"referenceID": 3, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 177, "endOffset": 189}, {"referenceID": 4, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 177, "endOffset": 189}, {"referenceID": 5, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 177, "endOffset": 189}, {"referenceID": 6, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 177, "endOffset": 189}, {"referenceID": 7, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 8, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 9, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 10, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 11, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 12, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 13, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 14, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 15, "context": "Traditionally, the research in these two fields seldom acknowledges the other domain, although some there are a number of shared techniques, such as SVM [3], the i-vector model [4, 5, 6, 7], and deep neural models [8, 9, 10, 11, 12, 13, 14, 15, 16].", "startOffset": 214, "endOffset": 248}, {"referenceID": 16, "context": "In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20].", "startOffset": 141, "endOffset": 153}, {"referenceID": 17, "context": "In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20].", "startOffset": 141, "endOffset": 153}, {"referenceID": 18, "context": "In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20].", "startOffset": 141, "endOffset": 153}, {"referenceID": 19, "context": "In speaker recognition, it has been confirmed that language mismatch indeed leads to serious performance degradation for speaker recognition [17, 18, 19], and some language-aware models have been demonstrated successfully [20].", "startOffset": 222, "endOffset": 226}, {"referenceID": 20, "context": ", by VTLN [21, 22] or CMLLR [23].", "startOffset": 10, "endOffset": 18}, {"referenceID": 21, "context": ", by VTLN [21, 22] or CMLLR [23].", "startOffset": 10, "endOffset": 18}, {"referenceID": 22, "context": ", by VTLN [21, 22] or CMLLR [23].", "startOffset": 28, "endOffset": 32}, {"referenceID": 8, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 315, "endOffset": 322}, {"referenceID": 23, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 315, "endOffset": 322}, {"referenceID": 14, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 331, "endOffset": 347}, {"referenceID": 21, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 331, "endOffset": 347}, {"referenceID": 24, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 331, "endOffset": 347}, {"referenceID": 25, "context": "It should be noted that collaborative learning is a general framework and the component for each task can be implemented using any model, but in this paper, we have chosen to make use of recurrent neural networks (RNN) due to their great potential and good results in various speech processing tasks, including SRE [9, 24] and LRE [15, 22, 25, 26].", "startOffset": 331, "endOffset": 347}, {"referenceID": 26, "context": "for addressing the close relationship between speech and speaker recognition [27].", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": ", [28, 29], and more research on this multi-task learning can be found in [30].", "startOffset": 2, "endOffset": 10}, {"referenceID": 28, "context": ", [28, 29], and more research on this multi-task learning can be found in [30].", "startOffset": 2, "endOffset": 10}, {"referenceID": 29, "context": ", [28, 29], and more research on this multi-task learning can be found in [30].", "startOffset": 74, "endOffset": 78}, {"referenceID": 30, "context": "In language recognition, the conventional phonetic approach [31, 32] relies on the compositional speech recognition system to deal with the speaker variation.", "startOffset": 60, "endOffset": 68}, {"referenceID": 31, "context": "In language recognition, the conventional phonetic approach [31, 32] relies on the compositional speech recognition system to deal with the speaker variation.", "startOffset": 60, "endOffset": 68}, {"referenceID": 20, "context": "In the HMM-GMM era, this often relied on various front-end normalization techniques, such as vocal track length normalization (VTLN) [21, 22] and constrained maximum likelihood linear regression (CMLLR) [23].", "startOffset": 133, "endOffset": 141}, {"referenceID": 21, "context": "In the HMM-GMM era, this often relied on various front-end normalization techniques, such as vocal track length normalization (VTLN) [21, 22] and constrained maximum likelihood linear regression (CMLLR) [23].", "startOffset": 133, "endOffset": 141}, {"referenceID": 22, "context": "In the HMM-GMM era, this often relied on various front-end normalization techniques, such as vocal track length normalization (VTLN) [21, 22] and constrained maximum likelihood linear regression (CMLLR) [23].", "startOffset": 203, "endOffset": 207}, {"referenceID": 32, "context": "This capability has been naturally used in i-vector based LRE approaches [33, 34].", "startOffset": 73, "endOffset": 81}, {"referenceID": 33, "context": "This capability has been naturally used in i-vector based LRE approaches [33, 34].", "startOffset": 73, "endOffset": 81}, {"referenceID": 13, "context": ", [14, 15], there is limited research into speaker-aware learning for LRE.", "startOffset": 2, "endOffset": 10}, {"referenceID": 14, "context": ", [14, 15], there is limited research into speaker-aware learning for LRE.", "startOffset": 2, "endOffset": 10}, {"referenceID": 16, "context": "However from the engineering perspective, language mismatch has been found to pose a serious problem due to the different patterns of acoustic space in different languages, according to their own phonetic systems [17, 18, 19].", "startOffset": 213, "endOffset": 225}, {"referenceID": 17, "context": "However from the engineering perspective, language mismatch has been found to pose a serious problem due to the different patterns of acoustic space in different languages, according to their own phonetic systems [17, 18, 19].", "startOffset": 213, "endOffset": 225}, {"referenceID": 18, "context": "However from the engineering perspective, language mismatch has been found to pose a serious problem due to the different patterns of acoustic space in different languages, according to their own phonetic systems [17, 18, 19].", "startOffset": 213, "endOffset": 225}, {"referenceID": 16, "context": "A simple approach is to train a multilingual speaker model by data pooling [17, 18], but this approach does not model the correlation between language identities and speaker traits.", "startOffset": 75, "endOffset": 83}, {"referenceID": 17, "context": "A simple approach is to train a multilingual speaker model by data pooling [17, 18], but this approach does not model the correlation between language identities and speaker traits.", "startOffset": 75, "endOffset": 83}, {"referenceID": 34, "context": "Another potential approach is to treat language and speaker as two random variables and represent them by a linear Gaussian model [35], but this linear Gaussian assumption is perhaps too strong.", "startOffset": 130, "endOffset": 134}, {"referenceID": 35, "context": "For the work in this paper we have chosen a particular RNN, the long short-term memory (LSTM) [36] approach to build the baseline single-task systems for SRE and LRE.", "startOffset": 94, "endOffset": 98}, {"referenceID": 8, "context": "LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25].", "startOffset": 61, "endOffset": 64}, {"referenceID": 14, "context": "LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25].", "startOffset": 73, "endOffset": 85}, {"referenceID": 21, "context": "LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25].", "startOffset": 73, "endOffset": 85}, {"referenceID": 24, "context": "LSTM has been shown to deliver good performance for both SRE [9] and LRE [15, 22, 25].", "startOffset": 73, "endOffset": 85}, {"referenceID": 36, "context": "In particular, the recurrent LSTM structure proposed in [37] is used here, as shown in Figure 2, and the associated computation is as follows:", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "Our previous research has demonstrated that both cases are suitable [27].", "startOffset": 68, "endOffset": 72}, {"referenceID": 37, "context": "The natural stochastic gradient descent (NSGD) algorithm [38] is employed to train the model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 38, "context": "All experiments were conducted with the Kaldi toolkit [39].", "startOffset": 54, "endOffset": 58}, {"referenceID": 39, "context": "We therefore use identification error rate (IDR) [40] to measure performance, which is the fraction of the identification mistakes in the total number of identification trials.", "startOffset": 49, "endOffset": 53}, {"referenceID": 14, "context": "The advantage of the r-vector model on short utterances has previously been observed, both for LRE [15] and SRE [10].", "startOffset": 99, "endOffset": 103}, {"referenceID": 9, "context": "The advantage of the r-vector model on short utterances has previously been observed, both for LRE [15] and SRE [10].", "startOffset": 112, "endOffset": 116}, {"referenceID": 26, "context": "Following research in [27], we selected the output of the recurrent projection layer as the feedback information, and tested several configurations, where the feedback information from one task is propagated into different components of the other task.", "startOffset": 22, "endOffset": 26}], "year": 2017, "abstractText": "This paper presents a unified model to perform language and speaker recognition simultaneously and together. This model is based on a multi-task recurrent neural network, where the output of one task is fed in as the input of the other, leading to a collaborative learning framework that can improve both language and speaker recognition by sharing information between the tasks. The preliminary experiments presented in this paper demonstrate that the multi-task model outperforms similar taskspecific models on both language and speaker tasks. The language recognition improvement is especially remarkable, which we believe is due to the speaker normalization effect caused by using the information from the speaker recognition component.", "creator": "LaTeX with hyperref package"}}}