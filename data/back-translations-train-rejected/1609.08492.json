{"id": "1609.08492", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "WS4A: a Biomedical Question and Answering System based on public Web Services and Ontologies", "abstract": "This paper describes our system, dubbed WS4A (Web Services for All), that participated in the fourth edition of the BioASQ challenge (2016). We used WS4A to perform the Question and Answering (QA) task 4b, which consisted on the retrieval of relevant concepts, documents, snippets, RDF triples, exact answers and ideal answers for each given question. The novelty in our approach consists on the maximum exploitation of existing web services in each step of WS4A, such as the annotation of text, and the retrieval of metadata for each annotation. The information retrieved included concept identifiers, ontologies, ancestors, and most importantly, PubMed identifiers. The paper describes the WS4A pipeline and also presents the precision, recall and f-measure values obtained in task 4b.", "histories": [["v1", "Tue, 27 Sep 2016 15:14:04 GMT  (326kb,D)", "https://arxiv.org/abs/1609.08492v1", "7 pages, 1 figure, 1 table, accepted as poster at BioASQ '16"], ["v2", "Thu, 17 Nov 2016 12:12:15 GMT  (355kb,D)", "http://arxiv.org/abs/1609.08492v2", "7 pages, 1 figure, 1 table, accepted as poster at BioASQ '16"]], "COMMENTS": "7 pages, 1 figure, 1 table, accepted as poster at BioASQ '16", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["miguel j rodrigues", "miguel fal\\'e", "re lamurias", "francisco m couto"], "accepted": false, "id": "1609.08492"}, "pdf": {"name": "1609.08492.pdf", "metadata": {"source": "CRF", "title": "WS4A: a Biomedical Question and Answering System based on public Web Services and Ontologies", "authors": ["Miguel J. Rodrigues", "Miguel Fal\u00e9", "Andre Lamurias", "Francisco M. Couto"], "emails": ["mrodrigues@lasige.di.fc.ul.pt"], "sections": [{"heading": null, "text": "Keywords: natural language processing, web services, name recognition, information search, support vector machines, semantic similarity, question and answer"}, {"heading": "1 Introduction", "text": "This paper describes our participation in the BioASQ Challenge edition of 20161, a large-scale challenge in biomedical semantic indexing and answering questions. Our participation focused on the question and answer (QA) task 4b, which consisted of retrieving relevant concepts, documents, snippets, RDF triples, exact answers and \"ideal answers\" for each given question. The competition consisted of two phases: the first was to take a series of queries as input and respond with a series of the most relevant concepts, articles, snippets and RDF triples. Ontologies and terminologies from which the list of concepts were to be retrieved are: the medical subject headings (MeSH); the genes ontology (GO); the Universal Protein Resource (UniProt); the Joint Chemical Dictionary (Jochem); the Disease Ontology (Phase DO) the second questions were initially answered in the 4v."}, {"heading": "2 Web Services", "text": "There are some other parameters that WS4A has explored, but for the purpose of better filtering the results, such as http http: / / www.http http: / / www.http http: / / www.iii) number exclusion (set to false); iii) full word only (set to true) and iv) synonymous exclusion (set to false).The results have been provided as a JSON dictionary, divided by annotation and ontology.With the UniProt Web Services [3], we can collect PubMedMedMedMedMedIds from protein descriptions, but to get them, we need to obtain this description, the Whatizit2 Web Service.Whatizit [4] is an alternative to BioPortal when it comes to specific terms in sentences."}, {"heading": "3 Pipeline", "text": "In fact, most of them will be able to play by the rules that they have shown in recent years, and they will be able to play by the rules that they have shown."}, {"heading": "4 Results", "text": "Table 1 shows WS4A results for each batch and the use of the final version of the system. Times were achieved in a desktop computer running an Intel (R) Core (TM) 2 Duo CPU and 6GB of RAM. Our results, among all participants placed in the lower half of the results table, except for the fourth batch result, where we did not present a results table. F-Measure's best result was an average of 0.24, so the overall results were not very high. These results reflect Phase A. In Phase B, we were able to achieve a top-3 result for the first batch, which resulted in lower table results for the other batches. We achieved two second places in the first batch for the accurate answers and ideal answers categories.Comparing the results of last year [8], WS4A was far from achieving a comparable performance for the first batch."}, {"heading": "5 Conclusions", "text": "In this paper, we demonstrated the feasibility of developing a question and answer system based largely on web services, many of the techniques used in WS4A have previously been used in other systems. For example, IIT [11] retrieved documents using PubMeds web services such as WS4A and extracted snippets based on similarity between the sentences of the retrieved documents and the query. LIMSI-CNRS [12] used WordNet relationships (namely synonyms) when comparing words from the query and the answer selection and the given short text. WS4A also used WordNet when comparing the words in the query and the words in the abstracts. The main difference between WS4A and the systems mentioned above is that our system is mainly based on web services and the choice of the answer and the given short text, and the semantics given by the ontologies fully investigated."}, {"heading": "Acknowledgments", "text": "This work was supported by FCT with the funding of the LaSIGE Research Unit, Ref. UID / CEC / 00408 / 2013."}], "references": [{"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V Dubourg"], "venue": "The Journal of Machine Learning Research 12", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Bioportal: ontologies and integrated data resources at the click of a mouse", "author": ["N.F. Noy", "N.H. Shah", "P.L. Whetzel", "B. Dai", "M. Dorf", "N. Griffith", "C. Jonquet", "D.L. Rubin", "M.A. Storey", "Chute", "C.G"], "venue": "Nucleic acids research", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "Uniprot: a hub for protein information", "author": ["U Consortium"], "venue": "Nucleic acids research", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Text processing through web services: calling whatizit", "author": ["D. Rebholz-Schuhmann", "M. Arregui", "S. Gaudan", "H. Kirsch", "A. Jimeno"], "venue": "Bioinformatics 24(2)", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2008}, {"title": "Pubchem: a public information system for analyzing bioactivities of small molecules", "author": ["Y. Wang", "J. Xiao", "T.O. Suzek", "J. Zhang", "J. Wang", "S.H. Bryant"], "venue": "Nucleic acids research", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Building customized data pipelines using the entrez programming utilities (eutils)", "author": ["E. Sayers", "D. Wheeler"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2004}, {"title": "Preface", "author": ["Linda Cappellato", "G.F.J. Nicola Ferro", "E.S. Juan"], "venue": "CLEF 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Results of the bioasq tasks of the question answering lab at clef 2015", "author": ["G. Balikas", "A. Kosmopoulos", "A. Krithara", "G. Paliouras", "I. Kakadiaris"], "venue": "CLEF 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning to answer biomedical factoid & list questions: Oaqa at bioasq 3b", "author": ["Zi Yang", "Niloy Gupta", "X.S.D.X.C.Z.", "E. Nyberg"], "venue": "CLEF 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Making large scale svm learning practical", "author": ["T. Joachims"], "venue": "Technical report, Universit\u00e4t Dortmund", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1999}, {"title": "Iiith at bioasq challange 2015 task 3b: Bio-medical question answering system", "author": ["Harish Yenala", "M.S. Avinash Kamineni", "M. Chinnakotla"], "venue": "CLEF 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Limsi-cnrs@clef 2015: Tree edit beam search for multiple choice question answering", "author": ["M. Gleize", "B. Grau"], "venue": "CLEF 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "The next generation of similarity measures that fully explore the semantics in biomedical ontologies", "author": ["F.M. Couto", "H.S. Pinto"], "venue": "Journal of bioinformatics and computational biology 11(05)", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Comic: Exploring text segmentation and similarity in the english entrance exams task", "author": ["B.R. Ramon Ziai"], "venue": "CLEF 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Biomedical question answering using the yodaqa system: Prototype notes", "author": ["J.S. Petr Baudis"], "venue": "CLEF 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Additionally, WS4A used Machine Learning [1] techniques to classify if an abstract is either relevant or not for the given query.", "startOffset": 41, "endOffset": 44}, {"referenceID": 1, "context": "The first Web Service used was the one provided by BioPortal [2], that given a text returns the ontology concepts mentioned in it.", "startOffset": 61, "endOffset": 64}, {"referenceID": 2, "context": "With the UniProt Web Services [3], we may gather PubMedIds from protein descriptions, but to get these, we need to use another Web Service in order the obtain this description, the Whatizit2 Web Service.", "startOffset": 30, "endOffset": 33}, {"referenceID": 3, "context": "Whatizit [4] is an alternative to BioPortal, when it comes to identifying specific terms in sentences.", "startOffset": 9, "endOffset": 12}, {"referenceID": 4, "context": "When it comes to retrieving PubMed identifiers (PubMed IDs), the PubChem Web Service [5] was the last one to be used.", "startOffset": 85, "endOffset": 88}, {"referenceID": 5, "context": "PubMedID/JSON, NCBI has a service, eutils [6], that provides a broad range of options.", "startOffset": 42, "endOffset": 45}, {"referenceID": 6, "context": "In CLEF 2015 [7] among the systems that entered the BioASQ task 3b [8], we noted that OAQA [9] used supervised learning techniques to predict answer types (answer type coercion) and score candidate answers.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "In CLEF 2015 [7] among the systems that entered the BioASQ task 3b [8], we noted that OAQA [9] used supervised learning techniques to predict answer types (answer type coercion) and score candidate answers.", "startOffset": 67, "endOffset": 70}, {"referenceID": 8, "context": "In CLEF 2015 [7] among the systems that entered the BioASQ task 3b [8], we noted that OAQA [9] used supervised learning techniques to predict answer types (answer type coercion) and score candidate answers.", "startOffset": 91, "endOffset": 94}, {"referenceID": 9, "context": "We used Support vector machines [10,1] to classify the relevance of the abstracts to a given query.", "startOffset": 32, "endOffset": 38}, {"referenceID": 0, "context": "We used Support vector machines [10,1] to classify the relevance of the abstracts to a given query.", "startOffset": 32, "endOffset": 38}, {"referenceID": 7, "context": "Comparing to last year\u2019s results [8], WS4A was far from achieving a comparable performance mainly because we registered for this competition some weeks before the date of the first batch release and no time for tuning the system was available.", "startOffset": 33, "endOffset": 36}, {"referenceID": 10, "context": "For example, IIT [11] retrieved documents using PubMed\u2019s web services like WS4A did, and extracted snippets based on similarity between sentences of the retrieved documents and the query.", "startOffset": 17, "endOffset": 21}, {"referenceID": 11, "context": "LIMSI-CNRS [12] used WordNet relations (namely, synonyms) when comparing between words from the query and the answer choice, and the given short text.", "startOffset": 11, "endOffset": 15}, {"referenceID": 12, "context": "In the future, we intend to explore semantic similarity measures [13], and build a cache database in order to save and revisit results.", "startOffset": 65, "endOffset": 69}, {"referenceID": 13, "context": "Snippet generation can also be improved through semantic similarity, as CoMiC [14] used semantic similarity for the short text\u2019s segmentation in the Entrance Exams task 2015 (using the C99 algorithm).", "startOffset": 78, "endOffset": 82}, {"referenceID": 14, "context": "YodaQA [15] resorts to DBPedia Spotlight, a service that automatically annotates DBPedia concepts from plain text.", "startOffset": 7, "endOffset": 11}], "year": 2016, "abstractText": "This paper describes our system, dubbed WS4A (Web Services for All), that participated in the fourth edition of the BioASQ challenge (2016). We used WS4A to perform the Question and Answering (QA) task 4b, which consisted on the retrieval of relevant concepts, documents, snippets, RDF triples, exact answers and \u201dideal answers\u201d for each given question. The novelty in our approach consists on the maximum exploitation of existing web services in each step of WS4A, such as the annotation of text, and the retrieval of metadata for each annotation. The information retrieved included concept identifiers, ontologies, ancestors, and most importantly, PubMed identifiers. The paper describes the WS4A pipeline and also presents the precision, recall and f-measure values obtained in task 4b. Our system achieved two second places in two subtasks on one of the five batches.", "creator": "LaTeX with hyperref package"}}}