{"id": "1609.08496", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "27-Sep-2016", "title": "Topic Modeling over Short Texts by Incorporating Word Embeddings", "abstract": "Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks, such as content charactering, user interest profiling, and emerging topic detecting. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this prob- lem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn se- mantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudo- texts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models.", "histories": [["v1", "Tue, 27 Sep 2016 15:26:07 GMT  (343kb,D)", "http://arxiv.org/abs/1609.08496v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["jipeng qiang", "ping chen", "tong wang", "xindong wu"], "accepted": false, "id": "1609.08496"}, "pdf": {"name": "1609.08496.pdf", "metadata": {"source": "CRF", "title": "Topic Modeling over Short Texts by Incorporating Word Embeddings", "authors": ["Jipeng Qiang", "Ping Chen", "Tong Wang", "Xindong Wu"], "emails": ["qjp2100@163.com"], "sections": [{"heading": null, "text": "Keywords: Topic Modeling; Short Text; Word Embdddings; Markov Random Field"}, {"heading": "1. INTRODUCTION", "text": "It has proven to be useful for automatic topic identification from a huge volume of text. [It is] a very complex model of text views as a mix of likely topics where a topic is represented by a series of topics, so many topic models such as latent dirichlet allocation (LDA) have shown great success on long texts [1, 4-4503-2138-9.DOI]. With the rapid development of the World Wide Web, short text has been an important source of information not only in traditional websites, e.g. website titles, text advertising, but in emerging social media, e.g. tweet, status message, and question on websites. Compared with long texts, such as news articles and academic essays, the topic of discovery of short texts has the following three challenges: very limited word advertising, but in emerging social media, tweet, status message, and question on websites."}, {"heading": "2. RELATED WORK", "text": "In this section we briefly describe the work on this subject from the following two aspects: modeling long text topics and modeling short text topics."}, {"heading": "2.1 Long Text Topic Modeling", "text": "Nigam et al. [17] proposed a mixture of unique models based on the assumption that each document is generated by a topic, a simple assumption that is often too limited to effectively model a large collection of long texts, and the complex assumption that each text is modeled over several topics has been widely used by topic-finding from long texts [5, 1, 4]. In a way, the complex assumption captures the possibility that a document can contain multiple topics, and based on this assumption, knowledge-based topic models have been proposed in recent years that invite human users to provide some prior knowledge to guide the model to better topics, rather than simply relying on how often words occur in different contexts together. For example, Chen and Liu code the must-links (meaning that two words should be in the same topic) and link the variable links between two words in a single context."}, {"heading": "2.2 Short Text Topic Modeling", "text": "The earliest work on short text models focused mainly on the use of external knowledge to enhance the presentation of short texts. Jin et al. [7] initially found the related long texts for each short text and learned topics about short texts and their associated long texts using LDA. Phan et al. [20], however, learned the topics on another large dataset using a conventional topic model such as PLSA and LDA for short text classification. However, these models are only effective if the additional data is closely related to the original data. Furthermore, finding such additional data can be expensive or even impossible. Since many short texts were collected from social networks such as Twitter, many people analyze this type of data to find latent topics for different tasks, such as event tracking [11], content recommendation [21] and influential user prediction [29]. Due to the lack of specific topic models for short texts, some directly applied long text models work."}, {"heading": "3. ALGORITHM", "text": "In this section, we discuss our method, the Embedding-based Topic Model (ETM), for identifying the key topics underlying a collection of short texts. Our ETM model comprises three steps: First, we build distributed word embeddings for the vocabulary of the collection; second, we aggregate short texts into long pseudo-texts by incorporating the semantic knowledge from word embeddings; and third, we implement K-means using a new metric, Word Mover's Distance (WMD) [9] to calculate the distance between two short texts; and third, we use a Markov Random Field regulated model that uses word embeddings in a soft and theme-dependent way to improve the coherence of theme modeling."}, {"heading": "3.1 Word Embeddings", "text": "Mikolov et al. introduced Word2Vec to learn a vector representation for each word using a flat neural network architecture consisting of an input layer, a projection layer and an output layer for predicting nearby words [14, 15]. Word2Vec applies a standard technique such as skipping words onto the given corpus. The model avoids non-linear transformations and therefore makes training extremely efficient. This allows learning embedded word vectors from huge datasets with billions of words and millions of words in vocabulary. Word embedding can capture subtle semantic relationships between words, such as vec (Berlin) - vec (Germany) + vec (France) - vec (pairs) - vec (Einstein) - vec (scientist) + vec (picasso) - vec (painter) - vec (painter) - vec (x) uses the vector of the word calculation window [15] to improve the global context - vector of the word calculation window [15]."}, {"heading": "3.2 Short Text Clustering", "text": "There are only a few words that occur in each text. For example, we have three short texts that we use when we use these two terms. (...) There are only two terms that we cannot understand. (...) We assume that each text is represented as a standardized word (nBOW) vector, ri RV is the vector of si, a vector of si, a vector of si, a vector of si, a vector of si, a vector of si, a vector of si, a vector of si, a vector of i, j V = 1 ci, where ci, j denotesthe occurence times of the jth word is the jth word of the vocabulary vocabulary in text si. We can see that a vector of si, a vector of si, a vector of i = 1 ci, j denotesthe ecurence times of the jth word is the vector of the text si, that we can see a vector of si."}, {"heading": "3.3 Topic Inference", "text": "In this subsection, we present the distribution of words in a topic derived from the long pseudo-texts that Markov Random Field Regularized (MRF) uses (MRF) model and parameter estimation based on collapsed Gibbs sampling.3.1 Model Descritpion We adopt the MRF model to learn the latent topics that can integrate the word distances into the MRF model [1] by extending the standard model of MRF by adding a Markov model to the latent theme layer.Suppose the corpus contains the word embeddings to calculate the distance between words. We can see from Figure 3, MRF model extends to the standard LDA model by imposing a latent topic that contains the corpus topics that represent unique words in the vocabulary language with L texts over V."}, {"heading": "4. EXPERIMENTS", "text": "In this section we show the experimental results to demonstrate the effectiveness of our model by comparing it with five baselines on two real datasets."}, {"heading": "4.1 Datasets Description and Setup", "text": "Datasets: We examine the empirical performance of ETM on two sets of short text data. \u2022 Tweet2011: Tweet2011 Collection is a standard short text collection published on TREC 2011 microblog track1, which includes approximately 16 million tweets sampled between January 23 and February 8, 2011. \u2022 GoogleNews: Similar to existing papers [34], we use Google news2 as a dataset to evaluate the performance of topic models. On Google News dataset, all news articles are automatically grouped into clusters. We took a snapshot of Google news from April 27, 2015 and crawled the titles of 6,974 news articles from 134 categories. For each dataset, we perform the following pre-processing: (1) Convert letters to lowercase letters; (2) Remove non-Latin characters and tags; (3) Remove words less than 20 or longer."}, {"heading": "4.2 Experimental Results", "text": "In fact, most people are able to decide for themselves what they want and what they want. It's not like they're able to decide what they want and what they want. It's like they're not doing it. It's like they're doing it. It's like they're doing it. It's like they're doing it. It's like they're doing it. It's like they're doing it. It's like they're doing it. It's like they're doing it. It's like they're not doing it. It's like they're doing it. It's like they're doing it. It's like they're doing it. It's like they're doing it. It's like they're not doing it."}, {"heading": "5. CONCLUSION", "text": "We propose a novel model, Embedding-based Topic Modeling (ETM), to discover latent topics from short texts. ETM first aggregates short texts into long pseudo-texts by incorporating semantic knowledge from word embedding, and then derives topics from long pseudo-texts using the Markov Random Field regularized model, which encourages words that are described as similar to share the same topic assignment. Therefore, by incorporating semantic knowledge, ETM can alleviate the problem of very limited word coincidence information in short texts. Experimental results on two real-world short data sets confirm its effectiveness both qualitatively and quantitatively compared to state-of-the-art methods."}, {"heading": "6. ADDITIONAL AUTHORS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7. REFERENCES", "text": "[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latentdirichlet G. 6 J. Daten Zuordnung. The Journal of Machine Learning research, 3: 993-1022, 2003. [2] Z. Chen and B. Liu. Mining topics in documents: standing on the shoulder of big data. In SIGKDD, pp. 1116-1125. ACM, 2014. [3] X. Cheng, X. Yan, Y. Lan, and J. Guo. Btm: Topic modeling over the short texts. Knowledge and Data Engineering, IEEE Transactions on, 26 (12): 2928-2941, 2014. [4] T. Griffiths and M. Steyvers. Proceedings of the National Academy of Sciences, 101 (suppl 1): 5228-5235, 2004. [5] T. Hofmann. Probabilistic latent semantic indexing."}], "references": [{"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2003}, {"title": "Mining topics in documents: standing on the shoulders of big data", "author": ["Z. Chen", "B. Liu"], "venue": "In SIGKDD,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Btm: Topic modeling over short texts. Knowledge and Data Engineering", "author": ["X. Cheng", "X. Yan", "Y. Lan", "J. Guo"], "venue": "IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Finding scientific topics", "author": ["T.L. Griffiths", "M. Steyvers"], "venue": "Proceedings of the National Academy of Sciences,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2004}, {"title": "Probabilistic latent semantic indexing", "author": ["T. Hofmann"], "venue": "In SIGIR, pages 50\u201357", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1999}, {"title": "Dirichlet process mixture model for document clustering with feature partition", "author": ["R. Huang", "G. Yu", "Z. Wang", "J. Zhang", "L. Shi"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2013}, {"title": "Transferring topical knowledge from auxiliary long texts for short text clustering", "author": ["O. Jin", "N.N. Liu", "K. Zhao", "Y. Yu", "Q. Yang"], "venue": "In Proceedings of the 20th ACM international conference on Information and knowledge management,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Taxicab geometry: An adventure in non-Euclidean geometry", "author": ["E.F. Krause"], "venue": "Courier Corporation,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "From word embeddings to document distances", "author": ["M.J. Kusner", "Y. Sun", "N.I. Kolkin", "K.Q. Weinberger"], "venue": "In ICML,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "On-line trend analysis with topic models:\\# twitter trends detection topic model online", "author": ["J.H. Lau", "N. Collier", "T. Baldwin"], "venue": "In COLING,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Pet: a statistical model for popular events tracking in social communities", "author": ["C.X. Lin", "B. Zhao", "Q. Mei", "J. Han"], "venue": "In SIGKDD,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2010}, {"title": "An efficient earth mover\u2019s distance algorithm for robust histogram comparison", "author": ["H. Ling", "K. Okada"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2007}, {"title": "Improving lda topic models for microblogs via tweet pooling and automatic labeling", "author": ["R. Mehrotra", "S. Sanner", "W. Buntine", "L. Xie"], "venue": "In SIGIR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "In NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Improving topic coherence with regularized topic models", "author": ["D. Newman", "E.V. Bonilla", "W. Buntine"], "venue": "In NIPS,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2011}, {"title": "Text classification from labeled and unlabeled documents using em", "author": ["K. Nigam", "A.K. McCallum", "S. Thrun", "T. Mitchell"], "venue": "Machine learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2000}, {"title": "Fast and robust earth mover\u2019s distances", "author": ["O. Pele", "M. Werman"], "venue": "In ICCV,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2009}, {"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "In EMNLP,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2014}, {"title": "Learning to classify short and sparse text & web with hidden topics from large-scale data collections", "author": ["X.-H. Phan", "L.-M. Nguyen", "S. Horiguchi"], "venue": "In WWW,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2008}, {"title": "Using twitter to recommend real-time topical news", "author": ["O. Phelan", "K. McCarthy", "B. Smyth"], "venue": "In Proceedings of the third ACM conference on Recommender systems,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Short and sparse text topic modeling via self-aggregation", "author": ["X. Quan", "C. Kit", "Y. Ge", "S.J. Pan"], "venue": "In Proceedings of the 24th International Conference on Artificial Intelligence,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Characterizing microblogs with topic models", "author": ["D. Ramage", "S.T. Dumais", "D.J. Liebling"], "venue": "International AAAI Conference on Weblogs and Social Media,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2010}, {"title": "A metric for distributions with applications to image databases", "author": ["Y. Rubner", "C. Tomasi", "L.J. Guibas"], "venue": "In Computer Vision,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1998}, {"title": "Probabilistic author-topic models for information discovery", "author": ["M. Steyvers", "P. Smyth", "M. Rosen-Zvi", "T. Griffiths"], "venue": "In SIGKDD,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2004}, {"title": "Extended topic model for word dependency", "author": ["T. Wang", "V. Viswanath", "P. Chen"], "venue": "In ACL: short paper,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Exploring social context for topic identification in short and noisy texts", "author": ["X. Wang", "Y. Wang", "W. Zuo", "G. Cai"], "venue": "In AAAI,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Tm-lda: efficient online modeling of latent topic transitions in social media", "author": ["Y. Wang", "E. Agichtein", "M. Benzi"], "venue": "In SIGKDD,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2012}, {"title": "Twitterrank: finding topic-sensitive influential twitterers", "author": ["J. Weng", "E.-P. Lim", "J. Jiang", "Q. He"], "venue": "In WSDM,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Integer and combinatorial optimization", "author": ["L.A. Wolsey", "G.L. Nemhauser"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Integrating document clustering and topic modeling", "author": ["P. Xie", "E.P. Xing"], "venue": "Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Incorporating word  correlation knowledge into topic modeling", "author": ["P. Xie", "D. Yang", "E.P. Xing"], "venue": "In Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "A probabilistic model for bursty topic discovery in microblogs", "author": ["X. Yan", "J. Guo", "Y. Lan", "J. Xu", "X. Cheng"], "venue": "In AAAI,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "A dirichlet multinomial mixture model-based approach for short text clustering", "author": ["J. Yin", "J. Wang"], "venue": "In SIGKDD,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Comparing twitter and traditional media using topic models", "author": ["W.X. Zhao", "J. Jiang", "J. Weng", "J. He", "E.-P. Lim", "H. Yan", "X. Li"], "venue": "In Advances in Information Retrieval,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "1145/1235 modeled over a set of topics, many topic models such as Latent Dirichlet Allocation (LDA) have demonstrated great success on long texts [1, 4, 26].", "startOffset": 146, "endOffset": 156}, {"referenceID": 3, "context": "1145/1235 modeled over a set of topics, many topic models such as Latent Dirichlet Allocation (LDA) have demonstrated great success on long texts [1, 4, 26].", "startOffset": 146, "endOffset": 156}, {"referenceID": 25, "context": "1145/1235 modeled over a set of topics, many topic models such as Latent Dirichlet Allocation (LDA) have demonstrated great success on long texts [1, 4, 26].", "startOffset": 146, "endOffset": 156}, {"referenceID": 21, "context": "Compared with long texts, such as news article and academic paper, topic discovery from short texts has the following three challenges: only very limited word co-occurrence information is available, the frequency of words plays a less discriminative role, and the limited contexts make it more difficult to identify the senses of ambiguous words [22].", "startOffset": 346, "endOffset": 350}, {"referenceID": 33, "context": "Therefore, LDA cannot work very well on short texts [34, 3].", "startOffset": 52, "endOffset": 59}, {"referenceID": 2, "context": "Therefore, LDA cannot work very well on short texts [34, 3].", "startOffset": 52, "endOffset": 59}, {"referenceID": 9, "context": "Finally, how to extract topics from short texts remains a challenging research problem [10, 27].", "startOffset": 87, "endOffset": 95}, {"referenceID": 26, "context": "Finally, how to extract topics from short texts remains a challenging research problem [10, 27].", "startOffset": 87, "endOffset": 95}, {"referenceID": 32, "context": "One follows the simple assumption that each text is sampled from only one latent topic which is totally unsuited to long texts, but it can be suitable for short texts compared to the complex assumption that each text is modeled over a set of topics [33, 35].", "startOffset": 249, "endOffset": 257}, {"referenceID": 34, "context": "One follows the simple assumption that each text is sampled from only one latent topic which is totally unsuited to long texts, but it can be suitable for short texts compared to the complex assumption that each text is modeled over a set of topics [33, 35].", "startOffset": 249, "endOffset": 257}, {"referenceID": 2, "context": "Therefore, many models for short texts were proposed based on this simple assumption [3, 34].", "startOffset": 85, "endOffset": 92}, {"referenceID": 33, "context": "Therefore, many models for short texts were proposed based on this simple assumption [3, 34].", "startOffset": 85, "endOffset": 92}, {"referenceID": 12, "context": "The other strategy takes advantage of various heuristic ties among short texts to aggregate them into long pseudo-texts before topic inference that can help improve word co-occurrence information [13, 22, 29].", "startOffset": 196, "endOffset": 208}, {"referenceID": 21, "context": "The other strategy takes advantage of various heuristic ties among short texts to aggregate them into long pseudo-texts before topic inference that can help improve word co-occurrence information [13, 22, 29].", "startOffset": 196, "endOffset": 208}, {"referenceID": 28, "context": "The other strategy takes advantage of various heuristic ties among short texts to aggregate them into long pseudo-texts before topic inference that can help improve word co-occurrence information [13, 22, 29].", "startOffset": 196, "endOffset": 208}, {"referenceID": 14, "context": "Our method leverages recent results by word embeddings that obtain vector representations for words[15, 19].", "startOffset": 99, "endOffset": 107}, {"referenceID": 18, "context": "Our method leverages recent results by word embeddings that obtain vector representations for words[15, 19].", "startOffset": 99, "endOffset": 107}, {"referenceID": 18, "context": "For example, all distances in Figure 1 are computed by word embedding model [19].", "startOffset": 76, "endOffset": 80}, {"referenceID": 31, "context": "Gaining insights from [32], ETM adopts a Markov Random Field regularized model based on collapsed Gibbs sampling which utilizes word embeddings in a soft and topic-dependent manner to improve the coherence of topic modeling.", "startOffset": 22, "endOffset": 26}, {"referenceID": 16, "context": "[17] proposed a mixture of unigrams model based on the assumption that each document is generated by one topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 4, "context": "The complex assumption that each text is modeled over multiple topics was widely used by topic discovery from long texts [5, 1, 4].", "startOffset": 121, "endOffset": 130}, {"referenceID": 0, "context": "The complex assumption that each text is modeled over multiple topics was widely used by topic discovery from long texts [5, 1, 4].", "startOffset": 121, "endOffset": 130}, {"referenceID": 3, "context": "The complex assumption that each text is modeled over multiple topics was widely used by topic discovery from long texts [5, 1, 4].", "startOffset": 121, "endOffset": 130}, {"referenceID": 4, "context": "Based on this assumption, many topic models such as Probabilistic Latent Semantic Analysis (PLSA) [5] and Latent Dirichlet Allocation (LDA) [1] have shown promising results.", "startOffset": 98, "endOffset": 101}, {"referenceID": 0, "context": "Based on this assumption, many topic models such as Probabilistic Latent Semantic Analysis (PLSA) [5] and Latent Dirichlet Allocation (LDA) [1] have shown promising results.", "startOffset": 140, "endOffset": 143}, {"referenceID": 1, "context": "For example, Chen and Liu encode the Must-Links (meaning that two words should be in the same topic) and Cannot-Links (meaning that two words should not be in the same topic) between words over the topic-word multinomials [2].", "startOffset": 222, "endOffset": 225}, {"referenceID": 15, "context": ", a quadratic regularized topic model based on semi-collapsed Gibbs sampler [16] and a Markov Random Field regularized Latent Dirichlet Allocation model based on Variational Inference[32], share the idea of incorporate the correlation between words.", "startOffset": 76, "endOffset": 80}, {"referenceID": 31, "context": ", a quadratic regularized topic model based on semi-collapsed Gibbs sampler [16] and a Markov Random Field regularized Latent Dirichlet Allocation model based on Variational Inference[32], share the idea of incorporate the correlation between words.", "startOffset": 183, "endOffset": 187}, {"referenceID": 6, "context": "[7] first found the related long texts for each short text, and learned topics over short texts and their related long texts using LDA.", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "[20] learned the topics on another largescale dataset using a conventional topic model such as PLSA and LDA for short text classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "As a lot of short texts have been collected from social networks such as Twitter, many people analyze this type of data to find latent topics for various tasks, such as event tracking [11], content recommendation [21], and influential users prediction [29].", "startOffset": 184, "endOffset": 188}, {"referenceID": 20, "context": "As a lot of short texts have been collected from social networks such as Twitter, many people analyze this type of data to find latent topics for various tasks, such as event tracking [11], content recommendation [21], and influential users prediction [29].", "startOffset": 213, "endOffset": 217}, {"referenceID": 28, "context": "As a lot of short texts have been collected from social networks such as Twitter, many people analyze this type of data to find latent topics for various tasks, such as event tracking [11], content recommendation [21], and influential users prediction [29].", "startOffset": 252, "endOffset": 256}, {"referenceID": 22, "context": "Initially, due to the lack of specific topic models for short texts, some works directly applied long text topic models [23, 28].", "startOffset": 120, "endOffset": 128}, {"referenceID": 27, "context": "Initially, due to the lack of specific topic models for short texts, some works directly applied long text topic models [23, 28].", "startOffset": 120, "endOffset": 128}, {"referenceID": 12, "context": "Since only very limited word co-occurrence information is available in short texts, some works took advantages of various heuristic ties among short texts to aggregate them into long pseudo-documents before topic inference [13, 22].", "startOffset": 223, "endOffset": 231}, {"referenceID": 21, "context": "Since only very limited word co-occurrence information is available in short texts, some works took advantages of various heuristic ties among short texts to aggregate them into long pseudo-documents before topic inference [13, 22].", "startOffset": 223, "endOffset": 231}, {"referenceID": 24, "context": "The strategy can be regarded as an application of the authortopic model [25] to tweets, where each tweet (text) has a single author.", "startOffset": 72, "endOffset": 76}, {"referenceID": 28, "context": "For example, some models aggregated all the tweets of a user as a pseudo-text [29].", "startOffset": 78, "endOffset": 82}, {"referenceID": 12, "context": "[13] aggregated all tweets into a pseudo-text based on hashtags.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "The other scheme directly aggregates short texts into long pseudo-texts through clustering methods [22], in which the clustering method will face this same problem of very limited word co-occurrence information.", "startOffset": 99, "endOffset": 103}, {"referenceID": 21, "context": "Recently, some works found that even through the assumption that each text is generated by one topic does not fit long texts, it can work well for short texts [22, 34].", "startOffset": 159, "endOffset": 167}, {"referenceID": 33, "context": "Recently, some works found that even through the assumption that each text is generated by one topic does not fit long texts, it can work well for short texts [22, 34].", "startOffset": 159, "endOffset": 167}, {"referenceID": 34, "context": "[35] empirically compared the data with traditional news media, and proposed a Twitter-LDA model by assuming that one tweet is generated from one topic.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "Yin and Wang [34] also adopted this assumption for topic inference based on Gibbs sampling.", "startOffset": 13, "endOffset": 17}, {"referenceID": 15, "context": "Therefore, motivated by the results that prior domain knowledge is useful for long text topic models[16, 32], we will propose a novel method for short texts by incorporating the external word correlation knowledge provided by word embeddings to improve the quality of topic modeling.", "startOffset": 100, "endOffset": 108}, {"referenceID": 31, "context": "Therefore, motivated by the results that prior domain knowledge is useful for long text topic models[16, 32], we will propose a novel method for short texts by incorporating the external word correlation knowledge provided by word embeddings to improve the quality of topic modeling.", "startOffset": 100, "endOffset": 108}, {"referenceID": 8, "context": "We implement K-means using a new metric, Word Mover\u2019s Distance (WMD) [9], to compute the distance between two short texts.", "startOffset": 69, "endOffset": 72}, {"referenceID": 13, "context": "introduced Word2Vec, to learn a vector representation for each word using a shallow neural network architecture that consists of an input layer, a projection layer, and an output layer to predict nearby words [14, 15].", "startOffset": 209, "endOffset": 217}, {"referenceID": 14, "context": "introduced Word2Vec, to learn a vector representation for each word using a shallow neural network architecture that consists of an input layer, a projection layer, and an output layer to predict nearby words [14, 15].", "startOffset": 209, "endOffset": 217}, {"referenceID": 14, "context": "Word embeddings can capture subtle semantic relationships between words, such as vec(Berlin) vec(Germany) + vec(France)\u2248 vec(Pairs) and vec(Einstein) - vec(scientist) + vec(Picasso) \u2248 vec(painter), where vec(x) denotes the vector of word x [15].", "startOffset": 240, "endOffset": 244}, {"referenceID": 7, "context": ", Euclidean distance, Manhattan distance [8], Cosine Similarity ) to measure distance between two texts, it is hard to find their difference.", "startOffset": 41, "endOffset": 44}, {"referenceID": 8, "context": "Therefore, we introduce a new metric, called the Word Mover\u2019s Distance (WMD)[9], to compute the distance between texts.", "startOffset": 76, "endOffset": 79}, {"referenceID": 23, "context": "Mover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18].", "startOffset": 23, "endOffset": 31}, {"referenceID": 29, "context": "Mover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18].", "startOffset": 23, "endOffset": 31}, {"referenceID": 11, "context": "Mover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 17, "context": "Mover\u2019s Distance (EMD) [24, 30], a well-known transportation problem for which specialized solvers have been developed [12, 18].", "startOffset": 119, "endOffset": 127}, {"referenceID": 31, "context": "We adopt the MRF model to learn the latent topics which can incorporate word distances into topic modeling for encouraging words labeled similarly to share the same topic assignment [32].", "startOffset": 182, "endOffset": 186}, {"referenceID": 0, "context": "We can see from Figure 3, MRF model extends the standard LDA model [1] by imposing a Markov Random Field on the latent topic layer.", "startOffset": 67, "endOffset": 70}, {"referenceID": 4, "context": "(5) b) draw wli \u223c Multinomial(\u03c6zli) for each word in lth pseudo-text There have been a number of inference methods that have been used to estimate the parameters of topic models, from basic expectation maximization [5], to approximate inference methods like Variational Inference [1] and Gibbs sampling [4].", "startOffset": 215, "endOffset": 218}, {"referenceID": 0, "context": "(5) b) draw wli \u223c Multinomial(\u03c6zli) for each word in lth pseudo-text There have been a number of inference methods that have been used to estimate the parameters of topic models, from basic expectation maximization [5], to approximate inference methods like Variational Inference [1] and Gibbs sampling [4].", "startOffset": 280, "endOffset": 283}, {"referenceID": 3, "context": "(5) b) draw wli \u223c Multinomial(\u03c6zli) for each word in lth pseudo-text There have been a number of inference methods that have been used to estimate the parameters of topic models, from basic expectation maximization [5], to approximate inference methods like Variational Inference [1] and Gibbs sampling [4].", "startOffset": 303, "endOffset": 306}, {"referenceID": 31, "context": "Therefore, different from this paper [32] based on Variational Inference, we will use collapsed Gibbs sampling to estimate parameters under Dirichlet priors in this paper.", "startOffset": 37, "endOffset": 41}, {"referenceID": 33, "context": "\u2022 GoogleNews: Similar to existing papers [34], we utilize Google news as a dataset to evaluate the performance of topic models.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "\u2022 Three short text topic models, Unigrams [17], DMM [34], and BTM [3].", "startOffset": 42, "endOffset": 46}, {"referenceID": 33, "context": "\u2022 Three short text topic models, Unigrams [17], DMM [34], and BTM [3].", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "\u2022 Three short text topic models, Unigrams [17], DMM [34], and BTM [3].", "startOffset": 66, "endOffset": 69}, {"referenceID": 3, "context": "\u2022 Two Long text topic models, LDA [4] and MRF-LDA [32].", "startOffset": 34, "endOffset": 37}, {"referenceID": 31, "context": "\u2022 Two Long text topic models, LDA [4] and MRF-LDA [32].", "startOffset": 50, "endOffset": 54}, {"referenceID": 14, "context": "Word Embeddings: Word2Vec [15] and Glob2Vec [19] are different word embeddings.", "startOffset": 26, "endOffset": 30}, {"referenceID": 18, "context": "Word Embeddings: Word2Vec [15] and Glob2Vec [19] are different word embeddings.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "As Glob2Vec has better performance than Word2Vec [19], the pre-trained embeddings by Glob2Vec based on Wikipedia is incorporated into our model and MRF-LDA.", "startOffset": 49, "endOffset": 53}, {"referenceID": 30, "context": "Similar to [31, 32], we also evaluate our model in a quantitative manner based on the coherence measure (CM) to assess how coherent the learned topics are.", "startOffset": 11, "endOffset": 19}, {"referenceID": 31, "context": "Similar to [31, 32], we also evaluate our model in a quantitative manner based on the coherence measure (CM) to assess how coherent the learned topics are.", "startOffset": 11, "endOffset": 19}, {"referenceID": 5, "context": "To provide alternative metrics, the normalized mutual information (NMI) is used to evaluate the quality of a clustering solution [6, 34].", "startOffset": 129, "endOffset": 136}, {"referenceID": 33, "context": "To provide alternative metrics, the normalized mutual information (NMI) is used to evaluate the quality of a clustering solution [6, 34].", "startOffset": 129, "endOffset": 136}], "year": 2016, "abstractText": "Inferring topics from the overwhelming amount of short texts becomes a critical but challenging task for many content analysis tasks, such as content charactering, user interest profiling, and emerging topic detecting. Existing methods such as probabilistic latent semantic analysis (PLSA) and latent Dirichlet allocation (LDA) cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. This paper studies how to incorporate the external word correlation knowledge into short texts to improve the coherence of topic modeling. Based on recent results in word embeddings that learn semantically representations for words from a large corpus, we introduce a novel method, Embedding-based Topic Model (ETM), to learn latent topics from short texts. ETM not only solves the problem of very limited word co-occurrence information by aggregating short texts into long pseudotexts, but also utilizes a Markov Random Field regularized model that gives correlated words a better chance to be put into the same topic. The experiments on real-world datasets validate the effectiveness of our model comparing with the state-of-the-art models.", "creator": "LaTeX with hyperref package"}}}