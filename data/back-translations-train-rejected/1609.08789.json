{"id": "1609.08789", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Memory Visualization for Gated Recurrent Neural Networks in Speech Recognition", "abstract": "Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks.", "histories": [["v1", "Wed, 28 Sep 2016 06:26:16 GMT  (995kb,D)", "http://arxiv.org/abs/1609.08789v1", "Submitted to ICASSP 2017"], ["v2", "Mon, 26 Dec 2016 09:25:14 GMT  (995kb,D)", "http://arxiv.org/abs/1609.08789v2", "ICASSP 2017"], ["v3", "Mon, 27 Feb 2017 02:07:34 GMT  (2135kb,D)", "http://arxiv.org/abs/1609.08789v3", "ICASSP 2017"]], "COMMENTS": "Submitted to ICASSP 2017", "reviews": [], "SUBJECTS": "cs.LG cs.CL cs.NE", "authors": ["zhiyuan tang", "ying shi", "dong wang", "yang feng", "shiyue zhang"], "accepted": false, "id": "1609.08789"}, "pdf": {"name": "1609.08789.pdf", "metadata": {"source": "CRF", "title": "MEMORY VISUALIZATION FOR GATED RECURRENT NEURAL NETWORKS IN SPEECH RECOGNITION", "authors": ["Zhiyuan Tang", "Ying Shi", "Dong Wang", "Yang Feng", "Shiyue Zhang"], "emails": ["tangzy@cslt.riit.tsinghua.edu.cn", "shiying@cslt.riit.tsinghua.edu.cn", "fengyang@cslt.riit.tsinghua.edu.cn", "zhangsy@cslt.riit.tsinghua.edu.cn", "wangdong99@mails.tsinghua.edu.cn"], "sections": [{"heading": null, "text": "Index terms - long-term short-term memory, gated recurrent unit, visualization, residual learning, speech recognition"}, {"heading": "1. INTRODUCTION", "text": "Among the various deep models, the recursive neural network (RNN) is of particular interest to ASR, in part because of its ability to model the complex temporal dynamics of speech signals as a continuous state development. A well-known problem of the vanilla RNN model is that the formation of the network is generally difficult, largely attributed to the disappearance of speech signals and the explosion problem. Furthermore, the vanilla RNN model tends to forget things quickly. To solve these problems, researchers have proposed a gated memory mechanism that leads to gated RNNs models leading to gated visualizations. [STGs models], which rely on a few portable gates to obtain, store and disseminate the most important information.GRNN structures are commonly used, two are used (which have recently been proposed)."}, {"heading": "2. RELATED WORK", "text": "Visualization has been used in several areas of research to study the behavior of neural models. Furthermore, in computer vision (CV), visualization is often used to demonstrate the hierarchical learning process of features with deep conventional neural networks (CNN), such as activation maximization and composition analysis [8-10]. Natural language processing (NLP) is another area where visualization is widely used. Since word / tag sequences are often modeled by an RNN, visualization in NLP focuses on analyzing the temporal dynamics of units in RNNs [11-14]. In speech recognition (and other speech processing tasks), visualization has not been applied as strongly as in CV and NLP, partly because the representation of speech signals as visual patterns is not as simple as in images and text. The only work we know for visualization in RNN is visualization in ASR [STung]."}, {"heading": "3. EXPERIMENTAL SETUP", "text": "We first describe the LSTM and GRU structures, whose behavior is visualized in the following sections, and then describe the settings of the ASR system on which the visualization is based."}, {"heading": "3.1. LSTM and GRU", "text": "We choose the LSTM structure described by Chung in [16] because it has shown a good performance for ASR. The calculation looks like this: it = \u03c3 (Wixxt + Wimmt \u2212 1 + Vicct \u2212 1) ft = \u03c3 (Wfxxt + Wfmmt \u2212 1 + Vfcct \u2212 1) ct = ft ct \u2212 1 + it g (Wcxxt + Wcmmt \u2212 1) ot = \u03c3 (Woxxt + Wommt \u2212 1 + Vocct) mt = ot h (ct).In the above equations, the terms W and V denote weight matrices, where V's are diagonal; xt is the input symbol; it, ft \u2212 ot respectively represents the input, forget and output gates; ct is the cell and mt is the unit output. \u03c3 (\u00b7) is the logistic sigmoid function, and g (\u00b7) and h (\u00b7) are hyperbolic activation functions."}, {"heading": "3.2. Speech recognition task", "text": "Our experiments are conducted on the basis of the WSJ database, whose profile is largely standard: 37, 318 expressions for model training and 1, 049 expressions (with dev93, eval92 and eval93) for testing. The input function is 40-dimensional Fbanks, with a symmetrical 2-frame window for joining adjacent frames. The number of recurring layers varies from 1 to 6, and the number of units in each hidden layer is set to 512. Units can be LSTM or GRU. The output layer consists of 3, 377 units, which corresponds to the total number of Gaussian components in the conventional GMM system used to boot the RNN model. The Kaldi toolkit [17] is used to perform the model training and performance evaluation, and the training process largely follows the WSJ s5 nnet3 recipe. The natural stochastic gradient descent (NSR) is better used on the GRD model of the units (the GRD is used as a reference to the GRD)."}, {"heading": "4. VISUALIZATION", "text": "This section presents some visualization results. Due to the limited space, our focus is on the comparison between LSTM and GRU. Detailed results and analysis can be found in the accompanying technical report [19]."}, {"heading": "4.1. Activation patterns", "text": "The first experiment explores how different gated RNNs encode information in different ways. For both LSTM and GRU RNNs, 50 units are randomly selected from each hidden layer, and for each unit, the distribution of cell values is calculated to 500 expressions. Results are shown in Figure 1 for the LSTM and GRU RNNNs, respectively. Due to the limited space, only the first and fourth layers are presented. For LSTM, we reset irregular values (less than \u2212 10 or greater than 10) to \u2212 10 or 10 for better visualization. It can be observed that most cell values in LSTM focus on zero values and the concentration decreases in the higher layer. This pattern suggests that LSTM relies on large positive or negative cell values of some units to represent information. In contrast, most cells in GRU-1 constellation are concentrated at \u2212 1 or + 1, and this pattern is clearer for the higher layer."}, {"heading": "4.2. Temporal trace", "text": "The second experiment investigates the evolution of cell activation in recognition, which is achieved by plotting the cell vectors of all images using the t-SNE tool [20] when decoding an enunciation. Results are shown in Fig. 2, where the temporal traces for the four layers are plotted from top to bottom in the diagrams. An interesting observation is that the traces are much smoother in LSTM than in GRU. This indicates that LSTM tends to remember more than in GRU: in long-term memory, the novelty of the current time is mostly averaged by the past memory, resulting in a smooth temporal trace. In GRU, new experience is quickly absorbed and thus memory changes drastically. Comparing the memory traces on different layers, it can be seen that in GRU, the traces on a higher level become smoother, while this trend is not clear in LSTM."}, {"heading": "4.3. Memory robustness", "text": "The third experiment tests the robustness of LSTM and GRU with noise interruptions. Specifically, during detection, a noise segment is inserted into the speech stream, and we observe the impact of this noise segment by visualizing the difference in cell values caused by the insertion of noises.The results are shown in Fig. 3.It can be seen that both units in higher layers accumulate longer memory, and GRU is more robust than LSTM under noisy conditions. In LSTM, the impact of noise on some cells persists almost to the end, even on the last layer for which the units are supposed to be noise-resistant. In GRU, the impact takes only a few images. This shows a major advantage of GRU and confirms the observation in the second experiment that GRU remembers less than LSTM."}, {"heading": "5. APPLICATION TO STRUCTURE DESIGN", "text": "The visualization results shown in the previous section show that LSTM and GRU have different characteristics in both information encoding and time evolution, so it is not easy to tell which model is better suited for a given task. In speech recognition, the experimental results in Section 3.2 seem to show that GRU is better suited, explained by the fact that speech signals are pseudo-stationary and typically do not last longer than 50 frames, meaning that short memories are likely to be beneficial, especially when the robustness of noise is taken into account. Inspired by these observations, we are introducing some modifications to LSTM and / or GRU, both of which result in performance improvements."}, {"heading": "5.1. Lazy cell update", "text": "A difference between LSTM and GRU, as shown in Section 3.1, is that GRU updates the cells as a last step, while LSTM updates the cells before calculating the output gates. To investigate the effects of the rotten update with GRU, we arrange the calculation in LSTM as shown in Fig. 4 (a).The detection results are shown in Table 2, and the rotten update time track is shown in Fig. 5 (a).Note that only the last LSTM layer has been modified. The results show that the rotten update improves the performance of LSTM. From the time track, the modified LSTM seems to behave more like a GRU: the track is less smooth and allows for faster adoption of new inputs. This shows that the short memory behavior of GRU may be an important factor for good performance, and this behavior is closely related to the rotten cell update."}, {"heading": "5.2. Shortcut connections for residual learning", "text": "Another modification is inspired by the visualization result that the gates at higher levels exhibit a similar pattern [12], which implies that the cells at higher levels are mostly learned by leftovers, which is also confirmed by recent research on leftover nets [21]. We borrow this idea and add explicit abbreviations alongside the abbreviation units, so that the main path is forced to learn leftover learning, as shown in Fig. 4 (b). Results with leftover learning are shown in Table 3, and the temporal traces are shown in Fig. 5 (b) (c). These results show that adding shortcuts actually leads to consistent performance gains with both LSTM and GRU. Time traces at different levels appear more consistent (note that for t-SNE, only topological relationships are important)."}, {"heading": "6. CONCLUSION", "text": "This paper presented some visualization results for gated RNNs and focused in particular on the comparison between LSTM and GRU. Results show that the two gated RNNNs use different ways to encode information and that the information in GRU is more distributed. In addition, LSTM has long-term memory but is also sensitive to noise. Inspired by these observations, we have introduced two modifications to improve gated RNNNs: lazy cell updates and short connections for residual learning time, both of which offer an interesting performance improvement. Future work will compare neural models in different categories, e.g. TDNN and RNN."}, {"heading": "7. REFERENCES", "text": "[1] Li Deng and Dong Yu, \"Deep learning: Methods and applications,\" Foundations and Trends in Signal Processing, vol. 7, no. 3-4, pp. 197-387, 2013. [2] Alex Graves, A-R Mohamed, and Geoffrey Hinton, \"Speech recognition with deep recurrent neural networks,\" in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). [3] Alex Graves and Navdeep Jaitly, \"Towards end-to-end speech recognition with recurrent neural networks,\" in Proceedings of the 31st International Conference on Machine Learning (ICML), 2014, pp. 1764-1772. [4] Hasim Sak, Andrew Senior, and Franc."}], "references": [{"title": "Deep learning: Methods and applications", "author": ["Li Deng", "Dong Yu"], "venue": "Foundations and Trends in Signal Processing, vol. 7, no. 3-4, pp. 197\u2013387, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "A-R Mohamed", "Geoffrey Hinton"], "venue": "Proceedings of IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 6645\u20136649.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["Alex Graves", "Navdeep Jaitly"], "venue": "Proceedings of the 31st International Conference on Machine Learning (ICML), 2014, pp. 1764\u20131772.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory recurrent neural network architectures for large scale acoustic modeling", "author": ["Hasim Sak", "Andrew Senior", "Fran\u00e7oise Beaufays"], "venue": "Proceedings of the Annual Conference of International Speech Communication Association (INTERSPEECH), 2014.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Long shortterm memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1997}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["Dario Amodei", "Rishita Anubhai", "Eric Battenberg", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Jingdong Chen", "Mike Chrzanowski", "Adam Coates", "Greg Diamos"], "venue": "arXiv preprint arXiv:1512.02595, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing higher-layer features of a deep network", "author": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "venue": "University of Montreal, vol. 1341, 2009.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2009}, {"title": "Visualizing and understanding convolutional networks", "author": ["Matthew D Zeiler", "Rob Fergus"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 818\u2013 833.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1312.6034, 2013.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen"], "venue": "Advances in Neural Information Processing Systems, 2013, pp. 190\u2013198.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding recurrent networks", "author": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1506.02078, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Representation of linguistic form and function in recurrent neural networks", "author": ["\u00c1kos K\u00e1d\u00e1r", "Grzegorz Chrupa\u0142a", "Afra Alishahi"], "venue": "arXiv preprint arXiv:1602.08952, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing and understanding neural models in nlp", "author": ["Jiwei Li", "Xinlei Chen", "Eduard Hovy", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1506.01066, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Simplifying long short-term memory acoustic models for fast training and decoding", "author": ["Yajie Miao", "Jinyu Li", "Yongqiang Wang", "Shi-Xiong Zhang", "Yifan Gong"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 2284\u20132288.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "Caglar Gulcehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2014}, {"title": "The kaldi speech recognition toolkit", "author": ["Daniel Povey", "Arnab Ghoshal", "Gilles Boulianne", "Lukas Burget", "Ondrej Glembek", "Nagendra Goel", "Mirko Hannemann", "Petr Motlicek", "Yanmin Qian", "Petr Schwarz"], "venue": "IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011, number EPFL-CONF-192584.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2011}, {"title": "Parallel training of dnns with natural gradient and parameter averaging", "author": ["Daniel Povey", "Xiaohui Zhang", "Sanjeev Khudanpur"], "venue": "arXiv preprint arXiv:1410.7455, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualization analysis for recurrent networks", "author": ["Zhiyuan Tang", "Ying Shi", "Dong Wang", "Yang Feng", "Shiyue Zhang"], "venue": "Tech. Rep., 2016, http://cslt.riit.tsinghua.edu.cn/mediawiki/images/6/6a/Visual.pdf.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Visualizing data using t-sne", "author": ["Laurens van der Maaten", "Geoffrey Hinton"], "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Deep learning has gained brilliant success in a wide spectrum of research areas including automatic speech recognition (ASR) [1].", "startOffset": 125, "endOffset": 128}, {"referenceID": 1, "context": "Promising results have been reported for the RNN-based ASR [2\u20134].", "startOffset": 59, "endOffset": 64}, {"referenceID": 2, "context": "Promising results have been reported for the RNN-based ASR [2\u20134].", "startOffset": 59, "endOffset": 64}, {"referenceID": 3, "context": "Promising results have been reported for the RNN-based ASR [2\u20134].", "startOffset": 59, "endOffset": 64}, {"referenceID": 4, "context": "Two widely used gated RNN structures are the long short-term memory (LSTM), proposed by Hochreiter [5], and the gated recurrent unit (GRU), proposed recently by Cho et al.", "startOffset": 99, "endOffset": 102}, {"referenceID": 5, "context": "[6].", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "Both of the two structures have delivered promising performance in ASR [4, 7].", "startOffset": 71, "endOffset": 77}, {"referenceID": 6, "context": "Both of the two structures have delivered promising performance in ASR [4, 7].", "startOffset": 71, "endOffset": 77}, {"referenceID": 7, "context": "For instance, in computer vision (CV), visualization is often used to demonstrate the hierarchical feature learning process with deep conventional neural networks (CNN), such as the activation maximization and composition analysis [8\u201310].", "startOffset": 231, "endOffset": 237}, {"referenceID": 8, "context": "For instance, in computer vision (CV), visualization is often used to demonstrate the hierarchical feature learning process with deep conventional neural networks (CNN), such as the activation maximization and composition analysis [8\u201310].", "startOffset": 231, "endOffset": 237}, {"referenceID": 9, "context": "For instance, in computer vision (CV), visualization is often used to demonstrate the hierarchical feature learning process with deep conventional neural networks (CNN), such as the activation maximization and composition analysis [8\u201310].", "startOffset": 231, "endOffset": 237}, {"referenceID": 10, "context": "Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].", "startOffset": 133, "endOffset": 140}, {"referenceID": 11, "context": "Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].", "startOffset": 133, "endOffset": 140}, {"referenceID": 12, "context": "Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].", "startOffset": 133, "endOffset": 140}, {"referenceID": 13, "context": "Since word/tag sequences are often modeled by an RNN, visualization in NLP focuses on analysis of temporal dynamics of units in RNNs [11\u201314].", "startOffset": 133, "endOffset": 140}, {"referenceID": 14, "context": "[15], which studied the input and forget gates of an LSTM, and found they are correlated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "We choose the LSTM structure described by Chung in [16], as it has shown good performance for ASR.", "startOffset": 51, "endOffset": 55}, {"referenceID": 5, "context": "GRU was introduced by Cho in [6].", "startOffset": 29, "endOffset": 32}, {"referenceID": 16, "context": "The Kaldi toolkit [17] is used to conduct the model training and performance evaluation, and the training process largely follows the WSJ s5 nnet3 recipe.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "The natural stochastic gradient descent (NSGD) algorithm [18] is used to train the model.", "startOffset": 57, "endOffset": 61}, {"referenceID": 18, "context": "More detailed results and analysis can be found in the associated technical report [19].", "startOffset": 83, "endOffset": 87}, {"referenceID": 19, "context": "This is achieved by drawing the cell vectors of all the frames using the t-SNE tool [20] when decoding an utterance.", "startOffset": 84, "endOffset": 88}, {"referenceID": 11, "context": "Another modification is inspired by the visualization result that the gates at high-level layers show a similar pattern [12].", "startOffset": 120, "endOffset": 124}, {"referenceID": 20, "context": "This is also confirmed by recent research on residual net [21].", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "Recurrent neural networks (RNNs) have shown clear superiority in sequence modeling, particularly the ones with gated units, such as long short-term memory (LSTM) and gated recurrent unit (GRU). However, the dynamic properties behind the remarkable performance remain unclear in many applications, e.g., automatic speech recognition (ASR). This paper employs visualization techniques to study the behavior of LSTM and GRU when performing speech recognition tasks. Our experiments show some interesting patterns in the gated memory, and some of them have inspired simple yet effective modifications on the network structure. We report two of such modifications: (1) lazy cell update in LSTM, and (2) shortcut connections for residual learning. Both modifications lead to more comprehensible and powerful networks.", "creator": "LaTeX with hyperref package"}}}