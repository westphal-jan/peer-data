{"id": "1609.08810", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Effective Combination of Language and Vision Through Model Composition and the R-CCA Method", "abstract": "We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields.", "histories": [["v1", "Wed, 28 Sep 2016 08:11:28 GMT  (49kb,D)", "https://arxiv.org/abs/1609.08810v1", "6 pages, 1 figure"], ["v2", "Tue, 4 Oct 2016 09:59:50 GMT  (50kb,D)", "http://arxiv.org/abs/1609.08810v2", "6 pages, 1 figure"]], "COMMENTS": "6 pages, 1 figure", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["hagar loeub", "roi reichart"], "accepted": false, "id": "1609.08810"}, "pdf": {"name": "1609.08810.pdf", "metadata": {"source": "CRF", "title": "Effective Combination of Language and Vision Through Model Composition and the R-CCA Method", "authors": ["Hagar Loeub", "Roi Reichart"], "emails": ["hagar.loeub@gmail.com", "roiri@ie.technion.ac.il"], "sections": [{"heading": null, "text": "We address the problem of integrating textual and visual information into vector space models for displaying word meanings. First, we introduce the Residual CCA (R-CCA) method, which complements the standard CCA method by representing for each modality the difference between the original signal and the signal projected onto the common, maximum correlation, space. Then, we show that the construction of visual and textual representations and their subsequent post-processing by assembling common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (also known as sequential modeling) lead to high-quality models. In five standard semantic benchmarks, our sequential models outperform the most recent multimodal imaging learning alternatives, including those based on shared imaging learning. In two of these benchmarks, our R-CCA method is part of the best configuration provided by our algorithm."}, {"heading": "1 Introduction", "text": "In recent years, vector space models (VSMs), which derive word meaning representations from word coordination patterns in text, have established themselves in lexical semantics research (Turney et al., 2010; Clark, 2012). Recent work has shown that when other modalities, especially the visual ones, are exploited together with the text, the resulting multimodal representations outperform strong textual models in a variety of tasks (Baroni, 2016). Models that can largely split text and vision into two types. Sequential models first construct separately visual and textual representations and then merge them with a variety of techniques: concatenation (Bruni et al., 2011; Silberer et al., 2013; Kiela and Bottou, 2014), linearly weighted combination of vectors (Bruni et al., 2012; Bruni et al., 2012; Bruni et al., 2013; Silberer et al., 2013; Kiela and Bottou, 2014)."}, {"heading": "2 Multimodal Composition", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Modeling Motifs", "text": "PCA is a standard method for reducing dimensionality. Therefore, we do not describe its details here and refer the interested reader to it (Jolliffe, 2002).CCA finds two projection vectors, one for each original vector, so that the projection of the original vectors yields the highest possible correlation under linear projection. In short: Faced with an n-word vocabulary with the representations X-Rn \u00b7 d1 and Y-Rn \u00b7 d2, CCA seeks two groups of projection vectors V-Rd1 \u00b7 d and W-Rd2 \u00b7 d that maximize the correlation between the projected vectors of each word: V, W-Rn \u00b7 d2, W-Rd2 \u00b7 d. The final projection is: X-Rd1 and Y-Rd2 \u00b7 d, which maximize the correlation (n-CCA) between the projected vectors of each word (R-CCA-CCA).CCA aims to project the representations involved in a common space, to maximize the correlation between them."}, {"heading": "2.2 Motif Composition", "text": "We divide the modeling motifs above into three layers to allow an efficient systematic search for optimal configurations (Figure 1): (a) Data: (a.1) original vectors; or (a.2) original vectors projected with unimodal PCA; (b) Fusion: (b.1) CCA and (b.2) RCCA, any method that outputs two projected vectors per word, one for each modality; (c) Combination: (c.1) vector concatenation; and (c.2) linear interpolation (LI) of model values. In our search, a higher layer method takes inputs from all lower layer methods, as long as both inputs are the output of the same method. That is, CCA (layer b.1) is applied to original textual and visual vector pairs (output of a.1) as well as to PCA-transformed vectors (a.2), but not to PCA-transformed vectors."}, {"heading": "3 Data and Experiments", "text": "Input Vectors Our textual VSM is word2vec skipgram (Mikolov et al., 2013), 1 trained on the 8G words corpus generated by the word2vec script.2 We follow the hyperparameter setting of (Schwartz et al., 2015) and, set vector dimensionality to 500. For visual modality we used the 5100 4096-dimensional vectors from Lazaridou et al. (2015), extracted using a trained Convolutional Neural Network (CNN, (Krizhevsky et al., 2012) and the Caffe dimensionality toolkit (Jia et al., 2014) from 100 pictures sampled for each word from its ImageNet et al., 2009) entry. While there are various alternatives for both textual and visual representations that we chose, we are based on state-of-the-art techniques.Benchmarks We the report (Lazearcorrelation et al, 2009) entry."}, {"heading": "4 Results", "text": "Table 1 presents the best results for SSim and VSim, but for the entire sets and not for Lazaridou (2014). Profits (in \u03c1 points) are: MEN: 4, WS: 1, SL: 9, SSim: 7 and VSim: 1. R-CCA is included in Best for MEN and SL, improvement over the best configuration that it does not by 23To facilitate a clean comparison with previous work, we copy the results of Bruni et al. (2014) and Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) we do not report the results for WS, while Bruni et al. (2014) report a different subset than ours consisting of 252 word pairs. Kiela and Bottou (2014) report the results of our subset of WS, and we copy their best results for Lapata (2014)."}, {"heading": "5 Conclusions", "text": "We demonstrated the power of the composition of common modeling motifs in multimodal VSM construction and introduced the R-CCA method, which utilizes the residuals of CCA signals. Our model provides current results on five leading semantic benchmarks, of which R-CCA is part of the best configuration. Furthermore, R-CCA performs much better than CCA in all five benchmarks, so our results support two lines of research. Firstly, they promote sequential modeling with systematic search in the configuration space for multimodal combinations. Secondly, our future goal is to make model composition a standard tool for this problem by developing efficient inference algorithms for optimal configurations in potentially more complex search spaces than we have studied it with an exhaustive grid search. Secondly, the encouraging results of R-CCA emphasize the potential of informed post-processing of CCA output. We intend to address this topic in more detail in the future."}], "references": [{"title": "Integrating experiential and distributional data to learn semantic representations", "author": ["Andrews et al.2009] Mark Andrews", "Gabriella Vigliocco", "David Vinson"], "venue": "Psychological review,", "citeRegEx": "Andrews et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Andrews et al\\.", "year": 2009}, {"title": "Grounding distributional semantics in the visual world", "author": ["Marco Baroni"], "venue": "Language and Linguistics Compass,", "citeRegEx": "Baroni.,? \\Q2016\\E", "shortCiteRegEx": "Baroni.", "year": 2016}, {"title": "Distributional semantics from text and images", "author": ["Bruni et al.2011] Elia Bruni", "Giang Binh Tran", "Marco Baroni"], "venue": "In Proc. of the GEMS workshop on geometrical models of natural language semantics,EMNLP,", "citeRegEx": "Bruni et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2011}, {"title": "Distributional semantics in technicolor", "author": ["Bruni et al.2012] Elia Bruni", "Gemma Boleda", "Marco Baroni", "Nam-Khanh Tran"], "venue": "In Proc. of ACL,", "citeRegEx": "Bruni et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2012}, {"title": "Multimodal distributional semantics", "author": ["Bruni et al.2014] Elia Bruni", "Nam-Khanh Tran", "Marco Baroni"], "venue": "Journal of Artificial Intelligence Research (JAIR),", "citeRegEx": "Bruni et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Vector space models of lexical meaning. Handbook of Contemporary Semantics, Wiley-Blackwell, \u00e0 para\u0131\u0302tre", "author": ["Stephen Clark"], "venue": null, "citeRegEx": "Clark.,? \\Q2012\\E", "shortCiteRegEx": "Clark.", "year": 2012}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["Deng et al.2009] Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Proc. of CVPR,", "citeRegEx": "Deng et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Deng et al\\.", "year": 2009}, {"title": "Visual information in semantic representation", "author": ["Feng", "Lapata2010] Yansong Feng", "Mirella Lapata"], "venue": "In Proc. of NAACL,", "citeRegEx": "Feng et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2010}, {"title": "Placing search in context: The concept revisited", "author": ["Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proc. of WWW, pages 406\u2013414.", "citeRegEx": "Solan et al\\.,? 2001", "shortCiteRegEx": "Solan et al\\.", "year": 2001}, {"title": "Model selection in compositional spaces", "author": ["Roger Baker Grosse"], "venue": "Ph.D. thesis,", "citeRegEx": "Grosse.,? \\Q2014\\E", "shortCiteRegEx": "Grosse.", "year": 2014}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["Sandor Szedmak", "John Shawe-Taylor"], "venue": "Neural Computation,", "citeRegEx": "Hardoon et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Hardoon et al\\.", "year": 2004}, {"title": "Learning abstract concept embeddings from multi-modal data: Since you probably can\u2019t see what i mean", "author": ["Hill", "Korhonen2014] Felix Hill", "Anna Korhonen"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Multi-modal models for concrete and abstract concept meaning. Transactions of the Association for Computational Linguistics, 2:285\u2013296", "author": ["Hill et al.2014] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2014}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Hill et al.2015] Felix Hill", "Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "A model of grounded language acquisition: Sensorimotor features improve lexical and grammatical learning", "author": ["Damian Jankowicz", "Suzanna Becker"], "venue": "Journal of Memory and Language,", "citeRegEx": "Howell et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Howell et al\\.", "year": 2005}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Jia et al.2014] Yangqing Jia", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "venue": "In Proc. of the ACM International Conference on Mul-", "citeRegEx": "Jia et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Jia et al\\.", "year": 2014}, {"title": "Principal component analysis", "author": ["Ian Jolliffe"], "venue": "Wiley Online Library", "citeRegEx": "Jolliffe.,? \\Q2002\\E", "shortCiteRegEx": "Jolliffe.", "year": 2002}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics", "author": ["Kiela", "Bottou2014] Douwe Kiela", "L\u00e9on Bottou"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Kiela et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kiela et al\\.", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Ilya Sutskever", "Geoffrey E Hinton"], "venue": "In Proc. of NIPS", "citeRegEx": "Krizhevsky et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Krizhevsky et al\\.", "year": 2012}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["Nghia The Pham", "Marco Baroni"], "venue": "In Proc. of NAACL", "citeRegEx": "Lazaridou et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Proc. of NIPS", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Improved lexical acquisition", "author": ["Reichart", "Korhonen2013] Roi Reichart", "Anna Korhonen"], "venue": null, "citeRegEx": "Reichart et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Reichart et al\\.", "year": 2013}, {"title": "A multimodal lda model integrating textual, cognitive and visual modalities", "author": ["Roller", "Im Walde2013] Stephen Roller", "Sabine Schulte Im Walde"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Roller et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Roller et al\\.", "year": 2013}, {"title": "Symmetric pattern based word embeddings for improved word similarity prediction", "author": ["Roi Reichart", "Ari Rappoport"], "venue": "In Proc. CoNLL", "citeRegEx": "Schwartz et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Schwartz et al\\.", "year": 2015}, {"title": "Grounded models of semantic representation", "author": ["Silberer", "Lapata2012] Carina Silberer", "Mirella Lapata"], "venue": "In Proc. of EMNLP-CoNLL", "citeRegEx": "Silberer et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2012}, {"title": "Learning grounded meaning representations with autoencoders", "author": ["Silberer", "Lapata2014] Carina Silberer", "Mirella Lapata"], "venue": "In Proc. of ACL", "citeRegEx": "Silberer et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2014}, {"title": "Models of semantic representation with visual attributes", "author": ["Vittorio Ferrari", "Mirella Lapata"], "venue": "In Proc. of ACL,", "citeRegEx": "Silberer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Silberer et al\\.", "year": 2013}, {"title": "From frequency to meaning: Vector space models of semantics", "author": ["Patrick Pantel"], "venue": "Journal of artificial intelligence research,", "citeRegEx": "Turney and Pantel,? \\Q2010\\E", "shortCiteRegEx": "Turney and Pantel", "year": 2010}], "referenceMentions": [{"referenceID": 5, "context": "In recent years, vector space models (VSMs), deriving word meaning representations from word cooccurrence patterns in text, have become prominent in lexical semantics research (Turney et al., 2010; Clark, 2012).", "startOffset": 176, "endOffset": 210}, {"referenceID": 1, "context": "Recent work has demonstrated that when other modalities, particularly the visual, are exploited together with text, the resulting multimodal representations outperform strong textual models on a variety of tasks (Baroni, 2016).", "startOffset": 212, "endOffset": 226}, {"referenceID": 2, "context": "Sequential models first separately construct visual and textual representations and then merge them using a variety of techniques: concatenation (Bruni et al., 2011; Silberer et al., 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al.", "startOffset": 145, "endOffset": 212}, {"referenceID": 26, "context": "Sequential models first separately construct visual and textual representations and then merge them using a variety of techniques: concatenation (Bruni et al., 2011; Silberer et al., 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al.", "startOffset": 145, "endOffset": 212}, {"referenceID": 3, "context": ", 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al., 2012; Bruni et al., 2014) or linear interpolation of model scores (Bruni et al.", "startOffset": 72, "endOffset": 112}, {"referenceID": 4, "context": ", 2013; Kiela and Bottou, 2014), linear weighted combination of vectors (Bruni et al., 2012; Bruni et al., 2014) or linear interpolation of model scores (Bruni et al.", "startOffset": 72, "endOffset": 112}, {"referenceID": 4, "context": ", 2014) or linear interpolation of model scores (Bruni et al., 2014), Canonical Correlation Analysis and its kernalized version (CCA, (Hill et al.", "startOffset": 48, "endOffset": 68}, {"referenceID": 11, "context": ", 2014), Canonical Correlation Analysis and its kernalized version (CCA, (Hill et al., 2014; Silberer and Lapata, 2012; Silberer et al., 2013)), Singular Value Decomposition (SVD, (Bruni et al.", "startOffset": 73, "endOffset": 142}, {"referenceID": 26, "context": ", 2014), Canonical Correlation Analysis and its kernalized version (CCA, (Hill et al., 2014; Silberer and Lapata, 2012; Silberer et al., 2013)), Singular Value Decomposition (SVD, (Bruni et al.", "startOffset": 73, "endOffset": 142}, {"referenceID": 4, "context": ", 2013)), Singular Value Decomposition (SVD, (Bruni et al., 2014)) and Weighted Gram Matrix Combination (Reichart and Korhonen, 2013; Hill et al.", "startOffset": 45, "endOffset": 65}, {"referenceID": 11, "context": ", 2014)) and Weighted Gram Matrix Combination (Reichart and Korhonen, 2013; Hill et al., 2014)).", "startOffset": 46, "endOffset": 94}, {"referenceID": 0, "context": "Joint models directly learn a joint representation from textual and visual resources using Bayesian modeling (Andrews et al., 2009; Feng and Lapata, 2010; Roller and Im Walde, 2013) and various neural network (NN) techniques: autoencoders (Silberer and Lapata, 2014), extensions of word2vec skip-gram (Hill and Korhonen, 2014; Lazaridou et al.", "startOffset": 109, "endOffset": 181}, {"referenceID": 19, "context": ", 2009; Feng and Lapata, 2010; Roller and Im Walde, 2013) and various neural network (NN) techniques: autoencoders (Silberer and Lapata, 2014), extensions of word2vec skip-gram (Hill and Korhonen, 2014; Lazaridou et al., 2015) and others (e.", "startOffset": 177, "endOffset": 226}, {"referenceID": 14, "context": "(Howell et al., 2005)).", "startOffset": 0, "endOffset": 21}, {"referenceID": 9, "context": "First, we advocate the sequential approach for text and vision combination and show that when a systematic search in the space of configurations of composition of common modeling motifs (Grosse, 2014) is employed, this approach outperforms recent joint models as well as sequential models that do not thoroughly search the space of configurations.", "startOffset": 186, "endOffset": 200}, {"referenceID": 16, "context": "Analysis ((PCA, (Jolliffe, 2002)), multimodal fusion with Canonical Correlation Analysis (CCA, (Hardoon et al.", "startOffset": 16, "endOffset": 32}, {"referenceID": 10, "context": "Analysis ((PCA, (Jolliffe, 2002)), multimodal fusion with Canonical Correlation Analysis (CCA, (Hardoon et al., 2004)) and model score combination with linear interpolation (LI, (Bruni et al.", "startOffset": 95, "endOffset": 117}, {"referenceID": 4, "context": ", 2004)) and model score combination with linear interpolation (LI, (Bruni et al., 2014)).", "startOffset": 68, "endOffset": 88}, {"referenceID": 4, "context": "The composed models outperform strong alternatives on semantic benchmarks for word pair similarity and association: MEN (Bruni et al., 2014), WordSim353 (WS, (Finkelstein et al.", "startOffset": 120, "endOffset": 140}, {"referenceID": 13, "context": ", 2001)), SimLex999 (SL (Hill et al., 2015)), SemSim and VisSim (SSim, VSim, (Silberer and Lapata, 2014)).", "startOffset": 24, "endOffset": 43}, {"referenceID": 16, "context": "We hence do not describe its details here and refer the interested reader to (Jolliffe, 2002).", "startOffset": 77, "endOffset": 93}, {"referenceID": 19, "context": "MMSKIP-A and MMSKIP-B are the (joint) models of Lazaridou et al. (2015), BR-EA-14 is the best performing model of Bruni", "startOffset": 48, "endOffset": 72}, {"referenceID": 20, "context": "Input Vectors Our textual VSM is word2vec skipgram (Mikolov et al., 2013), 1 trained on the 8G words corpus generated by the word2vec script.", "startOffset": 51, "endOffset": 73}, {"referenceID": 23, "context": "2 We followed the hyperparameter setting of (Schwartz et al., 2015) and, particularly, set vector dimensionality to 500.", "startOffset": 44, "endOffset": 67}, {"referenceID": 18, "context": "(2015), extracted with a pre-trained Convolutional Neural Network (CNN, (Krizhevsky et al., 2012)) and the Caffe toolkit (Jia et al.", "startOffset": 72, "endOffset": 97}, {"referenceID": 15, "context": ", 2012)) and the Caffe toolkit (Jia et al., 2014) from 100 pictures sampled for each word from its ImageNet (Deng et al.", "startOffset": 31, "endOffset": 49}, {"referenceID": 6, "context": ", 2014) from 100 pictures sampled for each word from its ImageNet (Deng et al., 2009) entry.", "startOffset": 66, "endOffset": 85}, {"referenceID": 16, "context": "For the visual modality, we used the 5100 4096-dimensional vectors of Lazaridou et al. (2015), extracted with a pre-trained Convolutional Neural Network (CNN, (Krizhevsky et al.", "startOffset": 70, "endOffset": 94}, {"referenceID": 19, "context": "Hence, following Lazaridou et al. (2015),", "startOffset": 17, "endOffset": 41}, {"referenceID": 19, "context": "Alternative Models We compare our results to strong alternatives: MSKIP-A and MMSKIP-B ((Lazaridou et al., 2015), joint models), the best performing model of Bruni et al.", "startOffset": 88, "endOffset": 112}, {"referenceID": 2, "context": ", 2015), joint models), the best performing model of Bruni et al. (2014) and Kiela and Bottou (2014) (sequential models).", "startOffset": 53, "endOffset": 73}, {"referenceID": 2, "context": ", 2015), joint models), the best performing model of Bruni et al. (2014) and Kiela and Bottou (2014) (sequential models).", "startOffset": 53, "endOffset": 101}, {"referenceID": 2, "context": "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al.", "startOffset": 74, "endOffset": 94}, {"referenceID": 2, "context": "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al.", "startOffset": 74, "endOffset": 125}, {"referenceID": 2, "context": "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS.", "startOffset": 74, "endOffset": 154}, {"referenceID": 2, "context": "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al.", "startOffset": 74, "endOffset": 194}, {"referenceID": 2, "context": "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs.", "startOffset": 74, "endOffset": 250}, {"referenceID": 2, "context": "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs. Kiela and Bottou (2014) report results on our subset of WS, and we copy their best result.", "startOffset": 74, "endOffset": 352}, {"referenceID": 2, "context": "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs. Kiela and Bottou (2014) report results on our subset of WS, and we copy their best result. Silberer and Lapata (2014) also report results for SSim and VSim but for the entire sets rather than for our subsets.", "startOffset": 74, "endOffset": 446}, {"referenceID": 2, "context": "To facilitate clean comparison with previous work, we copy the results of Bruni et al. (2014) and of Kiela and Bottou (2014) from Lazaridou et al. (2015), except for WS. Lazaridou et al. (2015) do not report results for WS, while Bruni et al. (2014) report results on a different subset than ours, consisting of 252 word pairs. Kiela and Bottou (2014) report results on our subset of WS, and we copy their best result. Silberer and Lapata (2014) also report results for SSim and VSim but for the entire sets rather than for our subsets. Section 5 of Lazaridou et al. (2015) provides the details of the alternative models, their training and parameter tuning.", "startOffset": 74, "endOffset": 574}], "year": 2016, "abstractText": "We address the problem of integrating textual and visual information in vector space models for word meaning representation. We first present the Residual CCA (R-CCA) method, that complements the standard CCA method by representing, for each modality, the difference between the original signal and the signal projected to the shared, max correlation, space. We then show that constructing visual and textual representations and then post-processing them through composition of common modeling motifs such as PCA, CCA, R-CCA and linear interpolation (a.k.a sequential modeling) yields high quality models. On five standard semantic benchmarks our sequential models outperform recent multimodal representation learning alternatives, including ones that rely on joint representation learning. For two of these benchmarks our R-CCA method is part of the Best configuration our algorithm yields.", "creator": "LaTeX with hyperref package"}}}