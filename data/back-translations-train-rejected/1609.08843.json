{"id": "1609.08843", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Hierarchical Memory Networks for Answer Selection on Unknown Words", "abstract": "Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentence-level memory and word-level memory respectively. Then, (k)-max pooling is exploited following reasoning module on the sentence-level memory to sample the (k) most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks.", "histories": [["v1", "Wed, 28 Sep 2016 10:03:05 GMT  (531kb)", "http://arxiv.org/abs/1609.08843v1", "10 pages, to appear in COLING 2016"]], "COMMENTS": "10 pages, to appear in COLING 2016", "reviews": [], "SUBJECTS": "cs.IR cs.AI cs.CL", "authors": ["jiaming xu", "jing shi", "yiqun yao", "suncong zheng", "bo xu", "bo xu"], "accepted": false, "id": "1609.08843"}, "pdf": {"name": "1609.08843.pdf", "metadata": {"source": "CRF", "title": "Hierarchical Memory Networks for Answer Selection on Unknown Words", "authors": ["Jiaming Xu", "Jing Shi", "Yiqun Yao", "Suncong Zheng", "Bo Xu"], "emails": ["yaoyiqun2014}@ia.ac.cn", "xubo}@ia.ac.cn"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.08 843v 1 [cs.I R] 28 Sep 2016"}, {"heading": "1 Introduction", "text": "In fact, most people who live in the United States are able to identify themselves and understand what they want and what they want. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2 Background: Memory Networks", "text": "(Weston et al., 2015; Bordes et al., 2015; Sukhbaatar et al., 2015) Memory Network, first introduced by Weston et al. (2015), is a new class of learning models that can easily be read and written into part of a long-term memory component and seamlessly combine it with predictive inferences. Formally, in addition to explicit memory, which is an array of cells to store the pre-trained vector representations, the personal information used in our examples and datasets is all synthetic and not real. 2It is noteworthy that the term \"hierarchical storage networks\" was mentioned in (Chandar et al., 2016), where the intention was to organize memory into multi-level groups based on hashing, tree or clustering structures to allow the reader to efficiently access memory."}, {"heading": "3 Hierarchical Memory Networks for Answer Selection", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Approach Overview", "text": "As described in Figure 1 (a), we illustrate our HMN for selecting the answers. We give a series of n sentences, which are referred to as: X = {xi} i = (1,2,..., n) and a query q, where i is the time frame of the sentence xi in the set. We first map these sentences into the sentence memory M (S) and the word memory M (W), each with low-dimensional distributed representations. Afterwards, thinking in the sentence memory is used to softly browse the corresponding sentences. Furthermore, we use k-max pooling to test the most relevant sentences based on the soft search results and to use the attention mechanism to concentrate on the word memory of the selected sentences."}, {"heading": "3.2 Sentence-level Memory and Reasoning", "text": "In this section, we apply the reasoning module to generate multiple interaction at the sentence level, based on the adjacent weight scheme of MemNN (Sukhbaatar et al., 2015), as shown in Figure 1 (b). Faced with two word embedding matrices A-R-V-D and C-V-V-D, where the vocabulary size and d is the dimension of the word embedding, we first encode the word embedding matrix l into the sentence xi into dual channels of word representation as axij-Rd and Cxij-R. To combine the order of the words into their representations, we first apply the word embedding matrix-l to insert the two-channel word embedding as lgj (axij) and lgj \u00b7 (cxij \u00b7), whence the Ji position is (1 \u2212 j / yi) 2j \u2212 Ji-i-i-i-i-sentence-i-i-i-cxij-i."}, {"heading": "3.4 Attention on Word-level Memory", "text": "For the Word Memory Performance we first apply a bidirectional GRU (BiGRU) to calculate the hidden states of all ordered words. (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (W) (Forward). (W) (W) (Forward). (W) (W) (W) (W) (W) (W) (Forward). (W) (W) (W) (Forward). (W) (W). (W). (W) (W). (W) (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W).). (W). (W). (W). (W) (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W). (W)"}, {"heading": "3.5 Joint Learning", "text": "In this thesis, we combine the probability distributions of the output words both at the sentence level and at the word level to predict the common probability distribution p (w) as follows: p (w) = p (S) (w) + p (W) (w). (8) Finally, we use the target answer y to simultaneously guide the learning of the thought module at the sentence level and the attention module at the word level. We select the cross entropy as a cost function and apply the stochastic gradient descent (SGD) (Bottou, 1991) as an optimization method to train our common model.Among the parameters learned are the word embedding matrices A1 and Cr} r = (1,2,..., R), the temporal coding matrices T 1 A and {T r C} r = (1,2,..., R) in equation. (2) and (4) the parameters {Biqu Model U, \"the codification matrices A,\" codification U and codification T (4) and simultaneous matrices T and T (4)."}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Datasets and Setup", "text": "We perform response selection tasks on four synthetic domain dialogue data sets, two from the flight booking domain and two from the hotel reservation domain. A complete dialog history of each data set contains eight round responses. In addition to greeting and end sentences, a dialog history consists of six round responses to the customer's personal information such as name, phone and passport number. The data sets contain hundreds of response patterns and thousands of entity information. For more detailed descriptions, see our published data sets. Data set statistics are summarized in Table 1. We use 45% of data for training, 5% for validation and the remaining 50% for the test. Statistics show that the proportion of invisible responses to dev / test sets overtakes all 57%. In our experiments, most hyperparameters are set uniformly for the data sets, 5% for validation and the remaining 50% for the test. The percentage of invisible responses to dev / test sets overtakes all 57%."}, {"heading": "4.2 Comparison with Memory Networks", "text": "In order to evaluate the effects of several hops on the thought module and the time coding on the memory of the sentenzel level, we are designing three variants based on MemNN (Sukhbaatar et al., 2015): MemNN-H1 (1 hop and time coding), MemNN-NT (3 hops but no time coding) and MemNN (3 hops and time coding) on the data sets. Furthermore, we are evaluating the prediction performance of our HMN on different memory components via HMN-Sent (prediction of the thought module on record memory such as Eqn. (5)), HMN-Word (prediction of the attention module on word memory such as Eqn. (7) and HMN-Joint (common prediction as Eqn. (8)). Comparison of these methods shows that MemNN-H1 without prediction on word coding HN significantly improves the result of this time coding, HN without multiple coding."}, {"heading": "4.3 How to Select the Correct Answers on Unknown Words", "text": "rf\u00fc ide rrf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc ide rf\u00fc eeirrrf\u00fc rf\u00fc rf\u00fc rf\u00fc eeirf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc"}, {"heading": "5 Related Works", "text": "Recently, many methods of deep learning with explicit memory and attention mechanisms have shown promising results in question and answer tasks. Thus, Yu et al. (2015) Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanisms and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory for solving QA tasks, and Sukhbaatar et al. (2015) have designed end-to-end memory networks and introduced multi-hop argumentation components to solve different types of QA tasks. These methods of imaging learning are not based on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015). However, most of these deep learning methods rarely focus on solving a word selection problem."}, {"heading": "6 Conclusion", "text": "In this paper, we introduce hierarchical memory networks to solve the problem of selecting solutions for unknown words. First, we encode the sentences into a sentence-level memory with time coding. Then, the thought module performs a multi-hop interaction to the memory to retrieve the corresponding sentences, and k-max pooling samples of the most commonly used sentences. For word-level memory, BiGRU is used to encode the words and introduce the context into the memory, then an ingenious attention mechanism is applied to the selected word memory to focus the fine-grained words. We conduct selection experiments on four synthetic domain dialog data sets that contain many invisible answers. Experimental results show that our hierarchical memory networks can achieve satisfactory performance."}, {"heading": "Acknowledgements", "text": "We thank the anonymous reviewers for their insightful comments and this work has been supported by the Strategic Priority Research Program of the Chinese Academy of Sciences (grant no. XDB02070005), the National High Technology Research and Development Program of China (863 Program) (grant no. 2015AA015402) and the National Natural Science Foundation (grant no. 61602479 and 61403385)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Proceedings of the 3rd International Conference on Learning Representations (ICLR)", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Large-scale simple question answering with memory", "author": ["Nicolas Usunier", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Bordes et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bordes et al\\.", "year": 2015}, {"title": "Stochastic gradient learning in neural networks", "author": ["L\u00e9on Bottou"], "venue": "Proceedings of Neuro-N\u0131mes,", "citeRegEx": "Bottou.,? \\Q1991\\E", "shortCiteRegEx": "Bottou.", "year": 1991}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Applying deep learning to answer selection: A study and an open task", "author": ["Feng et al.2015] Minwei Feng", "Bing Xiang", "Michael R Glass", "Lidan Wang", "Bowen Zhou"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU),", "citeRegEx": "Feng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Feng et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Towards zero unknown in neural machine translation", "author": ["Li et al.2016] Xiaoqing Li", "Jiajun Zhang", "Chengqing Zong"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Hierarchical recurrent neural network for document modeling", "author": ["Lin et al.2015] Rui Lin", "Shujie Liu", "Muyun Yang", "Mu Li", "Ming Zhou", "Sheng Li"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Lin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Named entity recognition: fallacies, challenges and opportunities", "author": ["Juli\u00e1n Urbano", "Sonia S\u00e1nchez-Cuadrado", "Jorge Morato", "Juan Miguel G\u00f3mez-Berb\u0131\u0301s"], "venue": "Computer Standards & Interfaces,", "citeRegEx": "Marrero et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Marrero et al\\.", "year": 2013}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL),", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "End-to-end memory networks", "author": ["Jason Weston", "Rob Fergus"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Xiong et al.2016] Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning (ICML)", "citeRegEx": "Xiong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Xiong et al\\.", "year": 2016}, {"title": "Hierarchical attention networks for document classification", "author": ["Yang et al.2016] Zichao Yang", "Diyi Yang", "Chris Dyer", "Xiaodong He", "Alex Smola", "Eduard Hovy"], "venue": "In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACLHLT)", "citeRegEx": "Yang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2016}, {"title": "Empirical study on deep learning models for question answering", "author": ["Yu et al.2015] Yang Yu", "Wei Zhang", "Chung-Wei Hang", "Bowen Zhou"], "venue": "arXiv preprint arXiv:1510.07526", "citeRegEx": "Yu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yu et al\\.", "year": 2015}, {"title": "Relation classification via convolutional deep neural network", "author": ["Zeng et al.2014] Daojian Zeng", "Kang Liu", "Siwei Lai", "Guangyou Zhou", "Jun Zhao"], "venue": "In Proceedings of the 25th International Conference on Computational Linguistics (COLING),", "citeRegEx": "Zeng et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zeng et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 4, "context": "1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015).", "startOffset": 192, "endOffset": 278}, {"referenceID": 13, "context": "1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015).", "startOffset": 192, "endOffset": 278}, {"referenceID": 17, "context": "1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015).", "startOffset": 192, "endOffset": 278}, {"referenceID": 5, "context": "1 Introduction With the recent resurgence of interest in Deep Neural Networks (DNN), many researchers have concentrated on using deep learning to solve natural language processing (NLP) tasks (Collobert et al., 2011; Sutskever et al., 2014; Zeng et al., 2014; Feng et al., 2015).", "startOffset": 192, "endOffset": 278}, {"referenceID": 3, "context": "However, the memory of these methods, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al.", "startOffset": 142, "endOffset": 160}, {"referenceID": 11, "context": ", 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al., 2015).", "startOffset": 197, "endOffset": 217}, {"referenceID": 6, "context": "Due to the drawback, these traditional DNN models encounter great limitation on Question Answering (QA), as a complex NLP task, which requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question (Hermann et al., 2015; Yu et al., 2015).", "startOffset": 243, "endOffset": 282}, {"referenceID": 16, "context": "Due to the drawback, these traditional DNN models encounter great limitation on Question Answering (QA), as a complex NLP task, which requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question (Hermann et al., 2015; Yu et al., 2015).", "startOffset": 243, "endOffset": 282}, {"referenceID": 12, "context": "Recently, lots of deep learning methods with explicit memory and attention mechanism are explored for Question Answering (QA) task, such as Memory Networks (MemNN) (Sukhbaatar et al., 2015), Neural Machine Translation (NMT) and Neural Turing Machine (NTM) (Yu et al.", "startOffset": 164, "endOffset": 189}, {"referenceID": 16, "context": ", 2015), Neural Machine Translation (NMT) and Neural Turing Machine (NTM) (Yu et al., 2015).", "startOffset": 74, "endOffset": 91}, {"referenceID": 3, "context": "However, the memory of these methods, such as Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014) compressing all the external sentences into a fixed-length vector, is typically too small to accurately remember facts from the past, and may lose important details for response generation (Shang et al., 2015). Due to the drawback, these traditional DNN models encounter great limitation on Question Answering (QA), as a complex NLP task, which requires deep understanding of semantic abstraction and reasoning over facts that are relevant to a question (Hermann et al., 2015; Yu et al., 2015). Recently, lots of deep learning methods with explicit memory and attention mechanism are explored for Question Answering (QA) task, such as Memory Networks (MemNN) (Sukhbaatar et al., 2015), Neural Machine Translation (NMT) and Neural Turing Machine (NTM) (Yu et al., 2015). These methods exploit a external memory to store the past sentences with a continuous representation and utilize attention mechanism to automatically soft-search for parts of the memory for prediction. Compared with NMT and NTM, MemNN, making multiple computational steps (termed as \u201chops\u201d) on the memory before making an output, is better qualified for textual reasoning tasks. However, for QA task, MemNN only conducts the reasoning on sentence-level memory and does not further take any attention mechanism to focus on words in the retrieved facts. More recently, Yu et al. (2015) constructed a Search-Response", "startOffset": 143, "endOffset": 1515}, {"referenceID": 9, "context": "Along the direction of that work, we believe that a joint learning model can achieve a better performance by designing a hierarchical architecture, with sentence-level and word-level components, which has shown promising results on document modeling (Lin et al., 2015) and document classification (Yang et al.", "startOffset": 250, "endOffset": 268}, {"referenceID": 15, "context": ", 2015) and document classification (Yang et al., 2016).", "startOffset": 36, "endOffset": 55}, {"referenceID": 10, "context": "Besides, rare and unknown word problem as an important issue should be considered in NLP tasks, especially for QA task, where the words that we are mainly interested in are usually named entities which are mostly unknown or rare words (Marrero et al., 2013; Gulcehre et al., 2016).", "startOffset": 235, "endOffset": 280}, {"referenceID": 8, "context": "In order to control the computational complexity, many methods limit the trained vocabulary size, which further leads to lots of low-frequency words outside the trained vocabulary (Li et al., 2016).", "startOffset": 180, "endOffset": 197}, {"referenceID": 1, "context": "2 Background: Memory Networks Here, we give a brief description of memory networks which have shown promising results on QA tasks (Weston et al., 2015; Bordes et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 130, "endOffset": 197}, {"referenceID": 12, "context": "2 Background: Memory Networks Here, we give a brief description of memory networks which have shown promising results on QA tasks (Weston et al., 2015; Bordes et al., 2015; Sukhbaatar et al., 2015).", "startOffset": 130, "endOffset": 197}, {"referenceID": 1, "context": ", 2015; Bordes et al., 2015; Sukhbaatar et al., 2015). Memory network first introduced by Weston et al. (2015) is a new class of learning models which can easily read and write to part of a long-term memory component, and combine this seamlessly with inference for prediction.", "startOffset": 8, "endOffset": 111}, {"referenceID": 12, "context": "Along the above framework, Sukhbaatar et al. (2015) put forward end-to-end memory networks which do not require the supervision of the supporting facts and are more generally applicable in realistic setting.", "startOffset": 27, "endOffset": 52}, {"referenceID": 12, "context": "2 Sentence-level Memory and Reasoning In this section, we apply reasoning module to make multiple interaction on sentence-level memory based on the adjacent weight tying scheme of MemNN (Sukhbaatar et al., 2015), as shown in Figure 1(b).", "startOffset": 186, "endOffset": 211}, {"referenceID": 14, "context": "This positional encoding scheme is also successfully applied in (Xiong et al., 2016).", "startOffset": 64, "endOffset": 84}, {"referenceID": 2, "context": "We choose the cross entropy as the cost function and apply Stochastic Gradient Descent (SGD) (Bottou, 1991) as the optimization method to train our joint model.", "startOffset": 93, "endOffset": 107}, {"referenceID": 12, "context": "2 Comparison with Memory Networks In order to evaluate the effect of multiple hops for reasoning module and temporal encoding on sentencelevel memory, we design three MemNN (Sukhbaatar et al., 2015) based variants: MemNN-H1 (1 hop and temporal encoding), MemNN-NT (3 hops but not temporal encoding) and MemNN (3 hops and temporal encoding) on the datasets.", "startOffset": 173, "endOffset": 198}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al.", "startOffset": 48, "endOffset": 71}, {"referenceID": 5, "context": "These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015).", "startOffset": 133, "endOffset": 152}, {"referenceID": 11, "context": "For example, Yu et al. (2015) applied Neural Machine Translation (NMT) (Bahdanau et al.", "startOffset": 13, "endOffset": 30}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task.", "startOffset": 49, "endOffset": 246}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task. These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015). However, most works of these deep learning based methods rarely focus on solving answer selection on unknown word problem. Recently, the unknown word problem has attracted more researchers\u2019 attention. Hermann et al. (2015) used NLP tools to recognize all the entity and establish co-references to replace all the rare entities by placeholders and trained an attention based model with softmax to predict the placeholder id.", "startOffset": 49, "endOffset": 739}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task. These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015). However, most works of these deep learning based methods rarely focus on solving answer selection on unknown word problem. Recently, the unknown word problem has attracted more researchers\u2019 attention. Hermann et al. (2015) used NLP tools to recognize all the entity and establish co-references to replace all the rare entities by placeholders and trained an attention based model with softmax to predict the placeholder id. Li et al. (2016) replaced the rare words in a test sentence with similarity in-vocabulary words to solve machine translation task, where the representation of the rare words still can be learned from a large mono-lingual corpus.", "startOffset": 49, "endOffset": 957}, {"referenceID": 0, "context": "(2015) applied Neural Machine Translation (NMT) (Bahdanau et al., 2015) with sophisticated attention mechanism and Neural Turing Machine (NTM) (Graves et al., 2014) with distributed external memory to solve QA tasks, and Sukhbaatar et al. (2015) designed end-to-end memory networks and introduced multi-hop reasoning component to solve various types of QA task. These representation learning based methods do not rely on any linguistic tools and can be applied to different languages or domains (Feng et al., 2015). However, most works of these deep learning based methods rarely focus on solving answer selection on unknown word problem. Recently, the unknown word problem has attracted more researchers\u2019 attention. Hermann et al. (2015) used NLP tools to recognize all the entity and establish co-references to replace all the rare entities by placeholders and trained an attention based model with softmax to predict the placeholder id. Li et al. (2016) replaced the rare words in a test sentence with similarity in-vocabulary words to solve machine translation task, where the representation of the rare words still can be learned from a large mono-lingual corpus. Gulcehre et al. (2016) utilized and extended the attention-based pointing mechanism (Vinyals et al.", "startOffset": 49, "endOffset": 1192}], "year": 2016, "abstractText": "Recently, end-to-end memory networks have shown promising results on Question Answering task, which encode the past facts into an explicit memory and perform reasoning ability by making multiple computational steps on the memory. However, memory networks conduct the reasoning on sentence-level memory to output coarse semantic vectors and do not further take any attention mechanism to focus on words, which may lead to the model lose some detail information, especially when the answers are rare or unknown words. In this paper, we propose a novel Hierarchical Memory Networks, dubbed HMN. First, we encode the past facts into sentencelevel memory and word-level memory respectively. Then, k-max pooling is exploited following reasoning module on the sentence-level memory to sample the k most relevant sentences to a question and feed these sentences into attention mechanism on the word-level memory to focus the words in the selected sentences. Finally, the prediction is jointly learned over the outputs of the sentence-level reasoning module and the word-level attention mechanism. The experimental results demonstrate that our approach successfully conducts answer selection on unknown words and achieves a better performance than memory networks.", "creator": "LaTeX with hyperref package"}}}