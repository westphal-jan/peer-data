{"id": "1609.09001", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Learning from the Hindsight Plan -- Episodic MPC Improvement", "abstract": "Model predictive control (MPC) is a popular control method that has proved effective for robotics, among other fields. MPC performs re-planning at every time step. Re-planning is done with a limited horizon per computational and real-time constraints and often also for robustness to potential model errors. However, the limited horizon leads to suboptimal performance. In this work, we consider the iterative learning setting, where the same task can be repeated several times, and propose a policy improvement scheme for MPC. The main idea is that between executions we can, offline, run MPC with a longer horizon, resulting in a hindsight plan. To bring the next real-world execution closer to the hindsight plan, our approach learns to re-shape the original cost function with the goal of satisfying the following property: short horizon planning (as realistic during real executions) with respect to the shaped cost should result in mimicking the hindsight plan. This effectively consolidates long-term reasoning into the short-horizon planning. We empirically evaluate our approach in contact-rich manipulation tasks both in simulated and real environments, such as peg insertion by a real PR2 robot.", "histories": [["v1", "Wed, 28 Sep 2016 16:43:18 GMT  (4379kb,D)", "https://arxiv.org/abs/1609.09001v1", null], ["v2", "Mon, 20 Mar 2017 22:29:23 GMT  (4419kb,D)", "http://arxiv.org/abs/1609.09001v2", "Additional experiments for neural network generalization and for varying the planning horizon. Paper accepted to ICRA 2017"]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG", "authors": ["aviv tamar", "garrett thomas", "tianhao zhang", "sergey levine", "pieter abbeel"], "accepted": false, "id": "1609.09001"}, "pdf": {"name": "1609.09001.pdf", "metadata": {"source": "CRF", "title": "Learning from the Hindsight Plan \u2013 Episodic MPC Improvement", "authors": ["Aviv Tamar", "Garrett Thomas", "Tianhao Zhang", "Sergey Levine", "Pieter Abbeel"], "emails": [], "sections": [{"heading": null, "text": "In this context, it should be noted that the measures in question are measures taken by the ECB."}, {"heading": "II. RELATED WORK", "text": "A standard approach to mitigating the limited planning horizon in MPC is to introduce a value function, also known as an infinite horizon MPC. [10] However, if the model is not fully known or is too large to calculate the value function, it must be observed that the standard work function is insufficient when combined with MPC. [11] Here, too, there will be a reduction in value capacity. [14]"}, {"heading": "III. PRELIMINARIES", "text": "In this thesis, we consider the standard episodic amplification learning or optimal control setting in discrete time with episode length T. We denote by xt the state at time t, and ut the control. The goal is to generate a control sequence U0: T = (u0,.., uT) that minimizes the total cost \u2211 T = 0't (xt, ut) in the face of an initial state x0, where t is a predefined task-specific scalar cost function, under dynamics given by xt + 1 = f (xt, ut). At the time step t, adaptive MPC methods calculate the control ut by first estimating the dynamic model and then approximating the solution for the optimal initial action. Specifically, we let Dt = {(xs, us, xs + 1) t \u2212 1s = 0 represent a dataset of observed interaction with the system up to the time. Let H \u2264 T denote the MPC horizon, which is a parameter of the algorithm."}, {"heading": "IV. THE HINDSIGHT PLAN", "text": "rE \"s,\" he says, \"it is that we are able to unite ourselves.\" (mA) D \"i\" n \"i\" n \"i\" i \"i\" n. \"D\" E \"i\" n \"i\" i \"i\" i. \"D\" i \"n\" i. \"D\" i \"i\" i \"i.\" D \"iW\" i \"n\" i \"i\" i. \"D\" i \"i\" i \"i\" i \"i\" i. \"D\" i \"i\" i \"i\" i. \"i\" i. \"\" i \"i.\" i \"i\" i, \"i\" i \"i\" i \"i,\" i \"i\" i \"i\" i, \"i\" i \"i,\" i \"i,\" i, \"i,\" i \"i,\" i., \"i.,\" i, \"i\" i, \"i,\" i \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i\" i, \"i,\" i, \"i\" i, \"i,\" i, \"i\" i, \"i,\" i \"i,\" i, \"i,\" i \"i,\" i, \"i\" i, \"i,\" i, \"i,\" i, \"i\" i, \"i,\" i \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i \"i,\" i, \"i,\" i, \"i,\" i, \"i,\" i, \"i\" i, \"i.\" i, \"i,\" i. \"i,\" i, \"i,\" i. \"i.\" D, \"i,\" i, \"i,\" i, \"i,\" i. \"i,\" D, \"i,\" i."}, {"heading": "V. MPC POLICY IMPROVEMENT", "text": "In this section, we'll show how the cost-cutting plan can be used to solve a political improvement problem by using it to learn a cost-cutting problem for MPC. (The idea is to learn a cost-cutting concept that encourages the online MPC solution to be more similar to the solution in retrospect.) Let's take a closer look at the MPC optimization problem and add a cost-cutting term to the original cost that raises the cost of implementing actions (xs, us, us, us), s.t. xs \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s (s, us) s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s."}, {"heading": "VI. AN LQR IMPLEMENTATION OF HIMPC", "text": "In this section, we describe a specific implementation of the HIMPC algorithm, based on LQR planning and GMM dynamics learning. We are inspired by the work of [6], in which similar methods are used within MPC for effectively contactless manipulation tasks. In addition, we are able to consider the goal of mimics hindsight controllers (1) as the starting point of the original MPC control (1), and collect tractor information to find the mimics hindsight controlleri 0 = arg minctuer L0 (2) as the starting point for i = 1,2,. we do 5 Run MPC controllers with cost-shaping parameters. \u2212 1, and collect trajectory information Ii 6 Calculate hindsight plan (2) with Ii 7 Solve goveri = arg minctui i i i i i k = 0 Lk (2) endpropose a novel cost structure that is both efficient for optimization and has an intuitive interpretation."}, {"heading": "VII. PRACTICAL IMPROVEMENTS OF HIMPC", "text": "As with most machine learning algorithms and neural networks in particular, successful application of the method requires some technical know-how [31], [32]. In this section, we report on several technical procedures we have found to improve the performance of HIMPC in our experiments.a) Add control noise to the MPC: This helps the MPC controller to bypass \"local minima\" in the trajectory through random exploration, and the HIMPC then learns a cost structure that consolidates this trajectory improvement. In particular, we have used the exploration scheme of [6] in our experiments.b) Gathering multiple trajectories in each trajectory: We have found that collecting multiple rollouts of the same shaped MPC controller in each iteration (lines 1 and 5 in algorithm 1) stabilizes the trajectory of the neural network before waiting for the successful MPC trajectory to be completed before we can set the cost modeling of its trajectory in the MPC: If the initial trajectory is sufficient due to MPC:"}, {"heading": "VIII. EXPERIMENTS", "text": "In this section, we evaluate the HIMPC algorithm in simulated and real robot experiments. In this work, we focus on contact manipulations that follow the work of Fu et al. [6] In our evaluation, we aim to answer the following two questions: 1) Can we focus on standard MPC tasks that are important for assembly and improve HIMPC through standard MPC solutions?"}, {"heading": "A. Simulated 2D Navigation", "text": "This task, as illustrated in Figure 2, requires the movement of a particle through an opening toward a target position in 2D space. Available controls apply horizontal and vertical forces to the particle, and obstacles cause friction and normal forces. Episodes are 200 time steps long, and three trajectories are executed in each iteration. Horizon H and Horizon H were selected as 10 and 30. The cost of the neural network g consists of two fully connected hidden layers, each executed with 25 units and tanh activations.The inputs to g are the particle position and velocity. GMM dynamics were initialized before a single episode with a random initial policy."}, {"heading": "B. Simulated 3D Peg Insertion", "text": "In this task, a 7-D robotic arm must insert a cylindrical rod into a tight-fitting rectangular hole, as shown in Figure 4a & 4b. Controls are the torques applied to the 7 joints, and the state space consists of the positions and angular velocities of these joints when the level is fully inserted into the hole. Thus, there is a degree of freedom in the target position, since there are only 2 EE points. The target is determined by the coordinates of the EE points (but not the joints) when the level is fully inserted into the hole."}, {"heading": "C. Simulated 3D Oblong Peg Insertion", "text": "In this experiment, we show how HIMPC can learn additional structure in the task with random exploration noise in the control, and represent it in the formed costs.This task is similar to the previous peg insertion task, except that the peg and the hole have an elongated shape that requires a certain orientation of the peg for successful insertion. As before, the goal is set by the coordinates of the 2 EE points when the peg is completely inserted into the hole. Therefore, the cost function does not contain information about the correct orientation to solve the task. With sufficient exploration noise, standard MPC can sometimes solve this task. Our goal is to show that HIMPC can consolidate the information from these \"happy\" runs and learn a cost structure that guides the peg into the correct orientation. As before, episodes are 400 time steps long, and 3 paths are executed at each iteration."}, {"heading": "D. Real PR2 Experiments", "text": "The task of the robot is to insert a small wooden peg into a hole in a wooden plank.The task is similar to the simulated peg insertion of Section VIII-B. The target position is determined by the position of 3 EE points when the peg is completely inserted into the hole. We constructed GMM dynamics in front of 40,000 samples of free space actions, which are collected by executing the iLQG algorithm for 30 minutes, achieving random target positions. Such a large dataset was required for stable MPC control. In addition, we used a shorter warning horizon than in the H-30 simulation to take into account the less accurate dynamic.5https: / sites.google.com / site / himpchindsightplan / In Figure 3c we plot the distance of the EE points from their target position, for the original MPC control, this task can be directly inserted into the MP3 episodes directly and then the MP3 approach in the MP3-Zipline."}, {"heading": "IX. CONCLUSION", "text": "In this paper, we have introduced a new approach to improving policy in repeated tasks. Instead of using value functions or policy histories = > QQ = > Function - the traditional drivers of policy improvement in RL - our methodology uses an online MPC policy and proposes improved measures based on offline retrospective calculation. We have a significant improvement over standard MPCs on several complex manipulation tasks with contacts and a remarkable improvement in sample efficiency via the status-of-the-art model based on RL. In future work, we intend to investigate the use of different dynamic prediction models in our methodology and applications in different robotic domains such as quadrotors. Furthermore, the explicit use of the prediction error as a driver for policy improvement could potentially be used in different RL algorithms. APPENDIX I Dynamics Prediction: We used the Dynamics Prediction Method of the [6] during the entire observational period."}], "references": [{"title": "An integrated system for real-time model predictive control of humanoid robots", "author": ["T. Erez", "K. Lowrey", "Y. Tassa", "V. Kumar", "S. Kolev", "E. Todorov"], "venue": "Humanoids, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Extensions of learningbased model predictive control for real-time application to a quadrotor helicopter", "author": ["A. Aswani", "P. Bouffard", "C. Tomlin"], "venue": "ACC, 2012, pp. 4661\u20134666.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Concurrent learning adaptive model predictive control", "author": ["G. Chowdhary", "M. M\u00fchlegg", "J.P. How", "F. Holzapfel"], "venue": "Advances in Aerospace Guidance, Navigation and Control. Springer, 2013, pp. 29\u201347.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["I. Lenz", "R. Knepper", "A. Saxena"], "venue": "Robotics Science and Systems, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "One-shot learning of manipulation skills with online dynamics adaptation and neural network priors", "author": ["J. Fu", "S. Levine", "P. Abbeel"], "venue": "IROS, 2016.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "The dependence of effective planning horizon on model accuracy", "author": ["N. Jiang", "A. Kulesza", "S. Singh", "R. Lewis"], "venue": "AAMAS. International Foundation for Autonomous Agents and Multiagent Systems, 2015, pp. 1181\u20131189.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning in robotics: A survey", "author": ["J. Kober", "J.A. Bagnell", "J. Peters"], "venue": "International Journal of Robotics Research, 2013.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "A quasi-infinite horizon nonlinear model predictive control scheme with guaranteed stability", "author": ["H. Chen", "F. ALLGoWER"], "venue": "Automatica, vol. 34, no. 10, pp. 1205\u20131217, 1998.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Infinite-horizon model predictive control for periodic tasks with contacts", "author": ["T. Erez", "Y. Tassa", "E. Todorov"], "venue": "Robotics: Science and systems, p. 73, 2012.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Value function approximation and model predictive control", "author": ["M. Zhong", "M. Johnson", "Y. Tassa", "T. Erez", "E. Todorov"], "venue": "2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL). IEEE, 2013, pp. 100\u2013107.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Iterative learning control", "author": ["K.L. Moore", "J.-X. Xu"], "venue": "Taylor & Francis,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2000}, {"title": "Iterative learning control and repetitive control for engineering practice", "author": ["R.W. Longman"], "venue": "International journal of control, vol. 73, no. 10, pp. 930\u2013954, 2000.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2000}, {"title": "A survey of iterative learning control", "author": ["D.A. Bristow", "M. Tharayil", "A.G. Alleyne"], "venue": "IEEE Control Systems, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Survey on iterative learning control, repetitive control, and run-to-run control", "author": ["Y. Wang", "F. Gao", "F.J. Doyle"], "venue": "Journal of Process Control, vol. 19, no. 10, pp. 1589\u20131600, 2009.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2009}, {"title": "Learning model predictive control for iterative tasks", "author": ["U. Rosolia", "F. Borrelli"], "venue": "arXiv preprint arXiv:1609.01387, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Reinforcement planning: RL for optimal planners", "author": ["M. Zucker", "J.A. Bagnell"], "venue": "ICRA. IEEE, 2012, pp. 1850\u20131855.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "A framework for simulation-based network control via hindsight optimization", "author": ["E.K. Chong", "R.L. Givan", "H.S. Chang"], "venue": "CDC, 2000.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2000}, {"title": "Maximum likelihood estimation of discrete control processes", "author": ["J. Rust"], "venue": "SIAM Journal on Control and Optimization, vol. 26, no. 5, pp. 1006\u20131024, 1988.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1988}, {"title": "Apprenticeship learning via inverse reinforcement learning", "author": ["P. Abbeel", "A.Y. Ng"], "venue": "ICML. ACM, 2004, p. 1.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "PILCO: A model-based and data-efficient approach to policy search", "author": ["M. Deisenroth", "C.E. Rasmussen"], "venue": "ICML, 2011, pp. 465\u2013 472.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "NIPS, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "NIPS, 2007.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2007}, {"title": "Learning contact-rich manipulation skills with guided policy search", "author": ["S. Levine", "N. Wagener", "P. Abbeel"], "venue": "ICRA, 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient estimation using stochastic computation graphs", "author": ["J. Schulman", "N. Heess", "T. Weber", "P. Abbeel"], "venue": "NIPS, 2015, pp. 3528\u20133536.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Value iteration networks", "author": ["A. Tamar", "Y. Wu", "G. Thomas", "S. Levine", "P. Abbeel"], "venue": "CoRR, vol. abs/1602.02867, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["D.C. Liu", "J. Nocedal"], "venue": "Mathematical programming, 1989.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1989}, {"title": "Optimal control: linear quadratic methods", "author": ["B.D. Anderson", "J.B. Moore"], "venue": "Courier Corporation,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2688}, {"title": "Efficient backprop", "author": ["Y.A. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "Neural networks: Tricks of the trade. Springer, 2012.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2012}, {"title": "Understanding the difficulty of training deep feedforward neural networks.", "author": ["X. Glorot", "Y. Bengio"], "venue": "in Aistats, vol", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2010}, {"title": "Mujoco: A physics engine for model-based control", "author": ["E. Todorov", "T. Erez", "Y. Tassa"], "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012, pp. 5026\u20135033.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2012}, {"title": "Guided policy search code implementation", "author": ["C. Finn", "M. Zhang", "J. Fu", "X. Tan", "Z. McCarthy", "E. Scharff", "S. Levine"], "venue": "2016. [Online]. Available: http://rll.berkeley.edu/gps", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning deep neural network policies with continuous memory states", "author": ["M. Zhang", "Z. McCarthy", "C. Finn", "S. Levine", "P. Abbeel"], "venue": "ICRA. IEEE, 2016, pp. 520\u2013527.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive probabilistic trajectory optimization via efficient approximate inference", "author": ["Y. Pan", "X. Yan", "E. Theodorou", "B. Boots"], "venue": "arXiv preprint arXiv:1608.06235, 2016.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": ", [1], [2]) among other domains.", "startOffset": 7, "endOffset": 10}, {"referenceID": 1, "context": "This approach has been successfully applied in various robotic tasks, from aerial vehicle flight [3], [4], to contact-rich object manipulation [5], [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 2, "context": "This approach has been successfully applied in various robotic tasks, from aerial vehicle flight [3], [4], to contact-rich object manipulation [5], [6].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "This approach has been successfully applied in various robotic tasks, from aerial vehicle flight [3], [4], to contact-rich object manipulation [5], [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 4, "context": "This approach has been successfully applied in various robotic tasks, from aerial vehicle flight [3], [4], to contact-rich object manipulation [5], [6].", "startOffset": 148, "endOffset": 151}, {"referenceID": 5, "context": "In these applications, the limited horizon of MPC serves a dual purpose: maintaining tractability of the planning problem and mitigating error propagation during planning as a result of inaccurate models [7].", "startOffset": 204, "endOffset": 207}, {"referenceID": 3, "context": "cutting vegetables [5] and object manipulation [6].", "startOffset": 19, "endOffset": 22}, {"referenceID": 4, "context": "cutting vegetables [5] and object manipulation [6].", "startOffset": 47, "endOffset": 50}, {"referenceID": 3, "context": "However, state-of-theart MPC methods that only learn dynamics, such as [5] and", "startOffset": 71, "endOffset": 74}, {"referenceID": 4, "context": "[6], are prone to repeatedly produce suboptimal behavior in each episode of the task due to the limited horizon.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "which are typically based on value functions or policy gradients [8], our approach exploits the predictive nature of MPC to drive policy improvement, by contrasting the predicted actions with actions calculated in hindsight.", "startOffset": 65, "endOffset": 68}, {"referenceID": 7, "context": "A standard approach for mitigating the limited planning horizon in MPC is to introduce a value function as the terminal cost, also known as infinite horizon MPC [9], [10].", "startOffset": 161, "endOffset": 164}, {"referenceID": 8, "context": "A standard approach for mitigating the limited planning horizon in MPC is to introduce a value function as the terminal cost, also known as infinite horizon MPC [9], [10].", "startOffset": 166, "endOffset": 170}, {"referenceID": 9, "context": "However, it has been observed that standard value function approximation methods perform poorly when combined with MPC [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 9, "context": "[11] also proposed ar X iv :1 60 9.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "However, instead of learning directly from observed scalar costs, as typical for value function approximation [8], our method learns the cost shaping from the hindsight plan, which contains desired controls and hence is much more informative.", "startOffset": 110, "endOffset": 113}, {"referenceID": 10, "context": "Our episodic formulation of improving the MPC controller is similar to the setting of iterative learning control (ILC) [12], [13], [14], [15].", "startOffset": 119, "endOffset": 123}, {"referenceID": 11, "context": "Our episodic formulation of improving the MPC controller is similar to the setting of iterative learning control (ILC) [12], [13], [14], [15].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "Our episodic formulation of improving the MPC controller is similar to the setting of iterative learning control (ILC) [12], [13], [14], [15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 13, "context": "Our episodic formulation of improving the MPC controller is similar to the setting of iterative learning control (ILC) [12], [13], [14], [15].", "startOffset": 137, "endOffset": 141}, {"referenceID": 14, "context": "We note that a very recent work [16] proposed an extension of ILC that does not require a reference trajectory, and also uses a form of learning MPC, based on adding a value function to the MPC cost, which is calculated for all previously visited states.", "startOffset": 32, "endOffset": 36}, {"referenceID": 15, "context": "Reinforcement planning [17] considers a discrete highlevel planner for a continuous low-level control algorithm, and learns cost parameters of the planner using reinforcement learning methods.", "startOffset": 23, "endOffset": 27}, {"referenceID": 16, "context": "We also note that our method is different from hindsight optimization [18], in which simulation is used to calculate a value function online.", "startOffset": 70, "endOffset": 74}, {"referenceID": 17, "context": "Learning a cost function from observed controls is a form of inverse optimal control (IOC) [19], [20].", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "Learning a cost function from observed controls is a form of inverse optimal control (IOC) [19], [20].", "startOffset": 97, "endOffset": 101}, {"referenceID": 19, "context": "While in IOC, the actions are assumed to be generated by an expert demonstrator, our method does not require such demonstrations, and the actions used for learning the cost shaping are generated by a planning algorithm which learns the model from interaction, as in model-based reinforcement learning (RL) [21], [22] and adaptive control [23].", "startOffset": 306, "endOffset": 310}, {"referenceID": 20, "context": "While in IOC, the actions are assumed to be generated by an expert demonstrator, our method does not require such demonstrations, and the actions used for learning the cost shaping are generated by a planning algorithm which learns the model from interaction, as in model-based reinforcement learning (RL) [21], [22] and adaptive control [23].", "startOffset": 312, "endOffset": 316}, {"referenceID": 6, "context": "Model-based RL is a standard approach in robotic control [8], achieving state-of-the-art results in various domains, from helicopter flight [24] to contact-rich manipulation [25].", "startOffset": 57, "endOffset": 60}, {"referenceID": 21, "context": "Model-based RL is a standard approach in robotic control [8], achieving state-of-the-art results in various domains, from helicopter flight [24] to contact-rich manipulation [25].", "startOffset": 140, "endOffset": 144}, {"referenceID": 22, "context": "Model-based RL is a standard approach in robotic control [8], achieving state-of-the-art results in various domains, from helicopter flight [24] to contact-rich manipulation [25].", "startOffset": 174, "endOffset": 178}, {"referenceID": 4, "context": "The use of MPC in RL allows for extremely data-efficient learning [6], which is especially important in robotics, where robot interaction time is often costly.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "In this work we improve the MPC controllers of [6] for contact-rich manipulation tasks.", "startOffset": 47, "endOffset": 50}, {"referenceID": 17, "context": "A similar problem is encountered in IOC [19], [20], and indeed, our approach can be seen as performing IOC with the hindsight plan replacing", "startOffset": 40, "endOffset": 44}, {"referenceID": 18, "context": "A similar problem is encountered in IOC [19], [20], and indeed, our approach can be seen as performing IOC with the hindsight plan replacing", "startOffset": 46, "endOffset": 50}, {"referenceID": 23, "context": "For planning algorithms that can be represented as a computation graph [26], such as linear quadratic regulator (LQR) and value iteration [27], the loss L can be minimized efficiently using gradient based methods, such as L-BFGS [28].", "startOffset": 71, "endOffset": 75}, {"referenceID": 24, "context": "For planning algorithms that can be represented as a computation graph [26], such as linear quadratic regulator (LQR) and value iteration [27], the loss L can be minimized efficiently using gradient based methods, such as L-BFGS [28].", "startOffset": 138, "endOffset": 142}, {"referenceID": 25, "context": "For planning algorithms that can be represented as a computation graph [26], such as linear quadratic regulator (LQR) and value iteration [27], the loss L can be minimized efficiently using gradient based methods, such as L-BFGS [28].", "startOffset": 229, "endOffset": 233}, {"referenceID": 7, "context": "Furthermore, if \u03b4s(xs,us, \u03b8) is set to 0 for all s < t+H , the cost shaping becomes a terminal value function, which is exactly the infinite horizon MPC formulation [9], [10].", "startOffset": 165, "endOffset": 168}, {"referenceID": 8, "context": "Furthermore, if \u03b4s(xs,us, \u03b8) is set to 0 for all s < t+H , the cost shaping becomes a terminal value function, which is exactly the infinite horizon MPC formulation [9], [10].", "startOffset": 170, "endOffset": 174}, {"referenceID": 9, "context": "Standard value function methods first approximate a value function, and then approximately solve the planning problem with that value function, resulting in two sources of approximation error, possibly explaining their poor performance observed in previous work [11].", "startOffset": 262, "endOffset": 266}, {"referenceID": 3, "context": "So far, we have not discussed the specific algorithms used for dynamics predictions and planning in MPC (1), nor the functional form of the cost shaping, and in principle, HIMPC can be combined with any adaptive MPC method such as [5] and [6].", "startOffset": 231, "endOffset": 234}, {"referenceID": 4, "context": "So far, we have not discussed the specific algorithms used for dynamics predictions and planning in MPC (1), nor the functional form of the cost shaping, and in principle, HIMPC can be combined with any adaptive MPC method such as [5] and [6].", "startOffset": 239, "endOffset": 242}, {"referenceID": 4, "context": "We are inspired by the work of [6], in which similar methods were used within MPC for effectively performing contact-rich manipulation tasks.", "startOffset": 31, "endOffset": 34}, {"referenceID": 4, "context": "The online dynamics adaptation algorithm of [6] is based on a Bayesian approach for estimation, where the recent observations within an episode are used to estimate a linear dynamics model, using observations from previous episodes as a Bayesian prior.", "startOffset": 44, "endOffset": 47}, {"referenceID": 26, "context": "Such a cost function is standard for many control tasks [29], and is particularly suitable for the manipulation experiments we consider, where the task is specified as moving the robot end-effector to some goal position, such as pushing a peg into a hole.", "startOffset": 56, "endOffset": 60}, {"referenceID": 26, "context": "With linear dynamics and a quadratic loss, the MPC planning problem (1) becomes a standard LQR problem [29], for which a solution can be calculated efficiently by dynamic programming, as described Appendix II.", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "This mapping can be represented as a computation graph, and the gradient \u2202ut/\u2202x \u2217 can be easily calculated using modern automatic differentiation packages such as Theano [30].", "startOffset": 170, "endOffset": 174}, {"referenceID": 0, "context": ", [2] for details.", "startOffset": 2, "endOffset": 5}, {"referenceID": 25, "context": "This gradient can be used for minimizing the similarity loss (4) with standard optimization algorithms such as L-BFGS [28].", "startOffset": 118, "endOffset": 122}, {"referenceID": 28, "context": "As with most machine learning algorithms, and neural networks in particular, a successful application of the method requires some technical know-how [31], [32].", "startOffset": 149, "endOffset": 153}, {"referenceID": 29, "context": "As with most machine learning algorithms, and neural networks in particular, a successful application of the method requires some technical know-how [31], [32].", "startOffset": 155, "endOffset": 159}, {"referenceID": 4, "context": "In particular, we used the exploration scheme of [6] in our experiments.", "startOffset": 49, "endOffset": 52}, {"referenceID": 4, "context": "[6], though our method can be applied to other tasks where MPC is applicable.", "startOffset": 0, "endOffset": 3}, {"referenceID": 30, "context": "All our simulations were performed using the MuJoCo physics engine [33], and our code, which is based on the guided policy search repository [34], will be made available.", "startOffset": 67, "endOffset": 71}, {"referenceID": 31, "context": "All our simulations were performed using the MuJoCo physics engine [33], and our code, which is based on the guided policy search repository [34], will be made available.", "startOffset": 141, "endOffset": 145}, {"referenceID": 4, "context": "In our evaluation, we compare HIMPC to the MPC method of [6], and, as a baseline, to iLQG \u2013 a state-of-the-art modelbased RL method [22].", "startOffset": 57, "endOffset": 60}, {"referenceID": 20, "context": "In our evaluation, we compare HIMPC to the MPC method of [6], and, as a baseline, to iLQG \u2013 a state-of-the-art modelbased RL method [22].", "startOffset": 132, "endOffset": 136}, {"referenceID": 4, "context": "Our dynamics model uses a GMM prior, combined with online dynamics adaptation [6], as further described in Appendix I.", "startOffset": 78, "endOffset": 81}, {"referenceID": 20, "context": "The iLQG method of [22] uses a similar GMM for a dynamics prior, but combines with time-varying linear dynamics, and therefore constitutes a fair comparison.", "startOffset": 19, "endOffset": 23}, {"referenceID": 4, "context": "However, MPC is expected to be much more sample-efficient, as was demonstrated in [6].", "startOffset": 82, "endOffset": 85}, {"referenceID": 27, "context": "However, solving the optimization problem in (4) was much slower, requiring several minutes of computation, since our Theano-based implementation [30] cannot differentiate a matrix inverse for multiple samples in parallel.", "startOffset": 146, "endOffset": 150}, {"referenceID": 32, "context": "This nontrivial task requires solving both a kinematic problem, and a control problem with complex contact dynamics, and has been used as a benchmark in previous studies [35].", "startOffset": 170, "endOffset": 174}, {"referenceID": 22, "context": "The NN g had 2 fully connected hidden layers of sizes [100, 25] with tanh activations, and its inputs were the positions of the EE points.", "startOffset": 54, "endOffset": 63}, {"referenceID": 4, "context": "Using dynamics models such as neural networks [6] or Gaussian processes [36] could potentially improve the dynamics learning, as we plan to investigate in future work.", "startOffset": 46, "endOffset": 49}, {"referenceID": 33, "context": "Using dynamics models such as neural networks [6] or Gaussian processes [36] could potentially improve the dynamics learning, as we plan to investigate in future work.", "startOffset": 72, "endOffset": 76}, {"referenceID": 22, "context": "The horizon H is 10, while the hindsight horizon H\u0304 was chosen as 60, and the NN g had 2 fully connected hidden layers of sizes [100, 25] with tanh activations.", "startOffset": 128, "endOffset": 137}, {"referenceID": 4, "context": "APPENDIX I Dynamics Prediction: We used the dynamics prediction method of [6].", "startOffset": 74, "endOffset": 77}, {"referenceID": 4, "context": "These withinepisode dynamics are mixed with a prior model of dynamics (\u03bcp,\u03a3p), by \u03bc = \u03b11\u03bc\u0302 + (1 \u2212 \u03b11)\u03bcp, and \u03a3 = \u03b12\u03a3p + \u03b13\u03a3\u0302+\u03b14(\u03bcp\u2212\u03bc\u0302)(\u03bcp\u2212\u03bc\u0302), where the prior is a GMM fit to samples from previous episodes, and the mixing coefficients \u03b11, \u03b12, \u03b13, \u03b14 are described in [6].", "startOffset": 267, "endOffset": 270}, {"referenceID": 4, "context": "We refer to [6] for the full details and theoretical motivation of this algorithm.", "startOffset": 12, "endOffset": 15}, {"referenceID": 26, "context": "Using dynamics programming [29]: Qxu,xut = `xu,xut + f > xutVx,xt+1fxut Qxut = `xut + f > xutVxt+1", "startOffset": 27, "endOffset": 31}], "year": 2017, "abstractText": "Model predictive control (MPC) is a popular control method that has proved effective for robotics, among other fields. MPC performs re-planning at every time step. Replanning is done with a limited horizon per computational and real-time constraints and often also for robustness to potential model errors. However, the limited horizon leads to suboptimal performance. In this work, we consider the iterative learning setting, where the same task can be repeated several times, and propose a policy improvement scheme for MPC. The main idea is that between executions we can, offline, run MPC with a longer horizon, resulting in a hindsight plan. To bring the next real-world execution closer to the hindsight plan, our approach learns to re-shape the original cost function with the goal of satisfying the following property: short horizon planning (as realistic during real executions) with respect to the shaped cost should result in mimicking the hindsight plan. This effectively consolidates long-term reasoning into the shorthorizon planning. We empirically evaluate our approach in contact-rich manipulation tasks both in simulated and real environments, such as peg insertion by a real PR2 robot.", "creator": "LaTeX with hyperref package"}}}