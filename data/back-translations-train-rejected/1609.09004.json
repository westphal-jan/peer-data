{"id": "1609.09004", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Byte-based Language Identification with Deep Convolutional Networks", "abstract": "We report on our system for the shared task on discriminating between similar languages (DSL 2016). The system uses only byte representations in a deep residual network (ResNet). The system, named ResIdent, is trained only on the data released with the task (closed training). We obtain 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A large difference in accuracy on development data can be observed with relatively minor changes in our network's architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies.", "histories": [["v1", "Wed, 28 Sep 2016 16:51:56 GMT  (146kb,D)", "https://arxiv.org/abs/1609.09004v1", "6 pages. arXiv admin note: text overlap witharXiv:1609.07053"], ["v2", "Fri, 28 Oct 2016 15:28:29 GMT  (74kb,D)", "http://arxiv.org/abs/1609.09004v2", "7 pages. Adapted reviewer comments. arXiv admin note: text overlap witharXiv:1609.07053"]], "COMMENTS": "6 pages. arXiv admin note: text overlap witharXiv:1609.07053", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["johannes bjerva"], "accepted": false, "id": "1609.09004"}, "pdf": {"name": "1609.09004.pdf", "metadata": {"source": "CRF", "title": "Byte-based Language Identification with Deep Convolutional Networks", "authors": ["Johannes Bjerva"], "emails": ["j.bjerva@rug.nl"], "sections": [{"heading": "1 Introduction", "text": "The identification of language is an unsolved problem, certainly in the context of the distinction between very similar languages (Baldwin and Lui, 2010). Most successful approaches to the DSL common task in recent years have relied on constellations containing groups of classifiers (Goutte et al., 2016), which often use different combinations of features based largely on word, character and / or byte programs (see e.g. Cavnar et al. (1994), Lui et al. (2012). We are interested in examining a single methodological aspect in the current edition of this common task (Malmasi et al., 2016)."}, {"heading": "2 Method", "text": "Several previous approaches to DSL collaborative tasks have formulated the task as a two-step classification, identifying first the language group and then the specific language (Zampieri et al., 2015). Instead of this approach, we are formulating the task as a multi-class classification problem, with each language / dialect representing its own class. Our system is a deep neural network consisting of a bi-directional gated recurrent unit (GRU) network at the upper level and a deep residual network (ResNet) at the lower level (Figure 1). Our system's inputs are byte-level representations of each input set, with byte embeddings learned during the training. The use of byte-level representations differs from the character-level representations in that UTF-8 does not encode ascii symbols with more than one byte (Asci symbols with more than one byte), which allows a more concrete example to be found if a relatively similar language and a more specific one can be found."}, {"heading": "2.1 Gated Recurrent Unit Networks", "text": "GRUs (Cho et al., 2014) are a recently introduced variant of RNNs and are designed to prevent disappearing gradients and thus handle longer input sequences than vanilla RNNs. GRUs are similar to the more commonly used Long Short-Term Memory Networks (LSTMs), both in purpose and in implementation (Chung et al., 2014). A bi-directional GRU performs both forward and backward transitions over sequences and can therefore use both previous and subsequent contexts to predict a day (Graves et al., 2005; Goldberg, 2015). Bidirectional GRUs and LSTMs have been shown to perform well on several NLP tasks, such as POS and semantic tagging, called entity tagging, and Chunking (Wang et al., 2015; Yang et al., 2016; Plank et al., 2016; Bjerva et al., 2016)."}, {"heading": "2.2 Deep Residual Networks", "text": "A residual unit can be expressed as follows: yl = h (xl) + F (xl, Wl), xl + 1 = f (yl), (1) where xl and xl + 1 are the input and output of the l-th layer, Wl is the weight for the l-th layer, and F is a residual function (He et al., 2016), e.g. the identity function (He et al., 2015), which we also use in our experiments. ResNets can be understood intuitively by treating residual functions as paths through which information can easily propagate. This means that a ResNet learns more complex function combinations in each layer, which it combines with the flatter representation of the remaining layer. This architecture enables the construction of much deeper networks. ResNets have recently proven to be impressive both in image recognition and in NLP tasks (He et al., 2016; Er and al, 2016; l)."}, {"heading": "2.3 System Description", "text": "We represent each sentence using a byte-based representation (Sb), which is a two-dimensional matrix Sb, Rs, and db, where s is the padded record length, and db is the dimensionality of the byte embeddings. Byte embeddings are first guided through a ResNet to obtain a representation that captures something like byte-N-gram characteristics.1 The size of n is determined by the wavy window size used. We use a wavy window size of length 8, which means that for each byte of input, the ResNet can learn a suitable representation that includes up to 8 bytes of context information.These overlapping byte-based N-gram characteristics are then passed to the bi-GRU, which gives a representation at record level. The softmax layer applied to the bi-GRU output is then used to obtain the predicted per-class input of the network."}, {"heading": "2.3.1 Hyperparameters", "text": "The hyperparameters used by the system were adjusted to a completely different task (semantic marker) and adapted to the current task. We use linear units (ReLUs) for all activation functions (Nair and Hinton, 2010) and apply dropouts with p = 0.1 to both input weights and recurring weights in the bi-GRU. All GRU layers contain 100 hidden units. All experiments were performed with early termination of the validation set loss, using a maximum of 50 epochs and a stack size of 100. Optimization is performed using the ADAM algorithm (Kingma and Ba, 2015), with the categorical cross-entropy losses being performed as a training object. For the B tasks, we train the model in the same way as for the A tasks. Only a handful of instances per group of five are classified in English."}, {"heading": "3 Results", "text": "We evaluate our system in subtasks A, B1 and B2. Subtask A contains data for five language groups with two to three languages in each group (Tan et al., 2014). Subtasks B1 and B2 contain data for a subset of 1Note that bytes are routed individually through the ResNet, resulting in one representation per byte and not as a whole sequence that would yield a single representation per sentence. Test Set Track Run Accuracy F1 (micro) F1 (macro) F1 (weighted) the languages in subtask A, compiled by Twitter. Subtask B1 contains the amount of tweets a human commentator needs to make reliable judgments, while B2 contains the maximum amount of data available per tweet. In subtask A and B1, run 3 leads to the best accuracy in the test, while run 2 leads to the best accuracy on B2. Results are presented in Table 2."}, {"heading": "4 Discussion", "text": "Although the system performs reasonably, there is a large gap between our system and the best performing systems (e.g. C-Ltekin and Rama (2016), which achieve an accuracy of 89.38% for Task A, 86.2% for Task B1 and 82.2% for Task B2), which can be explained to some extent by constraints caused by our implementation.The biggest limiting factor is the fact that we only allowed our system to use the first approximately 384 bytes of each training / test instance. For Training and Development Group and Sub-Task A, this was not a major limitation, as it allowed us to use more than 90% of the available data. However, for Tasks B1 and B2, this may have seriously impaired the performance of the system."}, {"heading": "5 Conclusions", "text": "We implemented a language identification system that uses deep residual networks (ResNets) in conjunction with a bi-directional gated recurrent unit network (Bi-GRU) using only byte-level representations. In the DSL 2016 Shared Task, we achieved reasonable performance with an accuracy of 84.88% for Subtask A, 68.80% for Subtask B1, and 69.80% for Subtask B2. Although acceptable performance was achieved, further fine-tuning of input representations and system architecture would likely improve performance."}, {"heading": "Acknowledgements", "text": "We would like to thank the Centre for Information Technology at the University of Groningen for their support and access to the Peregrine supercomputer cluster, as well as the anonymous reviewers for their valuable feedback."}], "references": [{"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467", "author": ["Vasudevan", "Fernanda B. Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "Vasudevan et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vasudevan et al\\.", "year": 2016}, {"title": "Language identification: The long and the short of the matter", "author": ["Baldwin", "Lui2010] Timothy Baldwin", "Marco Lui"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "citeRegEx": "Baldwin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Baldwin et al\\.", "year": 2010}, {"title": "Barbara Plank", "author": ["Johannes Bjerva"], "venue": "and Johan Bos.", "citeRegEx": "Bjerva et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "John M Trenkle", "author": ["William B Cavnar"], "venue": "et al.", "citeRegEx": "Cavnar et al.1994", "shortCiteRegEx": null, "year": 1994}, {"title": "Discriminating similar languages: experiments with linear SVMs and neural networks", "author": ["\u00c7\u00f6ltekin", "Rama2016] \u00c7a\u011fr\u0131 \u00c7\u00f6ltekin", "Taraka Rama"], "venue": "In Proceedings of the 3rd Workshop on Language Technology for Closely Related Languages, Varieties and Dialects (VarDial),", "citeRegEx": "\u00c7\u00f6ltekin et al\\.,? \\Q2016\\E", "shortCiteRegEx": "\u00c7\u00f6ltekin et al\\.", "year": 2016}, {"title": "Holger Schwenk", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "KyungHyun Cho", "author": ["Junyoung Chung", "Caglar Gulcehre"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lo\u0131\u0308c Barrault", "author": ["Alexis Conneau", "Holger Schwenk"], "venue": "and Yann Lecun.", "citeRegEx": "Conneau et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "A primer on neural network models for natural language processing. arXiv preprint arXiv:1510.00726", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Shervin Malmasi", "author": ["Cyril Goutte", "Serge L\u00e9ger"], "venue": "and Marcos Zampieri.", "citeRegEx": "Goutte et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "Shaoqing Ren", "author": ["Kaiming He", "Xiangyu Zhang"], "venue": "and Jian Sun.", "citeRegEx": "He et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Shaoqing Ren", "author": ["Kaiming He", "Xiangyu Zhang"], "venue": "and Jian Sun.", "citeRegEx": "He et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "author": ["Ioffe", "Szegedy2015] Sergey Ioffe", "Christian Szegedy"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "Ioffe et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Kingma", "Ba2015] Diederik Kingma", "Jimmy Ba"], "venue": "In Proceedings of ICLR", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "langid.py: An off-the-shelf language identification tool", "author": ["Lui", "Baldwin2012] Marco Lui", "Timothy Baldwin"], "venue": "In Proceedings of the ACL 2012 system demonstrations,", "citeRegEx": "Lui et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2012}, {"title": "Ahmed Ali", "author": ["Shervin Malmasi", "Marcos Zampieri", "Nikola Ljube\u0161i\u0107", "Preslav Nakov"], "venue": "and J\u00f6rg Tiedemann.", "citeRegEx": "Malmasi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["Nair", "Hinton2010] Vinod Nair", "Geoffrey E Hinton"], "venue": "In Proceedings of the 27th International Conference on Machine Learning", "citeRegEx": "Nair et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Nair et al\\.", "year": 2010}, {"title": "Morphological reinflection with convolutional neural networks", "author": ["Robert \u00d6stling"], "venue": "In Proceedings of the 2016 Meeting of SIGMORPHON,", "citeRegEx": "\u00d6stling.,? \\Q2016\\E", "shortCiteRegEx": "\u00d6stling.", "year": 2016}, {"title": "2016", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg"], "venue": "Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of ACL", "citeRegEx": "Plank et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Ilya Sutskever", "author": ["Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky"], "venue": "and Ruslan Salakhutdinov.", "citeRegEx": "Srivastava et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Nikola Ljube\u0161ic", "author": ["Liling Tan", "Marcos Zampieri"], "venue": "and J\u00f6rg Tiedemann.", "citeRegEx": "Tan et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Lei He", "author": ["Peilu Wang", "Yao Qian", "Frank K Soong"], "venue": "and Hai Zhao.", "citeRegEx": "Wang et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Ruslan Salakhutdinov", "author": ["Zhilin Yang"], "venue": "and William Cohen.", "citeRegEx": "Yang et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "2014", "author": ["Marcos Zampieri", "Liling Tan", "Nikola Ljube\u0161i\u0107", "J\u00f6rg Tiedemann"], "venue": "A Report on the DSL Shared Task", "citeRegEx": "Zampieri et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "2015", "author": ["Marcos Zampieri", "Liling Tan", "Nikola Ljube\u0161i\u0107", "J\u00f6rg Tiedemann", "Preslav Nakov"], "venue": "Overview of the DSL Shared Task", "citeRegEx": "Zampieri et al.2015", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [], "year": 2016, "abstractText": "We report on our system for the shared task on discrimination of similar languages (DSL 2016). The system uses only byte representations in a deep residual network (ResNet). The system, named ResIdent, is trained only on the data released with the task (closed training). We obtain 84.88% accuracy on subtask A, 68.80% accuracy on subtask B1, and 69.80% accuracy on subtask B2. A large difference in accuracy on development data can be observed with relatively minor changes in our network\u2019s architecture and hyperparameters. We therefore expect fine-tuning of these parameters to yield higher accuracies.", "creator": "LaTeX with hyperref package"}}}