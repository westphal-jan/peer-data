{"id": "1609.09007", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "28-Sep-2016", "title": "Unsupervised Neural Hidden Markov Models", "abstract": "In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag in- duction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.", "histories": [["v1", "Wed, 28 Sep 2016 16:55:52 GMT  (351kb,D)", "http://arxiv.org/abs/1609.09007v1", "accepted at EMNLP 2016, Workshop on Structured Prediction for NLP. Oral presentation"]], "COMMENTS": "accepted at EMNLP 2016, Workshop on Structured Prediction for NLP. Oral presentation", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["ke tran", "yonatan bisk", "ashish vaswani", "daniel marcu", "kevin knight"], "accepted": false, "id": "1609.09007"}, "pdf": {"name": "1609.09007.pdf", "metadata": {"source": "CRF", "title": "Unsupervised Neural Hidden Markov Models", "authors": ["Ke Tran", "Yonatan Bisk", "Ashish Vaswani", "Daniel Marcu", "Kevin Knight"], "emails": ["m.k.tran@uva.nl,", "ybisk@isi.edu,", "avaswani@google.com,", "marcu@isi.edu,", "knight@isi.edu"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Framework", "text": "Graphic models are widely used in NLP (z, x). Typically, potential functions \u043d (z, x) are defined via a series of latent variables, z and observed variables, x, based on handmade characteristics. In addition, independence assumptions between variables are often made for the sake of tractability. Furthermore, we propose to use neural networks (NNs) to generate the potentials, since neural networks are universal function approximators. Neural networks can extract useful task-specific abstract representations of data. Furthermore, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) is based on recurrent neural networks (RNNNNs), which allow us to model unlimited contexts with far fewer parameters than naive one-hot feature encodings. We use the repair parametrization of neural networks with NNNNNNNNNs (NNNNs)."}, {"heading": "3 Part-of-Speech Induction", "text": "In English, Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation. Tag induction is the task of using raw text to both detect these latent clusters and assign words in situ. Classes can be very specific (e.g. six verbal types in English) in terms of their syntactic role. Example tags are shown in Table 1. In this example, board is referred to as a singular noun, while Pierre Vinken is a singular idiom. Two natural uses of induced tags are the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed but unattended source for word embedding."}, {"heading": "3.1 The Hidden Markov Model", "text": "A common model for this task and our primary workhorse is the Hidden Markov model, which was developed using the unattended message transmission algorithm (BaumWelch, 2003).The HMMs model models a sentence based on the assumption that (a) each word symbol is generated by a latent class, and (b) the current class at the time t is based on local history t \u2212 1. Formally, this results in an emission sequence p (xt | zt) and a transition forecast p (zt \u2212 1) probability. The graphical model is illustrated in Figure 1, where shaded circles are observations and empty latent. The probability of a given sequence of observations x and latent variables z is given by multiplying transitions and emissions over all time steps (Eq. 6).Finding the optimal sequence of these latent classes corresponds to the calculation of an argmax over the values of e.p (x, z) + x1."}, {"heading": "3.2 Additional Comparisons", "text": "While the main focus of our work is on the seamless extension of an unattended generative latent variable model with neural networks, for the sake of completeness we will also include comparisons with other techniques that do not follow the generative assumption. We include Brown clusters (Brown et al., 1992) as a baseline and two cluster techniques as state-of-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012). Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word types by a greedy agglomerative clustering according to their mutual information throughout the corpus based on bigram probabilities. Brown clusters do not constitute the affiliation of a word to multiple syntactic classes, but are a very strong basis for tag induction. It is possible that our approach could be improved by extending our objective function by mutual information computations or a harder tendency to cluster formation."}, {"heading": "4 Neural HMM", "text": "The above-mentioned training of an HMM requires access to two distributions: (1) emissions with K \u00b7 V parameters and (2) transitions with K \u00b7 K parameters. Here, we assume that there are K clusters and word types in our vocabulary. Our neural HMM (NHMM) replaces these matrices by the output of simple upstream neural networks. All conditioning variables are represented as input to the network and its last Softmax layer provides probabilities. This should replicate the behavior of the standard HMM, but without an explicit representation of the necessary distributions."}, {"heading": "4.1 Producing Probabilities", "text": "The emission probability p (wi | zk) is achieved by p (wi | zk) = exp (v > k wi + bi) \u2211 V j = 1 exp (v > k wj + bj) (7) The emission probability can be implemented by a neural network, where wi is the weight of unit i at the output level of the network. Tag embeddings vk are achieved by a simple forward-facing neural network consisting of a lookup table followed by a non-linear activation (ReLU). If morphology information (\u00a7 5) is used, we will first use another network to generate the word embeddings. We generate the transition probability directly by using a linear layer of D \u00d7 Kb \u00b2 Kb > Kb = K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K \u00b7 K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K"}, {"heading": "4.2 Training the Neural Network", "text": "In this way, we perform the E step as if we were forming a vanilla HMM. Traditionally, these values would simply be normalized again during the M step to re-evaluate the model parameters. Instead, we use them to re-scale our gradients (following the discussion in \u00a7 2). Combining the HMM factorization of the joint probability p (x, z) from Equation 6 with the gradient from Equation 5, the following rule of update results: J (eg) = 0 p (z | x)."}, {"heading": "4.3 HMM and Neural HMM Equivalence", "text": "An important result we see in Table 2 is that the Neural HMM (NHMM) is almost identical to the HMM. At this point we have replaced the underlying machinery, but the model still has the same information bottlenecks as a standard HMM, limiting the amount and type of information transmitted between the words in the sentence. In addition, both approaches optimize the same objective function, the data probability, by calculating decimal points. Equivalence is an important health test. The following two sections will demonstrate the expandability of this approach."}, {"heading": "5 Convolutions for Morphology", "text": "The first experiment we will perform is the exchange of words by embedding vectors originating from a Convolutionary Neural Network (CNN) (Kim et al., 2016; Jozefowicz et al., 2016). We use a Convolutionary Nucleus with a width of 1 to 7 that contains up to 7 characters (Figure 2), which allows the model to automatically learn lexical representations based on prefix, suffix and stem information about a word. No additional changes to learning are required for the extension. Adding the convolution does not slow down our model dramatically, as the emission distributions for the entire stack can be calculated in one operation. We simply perform the entire vocabulary through the conversion in a single operation."}, {"heading": "6 Infinite Context with LSTMs", "text": "One of the strongest strengths of neural networks is their ability to generate a compact representation of data. We will examine this here when creating transition matrices. In particular, we have decided to extend the transition matrix by all the preceding words in the sentence: p (zt | zt \u2212 1, w0,..., wt \u2212 1). Incorporating this amount of context into a traditional HMM is insoluble and impossible to estimate, as the number of parameters increases exponentially. Therefore, we use a stacked LSTM to form a low-dimensional representation of the sentence (C0... t \u2212 1), which can easily be fed into our network when creating a transition matrix: p (zt | zt \u2212 1, C0... t \u2212 1) in Figure 3. By using the LSTM only down to the previous word, we do not break sequential generative model assumptions."}, {"heading": "7 Evaluation", "text": "Once a model is trained, the best latent sequence is extracted for each set and evaluated on three metrics. Many-to-One (M-1) Many-to-One calculates the most common true part-of-speech tag for each cluster. It then calculates the tagging accuracy as if the cluster were replaced by that tag, a metric that can be easily determined by introducing a large number of clusters. One-to-One (1-1) One-to-One performs the same calculation as Many-to-One, but only one cluster may be assigned to a given tag. This prevents the gaming of M-1.V-Measure (VM) V-Measure (VM) V-Measure from being the most informative and consistent metric, partly because it is induced by the number of tags."}, {"heading": "8 Data and Parameters", "text": "To evaluate our approaches, we follow the existing literature and train and test on the full WSJ corpus.1This interpretation does not complicate the calculation of forward-looking messages when it runs Tree-Welch, even though it does, by design, breaking Markovian assumption about the knowledge of the past. There are three components of our models that can be tuned. Something we have to be careful about if train and test encompass the same data. To avoid fraud, no values were tuned in this work. The first parameter is the number of hidden units. We chose 512 because it was the largest power of two that we could fit into the memory. When we expanded our model to include the Convolutionary Emission Network, we used only 128 units, due to the intensive compilation of Char-CNN across the entire vocabulary layers. The second design choice was the number of LSTM layers."}, {"heading": "9 Results", "text": "As mentioned above, we are pleased that our NHMM is almost identical to the standard HMM. Secondly, we note that our approach, while simple and fast, competes with Blunsom (2011). The hierarchical Pitman-Yor process for trigram HMM with character modeling is a very sophisticated Bayesian approach and the most appropriate comparison to our work. We see that both the extended context (+ LSTM) and the addition of morphological information (+ Conv) provide significant performance gains. Interestingly, the gains do not complement each other fully, as we find that the six and twelve points of these supplements only result in a total of sixteen points of intact improvement, which could imply that at least some of the syntactic context captured by the LSTM is reflected in the morphology of language. This hypothesis is something that morphologically speaking (our work could be improved morphologically)."}, {"heading": "10 Parameter Ablation", "text": "If we perform our best model (NHMM + Conv + LSTM) with all weights initialized from a uniform distribution U (\u2212 10 \u2212 4, 10 \u2212 4) 3, we find a dramatic decrease in V-Measure performance (61.7 vs. 71.7 in Table 3), which is in line with conventional wisdom that, unlike supervised learning (Luong et al., 2015), weight initialization is important to achieve good performance in unsupervised tasks. It is possible that performance could be further improved by the popular technique of rope pulling."}, {"heading": "11 Future Work", "text": "In addition to parameter optimization and multilingual evaluation, the biggest open questions for our approach are the effects of additional data and the expansion of loss function. Neural networks are notoriously data-hungry, suggesting that while we achieve competitive results, it is possible that our model will be scaled well when operated with large corporations, which would probably require the use of techniques such as NCE (Gutmann and Hyv\u00e4rinen, 2010), which have proven highly effective in related tasks such as modeling neural languages (Mnih and Teh, 2012; Vaswani et al., 2013). Second, they aim to maximize mutual information rather than probabilities. It is possible that extending or limiting our loss will produce additional performance glances."}, {"heading": "Acknowledgments", "text": "This work was supported by contracts W911NF-151-0543 and HR0011-15-C-0115 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO)."}], "references": [{"title": "Global optimization of a neural", "author": ["Ralf Kompe"], "venue": null, "citeRegEx": "Kompe.,? \\Q1991\\E", "shortCiteRegEx": "Kompe.", "year": 1991}, {"title": "Phylogenetic grammar induction", "author": ["Taylor Berg-Kirkpatrick", "Dan Klein."], "venue": "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1288\u20131297, Uppsala, Sweden, July.", "citeRegEx": "Berg.Kirkpatrick and Klein.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick and Klein.", "year": 2010}, {"title": "Painless unsupervised learning with features", "author": ["Taylor Berg-Kirkpatrick", "Alexandre Bouchard-C\u00f4t\u00e9", "John DeNero", "Dan Klein."], "venue": "Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Lin-", "citeRegEx": "Berg.Kirkpatrick et al\\.,? 2010", "shortCiteRegEx": "Berg.Kirkpatrick et al\\.", "year": 2010}, {"title": "Labeled grammar induction with minimal supervision", "author": ["Yonatan Bisk", "Christos Christodoulopoulos", "Julia Hockenmaier."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference", "citeRegEx": "Bisk et al\\.,? 2015", "shortCiteRegEx": "Bisk et al\\.", "year": 2015}, {"title": "A Hierarchical Pitman-Yor Process HMM for Unsupervised Part of Speech Induction", "author": ["Phil Blunsom", "Trevor Cohn."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 865\u2013874,", "citeRegEx": "Blunsom and Cohn.,? 2011", "shortCiteRegEx": "Blunsom and Cohn.", "year": 2011}, {"title": "ClassBased n-gram Models of Natural Language", "author": ["Peter F Brown", "Peter V deSouza", "Robert L Mercer", "Vincent J Della Pietra", "Jenifer C Lai."], "venue": "Computational Linguistics, 18.", "citeRegEx": "Brown et al\\.,? 1992", "shortCiteRegEx": "Brown et al\\.", "year": 1992}, {"title": "Two Decades of Unsupervised POS induction: How far have we come", "author": ["Christos Christodoulopoulos", "Sharon Goldwater", "Mark Steedman"], "venue": "In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Christodoulopoulos et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2010}, {"title": "A Bayesian Mixture Model for Part-of-Speech Induction Using Multiple Features", "author": ["Christos Christodoulopoulos", "Sharon Goldwater", "Mark Steedman."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh,", "citeRegEx": "Christodoulopoulos et al\\.,? 2011", "shortCiteRegEx": "Christodoulopoulos et al\\.", "year": 2011}, {"title": "Unsupervised structure prediction with nonparallel multilingual guidance", "author": ["Shay B. Cohen", "Dipanjan Das", "Noah A. Smith."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50\u201361, Edinburgh, Scot-", "citeRegEx": "Cohen et al\\.,? 2011", "shortCiteRegEx": "Cohen et al\\.", "year": 2011}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Dipanjan Das", "Slav Petrov."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 600\u2013609, Portland,", "citeRegEx": "Das and Petrov.,? 2011", "shortCiteRegEx": "Das and Petrov.", "year": 2011}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "author": ["A Dempster", "N Laird", "D Rubin."], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), January.", "citeRegEx": "Dempster et al\\.,? 1977", "shortCiteRegEx": "Dempster et al\\.", "year": 1977}, {"title": "Noisecontrastive estimation: A new estimation principle for unnormalized statistical models", "author": ["Michael Gutmann", "Aapo Hyv\u00e4rinen."], "venue": "International Conference on Artificial Intelligence and Statistics.", "citeRegEx": "Gutmann and Hyv\u00e4rinen.,? 2010", "shortCiteRegEx": "Gutmann and Hyv\u00e4rinen.", "year": 2010}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u2013 1780, November.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "author": ["Matthew J Johnson", "David Duvenaud", "Alexander B Wiltschko", "Sandeep R Datta", "Ryan P Adams."], "venue": "ArXiv e-prints, March.", "citeRegEx": "Johnson et al\\.,? 2016", "shortCiteRegEx": "Johnson et al\\.", "year": 2016}, {"title": "Why doesn\u2019t EM find good HMM POS-taggers", "author": ["Mark Johnson."], "venue": "Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), January.", "citeRegEx": "Johnson.,? 2007", "shortCiteRegEx": "Johnson.", "year": 2007}, {"title": "An empirical exploration of recurrent network architectures", "author": ["Rafal J\u00f3zefowicz", "Wojciech Zaremba", "Ilya Sutskever."], "venue": "Proceedings of the 32nd International Conference on Machine Learning, ICML 2015, Lille, France, 6-11 July 2015, pages 2342\u20132350.", "citeRegEx": "J\u00f3zefowicz et al\\.,? 2015", "shortCiteRegEx": "J\u00f3zefowicz et al\\.", "year": 2015}, {"title": "Exploring the Limits of Language Modeling", "author": ["Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu."], "venue": "ArXiv e-prints, February.", "citeRegEx": "Jozefowicz et al\\.,? 2016", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Character-aware neural language models", "author": ["Yoon Kim", "Yacine Jernite", "David Sontag", "Alexander M Rush."], "venue": "AAAI.", "citeRegEx": "Kim et al\\.,? 2016", "shortCiteRegEx": "Kim et al\\.", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "The International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Ba.,? 2015", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Autoencoding variational bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "The International Conference on Learning Representations (ICLR).", "citeRegEx": "Kingma and Welling.,? 2014", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "Variational dropout and the local reparameterization trick", "author": ["Diederik P Kingma", "Tim Salimans", "Max Welling."], "venue": "Advances in Neural Information Processing Systems 28, pages 2575\u20132583. Curran Associates, Inc.", "citeRegEx": "Kingma et al\\.,? 2015", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Unsupervised pos induction with word embeddings", "author": ["Chu-Cheng Lin", "Waleed Ammar", "Chris Dyer", "Lori Levin."], "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Tech-", "citeRegEx": "Lin et al\\.,? 2015", "shortCiteRegEx": "Lin et al\\.", "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Thang Luong", "Hieu Pham", "Christopher D. Manning."], "venue": "Proceedings of the", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "The Penn Treebank: Annotating Predicate Argument Structure", "author": ["Mitchell P Marcus", "Grace Kim", "Mary Ann Marcinkiewicz", "Robert MacIntyre", "Ann Bies", "Mark Ferguson", "Karen Katz", "Britta Schasberger."], "venue": "ARPA Human Language Technology Workshop.", "citeRegEx": "Marcus et al\\.,? 1994", "shortCiteRegEx": "Marcus et al\\.", "year": 1994}, {"title": "A fast and simple algorithm for training neural probabilistic language models", "author": ["Andriy Mnih", "Yee Whye Teh."], "venue": "Proceedings of the 29th International Conference on Machine Learning (ICML-12), pages 1751\u2013 1758, New York, NY, USA, July.", "citeRegEx": "Mnih and Teh.,? 2012", "shortCiteRegEx": "Mnih and Teh.", "year": 2012}, {"title": "Dropout improves recurrent neural networks for handwriting recognition", "author": ["Vu Pham", "Christopher Bluche", "Th\u00e9odore Kermorvant", "J\u00e9r\u00f4me Louradour."], "venue": "International Conference on Frontiers in Handwriting Recognition (ICFHR), pages 285\u2013290, Sept.", "citeRegEx": "Pham et al\\.,? 2014", "shortCiteRegEx": "Pham et al\\.", "year": 2014}, {"title": "Weighting finite-state transductions with neural context", "author": ["Pushpendre Rastogi", "Ryan Cotterell", "Jason Eisner."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,", "citeRegEx": "Rastogi et al\\.,? 2016", "shortCiteRegEx": "Rastogi et al\\.", "year": 2016}, {"title": "Optimization with em and expectationconjugate-gradient", "author": ["Ruslan Salakhutdinov", "Sam Roweis", "Zoubin Ghahramani."], "venue": "Proceedings, Intl. Conf. on Machine Learning (ICML, pages 672\u2013679.", "citeRegEx": "Salakhutdinov et al\\.,? 2003", "shortCiteRegEx": "Salakhutdinov et al\\.", "year": 2003}, {"title": "Improving word alignment using word similarity", "author": ["Theerawat Songyot", "David Chiang."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1840\u20131845, Doha, Qatar, October.", "citeRegEx": "Songyot and Chiang.,? 2014", "shortCiteRegEx": "Songyot and Chiang.", "year": 2014}, {"title": "Unsupervised dependency parsing without gold part-of-speech tags", "author": ["Valentin I. Spitkovsky", "Hiyan Alshawi", "Angel X. Chang", "Daniel Jurafsky."], "venue": "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1281\u20131290, Ed-", "citeRegEx": "Spitkovsky et al\\.,? 2011", "shortCiteRegEx": "Spitkovsky et al\\.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "JMLR, (1):1929\u20131958, January.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Recurrent memory networks for language modeling", "author": ["Ke Tran", "Arianna Bisazza", "Christof Monz."], "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 321\u2013", "citeRegEx": "Tran et al\\.,? 2016", "shortCiteRegEx": "Tran et al\\.", "year": 2016}, {"title": "Decoding with large-scale neural language models improves translation", "author": ["Ashish Vaswani", "Yinggong Zhao", "Victoria Fossum", "David Chiang."], "venue": "Proceedings of the 2013 Conference on Empirical Methods in", "citeRegEx": "Vaswani et al\\.,? 2013", "shortCiteRegEx": "Vaswani et al\\.", "year": 2013}, {"title": "Hidden Markov Models and the Baum-Welch Algorithm", "author": ["Lloyd R Welch."], "venue": "IEEE Information Theory Society Newsletter, 53(4):1\u201324, December.", "citeRegEx": "Welch.,? 2003", "shortCiteRegEx": "Welch.", "year": 2003}, {"title": "Learning Syntactic Categories Using Paradigmatic Representations of Word Context", "author": ["Mehmet Ali Yatbaz", "Enis Sert", "Deniz Yuret."], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Nat-", "citeRegEx": "Yatbaz et al\\.,? 2012", "shortCiteRegEx": "Yatbaz et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "Inspired by Berg-Kirkpatrick et al. (2010), who bridged the gap between supervised and unsupervised training with features, we bring neural networks to unsupervised learning by providing evidence that even in", "startOffset": 12, "endOffset": 43}, {"referenceID": 9, "context": "Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al.", "startOffset": 71, "endOffset": 146}, {"referenceID": 8, "context": "Using features in unsupervised learning has been a fruitful enterprise (Das and Petrov, 2011; BergKirkpatrick and Klein, 2010; Cohen et al., 2011) and attempts to combine HMMs and Neural Networks date back to 1991 (Bengio et al.", "startOffset": 71, "endOffset": 146}, {"referenceID": 28, "context": "Additionally, similarity metrics derived from word embeddings have also been shown to improve unsupervised word alignment (Songyot and Chiang, 2014).", "startOffset": 122, "endOffset": 148}, {"referenceID": 19, "context": "Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016).", "startOffset": 132, "endOffset": 180}, {"referenceID": 13, "context": "Interest in the interface of graphical models and neural networks has grown recently as new inference procedures have been proposed (Kingma and Welling, 2014; Johnson et al., 2016).", "startOffset": 132, "endOffset": 180}, {"referenceID": 12, "context": "Additionally, Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) based Recurrent Neural Networks (RNNs), allow for modeling unbounded context with far fewer parameters than naive one-hot feature encodings.", "startOffset": 44, "endOffset": 78}, {"referenceID": 10, "context": "We focus on cases when the posterior is tractable and we can use Generalized EM (Dempster et al., 1977) to estimate \u03b8.", "startOffset": 80, "endOffset": 103}, {"referenceID": 27, "context": "\u2202 \u2202\u03b8 ln p(x, z | \u03b8) is easy to evaluate, we can perform direct marginal likelihood optimization (Salakhutdinov et al., 2003).", "startOffset": 96, "endOffset": 124}, {"referenceID": 23, "context": "In English, the Penn Treebank (Marcus et al., 1994) distinguishes 36 categories and punctuation.", "startOffset": 30, "endOffset": 51}, {"referenceID": 29, "context": "Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings.", "startOffset": 80, "endOffset": 124}, {"referenceID": 3, "context": "Two natural applications of induced tags are as the basis for grammar induction (Spitkovsky et al., 2011; Bisk et al., 2015) or to provide a syntactically informed, though unsupervised, source of word embeddings.", "startOffset": 80, "endOffset": 124}, {"referenceID": 33, "context": "Welch (Welch, 2003).", "startOffset": 6, "endOffset": 19}, {"referenceID": 14, "context": "Training an HMM with EM is highly non-convex and likely to get stuck in local optima (Johnson, 2007).", "startOffset": 85, "endOffset": 100}, {"referenceID": 4, "context": "Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011).", "startOffset": 85, "endOffset": 109}, {"referenceID": 4, "context": "Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states.", "startOffset": 86, "endOffset": 135}, {"referenceID": 4, "context": "Despite this, sophisticated Bayesian smoothing leads to state-of-the-art performance (Blunsom and Cohn, 2011). Blunsom and Cohn (2011) further extend the HMM by augmenting its emission distributions with character models to capture morphological information and a tri-gram transition matrix which conditions on the previous two states. Recently, Lin et al. (2015) extended several models Algorithm 1 Baum-Welch Algorithm", "startOffset": 86, "endOffset": 364}, {"referenceID": 26, "context": "There has also been recent work on by Rastogi et al. (2016) on neuralizing Finite-State Transducers.", "startOffset": 38, "endOffset": 60}, {"referenceID": 5, "context": "We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al.", "startOffset": 26, "endOffset": 46}, {"referenceID": 5, "context": "We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al.", "startOffset": 27, "endOffset": 156}, {"referenceID": 5, "context": "We include Brown clusters (Brown et al., 1992) as a baseline and two clustering techniques as stateof-the-art comparisons: Christodoulopoulos et al. (2011) and Yatbaz et al. (2012).", "startOffset": 27, "endOffset": 181}, {"referenceID": 5, "context": "Of particular interest to us is the work of Brown et al. (1992). Brown clusters group word types through a greedy agglomerative clustering according to their mutual information across the corpus based on bigram probabilities.", "startOffset": 44, "endOffset": 64}, {"referenceID": 27, "context": "We therefore employ Direct Marginal Likelihood (DML) (Salakhutdinov et al., 2003) to optimize the model\u2019s parameters.", "startOffset": 53, "endOffset": 81}, {"referenceID": 6, "context": "Christodoulopoulos et al. (2010) found VM is to be the most informative and consistent metric, in part because it is agnostic to the number of induced tags.", "startOffset": 0, "endOffset": 33}, {"referenceID": 31, "context": "We used a three layer LSTM as it worked well for (Tran et al., 2016), and we applied dropout (Srivastava et al.", "startOffset": 49, "endOffset": 68}, {"referenceID": 30, "context": ", 2016), and we applied dropout (Srivastava et al., 2014) over the vertical connections of the LSTMs (Pham et al.", "startOffset": 32, "endOffset": 57}, {"referenceID": 25, "context": ", 2014) over the vertical connections of the LSTMs (Pham et al., 2014) with a rate of 0.", "startOffset": 51, "endOffset": 70}, {"referenceID": 18, "context": "All optimization was done using Adam (Kingma and Ba, 2015) with default hyper-parameters.", "startOffset": 37, "endOffset": 58}, {"referenceID": 15, "context": "2 Additionally, weights for the LSTMs are initialized using N (0, 1/2n), where n is the number of hidden units, and the bias of the forget gate is set to 1, as suggested by J\u00f3zefowicz et al. (2015). We present some parameter and modeling ablation analysis in \u00a710.", "startOffset": 173, "endOffset": 198}, {"referenceID": 34, "context": "Finally, the newer work of Yatbaz et al. (2012) outperforms our approach.", "startOffset": 27, "endOffset": 48}, {"referenceID": 22, "context": "This is consistent with the common wisdom that unlike supervised learning (Luong et al., 2015), weight initialization is important to achieve good performance on unsupervised tasks.", "startOffset": 74, "endOffset": 94}, {"referenceID": 20, "context": "To avoid tuning the dropout rate, future work might investigate the effect of variational dropout (Kingma et al., 2015) in unsupervised learning.", "startOffset": 98, "endOffset": 119}, {"referenceID": 11, "context": "This would likely require the use of techniques like NCE (Gutmann and Hyv\u00e4rinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al.", "startOffset": 57, "endOffset": 86}, {"referenceID": 24, "context": "This would likely require the use of techniques like NCE (Gutmann and Hyv\u00e4rinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al., 2013).", "startOffset": 179, "endOffset": 221}, {"referenceID": 32, "context": "This would likely require the use of techniques like NCE (Gutmann and Hyv\u00e4rinen, 2010) which have been shown to be highly effective in related tasks like neural language modeling (Mnih and Teh, 2012; Vaswani et al., 2013).", "startOffset": 179, "endOffset": 221}], "year": 2016, "abstractText": "In this work, we present the first results for neuralizing an Unsupervised Hidden Markov Model. We evaluate our approach on tag induction. Our approach outperforms existing generative models and is competitive with the state-of-the-art though with a simpler model easily extended to include additional context.", "creator": "LaTeX with hyperref package"}}}