{"id": "1609.09162", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Universum Learning for Multiclass SVM", "abstract": "We introduce Universum learning for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM). We also propose a span bound for MU-SVM that can be used for model selection thereby avoiding resampling. Empirical results demonstrate the effectiveness of MU-SVM and the proposed bound.", "histories": [["v1", "Thu, 29 Sep 2016 00:43:03 GMT  (728kb,D)", "http://arxiv.org/abs/1609.09162v1", "14 pages, 12 figures"]], "COMMENTS": "14 pages, 12 figures", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["sauptik dhar", "naveen ramakrishnan", "vladimir cherkassky", "mohak shah"], "accepted": false, "id": "1609.09162"}, "pdf": {"name": "1609.09162.pdf", "metadata": {"source": "CRF", "title": "Universum Learning for Multiclass SVM", "authors": ["Sauptik Dhar", "Naveen Ramakrishnan", "Vladimir Cherkassky", "Mohak Shah"], "emails": ["mohak.shah}@us.bosch.com", "cherk001@umn.edu"], "sections": [{"heading": "1 Introduction", "text": "Many machine learning applications involve analyzing the sparse high-dimensional data, where the number of input functions is greater than the number of real data samples. Such high-dimensional data sets present new challenges for most learning problems. (\"two\" - class-specific) classification problems have shown that many practical applications involve discrimination for more than two categories. Typical examples include speech recognition, object recognition from images, predictive health management, etc. [15] In order to integrate a priority knowledge (in the form of universe data) for such applications, we need to expand universe learning for multiclass problems. In this paper, we mainly focus on the formulation of universe learning for multiclass SVM with balanced settings with equal miscsification costs."}, {"heading": "3 Multiclass Universum SVM", "text": "The idea of universe learning was introduced by Vapnik = > Sensitive Samples from the Universe (1, 2]. The Universe contains data belonging to the same application domain as the training data. However, these samples are known to belong to neither of the two classes. In fact, this idea can also be extended to multi-class problems. For multi-class problems in addition to the designated training data, we are also faced with a set of unlabeled examples from the Universe. However, the Universe Samples are known that do not belong to any classes in the training data. For example, if the goal of learning is to create handwritten numbers 0, 2, 9; one can add additional \"knowledge\" in the form of handwritten letters A, B, C,..."}, {"heading": "3.2 Computational Implementation of MU-SVM", "text": "In this section we discuss the current conversion of the MU-SVM formulation into (2). Following [22] we create artificial samples for each universe sample (x *), which are assigned to all classes, i.e. (x * j, y * j = 1),.., (x * j, y * j = L). For simplicity we overload the variables as shown below: xi = {xi i = 1.. n \u2212 mL; j = 1. mLeil = {eil i = 1. mL; j = 1.. mL (universe samples) yi = {yi i i = 1. \u2212 mxi j = n + 1. mL; j = 1.. mLeil = {eil i = 1.. n; l = 1.. l; l = 1.."}, {"heading": "3.3 Model Selection", "text": "As shown in (5), the current MU-SVM algorithm has four selectable parameters: C, C, kernel parameters and \u2206. So in practice, the Multiclass SVM can deliver better results than MUSVM, simply because there is an inherently simpler model selection. Successful practical application of the proposed MU-SVM parameters depends greatly on the optimal matching of the model parameters. This step suggests applying a simplified strategy for selecting the parameters (previously used in [5, 23]), which mainly involves two steps. First, perform an optimal matching of the C and kernel parameters for the multiclass SVM classifiers. This step equivalent performs the model selection for the specific training samples in the MU-SVM formulation (2).Step b. Second, adjust the parameters while keeping C and kernel parameters fixed (as in Step)."}, {"heading": "4.1 Comparison between Multiclass SVM vs. U-SVM", "text": "Our first set of experiments uses the GTSRB dataset. Initial experiments indicate that linear parameterization is optimal for this dataset: \"Here, a linear systematic is optimal for these datasets; therefore, only a linear kernel was used.\" Here, model selection is performed across the range of parameters, C = [10 \u2212 4,.. 103], C * / C = nmL = 0.2 and 0. \"(SVM = 0.01, 0.05, 0.1] with stratified 5-fold cross-validation [26]. Performance comparisons between SVM and U-SVM for the different types of universe: characters\" no-entry \"and\" roadworks \"are shown in Table 2. The table shows the average tester error = 1nT class i = 1 [ytesti = y-testi] over 10 random formation / test partition of data in similar proportions as shown in Table 1."}, {"heading": "5 Conclusions", "text": "We introduced universe learning for multi-class problems and provided a new universe-based formulation for multi-class problems (MU-SVM). This formulation is reduced to the classic multi-class SVM formulation in the absence of universe samples and can use standard SVM solvers. We also proposed a novel range for MU-SVM that can be used to perform efficient model selection. We demonstrated empirically the effectiveness of the proposed formulation as well as the boundary for real data sets. Furthermore, we provided insights into the underlying behavior of universe learning and its dependence on the choice of universe samples using the proposed \"histogram-of-projections\" method. Modelling of results using l.o.o strategy was prohibitively slow and therefore could not be reported in this paper."}, {"heading": "A Proofs", "text": "The references cited in this document follow the numbering used in the main document."}, {"heading": "A.1 Proof of Theorem 1", "text": "The proof follows lines similar to those in [21]. As discussed in Section 3.3 above, the solution with a single output for U-SVM follows with the elapsed sample: max \u03b1W (\u03b1) = \u2212 < l = p = p = p = p = p = p = p = p = p = p = p = p = p = p (p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p = p p = p = p p p p = p = p = p = p p p = p p p p p p = p = p p = p p p p p p p"}, {"heading": "A.2 Proof of Corollary 1", "text": "The range is defined as: S2t = min \u03b2jl = i, j (\u2211 l \u03b2il\u03b2jl) K (xi, xj) (16) s.t. \u03b2tl = \u03b1 l = 1,.., L \u2211 l \u03b2il = 0; B (i, j).SV1 = min \u03b2 (\u03b1tl\u03b1tl) K (xt, xt) + 2 \u00b2. \u2212 \u2212 \u2212 A \u03b2 = min \u03b2 \u2212 \u2212 \u2212 \u2212 column > t [K (xt, xt).IL]: t (i, j) 6 = t (\u2211 l \u03b2il\u03b2jl) K (xi, xj) s.t. (I | SV1 \u2212 t) A \u03b2 = min \u03b2 \u2212 column > t [K (xt, xt).i6 = t (t).l \u03b2il\u03b2jl).- (H (xt, xi)."}, {"heading": "A.3 Proof of Corollary 2", "text": "The detection consists of three steps and depends mainly on the contribution of a sample to the \"leave-one-out error.\" - Placing such a sample does not change the original solution (5), and therefore does not contribute to an error. - Second, for a sample (xt, yt) with t-SV1 and t-T (training set) sentence 1. For a \"leave-one-out error\" applies (\u03b1tlgk) > \"I-SV t-l-tilK (xi, xt) \u2264 0\" \u03b1 > t [(H-1) tt] \u2212 1\u03b1t \"I-SV-l-ilK (xi, xt). [Von (14)] - Finally, for a sample (xt, yt) with t-SV2 and t-T (training) we add the\" leave-one-out error. \""}, {"heading": "B Additional Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1 Histogram of projections", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "B.1.1 GTSRB dataset", "text": "Figures 9 and 10 provide the histograms and frequency diagrams for SVM / MU-SVM models for the GTSRB dataset using the character \"Road Construction\" (as Universe). Figure 9 gives a high separability of the optimal SVM model for the training samples. Here, the universal samples tend to the positive side of the decision limit of the character \"80\" (see Figure 9 (c)) and are therefore predominantly classified as characters \"80\" (see Figure 9 (d)). From Figure 10 (a) - (c), the application of the MU-SVM model preserves the separability of the training samples and additionally reduces the dispersion of the universe samples. In such a model, the uncertainty resulting from the universe samples is uniform across all classes, i.e. the characters \"30,\" \"70\" and \"80\" (see Figure 10 (d))."}, {"heading": "B.1.2 ABCDETC dataset", "text": "Here we present the histograms and frequency diagrams for SVM / MU-SVM models for ABCDETC datasets using the letter \"i\" (as universe).Here, the SVM model leads to a wider distribution of universe samples (see Figure 11 (a) - (d)) and thus to a more random prediction of the majority of universe samples (see Fig. 12).This results in a more general model compared to SVM (see Table 2) B.2 Experiments of varying universe sizes This set of experiments shows how the generalization performance of MU-SVM is influenced by the number of universe samples for the GTSRB dataset."}], "references": [{"title": "Estimation of Dependences Based on Empirical Data (Information Science and Statistics)", "author": ["V. Vapnik"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2006}, {"title": "An analysis of inference with the universum", "author": ["F. Sinz", "O. Chapelle", "A. Agarwal", "B. Sch\u00f6lkopf"], "venue": "Advances in neural information processing systems 20. NY, USA: Curran, Sep. 2008, pp. 1369\u20131376.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Selecting informative universum sample for semi-supervised learning.", "author": ["S. Chen", "C. Zhang"], "venue": "in IJCAI,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Development and evaluation of cost-sensitive universum-svm", "author": ["S. Dhar", "V. Cherkassky"], "venue": "Cybernetics, IEEE Transactions on, vol. 45, no. 4, pp. 806\u2013818, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Weighted twin support vector machine with universum", "author": ["S. Lu", "L. Tong"], "venue": "Advances in Computer Science: an International Journal, vol. 3, no. 2, pp. 17\u201323, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "A nonparallel support vector machine for a classification problem with universum learning", "author": ["Z. Qi", "Y. Tian", "Y. Shi"], "venue": "Journal of Computational and Applied Mathematics, vol. 263, pp. 288\u2013298, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Uboost: Boosting with the universum", "author": ["C. Shen", "P. Wang", "F. Shen", "H. Wang"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 34, no. 4, pp. 825\u2013832, 2012.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-view learning with universum", "author": ["Z. Wang", "Y. Zhu", "W. Liu", "Z. Chen", "D. Gao"], "venue": "Knowledge-Based Systems, vol. 70, pp. 376\u2013391, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Semi-supervised classification with universum.", "author": ["D. Zhang", "J. Wang", "F. Wang", "C. Zhang"], "venue": "in SDM. SIAM,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2008}, {"title": "Least squares twin support vector machine with universum data for classification", "author": ["Y. Xu", "M. Chen", "G. Li"], "venue": "International Journal of Systems Science, pp. 1\u20139, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "\u03bd-twin support vector machine with universum data for classification", "author": ["Y. Xu", "M. Chen", "Z. Yang", "G. Li"], "venue": "Applied Intelligence, vol. 44, no. 4, pp. 956\u2013968, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Improved multi-kernel classification machine with nystr\u00f6m approximation technique and universum data", "author": ["C. Zhu"], "venue": "Neurocomputing, vol. 175, pp. 610\u2013634, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Double-fold localized multiple matrix learning machine with universum", "author": ["\u2014\u2014"], "venue": "Pattern Analysis and Applications, pp. 1\u201328.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 0}, {"title": "Learning from Data: Concepts, Theory, and Methods", "author": ["V. Cherkassky", "F.M. Mulier"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2007}, {"title": "The Elements of Statistical Learning, ser. Springer Series in Statistics", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "A comparison of methods for multiclass support vector machines", "author": ["C. Hsu", "C. Lin"], "venue": "Neural Networks, IEEE Transactions on, vol. 13, no. 2, pp. 415\u2013425, 2002.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2002}, {"title": "Large margin dags for multiclass classification.", "author": ["J.C. Platt", "N. Cristianini", "J. Shawe-Taylor"], "venue": "in NIPS, vol", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1999}, {"title": "A priori knowledge from non-examples", "author": ["F. Sinz"], "venue": "Ph.D. dissertation, Mar 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "On the learnability and design of output codes for multiclass problems", "author": ["K. Crammer", "Y. Singer"], "venue": "Machine learning, vol. 47, no. 2-3, pp. 201\u2013233, 2002.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-class support vector machines", "author": ["J. Weston", "C. Watkins"], "venue": "Citeseer, Tech. Rep., 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "Inference with the universum", "author": ["J. Weston", "R. Collobert", "F. Sinz", "L. Bottou", "V. Vapnik"], "venue": "Proceedings of the 23rd international conference on Machine learning. ACM, 2006, pp. 1009\u20131016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2006}, {"title": "Practical conditions for effectiveness of the universum learning", "author": ["V. Cherkassky", "S. Dhar", "W. Dai"], "venue": "Neural Networks, IEEE Transactions on, vol. 22, no. 8, pp. 1241\u20131255, 2011.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Bounds on error expectation for support vector machines", "author": ["V. Vapnik", "O. Chapelle"], "venue": "Neural computation, vol. 12, no. 9, pp. 2013\u20132036, 2000.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition", "author": ["J. Stallkamp", "M. Schlipsing", "J. Salmen", "C. Igel"], "venue": "Neural Networks, pp. \u2013, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Evaluating learning algorithms: a classification perspective", "author": ["N. Japkowicz", "M. Shah"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Abstract We introduce Universum learning [1], [2] for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM).", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 2, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 3, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 4, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 5, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 6, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 7, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 8, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 9, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 10, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 11, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 12, "context": "Recent studies have shown Universum learning to be particularly effective for such high-dimensional low sample size data settings [3\u201314].", "startOffset": 130, "endOffset": 136}, {"referenceID": 13, "context": "Typical examples include, speech recognition, object recognition from images, prognostic health management etc [15,16].", "startOffset": 111, "endOffset": 118}, {"referenceID": 14, "context": "Typical examples include, speech recognition, object recognition from images, prognostic health management etc [15,16].", "startOffset": 111, "endOffset": 118}, {"referenceID": 7, "context": "Typically these methods follow two basic approaches (see [9, 17] for more details).", "startOffset": 57, "endOffset": 64}, {"referenceID": 15, "context": "Typically these methods follow two basic approaches (see [9, 17] for more details).", "startOffset": 57, "endOffset": 64}, {"referenceID": 16, "context": ", one-vs-one, one-vs-all, directed acyclic graph SVM [18].", "startOffset": 53, "endOffset": 57}, {"referenceID": 2, "context": "Previous works, such as [4, 19] which follow the ensemble based setting, focus on the binary universum learning paradigm and only provide some hints for their extensions to the multiclass problems.", "startOffset": 24, "endOffset": 31}, {"referenceID": 17, "context": "Previous works, such as [4, 19] which follow the ensemble based setting, focus on the binary universum learning paradigm and only provide some hints for their extensions to the multiclass problems.", "startOffset": 24, "endOffset": 31}, {"referenceID": 18, "context": "An alternative to the ensemble based setting is the direct approach, where the entire multiclass problem is solved through a single larger optimization formulation (see [1, 20, 21]).", "startOffset": 169, "endOffset": 180}, {"referenceID": 19, "context": "An alternative to the ensemble based setting is the direct approach, where the entire multiclass problem is solved through a single larger optimization formulation (see [1, 20, 21]).", "startOffset": 169, "endOffset": 180}, {"referenceID": 18, "context": "In this paper we develop and discuss MU-SVM, a direct approach for universum learning following the Crammer & Singer\u2019s (C&S) multiclass SVM formulation [20].", "startOffset": 152, "endOffset": 156}, {"referenceID": 18, "context": "Section 2 describes the widely used multiclass SVM formulation in [20].", "startOffset": 66, "endOffset": 70}, {"referenceID": 18, "context": "This section provides a brief description of the multiclass SVM formulation following Crammer & Singer (C&S) [20].", "startOffset": 109, "endOffset": 113}, {"referenceID": 18, "context": "The C&S multiclass SVM [20] is a widely used formulation which generalizes the concept of large margin classifier for multiclass problems.", "startOffset": 23, "endOffset": 27}, {"referenceID": 18, "context": "This is accomplished by introducing a non-linear kernel function K(xi,xj) = \u3008\u03c6(xi) \u00b7 \u03c6(xj)\u3009 that implicitly captures the non-linear mapping of the data x\u2192 \u03c6(x) (see [20] for more details).", "startOffset": 165, "endOffset": 169}, {"referenceID": 0, "context": "The idea of Universum learning was introduced by Vapnik [1, 2] to incorporate a priori knowledge about admissible data samples.", "startOffset": 56, "endOffset": 62}, {"referenceID": 0, "context": "This argument follows from [2, 22], where the universum samples lying close to the decision boundaries are more likely to falsify the classifier.", "startOffset": 27, "endOffset": 34}, {"referenceID": 20, "context": "This argument follows from [2, 22], where the universum samples lying close to the decision boundaries are more likely to falsify the classifier.", "startOffset": 27, "endOffset": 34}, {"referenceID": 20, "context": "Note that, this idea of using a \u2206 - insensitive loss for Universum samples has been previously introduced in [22] for binary classification.", "startOffset": 109, "endOffset": 113}, {"referenceID": 20, "context": "However, different from [22], here the \u2206 - insensitive loss is introduced for the decision functions of all the classes i.", "startOffset": 24, "endOffset": 28}, {"referenceID": 20, "context": "Following [22], for each universum sample (x\u2217) we create artificial samples belonging to all the classes, i.", "startOffset": 10, "endOffset": 14}, {"referenceID": 3, "context": "This paper proposes to adopt a simplified strategy (previously used in [5, 23]) for model selection which mainly involves two steps, Step a.", "startOffset": 71, "endOffset": 78}, {"referenceID": 21, "context": "This paper proposes to adopt a simplified strategy (previously used in [5, 23]) for model selection which mainly involves two steps, Step a.", "startOffset": 71, "endOffset": 78}, {"referenceID": 22, "context": "In this paper we follow a very similar strategy as used in [24], and derive the new l.", "startOffset": 59, "endOffset": 63}, {"referenceID": 22, "context": "o bound for binary SVM in [24].", "startOffset": 26, "endOffset": 30}, {"referenceID": 22, "context": "Note that, this equality is very similar to the result in [24] obtained for binary SVM.", "startOffset": 58, "endOffset": 62}, {"referenceID": 22, "context": "Same as [24] we refer to St as the (constrained) span of the Type 1 support vectors.", "startOffset": 8, "endOffset": 12}, {"referenceID": 23, "context": "Our empirical results mainly use two real life datasets: German Traffic Sign Recognition Benchmark (GTSRB) dataset [25] : The goal here is to identify the traffic signs \u201830\u2019,\u201870\u2019 and \u201880\u2019 (shown in Fig.", "startOffset": 115, "endOffset": 119}, {"referenceID": 3, "context": "Here, the sample images are represented by their histogram of gradient (HOG) features (following [5, 8]).", "startOffset": 97, "endOffset": 103}, {"referenceID": 6, "context": "Here, the sample images are represented by their histogram of gradient (HOG) features (following [5, 8]).", "startOffset": 97, "endOffset": 103}, {"referenceID": 20, "context": "Real-life ABCDETC dataset [22]: This is a handwritten digit recognition dataset, where in addition to the digits \u20180-9\u2019 we are also provided with the images of the uppercase, lowercase handwritten letters and some additional special symbols.", "startOffset": 26, "endOffset": 30}, {"referenceID": 24, "context": "1] using stratified 5-Fold cross validation [26].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "For better understanding of the MU-SVM modeling results we adopt the technique of \u2018histogram of projections\u2019 originally introduced for binary classification [23, 27].", "startOffset": 157, "endOffset": 165}, {"referenceID": 0, "context": "[2] V.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[3] F.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[4] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "[5] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[6] S.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "[7] Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[8] C.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[9] Z.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[10] D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[11] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12] Y.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[14] \u2014\u2014, \u201cDouble-fold localized multiple matrix learning machine with universum,\u201d Pattern Analysis and Applications, pp.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[15] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[16] T.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[17] C.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[18] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[19] F.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[20] K.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "[22] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 21, "context": "[23] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 22, "context": "[24] V.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[25] J.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "[26] N.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "1 Proof of Theorem 1 The proof follows similar lines as in [21].", "startOffset": 59, "endOffset": 63}], "year": 2016, "abstractText": "We introduce Universum learning [1], [2] for multiclass problems and propose a novel formulation for multiclass universum SVM (MU-SVM). We also propose a span bound for MU-SVM that can be used for model selection thereby avoiding resampling. Empirical results demonstrate the effectiveness of MU-SVM and the proposed bound.", "creator": "LaTeX with hyperref package"}}}