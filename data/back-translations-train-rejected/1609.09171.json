{"id": "1609.09171", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Empirical Evaluation of RNN Architectures on Sentence Classification Task", "abstract": "Recurrent Neural Networks have achieved state-of-the-art results for many problems in NLP and two most popular RNN architectures are Tail Model and Pooling Model. In this paper, a hybrid architecture is proposed and we present the first empirical study using LSTMs to compare performance of the three RNN structures on sentence classification task. Experimental results show that the Tail Model and Hybrid Model consistently get a better performance over Pooling Model, and Hybrid Model is comparable with Tail Model.", "histories": [["v1", "Thu, 29 Sep 2016 01:53:08 GMT  (170kb)", "http://arxiv.org/abs/1609.09171v1", null], ["v2", "Sat, 8 Oct 2016 15:16:57 GMT  (195kb)", "http://arxiv.org/abs/1609.09171v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["lei shen", "junlin zhang"], "accepted": false, "id": "1609.09171"}, "pdf": {"name": "1609.09171.pdf", "metadata": {"source": "CRF", "title": "Empirical Evaluation of RNN Architectures on Sentence Classification Task", "authors": ["Lei Shen", "Junlin Zhang"], "emails": ["lorashen@126.com,", "zhangjlh@chanjet.com"], "sections": [{"heading": null, "text": "Keywords: RNN \u00b7 LSTM \u00b7 Sentence Classification"}, {"heading": "1 Introduction", "text": "Recurrent Neural Networks (RNNs), especially Long Short-Term Memories (LSTMs) [9], are good at modelling sequential data of varying lengths and have achieved state-of-the-art results on many problems of natural language processing, such as neural machine translation, question and text classification [2,3,12,14,19,20]. RNN is now the most popular method in sentence classification, which is also a typical NLP task. There are two widely used RNN structures in NLP tasks, and we call them separately \"tail model\" and \"pooling model.\" However, there is no work that focuses on comparing the performance of these different network architectures. This paper presents the first empirical study that uses LSTMs to evaluate different RNN structures in sentence classification tasks. We also present a hybrid architecture that combines tail model with \"pooling model.\" Experimental results show that \"model, tail and hybrid are more consistent.\""}, {"heading": "2 Related Work", "text": "A recursive neural network [6] is introduced to process arbitrary length sequences, and is commonly used in NLP tasks nowadays. As simple recursive networks are generally difficult because gradients either disappear or explode [1], various improvements to the basic architecture have been proposed, and the Long Short Term Memory Network is perhaps the most successful. LSTM was originally introduced by Hochreiter and Schmidhuber [10], and in recent years some types of simplified LSTM have also been introduced, such as gated recurrent units [3]. LSTM is used in our comparison because LSTM is similar to GRU and surpasses both basic RNN [4]. We use the LSTM structure introduced by Gers et al. [7] for comparison, since Greff et al. [9] evaluate the LSTM variants and the model of Gers et al. are comparable to other variants."}, {"heading": "3 Model", "text": "There are two commonly used RNN structures in NLP tasks and we call them \"tail model\" and \"pooling model\" separately [15,19,20]. Both use RNN to provide features for the fully connected layer in classification tasks. On the other hand, we consider the two different structures to be complementary. Thus, the third RNN structure is proposed in this paper and it is referred to as the \"hybrid model\" in the following part of the paper. Figure 1 demonstrates the mesh structure of the \"tail BLSTM model,\" in which the hidden layer represents the hidden hidden hidden hidden hidden hidden hidden states in the front RNN and backwards RNN."}, {"heading": "4 Datasets and Experimental Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Dataset", "text": "To evaluate the performance of the above-mentioned three RNN structures, we are designing experiments with the following sets of record classification data: MR: The record of film reviews with one set per review. The task is to detect positive / negative reviews [17].SST-1: Stanford Sentiment Treebank - an extension of Movie Review, but with fine-grained labels (very positive, neutral, negative, very negative).The record is also divided into train / development / test sets and re-labeled by Socher et al. [18]. SST-2: Same data as SST-1, but with binary labels only. All neutral reviews are deleted. The record is also divided into train / development / test set.Subj: Subjectivity dataset al. The task is to classify records into two categories: subjective and objective [16].TREC: test datasets (the Positivity dataset, the Positivity dataset, the Positivity dataset, the Positivity dataset, the Personnel Classification Question)."}, {"heading": "4.2 Experiment Design", "text": "We design two groups of experiments: one based on the BLSTM model and the other based on the LSTM model. And the \"pooling model\" and the \"hybrid model\" each contain two experiments to compare the performance of max pooling and the mean pooling strategy. Thus, for each data set, there are five experiments in each group to evaluate the performance of these RNN architectures."}, {"heading": "4.3 Parameters and Setup", "text": "We use word2vec vectors trained on 100 billion words of Google News as Word embedding, which can be accessed publicly; the dimensionality of the vectors is 300 and the words that are not in the vectors are determined randomly; we perform the embedding statically through all experiments to avoid the influence of the word embedding parameters in comparison; the initial weights of the network parameters are drawn from a uniform distribution with a standard deviation of 0.08 and the default risk is set to 0.5. Instead of achieving the best performance for a single model, our focus is on comparing the performance of the model in the sentence classification task under comparable conditions. To make the parameters of the RNN models comparable, we set the unidirectional LSTM unit size to 300 and the bidirectional LSTM unit size to 185, thus setting the parameter size for both unidirectional and bidirectional models to 100000 * STM."}, {"heading": "5 Results and Analysis", "text": "The experimental results are listed in Table 2 (LSTM-based models) and Table 3 (BLSTM-based models). Similar performance improvements can also be observed in BLSTM models on all data sets (Table 5). The following reason might explain the test results: Every word in the sentence can vote for the final classification in the \"pooling model,\" and this could weaken the obvious function of the feature in sentence classification. The above analysis of the test results suggests that we should give priority to the \"tail model\" in sentence classification as it is simpler than \"hybrid model.\" Although the performance gains of several experiments are not large enough to show an overwhelming advantage in sentence classification, we believe that the BLSTM model has no advantage due to the diversity of data sets and models in experiments."}, {"heading": "6 Conclusion", "text": "In this thesis we present the first empirical study using LSTMs to evaluate the performance of different RNN structures on sentence classification task. We also present a hybrid architecture that combines \"tail model\" with \"pooling model\" in this thesis. Experimental results show that the \"tail model\" and \"hybrid model\" consistently perform better over \"pooling model,\" and \"hybrid model\" is comparable to \"tail model.\" References 1. Bengio, Y., Simard, P.: Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5 (2): 157-166 (1994) 2. Cho, K. van Merrienboer, B., Bahdanau, D., Bengio, Y.: On the properties of neural machine translation: Encoder-decoder approaches."}], "references": [{"title": "Learning long-term dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE Transactions on Neural Networks, 5(2):157-166", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1994}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches", "author": ["K. Cho", "B. van Merrienboer", "D. Bahdanau", "Y. Bengio"], "venue": "Proceedings of Workshop on SSST", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "Proceedings of EMNLP", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "Proceedings of NIPS", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive Subgradient Methods for Online Learning and Stochastic Optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, 12:2121\u20132159", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Finding structure in time", "author": ["J.L. Elman"], "venue": "Cognitive Science, 14(2):179--211", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning to forget: Continual prediction with LSTM", "author": ["A. Gers", "J. Schmidhuber", "F. Xummins"], "venue": "Proceedings of ICANN", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1999}, {"title": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, 18(5-6): 602-610", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "LSTM: A search space odyssey", "author": ["K. Greff", "R. Srivastava", "J. Koutnik", "B. Steunebrink", "J. Schmidhuber"], "venue": "arXiv:1503.04069", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Computation 9(8): 1735-1780", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1997}, {"title": "Mining and Summarizing Customer Reviews", "author": ["M. Hu", "B. Liu"], "venue": "Proceedings of ACM SIGKDD", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2004}, {"title": "A neural network for factoid question answering over paragraphs", "author": ["M. Iyyer", "J. Boyd-Graber", "L. Claudino", "R. Socher", "H. Daume III"], "venue": "Proceedings of EMNLP", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning question classifiers", "author": ["X. Li", "D. Roth"], "venue": "Proceedings of ACL", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2002}, {"title": "Recurrent neural network for text classification with multi-task learning", "author": ["P. Liu", "X. Qiu", "X. Huang"], "venue": "Proceedings of IJCAI", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of ACL", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "author": ["B. Pang", "L. Lee"], "venue": "Proceedings of ACL", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2005}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["R. Socher", "A. Perelygin", "J. Wu", "J. Chuang", "C. Manning", "A. Ng", "C. Potts"], "venue": "Proceedings of EMNLP", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Document modeling with gated recurrent neural network for sentiment classification", "author": ["D. Tang", "B. Qin", "T. Liu"], "venue": "Proceedings of EMNLP", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "A Long short-term memory model for answer sentence selection in question answering", "author": ["D. Wang", "E. Nyberg"], "venue": "Proceedings of ACL", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 8, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 84, "endOffset": 87}, {"referenceID": 1, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 2, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 11, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 13, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 17, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 18, "context": "Recurrent Neural Networks (RNNs), especially those Long Short-Term Memories (LSTMs) [9], are good at modeling varying length sequential data and have achieved state-of-the-art results for many problems in natural language processing, such as neural machine translation, question answering and text classification [2,3,12,14,19,20].", "startOffset": 313, "endOffset": 330}, {"referenceID": 5, "context": "A recurrent neural network [6] is introduced to process arbitrary length sequence and widely used in nowadays NLP tasks.", "startOffset": 27, "endOffset": 30}, {"referenceID": 0, "context": "train because the gradients will either vanish or explode [1], various improvements to the basic architecture were proposed and the Long Short Term Memory network is perhaps the most successful one.", "startOffset": 58, "endOffset": 61}, {"referenceID": 9, "context": "LSTM was originally introduced by Hochreiter and Schmidhuber [10] and in recent years some kinds of simplified LSTM were also introduced such as Gated Recurrent Units [3].", "startOffset": 61, "endOffset": 65}, {"referenceID": 2, "context": "LSTM was originally introduced by Hochreiter and Schmidhuber [10] and in recent years some kinds of simplified LSTM were also introduced such as Gated Recurrent Units [3].", "startOffset": 167, "endOffset": 170}, {"referenceID": 3, "context": "LSTM is used in our comparison since LSTM is comparable to GRU and the two both outperform basic RNN [4].", "startOffset": 101, "endOffset": 104}, {"referenceID": 6, "context": "[7] for comparison, since Greff et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] evaluate the LSTM variants and find the model by Gers et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "There exist two commonly used RNN structures in NLP tasks and we call them \u201cTail Model\u201d and \u201cPooling Model\u201d separately [15,19,20].", "startOffset": 119, "endOffset": 129}, {"referenceID": 18, "context": "There exist two commonly used RNN structures in NLP tasks and we call them \u201cTail Model\u201d and \u201cPooling Model\u201d separately [15,19,20].", "startOffset": 119, "endOffset": 129}, {"referenceID": 15, "context": "The task is to detect positive/negative reviews [17].", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "The task is to classify sentences into two categories: subjective and objective [16].", "startOffset": 80, "endOffset": 84}, {"referenceID": 12, "context": "The dataset is split into train/dev/test set [13].", "startOffset": 45, "endOffset": 49}, {"referenceID": 10, "context": "The task is to predict positive/negative reviews [11].", "startOffset": 49, "endOffset": 53}, {"referenceID": 4, "context": "The Cross-Entropy criterion is used as loss function and stochastic gradient descent with Adagrad [5] is used for optimization.", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "Gradients are computed through full BPTT for LSTM [8].", "startOffset": 50, "endOffset": 53}], "year": 2016, "abstractText": "Recurrent Neural Networks have achieved state-of-the-art results for many problems in NLP and two most popular RNN architectures are \u201cTail Model\u201d and \u201cPooling Model\u201d. In this paper, a hybrid architecture is proposed and we present the first empirical study using LSTMs to compare performance of the three RNN structures on sentence classification task. Experimental results show that the \u201cTail Model\u201d and \u201cHybrid Model\u201d consistently get a better performance over \u201cPooling Model\u201d, and \u201cHybrid Model\u201d is comparable with \u201cTail Model\u201d.", "creator": "\u00fe\u00ff"}}}