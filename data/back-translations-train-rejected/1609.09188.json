{"id": "1609.09188", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis", "abstract": "Academic researchers often need to face with a large collection of research papers in the literature. This problem may be even worse for postgraduate students who are new to a field and may not know where to start. To address this problem, we have developed an online catalog of research papers where the papers have been automatically categorized by a topic model. The catalog contains 7719 papers from the proceedings of two artificial intelligence conferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet Allocation, we use a recently proposed method called hierarchical latent tree analysis for topic modeling. The resulting topic model contains a hierarchy of topics so that users can browse the topics from the top level to the bottom level. The topic model contains a manageable number of general topics at the top level and allows thousands of fine-grained topics at the bottom level. It also can detect topics that have emerged recently.", "histories": [["v1", "Thu, 29 Sep 2016 03:22:01 GMT  (2044kb,D)", "http://arxiv.org/abs/1609.09188v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR cs.LG", "authors": ["leonard k m poon", "nevin l zhang"], "accepted": false, "id": "1609.09188"}, "pdf": {"name": "1609.09188.pdf", "metadata": {"source": "CRF", "title": "Topic Browsing for Research Papers with Hierarchical Latent Tree Analysis", "authors": ["Leonard K.M. Poon", "Nevin L. Zhang"], "emails": ["kmpoon@eduhk.hk", "lzhang@cse.ust.hk"], "sections": [{"heading": "Introduction", "text": "The problem can be even worse for PhD students who are new to a field and may not know where to start. Researchers usually use keywords to search for related papers that start with a search engine. Researchers can then select a generic topic and limit themselves to more specific topics. Papers that relate to the topics can be presented to researchers if they want to. To make this possible, a top-down approach must start with the topic. Researchers can then select a generic topic and focus on more specific topics."}, {"heading": "Background", "text": "Suppose M-words are contained in the vocabulary V = {w1,.., wM}. Each document d can be represented as a vector d = (c1,..., cM), where ci represents the number of words wi that occur in document d. The objective of topic modeling is to give or learn a number K of topics z1,..., zK among documents D. The number K can be given or learned. the topic model defines a distribution over the words for each topic. A topic is often characterized by representative words based on distribution. Latent Dirichlet Allocation (LDA) is a popular method for topic modeling (lead, Ng, and Jordan 2003; lead 2012). LDA assumes that each document d is most likely to the K-topics according to a distribution P (Z = zk | d) zar over the topic z1 (Xik, s.DA) (lead, Ng, and Jordan 2003; lead 2012)."}, {"heading": "Latent Tree Models", "text": "A latent tree model (LTM) is a tree-structured probabilistic graphical model (Zhang 2004; Chen et al. 2012). Figure 2a shows an example of LTM. When an LTM is used for theme modeling, the leaf nodes represent the observed word variables W, while the internal nodes represent the unobserved topic variables Z. All variables are binary. Each word variable Wi-W indicates the presence or absence of the word wi-V in a document. Then, all the topic variables Zi-Z indicate whether a document belongs to the i-thing.For technical reasons, we often root an LTM at one of its latent nodes and consider it to be a Bayesian network (Pearl 1988). Then, all edges are deflected from the root. Numerical information of the model contains a boundary distribution for the root and a conditional distribution for each edge LTM. For example, the variable Z13dean \u2192 numerical document is a probability document (X) for the document-13."}, {"heading": "Hierarchical Latent Tree Analysis", "text": "In fact, most of them are able to play by the rules that they have established for their policies in recent years."}, {"heading": "Building Online Catalog with HLTA", "text": "In this section, we describe how to create an online catalog of documents using HLTA. The process begins with each document contained in a PDF file."}, {"heading": "Extract Text", "text": "For a PDF file, we extract the text content with Apache PDFBox.1 We remove hyphenation from the extracted text, then use Stanford Core NLP (Manning et al. 2014) for sentence division and lemmatization. 2Words normalize, we convert all letters to lowercase letters. We also remove accents and ligatures with the class java.text.Normalizer in the Java library. We use underscores to replace all non-alphanumeric characters and initial digits in a word. We remove stops with 3 and words with less than 4 characters."}, {"heading": "Convert Data", "text": "The term frequency tf (w, d) is defined as the number of occurrences of a word w in the document d. The frequency of the document df (w) is defined as the number of documents containing the word w.We remove words that occur in more than 25% of the documents. In other words, a word w is removed if df (w) \u2265 0.25N, where N is the number of documents. Faced with a number M, we select the M words with the highest TF-IDF, which is replaced by: tf-idf (w) = 1ln df (w).D tf (w, d).1http: / pdfbox.apache.org / 2http: / stanfordnlp.github.io / CoreNLP / 3http: / / jmlr.csail.mit.edu / papers / leto.404 / 11-clever.we list of the selected word (1).wi =."}, {"heading": "Building Model", "text": "After converting the documents into binary vectors, we run PEM-HLTA using these vectors as input data. Note that PEM-HLTA automatically determines the number of levels of latent variables and the number of latent variables in each level. PEM-HLTA results in an LTM."}, {"heading": "Extract Topic Hierarchy", "text": "An LTM defines a tree of nodes. Each internal node represents a topic. As the model defines a common distribution over the variables, we can calculate the MI between each pair of variables. For each topic, we calculate the MI between each subsequent word variable and the theme variable. We then select the following words with the highest MI to characterize the topic. Let's take as an example the extraction of topics from the model in Figure 2a. Each shaded node represents a topic. Let's say we want to characterize Z344. The following word variables are vector representation, word vector,... shadows. We calculate the MI between Z344 and each of these word variables. The 7 words with the highest MI are shown in the shaded line in Figure 2b. The line corresponds to the topic represented by Z344. The second and third lines correspond to the topics represented by Z212Z131 and topic 1313, respectively, with the highest probability being represented in the M2b figure."}, {"heading": "Build Online Catalog", "text": "A document d is assigned to the topic Z, if P (Z = 1 | d) > 0.5. We use a web page to display this information. On the web page, a topic hierarchy is displayed similar to Figure 2b. The hierarchy is built using a jquery plugin called jsTree.4, when a topic is clicked, a list of documents pertaining to that topic is displayed."}, {"heading": "Results and Observations", "text": "We have created an online catalogue of research papers using the method described above, drawn from the presentations of two AI conferences, the AAAI Conference on Artificial Intelligence and the International Joint Conference on Artificial Intelligence, which were held from 2000 to 2015. The resulting collection includes 7719 contributions. We included n-grams for n = 1, 2, 3 and selected 10,000 tokens based on TF-IDF."}, {"heading": "Online Catalog and Source Code", "text": "The online catalog is available at http: / / goo.gl / gtDJC8. A screenshot is shown in Figure 3. The program to build the online catalog was written in Scala and Java. Source code is available at https: / / github.com / kmpoon / hlta."}, {"heading": "Topic Hierarchy", "text": "Table 1 lists the number of topics for each level. 4https: / / www.jstree.comThe hierarchy includes 13 top-level topics (Figure 4). The first topic deals with core methods, matrix factorization, dimensionality reduction. The second topic deals with classification and support vector machines. The third topic deals with probabilistic models and Bayesian methods. The fourth to seventh topic deals with knowledge base, processing of natural language, computer vision and / or agent systems. The eighth topic deals with planning and heuristic search. The ninth topic deals with people and user interfaces. The 11th topic deals with social networks and recommendation systems. The last two deal with satisfaction and / or logic."}, {"heading": "Features", "text": "The online catalog provides some useful features for browsing.Expanding and Collapsing Topics. The Topic Hierarchy Web site allows the user to expand a topic node to see its subtopics or to fold a topic node to hide its subtopics, allowing the user to navigate between more general topics and more specific topics. For example, when we expand the top 5 topics in Figure 4, we see some more specific topics as in Figure 3. The lower part of Figure 3 shows that the overtopic can be divided into two more specific topics about processing natural language, one about processing natural language and one about retrievability of information. Users can specify a number of topics to display at the top of the Web page (Figure 3). By default, the top two levels of topics are shown. Keyword Search. Users can enter a keyword ending at the top of the Web page to search for topics that contain that keyword."}, {"heading": "Trends", "text": "For each document d, we can find the year of d and calculate the subject indicator based on P (Z | d). Therefore, we can build a linear regression model, with the subject indicator using the annual variable as the predictor variable as the response variable. We can then use the regression coefficient to estimate the trend of each topic. The trend allows researchers who are new to a field to consider whether a topic is worthwhile for new work. Table 2 shows the three top level 3 topics with an upward trend. Trends are supported by the growing proportion of documents on the topics, as shown in Figure 7."}, {"heading": "Related Works", "text": "Many tools use LDA for topic modeling (Gardner et al. 2010; Chaney and Lead 2012; Snyder et al. 2013; Sievert and Shirley 2014) and do not show a hierarchy of topics. Smith, Hawes, and Myers (2014) try to build a tool with a topic hierarchy by recursively splitting and reshaping a corpus based on LDA. Unlike HLTA, the topic model does not have a strong statistical basis."}, {"heading": "Conclusions", "text": "We present an online tool that allows the user to browse topics from a hierarchy, the hierarchy of topics is built up using the recently proposed PEM-HLTA. The tool allows the user to view documents on each topic. It offers several functions for easy browsing, and in the future we will consider using a more sophisticated method for detecting n-grams. We can also include hyperlinks to the papers in the document list for each topic. Currently, the online catalog takes some time to load in a web browser, and we will consider storing the data in a database so that the loading time can be significantly reduced."}, {"heading": "Acknowledgment", "text": "We thank Peixian Chen for implementing PEMHLTA and Zichao Li for downloading the PDF files of the research papers. The work in this paper was supported by the Education University of Hong Kong as part of the RG90 / 2014-2015R project and the Hong Kong Research Grants Council as part of grant 16202515."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Academic researchers often need to face with a large collection of research papers in the literature. This problem may be even worse for postgraduate students who are new to a field and may not know where to start. To address this problem, we have developed an online catalog of research papers where the papers have been automatically categorized by a topic model. The catalog contains 7719 papers from the proceedings of two artificial intelligence conferences from 2000 to 2015. Rather than the commonly used Latent Dirichlet Allocation, we use a recently proposed method called hierarchical latent tree analysis for topic modeling. The resulting topic model contains a hierarchy of topics so that users can browse the topics from the top level to the bottom level. The topic model contains a manageable number of general topics at the top level and allows thousands of fine-grained topics at the bottom level. It also can detect topics that have emerged recently.", "creator": "LaTeX with hyperref package"}}}