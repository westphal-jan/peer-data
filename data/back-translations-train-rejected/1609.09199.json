{"id": "1609.09199", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Structure-Aware Classification using Supervised Dictionary Learning", "abstract": "In this paper, we propose a supervised dictionary learning algorithm that aims to preserve the local geometry in both dimensions of the data. A graph-based regularization explicitly takes into account the local manifold structure of the data points. A second graph regularization gives similar treatment to the feature domain and helps in learning a more robust dictionary. Both graphs can be constructed from the training data or learned and adapted along the dictionary learning process. The combination of these two terms promotes the discriminative power of the learned sparse representations and leads to improved classification accuracy. The proposed method was evaluated on several different datasets, representing both single-label and multi-label classification problems, and demonstrated better performance compared with other dictionary based approaches.", "histories": [["v1", "Thu, 29 Sep 2016 04:30:10 GMT  (15kb)", "http://arxiv.org/abs/1609.09199v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CV", "authors": ["yael yankelevsky", "michael elad"], "accepted": false, "id": "1609.09199"}, "pdf": {"name": "1609.09199.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 160 9.09 199v 1 [cs.L G] 29 Sep 20Index Terms - assisted dictionary learning, sparse coding, Chart LapIslamic, Classification"}, {"heading": "1. INTRODUCTION", "text": "The learning method is not as important for classification as the dictionary's ability to discriminate. This motivates the emergence of SupervisedThe research results that lead to these results are from the European Union under the European Research Council Seventh Framework Program, ERC Grant Agreement No. 320649, and from the Israel Science Foundation (ISF) No. 1770 / 1770."}, {"heading": "2. GRAPH-CONSTRAINED SUPERVISED DICTIONARY LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1. Introducing the Data Manifold Structure", "text": "Our proposed algorithm is based on the LC-KSVD approach [8]. To take into account the local geometric structure of the data diversity, we will model the relationships between different data samples using a graph and require the smoothness of the sparse codes across the graph topology. Considering a set of training samples {y1,..., ym} and Rn, let us construct a weighted graph M with m vertices, each node representing a training point. (4) The weightwij associated with the edge linking the i-th and j-th nodes is designed to be inversely proportional to the distance between them. A common choice uses a Gaussian kernel functionwij = exp (\u2212 yi \u2212 yj \u00b2 2 2\u03b5). (4) The graph adjacency matrixWM consists of the edge weights of the edge weights."}, {"heading": "2.2. Introducing Feature Interdependencies", "text": "In order to take into account the interdependencies in the functional domain, we will construct a second graph to model the relationships between the different characteristics, and require the smoothness of the dictionary atoms over this new graph. That is, if two characteristics behave similarly across the training signals, this behavior should be reflected in the structure of the learned atoms. Explicitly, if we look at the set of training samples {y1,..., ym} and Rn, we construct a weighted graph G with n vertices, each node representing a characteristic (according to a row in the data matrix). The weight assigned to the edge that connects the i-th and j-th nodes is again designed in such a way that it is inversely proportional to the distance between them. The graph adjacence matrix WG consists of the edge weights, DG is the corresponding degree matrix, and the graph Laplacian is defined as a view of Daplimated DG = integral G in a Glimated regulation."}, {"heading": "2.3. Learning Dependencies", "text": "For this purpose, an initial weight matrix WG can be constructed for the characteristic diagram by calculating the pairs of distances between the rows in the data matrix Y corresponding to the different characteristics: wij = exp (\u2212 year Y (i,:) \u2212 Y (j,:). In order to better handle partial correlations, these dependencies can be learned and adapted together with the learning process of the dictionary, as previously proposed in [10]. After the graph has received a subdictionary D, the graph can be reestimated on the basis of the dictionary, instead of the original input data Y, according to the following optimization problem: arg min LG-nLDL r (DTLGD) + \u00b5 LG-2F, (11), from which the graph diagram results."}, {"heading": "3. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1. Single-Label Classification", "text": "First, we evaluate the performance of the proposed algorithm for single-level image classification in the AR Face database [14] and the Extended YaleB database [15]. The AR Face database consists of over 4000 images: 26 images per person for 126 different subjects. The images show frontal faces with different facial expressions, lighting conditions and occlusions (sunglasses and scarf). According to the standard evaluation procedure, we use a subset of the database, consisting of 2600 images of 50 male and 50 female subjects. Randomly selected 20 images per person form the training set, and the rest is used for testing. Each image is represented by a 540-dimensional feature vector that applies the procedure described in [8].The expanded YaleB database contains 2414 frontal face images of 38 subjects taken under different lighting conditions. We randomly selected half of the images (approximately 32 images per test) for the rest of the learning set and the rest of the images."}, {"heading": "3.2. Multi-Label Classification", "text": "Next, we evaluate the proposed algorithm for the more difficult task of multiple marking. Within this framework, each instance can be associated with several classes at the same time. Thus, the exploitation of interdependence between labels can significantly influence the success of the classification algorithm. Instead of selecting the maximum score, the relevant labels were selected as those that achieve a result above a threshold, i.e. they can allow several zeros per column. The classification method has also been extended to multiple labels to achieve the maximum score."}, {"heading": "4. CONCLUSIONS", "text": "In this work, we proposed an extension of our previously proposed dual graph-regulated dictionary learning algorithm to a monitored environment. In the new algorithm, the dictionary atoms are encouraged to maintain the characteristic similarities detected in the training data and encapsulated by the Laplacian LG graph, resulting in a more representative and robust dictionary. By sticking to the intrinsic geometric structure of the data captured by the Laplacian LM graph, the resulting sparse codes are more discriminatory and can significantly improve classification performance. Experiments with various data sets show that the proposed algorithm delivers very good classification results and outperforms other monitored dictionary learning algorithms for both single-marker and multi-marker classification tasks."}, {"heading": "5. REFERENCES", "text": "[1] J. Wright, A. Yang, A. Ganesh, S. S. Sastry, and Yi Ma, \"Robust face recognition via sparse representation,\" TPAMI, vol. 31, no. 2, pp. 210-227, Feb. 2009. [2] M. Yang, L. Zhang, J. Yang, and D. Zhang, \"Metafacelearning for sparse representation based face recognition,\" in ICIP, 2010, pp. 1601-1604. [3] M. Yang, L. Zhang, X. Feng, and D. Zhang, \"Fisher discrimination dictionary learning for sparse representation,\" in ICCV, 2011, pp. 543-550. I. Ramirez, P. Sprechmann, and G. Sapiro, \"Classification and clustering via dictionary learning with structured incoherence and shared features.\""}], "references": [{"title": "Robust face recognition via sparse representation", "author": ["J. Wright", "A.Y. Yang", "A. Ganesh", "S.S. Sastry", "Yi Ma"], "venue": "TPAMI, vol. 31, no. 2, pp. 210\u2013227, Feb 2009.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Metaface  learning for sparse representation based face recognition", "author": ["M. Yang", "L. Zhang", "J. Yang", "D. Zhang"], "venue": "ICIP, 2010, pp. 1601\u20131604.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Fisher discrimination dictionary learning for sparse representation", "author": ["M. Yang", "L. Zhang", "X. Feng", "D. Zhang"], "venue": "ICCV, 2011, pp. 543\u2013550.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2011}, {"title": "Classification and clustering via dictionary learning with structured incoherence and shared features", "author": ["I. Ramirez", "P. Sprechmann", "G. Sapiro"], "venue": "CVPR, 2010, pp. 3501\u20133508.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2010}, {"title": "Supervised dictionary learning", "author": ["J. Mairal", "F.R. Bach", "J. Ponce", "G. Sapiro", "A. Zisserman"], "venue": "NIPS, pp. 1033\u20131040. 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Joint learning and dictionary construction for pattern recognition", "author": ["D.-S. Pham", "S. Venkatesh"], "venue": "CVPR, June 2008, pp. 1\u20138.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2008}, {"title": "Discriminative K-SVD for Dictionary Learning in Face Recognition", "author": ["Q. Zhang", "B. Li"], "venue": "CVPR, June 2010, pp. 2691\u20132698.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning a Discriminative Dictionary for Sparse Coding via Label Consistent K-SVD", "author": ["Z. Jiang", "Z. Lin", "L.S. Davis"], "venue": "CVPR, 2011, pp. 1697\u20131704.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation", "author": ["M. Aharon", "M. Elad", "A. Bruckstein"], "venue": "IEEE Trans. Sig. Proc., vol. 54, no. 11, pp. 4311\u20134322, Nov. 2006.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2006}, {"title": "Dual Graph Regularized Dictionary Learning", "author": ["Y. Yankelevsky", "M. Elad"], "venue": "IEEE Transactions on Signal and Information Processing over Networks, 2016, doi:10.1109/TSIPN.2016.2605763.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Graph Regularized Sparse Coding for Image Representation", "author": ["M. Zheng", "J. Bu", "C. Chen", "C. Wang", "L. Zhang", "G. Qiu", "D. Cai"], "venue": "IEEE Trans. Img. Proc., vol. 20, no. 5, pp. 1327\u20131336, May 2011.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "Learning Dictionaries with Graph Embedding Constraints", "author": ["K.N. Ramamurthy", "J.J. Thiagarajan", "P. Sattigeri", "A. Spanias"], "venue": "ASILOMAR, Nov 2012, pp. 1974\u2013 1978.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Laplacian eigenmaps for dimensionality reduction and data representation", "author": ["M. Belkin", "P. Niyogi"], "venue": "Neural Comput., vol. 15, no. 6, pp. 1373\u20131396, June 2003.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2003}, {"title": "The AR Face Database", "author": ["A. Martinez", "R. Benavente"], "venue": "Tech. Rep. 24, CVC, June 1998.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1998}, {"title": "From Few to Many: Illumination Cone Models for Face Recognition Under Variable Lighting and Pose", "author": ["A.S. Georghiades", "P.N. Belhumeur", "D.J. Kriegman"], "venue": "TPAMI, vol. 23, no. 6, pp. 643\u2013660, June 2001.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2001}, {"title": "Learning multi-label scene classification", "author": ["M.R. Boutell", "J. Luo", "X. Shen", "C.M. Brown"], "venue": "Pattern Recognition, vol. 37, no. 9, pp. 1757\u20131771, 2004.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2004}, {"title": "A Kernel Method for Multi- Labelled Classification", "author": ["A. Elisseeff", "J. Weston"], "venue": "NIPS, 2001, pp. 681\u2013687.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2001}], "referenceMentions": [{"referenceID": 0, "context": "[1, 2, 3, 4]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 1, "context": "[1, 2, 3, 4]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 2, "context": "[1, 2, 3, 4]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 3, "context": "[1, 2, 3, 4]).", "startOffset": 0, "endOffset": 12}, {"referenceID": 4, "context": "[5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients.", "startOffset": 0, "endOffset": 12}, {"referenceID": 5, "context": "[5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients.", "startOffset": 0, "endOffset": 12}, {"referenceID": 6, "context": "[5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients.", "startOffset": 0, "endOffset": 12}, {"referenceID": 7, "context": "[5, 6, 7, 8]) learn a discriminative dictionary by introducing a classification-error term into the objective function, and enforcing some discriminative criteria on the optimized sparse coefficients.", "startOffset": 0, "endOffset": 12}, {"referenceID": 7, "context": "Of the latter category, we focus on the Label Consistent K-SVD (LC-KSVD) method [8] for joint learning of an overcomplete dictionary D and an optimal linear classifier W :", "startOffset": 80, "endOffset": 83}, {"referenceID": 8, "context": "Equation (3) can be efficiently solved using the K-SVD algorithm [9], which iteratively alternates between a sparse coding step (optimization over X) and a dictionary update step (that updates each dictionary atom in D\u0303 along with its related coefficients from X).", "startOffset": 65, "endOffset": 68}, {"referenceID": 9, "context": "In [10], we have suggested an unsupervised dictionary learning algorithm for graph signals.", "startOffset": 3, "endOffset": 7}, {"referenceID": 7, "context": "In this paper, we propose an extension of our dual graph regularized dictionary learning algorithm to a supervised setting by applying the same ideas to the LC-KSVD approach [8].", "startOffset": 174, "endOffset": 177}, {"referenceID": 7, "context": "Our proposed algorithm is based on the LC-KSVD approach [8].", "startOffset": 56, "endOffset": 59}, {"referenceID": 10, "context": "Similarly to the methods proposed in [11, 12], we incorporate LM into the objective function as a regularizer of the form Tr(XLMX T ).", "startOffset": 37, "endOffset": 45}, {"referenceID": 11, "context": "Similarly to the methods proposed in [11, 12], we incorporate LM into the objective function as a regularizer of the form Tr(XLMX T ).", "startOffset": 37, "endOffset": 45}, {"referenceID": 12, "context": "This term therefore encourages similar signals, having a large proximity measure wij , to have similar sparse codes, thus satisfying the commonly known manifold assumption [13].", "startOffset": 172, "endOffset": 176}, {"referenceID": 9, "context": "This can be done using the graphDL algorithm we proposed in [10] that reflects the added restrictions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 9, "context": "To better handle partial correlations, these dependencies can be learned and adapted along with the dictionary learning process, as previously suggested in [10].", "startOffset": 156, "endOffset": 160}, {"referenceID": 9, "context": "\u2022 Run a few iterations of graphDL (see [10], Algorithm 3), initialized with D\u0303, to solve Equation (9) and obtain D\u0303, X.", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "Single-Label Classification First, we evaluate the performance of the proposed algorithm for single-label image classification on the AR Face database [14] and on the Extended YaleB database [15].", "startOffset": 151, "endOffset": 155}, {"referenceID": 14, "context": "Single-Label Classification First, we evaluate the performance of the proposed algorithm for single-label image classification on the AR Face database [14] and on the Extended YaleB database [15].", "startOffset": 191, "endOffset": 195}, {"referenceID": 7, "context": "Each image is represented by a 540-dimensional feature vector using the procedure described in [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 7, "context": "We compare the proposed algorithm to the two variants proposed in [8]: LC-KSVD1, which refers to (2) for \u03b2 = 0, and LC-KSVD2, which refers to (2) for \u03b1, \u03b2 6= 0.", "startOffset": 66, "endOffset": 69}, {"referenceID": 7, "context": "results presented in [8].", "startOffset": 21, "endOffset": 24}, {"referenceID": 15, "context": "Each image is represented by a 294-dimensional feature vector using the procedure described in [16].", "startOffset": 95, "endOffset": 99}, {"referenceID": 16, "context": "The yeast dataset [17] is formed by micro-array expression data and phylogenetic profiles, and includes 2417 genes, 1500 of which are used for training and the rest constitute the test set.", "startOffset": 18, "endOffset": 22}, {"referenceID": 7, "context": "Similarly to the single-label classification experiment, we compare two variants of our method (with and without adapting the graphs) to LC-KSVD1 and LC-KSVD2 proposed in [8].", "startOffset": 171, "endOffset": 174}, {"referenceID": 15, "context": "To assess the accuracy of the algorithms in the multi-label experiments, we use the average precision measure as defined in [16].", "startOffset": 124, "endOffset": 128}], "year": 2016, "abstractText": "In this paper, we propose a supervised dictionary learning algorithm that aims to preserve the local geometry in both dimensions of the data. A graph-based regularization explicitly takes into account the local manifold structure of the data points. A second graph regularization gives similar treatment to the feature domain and helps in learning a more robust dictionary. Both graphs can be constructed from the training data or learned and adapted along the dictionary learning process. The combination of these two terms promotes the discriminative power of the learned sparse representations and leads to improved classification accuracy. The proposed method was evaluated on several different datasets, representing both single-label and multi-label classification problems, and demonstrated better performance compared with other dictionary based approaches.", "creator": "LaTeX with hyperref package"}}}