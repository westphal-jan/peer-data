{"id": "1609.09247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Training Dependency Parsers with Partial Annotation", "abstract": "Recently, these has been a surge on studying how to obtain partially annotated data for model supervision. However, there still lacks a systematic study on how to train statistical models with partial annotation (PA). Taking dependency parsing as our case study, this paper describes and compares two straightforward approaches for three mainstream dependency parsers. The first approach is previously proposed to directly train a log-linear graph-based parser (LLGPar) with PA based on a forest-based objective. This work for the first time proposes the second approach to directly training a linear graph-based parse (LGPar) and a linear transition-based parser (LTPar) with PA based on the idea of constrained decoding. We conduct extensive experiments on Penn Treebank under three different settings for simulating PA, i.e., random dependencies, most uncertain dependencies, and dependencies with divergent outputs from the three parsers. The results show that LLGPar is most effective in learning from PA and LTPar lags behind the graph-based counterparts by large margin. Moreover, LGPar and LTPar can achieve best performance by using LLGPar to complete PA into full annotation (FA).", "histories": [["v1", "Thu, 29 Sep 2016 08:12:14 GMT  (178kb)", "http://arxiv.org/abs/1609.09247v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["zhenghua li", "yue zhang", "jiayuan chao", "min zhang"], "accepted": false, "id": "1609.09247"}, "pdf": {"name": "1609.09247.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Zhenghua Li", "Yue Zhang", "Jiayuan Chao", "Min Zhang"], "emails": ["zhli13@suda.edu.cn,", "minzhang@suda.edu.cn,", "zhangyue1107@qq.com"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.09 247v 1 [cs.C L] 29 Sep 2016"}, {"heading": "1 Introduction", "text": "Traditional supervised approaches to structural classification assume a complete annotation (FA), which means that the training instances have complete manually labeled structures. In the case of dependency analysis, FA means that a complete parse tree is provided for each training set. However, recent studies suggest that it is more economical and effective to construct labeled data with partial annotation (PA). Many research efforts have been made to obtain partially labeled data for various tasks through active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Artie, 2014; Flannery and Mori, 2015; Li et al., 2016), lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicit on websites."}, {"heading": "2 Dependency Parsing", "text": "A dependency tree comprises a series of dependencies, namely d = {h y m: 0 \u2264 h \u2264 n, 1 \u2264 m \u2264 n}, where h y m is a dependency from a header h to a modifying word m. A complete dependency tree contains n dependencies, namely | d | = n, whereas a partial dependency tree contains less than n dependencies, namely | d | < n. Alternatively, FA can be understood as a special form of PA. To clarify, we call a complete tree d and a partial tree dp. The decryption procedure aims to find an optimal complete tree, d = arg max d = Y (x) Score (x, d; w) Score (w \u00b7 w; w), which contains a value (w) (w = v)."}, {"heading": "2.1 Graph-based Approach", "text": "To facilitate efficient search, the graph-based method takes into account the score of a dependence in the data of small sub-trees p: Score (x, d; w) = \u2211 p'dScore (x, p; w) (2) Dynamic programming based on exact search is usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of McDonald and Pereira (2006), which includes two types of sub-trees, i.e., individual dependencies and adjacent siblings, and the functionality described in Bohnet (2010). \u2212 A log-linear graph-based parser (LLGPar) defines the conditional probability of d given x asp (d | x; w) = eScore (x, d; w)."}, {"heading": "2.2 Transition-based Approach", "text": "The transition method builds dependence by applying a sequence of shift / reduction actions a and factoring the score of a tree into the sum of the scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011): Score (x, d; w) = Score (x, a \u2192 d; w) = \u2211 | a | i = 1 score (x, ci, ai; w) (4), where ai is the action taken in steps i and ci of the configuration status after action is a1... ai \u2212 1. Modern transition methods usually use inaccurate beam searching to find an action sequence with the highest score, and apply global perceptron-like training to learn w. We build an arc-eager dependency saver and the ultra-modern features described in (Zhang and Nivre, 2011), which are called linear, parser-based transition (Tar)."}, {"heading": "3 Directly training parsers with PA", "text": "D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D: D:"}, {"heading": "4 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data, parameter settings, and evaluation metric", "text": "We conduct experiments on Penn Treebank (PTB) and follow the standard data split data (sec 2-21 as training, sec 22 as development, and sec 23 as test). We build a CRF-based bigram part-of-speech (POS) tagger to produce automatic POS tags for all train / dev / test data (10-way jackknifing on training data), with tagging accuracy 97.3% on test data. As suggested by a previous anonymous reviewer, we split the training data into two parts. We assume that the first 1K training sets are provided as small training data with FA, which can be achieved by a small amount of manual annotation or by cross-lingual projection methods. We simulate PA for the remaining 39K sets. Table 1 shows the data statistics.We train LLGPar with stostic gradient (Finkel al)."}, {"heading": "4.3 Results of different parsers trained on FA", "text": "BerkeleyParser (v1.7) is a constituting structure parser whose results are converted into dependency structures (Petrov and Klein, 2007).TurboParser (v2.1.0) is a linear graph-based dependency parser that uses linear programming for inference (Martins et al., 2013).Mate tool (v3.3) is a linear graph-based dependency parser that is very similar to our implemented LGPar (Bohnet, 2010).ZPar (v0.6) is a linear transition-based dependency parser that is very similar to our implemented LGPar (Zhang and Clark, 2011).The results are shown in Table 2."}, {"heading": "4.4 Results of the directly-train approaches", "text": "The three parsers are trained directly on train-1K with FA and train-39K with PA. However, these data are roughly based on those in Section 3. Table 3 shows the results. When comparing the three parsers, we have several unique results. (1) LLGPar actually achieves the best performance over all settings and is very effective in learning from PA. (2) The accuracy gap between LGPar and LLGPar becomes larger with PA than with FA, suggesting that LGPar is less effective in learning from PA than LLGPar. (3) LTPar lags far behind LLGPar and is ineffective in learning from PA.FA (random) vs. PA (random): From the results in the two large columns, we can see that LLGPar achieves a higher accuracy of about 0.5% when we are trained on sets with \u03b1% random dependencies than when we are trained on these synchronized sets with FA. \"This is reasonable and can be explained by using the assumption that the full PPA can be made."}, {"heading": "4.5 Results of the complete-then-train methods", "text": "The easiest way to learn LGPar-1K, Tar-LLK-39K for 39FA-39K-39K is the Complete-then-learn method (Mirroshandel and Nasr, 39FA, 2011). The idea is first to use an existing parser to convert sub-trees in train-39K into complete trees based on restricted decoding, and then to train the target parser on train-1K with FA and train-39K with completed decoding. \"No Restrictions (0%)\" means that train-39K has no annotated dependencies and normal decoding without limitations. In the remaining columns, each parser performs restricted decoding on PA, where dependencies are provided in each set. \u2022 Permanently trained self-completion: We complete PA in FA with appropriate parsers trained only on train-1K with FA. We call these parsers LGPLPar-1K-1K-1K-39K."}, {"heading": "4.6 Results on test data: directly-train vs. complete-then-train", "text": "Table 6, the UAS reports on test data from parsers trained directly on Pull-1K with FA and Pull-39K with PA, and from parsers trained on Pull-1K with FA and Pull-39K with FA, supplemented by finely trained LLGPar1K + 39K. The results are consistent with those on the development data in Tables 3 and 5. Comparing the two settings, we can draw two interesting results: First, LLGPar performs slightly better with the Directlytrain method. Second, LGPar performs slightly better with the Complet-then-Train in in most cases, with the exception of unsafe (30%). Third, LTPar performs slightly better with the Complet-then-Train in most cases."}, {"heading": "5 Failed attempts to enhancing LTPar", "text": "All the experimental results in the previous section suggest that LTPar is ineffective in learning PA, and Table 4 shows that limited decoding itself works well for LTPar. In contrast, LGPar is also based on limited decoding and works much better than LTPar. However, the most important difference is that LGPar in line 4-7 of Algorithm 1 uses dynamic programming based on exact search algorithms to find the tree with the highest score according to the current model, while LTPar uses approximate beam search algorithms. Approximate search procedure can cause the optimal tree to fall off the beam too early, and the returned a + may cause the model to be updated for certain incorrect structures due to the lack of sufficient supervision in the PA scenario. We have tried three strategies to improve LTPar as much as possible, although little progress has been made."}, {"heading": "6 Related work", "text": "Sassano and Kurohashi (2010), Jiang et al. (2010), and Flannery and Mori (2015) convert partially commented cases into local dependency / non-dependency classification cases, which may suffer from the lack of non-local correlation between dependencies in a tree. Mirroshandel and Nasr (2011) and Majidi and Crane (2013) then adopt the fully learning method, using parsers roughly trained on existing FA data to complete them with limited decoding. However, our experiments show that this leads to a dramatic decrease in accuracy in the analysis. Nivre et al. (2014) present a limited decoding method for arcing-based transition parsers. However, their work focuses on allowing their parser to effectively exploit external constraints during the evaluation phase."}, {"heading": "7 Conclusions", "text": "This paper examines the problem of training dependence of parsers based on partially marked data. In addition, we focus on the realistic scenario where we have a small-scale training dataset with FA and a large-scale training dataset with PA. We experiment with three settings for simulating PA. We compare several direct-training and then full-training approaches with three mainstream parsers, i.e., LLGPar, LGPar and LTPar. Finally, we draw the following important conclusions. (1) For the Complete Training approach, the use of parsers roughly trained on small-scale data leads to unsatisfactory results. (2) LLGPar is able to make full use of the PA for training. In contrast, LGPar is marginally inferior and LTPar performs poorly in learning PA. (3) The complete and then underlying approach can make LGPar and LTar Par more dynamic in relation to a precision."}, {"heading": "Acknowledgments", "text": "The authors would like to thank the anonymous reviewers for their helpful comments."}], "references": [{"title": "Top accuracy and fast dependency parsing is not a contradiction", "author": ["Bernd Bohnet"], "venue": "In Proceedings of COLING,", "citeRegEx": "Bohnet.,? \\Q2010\\E", "shortCiteRegEx": "Bohnet.", "year": 2010}, {"title": "Experiments with a higher-order projective dependency parser", "author": ["Xavier Carreras"], "venue": "In Proceedings of EMNLP/CoNLL,", "citeRegEx": "Carreras.,? \\Q2007\\E", "shortCiteRegEx": "Carreras.", "year": 2007}, {"title": "Partial training for a lexicalized-grammar parser", "author": ["Clark", "Curran2006] Stephen Clark", "James Curran"], "venue": "In Proceedings of the Human Language Technology Conference of the NAACL,", "citeRegEx": "Clark et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Clark et al\\.", "year": 2006}, {"title": "Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms", "author": ["Michael Collins"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Collins.,? \\Q2002\\E", "shortCiteRegEx": "Collins.", "year": 2002}, {"title": "Sequence learning from data with multiple labels", "author": ["Dredze et al.2009] Mark Dredze", "Partha Pratim Talukdar", "Koby Crammer"], "venue": "In ECML/PKDD Workshop on Learning from Multi-Label Data", "citeRegEx": "Dredze et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Dredze et al\\.", "year": 2009}, {"title": "Efficient, feature-based, conditional random field parsing", "author": ["Alex Kleeman", "Christopher D. Manning"], "venue": "In Proceedings of ACL,", "citeRegEx": "Finkel et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2008}, {"title": "Combining active learning and partial annotation for domain adaptation of a japanese dependency parser", "author": ["Flannery", "Mori2015] Daniel Flannery", "Shinsuke Mori"], "venue": "In Proceedings of the 14th International Conference on Parsing Technologies,", "citeRegEx": "Flannery et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Flannery et al\\.", "year": 2015}, {"title": "Dependency grammar induction via bitext projection constraints", "author": ["Jennifer Gillenwater", "Ben Taskar"], "venue": "In Proceedings of ACL-IJCNLP", "citeRegEx": "Ganchev et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Ganchev et al\\.", "year": 2009}, {"title": "Dynamic programming for linear-time incremental parsing", "author": ["Huang", "Sagae2010] Liang Huang", "Kenji Sagae"], "venue": "In Proceedings of ACL", "citeRegEx": "Huang et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2010}, {"title": "Supervised grammar induction using training data with limited constituent information", "author": ["Rebecca Hwa"], "venue": "In Proceedings of ACL,", "citeRegEx": "Hwa.,? \\Q1999\\E", "shortCiteRegEx": "Hwa.", "year": 1999}, {"title": "Dependency parsing and projection based on word-pair classification", "author": ["Jiang et al.2010] Wenbin Jiang", "Qun Liu"], "venue": "In ACL,", "citeRegEx": "Jiang and Liu.,? \\Q2010\\E", "shortCiteRegEx": "Jiang and Liu.", "year": 2010}, {"title": "Discriminative learning with natural annotations: Word segmentation as a case study", "author": ["Jiang et al.2013] Wenbin Jiang", "Meng Sun", "Yajuan L\u00fc", "Yating Yang", "Qun Liu"], "venue": "In Proceedings of ACL,", "citeRegEx": "Jiang et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Jiang et al\\.", "year": 2013}, {"title": "Efficient third-order dependency parsers", "author": ["Koo", "Collins2010] Terry Koo", "Michael Collins"], "venue": "In ACL,", "citeRegEx": "Koo et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Koo et al\\.", "year": 2010}, {"title": "Dependency Parsing (Synthesis Lectures On Human Language Technologies)", "author": ["K\u00fcbler et al.2009] Sandra K\u00fcbler", "Ryan McDonald", "Joakim Nivre"], "venue": null, "citeRegEx": "K\u00fcbler et al\\.,? \\Q2009\\E", "shortCiteRegEx": "K\u00fcbler et al\\.", "year": 2009}, {"title": "Active learning for Chinese word segmentation", "author": ["Li et al.2012] Shoushan Li", "Guodong Zhou", "Chu-Ren Huang"], "venue": "In Proceedings of COLING 2012: Posters,", "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Soft cross-lingual syntax projection for dependency parsing", "author": ["Li et al.2014] Zhenghua Li", "Min Zhang", "Wenliang Chen"], "venue": "In COLING,", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "Active learning for dependency parsing with partial annotation", "author": ["Li et al.2016] Zhenghua Li", "Min Zhang", "Yue Zhang", "Zhanyi Liu", "Wenliang Chen", "Hua Wu", "Haifeng Wang"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Domain adaptation for CRFbased Chinese word segmentation using free annotations", "author": ["Liu et al.2014] Yijia Liu", "Yue Zhang", "Wanxiang Che", "Ting Liu", "Fan Wu"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Liu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2014}, {"title": "Probabilistic models for high-order projective dependency parsing", "author": ["Ma", "Zhao2015] Xuezhe Ma", "Hai Zhao"], "venue": "In arXiv:1502.04174,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Active learning for dependency parsing by a committee of parsers", "author": ["Majidi", "Crane2013] Saeed Majidi", "Gregory Crane"], "venue": "In Proceedings of IWPT,", "citeRegEx": "Majidi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Majidi et al\\.", "year": 2013}, {"title": "An experimental comparison of active learning strategies for partially labeled sequences", "author": ["Marcheggiani", "Arti\u00e8res2014] Diego Marcheggiani", "Thierry Arti\u00e8res"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Marcheggiani et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marcheggiani et al\\.", "year": 2014}, {"title": "Turning on the turbo: Fast thirdorder non-projective turbo parsers", "author": ["Miguel Almeida", "Noah A. Smith"], "venue": "In Proceedings of ACL,", "citeRegEx": "Martins et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Martins et al\\.", "year": 2013}, {"title": "Online learning of approximate dependency parsing algorithms", "author": ["McDonald", "Pereira2006] Ryan McDonald", "Fernando Pereira"], "venue": "In Proceedings of EACL,", "citeRegEx": "McDonald et al\\.,? \\Q2006\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2006}, {"title": "Online large-margin training of dependency parsers", "author": ["Koby Crammer", "Fernando Pereira"], "venue": "In Proceedings of ACL,", "citeRegEx": "McDonald et al\\.,? \\Q2005\\E", "shortCiteRegEx": "McDonald et al\\.", "year": 2005}, {"title": "Parse imputation for dependency annotations", "author": ["Liang Sun", "Jason Baldridge"], "venue": "In Proceedings of ACL-IJCNLP,", "citeRegEx": "Mielens et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mielens et al\\.", "year": 2015}, {"title": "Active learning for dependency parsing using partially annotated sentences", "author": ["Mirroshandel", "Alexis Nasr"], "venue": "In Proceedings of the 12th International Conference on Parsing Technologies,", "citeRegEx": "Mirroshandel et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Mirroshandel et al\\.", "year": 2011}, {"title": "Constrained arc-eager dependency parsing", "author": ["Nivre et al.2014] Joakim Nivre", "Yoav Goldberg", "Ryan McDonald"], "venue": "In Computational Linguistics,", "citeRegEx": "Nivre et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Nivre et al\\.", "year": 2014}, {"title": "An efficient algorithm for projective dependency parsing", "author": ["Joakim Nivre"], "venue": "In Proceedings of IWPT,", "citeRegEx": "Nivre.,? \\Q2003\\E", "shortCiteRegEx": "Nivre.", "year": 2003}, {"title": "Inside-outside reestimation from partially bracketed corpora", "author": ["Pereira", "Schabes1992] Fernando Pereira", "Yves Schabes"], "venue": "In Proceedings of the Workshop on Speech and Natural Language (HLT),", "citeRegEx": "Pereira et al\\.,? \\Q1992\\E", "shortCiteRegEx": "Pereira et al\\.", "year": 1992}, {"title": "Improved inference for unlexicalized parsing", "author": ["Petrov", "Klein2007] Slav Petrov", "Dan Klein"], "venue": "In Proceedings of NAACL", "citeRegEx": "Petrov et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2007}, {"title": "Parsing the wall street journal using a lexical-functional grammar and discriminative estimation techniques", "author": ["Tracy H. King", "Ronald M. Kaplan", "Richard Crouch", "John T. III Maxwell", "Mark Johnson"], "venue": "Proceedings of ACL,", "citeRegEx": "Riezler et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Riezler et al\\.", "year": 2002}, {"title": "Using smaller constituents rather than sentences in active learning for japanese dependency parsing", "author": ["Sassano", "Kurohashi2010] Manabu Sassano", "Sadao Kurohashi"], "venue": "In Proceedings of ACL,", "citeRegEx": "Sassano et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Sassano et al\\.", "year": 2010}, {"title": "Data-driven dependency parsing of new languages using incomplete and noisy training data", "author": ["Spreyer", "Kuhn2009] Kathrin Spreyer", "Jonas Kuhn"], "venue": "In CoNLL,", "citeRegEx": "Spreyer et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Spreyer et al\\.", "year": 2009}, {"title": "Target language adaptation of discriminative transfer parsers", "author": ["Ryan McDonald", "Joakim Nivre"], "venue": "In Proceedings of NAACL,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Statistical dependency analysis with support vector machines", "author": ["Yamada", "Matsumoto2003] Hiroyasu Yamada", "Yuji Matsumoto"], "venue": "In Proceedings of IWPT,", "citeRegEx": "Yamada et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Yamada et al\\.", "year": 2003}, {"title": "Semi-supervised Chinese word segmentation using partial-label learning with conditional random fields", "author": ["Yang", "Vozila2014] Fan Yang", "Paul Vozila"], "venue": "In Proceedings of EMNLP,", "citeRegEx": "Yang et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2014}, {"title": "Syntactic processing using the generalized perceptron and beam search", "author": ["Zhang", "Clark2011] Yue Zhang", "Stephen Clark"], "venue": "Computational Linguistics,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}, {"title": "Transition-based dependency parsing with rich nonlocal features", "author": ["Zhang", "Nivre2011] Yue Zhang", "Joakim Nivre"], "venue": "In Proceedings of ACL,", "citeRegEx": "Zhang et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Zhang et al\\.", "year": 2011}], "referenceMentions": [{"referenceID": 14, "context": "A lot of research effort has been attracted to obtain partially-labeled data for different tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti\u00e8res, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al.", "startOffset": 117, "endOffset": 267}, {"referenceID": 16, "context": "A lot of research effort has been attracted to obtain partially-labeled data for different tasks via active learning (Sassano and Kurohashi, 2010; Mirroshandel and Nasr, 2011; Li et al., 2012; Marcheggiani and Arti\u00e8res, 2014; Flannery and Mori, 2015; Li et al., 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al.", "startOffset": 117, "endOffset": 267}, {"referenceID": 7, "context": ", 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al.", "startOffset": 41, "endOffset": 124}, {"referenceID": 15, "context": ", 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al.", "startOffset": 41, "endOffset": 124}, {"referenceID": 11, "context": ", 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014).", "startOffset": 70, "endOffset": 151}, {"referenceID": 17, "context": ", 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014).", "startOffset": 70, "endOffset": 151}, {"referenceID": 26, "context": ", 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014).", "startOffset": 70, "endOffset": 151}, {"referenceID": 7, "context": ", 2016), cross-lingual syntax projection (Spreyer and Kuhn, 2009; Ganchev et al., 2009; Jiang et al., 2010; Li et al., 2014), or mining natural annotation implicitly encoded in web pages (Jiang et al., 2013; Liu et al., 2014; Nivre et al., 2014; Yang and Vozila, 2014). Figure 1 gives an example sentence partially annotated with two dependencies. However, there still lacks systematic study on how to train structural models such as dependency parsers with PA. Most previous works listed above rely on ad-hoc strategies designed for only basic dependency parsers. One exception is that Li et al. (2014) convert partial trees into forests and train a log-linear graph-based dependency parser (LLGPar) with PA based on a forest-base objective, showing promising results.", "startOffset": 66, "endOffset": 604}, {"referenceID": 14, "context": "We also implement the forestobjective based approach of Li et al. (2014) for LLGPar.", "startOffset": 56, "endOffset": 73}, {"referenceID": 13, "context": "wn, dependency parsing builds a complete dependency tree d rooted at w0, where w0 is an artificial token linking to the root of the sentence (K\u00fcbler et al., 2009).", "startOffset": 141, "endOffset": 162}, {"referenceID": 23, "context": "Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010).", "startOffset": 84, "endOffset": 174}, {"referenceID": 1, "context": "Dynamic programming based exact search are usually applied to find the optimal tree (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010).", "startOffset": 84, "endOffset": 174}, {"referenceID": 0, "context": ", 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). We adopt the second-order model of McDonald and Pereira (2006) which incorporates two kinds of subtrees, i.", "startOffset": 36, "endOffset": 139}, {"referenceID": 0, "context": ", single dependencies and adjacent siblings, and the feature set described in Bohnet (2010). A log-linear graph-based parser (LLGPar) defines the conditional probability of d given x as", "startOffset": 78, "endOffset": 92}, {"referenceID": 15, "context": "Recently, LLGPar attracts more attention due to its capability in producing subtree probabilities and learning from PA (Li et al., 2014; Ma and Zhao, 2015).", "startOffset": 119, "endOffset": 155}, {"referenceID": 27, "context": "2 Transition-based Approach The transition-based method builds a dependency by applying sequence of shift/reduce actions a, and factorizes the score of a tree into the sum of scores of each action in a (Yamada and Matsumoto, 2003; Nivre, 2003; Zhang and Nivre, 2011):", "startOffset": 202, "endOffset": 266}, {"referenceID": 30, "context": "(2014), LLGPar can naturally learn from PA based on the idea of ambiguous labeling, which allows a sentence to have multiple parse trees (forest) as its gold-standard reference (Riezler et al., 2002; Dredze et al., 2009; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 177, "endOffset": 244}, {"referenceID": 4, "context": "(2014), LLGPar can naturally learn from PA based on the idea of ambiguous labeling, which allows a sentence to have multiple parse trees (forest) as its gold-standard reference (Riezler et al., 2002; Dredze et al., 2009; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 177, "endOffset": 244}, {"referenceID": 33, "context": "(2014), LLGPar can naturally learn from PA based on the idea of ambiguous labeling, which allows a sentence to have multiple parse trees (forest) as its gold-standard reference (Riezler et al., 2002; Dredze et al., 2009; T\u00e4ckstr\u00f6m et al., 2013).", "startOffset": 177, "endOffset": 244}, {"referenceID": 13, "context": "3 Directly training parsers with PA As described in Li et al. (2014), LLGPar can naturally learn from PA based on the idea of ambiguous labeling, which allows a sentence to have multiple parse trees (forest) as its gold-standard reference (Riezler et al.", "startOffset": 52, "endOffset": 69}, {"referenceID": 13, "context": "Then, we can define an forest-based training objective function to maximize the likelihood of training data as described in Li et al. (2014). LGPar can be extended to directly learn from PA based on the idea of constrained decoding, as shown in Algorithm 1, which has been previously applied to Chinese word segmentation with partially labeled sequences (Jiang et al.", "startOffset": 124, "endOffset": 141}, {"referenceID": 26, "context": "Fortunately, Nivre et al. (2014) propose a procedure to enable arc-eager parsers to decode in the search space constrained by some given dependencies.", "startOffset": 13, "endOffset": 33}, {"referenceID": 5, "context": "We train LLGPar with stochastic gradient descent (Finkel et al., 2008).", "startOffset": 49, "endOffset": 70}, {"referenceID": 3, "context": "Following standard practice established by Collins (2002), we adopt averaged weights for evaluation of LGPar and LTPar and use the early-update strategy during training LTPar.", "startOffset": 43, "endOffset": 58}, {"referenceID": 3, "context": "Following standard practice established by Collins (2002), we adopt averaged weights for evaluation of LGPar and LTPar and use the early-update strategy during training LTPar. Since we have two sets of training data, we adopt the simple corpus-weighting strategy of Li et al. (2014). In each iteration, we merge train-1K and a subset of random 10K sentences from train-39K , shuffle them, and then use them for training.", "startOffset": 43, "endOffset": 283}, {"referenceID": 14, "context": "Uncertain (30% or 15%): In their work of active learning with PA, Li et al. (2016) show that the marginal probabilities from LLGPar is the most effective uncertainty measurement for selecting the most informative words to be annotated.", "startOffset": 66, "endOffset": 83}, {"referenceID": 14, "context": "Uncertain (30% or 15%): In their work of active learning with PA, Li et al. (2016) show that the marginal probabilities from LLGPar is the most effective uncertainty measurement for selecting the most informative words to be annotated. Following their work, we first train LLGPar on train-1K with FA, and then use LLGPar to parse train-39K and select \u03b1% most uncertain words to keep their heads. Following Li et al. (2016), we measure the uncertainty of a word wi according to the marginal probability gap between its two most likely heads hi and h 1 i .", "startOffset": 66, "endOffset": 423}, {"referenceID": 21, "context": "0) is a linear graph-based dependency parser using linear programming for inference (Martins et al., 2013).", "startOffset": 84, "endOffset": 106}, {"referenceID": 0, "context": "3) is a linear graph-based dependency parser very similar to our implemented LGPar (Bohnet, 2010).", "startOffset": 83, "endOffset": 97}, {"referenceID": 14, "context": "Please note that LLGPar-1K+39K actually performs closed test in this setting, meaning that it parses its Also, as suggested in the work of Li et al. (2016), annotating PA is more time-consuming than annotating FA in terms of averaged time for each dependency, since dependencies in the same sentence are correlated and earlier annotated dependencies usually make later annotation easier.", "startOffset": 139, "endOffset": 156}, {"referenceID": 17, "context": "Similar ideas have been extensively explored recently in sequence labeling tasks (Liu et al., 2014; Yang and Vozila, 2014; Marcheggiani and Arti\u00e8res, 2014).", "startOffset": 81, "endOffset": 155}, {"referenceID": 10, "context": "Sassano and Kurohashi (2010), Jiang et al. (2010), and Flannery and Mori (2015) convert partially annotated instances into local dependency/non-dependency classification instances, which may suffer from the lack of non-local correlation between dependencies in a tree.", "startOffset": 30, "endOffset": 50}, {"referenceID": 10, "context": "Sassano and Kurohashi (2010), Jiang et al. (2010), and Flannery and Mori (2015) convert partially annotated instances into local dependency/non-dependency classification instances, which may suffer from the lack of non-local correlation between dependencies in a tree.", "startOffset": 30, "endOffset": 80}, {"referenceID": 10, "context": "Sassano and Kurohashi (2010), Jiang et al. (2010), and Flannery and Mori (2015) convert partially annotated instances into local dependency/non-dependency classification instances, which may suffer from the lack of non-local correlation between dependencies in a tree. Mirroshandel and Nasr (2011) and Majidi and Crane (2013) adopt the complete-then-learn method.", "startOffset": 30, "endOffset": 298}, {"referenceID": 10, "context": "Sassano and Kurohashi (2010), Jiang et al. (2010), and Flannery and Mori (2015) convert partially annotated instances into local dependency/non-dependency classification instances, which may suffer from the lack of non-local correlation between dependencies in a tree. Mirroshandel and Nasr (2011) and Majidi and Crane (2013) adopt the complete-then-learn method.", "startOffset": 30, "endOffset": 326}, {"referenceID": 10, "context": "Sassano and Kurohashi (2010), Jiang et al. (2010), and Flannery and Mori (2015) convert partially annotated instances into local dependency/non-dependency classification instances, which may suffer from the lack of non-local correlation between dependencies in a tree. Mirroshandel and Nasr (2011) and Majidi and Crane (2013) adopt the complete-then-learn method. They use parsers coarsely trained on existing data with FA for completion via constrained decoding. However, our experiments show that this leads to dramatic decrease in parsing accuracy. Nivre et al. (2014) present a constrained decoding procedure for arc-eager transition-based parsers.", "startOffset": 30, "endOffset": 572}, {"referenceID": 10, "context": "Sassano and Kurohashi (2010), Jiang et al. (2010), and Flannery and Mori (2015) convert partially annotated instances into local dependency/non-dependency classification instances, which may suffer from the lack of non-local correlation between dependencies in a tree. Mirroshandel and Nasr (2011) and Majidi and Crane (2013) adopt the complete-then-learn method. They use parsers coarsely trained on existing data with FA for completion via constrained decoding. However, our experiments show that this leads to dramatic decrease in parsing accuracy. Nivre et al. (2014) present a constrained decoding procedure for arc-eager transition-based parsers. However, their work focuses on allowing their parser to effectively exploit external constraints during the evaluation phase. In this work, we directly employ their method and show that constrained decoding is effective for LTPar and thus irresponsible for its ineffectiveness in learning PA. Directly learning from PA based on constrained decoding is previously proposed by Jiang et al. (2013) for Chinese word segmentation, which is treated as a character-level sequence labeling problem.", "startOffset": 30, "endOffset": 1048}, {"referenceID": 10, "context": "Sassano and Kurohashi (2010), Jiang et al. (2010), and Flannery and Mori (2015) convert partially annotated instances into local dependency/non-dependency classification instances, which may suffer from the lack of non-local correlation between dependencies in a tree. Mirroshandel and Nasr (2011) and Majidi and Crane (2013) adopt the complete-then-learn method. They use parsers coarsely trained on existing data with FA for completion via constrained decoding. However, our experiments show that this leads to dramatic decrease in parsing accuracy. Nivre et al. (2014) present a constrained decoding procedure for arc-eager transition-based parsers. However, their work focuses on allowing their parser to effectively exploit external constraints during the evaluation phase. In this work, we directly employ their method and show that constrained decoding is effective for LTPar and thus irresponsible for its ineffectiveness in learning PA. Directly learning from PA based on constrained decoding is previously proposed by Jiang et al. (2013) for Chinese word segmentation, which is treated as a character-level sequence labeling problem. In this work, we first apply the idea to LGPar and LTPar for directly learning from PA. Directly learning from PA based on a forest-based objective in LLGPar is first proposed by Li et al. (2014), inspired by the idea of ambiguous labeling.", "startOffset": 30, "endOffset": 1340}, {"referenceID": 9, "context": "Hwa (1999) pioneers the idea of exploring PA for constituent grammar induction based on a variant Inside-Outside re-estimation algorithm (Pereira and Schabes, 1992).", "startOffset": 0, "endOffset": 11}, {"referenceID": 9, "context": "Hwa (1999) pioneers the idea of exploring PA for constituent grammar induction based on a variant Inside-Outside re-estimation algorithm (Pereira and Schabes, 1992). Clark and Curran (2006) propose to train a Combinatorial Categorial Grammar parser using partially labeled data only containing predicateargument dependencies.", "startOffset": 0, "endOffset": 190}, {"referenceID": 9, "context": "Hwa (1999) pioneers the idea of exploring PA for constituent grammar induction based on a variant Inside-Outside re-estimation algorithm (Pereira and Schabes, 1992). Clark and Curran (2006) propose to train a Combinatorial Categorial Grammar parser using partially labeled data only containing predicateargument dependencies. Mielens et al. (2015) propose to impute missing dependencies based on Gibbs sampling in order to enable traditional parsers to learn from partial trees.", "startOffset": 0, "endOffset": 348}, {"referenceID": 26, "context": "For example, Nivre et al. (2014) propose a constrained decoding procedure which can also incorporate bracketing constraints, i.", "startOffset": 13, "endOffset": 33}], "year": 2016, "abstractText": "Recently, these has been a surge on studying how to obtain partially annotated data for model supervision. However, there still lacks a systematic study on how to train statistical models with partial annotation (PA). Taking dependency parsing as our case study, this paper describes and compares two straightforward approaches for three mainstream dependency parsers. The first approach is previously proposed to directly train a log-linear graph-based parser (LLGPar) with PA based on a forest-based objective. This work for the first time proposes the second approach to directly training a linear graph-based parse (LGPar) and a linear transition-based parser (LTPar) with PA based on the idea of constrained decoding. We conduct extensive experiments on Penn Treebank under three different settings for simulating PA, i.e., random dependencies, most uncertain dependencies, and dependencies with divergent outputs from the three parsers. The results show that LLGPar is most effective in learning from PA and LTPar lags behind the graphbased counterparts by large margin. Moreover, LGPar and LTPar can achieve best performance by using LLGPar to complete PA into full annotation (FA).", "creator": "LaTeX with hyperref package"}}}