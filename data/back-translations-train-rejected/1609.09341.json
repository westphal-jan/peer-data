{"id": "1609.09341", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Machine Learning Techniques for Stackelberg Security Games: a Survey", "abstract": "The present survey aims at presenting the current machine learning techniques employed in security games domains. Specifically, we focused on papers and works developed by the Teamcore of University of Southern California, which deepened different directions in this field. After a brief introduction on Stackelberg Security Games (SSGs) and the poaching setting, the rest of the work presents how to model a boundedly rational attacker taking into account her human behavior, then describes how to face the problem of having attacker's payoffs not defined and how to estimate them and, finally, presents how online learning techniques have been exploited to learn a model of the attacker.", "histories": [["v1", "Thu, 29 Sep 2016 13:53:26 GMT  (80kb)", "http://arxiv.org/abs/1609.09341v1", null]], "reviews": [], "SUBJECTS": "cs.GT cs.LG", "authors": ["giuseppe de nittis", "francesco trov\\`o"], "accepted": false, "id": "1609.09341"}, "pdf": {"name": "1609.09341.pdf", "metadata": {"source": "META", "title": "Machine Learning Techniques for Stackelberg Security Games: a Survey", "authors": ["Giuseppe De Nittis", "Francesco Trov\u00f2"], "emails": ["}@polimi.it"], "sections": [{"heading": null, "text": "ar Xiv: 160 9.09 341v 1Table of Contents"}, {"heading": "1 Introduction 3", "text": "1.1 Stackelberg paradigm and SSG................. 3 1.2 Poaching...................."}, {"heading": "2 Using human behavior models in solving SSGs 5", "text": "2.1 SUQR: Modeling of an infinitely rational attacker......... 5 2.1.1 Learning SUQR parameters................................ 6 2.2 From MATCH to SHARP.........................................................................................................................................................................................."}, {"heading": "3 Determining attacker\u2019s payoffs exploiting regret\u2013based solutions 13", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4 Online learning 16", "text": "4.1 Handling exploration-exploitation tradeoffs in security games. 16 4.1.1 Problem formulation. 16 4.1.1 Problem formulation. 18 4.1.4. Restless Bandit formulation. 17 4.1.2. Learning model from previous observations of defenders. 18 4.1.3 Restless Multi-Armoured Bandit Problems. 18 4.1.4. 18 4.1.4. Restless Bandit formulation. 17 4.1.2. Learning model from previous observations of defenders. 19 4.1.5. Sufficient condition for indexability. 18. 18. 18. 22. 24. 24. Numerical Evaluation of indexability."}, {"heading": "1 Introduction", "text": "The present study aims to present the current main methods of machine learning used in the field of security games. Specifically, we focused on essays and papers developed by the University of Southern California team core and explored various directions in this area. Among several papers on this topic, e.g. [3, 9, 22, 15, 10, 11], this paper is essentially based on this work [12, 6, 4, 8, 17, 16]. After a brief introduction to Stackelberg Security Games (SSGs) and poaching, the rest of the work is organized according to the various issues dealt with. \u2022 Section 2 shows how to model a tremendously rational attacker taking into account his human behavior. \u2022 Section 3 faces the problem that the attacker's rewards are not defined and how to assess them by examining the defender's regret. \u2022 Section 4 shows how online learning techniques were used to learn an attacker's model."}, {"heading": "1.1 Stackelberg paradigm and SSG", "text": "Normally, in order to represent security scenarios, a certain class of games is assumed, i.e., Stackelberg Games [23]. Here, on the one hand, there is the Defender, who publicly declares a mixed strategy, i.e., a distribution of probability between the actions available to the player, and on the other hand, there is an attacker, who complies with the commitment of the Defender and consequently acts. Such games are called Stackelberg Security Games (SSG). Specifically, in SSGs, the Defender tries to protect a number of T targets from an attacker by optimally allocating a number of R resources, R < T denotes by x = {xt} the strategy of the Defender, where xt is the coverage probability on the target t, the amount of feasible strategies is: X = {x: 0 \u2264 xt \u2264 1, \u0445t xt the f-shaped attack on the opponent if the Defender does not protect it."}, {"heading": "1.2 The poaching setting", "text": "Poaching and illegal overfishing are critical international problems leading to the destruction of ecosystems. For example, three out of nine tiger species have become extinct in the last 100 years and others are now threatened by poaching. [18] Law enforcement agencies in many countries are therefore faced with the challenge of using their limited resources to protect endangered animals and fish stocks. Building on the success of the use of SSGs to protect infrastructure, including airports [14], ports [19] and trains [27], researchers are now applying the game theory to green security areas, such as protecting fishing from overfishing [1,5] and protecting wildlife from poaching [25]. There are several key features in green security areas that raise new research problems.1 The defender faces multiple opponents who repeatedly and frequently carry out illegal activities (attacks), creating the need to go beyond the one-year SSG model."}, {"heading": "2 Using human behavior models in solving SSGs", "text": "In game theory, the opponent is usually portrayed as a perfectly rational actor. In the real world, people are not entirely rational, i.e. their decisions are not simply determined by calculations. The problem addressed here is the representation of an infinitely rational attacker in SSGs. In fact, the state of total rationality is loosened to take an important step in modeling an attacker in the real world, both to extract useful information about data already collected, and to develop new models of attackers to increase the current level of security."}, {"heading": "2.1 SUQR: modeling a boundedly rational attacker", "text": "The QR model predicts a stochastic distribution of the opponent's reaction: the higher the expected value of a target, the more likely the opponent will attack that target. The key parameter of the QR represents the level of rationality in the opponent's reaction: As the predicted response of the QR model adapts to the opponent's optimal action, the best algorithm of the opponent is used until 2013 to calculate a robust defense strategy, guaranteeing that the opponent's loss is tied to his expected value if the opponent deviates from his optimal choice. Specifically, the defender's loss is limited to being no more than a factor of the opponent's loss in its expected value. The key parameter \u03b2 describes how much the defender is prepared to sacrifice if the opponent deviates from the optimal action."}, {"heading": "2.1.1 Learning SUQR parameters", "text": "As is usual in traditional machine learning, the Maximum Probability Estimate (MLE) is used to learn the parameters (w1, w2, w3). Considering the defense strategy x and N samples of the players, the log probability is given by (w1, w2, w3): logL (w1, w2, w3 | x) = N \u2211 j = 1log [qtj (w1, w2, w3 | x)], where tj is the target that in the sample j and qtj (w1, w2, w3 | x) is the probability that the opponent chooses the target tj. Let Nt be the number of persons attacking the target t. Then: logL (w1, w2, w3 | x)], where tj is the target chosen in the sample j and qtj (w1, w2, w3 | x)."}, {"heading": "2.2 From MATCH to SHARP", "text": "Based on the above results, a new model is now being introduced, SHARP [6], which shows: \u2022 Reasons based on the success or failure of the opponent's past actions on exposed parts of the attack surface up to the adversary's adaptability; \u2022 Reasons based on similarity between exposed and non-exposed areas of the attack surface and also includes a discount parameter to mitigate the adversary's lack of exposure to the attack surface sufficiently; \u2022 Integrates a non-linear probability weighting function to capture the actual weighting of the adversary's attack surface. Following the approach presented in the previous section, MLE was applied to capture the weights of the SUQR model based on data collected by our human subjects and determine that the weights of the coverage probability were positive for all experiments. Contrary to intuitively, people were attracted to regions with high probability of coverage, even if targets with very low probability of attack were not."}, {"heading": "2.2.1 Adaptive utility model", "text": "A second major innovation in SHARP is the adaptive nature of the opponent and tackling the problem of attack surface exposure. The attack surface \u03b1 is defined as the n-dimensional space of characteristics used to model the opponent's behavior. A target profile \u03b2\u03b2r, for example, is defined as a point on the attack surface \u03b1 and can be associated with a target. Therefore, exposing the opponent to a set of different target profiles would mean exposing the opponent to a larger part of the attack surface and collecting valuable information about their behavior. While a specific target site \u03b2k \u0445 \u03b1 is defined as a specific region in the 2-d space, only one target profile in a given round can be associated with more than one target with the same target profile in the same round. Observation 1. Opponents who have succeeded in attacking a target with a specific target profile in a round cannot be associated with a similar target profile in the same round."}, {"heading": "2.2.2 SHARP\u2019s utility Computation", "text": "Existing models (such as SUQR) only take into account the actions of the opponent in the round (r \u2212 1) to predict his actions in the round r. However, it is clear that the actions of the opponent in a given round depend on his successes and failures in the past. Therefore, a novel adaptive, probability-weighted subjective utility function is proposed that captures this adaptive nature of the opponent's behavior by capturing the changing trends in attractiveness of different target profiles in rounds. ASUR\u03b2i = (1 \u2212 d) w1f (x\u03b2i) + (1 + d) w2\u03b2i + (1 + d) w3P a \u03b2i + (1 \u2212 d) w4D\u03b2id is a discount parameter based on a measurement of the height of the attack surface. d is low in the initial rounds when the defender does not have enough of the right kind of data, but would gradually increase if more information about the attack surfaces are not available to the attacker."}, {"heading": "2.2.3 Generating defender\u2019s strategies against SHARP", "text": "While SHARP is a counter-model, the defense strategies against such a model must be generated. To this end, the parameters of SHARP are first learned from available data. Subsequently, future turn strategies against the infinitely rational opponent are generated, which are characterized by the learned model parameters by solving the following optimization problem: maxx-X [conservation i-TUdi (x) q R i (w | x)] where qRi (w | x) is the probability that the opponent attacks target i in turn R."}, {"heading": "2.3 Learning adversary models from three defender\u2019s strategies", "text": "Here, a new approach is developed to learn the parameters of the behavior model of a limited rational attacker (thus determining a near-optimal strategy) by observing how the attacker responds to only three strategies of the defender. Note: Although the setting is the same as in the previous sections, because some assumptions have changed, e.g. the adaptability of the attacker, we slightly change the used notation to avoid confusion. Let the attacker use function for each target t, ut (xt) and based on these tools, the probability vector x x responds, the benefit of the attacker under strategy x is defined as ut (xt). After observing defense strategy x, the attacker calculates the benefit for each target t, ut (xt) and based on these tools, he responds to the strategy of the defender. Here, a non-adaptable attacker x is considered."}, {"heading": "2.3.1 Linear utility functions", "text": "Suppose that the utility functions are linear and are denoted by ut (x) = wtx + 0. Suppose that the functions u1 (\u00b7),.., un () are linear. Let us consider all three strategies x, y, z, and X such that for each t < n, | (xt \u2212 yt) (xn \u2212 zn) \u2212 (xn \u2212 yn) (xt \u2212 zt), and for all two different strategies p, q (x, y, z}, it applies to each t < n, (xn \u2212 zn) \u2212 (xn \u2212 yn) (xt \u2212 zt)), and for all other strategies p, q (x, y, z}, it applies to each x."}, {"heading": "1\u2212 \u03b4, each ut(\u00b7) can be uniformly learned within error \u01eb.", "text": "Now any utility function that is continuous and is L-Lipschitz, i.e. for all t and values x and y, | ut (x) \u2212 ut (y) | \u2264 L | x \u2212 y | should be learned. Such utility functions can be uniformly learned up to the error limit by applying the O (L) strategies. For each L-Lipschitz function ut (x) there is a polynomial of degree m = 12L / x, which uniformly approximates ut (x) within the error of rule 3... y (d), y (d + 1) = x (1),..., un (\u00b7) are L-Lipschitz. For d = 12L / x consider all 2d + 1 strategies, y (d + 1) = x (1),."}, {"heading": "2.3.2 Learning the optimal strategy", "text": "Previously, the focus was on the problem of unified learning of the attacker's utility function. Now, it is shown that an accurate estimation of this utility function makes it possible to determine an almost optimal strategy for the defender. Let the defender's utility function on the target t (xt) be designated by vt: [0, 1] \u2192 [\u2212 1, 1]. Given a coverage probability vector x \u00b2 X, the defender receives the utility program on the target t (xt). The total expected benefit value of the defender is: V (x) = x (t) = x (xt) Let's not be the learned attacker's utility function, and D (t) is the predicted attack probability on the target t under strategy x, according to the utility values u \u00b2 t, i.e.: D (t) = eu (t) = eu (t) = eu (t)."}, {"heading": "3 Determining attacker\u2019s payoffs exploiting regret\u2013", "text": "The behaviour of the opponent may become unsafe in the next season. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the player is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe. (...) The behaviour of the opponent is unsafe."}, {"heading": "4 Online learning", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Handling exploration\u2013exploitation tradeoffs in Security Games", "text": "The problem, however, is that this research is not able to meet this challenge by planning the patrol strategies under the RMAB, providing two sufficient conditions for indexability and an algorithm that evaluates the unpredictability. Given the unpredictability of the patrol strategies, it is assumed that the patrol strategies in the area of the RMAB will not be sufficient to assess the unpredictability of the patrol strategies."}, {"heading": "4.1.1 Problem formulation", "text": "There are n targets, which are indexed by N = {1,., n} intensity. Defenders have k patrol resources, which can be used for these n targets. In each turn, defenders choose k targets to protect. Afterwards, defenders have an observation of the number of attack activities for targets they are protecting, and no information for targets they are not protecting. The objective of defenders is to decide which k targets to protect in each turn, in order to catch as many attackers as possible. Due to the partial observation on the part of defenders (defenders \"observation of attack activities is not perfect for targets they are protecting), a hidden variable attack intensity is introduced, which represents the true level of attack intensity at a particular target."}, {"heading": "4.1.2 Learning model from defenders\u2019 previous observations", "text": "In view of the plot history of the defenders and the observation history {ai}, our goal is to learn the transition matrices T 1 and T 0, the observation matrix O and the initial belief \u03c0. Due to the existence of hidden variables {si}, the algorithm of expectation maximization (EM) is used for learning.The update steps are the following: \u03c0 (d + 1) i = P (s1 = i | x; \u03b8 d) T 1 (d + 1) ij = \u2211 T \u2212 1 t = 1: at = 1 P (st = i, st + 1 = j | x; \u03b8 d) \u2211 T \u2212 1 t = 1: at = 1 P (st = i | x; successd) T 0 (d + 1) ij = 1: at = 0 P (st = i \u2212 t = 1 P = 2)."}, {"heading": "4.1.3 Restless multi\u2013armed bandit problems", "text": "In RMABs, each arm represents an n-dimensional problem, so the complexity with the proposed solution is reduced. In each round, the player selects k from n arms (k < n) to activate and receives the reward determined by the state of the activated arms. Afterwards, the states of all arms will move to new states according to certain Markov transition probabilities. The problem is called restless, because the states of passive arms also have a transition like active arms. The goal of the player is to maximize his cumulative reward by deciding which weapons will be activated in each round. It is PSPACE-difficult to find the optimal strategy for general RMABs [13]. An index policy assigns each state an index to x to measure how worthwhile it is to activate an arm in a particular state. In each round, the index policy chooses to select the k arms whose current states have the highest indices, as the index policy reduces the problem of an index to an arm's characteristics."}, {"heading": "4.1.4 Restless bandit formulation", "text": "Each target is considered an arm and defenders can choose weapons for activation (k targets for protection) in each turn. Considering a single arm (target), it is associated with ns (hidden) states, no observations, ns \u00b7 ns transition matrices T1 and T 0, ns \u00b7 no reward matrix O and reward function R (o), o \u00b2 O. For defenders, observation, reward related to observation and state transitions according to T 1. Note that the observation of defenders is not the state. Instead, it is a random variable conditioned on the state, and reveals some information about the state. For gun defenders, no observations are activated, reward 0 and state transitions according to T 0. Since defenders cannot observe the state directly, defenders maintain a belief of states for each target based on the defender's decisions. Faith is updated according to Bayesian rules."}, {"heading": "4.1.5 Sufficient condition for indexability", "text": "Two sufficient conditions for indexability if no = 2 and ns = 2 are provided. Name the transition matrices as T 0 and T 1, observation matrix as O. In our problem O11 > O01, O00 > O10 (higher attack intensity leads to a higher probability of seeing attack activity during patrol); T 111 > T 1 01, T 1 00 > T 1 10, T 0 01, T 0 00 > T 0 00 > T 0 10 (positively correlated arms). Define \u03b1 = max {T 011 \u2212 T 1 01, T 0 11 > T 1 01}. Since it is a two-state problem with S = {0, 1}, x represents the state of belief: x = b (s = 1), which is the probability of being in the state 1.Define \u03b1 1 (x) = xT 1 11 + (1 \u2212 x) T 1 01, which is faith for the next round, if the faith is taken for x and the active action is a precedent x (0)."}, {"heading": "4.1.6 Numerical evaluation of indexability", "text": "Sentence 1: If m < R (0) \u03b2R (no \u2212 1) \u2212 R (0) 1 \u2212 \u03b2, \u03a6 (m) = \u2205; if m > R (no \u2212 1), \u03a6 (m) is the entire state of faith. Thus, it should be determined whether the specified m-range is monotonously increased for m [R (0) \u03b2R (no \u2212 1) \u2212 R (0) 1 \u2212 \u03b2, R (no \u2212 1). Numerically, this limited m-range is discarded and then evaluated when it increases monotonously with the increase of the discarded m. In view of the subsidy m, [m] can be determined by solving a special POMDP model whose conditional probability of observation depends on the state of start and action. The algorithm returns a sentence D, the ns-length vectors d1, d2,.. < d | D |. Each vector di is associated with an optimal action."}, {"heading": "4.1.7 Computation of Whittle index Policy", "text": "Given indexability, the Whittle index can be found by performing a binary search within the range m '[ R (0) \u03b2R (no \u2212 1) \u2212 R (0) 1 \u2212 \u03b2, R (no \u2212 1)]. Given the upper and lower limit lb, the problem with the middle point lb + ub2 is sent as a passive subsidy to the special POMDP solver to find the optimal action for the current belief. If the optimal action is active, the Whittle index can be found by calculating the middle point as lb + ub2 and then selecting the k arms with the highest clues or ub + ub 2. This binary search algorithm can locate the Whittle index with arbitrary precision. Naively, the Whittle index policy can be found by calculating the precision indices of all arms and then selecting the k arms with the highest clues."}, {"heading": "4.1.8 Special POMDP formulation", "text": "Here, the algorithm for calculating the amount of passive action \u03a6 (m) with the subsidy m = = = 1. This problem can be considered a solution to a special POMDP model, the conditional observation probability of which depends on the starting state and action, while the conditional observation probability in our standard POMDPs depends on the end state and action. The initial state is s, the agent takes measures a, and the state transition to s according to P (s \u2032 | s, a). However, the observation o that the agent receives during this process is dependent on s and a in our special POMDPs; while it depends on s \u00b2 and a in standard POMDPs, the ability of the POMDP is straightforward. The special formulation for our problem is S = {0, 1,.. The state space is ns \u2212 1}. The scope of action is A = {0, 1} where the observation is o, where a = 0 is the passive action (not protect) and a = 1 is active action."}, {"heading": "4.1.9 Value iteration for the special POMDP", "text": "Deviating from the standard POMDP formulation, the faith update in the specific POMDP formulation reads: b \"(s\") = \"S\" b (s \") P (o\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s"}, {"heading": "4.1.10 Planning from POMDP view", "text": "Each individual target can be modeled as a special POMDP model. Since these POMDP models can be combined with all targets to form a special POMDP model that describes the entire problem, the solution of this particular POMDP model leads to an exact optimal strategy of the defenders. \u2022 The state space is S = S1 \u00b7 S2 \u00b7.. \u00b7 Sn. Denote s = (s1, s2,.., sn) The action space is A = (a1, a2,..) The state space is S = S1 \u00b7 S2 \u00b7. \u00b7 Sn. Denote s = (s1, s2,., sn) the action space is A = (a1, a2,.) the state space is a)."}, {"heading": "4.2 Online planning for optimal protector strategies in resource conservation games", "text": "In many areas, such as illegal fishing, the promoters know more about the distribution and wealth of resources than the promoters, making it extremely difficult for the promoters to optimally use their assets for patrols and prohibitions. Fortunately, the promoters often carry out illegal promotions, so that the promoters can learn more about the wealth of resources by observing the behaviour of the promoters. In areas of resource conservation, the promoter often does not know about the distribution of resources, while the promoter may have more information about it, e.g. to prevent illegal fishing. Our goal is to provide the protector with an optimal strategy for the use of resources (e.g. patrol) because she does not know about the distribution of resources."}, {"heading": "4.2.1 Problem formulation", "text": "The goal is to establish an online policy for the extractor in order to maximize its usefulness. In our model, the amount of resources will be fixed in each location and the extractor will have a complete knowledge of this distribution. The extractor will have a complete knowledge of this distribution, and the extractor will have a complete knowledge of this distribution. The protector will have to learn this distribution by observing the behavior of the extractor. There is a limited period of time in which he will maximize the knowledge of each other's activities. In our model, the amount of resources will be fixed in each location and the extractor will have full knowledge of this distribution. The protector will have to learn this distribution by observing the behavior of the extractor. There are n sites ranging from N = {1, 2,.,.} that represent the sites that represent the sites that represent the locations of the natural resources in question: the extractor wants to steal resources from these sites."}, {"heading": "4.2.2 GMOP algorithm", "text": "The size of the Utility Space U is mn, and the size of the Count Space is O (Tnn!). Calculation costs of the latest POMDP solvers will soon become prohibitive for us as the problem size grows. Xi and Veness [20] have proposed the POMCP algorithm, which provides high-quality solutions for large POMDPs. The POMCP algorithm uses a particle filter to approximate the state of belief. Then, it uses Monte Carlo Tree Search (MCTS) for online planning, where state samples are taken from the particle filter and the action with the highest expected benefit is selected based on Monte Carlo simulations. However, the particle filter is only an approximation of the true state of belief and is likely to go further away from the actual state of belief, as the game continues, especially when most particles are exhausted and new particles need to be added. Adding new particles will either follow the worse state of the faith state if the distribution does not result in additional particles."}, {"heading": "4.2.3 Applying Gibbs sampling in GMOP", "text": "Let Bt (u) be the probability distribution that represents the Protector's beliefs about the true benefits at the beginning of Round 1. Let B be the previous distribution of faith and B be the posterior distribution of faith. Our Bayesian rule of faith updates B from B and the observation is explicit: B (s) (u, C) = (o | s). If an and ot represent the actions that the Protector and the exploiter choose to take in Round t: Bt (u) s (s), s (s), s (s), s (s), b (u \u2212 aj), it is easy and the exploiter decides to take in Round t: Bt (u) = 1 (u) that the ability in Round t (u) is impossible. (u) It is impossible to take Bt (ot \u2212 1 | s), (u \u2212 aj), is probable."}], "references": [{"title": "Addressing scalability and robustness in security games with multiple boundedly rational adversaries", "author": ["Matthew Brown", "William B Haskell", "Milind Tambe"], "venue": "In Decision and Game Theory for Security,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Explaining the gibbs sampler", "author": ["George Casella", "Edward I George"], "venue": "The American Statistician,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1992}, {"title": "When security games go green: Designing defender strategies to prevent poaching and illegal fishing", "author": ["Fei Fang", "Peter Stone", "Milind Tambe"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Robust protection of fisheries with compass", "author": ["William B Haskell", "Debarun Kar", "Fei Fang", "Milind Tambe", "Sam Cheung", "Elizabeth Denicola"], "venue": "In AAAI,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2014}, {"title": "A game of thrones: when human behavior models compete in repeated stackelberg security games", "author": ["Debarun Kar", "Fei Fang", "Francesco Delle Fave", "Nicole Sintov", "Milind Tambe"], "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Quantal response equilibria for normal form games", "author": ["Richard D McKelvey", "Thomas R Palfrey"], "venue": "Games and economic behavior,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1995}, {"title": "Making the most of our regrets: Regret-based solutions to handle payoff uncertainty and elicitation in green security games", "author": ["Thanh H Nguyen", "Francesco M Delle Fave", "Debarun Kar", "Aravind S Lakshminarayanan", "Amulya Yadav", "Milind Tambe", "Noa Agmon", "Andrew J Plumptre", "Margaret Driciru", "Fred Wanyama"], "venue": "In Decision and Game Theory for Security,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Capture: A new predictive anti-poaching tool for wildlife protection", "author": ["Thanh H Nguyen", "Arunesh Sinha", "Shahrzad Gholami", "Andrew Plumptre", "Lucas Joppa", "Milind Tambe", "Margaret Driciru", "Fred Wanyama", "Aggrey Rwetsiba", "Rob Critchlow"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2016}, {"title": "Conquering adversary behavioral uncertainty in security games: An efficient modeling robust based algorithm. 2016", "author": ["Thanh H Nguyen", "Arunesh Sinha", "Milind Tambe"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Analyzing the effectiveness of adversary modeling in security games", "author": ["Thanh Hong Nguyen", "Rong Yang", "Amos Azaria", "Sarit Kraus", "Milind Tambe"], "venue": "In AAAI,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "The complexity of optimal queuing network control", "author": ["Christos H Papadimitriou", "John N Tsitsiklis"], "venue": "Mathematics of Operations Research,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Deployed armor protection: the application of a game theoretic model for security at the los angeles international airport", "author": ["James Pita", "Manish Jain", "Janusz Marecki", "Fernando Ord\u00f3\u00f1ez", "Christopher Portway", "Milind Tambe", "Craig Western", "Praveen Paruchuri", "Sarit Kraus"], "venue": "In Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems: industrial track,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Online planning for optimal protector strategies in resource conservation games. In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems, pages 733\u2013740", "author": ["Yundi Qian", "William B Haskell", "Albert Xin Jiang", "Milind Tambe"], "venue": "International Foundation for Autonomous Agents and Multiagent Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Restless poachers: Handling exploration-exploitation tradeoffs in security domains", "author": ["Yundi Qian", "Chao Zhang", "Bhaskar Krishnamachari", "Milind Tambe"], "venue": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Protect: A deployed game theoretic system to protect the ports of the united states", "author": ["Eric Shieh", "Bo An", "Rong Yang", "Milind Tambe", "Craig Baldwin", "Joseph Di- Renzo", "Ben Maule", "Garrett Meyer"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems- Volume", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2012}, {"title": "Monte-carlo planning in large pomdps", "author": ["David Silver", "Joel Veness"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "On the stackelberg strategy in nonzero-sum games", "author": ["Marwan Simaan", "Jose B Cruz Jr."], "venue": "Journal of Optimization Theory and Applications,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1973}, {"title": "Learning adversary behavior in security games: A pac model perspective", "author": ["Arunesh Sinha", "Debarun Kar", "Milind Tambe"], "venue": "arXiv preprint arXiv:1511.00043,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Restless bandits: Activity allocation in a changing world", "author": ["Peter Whittle"], "venue": "Journal of applied probability,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Adaptive resource allocation for wildlife protection against illegal poachers", "author": ["Rong Yang", "Benjamin Ford", "Milind Tambe", "Andrew Lemieux"], "venue": "In Proceedings of the 2014 international conference on Autonomous agents and multi-agent systems,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2014}, {"title": "Computing optimal strategy against quantal response in security games", "author": ["Rong Yang", "Fernando Ordonez", "Milind Tambe"], "venue": "In Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Trusts: Scheduling randomized patrols for fare inspection in transit systems using game theory", "author": ["Zhengyu Yin", "Albert Xin Jiang", "Milind Tambe", "Christopher Kiekintveld", "Kevin Leyton-Brown", "Tuomas Sandholm", "John P Sullivan"], "venue": "AI Magazine,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 2, "endOffset": 24}, {"referenceID": 7, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 2, "endOffset": 24}, {"referenceID": 17, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 2, "endOffset": 24}, {"referenceID": 8, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 4, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 6, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 13, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 12, "context": ", [3, 9, 22, 15, 10, 11], this paper is essentially based on these works [12, 6, 4, 8, 17, 16].", "startOffset": 73, "endOffset": 94}, {"referenceID": 16, "context": "In zero\u2013sum games, the Leader\u2013follower equilibrium coincides with the Nash equilibrium [21] and the maxmin/minmax strategies.", "startOffset": 87, "endOffset": 91}, {"referenceID": 11, "context": "Building upon the success of applying SSGs to protect infrastructure including airports [14], ports [19] and trains [27], researchers are now applying game theory to green security domains, e.", "startOffset": 88, "endOffset": 92}, {"referenceID": 14, "context": "Building upon the success of applying SSGs to protect infrastructure including airports [14], ports [19] and trains [27], researchers are now applying game theory to green security domains, e.", "startOffset": 100, "endOffset": 104}, {"referenceID": 21, "context": "Building upon the success of applying SSGs to protect infrastructure including airports [14], ports [19] and trains [27], researchers are now applying game theory to green security domains, e.", "startOffset": 116, "endOffset": 120}, {"referenceID": 0, "context": ", protecting fisheries from over\u2013 fishing [1, 5] and protecting wildlife from poaching [25].", "startOffset": 42, "endOffset": 48}, {"referenceID": 3, "context": ", protecting fisheries from over\u2013 fishing [1, 5] and protecting wildlife from poaching [25].", "startOffset": 42, "endOffset": 48}, {"referenceID": 19, "context": ", protecting fisheries from over\u2013 fishing [1, 5] and protecting wildlife from poaching [25].", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "1 SUQR: modeling a boundedly rational attacker In SSGs, attacker bounded rationality is often modeled via behavior models such as Quantal Response (QR) [7].", "startOffset": 152, "endOffset": 155}, {"referenceID": 9, "context": "Inspired by the idea of SEU, it has been proposed [12] a subjective utility function of the adversary for SSG as the following:", "startOffset": 50, "endOffset": 54}, {"referenceID": 4, "context": "2 From MATCH to SHARP Starting from the above results, a new model is now introduced, SHARP [6], which:", "startOffset": 92, "endOffset": 95}, {"referenceID": 0, "context": "Let the utility function of the attacker for target t \u2208 T be denoted by ut : [0, 1] \u2192 R.", "startOffset": 77, "endOffset": 83}, {"referenceID": 0, "context": "\u00fbt : [0, 1] \u2192 R uniformly approximates or uniformly learns ut(\u00b7) within an error of \u01eb, if \u2200x \u2208 [0, 1], |\u00fbt(x) \u2212 ut(x)| \u2264 \u01eb.", "startOffset": 5, "endOffset": 11}, {"referenceID": 0, "context": "\u00fbt : [0, 1] \u2192 R uniformly approximates or uniformly learns ut(\u00b7) within an error of \u01eb, if \u2200x \u2208 [0, 1], |\u00fbt(x) \u2212 ut(x)| \u2264 \u01eb.", "startOffset": 95, "endOffset": 101}, {"referenceID": 0, "context": "Let the utility function of the defender on target t \u2208 T be denoted by vt : [0, 1] \u2192 [\u22121, 1].", "startOffset": 76, "endOffset": 82}, {"referenceID": 20, "context": "The corresponding optimal strategies for the defender given these payoff samples, denoted x, are then computed using the PASAQ algorithm [26] to obtain a finite set S of sampled constraints.", "startOffset": 137, "endOffset": 141}, {"referenceID": 10, "context": "It is PSPACE\u2013hard to find the optimal strategy to general RMABs [13].", "startOffset": 64, "endOffset": 68}, {"referenceID": 18, "context": "Whittle proposed a heuristic index policy for RMABs by considering the Lagrangian relaxation of the problem [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 15, "context": "Silver and Veness [20] have proposed POMCP algorithm, which provides high quality solutions and is scalable to large POMDPs.", "startOffset": 18, "endOffset": 22}, {"referenceID": 15, "context": "Silver and Veness [20] have proposed the POMCP algorithm, which provides high quality solutions for large POMDPs.", "startOffset": 18, "endOffset": 22}, {"referenceID": 1, "context": "Gibbs sampling [2] is a Markov chain Monte Carlo (MCMC) algorithm for sampling from multivariate probability distributions.", "startOffset": 15, "endOffset": 18}], "year": 2016, "abstractText": "The present survey aims at presenting the current machine learning techniques employed in security games domains. Specifically, we focused on papers and works developed by the Teamcore of University of Southern California, which deepened different directions in this field. After a brief introduction on Stackelberg Security Games (SSGs) and the poaching setting, the rest of the work presents how to model a boundedly rational attacker taking into account her human behavior, then describes how to face the problem of having attacker\u2019s payoffs not defined and how to estimate them and, finally, presents how online learning techniques have been exploited to learn a model of the attacker.", "creator": "LaTeX with hyperref package"}}}