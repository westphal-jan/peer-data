{"id": "1609.09365", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks", "abstract": "This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach [Ondruska, 2016], we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work.", "histories": [["v1", "Thu, 29 Sep 2016 14:39:10 GMT  (583kb,D)", "http://arxiv.org/abs/1609.09365v1", null], ["v2", "Thu, 9 Feb 2017 21:35:15 GMT  (569kb,D)", "http://arxiv.org/abs/1609.09365v2", null], ["v3", "Wed, 19 Apr 2017 14:31:32 GMT  (578kb,D)", "http://arxiv.org/abs/1609.09365v3", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG cs.RO", "authors": ["julie dequaire", "dushyant rao", "peter ondruska", "dominic wang", "ingmar posner"], "accepted": false, "id": "1609.09365"}, "pdf": {"name": "1609.09365.pdf", "metadata": {"source": "CRF", "title": "Deep Tracking on the Move: Learning to Track the World from a Moving Vehicle using Recurrent Neural Networks", "authors": ["Julie Dequaire", "Dushyant Rao", "Peter Ondr\u00fa\u0161ka", "Dominic Zeng Wang", "Ingmar Posner"], "emails": ["ingmar@robots.ox.ac.uk"], "sections": [{"heading": null, "text": "In fact, it is the case that most people who are able, are able, are able to move, are able to survive themselves."}, {"heading": "II. RELATED WORK", "text": "A number of previous studies have examined the problem of model-free tracking of objects in the vicinity of an autonomous vehicle ([3], [4], [5]). Typically, these approaches follow the traditional paradigm of a multi-component pipeline, with separate components for parameterizing and detecting objects, associate new metrics to existing tracks, and estimate the state of each individual object by exploiting the neural networks within the framework of cumbersome and unnecessary refusal modes for the tracking algorithm. Recent's work suggests replacing these multiple stages with an end-to-end learning framework by activating the neural networks to form the commissioning of raw materials into an unnecessary failure (1)."}, {"heading": "III. TRACKING PROBLEM FORMULATION", "text": "The problem we address in this paper is to uncover the true state of the world, in relation to a 2D occupation network yt, since a sequence of partially observed states of the environment x1: t is calculated by raw sensor measurements. In particular, we solve the problem for P (yt / x1: t), which has given the probability of the true, unoccupied state of the world in the time t in which a sequence of partial observations takes place in all previous time steps. This formulation can also be used to predict future states by using the empty input for xt + n.P (yt / x1: t) for P (yt / x1: t)."}, {"heading": "IV. TECHNICAL OVERVIEW", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Deep Tracking from a Static Sensor", "text": "First, we consider [2] the basic DeepTracking architecture for real-world applications, which we extend to a movable platform. At each step t, the partially observed grid used as input into the network is calculated from raw 2D laser scans by beam tracking. Cells where a measurement ends are marked as occupied, all cells from the source of the sensor to the end of the beam are marked as free, and cells beyond the beam are marked as unobserved. The input text is processed by a multilayered network depicted in Figure 2, whereby the module of the space transformer is only used in the scenario of the moving vehicle. The architecture originally proposed in [1] is scaled with extended revolutions [14] and a variant of gated recurrent units ([9], [15]) to allow simultaneous tracking of objects of different sizes such as cars and pedestrians."}, {"heading": "B. Deep Tracking from a Moving Vehicle", "text": "In the static scenario, information about an object located in an index is normally stored in an adjacent spatial frame, and the latent update would then occur in accordance with the observed relative motion of the input object. (Figure 4) In the static scenario, the receptive fields of each neuron are viewed in a hidden state.) The latent update will then take place in accordance with the observed relative motion of the inserted object. (Figure 4) The dynamics of the scene are viewed in a coherent frame viewed from a local sensor frame."}, {"heading": "C. Training", "text": "In both static and dynamic cases, the input sequence x1: t is presented to the network and trained to predict the next n input images xt + 1: t + n. The binary entropy loss is calculated only on the visible part of the output and propagated backwards, which is achieved by simply masking the prediction yt with xt and multiplying the resulting grid element by the allocation part grid xt, occ. By using a loss that encourages the model to predict the state of the environment in the future, the model is forced to capture the movement of each object in the hidden representation. In the static sensor scenario, the error is propagated backwards only on the available ground, i.e. on the visible part of the space. In the case of a moving sensor, however, an additional limitation must be imposed to take into account the fact that the robot moves in future images, discovers new space that lies outside the current field of view, i.e., on the visible part of the space."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "In this section we perform an experimental validation of both the base line DeepTracking and the STM-based variant proposed in this article. No spatial transformation is required for a stationary vehicle and both are identical."}, {"heading": "A. Static Vehicle", "text": "For the static case, we consider an architecture with three hidden layers of 16 GRU cards, each with a total volume of 48 million Euros and a total volume of 11 million Euros, divided into the three levels (from bottom to top), with the individual areas each having a total volume of 3 million Euros, 7 million Euros and 11 million Euros."}, {"heading": "B. Moving vehicle", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "VI. CONCLUSION", "text": "In this paper, we have proposed an approach to object tracking for a mobile robot operating in densely populated urban environments, building on the previously proposed DeepTracking Framework ([1], [2]). Unlike traditional techniques, which use a multi-stage pipeline, this approach is learned end-to-end with limited architectural decisions. By using a spatial transformer module, the model is able to use loud estimates of visual ego as a proxy for real vehicle movements. Experimental results show that our DeepTracking method is advantageous for accurately predicting future conditions and show that the model can capture the position and movement of cars, pedestrians, bikers and buses, even when they are completely obscured."}, {"heading": "ACKNOWLEDGEMENT", "text": "The authors would like to thank the UK Engineering and Physical Sciences Research Council (EPSRC), the Doctoral Training Programme (DTP) and the DFR-01420 Programme Grant as well as the Advanced Research Computing Services of Oxford University for their support."}], "references": [{"title": "Deep tracking: Seeing beyond seeing using recurrent neural networks", "author": ["P. Ondr\u00fa\u0161ka", "I. Posner"], "venue": "The Thirtieth AAAI Conference on Artificial Intelligence (AAAI), Phoenix, Arizona USA, February 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end tracking and semantic segmentationusing recurrent neural networks", "author": ["P. Ondr\u00fa\u0161ka", "J. Dequaire", "D.Z. Wang", "I. Posner"], "venue": "arXiv preprint arXiv:1604.05091, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Model-free detection and tracking of dynamic objects with 2d lidar", "author": ["D.Z. Wang", "I. Posner", "P. Newman"], "venue": "The International Journal of Robotics Research, vol. 34, no. 7, pp. 1039\u20131063, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Model based vehicle detection and tracking for autonomous urban driving", "author": ["A. Petrovskaya", "S. Thrun"], "venue": "Autonomous Robots, vol. 26, no. 2, pp. 123\u2013139, 2009.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in neural information processing systems, 2012, pp. 1097\u20131105.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Context-dependent pretrained deep neural networks for large-vocabulary speech recognition", "author": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 1, pp. 30\u201342, 2012.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "End-to-end text recognition with convolutional neural networks", "author": ["T. Wang", "D.J. Wu", "A. Coates", "A.Y. Ng"], "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on. IEEE, 2012, pp. 3304\u20133308.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. Van Merri\u00ebnboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "arXiv preprint  arXiv:1406.1078, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Spatio-temporal video autoencoder with differentiable memory", "author": ["V. Patraucean", "A. Handa", "R. Cipolla"], "venue": "arXiv preprint arXiv:1511.06309, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep predictive coding networks for video prediction and unsupervised learning", "author": ["W. Lotter", "G. Kreiman", "D. Cox"], "venue": "arXiv preprint arXiv:1605.08104, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Spatial transformer networks", "author": ["M. Jaderberg", "K. Simonyan", "A. Zisserman"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 2017\u20132025.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-scale context aggregation by dilated convolutions", "author": ["F. Yu", "V. Koltun"], "venue": "arXiv preprint arXiv:1511.07122, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Convolutional lstm network: A machine learning approach for precipitation nowcasting", "author": ["S. Xingjian", "Z. Chen", "H. Wang", "D.-Y. Yeung", "W.-k. Wong", "W.-c. WOO"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 802\u2013810.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Module for spatial transformer networks", "author": ["M. Oquab"], "venue": "https://github. com/qassemoquab/stnbhwd/.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 0}], "referenceMentions": [{"referenceID": 0, "context": "Inspired by the recently presented DeepTracking approach ([1], [2]), we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle.", "startOffset": 58, "endOffset": 61}, {"referenceID": 1, "context": "Inspired by the recently presented DeepTracking approach ([1], [2]), we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle.", "startOffset": 63, "endOffset": 66}, {"referenceID": 2, "context": "object detection, semantic classification, data association, state estimation and motion modelling, occupancy grid generation) in order to represent the state and evolution of the world ([3], [4], [5]).", "startOffset": 192, "endOffset": 195}, {"referenceID": 3, "context": "object detection, semantic classification, data association, state estimation and motion modelling, occupancy grid generation) in order to represent the state and evolution of the world ([3], [4], [5]).", "startOffset": 197, "endOffset": 200}, {"referenceID": 4, "context": "Recent advances in machine learning, particularly those of deep neural networks, have demonstrated the ability to capture complex structure in the real world, and have led to significant improvements in numerous computer vision and natural language processing applications ([6], [7], [8]).", "startOffset": 274, "endOffset": 277}, {"referenceID": 5, "context": "Recent advances in machine learning, particularly those of deep neural networks, have demonstrated the ability to capture complex structure in the real world, and have led to significant improvements in numerous computer vision and natural language processing applications ([6], [7], [8]).", "startOffset": 279, "endOffset": 282}, {"referenceID": 6, "context": "Recent advances in machine learning, particularly those of deep neural networks, have demonstrated the ability to capture complex structure in the real world, and have led to significant improvements in numerous computer vision and natural language processing applications ([6], [7], [8]).", "startOffset": 284, "endOffset": 287}, {"referenceID": 1, "context": "In recent work, [2] took an alternative approach and presented an end-to-end fully and efficiently trainable framework for learning a model of the world dynamics, building on the original DeepTracking work by [1].", "startOffset": 16, "endOffset": 19}, {"referenceID": 0, "context": "In recent work, [2] took an alternative approach and presented an end-to-end fully and efficiently trainable framework for learning a model of the world dynamics, building on the original DeepTracking work by [1].", "startOffset": 209, "endOffset": 212}, {"referenceID": 1, "context": "We extend the neural network architecture proposed in [2] to account for the egomotion of the sensor frame as it moves in the world frame, and demonstrate improved tracking accuracy as compared to previous work.", "startOffset": 54, "endOffset": 57}, {"referenceID": 0, "context": "highlights related work and Section III summarises the problem definition and DeepTracking framework first presented in ([1], [2]).", "startOffset": 121, "endOffset": 124}, {"referenceID": 1, "context": "highlights related work and Section III summarises the problem definition and DeepTracking framework first presented in ([1], [2]).", "startOffset": 126, "endOffset": 129}, {"referenceID": 2, "context": "A number of previous works have explored the problem of model-free tracking of objects in the environment of an autonomous vehicle ([3], [4], [5]).", "startOffset": 137, "endOffset": 140}, {"referenceID": 3, "context": "A number of previous works have explored the problem of model-free tracking of objects in the environment of an autonomous vehicle ([3], [4], [5]).", "startOffset": 142, "endOffset": 145}, {"referenceID": 0, "context": "Recent work proposes to replace these multiple stages with an end-to-end learning framework known as DeepTracking, by leveraging neural networks to directly learn the mapping from raw laser input to an unoccluded occupancy grid ([1], [2]), even with relatively small amounts of data.", "startOffset": 229, "endOffset": 232}, {"referenceID": 1, "context": "Recent work proposes to replace these multiple stages with an end-to-end learning framework known as DeepTracking, by leveraging neural networks to directly learn the mapping from raw laser input to an unoccluded occupancy grid ([1], [2]), even with relatively small amounts of data.", "startOffset": 234, "endOffset": 237}, {"referenceID": 7, "context": "The approach utilises an RNN architecture using gated recurrent units [9] to capture the state and evolution of the world in a sequence of laser scan frames.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "DeepTracking shares similarities with deep learning approaches to predictive video modelling ([11], [12]), in that it is trained to predict the future state of the world based on current input data.", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "DeepTracking shares similarities with deep learning approaches to predictive video modelling ([11], [12]), in that it is trained to predict the future state of the world based on current input data.", "startOffset": 100, "endOffset": 104}, {"referenceID": 1, "context": "While this eliminates the need to design individual components by hand, the model assumes a static vehicle [2].", "startOffset": 107, "endOffset": 110}, {"referenceID": 0, "context": "We scale up the RNN-based models proposed by ([1], [2]) for real-world application on dynamic vehicles, and exploit Spatial Transformer modules [13], which allow the internal memory state representations to be spatially transformed according to the estimated egomotion.", "startOffset": 46, "endOffset": 49}, {"referenceID": 1, "context": "We scale up the RNN-based models proposed by ([1], [2]) for real-world application on dynamic vehicles, and exploit Spatial Transformer modules [13], which allow the internal memory state representations to be spatially transformed according to the estimated egomotion.", "startOffset": 51, "endOffset": 54}, {"referenceID": 10, "context": "We scale up the RNN-based models proposed by ([1], [2]) for real-world application on dynamic vehicles, and exploit Spatial Transformer modules [13], which allow the internal memory state representations to be spatially transformed according to the estimated egomotion.", "startOffset": 144, "endOffset": 148}, {"referenceID": 1, "context": "1) A more in-depth analysis of the performance of DeepTracking in the case of a static vehicle, building on the experiments presented in [2].", "startOffset": 137, "endOffset": 140}, {"referenceID": 0, "context": "We refer the reader to [1] and [2] for further details on the RNN and the training procedure.", "startOffset": 23, "endOffset": 26}, {"referenceID": 1, "context": "We refer the reader to [1] and [2] for further details on the RNN and the training procedure.", "startOffset": 31, "endOffset": 34}, {"referenceID": 1, "context": "In the next section, we build upon [2] to deploy the DeepTracking paradigm on a real-world moving platform.", "startOffset": 35, "endOffset": 38}, {"referenceID": 1, "context": "First, we revisit [2] to detail the baseline DeepTracking architecture for real world application, which we extend to a moving platform.", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians.", "startOffset": 40, "endOffset": 43}, {"referenceID": 11, "context": "The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians.", "startOffset": 83, "endOffset": 87}, {"referenceID": 7, "context": "The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians.", "startOffset": 128, "endOffset": 131}, {"referenceID": 12, "context": "The architecture originally proposed in [1] is scaled up with dilated convolutions [14] and a variant of gated recurrent units ([9], [15]) to allow for the simultaneous tracking of different-sized objects such as cars and pedestrians.", "startOffset": 133, "endOffset": 137}, {"referenceID": 10, "context": "We aid the network in decoupling egomotion and object motion in this way by introducing a Spatial Transformer [13] module (STM) in the hidden state (Figure 2).", "startOffset": 110, "endOffset": 114}, {"referenceID": 10, "context": "In the original work by [13], the STM is introduced as a learnable module for actively spatially transforming feature maps to aid tasks such as the classification of distorted datasets.", "startOffset": 24, "endOffset": 28}, {"referenceID": 2, "context": "75 1 Model-free tracker [4]", "startOffset": 24, "endOffset": 27}, {"referenceID": 1, "context": "RNN16 [2]", "startOffset": 6, "endOffset": 9}, {"referenceID": 0, "context": "We compare a number of different architectures ranging from the original [1] to the proposed model in [2], and compare performance on the task of predicting the observable nearfuture scene occupancy y\u2032 t+1:t+n, given the input sequence x1:t.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "We compare a number of different architectures ranging from the original [1] to the proposed model in [2], and compare performance on the task of predicting the observable nearfuture scene occupancy y\u2032 t+1:t+n, given the input sequence x1:t.", "startOffset": 102, "endOffset": 105}, {"referenceID": 2, "context": "There is a notable step change in performance with neural architectures compared to a state-of-the art model-free tracking pipeline approach by [4].", "startOffset": 144, "endOffset": 147}, {"referenceID": 0, "context": "If we increase the capacity of the original RNN architecture of [1] (RNN16) from 16 to 48 feature maps (RNN48), we do not find any significant performance increase.", "startOffset": 64, "endOffset": 67}, {"referenceID": 1, "context": "In comparison, replacing the standard RNN hidden state with the modified gated recurrent units (GRU3) detailed in [2] provides significant improvement in prediction ability.", "startOffset": 114, "endOffset": 117}, {"referenceID": 13, "context": "Our architecture is implemented on Torch and uses the Spatial Transformer GPU-implementation of [16].", "startOffset": 96, "endOffset": 100}, {"referenceID": 0, "context": "In this paper, we have proposed an approach to perform object tracking for a mobile robot travelling in crowded urban environments, building on the previously proposed DeepTracking framework ([1], [2]).", "startOffset": 192, "endOffset": 195}, {"referenceID": 1, "context": "In this paper, we have proposed an approach to perform object tracking for a mobile robot travelling in crowded urban environments, building on the previously proposed DeepTracking framework ([1], [2]).", "startOffset": 197, "endOffset": 200}], "year": 2016, "abstractText": "This paper presents an end-to-end approach for tracking static and dynamic objects for an autonomous vehicle driving through crowded urban environments. Unlike traditional approaches to tracking, this method is learned end-to-end, and is able to directly predict a full unoccluded occupancy grid map from raw laser input data. Inspired by the recently presented DeepTracking approach ([1], [2]), we employ a recurrent neural network (RNN) to capture the temporal evolution of the state of the environment, and propose to use Spatial Transformer modules to exploit estimates of the egomotion of the vehicle. Our results demonstrate the ability to track a range of objects, including cars, buses, pedestrians, and cyclists through occlusion, from both moving and stationary platforms, using a single learned model. Experimental results demonstrate that the model can also predict the future states of objects from current inputs, with greater accuracy than previous work.", "creator": "LaTeX with hyperref package"}}}