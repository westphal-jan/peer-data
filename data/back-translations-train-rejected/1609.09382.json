{"id": "1609.09382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks", "abstract": "This work focuses on the rapid development of linguistic annotation tools for resource-poor languages. We experiment several cross-lingual annotation projection methods using Recurrent Neural Networks (RNN) models. The distinctive feature of our approach is that our multilingual word representation requires only a parallel corpus between the source and target language. More precisely, our method has the following characteristics: (a) it does not use word alignment information, (b) it does not assume any knowledge about foreign languages, which makes it applicable to a wide range of resource-poor languages, (c) it provides truly multilingual taggers. We investigate both uni- and bi-directional RNN models and propose a method to include external information (for instance low level information from POS) in the RNN to train higher level taggers (for instance, super sense taggers). We demonstrate the validity and genericity of our model by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers.", "histories": [["v1", "Thu, 29 Sep 2016 15:19:13 GMT  (860kb,D)", "http://arxiv.org/abs/1609.09382v1", "accepted to COLING 2016"]], "COMMENTS": "accepted to COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["othman zennaki", "nasredine semmar", "laurent besacier"], "accepted": false, "id": "1609.09382"}, "pdf": {"name": "1609.09382.pdf", "metadata": {"source": "CRF", "title": "Inducing Multilingual Text Analysis Tools Using Bidirectional Recurrent Neural Networks", "authors": ["Othman Zennaki", "Laurent Besacier"], "emails": ["othman.zennaki@cea.fr", "nasredine.semmar@cea.fr", "laurent.besacier@imag.fr"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is so that most people are able to understand themselves and to understand what they are doing to change and change the world, \"he said in an interview with the\" New York Times, \"in which he said the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \""}, {"heading": "2 Related Work", "text": "In this year it has come to the point where we will see ourselves in a position to retaliate, \"he said in an interview with\" Welt am Sonntag, \"in which he cared for the\" world \"and the\" world, \"in which he cared for the\" world \"and the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" world, \"in which he cared for the\" and the \"world,\" in which he cared for the \"world,\" in which he cared for the \"world,\" in which he cared for the \"and in which he cared for the\" and in which he cared for the \"world,\" in which he cared for the \"and in which he cared for the\" and in which he cared for the \"and in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which he cared in which"}, {"heading": "3 Unsupervised Approach Overview", "text": "To prevent markup information from being projected from deterministic and error-prone word alignments, we propose that the word alignment information is basically presented in a recursive neural network architecture, the idea being to implement a recursive neural network as a tool for multilingual sequence marking (we study POS tagging and SST tagging), and before describing our lingual (multilingual, if a multiparallel corpus is used) neural network tag, we present the simple method of cross-lingual projection that is the starting point of this work."}, {"heading": "3.1 Baseline Cross-lingual Annotation Projection", "text": "We use direct transfer as the base system, similar to the method described in (Yarowsky et al., 2001). First, we mark the source page of the parallel corpus with the available monitored tagger. Next, we align words in the parallel corpus to find the corresponding source and target words. Tags are then projected onto the (resource-poor) target language. The target language tag is trained using any machine learning approach (in our experiments we use the TNT tagger (Brants, 2000))."}, {"heading": "3.2 Proposed Approach", "text": "We propose a method for learning multilingual sequence tagging tools based on RNN, as can be seen in Figure 1. In our approach, a parallel or multiparallel corpus between a resource-rich language and one or many less resource-rich languages is used to extract common (multilingual) and agnostic word representations. These representations, which are only aligned at the sentence level, are used on the source side of the parallel / multiparallel corpus to learn a neural network tag in the source language. By choosing a common representation of source and target words, this neural network tag is truly multilingual and can also be used to mark texts in target language (s)."}, {"heading": "3.2.1 Common Words Representation", "text": "In our agnostic representation, we associate a common vector representation for each word (in the source and target vocabulary), namely Vwi, i = 1,..., N, where N is the number of parallel sentences (bisentences in the parallel corpus). If w appears in the i-th second sentence of the parallel corpus, then Vwi = 1. The idea is that in general a source word and its target translation appear together in the same bisentences and their vector representations are close to each other. We can then use the RNN tagger originally trained on the source page to mark the target page (due to our common vector representation). This simple representation does not require multilingual word alignment and allows the RNN to learn the optimal internal representation for the annotation task (for example, the hidden layers of the RNN can be considered a multilingual embedding of the words)."}, {"heading": "3.2.2 Recurrent Neural Networks", "text": "There are two major architectures of neural networks: Feedforward (Bengio et al., 2003) and Recurrent Neural Networks (RNN) (Schmidhuber, 1992; Mikolov et al., 2010). Sundermeyer et al. (2013) showed that language models based on recursive architecture perform better than language models based on recursive architecture, due to the fact that recursive neural networks do not use a context of limited size, which has led us to use the recurrent Elman architecture (Elman, 1990) in our experiments, where recursive connections occur at the level of the hidden layer. In this paper, we consider two Elman RNN architectures (see Figure 2): Simple RNN (SRNN) and bi-directional RNN (BRNN). In addition, we propose three new RNN variants to be able to incorporate linguistic information into our lower-level sequencing tasks designed for our architecture."}, {"heading": "A. Simple RNN", "text": "In the simple Elman RNN (SRNN), the recursive connection is a loop at the level of the hidden layer. This connection allows SRNN to use the states of the hidden layer of previous time steps. In other words, the hidden layer of SRNN represents all previous history and not just n \u2212 1 previous inputs, so the model can theoretically represent a long context.The architecture of the SRNN considered in this work is shown in Figure 2. In this architecture we have 4 layers: input layer, forward (also referred to as recursive or context layer), compression hidden layer and output layer. All neurons of the input layer are connected to each neuron of the input layer by weight matrix IF and RF. The weight matrix HF connects all neurons of the front layer to each neuron of the compression layer and all neurons of the output layer are connected to all of the compression layer."}, {"heading": "B. Bidirectional RNN", "text": "The basic idea of BRNN is to present each training sequence forwards and backwards on two separate recurring hidden levels (forward and backward hidden levels) and then somehow merge the results. This structure provides the compression and output levels for each point in the input sequence with a complete past and future context. Note that without the backward level, this structure simplifies into an SRNN."}, {"heading": "C. RNN Variants", "text": "As mentioned in the introduction, we propose three new RNN variants to accommodate low (POS) information in a higher (SST) annotation task. Here, the question is: At what level of the RNN should this low information be included to improve SST performance? As shown in Figure 3, the POS information can be introduced either at the input level, at the front level (forward and backward layer for BRNN), or on the compression layer. In all these RNN variants, the POS of the current word is also represented with a vector (POS (t)), its dimension corresponding to the number of POS tags in the tagset (universal tagset by Petrov et al. (2012). We propose a hot vector representation where only one value is set to 1 and corresponds to the index of the current tag (all other values are 0)."}, {"heading": "3.2.3 Network Training", "text": "The first step of our approach is to train the neural network, since it has a parallel corpus (training corpus) and a validation corpus (as opposed to train data) in the source language. In typical applications, the source language is a resource-rich language (which already has an efficient tagger or manually marked resources) Our RNN models are trained by stochastic gradient descent using common back propagation and back propagation through time algorithms (Rumelhart et al., 1985). We learn our RNN models with an iterative process on the tagged source side of the parallel corpus. After each epoch (iteration) in the training, validation data is used to calculate pro-token accuracy of the model. Thereafter, as the pro-token accuracy increases, the training continues in the new epoch. Otherwise, the learning rate is multiplied at the beginning of the new epoch."}, {"heading": "3.3 Dealing with out-of-vocabulary words", "text": "Consequently, during the test, the RNN model only uses the context information to highlight the OOV words found in the test corpus. To deal with these types of OOV words, we use the CBOW model (Mikolov et al., 2013) to replace each OOV word with the next known word in the current OOV word context. Once the next word is found, its common vector representation (instead of the vector of the zero values) is used when entering the RNN."}, {"heading": "3.4 Combining Simple Cross-lingual Projection and RNN Models", "text": "Since the simple translingual projection model M1 and the RNN model M2 use different strategies for marking (TNT is based on Markov models, while RNN is a neural network), we assume that these two models can complement each other. In order to maintain the advantages of each approach, we examine how they can be combined with linear interpolation. Formally, the probability of marking a specific word w is calculated as PM12 (t | w) = (\u00b5PM1 (t | w, CM1) + (1 \u2212 \u00b5) PM2 (t | w, CM2) (4), where CM1 and CM2 are the context of w considered by M1 and M2 respectively. The relative meaning of each model is adjusted by the interpolation parameter \u00b5. The word w is marked with the most likely marker, with the function f asf (w) = arg max t (PM12 (t | w) (5)."}, {"heading": "4 Experiments", "text": "Our models are evaluated based on two labeling tasks: cross-language part-of-speech tagging (POS) and multilingual super sense tagging (SST)."}, {"heading": "4.1 Multilingual POS Tagging", "text": "We applied our method to build RNN POS taggers for four target languages - French, German, Greek and Spanish - with English as the source language. In order to determine the effectiveness of our common word representation described in Section 3.2.1, we also investigated the use of state-of-the-art bilingual word embedding (using the MultiVec Toolkit (B\u00e9rard et al., 2016)) as input into our RNN."}, {"heading": "4.1.1 Dataset", "text": "For French as the target language, we used a training set of 10,000 parallel sentences, a validation set of 1,000 English sentences and a test set of 1,000 French sentences, all extracted from the ARCADE II English-French Corpus (Veronis et al., 2008), which is marked with the French TreeTagger (Schmid, 1995) and then checked manually. For German, Greek and Spanish as the target language, we used training and validation data from the Europarl Corpus (Koehn, 2005), which is a subset of the training data used in (Das und Petrov, 2011; Duong et al., 2013).This choice allows us to compare our results with those of (Das and Petrov, 2011; Gouws and S\u00f8gaard, 2015).The train data set contains 65,000 bi-sentences; a validation set of 10,000 bi-sentences is also available."}, {"heading": "4.1.2 Results and discussion", "text": "We note that the POS tagger, which is based on bi-directional RNN (BRNN), performs better than the simple RNN (SRNN), which means that both past and future contexts help in selecting the right tag. Table 1 also shows the performance before and after performing our procedure for handling OOOVs in BRNN. It shows that after replacing OOVs with the next words using CBOW, tagging accuracy increases significantly. As shown in the same table, our RNN models are close to the accuracy of the simple projection tag. They achieve comparable results with Das and Petrov (2011), Duong et al. (2013) (which used the full Europarl corpus while we use only a subset of it) and with Gouws and S\u00f8gaard (which used additional resources such as Wiktionary and Wikipedia)."}, {"heading": "4.2 Multilingual SST", "text": "To measure the impact of parallel corpus quality on our method, we also learn our SST models using the MultiSemCor multilingual parallel corpus (MSC), which is the result of a manual or automatic translation of SemCor from English to Italian and French."}, {"heading": "4.2.1 Dataset", "text": "SemCor The SemCor (Miller et al., 1993) is a subset of the Brown Corpus (Kucera and Francis, 1979) labeled with the senses WordNet (Fellbaum et al., 1998). MultiSemCor The English-Italian MultiSemcor (MSC-IT-1) corpus is a manual translation of the English SemCor into Italian (Bentivogli et al., 2004). As we have already mentioned, we are also interested in measuring the effects of parallel Corpus quality on our method. For this, we use two translation systems: (a) Google Translate to translate the English SemCor into Italian (MSC-IT-2) and French (MSCFR-2). (b) LIG Machine Translation System (Besacier et al., 2012) to translate the English SemCor into French (MSC-FR-1)."}, {"heading": "4.2.2 SST Systems Evaluated", "text": "The objectives of our SST experiments are twofold: firstly, to investigate the effectiveness of using POS information to create multilingual Super Sense taggers; secondly, to measure the effects of parallel corpus quality (manual or automatic translation) on our RNN models (SRNN, BRNN and our proposed variants); summarizing, we build four Super Sense taggers based on a lingual base projection (see Section 3.1) using four versions of MultiSemcor (MSC-IT-1, MSC-IT-2, MSC-FR-1, MSC-FR-2) as described above; and then we use the same four versions to train our multilingual SST models based on SRNN and BRNN. To learn our multilingual SST models based on the RNN variants proposed in Part (C) of Section 3.2.2, we also mark Semor 1995 with Tageger (TageSchmid)."}, {"heading": "4.2.3 Results and discussion", "text": "The results are directly comparable to those of the systems that participated in this evaluation campaign. We report on two SemEval 2013 (unattended) system results for comparison: \u2022 MFS Semeval 2013: The most common sense is the baseline provided by SemEval 2013 for Task 12, which is achieved by using an external resource (the most common sense of WordNet). \u2022 GETALP: a completely unattended WSD system proposed by (Schwab et al., 2012) based on AntColony algorithms. DAEBAK! (Navigli and Lapata, 2010) and UMCC-DLSI systems (Guti\u00e9rrez V\u00e1zquez et al., 2011) have also participated in SemEval 2013 Task 12."}, {"heading": "5 Conclusion", "text": "In this paper, we have presented an approach based on recurrent neural networks (RNN) to induce multilingual text analysis tools. We have studied simple and bidirectional RNN architectures for multilingual POS and SST tagging and proposed new RNN variants to incorporate low-level information (POS) into a super sense tagging task. Our approach has the following advantages: (a) it uses language-independent word representation (based only on word triggers in a parallel corpus), (b) it offers truly multilingual taggers (1 tagger for N languages) (c) it can be easily adapted to a new target language (when a small amount of monitored data is available), an earlier study (Zennaki et al., 2015a; Zennaki et al., 2015b) has demonstrated the effectiveness of our method in a weakly monitored context. Short-term analysis is intended to serve simultaneously to build multiple systems to perform a synvisory task."}], "references": [{"title": "Polyglot: Distributed word representations for multilingual nlp", "author": ["Al-Rfou et al.2013] Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": null, "citeRegEx": "Al.Rfou et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Al.Rfou et al\\.", "year": 2013}, {"title": "A neural probabilistic language model", "author": ["Bengio et al.2003] Yoshua Bengio", "R\u00e9jean Ducharme", "Pascal Vincent", "Christian Jauvin"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Bengio et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2003}, {"title": "Evaluating cross-language annotation transfer in the multisemcor corpus", "author": ["Pamela Forner", "Emanuele Pianta"], "venue": "In COLING,", "citeRegEx": "Bentivogli et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Bentivogli et al\\.", "year": 2004}, {"title": "Multivec: a multilingual and multilevel representation learning toolkit for nlp. In The 10th edition of the Language Resources and Evaluation Conference (LREC 2016)", "author": ["Christophe Servan", "Olivier Pietquin", "Laurent Besacier"], "venue": null, "citeRegEx": "B\u00e9rard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "B\u00e9rard et al\\.", "year": 2016}, {"title": "The lig english to french machine translation system for iwslt", "author": ["Benjamin Lecouteux", "Marwen Azouzi", "Ngoc-Quang Luong"], "venue": "In IWSLT,", "citeRegEx": "Besacier et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Besacier et al\\.", "year": 2012}, {"title": "Tnt: a statistical part-of-speech tagger", "author": ["Thorsten Brants"], "venue": "In Proceedings of the sixth conference on Applied natural language processing,", "citeRegEx": "Brants.,? \\Q2000\\E", "shortCiteRegEx": "Brants.", "year": 2000}, {"title": "Conll-x shared task on multilingual dependency parsing", "author": ["Buchholz", "Marsi2006] Sabine Buchholz", "Erwin Marsi"], "venue": "In Proceedings of the Tenth CoNLL,", "citeRegEx": "Buchholz et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Buchholz et al\\.", "year": 2006}, {"title": "On the properties of neural machine translation: Encoder\u2013decoder approaches. Syntax, Semantics and Structure in Statistical Translation", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Broad-coverage sense disambiguation and information extraction with a supersense sequence tagger", "author": ["Ciaramita", "Yasemin Altun"], "venue": "In Proceedings of the EMNLP-2006,", "citeRegEx": "Ciaramita et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Ciaramita et al\\.", "year": 2006}, {"title": "Unsupervised models for named entity classification", "author": ["Collins", "Singer1999] Michael Collins", "Yoram Singer"], "venue": "In Proceedings of the joint SIGDAT conference on EMNLP and very large corpora,", "citeRegEx": "Collins et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Collins et al\\.", "year": 1999}, {"title": "Natural language processing from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Unsupervised part-of-speech tagging with bilingual graph-based projections", "author": ["Das", "Petrov2011] Dipanjan Das", "Slav Petrov"], "venue": "Proceedings of the 49th ACL,", "citeRegEx": "Das et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Das et al\\.", "year": 2011}, {"title": "Simpler unsupervised pos tagging with bilingual projections", "author": ["Duong et al.2013] Long Duong", "Paul Cook", "Steven Bird", "Pavel Pecina"], "venue": "ACL", "citeRegEx": "Duong et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Duong et al\\.", "year": 2013}, {"title": "Syntactic transfer using a bilingual lexicon", "author": ["Durrett et al.2012] Greg Durrett", "Adam Pauls", "Dan Klein"], "venue": "In The Joint Conference on EMNLP and CoNLL,", "citeRegEx": "Durrett et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Durrett et al\\.", "year": 2012}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Measuring word alignment quality for statistical machine translation", "author": ["Fraser", "Marcu2007] Alexander Fraser", "Daniel Marcu"], "venue": "Computational Linguistics,", "citeRegEx": "Fraser et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Fraser et al\\.", "year": 2007}, {"title": "New directions in semi-supervised learning", "author": ["Andrew Brian Goldberg"], "venue": "Ph.D. thesis, University of Wisconsin\u2013Madison", "citeRegEx": "Goldberg.,? \\Q2010\\E", "shortCiteRegEx": "Goldberg.", "year": 2010}, {"title": "Simple task-specific bilingual word embeddings", "author": ["Gouws", "S\u00f8gaard2015] Stephan Gouws", "Anders S\u00f8gaard"], "venue": "In NAACL-HLT,", "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Bilbowa: Fast bilingual distributed representations without word alignments", "author": ["Gouws et al.2015] Stephan Gouws", "Yoshua Bengio", "Greg Corrado"], "venue": null, "citeRegEx": "Gouws et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gouws et al\\.", "year": 2015}, {"title": "Supervised sequence labelling", "author": ["Alex Graves"], "venue": null, "citeRegEx": "Graves.,? \\Q2012\\E", "shortCiteRegEx": "Graves.", "year": 2012}, {"title": "Enriching the integration of semantic resources based on wordnet", "author": ["Antonio Fern\u00e1ndez Orqu\u00edn", "Andr\u00e9s Montoyo Guijarro", "Sonia V\u00e1zquez P\u00e9rez"], "venue": null, "citeRegEx": "V\u00e1zquez et al\\.,? \\Q2011\\E", "shortCiteRegEx": "V\u00e1zquez et al\\.", "year": 2011}, {"title": "The unsupervised learning of natural language structure", "author": ["Dan Klein"], "venue": "Ph.D. thesis,", "citeRegEx": "Klein.,? \\Q2005\\E", "shortCiteRegEx": "Klein.", "year": 2005}, {"title": "Europarl: A parallel corpus for statistical machine translation", "author": ["Philipp Koehn"], "venue": "In MT summit,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "A standard corpus of present-day edited american english, for use with digital computers (revised and amplified", "author": ["Kucera", "Francis1979] H Kucera", "W Francis"], "venue": null, "citeRegEx": "Kucera et al\\.,? \\Q1979\\E", "shortCiteRegEx": "Kucera et al\\.", "year": 1979}, {"title": "Bilingual word representations with monolingual quality in mind", "author": ["Luong et al.2015] Thang Luong", "Hieu Pham", "Christopher D. Manning"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Recurrent neural network based language model", "author": ["Martin Karafi\u00e1t", "Lukas Burget", "Jan Cernock\u1ef3", "Sanjeev Khudanpur"], "venue": "INTERSPEECH", "citeRegEx": "Mikolov et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "A semantic concordance", "author": ["Claudia Leacock", "Randee Tengi", "Ross T Bunker"], "venue": "In Proceedings of the workshop on HLT,", "citeRegEx": "Miller et al\\.,? \\Q1993\\E", "shortCiteRegEx": "Miller et al\\.", "year": 1993}, {"title": "An experimental study of graph connectivity for unsupervised word sense disambiguation", "author": ["Navigli", "Lapata2010] Roberto Navigli", "Mirella Lapata"], "venue": "Pattern Analysis and Machine Intelligence,", "citeRegEx": "Navigli et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2010}, {"title": "Babelnet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network", "author": ["Navigli", "Ponzetto2012] Roberto Navigli", "Simone Paolo Ponzetto"], "venue": "Artificial Intelligence,", "citeRegEx": "Navigli et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2012}, {"title": "Semeval-2013 : Multilingual word sense disambiguation", "author": ["David Jurgens", "Daniele Vannella"], "venue": "In Second Joint Conference on Lexical and Computational Semantics,", "citeRegEx": "Navigli et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Navigli et al\\.", "year": 2013}, {"title": "Improved statistical alignment models", "author": ["Och", "Ney2000] Franz Josef Och", "Hermann Ney"], "venue": "In Proceedings of the 38th Annual Meeting on ACL,", "citeRegEx": "Och et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Och et al\\.", "year": 2000}, {"title": "Cross-lingual annotation projection for semantic roles", "author": ["Pad\u00f3", "Lapata2009] Sebastian Pad\u00f3", "Mirella Lapata"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "Pad\u00f3 et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Pad\u00f3 et al\\.", "year": 2009}, {"title": "A universal part-of-speech tagset", "author": ["Petrov et al.2012] Slav Petrov", "Dipanjan Das", "Ryan McDonald"], "venue": null, "citeRegEx": "Petrov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Petrov et al\\.", "year": 2012}, {"title": "Learning internal representations by error propagation", "author": ["Geoffrey E Hinton", "Ronald J Williams"], "venue": null, "citeRegEx": "Rumelhart et al\\.,? \\Q1985\\E", "shortCiteRegEx": "Rumelhart et al\\.", "year": 1985}, {"title": "Treetagger| a language independent part-of-speech tagger. Institut f\u00fcr Maschinelle Sprachverarbeitung, Universit\u00e4t Stuttgart, 43:28", "author": ["Helmut Schmid"], "venue": null, "citeRegEx": "Schmid.,? \\Q1995\\E", "shortCiteRegEx": "Schmid.", "year": 1995}, {"title": "A fixed size storage o (n3) time complexity learning algorithm for fully recurrent continually running networks", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Computation,", "citeRegEx": "Schmidhuber.,? \\Q1992\\E", "shortCiteRegEx": "Schmidhuber.", "year": 1992}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "Paliwal1997] Mike Schuster", "Kuldip K Paliwal"], "venue": "Signal Processing,", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Ant colony algorithm for the unsupervised word sense disambiguation of texts: Comparison and evaluation", "author": ["Schwab et al.2012] Didier Schwab", "J\u00e9r\u00f4me Goulian", "Andon Tchechmedjiev", "Herv\u00e9 Blanchon"], "venue": "In COLING,", "citeRegEx": "Schwab et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Schwab et al\\.", "year": 2012}, {"title": "Comparison of feedforward and recurrent neural network language models", "author": ["Ilya Oparin", "J-L Gauvain", "Ben Freiberg", "Ralf Schluter", "Hermann Ney"], "venue": "In ICASSP,", "citeRegEx": "Sundermeyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2013}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Cross-lingual word clusters for direct transfer of linguistic structure", "author": ["Ryan McDonald", "Jakob Uszkoreit"], "venue": "In Proceedings of the 2012 conference of the NAACL-HLT,", "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2012\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2012}, {"title": "Target language adaptation of discriminative transfer parsers", "author": ["Ryan McDonald", "Joakim Nivre"], "venue": null, "citeRegEx": "T\u00e4ckstr\u00f6m et al\\.,? \\Q2013\\E", "shortCiteRegEx": "T\u00e4ckstr\u00f6m et al\\.", "year": 2013}, {"title": "Crosslingual induction of semantic roles", "author": ["Titov", "Klementiev2012] Ivan Titov", "Alexandre Klementiev"], "venue": "In Proceedings of the 50th Annual Meeting of the ACL,", "citeRegEx": "Titov et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Titov et al\\.", "year": 2012}, {"title": "Arcade ii action de recherche concert\u00e9e sur l\u2019alignement de documents et son \u00e9valuation", "author": ["Veronis et al.2008] J Veronis", "O Hamon", "C Ayache", "R Belmouhoub", "O Kraif", "D Laurent", "TMH Nguyen", "N Semmar", "F Stuck", "W Zaghouani"], "venue": null, "citeRegEx": "Veronis et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Veronis et al\\.", "year": 2008}, {"title": "Inducing multilingual text analysis tools via robust projection across aligned corpora", "author": ["Grace Ngai", "Richard Wicentowski"], "venue": "In Proceedings of the first international conference on Human language technology research,", "citeRegEx": "Yarowsky et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Yarowsky et al\\.", "year": 2001}, {"title": "Unsupervised and lightly supervised part-of-speech tagging using recurrent neural networks", "author": ["Nasredine Semmar", "Laurent Besacier"], "venue": null, "citeRegEx": "Zennaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zennaki et al\\.", "year": 2015}, {"title": "Utilisation des r\u00e9seaux de neurones r\u00e9currents pour la projection interlingue d\u2019\u00e9tiquettes morpho-syntaxiques \u00e0 partir d\u2019un corpus parall\u00e8le", "author": ["Nasredine Semmar", "Laurent Besacier"], "venue": "TALN", "citeRegEx": "Zennaki et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zennaki et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "In order to minimize the need for annotated resources (produced through manual annotation, or by manual check of automatic annotation), several research works were interested in building Natural Language Processing (NLP) tools based on unsupervised or semi-supervised approaches (Collins and Singer, 1999; Klein, 2005; Goldberg, 2010).", "startOffset": 279, "endOffset": 334}, {"referenceID": 16, "context": "In order to minimize the need for annotated resources (produced through manual annotation, or by manual check of automatic annotation), several research works were interested in building Natural Language Processing (NLP) tools based on unsupervised or semi-supervised approaches (Collins and Singer, 1999; Klein, 2005; Goldberg, 2010).", "startOffset": 279, "endOffset": 334}, {"referenceID": 45, "context": "For example, NLP tools based on cross-language projection of linguistic annotations achieved good performances in the early 2000s (Yarowsky et al., 2001).", "startOffset": 130, "endOffset": 153}, {"referenceID": 10, "context": "While most NLP researches on RNN have focused on monolingual tasks1 and sequence labeling (Collobert et al., 2011; Graves, 2012), this paper, however, considers the problem of learning multilingual NLP tools using RNN.", "startOffset": 90, "endOffset": 128}, {"referenceID": 19, "context": "While most NLP researches on RNN have focused on monolingual tasks1 and sequence labeling (Collobert et al., 2011; Graves, 2012), this paper, however, considers the problem of learning multilingual NLP tools using RNN.", "startOffset": 90, "endOffset": 128}, {"referenceID": 7, "context": "Exceptions are the recent propositions on Neural Machine Translation (Cho et al., 2014; Sutskever et al., 2014) This work is licensed under a Creative Commons Attribution 4.", "startOffset": 69, "endOffset": 111}, {"referenceID": 40, "context": "Exceptions are the recent propositions on Neural Machine Translation (Cho et al., 2014; Sutskever et al., 2014) This work is licensed under a Creative Commons Attribution 4.", "startOffset": 69, "endOffset": 111}, {"referenceID": 27, "context": "For the SST task, we measure the impact of the parallel corpus quality with manual or automatic translations of the SemCor (Miller et al., 1993) translated from English into Italian (manually and automatically) and French (automatically).", "startOffset": 123, "endOffset": 144}, {"referenceID": 12, "context": "This approach has been successfully used to transfer several linguistic annotations between languages (efficient learning of POS taggers (Das and Petrov, 2011; Duong et al., 2013) and accurate projection of word senses (Bentivogli et al.", "startOffset": 137, "endOffset": 179}, {"referenceID": 2, "context": ", 2013) and accurate projection of word senses (Bentivogli et al., 2004)).", "startOffset": 47, "endOffset": 72}, {"referenceID": 13, "context": "First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; T\u00e4ckstr\u00f6m et al., 2013; Luong et al., 2015; Gouws and S\u00f8gaard, 2015; Gouws et al., 2015).", "startOffset": 93, "endOffset": 226}, {"referenceID": 0, "context": "First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; T\u00e4ckstr\u00f6m et al., 2013; Luong et al., 2015; Gouws and S\u00f8gaard, 2015; Gouws et al., 2015).", "startOffset": 93, "endOffset": 226}, {"referenceID": 42, "context": "First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; T\u00e4ckstr\u00f6m et al., 2013; Luong et al., 2015; Gouws and S\u00f8gaard, 2015; Gouws et al., 2015).", "startOffset": 93, "endOffset": 226}, {"referenceID": 24, "context": "First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; T\u00e4ckstr\u00f6m et al., 2013; Luong et al., 2015; Gouws and S\u00f8gaard, 2015; Gouws et al., 2015).", "startOffset": 93, "endOffset": 226}, {"referenceID": 17, "context": "First, these approaches learn language-independent features, across many different languages (Durrett et al., 2012; Al-Rfou et al., 2013; T\u00e4ckstr\u00f6m et al., 2013; Luong et al., 2015; Gouws and S\u00f8gaard, 2015; Gouws et al., 2015).", "startOffset": 93, "endOffset": 226}, {"referenceID": 41, "context": "Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and S\u00f8gaard, 2015), cross-language named entity recognition (T\u00e4ckstr\u00f6m et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al.", "startOffset": 216, "endOffset": 240}, {"referenceID": 17, "context": ", 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al.", "startOffset": 76, "endOffset": 96}, {"referenceID": 13, "context": ", 2015), cross language dependency parsing (Durrett et al., 2012; T\u00e4ckstr\u00f6m et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012).", "startOffset": 43, "endOffset": 89}, {"referenceID": 42, "context": ", 2015), cross language dependency parsing (Durrett et al., 2012; T\u00e4ckstr\u00f6m et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012).", "startOffset": 43, "endOffset": 89}, {"referenceID": 36, "context": "Cross-lingual projection of linguistic annotations was pioneered by Yarowsky et al. (2001) who created new monolingual resources by transferring annotations from resource-rich languages onto resource-poor languages through the use of word alignments.", "startOffset": 68, "endOffset": 91}, {"referenceID": 0, "context": ", 2012; Al-Rfou et al., 2013; T\u00e4ckstr\u00f6m et al., 2013; Luong et al., 2015; Gouws and S\u00f8gaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and S\u00f8gaard, 2015), cross-language named entity recognition (T\u00e4ckstr\u00f6m et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; T\u00e4ckstr\u00f6m et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to induce a common language-independent feature space (crosslingual words embeddings). Unlike Durrett et al. (2012) and Gouws and S\u00f8gaard (2015), who use bilingual lexicons, and unlike Luong et al.", "startOffset": 8, "endOffset": 962}, {"referenceID": 0, "context": ", 2012; Al-Rfou et al., 2013; T\u00e4ckstr\u00f6m et al., 2013; Luong et al., 2015; Gouws and S\u00f8gaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and S\u00f8gaard, 2015), cross-language named entity recognition (T\u00e4ckstr\u00f6m et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; T\u00e4ckstr\u00f6m et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to induce a common language-independent feature space (crosslingual words embeddings). Unlike Durrett et al. (2012) and Gouws and S\u00f8gaard (2015), who use bilingual lexicons, and unlike Luong et al.", "startOffset": 8, "endOffset": 991}, {"referenceID": 0, "context": ", 2012; Al-Rfou et al., 2013; T\u00e4ckstr\u00f6m et al., 2013; Luong et al., 2015; Gouws and S\u00f8gaard, 2015; Gouws et al., 2015). Then, the induced representation space is used to train NLP tools by exploiting labeled data from the source language and apply them in the target language. Cross-lingual representation learning approaches have achieved good results in different NLP applications such as cross-language SST and POS tagging (Gouws and S\u00f8gaard, 2015), cross-language named entity recognition (T\u00e4ckstr\u00f6m et al., 2012), cross-lingual document classification and lexical translation task (Gouws et al., 2015), cross language dependency parsing (Durrett et al., 2012; T\u00e4ckstr\u00f6m et al., 2013) and cross-language semantic role labeling (Titov and Klementiev, 2012). Our approach described in next section, is inspired by these works since we also try to induce a common language-independent feature space (crosslingual words embeddings). Unlike Durrett et al. (2012) and Gouws and S\u00f8gaard (2015), who use bilingual lexicons, and unlike Luong et al. (2015) who use word alignments between the source and target languages2 our common multilingual representation is very agnostic.", "startOffset": 8, "endOffset": 1051}, {"referenceID": 45, "context": "We use direct transfer as a baseline system which is similar to the method described in (Yarowsky et al., 2001).", "startOffset": 88, "endOffset": 111}, {"referenceID": 5, "context": "The target language tagger is trained using any machine learning approach (we use TNT tagger (Brants, 2000) in our experiments).", "startOffset": 93, "endOffset": 107}, {"referenceID": 1, "context": "2 Recurrent Neural Networks There are two major architectures of neural networks: Feedforward (Bengio et al., 2003) and Recurrent Neural Networks (RNN) (Schmidhuber, 1992; Mikolov et al.", "startOffset": 94, "endOffset": 115}, {"referenceID": 36, "context": ", 2003) and Recurrent Neural Networks (RNN) (Schmidhuber, 1992; Mikolov et al., 2010).", "startOffset": 44, "endOffset": 85}, {"referenceID": 25, "context": ", 2003) and Recurrent Neural Networks (RNN) (Schmidhuber, 1992; Mikolov et al., 2010).", "startOffset": 44, "endOffset": 85}, {"referenceID": 14, "context": "This property led us to use, in our experiments, the Elman recurrent architecture (Elman, 1990), in which recurrent connections occur at the hidden layer level.", "startOffset": 82, "endOffset": 95}, {"referenceID": 1, "context": "2 Recurrent Neural Networks There are two major architectures of neural networks: Feedforward (Bengio et al., 2003) and Recurrent Neural Networks (RNN) (Schmidhuber, 1992; Mikolov et al., 2010). Sundermeyer et al. (2013) showed that language models based on recurrent architecture achieve better performance than language models based on feedforward architecture.", "startOffset": 95, "endOffset": 221}, {"referenceID": 5, "context": "This SRNN is thus penalized compared with our baseline projection based on TNT (Brants, 2000) which considers both left and right contexts.", "startOffset": 79, "endOffset": 93}, {"referenceID": 33, "context": "Its dimension corresponds to the number of POS tags in the tagset (universal tagset of Petrov et al. (2012) is used).", "startOffset": 87, "endOffset": 108}, {"referenceID": 34, "context": "Our RNN models are trained by stochastic gradient descent using usual back-propagation and back-propagation through time algorithms (Rumelhart et al., 1985).", "startOffset": 132, "endOffset": 156}, {"referenceID": 26, "context": "To deal with these types of OOV words3, we use the CBOW model of (Mikolov et al., 2013) to replace each OOV word by its closest known word in the current OOV word context.", "startOffset": 65, "endOffset": 87}, {"referenceID": 3, "context": "1, we also investigated the use of state-of-the-art bilingual word embeddings (using MultiVec Toolkit (B\u00e9rard et al., 2016)) as input to our RNN.", "startOffset": 102, "endOffset": 123}, {"referenceID": 44, "context": "1 Dataset For French as a target language, we used a training set of 10, 000 parallel sentences, a validation set of 1000 English sentences, and a test set of 1000 French sentences, all extracted from the ARCADE II English-French corpus (Veronis et al., 2008).", "startOffset": 237, "endOffset": 259}, {"referenceID": 35, "context": "The test set is tagged with the French TreeTagger (Schmid, 1995) and then manually checked.", "startOffset": 50, "endOffset": 64}, {"referenceID": 22, "context": "For German, Greek and Spanish as a target language, we used training and validation data extracted from the Europarl corpus (Koehn, 2005) which are a subset of the training data used in (Das and Petrov, 2011; Duong et al.", "startOffset": 124, "endOffset": 137}, {"referenceID": 12, "context": "For German, Greek and Spanish as a target language, we used training and validation data extracted from the Europarl corpus (Koehn, 2005) which are a subset of the training data used in (Das and Petrov, 2011; Duong et al., 2013).", "startOffset": 186, "endOffset": 228}, {"referenceID": 12, "context": "This choice allows us to compare our results with those of (Das and Petrov, 2011; Duong et al., 2013; Gouws and S\u00f8gaard, 2015).", "startOffset": 59, "endOffset": 126}, {"referenceID": 12, "context": "For testing, we use the same test corpora as (Das and Petrov, 2011; Duong et al., 2013; Gouws and S\u00f8gaard, 2015) (bi-sentences from CoNLL shared", "startOffset": 45, "endOffset": 112}, {"referenceID": 5, "context": "After tags projection, a target language POS tagger based on TNT approach (Brants, 2000) is trained.", "startOffset": 74, "endOffset": 88}, {"referenceID": 32, "context": "The evaluation metric (per-token accuracy) and the Petrov et al. (2012) universal tagset are used for evaluation.", "startOffset": 51, "endOffset": 72}, {"referenceID": 32, "context": "The evaluation metric (per-token accuracy) and the Petrov et al. (2012) universal tagset are used for evaluation. For training, the English (source) sides of the training corpora (ARCADE II and Europarl) and of the validation corpora are tagged with the English TreeTagger toolkit. Using the matching provided by Petrov et al. (2012), we map the TreeTagger and the CoNLL tagsets to the common Universal Tagset.", "startOffset": 51, "endOffset": 334}, {"referenceID": 12, "context": "It achieves comparable results to Das and Petrov (2011), Duong et al. (2013) (who used the full Europarl corpus while we use only a 65, 000 subset of it) and to Gouws and S\u00f8gaard (2015) (who used extra resources such as Wiktionary and Wikipedia).", "startOffset": 57, "endOffset": 77}, {"referenceID": 12, "context": "It achieves comparable results to Das and Petrov (2011), Duong et al. (2013) (who used the full Europarl corpus while we use only a 65, 000 subset of it) and to Gouws and S\u00f8gaard (2015) (who used extra resources such as Wiktionary and Wikipedia).", "startOffset": 57, "endOffset": 186}, {"referenceID": 27, "context": "1 Dataset SemCor The SemCor (Miller et al., 1993) is a subset of the Brown Corpus (Kucera and Francis, 1979) labeled with the WordNet (Fellbaum, 1998) senses.", "startOffset": 28, "endOffset": 49}, {"referenceID": 2, "context": "MultiSemCor The English-Italian MultiSemcor (MSC-IT-1) corpus is a manual translation of the English SemCor to Italian (Bentivogli et al., 2004).", "startOffset": 119, "endOffset": 144}, {"referenceID": 4, "context": "(b) LIG machine translation system (Besacier et al., 2012) to translate the English SemCor to French (MSC-FR-1).", "startOffset": 35, "endOffset": 58}, {"referenceID": 30, "context": "Test Corpus To evaluate our models, we used the SemEval 2013 Task 12 (Multilingual Word Sense Disambiguation) (Navigli et al., 2013) test corpora, which are available in 5 languages (English, French, German, Spanish and Italian) and labeled with BabelNet (Navigli and Ponzetto, 2012) senses.", "startOffset": 110, "endOffset": 132}, {"referenceID": 35, "context": "2, we also tag SemCor using TreeTagger (POS tagger proposed by Schmid (1995)).", "startOffset": 63, "endOffset": 77}, {"referenceID": 38, "context": "\u2022 GETALP : a fully unsupervised WSD system proposed by (Schwab et al., 2012) based on AntColony algorithm.", "startOffset": 55, "endOffset": 76}, {"referenceID": 38, "context": "4 GETALP (Schwab et al., 2012) 40.", "startOffset": 9, "endOffset": 30}], "year": 2016, "abstractText": "This work focuses on the rapid development of linguistic annotation tools for resource-poor languages. We experiment several cross-lingual annotation projection methods using Recurrent Neural Networks (RNN) models. The distinctive feature of our approach is that our multilingual word representation requires only a parallel corpus between the source and target language. More precisely, our method has the following characteristics: (a) it does not use word alignment information, (b) it does not assume any knowledge about foreign languages, which makes it applicable to a wide range of resource-poor languages, (c) it provides truly multilingual taggers. We investigate both uniand bi-directional RNN models and propose a method to include external information (for instance low level information from POS) in the RNN to train higher level taggers (for instance, super sense taggers). We demonstrate the validity and genericity of our model by using parallel corpora (obtained by manual or automatic translation). Our experiments are conducted to induce cross-lingual POS and super sense taggers.", "creator": "LaTeX with hyperref package"}}}