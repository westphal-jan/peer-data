{"id": "1609.09475", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge", "abstract": "Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC). A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multi-view RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MIT-Princeton Team system that took 3rd- and 4th- place in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at", "histories": [["v1", "Thu, 29 Sep 2016 19:39:13 GMT  (2091kb,D)", "http://arxiv.org/abs/1609.09475v1", "Under review at the International Conference on Robotics and Automation (ICRA) 2017. Project webpage:this http URL"], ["v2", "Sun, 2 Oct 2016 00:24:29 GMT  (2091kb,D)", "http://arxiv.org/abs/1609.09475v2", "Under review at the International Conference on Robotics and Automation (ICRA) 2017. Project webpage:this http URL"], ["v3", "Sun, 7 May 2017 20:12:55 GMT  (2091kb,D)", "http://arxiv.org/abs/1609.09475v3", "To appear at the International Conference on Robotics and Automation (ICRA) 2017. Project webpage:this http URL"]], "COMMENTS": "Under review at the International Conference on Robotics and Automation (ICRA) 2017. Project webpage:this http URL", "reviews": [], "SUBJECTS": "cs.CV cs.LG cs.RO", "authors": ["andy zeng", "kuan-ting yu", "shuran song", "daniel suo", "ed walker jr", "alberto rodriguez", "jianxiong xiao"], "accepted": false, "id": "1609.09475"}, "pdf": {"name": "1609.09475.pdf", "metadata": {"source": "CRF", "title": "Multi-view Self-supervised Deep Learning for 6D Pose Estimation in the Amazon Picking Challenge", "authors": ["Andy Zeng", "Kuan-Ting Yu", "Shuran Song", "Daniel Suo", "Ed Walker Jr.", "Alberto Rodriguez", "Jianxiong Xiao"], "emails": [], "sections": [{"heading": null, "text": "In the proposed approach, we segment and label multiple views of a scene with a fully revolutionary neural network, and then match automatically scanned 3D object models to the resulting segmentation to obtain the 6D object. Formation of a deep neural network for segmentation typically requires a large amount of training data. We propose a self-monitored method to generate a large volume of data without tedious manual segmentation. We show that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All codes, and benchmarks are available at http: / / cs.princeton.us / andyz / apc.us The past two decades have seen a rapid increase in warehouse technologies that satisfy the growing demand for e-commerce products and enable the delivery of faster, cheaper deliveries. Some tasks, especially those involving physical interaction, are still difficult to automate."}, {"heading": "II. RELATED WORK", "text": "The choice depends primarily on the manipulation needs. While the 2015 APC winning team uses a method for the rear projection of histograms with manually defined properties [8], recent work in computer vision has shown that deep learning significantly improves object segmentation [2]. In this work, we extend the state-of-the-art deep learning architecture used for image segmentation to integrate depth and multiview information. There are two primary approaches to evaluating an object's 6D position."}, {"heading": "III. AMAZON PICKING CHALLENGE 2016", "text": "The APC 2016 presented a simplified version of the general picking and stowing tasks in a warehouse. In the picking task, robots sit within a 2x2 meter area in front of a shelf with objects and pick 12 desired objects independently and place them in a container. In the picking task, robots pick all 12 objects within a container and place them on a prefilled shelf. Before the competition, the teams were given a list of 39 possible objects along with 3D CAD models of the shelf and container. At runtime, the robots received the initial contents of each container on the shelf and a work order containing which items to pick. After picking and stowing the corresponding objects, the system had to report the final contents of both the shelf and the container. Competition details can be found in [1]."}, {"heading": "IV. SYSTEM DESCRIPTION", "text": "Our machine vision system takes RGB-D images from multiple views and issues 6D poses and a segmented dot cloud for the robot to complete picking and stowing tasks (Figure 1).The camera is compactly integrated into the end effector of an ABB IRB1600id 6DOF industrial manipulator and points to the fingertip (Figure 1).This configuration gives the robot full control of the camera perspective and feedback on the grip or suction success. The camera of choice is the RealSense F200 RGB-D because its depth range (0.2-1.5 m) is suitable for tight manipulation and because it is a consumer-level range sensor with a reasonable degree of flexibility in the data acquisition process. Due to the tight integration of the camera, the gripper fingers, even when fully open, take up a small part of the view."}, {"heading": "A. Object Segmentation with Fully Convolutional Networks", "text": "It is indeed the case that we are able to go in search of a solution."}, {"heading": "C. Handling Objects with Missing Depth.", "text": "Many objects in the APC have surfaces that challenge infrared depth sensors, such as a plastic shell that reflects loudly or multiple times, or transparent or grid-like materials that may not register at all. In these objects, the captured point cloud is loud and sparse, and our pose estimation algorithm works poorly. Our solution uses multi-view segmentation to estimate a convex shell of the object by carving a 3D grid space of voxels with the segmented RGB images, which results in a 3D mask enclosing the real object. We use the convex shell of this mask to estimate the geometric center of the object and approximate its orientation (provided the object is aligned with the axis)."}, {"heading": "VI. SELF-SUPERVISED TRAINING", "text": "In fact, it is so that most people are able to determine for themselves what they want and what they want. \"I do not believe that I want this,\" he says, \"but I believe that I want it.\" \"I believe it.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"No.\" \"\" No. \"\" \"No.\" \"\" No. \"\" \"No.\" \"\" No. \"\" \"No.\" \"\" No. \"\" \"No..\" \"\" No.. \"\" \"\" No. \"\" \"\" No. \"\" \"\" \"No.\" \"\" \"No.\" \"\" \"No.\" \"\" \"No.\" \"\" No.. \"\" \"\" No. \".\" \"\" No.. \"\" \"No..\" \"\" No. \".\" \"\" No. \"\" No. \"\" \"No.\". \"\" No. \".\" \"No.\". \".\" \"No.\". \"\" \"No.\". \"\" No. \".\" \"\" \"No.\". \"\" \"\" No. \"No.\". \"\" \"No.\" \"No.\" No. \"No.\". \".\" \"\" No. \".\" No.. \"No.\" No......... \"\" \"\" No.... \"\" \"\" No. \"No........\" \"\" \"No.....\" \"\" No........... \"\" \"No....\" \"\" No......... \"\" \"\" No. \"No...\" No. \"No.\" No.......... \"\" \"No...\" \"\" No... \"\" \"No.\" \"No....\" \"No...\" \"No..\" No. \"No.\" No. \"No.............\" \"\" \"No.\" \"\" \"No.\" No. \"No.\" No..... \"No....."}, {"heading": "VIII. EVALUATION", "text": "We evaluate variants of our method in different scenarios in the benchmark dataset to (1) understand how segmentation behaves under different input modalities and training dataset sizes, and (2) how the Full Vision System behaves."}, {"heading": "A. Benchmark Dataset", "text": "Our benchmark \"Shelf & Tote\" dataset contains more than 7,000 RGB-D images of 477 (Figure 6) scenes in 640 x 480 resolution, reflecting various challenges arising in the vicinity of the warehouse: reflective materials, different lighting conditions, partial views and sensor constraints (noisy and lack of depth) in crowded environments. Tables I and II summarize our experimental results and highlight performance differences in various overlapping scene categories: \u00b7 cptn: during the APC finals competition. \u00b7 Environment: in an office (off); in the APC competition warehouse (whs). \u00b7 Task: Picking from a shelf or from a wallet. \u00b7 Disorder: with multiple objects. \u00b7 Occurrence: with% of the object calculated from another object, the truth or with thin objects that have no properties."}, {"heading": "B. Evaluating Object Segmentation", "text": "We are testing several variants of our FCN on object segmentation to answer two questions: (1) Can we use both color and depth segmentation of the benchmark dataset with pixel-by-pixel precision and retrieval? (2) Is the training data more useful? Metrics. We are comparing the predicted object segmentation of our trained FCNs with the segmentation marking of the ground truth of the benchmark dataset. We are using HHA characteristics [23] to encode depth information into three channels: horizontal disparity, height above the ground, and angle of the local surface normal with the inferred direction of gravity. We are comparing AlexNet, which was trained on this coding, to VGG on RGB data, and both networks, which are shown in Table I. We find that adding depth does not bring significant improvements in segmentation performance, partly due to the noise of depth information from our object sensor."}, {"heading": "C. Evaluating Pose Estimation", "text": "We evaluate several key components of our system to determine whether they increase performance in insulation. We report on the percentage of predictions in terms of orientation less than 15 and the percentage in terms of gear ratio less than 5 cm. The metric also detects the structural invariance of several objects, some of which are axially symmetric (cuboids), radially symmetric (bottles, cylinders) or deformable (see website [3] for more details). We have observed that these bonds of 15 cm and 5 cm are sufficient for selection with sensory movements."}, {"heading": "D. Common Failure Modes", "text": "Here we summarize the most common error modes of our visual system, as illustrated in Figure 9: \u00b7 FCN segmentation for heavily shuttered or cluttered objects is likely to be incomplete, leading to poor estimation of the pose (Figure 8.e) or not detected (Figure 9.m and p), with greater frequency on the back of the trash with poor illumination. \u00b7 Color textures of the objects are confused with each other. Figure 9.r shows a white box on an envelope that together looks similar to the outlet connector. \u00b7 Model adaptation for rectangular objects often confuses corner alignments (marker fields in Figure 9.o). However, this inaccuracy is still within the tolerance range that the robot can tolerate thanks to sensor-monitored movements."}, {"heading": "IX. DISCUSSION", "text": "Despite enormous advances in computer vision, many well-known state-of-the-art approaches often correlate with performance. External limitations limit what systems can do, and indirectly limit the number of states the system can be in, which can lead to opportunities for simplification and robustness in the perception system. In the picking task, each team was given a list of items, their assignment to the containers, and a shelf model. All teams used the assignment to the containers to exclude objects from view, and the shelf model to calibrate their robots. These optimizations are simple and useful. However, further research reveals more possibilities. By using these same constraints, we constructed a self-monitoring mechanism to train a deep neural network with significantly more data. As our evaluations show, the volume of training data is strongly correlated to performance."}, {"heading": "X. CONCLUSION", "text": "In this post, we introduce the vision system of Team MIT Princeton, which came third and fourth in the 2016 AmazonPicking Challenge. To address the challenges posed by the warehouse environment, our framework uses multi-view RGB-D data and data-driven, self-monitored deep learning to reliably assess the 6D poses of objects under a variety of scenarios. We also provide a well-labeled APC 2016 benchmark dataset with over 7,000 images from 477 scenes."}], "references": [{"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, 2015, pp. 3431\u20133440.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic multi-class segmentation for the amazon picking challenge", "author": ["R. Jonschkowski", "C. Eppner", "S. H\u00f6fer", "R. Mart\u0131\u0301n-Mart\u0131\u0301n", "O. Brock"], "venue": "http://dx.doi.org/10.14279/depositonce-5051, 2016.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Lessons from the amazon picking challenge: Four aspects of building robotic systems", "author": ["C. Eppner", "S. H\u00f6fer", "R. Jonschkowski", "R. Mart\u0131n-Mart\u0131n", "A. Sieverling", "V. Wall", "O. Brock"], "venue": "RSS, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Dorapicker: An autonomous picking system for general objects", "author": ["H. Zhang", "P. Long", "D. Zhou", "Z. Qian", "Z. Wang", "W. Wan", "D. Manocha", "C. Park", "T. Hu", "C. Cao", "Y. Chen", "M. Chow", "J. Pan"], "venue": "arXiv: 1603.06317, 2016. [Online]. Available: http://arxiv.org/abs/1603.06317", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "A summary of team mit\u2019s approach to the amazon picking challenge 2015", "author": ["K.-T. Yu", "N. Fazeli", "N.C. Dafle", "O. Taylor", "E. Donlon", "G.D. Lankenau", "A. Rodriguez"], "venue": "arXiv:1604.03639, 2016.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "Fast object localization and pose estimation in heavy clutter for robotic bin picking", "author": ["M.-Y. Liu", "O. Tuzel", "A. Veeraraghavan", "Y. Taguchi", "T.K. Marks", "R. Chellappa"], "venue": "IJRR, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "A method for registration of 3-d shapes", "author": ["P.J. Besl", "N.D. McKay"], "venue": "PAMI, 1992.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1992}, {"title": "Sift-based homographies for efficient multiview distributed visual sensing", "author": ["A. Dias", "C. Brites", "J. Ascenso", "F. Pereira"], "venue": "IEEE Sensors Journal, vol. 15, no. 5, pp. 2643\u20132656, May 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "3dmatch:  Learning the matching of local 3d geometry in range scans", "author": ["A. Zeng", "S. Song", "M. Nie\u00dfner", "M. Fisher", "J. Xiao"], "venue": "arXiv: 1603.08182, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes", "author": ["S. Hinterstoisser", "S. Holzer", "C. Cagniart", "S. Ilic", "K. Konolige", "N. Navab", "V. Lepetit"], "venue": "ICCV, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "The moped framework: Object recognition and pose estimation for manipulation", "author": ["A. Collet", "M. Martinez", "S.S. Srinivasa"], "venue": "IJRR, vol. 30, no. 10, pp. 1284\u20131306, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "Analysis and Observations from the First Amazon Picking Challenge", "author": ["N. Correll", "K. Bekris", "D. Berenson", "O. Brock", "A. Causo", "K. Hauser", "K. Okada", "A. Rodriguez", "J. Romano", "P. Wurman"], "venue": "IEEE T-ASE, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Bigbird: A large-scale 3d database of object instances", "author": ["A. Singh", "J. Sha", "K.S. Narayan", "T. Achim", "P. Abbeel"], "venue": "ICRA, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "A dataset for improved rgbd-based object detection and pose estimation for warehouse pick-and-place", "author": ["C. Rennie", "R. Shome", "K.E. Bekris", "A.F. De Souza"], "venue": "Robotics and Automation Letters, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR, 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv:1409.1556, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Geometrically stable sampling for the icp algorithm", "author": ["N. Gelfand", "L. Ikemoto", "S. Rusinkiewicz", "M. Levoy"], "venue": "3DIM, 2003.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2003}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "IJCV, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Parametric estimate of intensity inhomogeneities applied to mri", "author": ["M. Styner", "C. Brechbuhler", "G. Szckely", "G. Gerig"], "venue": "IEEE Transactions on Medical Imaging, 2000.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning rich features from rgb-d images for object detection and segmentation", "author": ["S. Gupta", "R. Girshick", "P. Arbel\u00e1ez", "J. Malik"], "venue": "ECCV, 2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "Each color image is fed into a fully convolutional network [2] for 2D object segmentation.", "startOffset": 59, "endOffset": 62}, {"referenceID": 1, "context": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects.", "startOffset": 104, "endOffset": 110}, {"referenceID": 2, "context": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects.", "startOffset": 104, "endOffset": 110}, {"referenceID": 3, "context": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects.", "startOffset": 124, "endOffset": 130}, {"referenceID": 4, "context": "Vision algorithms for robotic manipulation typically output 2D bounding boxes, pixel-level segmentation [4, 5], or 6D poses [6, 7] of the objects.", "startOffset": 124, "endOffset": 130}, {"referenceID": 5, "context": "While the 2015 APC winning team used a histogram backprojection method [8] with manually defined features [5, 4], recent work in computer vision has shown that deep learning considerably improves object", "startOffset": 71, "endOffset": 74}, {"referenceID": 2, "context": "While the 2015 APC winning team used a histogram backprojection method [8] with manually defined features [5, 4], recent work in computer vision has shown that deep learning considerably improves object", "startOffset": 106, "endOffset": 112}, {"referenceID": 1, "context": "While the 2015 APC winning team used a histogram backprojection method [8] with manually defined features [5, 4], recent work in computer vision has shown that deep learning considerably improves object", "startOffset": 106, "endOffset": 112}, {"referenceID": 0, "context": "segmentation [2].", "startOffset": 13, "endOffset": 16}, {"referenceID": 6, "context": "The first aligns 3D CAD models to 3D point clouds with algorithms such as iterative closest point [9].", "startOffset": 98, "endOffset": 101}, {"referenceID": 7, "context": "The second uses more elaborated local descriptors such as SIFT keypoints [10] for color data", "startOffset": 73, "endOffset": 77}, {"referenceID": 8, "context": "or 3DMatch [11] for 3D data.", "startOffset": 11, "endOffset": 15}, {"referenceID": 9, "context": "Existing frameworks such as LINEMOD [12] or MOPED [13] work well under certain assumptions such as", "startOffset": 36, "endOffset": 40}, {"referenceID": 10, "context": "Existing frameworks such as LINEMOD [12] or MOPED [13] work well under certain assumptions such as", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "objects sitting on a table top with good illumination, but underperform when confronted with the limited visibility, shadows, and clutter imposed by the APC scenario [14].", "startOffset": 166, "endOffset": 170}, {"referenceID": 12, "context": "Previous efforts to construct benchmark datasets include Berkeley\u2019s dataset [15] with a number of objects from and beyond APC 2015 and Rutgers\u2019s dataset [16] with semi-automatically labeled data.", "startOffset": 76, "endOffset": 80}, {"referenceID": 13, "context": "Previous efforts to construct benchmark datasets include Berkeley\u2019s dataset [15] with a number of objects from and beyond APC 2015 and Rutgers\u2019s dataset [16] with semi-automatically labeled data.", "startOffset": 153, "endOffset": 157}, {"referenceID": 14, "context": "In recent years, ConvNets have made tremendous progress for computer vision tasks [17, 2].", "startOffset": 82, "endOffset": 89}, {"referenceID": 0, "context": "In recent years, ConvNets have made tremendous progress for computer vision tasks [17, 2].", "startOffset": 82, "endOffset": 89}, {"referenceID": 15, "context": "More explicitly, we train a VGG architecture [18] Fully Convolutional Network (FCN) [2] to perform 2D object", "startOffset": 45, "endOffset": 49}, {"referenceID": 0, "context": "More explicitly, we train a VGG architecture [18] Fully Convolutional Network (FCN) [2] to perform 2D object", "startOffset": 84, "endOffset": 87}, {"referenceID": 16, "context": "We use the iterative closest point (ICP) algorithm [19] on the segmented point cloud to fit pre-scanned 3D models of", "startOffset": 51, "endOffset": 55}, {"referenceID": 17, "context": "ImageNet [20]) are mostly Internet photos, which have very different object and image statistics from our warehouse setting.", "startOffset": 9, "endOffset": 13}, {"referenceID": 14, "context": "To automatically capture and pixel-wise label images, we propose a self-supervised method, based on three observations: \u00b7 Batch-training on scenes with a single object can yield deep models that perform well on scenes with multiple objects [17] (i.", "startOffset": 240, "endOffset": 244}, {"referenceID": 18, "context": "The 2D pipeline starts by fixing minor possible image misalignments by using multimodal 2D intensity-based registration to align the two RGB-D images [21].", "startOffset": 150, "endOffset": 154}, {"referenceID": 15, "context": "To leverage features trained from a larger image domain, we use the sizable FCN-VGG network architecture from [18] and initialize the network weights using a model pre-trained on ImageNet for 1000-way object classification.", "startOffset": 110, "endOffset": 114}, {"referenceID": 19, "context": "We use HHA features [23] to encode depth information into three channels: horizontal disparity, height above ground, and angle of local surface normal with the inferred direction of gravity.", "startOffset": 20, "endOffset": 24}], "year": 2016, "abstractText": "Robot warehouse automation has attracted significant interest in recent years, perhaps most visibly in the Amazon Picking Challenge (APC) [1]. A fully autonomous warehouse pick-and-place system requires robust vision that reliably recognizes and locates objects amid cluttered environments, self-occlusions, sensor noise, and a large variety of objects. In this paper we present an approach that leverages multiview RGB-D data and self-supervised, data-driven learning to overcome those difficulties. The approach was part of the MITPrinceton Team system that took 3rdand 4thplace in the stowing and picking tasks, respectively at APC 2016. In the proposed approach, we segment and label multiple views of a scene with a fully convolutional neural network, and then fit pre-scanned 3D object models to the resulting segmentation to get the 6D object pose. Training a deep neural network for segmentation typically requires a large amount of training data. We propose a self-supervised method to generate a large labeled dataset without tedious manual segmentation. We demonstrate that our system can reliably estimate the 6D pose of objects under a variety of scenarios. All code, data, and benchmarks are available at http://www.cs.princeton.edu/\u223candyz/apc2016.", "creator": "LaTeX with hyperref package"}}}