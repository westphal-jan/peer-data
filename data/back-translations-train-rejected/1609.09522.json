{"id": "1609.09522", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "29-Sep-2016", "title": "Charged Point Normalization: An Efficient Solution to the Saddle Point Problem", "abstract": "Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-CPN neural networks.", "histories": [["v1", "Thu, 29 Sep 2016 20:43:21 GMT  (462kb)", "http://arxiv.org/abs/1609.09522v1", null], ["v2", "Tue, 7 Feb 2017 08:29:40 GMT  (462kb)", "http://arxiv.org/abs/1609.09522v2", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["armen aghajanyan"], "accepted": false, "id": "1609.09522"}, "pdf": {"name": "1609.09522.pdf", "metadata": {"source": "CRF", "title": "CHARGED POINT NORMALIZATION AN EFFICIENT SOLUTION TO THE SADDLE POINT PROBLEM", "authors": ["Armen Aghajanyan"], "emails": ["armen.ag@live.com"], "sections": [{"heading": "1 SADDLE POINT PROBLEM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 INTRODUCTION", "text": "Recently more and more attention has been focused on the problem of saddle points in very high-dimensional non-convex optimizations. Saddle points represent points in the optimization problem where the first-order gradients are all zero, but the stationary point is neither a maximum nor a minimum. The saddle point of a function can be confirmed by using the eigenvalues of the Hessian matrix. If the eigenvalues of the eigenvalues contain at least one negative eigenvalue and at least one positive eigenvalue, then the critical point is a local maximum. \u2022 If the eigenvalues are all positive, then the critical point of prevalence of the saddle point is a local minimum of the probability density of the eigenvalues of the Hessian matrix. \u2022 If the eigenvalues are all negative, then the critical point is a local maximum. \u2022 If the eigenvalues are all positive, then the critical point is a local minimum. \u2022 If the eigenvalues of the Hessian matrix axis contain at least one positive and at least one negative eigenvalue of the Hessian matrix, then a eigenvalue of the matrix."}, {"heading": "1.2 GRADIENT DESCENT BEHAVIOR AROUND SADDLE POINTS", "text": "To understand the inadequacies of the first order gradient descending algorithms around saddle points, we analyze the neighborhood around one saddle point. Faced with a function f, the Taylor expansion around saddle point x is given by the following factors: f (x + \u03b4) = f (x) + 12 \u03b4TH\u03b4 (2) The term first order disappears because we are at a critical point. e1, e2,..., en as the eigenvectors of the non-degenerated Hessian H, and \u03bb1, \u03bb2,..., \u03bbn as the respective eigenvalues, we can use the modification of the coordinate methods to rewrite the Taylor expansion with respect to eigenvectors: v = 12 eT1... eTn \u03b4 f (x + \u03b4) = f (x) + 12 n \u2211 i = 1 (e T i \u03b4) 2 = f (x) + n \u0445 i = 1 (3). From the last equation, we can analyze the behavior of the gradient descending algorithms of the first order."}, {"heading": "2 CHARGED POINT NORMALIZATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 METAPHOR", "text": "The metaphor for our method is that the current point in our optimization is a small, positively charged point that moves on the neutral error surface. Our normalization works by dynamically placing other positively charged points around the error surface to push our optimization point away from undesirable positions. Ideally, it would be to run the gradient sink algorithm to convergence, check if the converged point is a saddle point, place a positively charged point near the saddle point, and continue optimizing. It was this metaphor that inspired us to derive our normalization."}, {"heading": "2.2 INTRODUCTION", "text": "The general optimization problem is defined as: L (f; X, Y) = n \u2211 i = 1V (f (Xi), Yi) + \u03bbR (f) (4) The formulation is static, because the same function and the same X and Y will always be the same loss. Our formulation introduces a dynamic normalization function R. The function f therefore contains the dynamic parameters W t1, W t 2,..., W t n, while the function R contains parameters: \u03b2, p, \u03c6 and W-t1, W-t 2,..., W-t n, which respectfully symbolize the decay factor, the standard, the merging function, and the merging values. The t-term inW tn represents the value of Wn at the time t of the optimization algorithm."}, {"heading": "2.3 CHOICE OF HYPERPARAMETER", "text": "The \u03c6 function can be any function that merges the two parameters into one parameter of the same dimension. Throughout the essay, we used the exponential moving average for our \u03c6 function.? (W-ti, W-i) = \u03b1W-t + (1 \u2212 \u03b1) W-ti-\u03b1 (0, 1) (8) Although Coulomb's inverse square law, to keep up with the metaphor, did not work as well as projected by trial and error, the p-value that worked best was 1. The 1 norm is simply the sum of absolute values."}, {"heading": "3 EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 INTRODUCTION", "text": "The loaded point normalization was implemented in Theano (Bastien et al., 2012) and integrated into the Keras library (Chollet, 2015). All training and tests were performed on an Nvidia GTX 980 GPU. We do not show results on a validation set because we care about the efficiency and performance of the optimization algorithm, not whether it is too high or not. Overfitting a model is not the fault of the optimization routine, but rather of the field on which it is optimized. All comparisons between the standard and the loaded model started with identical weights. In all of our experiments, we use a Softmax layer as the last layer, and as a result, all losses measured in this paper will be cross-category."}, {"heading": "3.2 SIMPLE DEEP NEURAL NETWORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.2.1 MNIST: MULTILAYER PERCEPTRON", "text": "The architecture of the neural network contained layers with the sizes 784 \u2192 512 \u2192 512 \u2192 10. All intermediate layers contained rectified linear activations (He et al., 2015), while the last layer is a Softmax layer. A dropout was added between the layers (Hinton et al., 2012) with a probability of 0.2. We compare the standard stack descend algorithm with a step size of 0.001 and a stack size of 128 on the above mesh and the same mesh with Charged Point Normalization (CPN). The CPN hyperparameters were: \u03b2 = 0.001, \u03bb = 0.1 with the moving average parameter \u03b1 = 0.95. The loss we optimized was a categorical cross entropy."}, {"heading": "3.2.2 MNIST:DEEP AUTOENCODER", "text": "The second test performed on simple neural networks was in the form of an autoencoder. The architecture of the autoencoder contained layers with sizes 784 \u2192 512 \u2192 512 \u2192 10 \u2192 512 \u2192 512 \u2192 10. All layers contained rectified linear activations. Between layers a dropout was added with a probability of 0.2. The structure of the experiment is almost identical to the previous experiment. The only difference is that in this case we optimized for binary transverse entropies. 0 20 40 60 80 100 00.20.40.61EpochLoss on MNIST Dataset (MLP) Charged Standard0 20 40 60 80 100 0.20.40.6EpochLoss on MNIST Dataset (AutoEncoder) Charged Standard"}, {"heading": "3.2.3 NOTES", "text": "It is interesting to note that when the optimization problem is relatively simple, or more precisely when the optimization algorithm takes gentle steps, CPN allows the optimization algorithm to take larger steps in the right direction. CPN does not exhibit periodic or chaotic behavior in this scenario. More complicated optimization problems described below are not the case."}, {"heading": "3.3 CONVOLUTIONAL NEURAL NETWORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.3.1 CIFAR10", "text": "The next experiment was performed with a revolutionary neural network on the CIFAR10 (Krizhevsky et al., a). Architecture was as follows: Convolution2D (32,3,3) \u2192 ReLU \u2192 Convolution2D (32,3,3) \u2192 ReLU \u2192 MaxPooling (2,2) \u2192 Dropout (0,25) \u2192 Convolution2D (64,3,3) \u2192 ReLU \u2192 Convolution2D respectfully accepts the parameters, number of filters, width and height. Density take a parameter that describes the size of the layer (0,25) \u2192 Dropout (512) \u2192 ReLU \u2192 Dropout (0,5) \u2192 Dropout (10) \u2192 Softmax Convolution2D respects the parameters, width and height. Density take a parameter that describes the size of the layer. MaxPooling takes two parameters that mean the size of the pool."}, {"heading": "3.3.2 CIFAR100", "text": "The CIFAR100 setup (Krizhevsky et al., b) was almost identical to the CIFAR10 setup, using the same architecture of the neural network. The only difference was the \u03bb parameter in the normalization term, which in this case corresponded to 0.01. 20,000 random images were used. 0 20 40 60 80 100 00.511.522.533.544.55EpochLoss on CIFAR100 DatasetCharged Standard 0 20 40 60 80 100 00.20.40.60.81EpochAccuracy on CIFAR100 DatasetCharged StandardThe same behavior as in the CIFAR10 experiment was demonstrated."}, {"heading": "3.4 RECURRENT NEURAL NETWORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.4.1 INTRODUCTION", "text": "Recurrent neural networks are notorious for being difficult to train and have a tendency to generally underfit (Pascanu et al., 2012).In this section we show that CPN successfully eludes the saddle points contained in recurrent neural networks.Sentence Embedding Question EmbeddingRNN RNNConcatDense"}, {"heading": "3.4.2 BABI", "text": "We chose the path finding problem of the BABI dataset because it was the most difficult task. The architecture consisted of two recursive networks and a standard neural network. Each of the recursive neural networks had a structure: Embedding \u2192 RNN. The size of the embedding, the set and the hidden query was increased to 3. The final network concatenated the two recursive network outputs and fed the result into a dense layer with an output size of vocabulary size.See Figure 1. We performed our experiment with two different recursive neural network structures: Gated Recurrent Units (GRU) (Chung et al., 2014) and Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997).The ADAM (Kingma & Ba, 2014) optimization algorithm was used for both recurrent structures: PT = 0.001, \u03b21 = 0.008 for the architecture P00T = 0.001-1, G00T = 0.001."}, {"heading": "4 NORMALIZATION BEHAVIOR", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 EXPLORATION VS EXPLOITATION", "text": "In a standard gradient without normalization, the updates made by the algorithm are always greedy as regards minimizing the loss of the model Targeted: -0.0 05 0. 0. 00 5 0. 01 0. 01 5 0. 02 0. 02 5 0. 03 5 0. 03 5 0. 040102030405060 H- > Charged: eigenvalue distribution-0.0 05 0. 0. 00 5 0. 02 0. 02 0. 06 0. 08 0. 1 0. 12 0. 14 0. 1601030405060 H- > Charged: eigenvalue distribution-0.0 05 0. 15 0. 00 5 0. 01 0. 01 5 0. 02 0. 02 0. 03 0. 03 0. 5010203040507080 I- > Optimization of gradient-0. 05 0. 15 0. 2 0. 25012345678 H- > OVanilla: eigenvalue distribution-gradient-gradient-gradient-gradient-gradient Targeted-directed-0."}, {"heading": "4.2 BEHAVIOR AROUND SADDLE POINTS", "text": "We compare the differences between eigenvalue distributions between the neural network with CPN and the neural network without CPN. Remember, the narrower the range of eigenvalues, the greater the steps the gradient descend algorithm can take without worrying about divergences, as explained in Section 1.2. The diagram above shows an estimate of the kernel density to hide and hide at the input in order to output the Hessian values at the near critical point. There are both negative and positive eigenvalues, especially in the hidden to the initial weights, so it is safe enough to say that we are at a saddle point (Turlach, 1993). The first diagram represents the neural network of the CPN, while the next represents a non-normalized neural network. The CPN network shows a narrower distribution as well as a more focused distribution of eigenvalues."}, {"heading": "4.3 TOY EXAMPLE", "text": "To ensure that the normalization actually repels the optimization point from the saddle points, and that the results obtained in the experimental section are not due to some disruptive factors, we use a low-dimensional experiment to show the repellent effects of CPN. We use the monkey saddle as the optimization surface. The monkey saddle has a saddle point surrounded by plateaus in all directions. It is defined as x3 \u2212 3xy2. Referring to Section 1.2, we discussed that the direction of the gradient descend algorithms was not the lack of gradient descend algorithms around saddle points, but the problem was with the step size. What CPN should do in theory is allow the optimization routine to take larger steps. Below are two numbers. The first shows the behavior of five common gradient descend algorithms starting from a point near the saddle point, but the problem with the step size."}, {"heading": "4.4 PERIODICITY AND TERMINAL BEHAVIOR", "text": "As the experiments with the CIFAR datasets have shown, the CPN tends to force the optimization algorithm to behave more chaotically; the exponential term in the normalization term is there to ensure that the optimization algorithm is not stuck in a periodic path. It is trivial to see that the effects of normalization tend toward 0, so if the optimization algorithm does not reach a local minimum, but rather is on an elliptical path, provided that the level term is not large enough to push the point out of the local minimum, the optimization algorithm will eventually reach the local minimum."}, {"heading": "5 NOTES ON HYPER-PARAMETERS", "text": "Due to the limitations of our hardware resources, we did not have enough time to conduct a comprehensive study of CPN's behavior with respect to its hyperparameters. During this essay, the selection of hyperparameters was kept relatively simple. We selected the hyperparameters within a feasible range and then corrected them manually about 4-8 times. Therefore, the hyperparameters chosen for CPN in this essay are by no means optimal for the various setups, but the results we found were somewhat substantial, which we find quite optimistic."}, {"heading": "6 WEAKNESSES", "text": "\u2022 CPN with exponentially moving average for the \u03c6 function introduces two additional hyperparameters, without the scalar normalization value \u03bb. \u2022 Regarding implementation: CPN doubles the memory requirement for the optimization problem, since a subsequent copy of the parameters must be maintained. \u2022 The fraction term in CPN generally contains small floating points in both the counter and the denominator, which can sometimes lead to numerical instability. \u2022 When saddle points are reached at a really late point in the optimization algorithm, the exponential decay negates the effects of CPN. One possible solution would be to replace the exponential decay term with some kind of periodic decay."}, {"heading": "7 CONCLUSION", "text": "In this paper, we presented a new type of dynamic normalization that enables gradient-based optimization algorithms to escape saddle points. We presented empirical results on standard data sets that show that CPN successfully escapes saddle points on various neural network architectures. We discussed the theoretical properties of first-order gradient descend algorithms around saddle points and discussed the influence of the largest eigenvalue on the step undertaken. Empirical results showed that the hunch was confirmed that the Hessian charged point contains properties of normalized neural networks that are less pronounced than their non-normalized counterparts."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": "Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop,", "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "KyungHyun Cho", "Yoshua Bengio"], "venue": "CoRR, abs/1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Identifying and attacking the saddle point problem in high-dimensional non-convex optimization", "author": ["Yann Dauphin", "Razvan Pascanu", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Surya Ganguli", "Yoshua Bengio"], "venue": "CoRR, abs/1406.2572,", "citeRegEx": "Dauphin et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Dauphin et al\\.", "year": 2014}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Technical Report UCB/EECS-2010-24, EECS Department,", "citeRegEx": "Duchi et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2010}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "CoRR, abs/1502.01852,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E. Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "CoRR, abs/1207.0580,", "citeRegEx": "Hinton et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural Comput.,", "citeRegEx": "Hochreiter and Schmidhuber.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Matplotlib: A 2d graphics environment", "author": ["J.D. Hunter"], "venue": "Computing In Science & Engineering,", "citeRegEx": "Hunter.,? \\Q2007\\E", "shortCiteRegEx": "Hunter.", "year": 2007}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik P. Kingma", "Jimmy Ba"], "venue": "CoRR, abs/1412.6980,", "citeRegEx": "Kingma and Ba.,? \\Q2014\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "A method of solving a convex programming problem with convergence rate O(1/sqr(k))", "author": ["Yurii Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "Nesterov.,? \\Q1983\\E", "shortCiteRegEx": "Nesterov.", "year": 1983}, {"title": "Dynamics of on-line gradient descent learning for multilayer neural networks", "author": ["David Saad", "Sara A. Solla"], "venue": "Advances in neural information processing systems,", "citeRegEx": "Saad and Solla.,? \\Q1996\\E", "shortCiteRegEx": "Saad and Solla.", "year": 1996}, {"title": "Bandwidth Selection in Kernel Density Estimation: A Review", "author": ["Berwin A. Turlach"], "venue": "In CORE and Institut de Statistique, pp", "citeRegEx": "Turlach.,? \\Q1993\\E", "shortCiteRegEx": "Turlach.", "year": 1993}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler"], "venue": "CoRR, abs/1212.5701,", "citeRegEx": "Zeiler.,? \\Q2012\\E", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "A very similiar derivation and explanation was shows in (Dauphin et al., 2014)", "startOffset": 56, "endOffset": 78}, {"referenceID": 0, "context": "1 INTRODUCTION Charged Point Normalization was implemented in Theano (Bastien et al., 2012) and integrated with the Keras (Chollet, 2015) library.", "startOffset": 69, "endOffset": 91}, {"referenceID": 4, "context": "All intermediate layers contained rectified linear activations (He et al., 2015), while the final layer is a softmax layer.", "startOffset": 63, "endOffset": 80}, {"referenceID": 5, "context": "Between layers, dropout (Hinton et al., 2012) with a probability of 0.", "startOffset": 24, "endOffset": 45}, {"referenceID": 1, "context": "We ran our experiment with two different recurrent neural network structures: Gated Recurrent Units (GRU) (Chung et al., 2014) and Long Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) .", "startOffset": 106, "endOffset": 126}, {"referenceID": 11, "context": "There are both negative and positive eigenvalues, especially in the hidden to output weights, therefore it is safe enough to say that we are at a saddle point (Turlach, 1993).", "startOffset": 159, "endOffset": 174}, {"referenceID": 12, "context": "0001)) (Zeiler, 2012), (Duchi et al.", "startOffset": 7, "endOffset": 21}, {"referenceID": 3, "context": "0001)) (Zeiler, 2012), (Duchi et al., 2010).", "startOffset": 23, "endOffset": 43}, {"referenceID": 7, "context": "All visualization were done using the matplotlib library (Hunter, 2007).", "startOffset": 57, "endOffset": 71}, {"referenceID": 9, "context": "SGD Accelerated refers to the standard SGD algorithm using momentum and nesterov acceleration (Nesterov, 1983).", "startOffset": 94, "endOffset": 110}], "year": 2016, "abstractText": "Recently, the problem of local minima in very high dimensional non-convex optimization has been challenged and the problem of saddle points has been introduced. This paper introduces a dynamic type of normalization that forces the system to escape saddle points. Unlike other saddle point escaping algorithms, second order information is not utilized, and the system can be trained with an arbitrary gradient descent learner. The system drastically improves learning in a range of deep neural networks on various data-sets in comparison to non-CPN neural networks. 1 SADDLE POINT PROBLEM 1.", "creator": "LaTeX with hyperref package"}}}