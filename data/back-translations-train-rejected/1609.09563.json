{"id": "1609.09563", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Asynchronous Multi-Task Learning", "abstract": "Many real-world machine learning applications involve several learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data-centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for shared subspace learning. Empirical studies on both synthetic and real-world datasets demonstrate the efficiency and effectiveness of the proposed framework.", "histories": [["v1", "Fri, 30 Sep 2016 01:23:15 GMT  (1459kb,D)", "http://arxiv.org/abs/1609.09563v1", "IEEE International Conference on Data Mining (ICDM) 2016"]], "COMMENTS": "IEEE International Conference on Data Mining (ICDM) 2016", "reviews": [], "SUBJECTS": "cs.LG cs.DC", "authors": ["inci m baytas", "ming yan", "anil k jain", "jiayu zhou"], "accepted": false, "id": "1609.09563"}, "pdf": {"name": "1609.09563.pdf", "metadata": {"source": "CRF", "title": "Asynchronous Multi-Task Learning", "authors": ["Inci M. Baytas", "Ming Yan", "Anil K. Jain", "Jiayu Zhou"], "emails": ["jiayuz}@msu.edu"], "sections": [{"heading": null, "text": "This year, it will be able to fix and fix the problems mentioned above."}, {"heading": "II. RELATED WORK", "text": "This section provides a literature overview of distributed optimization and distributed MTL."}, {"heading": "A. Distributed optimization", "text": "Distributed optimization techniques use technological improvements in hardware to quickly solve massive optimization problems. A commonly used method of distributed optimization is the method of multipliers (ADMM), which was first proposed in the 1970s [2]. Boyd et al. defined it as a well-suited method of distributed convex optimization. In the distributed ADMM framework, local copies are introduced for local subproblems, and communication between the working nodes and the center node is for the purpose of consensus. Although it can fit into a large-scale distributed optimization method, the introduction of local copies increases the number of iterations to achieve the same accuracy. Moreover, the approaches in [2] are all synchronous. To avoid the introduction of multiple local copies, Iutzeler et algorithm proposed algorithmic optimization."}, {"heading": "B. Distributed multi-task learning", "text": "In this section, studies on distributed MTL have been summarized in the literature. In the real world, MTL problems have been multiplied, huge amounts of data used geographically, such as health records. Each hospital has its own data, which consists of information such as medical records, including diagnostics, drugs, test results, etc. In this scenario, two of the most important challenges are network communication and privacy. In traditional MTL approaches, data transfer from different sources is required, but this task can be quite costly due to bandwidth constraints. Another point is that hospitals may not want to share their data with others in terms of patient privacy. Distributed MTL provides a solution by using distributed optimization techniques for the transmitted challenges. In distributed MTL, data is not required to be transferred to a central node. As only the learned models are transferred, all raw data is transferred, the costs of network communication are reduced."}, {"heading": "III. ASYNCHRONOUS MULTI-TASK LEARNING", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Regularized multi-task learning", "text": "MTL involves several related learning tasks, and the goal of MTL is to obtain models with improved generalization performance for all tasks. Suppose we have T-shaped learning tasks, and for each task we will get the training data Dt = {xt, yt} from nt data points where xt-rnt \u00b7 d are the feature vectors for the training data and yt-Rnt contains the corresponding answers. Suppose the target model is a linear model parameterized by the wt-Rd vector (in this paper we misuse the term \"model\" to denote the vector), and we use \"t\" to denote the loss function. t, yt) examples that include the least loss and logistical loss."}, {"heading": "B. (Synchronized) distributed optimization of MTL", "text": "The fact is that we are in a position to put ourselves in a time where we are in a time where we are able to put ourselves in a time where we are able to put ourselves in a time where we are able to put ourselves in a time where we are able to put ourselves in a time where we are able to put ourselves in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are able to put ourselves in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in which we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are in a time where we are a time where we are in a time where we are a time where we are a time where we are a time where we are in a time where we are a time where we are a time where we are a time where we are a time where we are a time where we are a time where we are a time of a time where we are a time where we are a time of a time where we are a time where we are a time where we are a time where we are a time where we are a"}, {"heading": "C. Asynchronized framework for distributed MTL", "text": "To address the above challenges in distributed MTL processes, we propose to perform asynchronous Multi-Task Learning (AMTL), in which the central server starts updating the model after receiving a gradient calculation from one task node without waiting for the others to finish their calculations. While the server and all task agents maintain their own copies of W in memory, the copy at one task node may be different from the copies at other nodes. Convergence of the proposed AMTL frameworkis updated by a current approach to asynchronous parallel coordinates using Krasnosel'skii-Mann (KM). [7] Task is said to be activated when performing compilation and network communication with the central server for updates. The framework is based on the following assumption of the activation rate: All the activation processes follow the same task and independent task."}, {"heading": "D. Dynamic step size controlling in AMTL", "text": "As discussed in note 1, the AMTL is based on the same activation rate in assumption 1. However, to ensure convergence, the increments used in asynchronous optimization algorithms are typically much smaller than those used in the synchronous optimization algorithms, limiting the algorithmic efficiency of the solvers. The dynamic step was recently used in a specific setting using asynchronous optimization to achieve better overall performance [26]. Our dynamic step size is motivated by this design and revises the update of the AMTL into equation. III.4 by augmenting a time-related multiplier: vk + 1t = v k t + c (t, k).Ik in the final phase of Ik."}, {"heading": "IV. NUMERICAL EXPERIMENTS", "text": "The proposed asynchronous, distributed MTL framework is implemented in C + +. In our experiments, we simulate the distributed environment using the shared memory architecture in [6] with network delays transmitted to the working nodes. In this section, we first explain how AMTL works, then present experiments with synthetic and real data sets to compare the training times of the proposed AMTL and synchronous, distributed multi-task learning (SMTL), and examine the empirical convergence behavior of AMTL and traditional SMTL by comparing them with synthetic data sets. Finally, we investigate the effects of dynamic increments on synthetic data sets with different tasks. The experiments were conducted with an Intel Core i5-5200U CPU 2.20GHz x 4 laptop."}, {"heading": "A. Experimental setting", "text": "To simulate the distributed environment with a shared storage system, we use threads to simulate task nodes, and the number of threads equals the number of tasks. As such, each thread is responsible for learning the model parameters of a task and communicates with the central node to achieve knowledge transfer, where the central node is simulated by the shared memory. (SVX) Although the framework can be used to solve many regulated MTL formulations, in this paper we focus on a specific MTL formulation with low rank for common sub-space tasks. In the formulation, we assume that all tasks are a regression model with the lowest square loss T = 1 xtwt \u2212 yt \u00b2 2. Recall that xt, xt, i, and yt, i denote the data matrix, i-th data sample of the task y, and the nuclear standard of the task."}, {"heading": "B. Comparison between AMTL and SMTL", "text": "Dre rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the"}, {"heading": "C. Dynamic step size", "text": "In this section we present the experimental result of the proposed dynamic step size. A dynamic step size is suggested by using the delays in the network. We simulate the delays caused by the network communication by leaving the task nodes idle for a while after they have completed the forward step. Dynamic step size was calculated by Eq. (III.6). In this experiment, the average delay of the last 5 iterations was used to modify the step size. the effect of the dynamic step size was calculated for randomly generated synthetic data sets with 100 samples in each task with dimensionality to 50. Due to the delays, some of the task nodes with different number of tasks and different offset values were examined. These objective values were calculated at the end of the total number of iterations and the final updated versions of the model vectors were used. Due to the delays, some of the task nodes have to wait much longer than others for these nodes and the nodes."}, {"heading": "V. CONCLUSION", "text": "Compared to other distributed MTL approaches, AMTL is more time-saving, as task nodes do not have to wait for other nodes to perform the gradient updates. A dynamic step size to increase convergence performance is investigated by scaling the step size according to communication delays. Training times are compared for multiple synthetic and public data sets, and the results showed that the proposed AMTL is faster than traditional synchronous MTL. We also investigate the convergence behavior of AMTL and SMTL by comparing the precision of the two approaches. We note that the current AMTL implementation is based on the ARock [6] framework, which largely limits our ability to conduct experiments for different network structures."}, {"heading": "ACKNOWLEDGMENT", "text": "This research is partially funded by the National Science Foundation (NSF) under grant numbers IIS-1565596, IIS1615597 and DMS-1621798 and the Office of Naval Research (ONR) under grant number N00014-14-1-0631."}], "references": [{"title": "Malsar: Multi-task learning via structural regularization,", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": "Arizona State University,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2011}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers,", "author": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Asynchronous distributed optimization using a randomized alternating direction method of multipliers,", "author": ["F. Iutzeler", "P. Bianchi", "P. Ciblat", "W. Hachem"], "venue": "IEEE Conference on Decision and Control,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2013}, {"title": "An asynchronous distributed proximal gradient method for composite convex optimization,", "author": ["N. Aybat", "Z. Wang", "G. Iyengar"], "venue": "Proceedings of the 32nd International Conference on Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties,", "author": ["J. Liu", "S.J. Wright"], "venue": "SIAM Journal of Optimization,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "ARock: An algorithmic framework for asynchronous parallel coordinate updates,", "author": ["Z. Peng", "Y. Xu", "M. Yan", "W. Yin"], "venue": "SIAM Journal on Scientific Computing,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "Tmac: A toolbox of modern asyncparallel, coordinate, splitting, and stochastic methods,", "author": ["B. Edmunds", "Z. Peng", "W. Yin"], "venue": "UCLA CAM Report", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Coordinate-friendly structures, algorithms and applications,", "author": ["Z. Peng", "T. Wu", "Y. Xu", "M. Yan", "W. Yin"], "venue": "Annals of Mathematical Sciences and Applications,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Clientserver multitask learning from distributed datasets,", "author": ["F. Dinuzzo", "G. Pillonetto", "G. De Nicolao"], "venue": "IEEE Transactions on Neural Networks, vol. 22,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Collaborating between local and global learning for distributed online multiple tasks,", "author": ["X. Jin", "P. Luo", "F. Zhuang", "J. He", "H. Qing"], "venue": "Proceedings of the 24th ACM International Conference on Information and Knowledge Management,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Heterogeneous multitask learning with joint sparsity constraints,", "author": ["X. Yang", "S. Kim", "E.P. Xing"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2009}, {"title": "Multitask learning,", "author": ["R. Caruana"], "venue": "Machine Learning,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1997}, {"title": "Regularized multi-task learning,", "author": ["T. Evgeniou", "M. Pontil"], "venue": "Proceedings of the 10th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2004}, {"title": "Clustered multi-task learning via alternating structure optimization,", "author": ["J. Zhou", "J. Chen", "J. Ye"], "venue": "in NIPS,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "Multi-task feature learning via efficient l2,1-norm minimization,", "author": ["J. Liu", "S. Ji", "J. Ye"], "venue": "Proceedings of the 21th Conference on Uncertainty in Artifical Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Convex multi-task feature learning,", "author": ["A. Argyriou", "T. Evgeniou", "M. Pontil"], "venue": "Machine Learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2008}, {"title": "A rank minimization heuristic with application to minimum order system approximation,", "author": ["M. Fazel", "H. Hindi", "S.P. Boyd"], "venue": "Proceedings of the American Control Conference,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2001}, {"title": "An accelerated gradient method for trace norm minimization,", "author": ["S. Ji", "J. Ye"], "venue": "Proceedings of the 26th Annual International Conference on Machine Learning,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems,", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2009}, {"title": "Sparse reconstruction by separable approximation,", "author": ["S.J. Wright", "R.D. Nowak", "M.A. Figueiredo"], "venue": "IEEE Transactions on Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "Proximal newton-type methods for minimizing composite functions,", "author": ["J.D. Lee", "Y. Sun", "M.A. Saunders"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Signal recovery by proximal forwardbackward splitting,", "author": ["P.L. Combettes", "V.R. Wajs"], "venue": "Multiscale Modeling & Simulation,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2005}, {"title": "Regularization and variable selection via the elastic net,", "author": ["H. Zou", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2005}, {"title": "Amortized analysis on asynchronous gradient descent,", "author": ["Y.K. Cheung", "R. Cole"], "venue": "arXiv preprint arXiv:1412.0159,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "A singular value thresholding algorithm for matrix completion,", "author": ["J.-F. Cai", "E.J. Cand\u00e8s", "Z. Shen"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2010}, {"title": "Fast online SVD revisions for lightweight recommender systems.", "author": ["M. Brand"], "venue": "Proceedings of SIAM International Conference on Data Mining,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2003}, {"title": "Rasbash, \u201cDifferential school effectiveness,", "author": ["D.L. Nuttall", "H. Goldstein", "R. Prosser"], "venue": "International Journal of Educational Research,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1989}, {"title": "Facial landmark detection by deep multi-task learning,", "author": ["Z. Zhang", "P. Luo", "C.C. Loy", "X. Tang"], "venue": "Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "The framework is capable of solving most existing regularized MTL formulations and is compatible with the formulations in the MTL package MALSAR [1].", "startOffset": 145, "endOffset": 148}, {"referenceID": 1, "context": "One commonly used distributed optimization approach is alternating direction method of multipliers (ADMM), which was firstly proposed in the 1970s [2].", "startOffset": 147, "endOffset": 150}, {"referenceID": 1, "context": "In the distributed ADMM framework in [2], local copies are introduced for local subproblems, and the communication between the work nodes and the center node is for the purpose of consensus.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "Furthermore, the approaches in [2] are all synchronized.", "startOffset": 31, "endOffset": 34}, {"referenceID": 2, "context": "proposed an asynchronous distributed approach using randomized ADMM [3] based on randomized Gauss-Seidel iterations of a Douglas-Rachford splitting (DRS) because ADMM is equivalent to DRS.", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "introduced an asynchronous distributed proximal gradient method by using the randomized block coordinate descent method in [4] for minimizing the sum of a smooth and a non-smooth functions.", "startOffset": 123, "endOffset": 126}, {"referenceID": 4, "context": "proposed an asynchronous stochastic proximal coordinate descent method in [5].", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "proposed a general asynchronous parallel framework for coordinate updates based on solving fixed-point problems with non-expansive operators [6], [7].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "proposed a general asynchronous parallel framework for coordinate updates based on solving fixed-point problems with non-expansive operators [6], [7].", "startOffset": 146, "endOffset": 149}, {"referenceID": 7, "context": "monotone inclusion problems [8].", "startOffset": 28, "endOffset": 31}, {"referenceID": 8, "context": "For instance, Dinuzzo and Pillonetto in [9] proposed a client-server MTL from distributed datasets in 2011.", "startOffset": 40, "endOffset": 43}, {"referenceID": 9, "context": ", on the other hand, proposed collaborating between local and global learning for distributed online multiple tasks in [11].", "startOffset": 119, "endOffset": 123}, {"referenceID": 10, "context": "heterogeneous [12], i.", "startOffset": 14, "endOffset": 18}, {"referenceID": 11, "context": "In a single learning task, we treat the task independently and minimize the corresponding loss function, while in MTL, the tasks are related, and we hope that by properly assuming task relatedness, the learning of one task (the inference process of wt) can benefit from other tasks [13].", "startOffset": 282, "endOffset": 286}, {"referenceID": 12, "context": "Therefore, MTL is typically achieved by adding a penalty term [14], [1], [15]:", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Therefore, MTL is typically achieved by adding a penalty term [14], [1], [15]:", "startOffset": 68, "endOffset": 71}, {"referenceID": 13, "context": "Therefore, MTL is typically achieved by adding a penalty term [14], [1], [15]:", "startOffset": 73, "endOffset": 77}, {"referenceID": 14, "context": "One representative task relatedness is joint feature learning, which is achieved via the grouped sparsity induced by penalizing the `2,1-norm [16] of the model matrix W : g(W ) = \u2016W\u20162,1 = \u2211d i=1 \u2016w\u20162 where w is the ith row of W .", "startOffset": 142, "endOffset": 146}, {"referenceID": 15, "context": "Another commonly used MTL method is the shared subspace learning [17], which is achieved by penalizing the nuclear", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "The nuclear norm is the tightest convex relaxation of the rank function [18], and the problem can be solved via proximal gradient methods [19].", "startOffset": 72, "endOffset": 76}, {"referenceID": 17, "context": "The nuclear norm is the tightest convex relaxation of the rank function [18], and the problem can be solved via proximal gradient methods [19].", "startOffset": 138, "endOffset": 142}, {"referenceID": 18, "context": "Because of the non-smooth regularization g(W ), the composite objectives in MTL are typically solved via the proximal gradient based first order optimization methods such as FISTA [20], SpaRSA [21], and more recently, second order proximal methods such as PNOPT [22].", "startOffset": 180, "endOffset": 184}, {"referenceID": 19, "context": "Because of the non-smooth regularization g(W ), the composite objectives in MTL are typically solved via the proximal gradient based first order optimization methods such as FISTA [20], SpaRSA [21], and more recently, second order proximal methods such as PNOPT [22].", "startOffset": 193, "endOffset": 197}, {"referenceID": 20, "context": "Because of the non-smooth regularization g(W ), the composite objectives in MTL are typically solved via the proximal gradient based first order optimization methods such as FISTA [20], SpaRSA [21], and more recently, second order proximal methods such as PNOPT [22].", "startOffset": 262, "endOffset": 266}, {"referenceID": 5, "context": "coordinate update problems by using Krasnosel\u2019skii-Mann (KM) iteration [6], [7].", "startOffset": 71, "endOffset": 74}, {"referenceID": 6, "context": "coordinate update problems by using Krasnosel\u2019skii-Mann (KM) iteration [6], [7].", "startOffset": 76, "endOffset": 79}, {"referenceID": 21, "context": "The proposed AMTL uses a backward-forward operator splitting method [23], [8] to solve problem (III.", "startOffset": 68, "endOffset": 72}, {"referenceID": 7, "context": "The proposed AMTL uses a backward-forward operator splitting method [23], [8] to solve problem (III.", "startOffset": 74, "endOffset": 77}, {"referenceID": 5, "context": "We can thus follow [6] to perform task block coordinate update at the backward-forward iteration,", "startOffset": 19, "endOffset": 22}, {"referenceID": 5, "context": "4 is the update rule of KM iteration discussed in [6].", "startOffset": 50, "endOffset": 53}, {"referenceID": 5, "context": "4 of [6] to see how Eq.", "startOffset": 5, "endOffset": 8}, {"referenceID": 5, "context": "When applying the coordinate update scheme in [6], all task nodes have access to the shared memory, and they do not communicate with each other.", "startOffset": 46, "endOffset": 49}, {"referenceID": 5, "context": "The results in [6] can be applied to directly obtain the convergence.", "startOffset": 15, "endOffset": 18}, {"referenceID": 5, "context": "We note that some MTL algorithms based on sparsity-inducing norms may not have a unique solution, as commonly seen in many sparse learning formulations, but in this case we typically can add an `2 term to ensure the strict convexity and obtain linear convergence as shown in [6].", "startOffset": 275, "endOffset": 278}, {"referenceID": 22, "context": "An example of such technique is the elastic net variant from the original Lasso problem [25].", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "The dynamic step was recently used in a specific setting via asynchronous optimization to achieve better overall performance [26].", "startOffset": 125, "endOffset": 129}, {"referenceID": 5, "context": "In our experiments, we simulate the distributed environment using the shared memory architecture in [6] with network delays introduced to the work nodes.", "startOffset": 100, "endOffset": 103}, {"referenceID": 17, "context": "The proximal mapping for nuclear norm regularization is given by [19], [27]:", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "The proximal mapping for nuclear norm regularization is given by [19], [27]:", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "Instead of computing the full SVD at every step, online SVD can also be used [28].", "startOffset": 77, "endOffset": 81}, {"referenceID": 26, "context": "School dataset has exam records of 139 schools in 1985, 1986, and 1987 provided by the London Education Authority (ILEA) [29].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "Another public dataset used in experiments was Multi-Task Facial Landmark (MTFL) dataset [31].", "startOffset": 89, "endOffset": 93}, {"referenceID": 5, "context": "We note that current AMTL implementation is based on the ARock [6] framework, which largely limits our capability of conducting experiments for different network structures.", "startOffset": 63, "endOffset": 66}], "year": 2016, "abstractText": "Many real-world machine learning applications involve several learning tasks which are inter-related. For example, in healthcare domain, we need to learn a predictive model of a certain disease for many hospitals. The models for each hospital may be different because of the inherent differences in the distributions of the patient populations. However, the models are also closely related because of the nature of the learning tasks modeling the same disease. By simultaneously learning all the tasks, multi-task learning (MTL) paradigm performs inductive knowledge transfer among tasks to improve the generalization performance. When datasets for the learning tasks are stored at different locations, it may not always be feasible to transfer the data to provide a data-centralized computing environment due to various practical issues such as high data volume and privacy. In this paper, we propose a principled MTL framework for distributed and asynchronous optimization to address the aforementioned challenges. In our framework, gradient update does not wait for collecting the gradient information from all the tasks. Therefore, the proposed method is very efficient when the communication delay is too high for some task nodes. We show that many regularized MTL formulations can benefit from this framework, including the low-rank MTL for shared subspace learning. Empirical studies on both synthetic and realworld datasets demonstrate the efficiency and effectiveness of the proposed framework.", "creator": "LaTeX with hyperref package"}}}