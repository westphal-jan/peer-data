{"id": "1609.09580", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Referential Uncertainty and Word Learning in High-dimensional, Continuous Meaning Spaces", "abstract": "This paper discusses lexicon word learning in high-dimensional meaning spaces from the viewpoint of referential uncertainty. We investigate various state-of-the-art Machine Learning algorithms and discuss the impact of scaling, representation and meaning space structure. We demonstrate that current Machine Learning techniques successfully deal with high-dimensional meaning spaces. In particular, we show that exponentially increasing dimensions linearly impact learner performance and that referential uncertainty from word sensitivity has no impact.", "histories": [["v1", "Fri, 30 Sep 2016 03:20:52 GMT  (604kb,D)", "http://arxiv.org/abs/1609.09580v1", "Published as Spranger, M. and Beuls, K. (2016). Referential uncertainty and word learning in high-dimensional, continuous meaning spaces. In Hafner, V. and Pitti, A., editors, Development and Learning and Epigenetic Robotics (ICDL-Epirob), 2016 Joint IEEE International Conferences on, 2016. IEEE"]], "COMMENTS": "Published as Spranger, M. and Beuls, K. (2016). Referential uncertainty and word learning in high-dimensional, continuous meaning spaces. In Hafner, V. and Pitti, A., editors, Development and Learning and Epigenetic Robotics (ICDL-Epirob), 2016 Joint IEEE International Conferences on, 2016. IEEE", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["michael spranger", "katrien beuls"], "accepted": false, "id": "1609.09580"}, "pdf": {"name": "1609.09580.pdf", "metadata": {"source": "CRF", "title": "Referential Uncertainty and Word Learning in High-dimensional, Continuous Meaning Spaces", "authors": ["Michael Spranger", "Katrien Beuls"], "emails": ["michael.spranger@gmail.com", "katrien@ai.vub.ac.be"], "sections": [{"heading": null, "text": "In fact, it is such that most of them will be able to move into a different world, in which they are able to live than in another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they live, in which they, in which they, in which they, in which they, in which they, in which they are able"}, {"heading": "II. DESCRIPTION GAMES", "text": "An interaction pattern (game) that is commonly used in word learning models is the Description Game (DG) = 1. The basic structure of DG is as follows: The learner observes a specific situation (context) of l objects (l = 1 for this work) and also observes k words uttered by the tutor (k = 5 for this work).The learner must then learn the meaning of these words by integrating information about various experiments.The success of the learner is measured by testing which words the learner produces for objects and how these overlap with the production of words by the tutor. An important aspect of these games is the tutor strategy - the representation and algorithm that the tutor uses to produce k words for an object."}, {"heading": "III. DESCRIPTION GAMES AND MACHINE LEARNING", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "IV. EXPERIMENTAL SETUP", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Datasets", "text": "We compare different learners with simulated and robot datasets. The robot datasets consist of 20 different objects (see Figure 2) of different color, shape, size. We have recorded approximately 1000 scenes in which two robots observe objects from different perspectives. Scenes differ in the position of objects, which also changes the perceived shape and color characteristics of the objects. For each object, n = 17 feature dimensions are extracted: x, y, z position, YUV color values (mean min, max), width, height, length. The data is scaled between 0 and 1 using a linear scaler and for some classifiers to zero mean, unit variance.i) GRO1: This dataset consists of all object observations made by each of the two robots in a matrix (4532 \u00d7 17). GRO1 is used to experiment, with the tutor and the learner having the same object perception, in particular the same object datasets for two different object datasets. These are the same object datasets:"}, {"heading": "B. Methods", "text": "Students are trained using samples of objects and words encoded by a hot vector generated by the tutor. Each classifier must predict the correct set of (new) vectors of objects by predicting a one-time vector encoding of word activations. Then, we measure the difference between the tutor's production and the learner's prediction. For learning algorithms that predict probabilities p for words (e.g. MLP), if p > 0.5 then the word is counted as a prediction. For all experiments here, we draw | W | = 100 prototypes and weights for the tutor (as described in Section II) and perform a quadruple validation of data sets, each consisting of 4532 samples, which means that the training takes place on around 3400 samples and tests on 1100 new samples are optimized. In summary, the standard parameters for our evaluations are SI17 (number of features), 100 samples (M = number of words), and (number of M = number of words)."}, {"heading": "C. Measures", "text": "In this paper, we use a single measure of performance: f-score [23]. F-score is defined as the harmonious mean of precision and memory. Depending on the classification problem (binary, multiclass, multi-label), there are different definitions of precision and memory. In general, precision measures the number of wrong words per sample that the learner has found. Let's remember how many of the correct words were predicted by the learner. We use a specific f-score measure that is called an example (or sample) and does not take into account unbalanced word distributions. An f-score of 100 means that all words and only the words pronounced by the learner are pronounced by the learner."}, {"heading": "V. RESULTS", "text": "Table I shows the performance of different classifiers based on grounded and simulated data. Many learners perform well on the task in terms of complexity of the task. Simple algorithms such as Gaussian NB or linear models perform best. More complex methods such as ensemble methods generally perform well. SIM and GRO2 have the most powerful method, the multi-layered Perceptron MLP. GRO1 Gradient Boosting has a lead of up to 20 points. Although ensemble methods generally perform fairly similarly, none of the methods fail disastrously, mainly due to hyperparameter optimization. Interestingly, all methods perform worse than GRO1 in some cases (e.g. Passive Agressive) by up to 20 points. This indicates that methods are able to exploit the structure present in grounded data. However, all methods perform worse in GRO2 than in GRO1. In some cases, the performance differs between GROA 1.30 and GROA 1.30 points."}, {"heading": "A. Scaling Object Feature Dimensions", "text": "Consequently, we manipulated the number of dimensions of object characteristics n. The number of dimensions in the grounded datasets is determined by the vision system, so we increased the number of dimensions for simulated data. All other parameters remain the same. Figure 3 shows how classifiers for n * (10, 100, 1000, 10000) behave. The average performance of all classifiers deteriorates linearly with orders of magnitude differences in n for the best-performing classifiers (MLP, AdaBoost). These results suggest that classifiers optimized for different n and / or the number of training samples could actually handle even higher n-dimensional data (remember that all classifiers were optimized for n = 17). There is one classifier that works poorly all the time (MultinomialNB), while others (e.g. ExtraTrees, RandomForest) work much faster with the number of others (the others, the ones that work best)."}, {"heading": "B. Scaling Word Sensitivity", "text": "Another dimension of scaling is the sensitivity of words. Prototype weights for all experiments reported so far are based on a binomial distribution B (1, p = 0.5), which means that words are on average sensitive to half of the dimensions. In our experiments, reference uncertainty is linked to the fact that words can refer to aspects of objects. To test learners, we conducted experiments for different p dimensions using {0.1, 0.25, 0.5, 0.75, 1.0} and n = 100. All other parameters remain the same. Figure 4 shows that quite a few learners (AdaBoost, ExtraTrees, MLP, etc.) have absolutely no problem with different p. We can conclude that these learners will perform well in mixed languages, where some words encompass all characteristics of an object and some are more specific and relate only to certain characteristics. Other learners, such as KNeighbors, will do better with greater p."}, {"heading": "C. Online Learning", "text": "One important aspect of language acquisition from a developmental point of view is online learning. We tested how well the algorithms performed over time by gradually training classifiers on the training set. For example, we train on the first m object observations and evaluate the f-score on the test data set. Figure 5 shows the performance of classifiers over time. Pretty much all classifiers learn very quickly, with most successes achieved on the first 500 training samples. In other words, the learner will be fully sufficient after 500 interactions with the tutor. After 1000 training samples, all classifiers are within 5 points of their final f-score. These results for online learning are quite remarkable as there are 100 words to be learned. Certainly, it helps here that certain words are used frequently and others less frequently. We analysed the results in terms of the frequency of the words and it becomes clear that this is actually a big driver for the speed of learning."}, {"heading": "D. Discussion of Results", "text": "In fact, it is such that the majority of them are able to survive themselves without there being a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is a process in which there is in which there is a process in which there is a process in which there is in which there is a process in which there is in which there is a process in which there is a process in"}, {"heading": "VI. RELATED WORK", "text": "In this sense, the problem in Quine is greater; even if one knows the speaker, one still knows nothing about the aspect of the speaker to which the word refers (color, shape, etc.); the question remains what reference uncertainty is solved by the children (possibly both)."}, {"heading": "VII. CONCLUSION", "text": "Another goal of models is to capture the essence of the learning problem, describe how difficult it is and how the most common algorithms work. This paper covers aspects of the second goal. We defined an abstract version of the word learning problem, translated it into machine learning, and compared the state of the art with the problem. This forms a baseline that can be used to understand word learning from a complexity standpoint. Source code and data sets are published online at https: / / github.com / mspranger / icdl2016language."}], "references": [{"title": "Word and object", "author": ["W.V.O. Quine"], "venue": "MIT press", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "and L", "author": ["J. Fontanari", "V. Tikhanoff", "A. Cangelosi", "R. Ilin"], "venue": "Perlovsky, \u201cCross-situational learning of object\u2013word mapping using neural modeling fields,\u201d Neural Networks, vol. 22, no. 5, pp. 579\u2013585", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2009}, {"title": "and R", "author": ["G. Kachergis", "C. Yu"], "venue": "M. Shiffrin, \u201cAn associative model of adaptive inference for learning word\u2013referent mappings,\u201d Psychonomic bulletin & review, vol. 19, no. 2, pp. 317\u2013324", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "A computational study of cross-situational techniques for learning word-to-meaning mappings,", "author": ["J.M. Siskind"], "venue": "Cognition, pp", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1996}, {"title": "and T", "author": ["J. De Beule", "B. De Vylder"], "venue": "Belpaeme, \u201cA cross-situational learning algorithm for damping homonymy in the guessing game,\u201d in Artificial Life X. MIT Press", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2006}, {"title": "Coping with combinatorial uncertainty in word learning: A flexible usage-based model,\u201d in The Evolution of Language", "author": ["P. Wellens"], "venue": "Proceedings of the 7Th International Conference. World Scientific,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2008}, {"title": "Word and category learning in a continuous semantic domain: comparing cross-situational and interactive learning,", "author": ["T. Belpaeme", "A. Morse"], "venue": "Advances in Complex Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Grounded lexicon acquisition - case studies in spatial language,\u201d in Development and Learning and Epigenetic Robotics (ICDL- Epirob)", "author": ["M. Spranger"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "A review on multi-label learning algorithms,", "author": ["M.-L. Zhang", "Z.-H. Zhou"], "venue": "Knowledge and Data Engineering, IEEE Transactions on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "and I", "author": ["G. Tsoumakas", "I. Katakis"], "venue": "Vlahavas, \u201cMining multi-label data,\u201d in Data mining and knowledge discovery handbook. Springer", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Nearest neighbor pattern classification,", "author": ["T.M. Cover", "P.E. Hart"], "venue": "Information Theory, IEEE Transactions on,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1967}, {"title": "Cognitive representations of semantic categories,", "author": ["E. Rosch"], "venue": "Journal of Experimental Psychology: General,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1975}, {"title": "and Y", "author": ["K. Crammer", "O. Dekel", "J. Keshet", "S. Shalev-Shwartz"], "venue": "Singer, \u201cOnline passive-aggressive algorithms,\u201d The Journal of Machine Learning Research, vol. 7, pp. 551\u2013585", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2006}, {"title": "Large-scale machine learning with stochastic gradient descent,", "author": ["L. Bottou"], "venue": "Proceedings of COMPSTAT\u20192010. Springer,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2010}, {"title": "A desicion-theoretic generalization of online learning and an application to boosting,", "author": ["Y. Freund", "R.E. Schapire"], "venue": "Computational learning theory. Springer,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1995}, {"title": "Greedy function approximation: a gradient boosting machine,", "author": ["J.H. Friedman"], "venue": "Annals of statistics,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2001}, {"title": "Random forests,", "author": ["L. Breiman"], "venue": "Machine learning,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2001}, {"title": "and L", "author": ["P. Geurts", "D. Ernst"], "venue": "Wehenkel, \u201cExtremely randomized trees,\u201d Machine learning, vol. 63, no. 1, pp. 3\u201342", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2006}, {"title": "Data mining with decision trees: theory and applications", "author": ["L. Rokach", "O. Maimon"], "venue": "World scientific", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "On discriminative vs", "author": ["A. Jordan"], "venue": "generative classifiers: A comparison of logistic regression and naive bayes,\u201d Advances in neural information processing systems, vol. 14, p. 841", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2002}, {"title": "Dealing with Perceptual Deviation - Vague Semantics for Spatial Language and Quantification,", "author": ["M. Spranger", "S. Pauw"], "venue": "in Language Grounding in Robots. Springer,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "and H", "author": ["N. Spola\u00f4r", "M.C. Monard", "G. Tsoumakas"], "venue": "D. Lee, \u201cA systematic review of multi-label feature selection and a new method based on label construction,\u201d Neurocomputing", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "A systematic analysis of performance measures for classification tasks,", "author": ["M. Sokolova", "G. Lapalme"], "venue": "Information Processing & Management,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2009}, {"title": "Incremental grounded language learning in robot-robot interactions - examples from spatial language,\u201d in Development and Learning and Epigenetic Robotics (ICDL-Epirob)", "author": ["M. Spranger"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "and J", "author": ["M. Frank", "N.D. Goodman"], "venue": "B. Tenenbaum, \u201cA bayesian framework for cross-situational word-learning.\u201d in Advances in Neural Information Processing Systems", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "and E", "author": ["S.M. Pruden", "K. Hirsh-Pasek", "R.M. Golinkoff"], "venue": "A. Hennon, \u201cThe birth of words: Ten-month-olds learn words through perceptual salience,\u201d Child development, vol. 77, no. 2, pp. 266\u2013280", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "The structural sources of verb meanings,", "author": ["L. Gleitman"], "venue": "Language acquisition,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1990}, {"title": "Children\u2019s use of mutual exclusivity to constrain the meanings of words,", "author": ["E.M. Markman", "G.F. Wachtel"], "venue": "Cognitive psychology,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "The principle of contrast: A constraint on language acquisition,", "author": ["E.V. Clark"], "venue": "Mechanisms of language acquisition,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 1987}], "referenceMentions": [{"referenceID": 0, "context": "Quine [1] famously framed referential uncertainty as a general problem everybody faces when trying to learn an unknown language.", "startOffset": 6, "endOffset": 9}, {"referenceID": 1, "context": "[2], [3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[2], [3].", "startOffset": 5, "endOffset": 8}, {"referenceID": 3, "context": "b) Combinations of Feature Models (CFM): Some researchers [4], [5], [6] have modeled word meaning as combinations of symbolic features.", "startOffset": 58, "endOffset": 61}, {"referenceID": 4, "context": "b) Combinations of Feature Models (CFM): Some researchers [4], [5], [6] have modeled word meaning as combinations of symbolic features.", "startOffset": 63, "endOffset": 66}, {"referenceID": 5, "context": "b) Combinations of Feature Models (CFM): Some researchers [4], [5], [6] have modeled word meaning as combinations of symbolic features.", "startOffset": 68, "endOffset": 71}, {"referenceID": 6, "context": "c) Continuous Meaning Space Models (CMS): Few models address the learning of words related to representations in continuous vector spaces [7], [8].", "startOffset": 138, "endOffset": 141}, {"referenceID": 7, "context": "c) Continuous Meaning Space Models (CMS): Few models address the learning of words related to representations in continuous vector spaces [7], [8].", "startOffset": 143, "endOffset": 146}, {"referenceID": 0, "context": "In this paper, objects are represented by n dimensional feature vectors o \u2208 [0, 1].", "startOffset": 76, "endOffset": 82}, {"referenceID": 0, "context": "The tutor represents each word using a prototype p \u2208 [0, 1] and a weight vector w \u2208 [0, 1].", "startOffset": 53, "endOffset": 59}, {"referenceID": 0, "context": "The tutor represents each word using a prototype p \u2208 [0, 1] and a weight vector w \u2208 [0, 1].", "startOffset": 84, "endOffset": 90}, {"referenceID": 0, "context": "For an object o \u2208 [0, 1] the tutor first computes a weighted Euclidean distance wd wdw,p(o) = \u221a\u221a\u221a\u221a n \u2211", "startOffset": 18, "endOffset": 24}, {"referenceID": 0, "context": "Together with the object distribution in [0, 1] this leads to interesting, non-linear interactions between words and objects.", "startOffset": 41, "endOffset": 47}, {"referenceID": 8, "context": "problem [9], [10].", "startOffset": 8, "endOffset": 11}, {"referenceID": 9, "context": "problem [9], [10].", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "d) Nearest neighbor (NN): One of the simplest and often best performing methods is nearest neighbor - also called KNN or in this paper KNeighbors [11] .", "startOffset": 146, "endOffset": 150}, {"referenceID": 11, "context": "This method is among the most widely used in word learning because it corresponds nicely with ideas in psychology [12].", "startOffset": 114, "endOffset": 118}, {"referenceID": 12, "context": "We are using various classifiers: Logistic Regression, Online Passive Aggressive (PA) [13] and Stochastic Gradient Descent (SGD) [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 13, "context": "We are using various classifiers: Logistic Regression, Online Passive Aggressive (PA) [13] and Stochastic Gradient Descent (SGD) [14].", "startOffset": 129, "endOffset": 133}, {"referenceID": 14, "context": "There are two main methods in this group: AdaBoost [15] and Gradient Boosting [16].", "startOffset": 51, "endOffset": 55}, {"referenceID": 15, "context": "There are two main methods in this group: AdaBoost [15] and Gradient Boosting [16].", "startOffset": 78, "endOffset": 82}, {"referenceID": 16, "context": "Here, we use Random Forest [17] and Extra Trees [18], which use ensembles of decision trees.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "Here, we use Random Forest [17] and Extra Trees [18], which use ensembles of decision trees.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "Decision trees are a non-parametric method that learns binary decisions (nodes) and arranges them in a binary tree [19].", "startOffset": 115, "endOffset": 119}, {"referenceID": 19, "context": "We use Gaussian Naive Bayes classifiers [20] (normal distributions, independent features) and Multinomial Naive Bayes (multinomial distributions, independent features).", "startOffset": 40, "endOffset": 44}, {"referenceID": 20, "context": "GRO2 is used to evaluate what happens if there is perceptual deviation [21].", "startOffset": 71, "endOffset": 75}, {"referenceID": 21, "context": "Classifiers not supporting multi-class, multi-label by default were trained using one-vs-rest [22].", "startOffset": 94, "endOffset": 98}, {"referenceID": 22, "context": "In this paper we use a single performance measure: f-score [23].", "startOffset": 59, "endOffset": 63}, {"referenceID": 7, "context": "Generally speaking it has been found that tutoring strategies help learners [8], [24].", "startOffset": 76, "endOffset": 79}, {"referenceID": 23, "context": "Generally speaking it has been found that tutoring strategies help learners [8], [24].", "startOffset": 81, "endOffset": 85}, {"referenceID": 24, "context": "Many studies are concerned with enumerable objects in context and how this leads to referential uncertainty (see [25], [7], [5] among", "startOffset": 113, "endOffset": 117}, {"referenceID": 6, "context": "Many studies are concerned with enumerable objects in context and how this leads to referential uncertainty (see [25], [7], [5] among", "startOffset": 119, "endOffset": 122}, {"referenceID": 4, "context": "Many studies are concerned with enumerable objects in context and how this leads to referential uncertainty (see [25], [7], [5] among", "startOffset": 124, "endOffset": 127}, {"referenceID": 3, "context": "In this paper, we use referential uncertainty closer to Quine\u2019s formulation and early studies by Siskind [4].", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "Often the learner is modeled after the tutor and both use the same production and interpretation algorithms (see [8], [6] for some recent examples).", "startOffset": 113, "endOffset": 116}, {"referenceID": 5, "context": "Often the learner is modeled after the tutor and both use the same production and interpretation algorithms (see [8], [6] for some recent examples).", "startOffset": 118, "endOffset": 121}, {"referenceID": 25, "context": "Some of them such as perceptual biases [26] could potentially be exploited by learning algorithms - if the language affords it.", "startOffset": 39, "endOffset": 43}, {"referenceID": 26, "context": "Other child language learning strategies based on linguistic constraints [27] are not by definition part of the learning paradigm discussed in this paper.", "startOffset": 73, "endOffset": 77}, {"referenceID": 27, "context": "The impact of strategies such as mutual exclusivity [28] and contrast [29] on the learning problem defined in this paper is subject to future work.", "startOffset": 52, "endOffset": 56}, {"referenceID": 28, "context": "The impact of strategies such as mutual exclusivity [28] and contrast [29] on the learning problem defined in this paper is subject to future work.", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "This paper discusses lexicon word learning in highdimensional meaning spaces from the viewpoint of referential uncertainty. We investigate various state-of-the-art Machine Learning algorithms and discuss the impact of scaling, representation and meaning space structure. We demonstrate that current Machine Learning techniques successfully deal with high-dimensional meaning spaces. In particular, we show that exponentially increasing dimensions linearly impact learner performance and that referential uncertainty from word sensitivity has no impact.", "creator": "LaTeX with hyperref package"}}}