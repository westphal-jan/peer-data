{"id": "1610.00030", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Modeling Language Change in Historical Corpora: The Case of Portuguese", "abstract": "This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8% accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change.", "histories": [["v1", "Fri, 30 Sep 2016 20:57:01 GMT  (71kb,D)", "http://arxiv.org/abs/1610.00030v1", "Proceedings of Language Resources and Evaluation (LREC)"]], "COMMENTS": "Proceedings of Language Resources and Evaluation (LREC)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcos zampieri", "shervin malmasi", "mark dras"], "accepted": false, "id": "1610.00030"}, "pdf": {"name": "1610.00030.pdf", "metadata": {"source": "CRF", "title": "Modeling Language Change in Historical Corpora: The Case of Portuguese", "authors": ["Marcos Zampieri", "Shervin Malmasi", "Mark Dras"], "emails": ["marcos.zampieri@dfki.de,", "shervin.malmasi@mq.edu.au,", "mark.dras@mq.edu.au"], "sections": [{"heading": null, "text": "Keywords: Language change, Timetext classification, Vector machine support, Text categorization"}, {"heading": "1. Introduction", "text": "It is well known that language changes over time, both in spoken and written forms. Changes in written language can manifest themselves in many ways, such as the use of the lexicon, grammatical structures, and textual stylistics. Recent studies have shown that it is possible to use language changes to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015), a task known as temporal text classification, which, to our knowledge, has not been substantially explored in literature as other text classification tasks. This paper contributes in this direction. In this paper, we examine the use of supervised machine learning classifiers to predict when a text was written using lexical and morphosyntactic information, and the classifiers were tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, texts from the 16th century, 2013 and early 20th century), and the early 20th century."}, {"heading": "2. Related Work", "text": "The time in which they engage with the dynamics of the content they have found in databases (e.g., after 2010) will be determined by the time in which they engage in the search for the time, and the time in which they engage with the time, to the time in which they engage with the time. The time in which they engage with the time, will be determined by the time in which they look back at the time in which they live. The time in which they look back at the time in which they live, will be from time to time, from time to time, from time to time, from time to time to time, from time to time to time, from time to time to time, from time to time to time, from time to time to time, from time to time to time, from time to time to time, from time to time to time, from time to time to time to time, from time to time to time to time, from time to time to time to time to time, from time to time to time to time, from time to time to time to time to time, from time to time to time to time to time, from time to time to time to time to time, from time to time to time to time, from time to time to time to time, from time to time to time to time, from time to time to time to time, from time to time to time to time, from time to time to time to time, from time to time to time, from time to time to time to time, from time to time to time to time, from time to time to time to time, from time to time to time, from time to time to time to time, from time to time to time to time to time, from time to time to time to time, from time to time to time to time to time, from time to time to time to time, from time to time to time to time to time, from time to time to time to time, from time to time to time to time to time to time, from time to time to time to time to time to time, from time to time to time, from time to time to time to time to time to time, from time to time to time to time to time to time, from time to time to time, from time to time to time to time to time to time to time, from time to time to time to time to time to"}, {"heading": "3. Methods", "text": "Following the results obtained through supervised learning approaches to the SemEval DTE task, we approach the task in this paper using a supervised multi-class classification. To test our method, we used a Portuguese historical corpus, the aforementioned Colonia2, and assign a label to each text in the corpus corresponding to the time interval in which the text was written. As characteristics, we use the lexicon, which is arranged as a wordpocket or verbs, and morphosyntatic information, which is represented by POS tags. First, we present a preliminary experiment based on a small sample of the data, each containing excerpts of about 2,000 characters. In this experiment, we compared the performance of two machine-learning 2006 sample classifiers (NIBM) and 2008 (NIBM), (NIBM)."}, {"heading": "3.1. Data and Features", "text": "In fact, we are able to assert ourselves, we are able to assert ourselves, we are able to assert ourselves in the world, and we are able to assert ourselves in the world, we are able to assert ourselves, we are in the world, and we are able to assert ourselves in the world, we are able to assert ourselves in the world, we are in the world. \""}, {"heading": "4. Results", "text": "In this section, we present the results of three experiments: 1. In Section 4.1, we describe preliminary experiments using a small sample of the corpus of 87 documents from the 17th to early 20th centuries. We train two algorithms (SVM and MNB), using both words and POS tags that are presented as a word bag to predict the century in which the text was published. 2. In Section 4.2, we use the method of Malmasi and Dras (2014a) to create artificial compound documents for training and testing using the entire text set available in Colonia (from the 16th to the early 20th centuries). We train an SVM classifier to predict the century in which the text was published, and in light of the significant increase in training material, we report a significant increase in performance using POS tags or words presented as universes, bi- and trigramms.3. Finally, in Section 4.3, we replicate the methods used in Section 4.2 for a smaller period of 50 years."}, {"heading": "4.1. Preliminary Experiments", "text": "As preliminary experiments of this work we use a bag word model on a sample of the data previously used in the above mentioned study by S. Tajner and Zampieri (2013). Each document in this sample contains up to 2,000 characters. Criteria for the sample were inspired by the Brown familycorpora (Francis and Kucera, 1979). The distribution of texts over centuries is presented below. In Colonia there are not many texts from each class and only a few from the 16th century. Therefore, in S. tajner and S. Zampieri (2013) and in this preliminary experiment we consider texts from this century and propose an experiment with four instead of the five classes represented in Colonia. In Table 2 we present accuracy results with k = 10. We considered the majority class (19th century) as the output power. The best results were obtained by SVM, regardless of the characteristics used. SVM reached 74.1% accuracy in the identification of the century of texts with k = 10th majority class (the 19th century was considered the initial power)."}, {"heading": "4.2. Increasing the Sample", "text": "The small number of texts is a known limitation of most historical corpora. We address this question by generating artificial data using the methods described in Section 3. In this experiment, we used a series of 1,500 artificially generated documents, each of which combines sets of different texts to represent each class. 7 Details of the final data used in this experiment are presented in Table 3. Results obtained with an SVM classifier are presented in Table 4. Word n-grams of order 1-3 and POS n-grams of order 1-3 have been extracted and a single SVM classifier has been trained on each of these six n-gram-specific characteristics. Using the same evaluation methods of the preliminary experiment, we report accuracy results under k-fold cross validation, with k = 10. The results are compared with a random baseline of 20%. 7This was the largest number possible to achieve an even distribution between classes to the sequence, the best results would be achieved with unrasteric results."}, {"heading": "4.3. Smaller Time Intervals", "text": "Since our method of predicting the century of each text is almost perfect, we would like to evaluate its performance in predicting the publication date of texts with a shorter time interval. To this end, we divided the documents in the corpus into 50-year intervals, resulting in 9 classes. Distribution of the data along with the total number of tokens per time interval is given in Table 5. We used the same methodology for generating artificial documents, resulting in a total of 450 documents per class, 8 4,050 documents in total, and 1.35 million tokens. Distribution of the data along with the total number of tokens per time interval is given in Table 5. We used the sample for automatic classification and the results are presented in Table 6. Once again, our best performance through word unigrations achieved an accuracy of 99.8% using word unigrams. For the other settings, we observed a slight (and expected) decrease in performance, with the lowest results achieved by POS unigraphs."}, {"heading": "5. Indicative Features of Language Change", "text": "In this section, we look at the most informative characteristics for each of the five centuries represented in Colonia 3. We try to highlight patterns that are, or are likely to be, generally good indicators of language change. Therefore, most informative characteristics have been elaborated using the methodology proposed in Malmasi and Dras. This works by evaluating the characteristics according to the weights assigned by the SVM model. (2013) In this way, we have a similar text classification task that includes diatopic variation, linguistically motivated characteristics that usually do not exceed word- and character-based characteristics. In fact, our results corroborate this assertion so that the use of characteristics represented by POS tags and / or morphological information can provide interesting insights into language variations that are analyzed by looking at the results of classifiers."}, {"heading": "6. Conclusion", "text": "We investigated the use of words and POS tags to model lexical and syntactical variations in historical corpora for the purpose of temporal text classification. Our work expands general knowledge in the task by using lexical and POS information for this task for the first time. We claim that this methodology is an interesting strategy for coping with small amounts of available text, which is a well-known limitation of many historical corpora. The approach proposed in this paper is capable of predicting the date of publication of texts at intervals of 100 and 50 years, using word unigrams with 99.8% accuracy. It is also capable of predicting the date of publication of texts that have only POS tags that reach performance of 100 years, and 90.1% accuracy for intervals of 50 years."}, {"heading": "6.1. Future Work: Finding Optimal Time Intervals", "text": "In our understanding, one of the limitations of most temporal text classification experiments (including ours) is the arbitrary definition of time intervals. In the case of our dataset, time intervals of a century are too long, and they often fail to capture linguistic changes that occur at a particular time and do not coincide with the turn of the century. On the other hand, working with too short intervals for historical datasets is often unfeasible because there are not enough data points that can be divided into a large number of classes. The smallest time interval we could work with this corpus was 50 years. Nevertheless, we claim that defining arbitrary intervals as evidence of a concept and a perfect fit for supervised classification methods that require a predefined set of classes, such as the SVM-based approach that is best presented in this paper. In real-world tasks, however, one might be interested in using finer grained time intervals to better predict all the structure of time and better predict it."}, {"heading": "Acknowledgements", "text": "We would like to thank the anonymous reviewers for providing us with important feedback and constructive comments on the historical texts of Dakka. In Proceedings of ICDM Workshops.Castro, I. (1991). Curso de Historia da Lingua Portuguesa. Universidade Aberta.Chambers, N. (2012). Labeling documents with timestamps: Learning from their time expressions. In Proceedings of ACL.Ciobanu, A. M., Dinu, L. P., Niculae, V., and Sulea, O.-M. (2013a). Temporal text classification for romanian novels set in the past. In Proceedings of RANLP.Ciobanu, A. M., Dinu, L. P., Dinu, Dinu, Dinu, A., and Niculae, V. (2013b)."}], "references": [{"title": "Text categorization with considering temporal patterns of term usages", "author": ["H. Abe", "S. Tsumoto"], "venue": "Proceedings of ICDM Workshops.", "citeRegEx": "Abe and Tsumoto,? 2010", "shortCiteRegEx": "Abe and Tsumoto", "year": 2010}, {"title": "Curso de Historia da Lingua Portuguesa", "author": ["I. Castro"], "venue": "Universidade Aberta.", "citeRegEx": "Castro,? 1991", "shortCiteRegEx": "Castro", "year": 1991}, {"title": "Labeling documents with timestamps: Learning from their time expressions", "author": ["N. Chambers"], "venue": "Proceedings of ACL.", "citeRegEx": "Chambers,? 2012", "shortCiteRegEx": "Chambers", "year": 2012}, {"title": "Temporal text classification for romanian novels set in the past", "author": ["A.M. Ciobanu", "A. Dinu", "L.P. Dinu", "V. Niculae", "Sulea", "O.-M."], "venue": "Proceedings of RANLP.", "citeRegEx": "Ciobanu et al\\.,? 2013a", "shortCiteRegEx": "Ciobanu et al\\.", "year": 2013}, {"title": "Temporal classification for historical Romanian texts", "author": ["A.M. Ciobanu", "L.P. Dinu", "A. Dinu", "V. Niculae"], "venue": "Proceedings of LaTeCH.", "citeRegEx": "Ciobanu et al\\.,? 2013b", "shortCiteRegEx": "Ciobanu et al\\.", "year": 2013}, {"title": "Answering general time-sensitive queries", "author": ["W. Dakka", "L. Gravano", "P.G. Ipeirotis"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 24(2):220\u2013 235.", "citeRegEx": "Dakka et al\\.,? 2012", "shortCiteRegEx": "Dakka et al\\.", "year": 2012}, {"title": "Automatic dating of documents and temporal text classification", "author": ["A. Dalli", "Y. Wilks"], "venue": "Proceedings of ARTE.", "citeRegEx": "Dalli and Wilks,? 2006", "shortCiteRegEx": "Dalli and Wilks", "year": 2006}, {"title": "Recovering the number of clusters in data sets with noise features using feature rescaling factors", "author": ["R.C. de Amorim", "C. Hennig"], "venue": "Information Sciences,", "citeRegEx": "Amorim and Hennig,? \\Q2015\\E", "shortCiteRegEx": "Amorim and Hennig", "year": 2015}, {"title": "Temporal language models for the disclosure of historical text", "author": ["F. de Jong", "H. Rode", "D. Hiemstra"], "venue": "In Proceedings of AHC", "citeRegEx": "Jong et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Jong et al\\.", "year": 2005}, {"title": "Effects of evolutionary linguistics in text classification", "author": ["J. Efremova", "A.M. Garc\u0131\u0301a", "J. Zhang", "T. Calders"], "venue": "In Proceedings of SLSP", "citeRegEx": "Efremova et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Efremova et al\\.", "year": 2015}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["Fan", "R.-E.", "Chang", "K.-W.", "Hsieh", "C.-J.", "Wang", "X.-R.", "Lin", "C.-J."], "venue": "Journal of Machine Learning Research, 9:1871\u20131874.", "citeRegEx": "Fan et al\\.,? 2008", "shortCiteRegEx": "Fan et al\\.", "year": 2008}, {"title": "Brown corpus manual", "author": ["W.N. Francis", "H. Kucera"], "venue": "Brown University.", "citeRegEx": "Francis and Kucera,? 1979", "shortCiteRegEx": "Francis and Kucera", "year": 1979}, {"title": "Naive bayes for text classification with unbalanced classes", "author": ["E. Frank", "R.R. Bouckaert"], "venue": "Proceedings of KDD.", "citeRegEx": "Frank and Bouckaert,? 2006", "shortCiteRegEx": "Frank and Bouckaert", "year": 2006}, {"title": "A bayesian model of diachronic meaning change", "author": ["L. Frermann", "M. Lapata"], "venue": "Transactions of the Association for Computational Linguistics, 4:31\u201345.", "citeRegEx": "Frermann and Lapata,? 2016", "shortCiteRegEx": "Frermann and Lapata", "year": 2016}, {"title": "When was it written? Automatically determining publication dates", "author": ["A. Garcia-Fernandez", "Ligozat", "A.-L.", "M. Dinarelli", "D. Bernhard"], "venue": "Proceedings of SPIRE.", "citeRegEx": "Garcia.Fernandez et al\\.,? 2011", "shortCiteRegEx": "Garcia.Fernandez et al\\.", "year": 2011}, {"title": "Pr\u00e9sentation et r\u00e9sultats du d\u00e9fi fouille de texte DEFT2010 o\u00f9 et quand un article de presse a-t-il \u00e9t\u00e9 \u00e9crit? Proceedings of DEFT", "author": ["C. Grouin", "D. Forest", "L. Da Sylva", "P. Paroubek", "P. Zweigenbaum"], "venue": null, "citeRegEx": "Grouin et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Grouin et al\\.", "year": 2010}, {"title": "Pr\u00e9sentation et r\u00e9sultats du d\u00e9fi fouille de texte deft2011 quand un article de presse a t-il \u00e9t\u00e9 \u00e9crit? \u00e0 quel article scientifique correspond ce r\u00e9sum\u00e9? Proceedings of DEFT", "author": ["C. Grouin", "D. Forest", "P. Paroubek", "P. Zweigenbaum"], "venue": null, "citeRegEx": "Grouin et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Grouin et al\\.", "year": 2011}, {"title": "Gene selection for cancer classification using support vector machines", "author": ["I. Guyon", "J. Weston", "S. Barnhill", "V. Vapnik"], "venue": "Machine learning, 46(1-3):389\u2013422.", "citeRegEx": "Guyon et al\\.,? 2002", "shortCiteRegEx": "Guyon et al\\.", "year": 2002}, {"title": "Quantitative patterns of stylistic influence in the evolution of literature", "author": ["J. Hughes", "N. Foti", "D. Krakauer", "D. Rockmore"], "venue": "Proceedings of the National Academy of Sciences, 109(20):7682\u20137686.", "citeRegEx": "Hughes et al\\.,? 2012", "shortCiteRegEx": "Hughes et al\\.", "year": 2012}, {"title": "Training linear SVMs in linear time", "author": ["T. Joachims"], "venue": "Proceedings of KDD.", "citeRegEx": "Joachims,? 2006", "shortCiteRegEx": "Joachims", "year": 2006}, {"title": "Temporal Information Retrieval. now Publishers", "author": ["N. Kanhabua", "R. Blanco", "K N\u00f8rv\u00e5g"], "venue": null, "citeRegEx": "Kanhabua et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kanhabua et al\\.", "year": 2015}, {"title": "Variations on language modeling for information retrieval", "author": ["W. Kraaij"], "venue": "Ph.D. thesis, University of Twente.", "citeRegEx": "Kraaij,? 2004", "shortCiteRegEx": "Kraaij", "year": 2004}, {"title": "Supervised language modelling for temporal resolution of texts", "author": ["A. Kumar", "M. Lease", "J. Baldridge"], "venue": "Proceedings of CIKM.", "citeRegEx": "Kumar et al\\.,? 2011", "shortCiteRegEx": "Kumar et al\\.", "year": 2011}, {"title": "Chinese Native Language Identification", "author": ["S. Malmasi", "M. Dras"], "venue": "Proceedings of EACL.", "citeRegEx": "Malmasi and Dras,? 2014a", "shortCiteRegEx": "Malmasi and Dras", "year": 2014}, {"title": "Language Transfer Hypotheses with Linear SVM Weights", "author": ["S. Malmasi", "M. Dras"], "venue": "Proceedings of EMNLP.", "citeRegEx": "Malmasi and Dras,? 2014b", "shortCiteRegEx": "Malmasi and Dras", "year": 2014}, {"title": "Multilingual Native Language Identification", "author": ["S. Malmasi", "M. Dras"], "venue": "Natural Language Engineering.", "citeRegEx": "Malmasi and Dras,? 2015", "shortCiteRegEx": "Malmasi and Dras", "year": 2015}, {"title": "Arabic Dialect Identification using a Parallel Multidialectal Corpus", "author": ["S. Malmasi", "E. Refaee", "M. Dras"], "venue": "Proceedings of PACLING.", "citeRegEx": "Malmasi et al\\.,? 2015", "shortCiteRegEx": "Malmasi et al\\.", "year": 2015}, {"title": "Word epoch disambiguation: Finding how words change over time", "author": ["R. Mihalcea", "V. Nastase"], "venue": "Proceedings of ACL.", "citeRegEx": "Mihalcea and Nastase,? 2012", "shortCiteRegEx": "Mihalcea and Nastase", "year": 2012}, {"title": "The rise and fall of the l-shaped morphome: diachronic and experimental studies", "author": ["A. Nevins", "C. Rodrigues", "K. Tang"], "venue": "Probus, 27(1):101\u2013155.", "citeRegEx": "Nevins et al\\.,? 2015", "shortCiteRegEx": "Nevins et al\\.", "year": 2015}, {"title": "Temporal text ranking and automatic dating of texts", "author": ["V. Niculae", "M. Zampieri", "L.P. Dinu", "A.M. Ciobanu"], "venue": "Proceedings of EACL.", "citeRegEx": "Niculae et al\\.,? 2014", "shortCiteRegEx": "Niculae et al\\.", "year": 2014}, {"title": "Behind the times: Detecting epoch changes using large corpora", "author": ["O. Popescu", "C. Strapparava"], "venue": "Proceedings of IJCNLP.", "citeRegEx": "Popescu and Strapparava,? 2013", "shortCiteRegEx": "Popescu and Strapparava", "year": 2013}, {"title": "Semeval-2015 task 7: Diachronic text evaluation", "author": ["O. Popescu", "C. Strapparava"], "venue": "Proceedings of SemEval.", "citeRegEx": "Popescu and Strapparava,? 2015", "shortCiteRegEx": "Popescu and Strapparava", "year": 2015}, {"title": "Temporal models of streaming social media data", "author": ["D. Preotiuc-Pietro"], "venue": "Ph.D. thesis, University of Sheffield.", "citeRegEx": "Preotiuc.Pietro,? 2014", "shortCiteRegEx": "Preotiuc.Pietro", "year": 2014}, {"title": "Ixagroupehudiac: A multiple approach system towards the diachronic evaluation of texts", "author": ["H. Salaberri", "I. Salaberri", "O. Arregi", "B. Zapirain"], "venue": "Proceedings of SemEval.", "citeRegEx": "Salaberri et al\\.,? 2015", "shortCiteRegEx": "Salaberri et al\\.", "year": 2015}, {"title": "Probabilistic part-of-speech tagging using decision trees", "author": ["H. Schmid"], "venue": "Proceedings of International Conference on New Methods in Language Processing, Manchester, UK.", "citeRegEx": "Schmid,? 1994", "shortCiteRegEx": "Schmid", "year": 1994}, {"title": "Stylistic changes for temporal text classification", "author": ["S. \u0160tajner", "M. Zampieri"], "venue": "Proceedings of TSD.", "citeRegEx": "\u0160tajner and Zampieri,? 2013", "shortCiteRegEx": "\u0160tajner and Zampieri", "year": 2013}, {"title": "UCD: Diachronic text classification with character, word, and syntactic ngrams", "author": ["T. Szymanski", "G. Lynch"], "venue": "Proceedings of SemEval.", "citeRegEx": "Szymanski and Lynch,? 2015", "shortCiteRegEx": "Szymanski and Lynch", "year": 2015}, {"title": "Colonia: Corpus of historical Portuguese", "author": ["M. Zampieri", "M. Becker"], "venue": "ZSM Studien, Special Volume on Non-Standard Data Sources in Corpus-Based Research.", "citeRegEx": "Zampieri and Becker,? 2013", "shortCiteRegEx": "Zampieri and Becker", "year": 2013}, {"title": "Ngram language models and POS distribution for the identification of Spanish varieties", "author": ["M. Zampieri", "B.G. Gebre", "S. Diwersy"], "venue": "Proceedings of TALN.", "citeRegEx": "Zampieri et al\\.,? 2013", "shortCiteRegEx": "Zampieri et al\\.", "year": 2013}, {"title": "AMBRA: A ranking approach to temporal text classification", "author": ["M. Zampieri", "A.M. Ciobanu", "V. Niculae", "L.P. Dinu"], "venue": "Proceedings of SemEval.", "citeRegEx": "Zampieri et al\\.,? 2015", "shortCiteRegEx": "Zampieri et al\\.", "year": 2015}, {"title": "Sub-document timestamping of web documents", "author": ["Y. Zhao", "C. Hauff"], "venue": "Proceedings of SIGIR.", "citeRegEx": "Zhao and Hauff,? 2015a", "shortCiteRegEx": "Zhao and Hauff", "year": 2015}, {"title": "Temporal information retrieval revisited: a focused study on the web", "author": ["Y. Zhao", "C. Hauff"], "venue": "Proceedings of the FDIA Symposium.", "citeRegEx": "Zhao and Hauff,? 2015b", "shortCiteRegEx": "Zhao and Hauff", "year": 2015}], "referenceMentions": [{"referenceID": 3, "context": "Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015).", "startOffset": 153, "endOffset": 207}, {"referenceID": 31, "context": "Recent studies have shown that it is possible to use language change to predict the approximate publication date of texts in diachronic text collections (Ciobanu et al., 2013a; Popescu and Strapparava, 2015).", "startOffset": 153, "endOffset": 207}, {"referenceID": 37, "context": "The classifiers were trained and tested on a sample of a historical Portuguese corpus called Colonia (Zampieri and Becker, 2013) which contains texts spanning from the 16th to the early 20th century.", "startOffset": 101, "endOffset": 128}, {"referenceID": 5, "context": "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).", "startOffset": 200, "endOffset": 312}, {"referenceID": 32, "context": "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).", "startOffset": 200, "endOffset": 312}, {"referenceID": 20, "context": "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).", "startOffset": 200, "endOffset": 312}, {"referenceID": 41, "context": "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).", "startOffset": 200, "endOffset": 312}, {"referenceID": 40, "context": "Information Retrieval (IR) methods, for example, often have to process temporal information in both queries and documents to deal with dynamicity of the content found in data repositories and the Web (Dakka et al., 2012; Preotiuc-Pietro, 2014; Kanhabua et al., 2015; Zhao and Hauff, 2015b; Zhao and Hauff, 2015a).", "startOffset": 200, "endOffset": 312}, {"referenceID": 2, "context": "after 2010), can help algorithms to identify the approximate publication date of texts (Chambers, 2012), but there are a number of cases in which they are not present in text and one alternative is to use features related to language change as we propose in this paper.", "startOffset": 87, "endOffset": 103}, {"referenceID": 21, "context": "(2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans.", "startOffset": 111, "endOffset": 125}, {"referenceID": 6, "context": "The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans.", "startOffset": 15, "endOffset": 34}, {"referenceID": 6, "context": "The work by de Jong et al. (2005) uses unigram language models combined with smoothing techniques and log-likelihood ratio measure (NLLR) (Kraaij, 2004) to classify documents within different time spans. The method was tested on a collection of Dutch journalistic texts published from January 1999 to February 2005. Other methods, such as Kumar et al. (2011), make use of information gain to estimate the best features in classification.", "startOffset": 15, "endOffset": 359}, {"referenceID": 5, "context": "In Dalli and Wilks (2006) researchers train a classifier to predict the publication date of texts within a time span of nine years.", "startOffset": 3, "endOffset": 26}, {"referenceID": 0, "context": "Another study that works under a similar assumption is the one published by Abe and Tsumoto (2010). The authors proposed the use of similarity metrics to categorize texts based on keywords calculated using tf-idf (term frequency - inverse document frequency).", "startOffset": 76, "endOffset": 99}, {"referenceID": 16, "context": "The corpus used was provided by the organizers of the DEFT2011 challenge (Grouin et al., 2011) which was essentially a temporal text classification ar X iv :1 61 0.", "startOffset": 73, "endOffset": 94}, {"referenceID": 15, "context": "task for French following a similar DEFT2010 challenge that included both diachronic and diatopic (regional) variation (Grouin et al., 2010).", "startOffset": 119, "endOffset": 140}, {"referenceID": 14, "context": "Garcia-Fernandez et al. (2011) report 14% accuracy in predicting the year of publication of texts and 42% accuracy in predicting the correct decade of publication.", "startOffset": 0, "endOffset": 31}, {"referenceID": 13, "context": "Lexical changes are regarded to be an important feature of diachronic text collections and researchers have proposed methods to track meaning change over time (Frermann and Lapata, 2016).", "startOffset": 159, "endOffset": 186}, {"referenceID": 11, "context": "Lexical changes are regarded to be an important feature of diachronic text collections and researchers have proposed methods to track meaning change over time (Frermann and Lapata, 2016). The study by Mihalcea and Nastase (2012) investigates how word meanings change over time in three major periods in time: 1800, 1900 and 2000.", "startOffset": 160, "endOffset": 229}, {"referenceID": 11, "context": "Lexical changes are regarded to be an important feature of diachronic text collections and researchers have proposed methods to track meaning change over time (Frermann and Lapata, 2016). The study by Mihalcea and Nastase (2012) investigates how word meanings change over time in three major periods in time: 1800, 1900 and 2000. Popescu and Strapparava (2013) look at significant changes in the use of words across time for the purpose of characterizing epochs using the Google N-Gram collection from 1614 to 2009.", "startOffset": 160, "endOffset": 361}, {"referenceID": 3, "context": "Ciobanu et al. (2013a) and Ciobanu et al.", "startOffset": 0, "endOffset": 23}, {"referenceID": 3, "context": "Ciobanu et al. (2013a) and Ciobanu et al. (2013b) applied SVM and Random Forest algorithms to classify texts of a historical Romanian text collection regarding their publication date.", "startOffset": 0, "endOffset": 50}, {"referenceID": 28, "context": "The study by Niculae et al. (2014) approached the task using ranking and pairwise comparisons to predict for each pair of documents which one is older and finally to produce a rank of all documents in a collection from older to newer.", "startOffset": 13, "endOffset": 35}, {"referenceID": 9, "context": "Another recent study to tackle the issue of time intervals is Efremova et al. (2015). In this study authors apply clustering methods to automatically obtain optimal time partitions in a dataset of historical Dutch notary acts.", "startOffset": 62, "endOffset": 85}, {"referenceID": 34, "context": "In \u0160tajner and Zampieri (2013) researchers used the style of texts calculated using readability scores to predict the publication date of Portuguese texts in the Colonia corpus.", "startOffset": 3, "endOffset": 31}, {"referenceID": 18, "context": "Another related study is the one by Hughes et al. (2012) that investigates the evolution of the style of 537 authors of the Project Gutenberg collection by looking at the usage of grammatical words.", "startOffset": 36, "endOffset": 57}, {"referenceID": 31, "context": "Results and methods are described in detail in the shared task report (Popescu and Strapparava, 2015).", "startOffset": 70, "endOffset": 101}, {"referenceID": 33, "context": "and the only team to participate in all three sub-tasks was the IXA team (Salaberri et al., 2015) who used external resources such as Google N-grams and Wikipedia Entity Linking to accomplish the task.", "startOffset": 73, "endOffset": 97}, {"referenceID": 36, "context": "The best performing system in the DTE task was the UCD team (Szymanski and Lynch, 2015) who achieved 54.", "startOffset": 60, "endOffset": 87}, {"referenceID": 35, "context": "The same sample was previously used in another temporal text classification approach relying on stylistic and readability features (\u0160tajner and Zampieri, 2013).", "startOffset": 131, "endOffset": 159}, {"referenceID": 12, "context": "In this experiment we compared the performance of two machine learning classifiers, Multinomial Naive Bayes (MNB) (Frank and Bouckaert, 2006) and SVM (Joachims, 2006).", "startOffset": 114, "endOffset": 141}, {"referenceID": 19, "context": "In this experiment we compared the performance of two machine learning classifiers, Multinomial Naive Bayes (MNB) (Frank and Bouckaert, 2006) and SVM (Joachims, 2006).", "startOffset": 150, "endOffset": 166}, {"referenceID": 10, "context": "In particular, we use the LIBLINEAR3 package (Fan et al., 2008) which has been shown to be efficient for large-scale text classification problems such as this (Malmasi and Dras, 2015).", "startOffset": 45, "endOffset": 63}, {"referenceID": 25, "context": ", 2008) which has been shown to be efficient for large-scale text classification problems such as this (Malmasi and Dras, 2015).", "startOffset": 103, "endOffset": 127}, {"referenceID": 37, "context": "The Colonia corpus is a historical Portuguese corpus, which contains texts spanning from the 16th century to the early 20th century (Zampieri and Becker, 2013).", "startOffset": 132, "endOffset": 159}, {"referenceID": 34, "context": "It contains sentence boundary mark-up and coarse-grained POS annotation carried out using TreeTagger (Schmid, 1994).", "startOffset": 101, "endOffset": 115}, {"referenceID": 28, "context": "6 To our knowledge, the corpus has been used to study different aspects of the evolution of Portuguese such as diachronic morphology (Nevins et al., 2015).", "startOffset": 133, "endOffset": 154}, {"referenceID": 26, "context": "Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015).", "startOffset": 103, "endOffset": 125}, {"referenceID": 23, "context": "Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing.", "startOffset": 29, "endOffset": 54}, {"referenceID": 23, "context": "Following the methodology of Malmasi and Dras (2014a), we randomly select and combine the sentences from the same class (time period) to generate artificial texts of approximately 330 tokens on average, creating a set of documents for training and testing. This methodology ensures that the texts for each class are a mix of different authorship styles and topics. It also means that all documents are similar and comparable in length making the task more challenging. Previous work in other text classification tasks has shown that longer texts can be easier to classify (Malmasi et al., 2015). In our experiments we model two dimensions of language variation across time, lexical and (morpho-)syntactical. We do so by extracting words and part-of-speech (POS) tags from the corpus and using them as features. To the best of our knowledge these features were not yet tested in multiclass temporal text classification for Portuguese. The most similar approach to use these features is the ranking approach proposed by Niculae et al. (2014).", "startOffset": 29, "endOffset": 1040}, {"referenceID": 23, "context": "2 we apply the method of Malmasi and Dras (2014a) to generate artificial composite documents for training and testing using the complete set of texts available in Colonia (from the 16th to the early 20th century).", "startOffset": 25, "endOffset": 50}, {"referenceID": 11, "context": "The criteria for sampling was inspired in the Brown family corpora (Francis and Kucera, 1979).", "startOffset": 67, "endOffset": 93}, {"referenceID": 34, "context": "As the preliminary experiments of this paper we use a bagof-words model on a sample of the data previously used in the aforementioned study by \u0160tajner and Zampieri (2013). Each document in this sample contains up to 2,000 token.", "startOffset": 143, "endOffset": 171}, {"referenceID": 35, "context": "For this reason, in \u0160tajner and Zampieri (2013) and in this preliminary experiment, we disregard texts from this century and propose an experiment with four classes instead of the five represented in Colonia.", "startOffset": 20, "endOffset": 48}, {"referenceID": 35, "context": "An expected outcome of these preliminary experiments is that the results using lexical and morphsyntactic features are substantially higher than the 59% accuracy reported by \u0160tajner and Zampieri (2013) using readability/stylistic features.", "startOffset": 174, "endOffset": 202}, {"referenceID": 38, "context": "This is an indication that the algorithm is able to capture language variation beyond word forms, as noted by (Zampieri et al., 2013) in a similar classification experiment to study the diatopic variation of Spanish.", "startOffset": 110, "endOffset": 133}, {"referenceID": 17, "context": "In this manner, SVMs have been successfully applied in data mining and knowledge discovery in a wide range of tasks such as identifying discriminant cancer genes (Guyon et al., 2002).", "startOffset": 162, "endOffset": 182}, {"referenceID": 22, "context": "The most informative features were extracted using the methodology proposed in Malmasi and Dras (2014b). This works by ranking the features according to the weights assigned by the SVM model.", "startOffset": 79, "endOffset": 104}, {"referenceID": 17, "context": "In this manner, SVMs have been successfully applied in data mining and knowledge discovery in a wide range of tasks such as identifying discriminant cancer genes (Guyon et al., 2002). As noted by Zampieri et al. (2013), in a similar text classification task involving diatopic variation, linguistically motivated features usually do not outperform word- and character-based features.", "startOffset": 163, "endOffset": 219}, {"referenceID": 1, "context": "In the 16th century we observed the use of a number welldocument archaisms used in that period (Castro, 1991).", "startOffset": 95, "endOffset": 109}, {"referenceID": 29, "context": "In light of this, our efforts are now concentrated on exploring ways to better represent the linearity of time (Niculae et al., 2014) or to find optimal time intervals in historical corpora as proposed by the aforementioned study by Efremova et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 39, "context": "(2014) competed in the SemEval 2015 \u2018Diachronic Text Evaluation\u2019 as the AMBRA team (Zampieri et al., 2015) and did not outperform supervised learning approaches such as the one by the UCD team (Szymanski and Lynch, 2015).", "startOffset": 83, "endOffset": 106}, {"referenceID": 36, "context": ", 2015) and did not outperform supervised learning approaches such as the one by the UCD team (Szymanski and Lynch, 2015).", "startOffset": 94, "endOffset": 121}, {"referenceID": 8, "context": ", 2014) or to find optimal time intervals in historical corpora as proposed by the aforementioned study by Efremova et al. (2015). We are currently experimenting with a recently proposed clustering method for this purpose (de Amorim and Hennig, 2015).", "startOffset": 107, "endOffset": 130}, {"referenceID": 7, "context": "We are currently experimenting with a recently proposed clustering method for this purpose (de Amorim and Hennig, 2015). It is important to note, however, that an adaptation of the ranking-based approach by Niculae et al. (2014) competed in the SemEval 2015 \u2018Diachronic Text Evaluation\u2019 as the AMBRA team (Zampieri et al.", "startOffset": 95, "endOffset": 229}], "year": 2016, "abstractText": "This paper presents a number of experiments to model changes in a historical Portuguese corpus composed of literary texts for the purpose of temporal text classification. Algorithms were trained to classify texts with respect to their publication date taking into account lexical variation represented as word n-grams, and morphosyntactic variation represented by part-of-speech (POS) distribution. We report results of 99.8% accuracy using word unigram features with a Support Vector Machines classifier to predict the publication date of documents in time intervals of both one century and half a century. A feature analysis is performed to investigate the most informative features for this task and how they are linked to language change.", "creator": "LaTeX with hyperref package"}}}