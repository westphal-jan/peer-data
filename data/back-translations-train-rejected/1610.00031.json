{"id": "1610.00031", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "Discriminating Similar Languages: Evaluations and Explorations", "abstract": "We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate the progress made between the two tasks, estimate an upper bound on possible performance using ensemble and oracle combination, and provide learning curves to help us understand which languages are more challenging. A number of difficult sentences are identified and investigated further with human annotation.", "histories": [["v1", "Fri, 30 Sep 2016 20:57:52 GMT  (321kb,D)", "http://arxiv.org/abs/1610.00031v1", "Proceedings of Language Resources and Evaluation (LREC)"]], "COMMENTS": "Proceedings of Language Resources and Evaluation (LREC)", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["cyril goutte", "serge l\\'eger", "shervin malmasi", "marcos zampieri"], "accepted": false, "id": "1610.00031"}, "pdf": {"name": "1610.00031.pdf", "metadata": {"source": "CRF", "title": "Discriminating Similar Languages: Evaluations and Explorations", "authors": ["Cyril Goutte", "Serge L\u00e9ger", "Shervin Malmasi", "Marcos Zampieri"], "emails": ["firstname.lastname@nrc.ca,", "shervin.malmasi@mq.edu.au,", "marcos.zampieri@uni-saarland.de"], "sections": [{"heading": null, "text": "Keywords: language identification, language variants, evaluation"}, {"heading": "1. Introduction", "text": "Distinguishing between similar languages and language variants is one of the greatest challenges of modern language identification systems (Tiedemann and Ljubes, 2012). Closely related languages such as Indonesian and Malay or Croatian and Serbian are very similar in both spoken and written forms, making it difficult for systems to distinguish between them. Varieties of the same language, e.g. Spanish from South America or Spain, are even more difficult to identify than similar languages. Nevertheless, recent work in both cases has shown that it is possible to train algorithms to distinguish between similar languages and language variants with high accuracy (Goutte et al., 2014; Malmasi and Dras, 2015b). This study examines in more detail the characteristics that help algorithms distinguish between similar languages, taking into account the results of two recent editions of the Discriminating between Similar Languages (DSL) shared task (Zampieri et al., 2014; Zampieri problem, 2015b)."}, {"heading": "2. Related Work", "text": "The interest in this task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Simo-Malanc et al., 2014). Interest in discrimination of similar languages, language varieties and dialects is younger, but it has increased in recent years. Examples of studies are the cases of Malaysian and Indonesian (Ranaivo-Malanc, 2006), Chinese varieties (Huang and Lee, 2008), Southern Slavic languages (Ljubes) et al, 2007; Ljubes-ic and Kranjc-ic-ic-ic-ic-ic-ic-ic-ic-ic-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-la-"}, {"heading": "2.1. DSL Shared Task 2014", "text": "The first edition of the DSL task was organized in 2014 within the framework of the workshop on the application of NLP tools to similar languages, varieties and dialects (VarDial) with COLING. Organizers created and published a new dataset for this purpose, which they claimed as the first resource of its kind (Tan et al., 2014). The dataset is called DSL Corpus Collection, or DSLCC, and it contains brief excerpts from previously published texts and repository.2 Texts in DSLCC v. 1.0 were written in thirteen languages or language variants and divided into the following six groups: Group A, Serbian, Group B (Indonesian, Malay), Group C), Group C, Uniques D (Brazilian, European, Portuguese, Portuguese, Portuguese, Portuguese, Portuguese."}, {"heading": "2.2. DSL Shared Task 2015", "text": "The 2015 edition of the DSL Joint Task was organized as part of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects (LT4VarDial) with RANLP. For DSL 2015, the organizers released version 2.0 of the DSLCC, which included the same set of languages and language varieties as version 1.0 in groups A to E. The two3There were many cases of republication (e.g. British texts reprinted by an American newspaper and marked as American) that made the task impracticable for this language group. (Zampieri et al., 2014) Major changes between the two versions are the exclusion of group F (British and American English) and the inclusion of group G (Bulgarian and Macedonian).4 A new addition to the DSL 2015 is the use of two test sets (A and B). In Testset A instances A instances are presented exactly as they are presented in newspaper copies with designated entities."}, {"heading": "3. Methods", "text": "In the next sections we describe the methodology behind our 4 experiments and the data used."}, {"heading": "3.1. Data", "text": "All the experiments reported here are carried out on the DSL Corpus Collection (DSLCC) versions 1.0. and 2.0. (Tan et al., 2014), both versions covering five groups of two to three languages or varieties (Groups A-E, Table 3). In the 2015 collection, Bulgarian and Macedonian (Group G) are added, as well as sentences from \"other\" languages. In some experiments (Sections 3.3. & 4.2.), we use the results of the 22 entries submitted for the 2015 joint task."}, {"heading": "3.2. Progress Test", "text": "In our first experiment, we evaluate the improvements made from one common task to another. To this end, we measure the performance of three systems that are representative of peak performance in both years: 5https: / / github.com / Simdiva / DSL-Task. \u2022 The Top 2014 System, NRC-closed-2014 (Goutte et al., 2014); \u2022 The Top 2015 System for Closed Tasks, MAC-closed-2015 (Malmasi and Dras, 2015b); \u2022 The Top 2015 System for Open Tasks, NRC-open-2015 (Goutte and Le \u0301 ger, 2015). The 2015 joint task included two important additions: a new group of narrow languages (Bulgarian / Macedonian, Group G) and data from other languages (Group X). The 2015 results were measured in all groups, but the 2014 system was not trained to recognise either Group G or Group X data."}, {"heading": "3.3. Ensemble and Oracle", "text": "An interesting research question for this task is the measurement of the upper limit of accuracy. This can be measured by treating each common task template as an independent system and combining the results with ensemble fusion methods such as a plurality voting or oracle. This type of analysis has previously proven to be informative for the similar task of Native Language Identification (Malmasi et al., 2015b). In addition, this analysis can also help to identify interesting error patterns in the submissions. Following the approach of Malmasi et al. (2015b), we apply the following combination methods to the data. Plurality voting: This is the standard combination strategy that selects the label with the highest number of votes it has received (Polikar, 2006). This differs from a majority combiner, where a label must receive more than 50% of voters."}, {"heading": "3.4. Learning Curves", "text": "Learning curves are an important tool to understand how statistical models learn from data. They show how the models behave in terms of performance as the amount of data increases. To calculate learning curves for the DSL task, we chose a simple model that is easy to train and works close to the top systems. We take partial samples of different sample sizes from the complete training set. To keep the training data balanced, we take the same amount of Ns of examples from each language variant. In our setup, we use Ns = 20 000 (full training set), 10 000, 5000, 2000, 1000, 500, 200 and 100. For each subsample sample, we train a statistical model and test it on the official test set 2015. We replicate this experiment 10 times for each sample size, except the full training set. This helps us estimate the expected performance for each sample size as well as error bars for expectation."}, {"heading": "3.5. Manual Annotation", "text": "To make this evaluation even more comprehensive, we also conducted a human evaluation experiment on some of the misclassified instances. We asked human commentators to assign the correct language or linguistic diversity of each sentence to the most difficult language groups, namely Group A (Bosnian, Croatian and Serbian), D (Brazilian and European Portuguese) and E (Argentine and Spanish on the peninsula). In this experiment, we took into account all instances that were misclassified by the oracle (i.e. no submission was correct). For Group D and E, we added sentences that were misclassified according to the plurality voting method, as well as twelve instances per group. Such analyses of misclassifications can provide further insights and help us better understand the difficulty of the task."}, {"heading": "4. Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1. Progress Test", "text": "Table 4 shows the results of the Progress Test. The 2014 system will be evaluated on the basis of the 2014 Test Kit and the 2015 Progress Kit, which includes 5 groups and 11 languages. 2015 systems will be evaluated on the basis of the 2014 and 2015 Complete Test Kit and the 2015 Progress Kit. Table 4 shows that the increase in performance of the 2015 systems compared to the 2014 Test Kit is significant but modest (+ 0.3-0.6%). However, it should be noted that the 2014 system is the only one for which the training data exactly matches the test data. Slightly surprisingly, the best closed submission in 2015, which is only trained on the 2015 Training Kit, performs slightly better than NRC-open-2015, which was trained on both the 2014 and 2015 data, and is therefore expected to show improvements more clearly from year to year from year from year to year from the results of the Progress Test."}, {"heading": "4.2. Ensemble and Oracle", "text": "The 22 entries in the split task (normal test set) were combined into different groups, and the results are shown in Table 5 below. We observe that plurality matching among all entries shows very little improvement over the best single system (Malmasi and Dras, 2015b).However, the oracle results are significantly higher than the vote result and close to 100% accuracy. Accuracy @ 2 and Accuracy @ 3 are almost identical to the complete oracle, suggesting that almost all errors are the result of confusion between the top 2-3 outcomes. This is because DSL errors are almost always within a group, i.e. between 2 or 3 variants. As shown in the learning curves in Figure 1, the group prediction achieves perfect performance with relatively few examples, so that the remaining confusion is always within a group of languages or variants. This differs from the results observed in Native Language Identification, where there is a large difference between the Oracle and the Oracle results @."}, {"heading": "4.3. Learning Curves", "text": "For the sake of simplicity, the learning curves for a simple system trained on only 6-gram characters were determined. This is essentially the first run of the NRC in 2015, which was 0.7% below the top system trained on various training sizes. Figure 1 shows learning curves for the average group and language classification performance (top) and within each group (bottom). The group curve (dotted, top) shows that the prediction of the group is perfectly performed on about 1000 examples per language. Average performance at language level is lower and is still rising at 20,000 examples per language. Taking a closer look at the discrimination performance for each group, we see that the performance for groups C, G is essentially already perfect on 100-500 examples. This suggests that it is easy to distinguish Czech from Slovak and Bulgarian from Macedonian. In these cases, it may be more difficult to examine marginal issues such as robustness versus changes in source, genre or regional proximity, but there was little obvious improvement for 10,000 examples of group B."}, {"heading": "4.4. Manual Annotation", "text": "As noted in section 4.2, the oracle achieves an accuracy of 99.83%, leaving only 24 misclassified sentences: sixteen from group A, three from group D, and five from group E. We have summarized most of these cases in a manual annotation experiment that contains twelve examples of different native speakers of these languages and asks them to assign the correct language or linguistic diversity of each text. In our experimental setting, we ensured that the annotators were not exclusively speakers of one of the languages or varieties of the group. However, it was very difficult to achieve a perfect balance in participation between the languages. Within a group of two (or three) languages or varieties, the perception of the native speaker was confirmed by the fact that a given text corresponds to his own language or not to his own language. We will discuss this problem later in this section, taking Group D (Brazilian and European Portuguese) as an example."}, {"heading": "5. Conclusion and Future Work", "text": "This paper presents a comprehensive evaluation of speech recognition systems that are trained to recognize similar languages and language variants, based on the results of the first two DSL tasks. We assess the progress made from one issue of the common task to the next. Using pluralism alignments and oracles, we estimate the upper limit of achievable performance and identify some particularly challenging sentences. We show learning curves that help us recognize how the task is learned and which language groups require more attention. Finally, we propose an experiment with native speakers of the three most difficult language groups: Group A (Bosnian, Croatian and Serbian), Group D (Brazilian and European Portuguese) and Group E (Argentine and Spanish on the Peninsula). Our results suggest that people also find it difficult to distinguish between similar languages and language variants. In future work, we would like to examine human performance in this task, focusing on two aspects: 1) how native language variations can identify language expressions; 2) which Brazilian words are the most commonly used in the Brazilian language, or tactical expressions in 2010."}, {"heading": "Acknowledgements", "text": "We also thank the DSL Shared Task Organizers for providing the data we included in this paper.Bibliographical References A \u0301 cs, J., Grad-Gyenge, L., and de Rezende Oliveira, T. B. R. (2015). In Proceedings of the LT4VarDial Workshop.Baldwin, T. and Lui, M. (2010). Multilingual Language Identification: ALTW 2010 shared task data. In Proceedings of the Australasian Language Technology Workshop.Bobicev, V. (2015). Discriminating between similar languages using ppm. In Proceedings of the LT4VarDial Workshop.Brown, R. (2013). Selecting and weighting n-grams to identify 1100 languages. In Proceedings of TSD.Brown, R. D. (2014). Non-linear mapping for improved."}], "references": [{"title": "A two-level classifier for discriminating similar languages", "author": ["J. \u00c1cs", "L. Grad-Gyenge", "T.B.R. de Rezende Oliveira"], "venue": "In Proceedings of the LT4VarDial Workshop", "citeRegEx": "\u00c1cs et al\\.,? \\Q2015\\E", "shortCiteRegEx": "\u00c1cs et al\\.", "year": 2015}, {"title": "Multilingual language identification: ALTW 2010 shared task data", "author": ["T. Baldwin", "M. Lui"], "venue": "In Proceedings of Australasian Language Technology Workshop", "citeRegEx": "Baldwin and Lui,? \\Q2010\\E", "shortCiteRegEx": "Baldwin and Lui", "year": 2010}, {"title": "Discriminating between similar languages using ppm", "author": ["V. Bobicev"], "venue": "In Proceedings of the LT4VarDial Workshop", "citeRegEx": "Bobicev,? \\Q2015\\E", "shortCiteRegEx": "Bobicev", "year": 2015}, {"title": "Selecting and weighting n-grams to identify 1100 languages", "author": ["R. Brown"], "venue": "In Proceedings of TSD", "citeRegEx": "Brown,? \\Q2013\\E", "shortCiteRegEx": "Brown", "year": 2013}, {"title": "Non-linear mapping for improved identification of 1300+ languages", "author": ["R.D. Brown"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Brown,? \\Q2014\\E", "shortCiteRegEx": "Brown", "year": 2014}, {"title": "A computational perspective on Romanian dialects", "author": ["A.M. Ciobanu", "L.P. Dinu"], "venue": "In Proceedings of LREC", "citeRegEx": "Ciobanu and Dinu,? \\Q2016\\E", "shortCiteRegEx": "Ciobanu and Dinu", "year": 2016}, {"title": "Statistical identification of language", "author": ["T. Dunning"], "venue": "Technical report,", "citeRegEx": "Dunning,? \\Q1994\\E", "shortCiteRegEx": "Dunning", "year": 1994}, {"title": "Sentence level dialect identification in Arabic", "author": ["H. Elfardy", "M.T. Diab"], "venue": "In Proceedings of ACL", "citeRegEx": "Elfardy and Diab,? \\Q2014\\E", "shortCiteRegEx": "Elfardy and Diab", "year": 2014}, {"title": "NLEL UPV autoritas participation at Discrimination between Similar Languages (DSL) 2015 shared task", "author": ["R. Fabra-Boluda", "F. Rangel", "P. Rosso"], "venue": "In Proceedings of the LT4VarDial Workshop", "citeRegEx": "Fabra.Boluda et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fabra.Boluda et al\\.", "year": 2015}, {"title": "Distributed representations of words and documents for discriminating similar languages", "author": ["M. Franco-Salvador", "P. Rosso", "F. Rangel"], "venue": "In Proceedings of the LT4VarDial Workshop", "citeRegEx": "Franco.Salvador et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Franco.Salvador et al\\.", "year": 2015}, {"title": "Improving native language identification with tfidf weighting", "author": ["B.G. Gebre", "M. Zampieri", "P. Wittenburg", "T. Heskens"], "venue": "In Proceedings of the 8th BEA workshop", "citeRegEx": "Gebre et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gebre et al\\.", "year": 2013}, {"title": "Experiments in discriminating similar languages", "author": ["C. Goutte", "S. L\u00e9ger"], "venue": "In Proceedings of the LT4VarDial Workshop", "citeRegEx": "Goutte and L\u00e9ger,? \\Q2015\\E", "shortCiteRegEx": "Goutte and L\u00e9ger", "year": 2015}, {"title": "The NRC system for discriminating similar languages", "author": ["C. Goutte", "S. L\u00e9ger", "M. Carpuat"], "venue": "In Proceedings of the VarDial Workshop", "citeRegEx": "Goutte et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Goutte et al\\.", "year": 2014}, {"title": "Comparing two language identification schemes", "author": ["G. Grefenstette"], "venue": "In Proceedings of JADT 1995, 3rd International Conference on Statistical Analysis of Textual Data,", "citeRegEx": "Grefenstette,? \\Q1995\\E", "shortCiteRegEx": "Grefenstette", "year": 1995}, {"title": "Contrastive approach towards text source classification based on top-bag-ofword similarity", "author": ["C. Huang", "L. Lee"], "venue": "In Proceedings of PACLIC", "citeRegEx": "Huang and Lee,? \\Q2008\\E", "shortCiteRegEx": "Huang and Lee", "year": 2008}, {"title": "Discriminating similar languages with token-based backoff", "author": ["T. Jauhiainen", "H. Jauhiainen", "K. Lind\u00e9n"], "venue": "In Proceedings of the LT4VarDial Workshop", "citeRegEx": "Jauhiainen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Jauhiainen et al\\.", "year": 2015}, {"title": "Experiments in sentence language identification with groups of similar languages", "author": ["B. King", "D. Radev", "S. Abney"], "venue": "In Proceedings of the VarDial Workshop", "citeRegEx": "King et al\\.,? \\Q2014\\E", "shortCiteRegEx": "King et al\\.", "year": 2014}, {"title": "Decision templates for multiple classifier fusion: an experimental comparison", "author": ["L.I. Kuncheva", "J.C. Bezdek", "R.P. Duin"], "venue": "Pattern Recognition,", "citeRegEx": "Kuncheva et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Kuncheva et al\\.", "year": 2001}, {"title": "Discriminating between closely related languages on twitter", "author": ["N. Ljube\u0161i\u0107", "D. Kranj\u010di\u0107"], "venue": null, "citeRegEx": "Ljube\u0161i\u0107 and Kranj\u010di\u0107,? \\Q2015\\E", "shortCiteRegEx": "Ljube\u0161i\u0107 and Kranj\u010di\u0107", "year": 2015}, {"title": "Language identification: How to distinguish similar languages", "author": ["N. Ljube\u0161i\u0107", "N. Mikelic", "D. Boras"], "venue": "In Proceedings of the 29th International Conference on Information Technology Interfaces", "citeRegEx": "Ljube\u0161i\u0107 et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Ljube\u0161i\u0107 et al\\.", "year": 2007}, {"title": "Classifying English documents by national dialect", "author": ["M. Lui", "P. Cook"], "venue": "In Proceedings of Australasian Language Technology Workshop", "citeRegEx": "Lui and Cook,? \\Q2013\\E", "shortCiteRegEx": "Lui and Cook", "year": 2013}, {"title": "Automatic detection and language identification of multilingual documents. Transactions of the Association for Computational Linguistics, 2:27\u201340", "author": ["M. Lui", "J.H. Lau", "T. Baldwin"], "venue": null, "citeRegEx": "Lui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2014}, {"title": "Exploring methods and resources for discriminating similar languages", "author": ["M. Lui", "N. Letcher", "O. Adams", "L. Duong", "P. Cook", "T. Baldwin"], "venue": "In Proceedings of VarDial", "citeRegEx": "Lui et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Lui et al\\.", "year": 2014}, {"title": "Language variety identification in Spanish tweets", "author": ["W. Maier", "C. G\u00f3mez-Rodr\u0131guez"], "venue": "In Proceedings of the LT4CloseLang Workshop", "citeRegEx": "Maier and G\u00f3mez.Rodr\u0131guez,? \\Q2014\\E", "shortCiteRegEx": "Maier and G\u00f3mez.Rodr\u0131guez", "year": 2014}, {"title": "Language Transfer Hypotheses with Linear SVM Weights", "author": ["S. Malmasi", "M. Dras"], "venue": "In Proceedings of EMNLP", "citeRegEx": "Malmasi and Dras,? \\Q2014\\E", "shortCiteRegEx": "Malmasi and Dras", "year": 2014}, {"title": "Automatic Language Identification for Persian and Dari texts", "author": ["S. Malmasi", "M. Dras"], "venue": "In Proceedings of PACLING", "citeRegEx": "Malmasi and Dras,? \\Q2015\\E", "shortCiteRegEx": "Malmasi and Dras", "year": 2015}, {"title": "Language identification using classifier ensembles", "author": ["S. Malmasi", "M. Dras"], "venue": "In Proceedings of the LT4VarDial Workshop", "citeRegEx": "Malmasi and Dras,? \\Q2015\\E", "shortCiteRegEx": "Malmasi and Dras", "year": 2015}, {"title": "Multilingual Native Language Identification", "author": ["S. Malmasi", "M. Dras"], "venue": "In Natural Language Engineering", "citeRegEx": "Malmasi and Dras,? \\Q2015\\E", "shortCiteRegEx": "Malmasi and Dras", "year": 2015}, {"title": "Arabic Dialect Identification using a Parallel Multidialectal Corpus", "author": ["S. Malmasi", "E. Refaee", "M. Dras"], "venue": "In Proceedings of PACLING", "citeRegEx": "Malmasi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malmasi et al\\.", "year": 2015}, {"title": "Oracle and human baselines for native language identification", "author": ["S. Malmasi", "J. Tetreault", "M. Dras"], "venue": "In Proceedings of the BEA workshop", "citeRegEx": "Malmasi et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Malmasi et al\\.", "year": 2015}, {"title": "Evaluation in information retrieval. In Introduction to Information Retrieval, pages 151\u2013175", "author": ["C.D. Manning", "P. Raghavan", "H. Sch\u00fctze"], "venue": null, "citeRegEx": "Manning et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Manning et al\\.", "year": 2008}, {"title": "Ensemble based systems in decision making", "author": ["R. Polikar"], "venue": "Circuits and systems magazine,", "citeRegEx": "Polikar,? \\Q2006\\E", "shortCiteRegEx": "Polikar", "year": 2006}, {"title": "Using maximum entropy models to discriminate between similar languages and varieties. In Proceedings of the 1st Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects (VarDial)", "author": ["J. Porta", "Sancho", "J.-L"], "venue": null, "citeRegEx": "Porta et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Porta et al\\.", "year": 2014}, {"title": "A simple baseline for discriminating similar language", "author": ["M. Purver"], "venue": "In Proceedings of the 1st Workshop", "citeRegEx": "Purver,? \\Q2014\\E", "shortCiteRegEx": "Purver", "year": 2014}, {"title": "Automatic identification of close languages - case study: Malay and Indonesian", "author": ["B. Ranaivo-Malan\u00e7on"], "venue": "ECTI Transactions on Computer and Information Technology,", "citeRegEx": "Ranaivo.Malan\u00e7on,? \\Q2006\\E", "shortCiteRegEx": "Ranaivo.Malan\u00e7on", "year": 2006}, {"title": "Language identification: a neural network approach", "author": ["A. Sim\u00f5es", "J.J. Almeida", "S.D. Byers"], "venue": null, "citeRegEx": "Sim\u00f5es et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sim\u00f5es et al\\.", "year": 2014}, {"title": "Measuring and parameterizing lexical convergence and divergence between European and Brazilian Portuguese: endo/exogeneousness and foreign and normative influence. Advances in Cognitive Sociolinguistics", "author": ["A. Soares da Silva"], "venue": null, "citeRegEx": "Silva,? \\Q2010\\E", "shortCiteRegEx": "Silva", "year": 2010}, {"title": "Overview for the first shared task on language identification in code-switched data", "author": ["T. Solorio", "E. Blair", "S. Maharjan", "S. Bethard", "M. Diab", "M. Ghoneim", "A. Hawwari", "F. AlGhamdi", "J. Hirschberg", "A. Chang", "P. Fung"], "venue": "In Proceedings of the First Workshop on Computa-", "citeRegEx": "Solorio et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Solorio et al\\.", "year": 2014}, {"title": "Merging comparable data sources for the discrimination of similar languages: The DSL corpus collection", "author": ["L. Tan", "M. Zampieri", "N. Ljube\u0161i\u0107", "J. Tiedemann"], "venue": "In Proceedings of The BUCC Workshop", "citeRegEx": "Tan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tan et al\\.", "year": 2014}, {"title": "Efficient discrimination between closely related languages", "author": ["J. Tiedemann", "N. Ljube\u0161i\u0107"], "venue": "In Proceedings of COLING", "citeRegEx": "Tiedemann and Ljube\u0161i\u0107,? \\Q2012\\E", "shortCiteRegEx": "Tiedemann and Ljube\u0161i\u0107", "year": 2012}, {"title": "Improved sentence-level Arabic dialect classification", "author": ["C. Tillmann", "S. Mansour", "Y. Al-Onaizan"], "venue": "In Proceedings of the VarDial Workshop,", "citeRegEx": "Tillmann et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tillmann et al\\.", "year": 2014}, {"title": "Arabic dialect identification", "author": ["O.F. Zaidan", "C. Callison-Burch"], "venue": "Computational Linguistics", "citeRegEx": "Zaidan and Callison.Burch,? \\Q2014\\E", "shortCiteRegEx": "Zaidan and Callison.Burch", "year": 2014}, {"title": "Automatic identification of language varieties: The case of Portuguese", "author": ["M. Zampieri", "B.G. Gebre"], "venue": "In Proceedings of KONVENS", "citeRegEx": "Zampieri and Gebre,? \\Q2012\\E", "shortCiteRegEx": "Zampieri and Gebre", "year": 2012}, {"title": "Ngram language models and POS distribution for the identification of Spanish varieties", "author": ["M. Zampieri", "B.G. Gebre", "S. Diwersy"], "venue": "In Proceedings of TALN", "citeRegEx": "Zampieri et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Zampieri et al\\.", "year": 2013}, {"title": "A report on the DSL shared task", "author": ["M. Zampieri", "L. Tan", "N. Ljube\u0161i\u0107", "J. Tiedemann"], "venue": "In Proceedings of the VarDial Workshop", "citeRegEx": "Zampieri et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zampieri et al\\.", "year": 2014}, {"title": "Comparing approaches to the identification of similar languages", "author": ["M. Zampieri", "B.G. Gebre", "H. Costa", "J. van Genabith"], "venue": "In Proceedings of the LT4VarDial Workshop", "citeRegEx": "Zampieri et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zampieri et al\\.", "year": 2015}, {"title": "Overview of the DSL shared task", "author": ["M. Zampieri", "L. Tan", "N. Ljube\u0161i\u0107", "J. Tiedemann", "P. Nakov"], "venue": "In Proceedings of LT4VarDial", "citeRegEx": "Zampieri et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zampieri et al\\.", "year": 2015}, {"title": "Overview of TweetLID: Tweet language identification", "author": ["A. Zubiaga", "I. San Vicente", "P. Gamallo", "J.R. Pichel", "I. Alegria", "N. Aranberri", "A. Ezeiza", "V. Fresno"], "venue": "SEPLN", "citeRegEx": "Zubiaga et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2014}, {"title": "Tweetlid: a benchmark for tweet language identification", "author": ["A. Zubiaga", "I. San Vicente", "P. Gamallo", "J.R. Pichel", "I. Alegria", "N. Aranberri", "A. Ezeiza", "V. Fresno"], "venue": "Language Resources and Evaluation,", "citeRegEx": "Zubiaga et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Zubiaga et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 39, "context": "Discriminating between similar languages and language varieties is one of the main challenges of state-of-the-art language identification systems (Tiedemann and Ljube\u0161i\u0107, 2012).", "startOffset": 146, "endOffset": 176}, {"referenceID": 12, "context": "Nevertheless, in both cases, recent work has shown that it is possible to train algorithms to discriminate between similar languages and language varieties with high accuracy (Goutte et al., 2014; Malmasi and Dras, 2015b).", "startOffset": 175, "endOffset": 221}, {"referenceID": 44, "context": "This study looks in more detail into the features that help algorithms discriminating between similar languages, taking into account the results of two recent editions of the Discriminating between Similar Languages (DSL) shared task (Zampieri et al., 2014; Zampieri et al., 2015b).", "startOffset": 234, "endOffset": 281}, {"referenceID": 6, "context": "Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al.", "startOffset": 67, "endOffset": 102}, {"referenceID": 13, "context": "Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al.", "startOffset": 67, "endOffset": 102}, {"referenceID": 3, "context": "Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim\u00f5es et al., 2014).", "startOffset": 126, "endOffset": 192}, {"referenceID": 4, "context": "Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim\u00f5es et al., 2014).", "startOffset": 126, "endOffset": 192}, {"referenceID": 35, "context": "Interest in the task is evidenced by early n-gram-based approaches (Dunning, 1994; Grefenstette, 1995) to more recent studies (Brown, 2013; Lui et al., 2014a; Brown, 2014; Sim\u00f5es et al., 2014).", "startOffset": 126, "endOffset": 192}, {"referenceID": 34, "context": "Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malan\u00e7on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljube\u0161i\u0107 et al.", "startOffset": 62, "endOffset": 86}, {"referenceID": 14, "context": "Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malan\u00e7on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljube\u0161i\u0107 et al.", "startOffset": 106, "endOffset": 127}, {"referenceID": 19, "context": "Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malan\u00e7on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljube\u0161i\u0107 et al., 2007; Ljube\u0161i\u0107 and Kranj\u010di\u0107, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al.", "startOffset": 152, "endOffset": 204}, {"referenceID": 18, "context": "Examples of studies include the cases of Malay and Indonesian (Ranaivo-Malan\u00e7on, 2006), Chinese varieties (Huang and Lee, 2008), South Slavic languages (Ljube\u0161i\u0107 et al., 2007; Ljube\u0161i\u0107 and Kranj\u010di\u0107, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al.", "startOffset": 152, "endOffset": 204}, {"referenceID": 42, "context": ", 2007; Ljube\u0161i\u0107 and Kranj\u010di\u0107, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al.", "startOffset": 59, "endOffset": 85}, {"referenceID": 43, "context": ", 2007; Ljube\u0161i\u0107 and Kranj\u010di\u0107, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G\u00f3mez-Rodr\u0131guez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al.", "startOffset": 105, "endOffset": 161}, {"referenceID": 23, "context": ", 2007; Ljube\u0161i\u0107 and Kranj\u010di\u0107, 2015), Portuguese varieties (Zampieri and Gebre, 2012), Spanish varieties (Zampieri et al., 2013; Maier and G\u00f3mez-Rodr\u0131guez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al.", "startOffset": 105, "endOffset": 161}, {"referenceID": 20, "context": ", 2013; Maier and G\u00f3mez-Rodr\u0131guez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al.", "startOffset": 60, "endOffset": 80}, {"referenceID": 5, "context": ", 2013; Maier and G\u00f3mez-Rodr\u0131guez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al.", "startOffset": 144, "endOffset": 168}, {"referenceID": 7, "context": ", 2013; Maier and G\u00f3mez-Rodr\u0131guez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a)", "startOffset": 213, "endOffset": 316}, {"referenceID": 41, "context": ", 2013; Maier and G\u00f3mez-Rodr\u0131guez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a)", "startOffset": 213, "endOffset": 316}, {"referenceID": 40, "context": ", 2013; Maier and G\u00f3mez-Rodr\u0131guez, 2014), English varieties (Lui and Cook, 2013), Persian and Dari (Malmasi and Dras, 2015a), Romanian dialects (Ciobanu and Dinu, 2016), and a number of studies on Arabic dialects (Elfardy and Diab, 2014; Zaidan and Callison-Burch, 2014; Tillmann et al., 2014; Malmasi et al., 2015a)", "startOffset": 213, "endOffset": 316}, {"referenceID": 1, "context": "A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al.", "startOffset": 144, "endOffset": 167}, {"referenceID": 47, "context": "A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al.", "startOffset": 259, "endOffset": 303}, {"referenceID": 48, "context": "A number of shared tasks on language identification have been organized in the recent years ranging from generalpurpose language identification (Baldwin and Lui, 2010) to more specific challenges such as the TweetLID shared task which focused on Twitter data (Zubiaga et al., 2014; Zubiaga et al., 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al.", "startOffset": 259, "endOffset": 303}, {"referenceID": 37, "context": ", 2015), the shared task on Language Identification in Code-Switched Data (Solorio et al., 2014), and the two editions of the discriminating between similar languages (DSL) shared task.", "startOffset": 74, "endOffset": 96}, {"referenceID": 38, "context": "The organizers compiled and released a new dataset for this purpose, which they claim to be the first resource of its kind (Tan et al., 2014).", "startOffset": 123, "endOffset": 141}, {"referenceID": 24, "context": "language (Malmasi and Dras, 2014; Malmasi and Dras, 2015c).", "startOffset": 9, "endOffset": 58}, {"referenceID": 24, "context": "language (Malmasi and Dras, 2014; Malmasi and Dras, 2015c). See Tan et al. (2014) for a complete list of sources.", "startOffset": 10, "endOffset": 82}, {"referenceID": 43, "context": "The complete shared task report is available in Zampieri et al. (2014). We summarize the results in Table 1 in terms of accuracy (best performing entries displayed in bold).", "startOffset": 48, "endOffset": 71}, {"referenceID": 12, "context": "7 (Goutte et al., 2014) RAE 94.", "startOffset": 2, "endOffset": 23}, {"referenceID": 16, "context": "9 (King et al., 2014) UniMelb-NLP 91.", "startOffset": 2, "endOffset": 21}, {"referenceID": 33, "context": "6 (Purver, 2014) LIRA 76.", "startOffset": 2, "endOffset": 16}, {"referenceID": 12, "context": "In the closed submission track the best performance was obtained by the NRC-CNRC (Goutte et al., 2014) team, which used a two-step classification approach to predict first the language group of the text, and subsequently the language.", "startOffset": 81, "endOffset": 102}, {"referenceID": 12, "context": "Both NRC-CNRC (Goutte et al., 2014) and QMUL (Purver, 2014), ranked 5th used linear support vector machines (SVM) classifiers with words and characters as features.", "startOffset": 14, "endOffset": 35}, {"referenceID": 33, "context": ", 2014) and QMUL (Purver, 2014), ranked 5th used linear support vector machines (SVM) classifiers with words and characters as features.", "startOffset": 17, "endOffset": 31}, {"referenceID": 16, "context": "Two teams used information gain to estimate the best features for classification, UMich (King et al., 2014) and UniMelb-NLP (Lui et al.", "startOffset": 88, "endOffset": 107}, {"referenceID": 44, "context": "(Zampieri et al., 2014) main modifications between the two versions are the exclusion of group F (British and American English) and the inclusion of group G (Bulgarian and Macedonian).", "startOffset": 0, "endOffset": 23}, {"referenceID": 43, "context": "(Zampieri et al., 2014) main modifications between the two versions are the exclusion of group F (British and American English) and the inclusion of group G (Bulgarian and Macedonian).4 A new addition in the DSL 2015 is the use of two test sets (A and B). In test set A instances are presented exactly as they appear in newspaper texts whereas in test set B named entities were substituted by placeholders. According to the organizers, the release of test set B aimed to evaluate the extent to which named entities influence classification performance. Ten teams submitted their results and eight of them published system description papers. Results of the DSL 2015 are described in detail in Zampieri et al. (2015b) In Table 2 we summarize the results obtained by all teams using test sets A and B in both open and close submissions.", "startOffset": 1, "endOffset": 717}, {"referenceID": 11, "context": "65 (Goutte and L\u00e9ger, 2015) SUKI 94.", "startOffset": 3, "endOffset": 27}, {"referenceID": 15, "context": "67 (Jauhiainen et al., 2015) BOBICEV 94.", "startOffset": 3, "endOffset": 28}, {"referenceID": 2, "context": "14 (Bobicev, 2015)", "startOffset": 3, "endOffset": 18}, {"referenceID": 0, "context": "66 (\u00c1cs et al., 2015) PRHLT 92.", "startOffset": 3, "endOffset": 21}, {"referenceID": 9, "context": "74 - (Franco-Salvador et al., 2015) INRIA 83.", "startOffset": 5, "endOffset": 35}, {"referenceID": 8, "context": "84 (Fabra-Boluda et al., 2015) OSEVAL - 76.", "startOffset": 3, "endOffset": 30}, {"referenceID": 15, "context": "02 (Jauhiainen et al., 2015) NRC 93.", "startOffset": 3, "endOffset": 28}, {"referenceID": 11, "context": "41 (Goutte and L\u00e9ger, 2015) MMS 92.", "startOffset": 3, "endOffset": 27}, {"referenceID": 2, "context": "22 (Bobicev, 2015) PRHLT 90.", "startOffset": 3, "endOffset": 18}, {"referenceID": 9, "context": "80 - (Franco-Salvador et al., 2015) NLEL 62.", "startOffset": 5, "endOffset": 35}, {"referenceID": 8, "context": "56 (Fabra-Boluda et al., 2015) OSEVAL - 75.", "startOffset": 3, "endOffset": 30}, {"referenceID": 11, "context": "Two other SVMbased approaches were tied in 2nd for test set A, one by the NRC team (Goutte and L\u00e9ger, 2015) and MMS (Zampieri et al.", "startOffset": 83, "endOffset": 107}, {"referenceID": 10, "context": ", 2015a), which experimented with three different approaches and obtained the best results combining TF-IDF and an SVM classifier previously used for native language identification (Gebre et al., 2013).", "startOffset": 181, "endOffset": 201}, {"referenceID": 8, "context": "Two other teams used two-stage classification approaches: NLEL (Fabra-Boluda et al., 2015) and BRUniBP (\u00c1cs et al.", "startOffset": 63, "endOffset": 90}, {"referenceID": 0, "context": ", 2015) and BRUniBP (\u00c1cs et al., 2015).", "startOffset": 20, "endOffset": 38}, {"referenceID": 15, "context": "A number of computational techniques have been explored in the DSL 2015 including token-based backoff by SUKI team (Jauhiainen et al., 2015), prediction by partial matching (PPM) by BOBICEV (Bobicev, 2015), and word and sentence vectors by PRHLT (Franco-Salvador et al.", "startOffset": 115, "endOffset": 140}, {"referenceID": 2, "context": ", 2015), prediction by partial matching (PPM) by BOBICEV (Bobicev, 2015), and word and sentence vectors by PRHLT (Franco-Salvador et al.", "startOffset": 57, "endOffset": 72}, {"referenceID": 9, "context": ", 2015), prediction by partial matching (PPM) by BOBICEV (Bobicev, 2015), and word and sentence vectors by PRHLT (Franco-Salvador et al., 2015).", "startOffset": 113, "endOffset": 143}, {"referenceID": 38, "context": "(Tan et al., 2014).", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "\u2022 the top 2014 system, NRC-closed-2014 (Goutte et al., 2014);", "startOffset": 39, "endOffset": 60}, {"referenceID": 11, "context": "\u2022 the top 2015 open task system, NRC-open-2015 (Goutte and L\u00e9ger, 2015).", "startOffset": 47, "endOffset": 71}, {"referenceID": 28, "context": "This type of analysis has previously been shown to be informative for the similar task of Native Language Identification (Malmasi et al., 2015b). Moreover, this analysis can also help reveal interesting error patterns in the submissions. Following the approach of Malmasi et al. (2015b), we apply the following combination methods to the data.", "startOffset": 122, "endOffset": 287}, {"referenceID": 31, "context": "Plurality Voting: This is the standard combination strategy that selects the label with the highest number of votes, regardless of the percentage of votes it received (Polikar, 2006).", "startOffset": 167, "endOffset": 182}, {"referenceID": 17, "context": "This method has previously been used to analyze the limits of majority vote classifier combination (Kuncheva et al., 2001).", "startOffset": 99, "endOffset": 122}, {"referenceID": 30, "context": ", 2015b) This method is inspired by the \u201cPrecision at k\u201d metric from Information Retrieval (Manning et al., 2008) which measures precision at fixed low levels of results (e.", "startOffset": 91, "endOffset": 113}, {"referenceID": 0, "context": "\u00c1cs et al. (2015) showed that for 52 misclassified Portuguese instances, only 22 have been labeled correctly by the annotators with low inter-annotator agreement.", "startOffset": 0, "endOffset": 18}, {"referenceID": 42, "context": "This corroborates the findings of Zampieri and Gebre (2012) who showed that due to differences in spelling and lexical variation, Brazilian and Portuguese texts can be discriminated automatically with almost perfect performance (researchers report 99.", "startOffset": 34, "endOffset": 60}, {"referenceID": 43, "context": "0 (see Zampieri et al. (2014) for a discussion).", "startOffset": 7, "endOffset": 30}, {"referenceID": 36, "context": "See Soares da Silva (2010) for a study on lexical variation", "startOffset": 14, "endOffset": 27}], "year": 2016, "abstractText": "We present an analysis of the performance of machine learning classifiers on discriminating between similar languages and language varieties. We carried out a number of experiments using the results of the two editions of the Discriminating between Similar Languages (DSL) shared task. We investigate the progress made between the two tasks, estimate an upper bound on possible performance using ensemble and oracle combination, and provide learning curves to help us understand which languages are more challenging. A number of difficult sentences are identified and investigated further with human annotation.", "creator": "LaTeX with hyperref package"}}}