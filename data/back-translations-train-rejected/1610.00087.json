{"id": "1610.00087", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2016", "title": "Very Deep Convolutional Neural Networks for Raw Waveforms", "abstract": "Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (~2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperform the CNN with 3 weight layers by over 15% in absolute accuracy for an environmental sound recognition task and matches the performance of models using log-mel features.", "histories": [["v1", "Sat, 1 Oct 2016 05:15:15 GMT  (290kb,D)", "http://arxiv.org/abs/1610.00087v1", "5 pages, 2 figures, under submission to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017"]], "COMMENTS": "5 pages, 2 figures, under submission to International Conference on Acoustics, Speech and Signal Processing (ICASSP) 2017", "reviews": [], "SUBJECTS": "cs.SD cs.LG cs.NE", "authors": ["wei dai", "chia dai", "shuhui qu", "juncheng li", "samarjit das"], "accepted": false, "id": "1610.00087"}, "pdf": {"name": "1610.00087.pdf", "metadata": {"source": "CRF", "title": "VERY DEEP CONVOLUTIONAL NEURAL NETWORKS FOR RAW WAVEFORMS", "authors": ["Wei Dai", "Chia Dai", "Shuhui Qu", "Juncheng Li", "Samarjit Das"], "emails": ["wdai@cs.cmu.edu,", "chiad@cs.cmu.edu,", "shuhuiq@stanford.edu,", "billy.li@us.bosch.com", "samarjit.das@us.bosch.com"], "sections": [{"heading": null, "text": "In fact, most of them are able to survive themselves if they do not put themselves in a position to survive themselves. Most of them are not able to survive themselves, and most of them are able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are not able to survive themselves. Most of them are able to survive themselves, and most of them are able to survive themselves."}, {"heading": "2. VERY DEEP CONVOLUTIONAL NETWORKS", "text": "This year, we will be able to change the world we live in, \"he said in an interview with the Taiwanese daily\" taz. \""}, {"heading": "3. EXPERIMENT DETAILS", "text": "In fact, it is a mere red herring."}, {"heading": "4. RESULTS AND ANALYSES", "text": "For the time being, we have only a very limited number of models that we know very well compared to the other countries. So it's not like we are able to compare the results, \"he said.\" But it's not like if. \"\" It's not like if. \"\" It's like if. \"\" It's not like if. \"\" It's not like if. \"\" It's like if. \"\" \"It's like if.\" \"\". \"\" \"\" It's like if. \"\" \"It's like if.\" \"\" It's like if. \".\" \"\" \"\" \"It's like if.\" \".\" \"\" \"\" It. \"\" \".\" \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\". \".\" \"\" \".\" \"\". \"\" \"\". \"\". \"\" \"\". \"\". \"\" \"\" \".\" \"\" \".\" \".\" \"\". \"\" \"\". \"\". \"\" \"\" \".\" \".\" \"\" \".\" \"\" \"\". \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \"\". \"\" \".\" \"\" \".\" \".\" \"\" \"\" \".\" \"\". \"\". \"\" \"\" \".\" \"\" \"\". \"\" \"\" \".\" \"\". \"\" \"\" \"\" \".\" \"\" \".\" \"\" \"\". \"\" \".\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \".\" \"\" \"\" \"\" \"\". \"\" \"\" \".\" \"\" \"\" \"\". \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \"\" \""}, {"heading": "5. CONCLUSION", "text": "In this paper, we propose very deep Convolutionary Neural Networks that operate directly on acoustic waveform inputs. We efficiently optimize our networks with up to 34 weight layers by combining batch normalization, residual learning and down sampling. We use a wide receiver field (RF) in the first Convolutionary Layer and narrow RFs in the rest of the network. Our results show that a deep network with 18 weight layers absolutely outperforms networks with 2 Convolutionary layers by 15.56% and achieves an accuracy of 71.8% that competes with CNNs that use protocol spectrogram input [11]. Our fully Convolutionary Networks are favorable compared to those with fully connected layers. Our proposed deep architectures promise to improve CNNs for speech recognition and other modeling of time series."}, {"heading": "6. ACKNOWLEDGEMENT", "text": "This work is supported by the contract FA8702-15-D-0002 with the Software Engineering Institute, a center sponsored by the U.S. Department of Defense."}, {"heading": "7. REFERENCES", "text": "[1] Hamid Eghbal-Zadeh, Bernhard Lehner, Matthias Dorfer, and Gerhard Widmer, \"CP-JKU Submissions for DCASE-2016: a hybrid approach using binaural ivectors and deep convolutional neural networks,\" Tech. Rep., DCASE2016 Challenge, September 2016. [2] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., \"Deep speech: Scaling up end-to-end speech recognition,\" arXiXiXiXiXiv preprint arXiv: 1412.5567, 2014. [3] Yedid Hoshen, Ron J Weiss, and Kevin W Wilson \"Speech acoustic modeling from raw multichannel waveforms,\" in ICASSP."}], "references": [{"title": "CP-JKU submissions for DCASE-2016: a hybrid approach using binaural ivectors and deep convolutional neural networks", "author": ["Hamid Eghbal-Zadeh", "Bernhard Lehner", "Matthias Dorfer", "Gerhard Widmer"], "venue": "Tech. Rep., DCASE2016 Challenge, September 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep speech: Scaling up end-to-end speech recognition", "author": ["Awni Hannun", "Carl Case", "Jared Casper", "Bryan Catanzaro", "Greg Diamos", "Erich Elsen", "Ryan Prenger", "Sanjeev Satheesh", "Shubho Sengupta", "Adam Coates"], "venue": "arXiv preprint arXiv:1412.5567, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech acoustic modeling from raw multichannel waveforms", "author": ["Yedid Hoshen", "Ron J Weiss", "Kevin W Wilson"], "venue": "ICASSP. IEEE, 2015, pp. 4624\u20134628.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning the speech frontend with raw waveform cldnns", "author": ["Tara N Sainath", "Ron J Weiss", "Andrew Senior", "Kevin W Wilson", "Oriol Vinyals"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoff Hinton"], "venue": "Advances in Neural Information Processing Systems 25, P. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, and K.Q. Weinberger, Eds., pp. 1106\u20131114. 2012.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "venue": "arXiv preprint arXiv:1512.03385, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Deepface: Closing the gap to human-level performance in face verification", "author": ["Yaniv Taigman", "Ming Yang", "Marc\u2019Aurelio Ranzato", "Lior Wolf"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 2014, pp. 1701\u20131708.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Facenet: A unified embedding for face recognition and clustering", "author": ["Florian Schroff", "Dmitry Kalenichenko", "James Philbin"], "venue": "CVPR, 2015, pp. 815\u2013823.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Acoustic modeling with deep neural networks using raw time signal for lvcsr", "author": ["Zolt\u00e1n T\u00fcske", "Pavel Golik", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "INTERSPEECH, 2014, pp. 890\u2013894.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Convolutional neural networks for acoustic modeling of raw time signal in lvcsr", "author": ["Pavel Golik", "Zolt\u00e1n T\u00fcske", "Ralf Schl\u00fcter", "Hermann Ney"], "venue": "Sixteenth Annual  Conference of the International Speech Communication Association, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Environmental sound classification with convolutional neural networks", "author": ["Karol J Piczak"], "venue": "2015 IEEE 25th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2015, pp. 1\u20136.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "A dataset and taxonomy for urban sound research", "author": ["Justin Salamon", "Christopher Jacoby", "Juan Pablo Bello"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 1041\u20131044.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "venue": "Journal of Machine Learning Research, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Going deeper with convolutions", "author": ["Christian et al Szegedy"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 1\u20139.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "On rectified linear units for speech processing", "author": ["Matthew D et al. Zeiler"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3517\u20133521.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["Jonathan Long", "Evan Shelhamer", "Trevor Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431\u2013 3440.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep multilingual convolutional neural networks for lvcsr", "author": ["Tom Sercu", "Christian Puhrsch", "Brian Kingsbury", "Yann LeCun"], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4955\u20134959.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio"], "venue": "Aistats, 2010, vol. 9, pp. 249\u2013256.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "author": ["Mart\u0131n Abadi", "Ashish Agarwal"], "venue": "arXiv preprint arXiv:1603.04467, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Deep neural networks, which have achieved state-of-the-art performances in acoustic scene recognition [1] and speech recognition [2], have increasingly blurred the line between representation learning and predictive modeling.", "startOffset": 102, "endOffset": 105}, {"referenceID": 1, "context": "Deep neural networks, which have achieved state-of-the-art performances in acoustic scene recognition [1] and speech recognition [2], have increasingly blurred the line between representation learning and predictive modeling.", "startOffset": 129, "endOffset": 132}, {"referenceID": 1, "context": "Instead of using the hand-tuned Gaussian Mixture Model features and Mel-frequency cepstrum coefficients, neural network models can directly take as input features such as spectrograms [2] and even raw waveforms [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 2, "context": "Instead of using the hand-tuned Gaussian Mixture Model features and Mel-frequency cepstrum coefficients, neural network models can directly take as input features such as spectrograms [2] and even raw waveforms [3].", "startOffset": 211, "endOffset": 214}, {"referenceID": 3, "context": "neural networks can be viewed as extracting feature representation jointly with classification, rather than separately [4].", "startOffset": 119, "endOffset": 122}, {"referenceID": 1, "context": "This joint optimization is highly effective in speech recognition [2] and image classification [5], among others.", "startOffset": 66, "endOffset": 69}, {"referenceID": 4, "context": "This joint optimization is highly effective in speech recognition [2] and image classification [5], among others.", "startOffset": 95, "endOffset": 98}, {"referenceID": 5, "context": "CNNs have famously achieved performance competitive or even surpassing human-level performance in the visual domains, such as object recognition [6] and face recognition [7, 8].", "startOffset": 145, "endOffset": 148}, {"referenceID": 6, "context": "CNNs have famously achieved performance competitive or even surpassing human-level performance in the visual domains, such as object recognition [6] and face recognition [7, 8].", "startOffset": 170, "endOffset": 176}, {"referenceID": 7, "context": "CNNs have famously achieved performance competitive or even surpassing human-level performance in the visual domains, such as object recognition [6] and face recognition [7, 8].", "startOffset": 170, "endOffset": 176}, {"referenceID": 8, "context": "Recent works have applied CNNs to audio tasks such as environmental sound recognition and speech recognition and found that CNNs perform well with just the raw waveforms [9, 4, 10].", "startOffset": 170, "endOffset": 180}, {"referenceID": 3, "context": "Recent works have applied CNNs to audio tasks such as environmental sound recognition and speech recognition and found that CNNs perform well with just the raw waveforms [9, 4, 10].", "startOffset": 170, "endOffset": 180}, {"referenceID": 9, "context": "Recent works have applied CNNs to audio tasks such as environmental sound recognition and speech recognition and found that CNNs perform well with just the raw waveforms [9, 4, 10].", "startOffset": 170, "endOffset": 180}, {"referenceID": 3, "context": "In one case, CNNs with time-domain waveforms can match the performance of models using conventional features like log-mel features [4].", "startOffset": 131, "endOffset": 134}, {"referenceID": 3, "context": "These works, however, have mostly considered only less deep networks, such as two convolutional layers [4, 11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 10, "context": "These works, however, have mostly considered only less deep networks, such as two convolutional layers [4, 11].", "startOffset": 103, "endOffset": 110}, {"referenceID": 11, "context": "By applying batch normalization [12], residual learning [6], and a careful design of down-sampling layers, we overcome the difficulties in training very deep models while keeping the computation cost low.", "startOffset": 32, "endOffset": 36}, {"referenceID": 5, "context": "By applying batch normalization [12], residual learning [6], and a careful design of down-sampling layers, we overcome the difficulties in training very deep models while keeping the computation cost low.", "startOffset": 56, "endOffset": 59}, {"referenceID": 12, "context": "On an environmental sound recognition task [13], we show that deep networks improve the performance of networks with 2 convolutional layers by over 15% in absolute accuracy.", "startOffset": 43, "endOffset": 47}, {"referenceID": 10, "context": "We further demonstrate that the performance of deep models using just the raw signal is competitive with models using log-mel features [11].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "Furthermore, we aggressively reduce the temporal resolution in the first two layers by 16x with large convolutional and max pooling strides to limit the computation cost in the rest of the network [16].", "startOffset": 197, "endOffset": 201}, {"referenceID": 16, "context": "We use rectified linear units (ReLU) for lower computation cost, following [17, 15].", "startOffset": 75, "endOffset": 83}, {"referenceID": 14, "context": "We use rectified linear units (ReLU) for lower computation cost, following [17, 15].", "startOffset": 75, "endOffset": 83}, {"referenceID": 14, "context": ", 4096 in [15, 5]) for discriminative modeling, leading to a very high number of parameters.", "startOffset": 10, "endOffset": 17}, {"referenceID": 4, "context": ", 4096 in [15, 5]) for discriminative modeling, leading to a very high number of parameters.", "startOffset": 10, "endOffset": 17}, {"referenceID": 5, "context": "We therefore adopt a fully convolutional design for our network construction [6, 18].", "startOffset": 77, "endOffset": 84}, {"referenceID": 17, "context": "We therefore adopt a fully convolutional design for our network construction [6, 18].", "startOffset": 77, "endOffset": 84}, {"referenceID": 14, "context": "If we exclusively use small receptive field for all convolutional layers such as in [15], which uses 3x3 in pixel for all layers, our model would need many layers in order to abstract high level features, which could be computationally expensive.", "startOffset": 84, "endOffset": 88}, {"referenceID": 14, "context": "1Small receptive fields were first popularized by [15] for 2D images.", "startOffset": 50, "endOffset": 54}, {"referenceID": 11, "context": "We adopt auxiliary layers called batch normalization (BN) [12] that alleviates the problem of exploding and vanishing gradients, a common problem in optimizing deep architectures.", "startOffset": 58, "endOffset": 62}, {"referenceID": 18, "context": "This makes possible training very deep networks (M18, M34-res) that were not studied previously [19].", "startOffset": 96, "endOffset": 100}, {"referenceID": 11, "context": "Following [12], we apply BN on the output of each convolutional layer before applying ReLU non-linearity.", "startOffset": 10, "endOffset": 14}, {"referenceID": 5, "context": "Residual learning [6] is a recently proposed learning framework to ease the training of very deep networks.", "startOffset": 18, "endOffset": 21}, {"referenceID": 12, "context": "We use UrbanSound8k dataset which contains 10 environmental sounds in urban areas, such as drilling, car horn, and children playing [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 19, "context": "We train the CNN models using Adam [20], a variant of", "startOffset": 35, "endOffset": 39}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 9, "endOffset": 17}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 18, "endOffset": 25}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 30, "endOffset": 37}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 42, "endOffset": 57}, {"referenceID": 2, "context": "[3, 256] [3, 128] [3, 64] \u00d7 2 [3, 64] \u00d7 4 [ 3, 48 3, 48 ] \u00d7 3", "startOffset": 42, "endOffset": 57}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 9, "endOffset": 17}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 22, "endOffset": 30}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 35, "endOffset": 50}, {"referenceID": 2, "context": "[3, 256] [3, 128] \u00d7 2 [3, 128] \u00d7 4 [ 3, 96 3, 96 ] \u00d7 4", "startOffset": 35, "endOffset": 50}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 9, "endOffset": 17}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 22, "endOffset": 30}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 35, "endOffset": 52}, {"referenceID": 2, "context": "[3, 512] [3, 256] \u00d7 3 [3, 256] \u00d7 4 [ 3, 192 3, 192 ] \u00d7 6", "startOffset": 35, "endOffset": 52}, {"referenceID": 2, "context": "[3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384 3, 384 ] \u00d7 3", "startOffset": 0, "endOffset": 8}, {"referenceID": 2, "context": "[3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384 3, 384 ] \u00d7 3", "startOffset": 13, "endOffset": 21}, {"referenceID": 2, "context": "[3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384 3, 384 ] \u00d7 3", "startOffset": 26, "endOffset": 43}, {"referenceID": 2, "context": "[3, 512] \u00d7 2 [3, 512] \u00d7 4 [ 3, 384 3, 384 ] \u00d7 3", "startOffset": 26, "endOffset": 43}, {"referenceID": 2, "context": ", [3, 256] has stride 1).", "startOffset": 2, "endOffset": 10}, {"referenceID": 13, "context": "Without fully connected layers, we do not use dropout [14] in these architectures.", "startOffset": 54, "endOffset": 58}, {"referenceID": 20, "context": "We use glorot initialization [21] to avoid exploding or vanishing gradients.", "startOffset": 29, "endOffset": 33}, {"referenceID": 21, "context": "Our models are implemented in Tensorflow [22] and trained on machines equipped with a Titan X GPU.", "startOffset": 41, "endOffset": 45}, {"referenceID": 4, "context": "The \u201cfc\u201d models replace global average pooling layer with 2 fully connected (FC) layers of dimension 1000 (Table 5), since many conventional deep convolutional networks use 2 FC layers of dimension in the thousands [5, 15, 11].", "startOffset": 215, "endOffset": 226}, {"referenceID": 14, "context": "The \u201cfc\u201d models replace global average pooling layer with 2 fully connected (FC) layers of dimension 1000 (Table 5), since many conventional deep convolutional networks use 2 FC layers of dimension in the thousands [5, 15, 11].", "startOffset": 215, "endOffset": 226}, {"referenceID": 10, "context": "The \u201cfc\u201d models replace global average pooling layer with 2 fully connected (FC) layers of dimension 1000 (Table 5), since many conventional deep convolutional networks use 2 FC layers of dimension in the thousands [5, 15, 11].", "startOffset": 215, "endOffset": 226}, {"referenceID": 10, "context": "This is in contrast with models using the spectrogram as input, which achieve good performance with just 2 convolutional layers [11], and shows that applying CNN directly on time-series data is challenging.", "startOffset": 128, "endOffset": 132}, {"referenceID": 10, "context": "68% accuracy that is competitive with the reported test accuracy of CNNs on spectrogram input using the same dataset [11]3.", "startOffset": 117, "endOffset": 121}, {"referenceID": 10, "context": "3Figure 4 in [11] reports \u223c68% accuracy using a baseline CNN model.", "startOffset": 13, "endOffset": 17}, {"referenceID": 10, "context": "We point out that we have a different evaluation scheme: we use the 10-th fold as test set, while [11] performs 10-fold evaluation.", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "Overfitting caused by very deep networks is well documented [6].", "startOffset": 60, "endOffset": 63}, {"referenceID": 8, "context": "Previous works have shown that the first convolutional layer, when trained on raw waveforms, mimics wavelet transforms [9, 4].", "startOffset": 119, "endOffset": 125}, {"referenceID": 3, "context": "Previous works have shown that the first convolutional layer, when trained on raw waveforms, mimics wavelet transforms [9, 4].", "startOffset": 119, "endOffset": 125}, {"referenceID": 11, "context": "Note that M18-no-bn results in lower test accuracy, indicating that BN has a regularization effect [12].", "startOffset": 99, "endOffset": 103}, {"referenceID": 10, "context": "8% accuracy, competitive with CNNs using log-mel spectrogram inputs [11].", "startOffset": 68, "endOffset": 72}], "year": 2016, "abstractText": "Learning acoustic models directly from the raw waveform data with minimal processing is challenging. Current waveform-based models have generally used very few (\u223c2) convolutional layers, which might be insufficient for building high-level discriminative features. In this work, we propose very deep convolutional neural networks (CNNs) that directly use time-domain waveforms as inputs. Our CNNs, with up to 34 weight layers, are efficient to optimize over very long sequences (e.g., vector of size 32000), necessary for processing acoustic waveforms. This is achieved through batch normalization, residual learning, and a careful design of down-sampling in the initial layers. Our networks are fully convolutional, without the use of fully connected layers and dropout, to maximize representation learning. We use a large receptive field in the first convolutional layer to mimic bandpass filters, but very small receptive fields subsequently to control the model capacity. We demonstrate the performance gains with the deeper models. Our evaluation shows that the CNN with 18 weight layers outperform the CNN with 3 weight layers by over 15% in absolute accuracy for an environmental sound recognition task and matches the performance of models using log-mel features.", "creator": "LaTeX with hyperref package"}}}