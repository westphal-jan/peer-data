{"id": "1610.00388", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Learning to Translate in Real-time with Neural Machine Translation", "abstract": "Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.", "histories": [["v1", "Mon, 3 Oct 2016 02:11:03 GMT  (1464kb,D)", "http://arxiv.org/abs/1610.00388v1", "9 pages, 8 figures"], ["v2", "Thu, 6 Oct 2016 00:46:39 GMT  (1467kb,D)", "http://arxiv.org/abs/1610.00388v2", "10 pages, 8 figures (with additional related works)"], ["v3", "Tue, 10 Jan 2017 21:07:56 GMT  (2974kb,D)", "http://arxiv.org/abs/1610.00388v3", "10 pages, camera ready"]], "COMMENTS": "9 pages, 8 figures", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["jiatao gu", "graham neubig", "kyunghyun cho", "victor o k li"], "accepted": false, "id": "1610.00388"}, "pdf": {"name": "1610.00388.pdf", "metadata": {"source": "CRF", "title": "Learning to Translate in Real-time with Neural Machine Translation", "authors": ["Jiatao Gu", "Graham Neubig", "Kyunghyun Cho", "Victor O.K. Li"], "emails": ["vli}@eee.hku.hk", "gneubig@cs.cmu.edu", "kyunghyun.cho@nyu.edu"], "sections": [{"heading": "1 Introduction", "text": "The task of translating content in real time as it is produced is an important tool for understanding spoken lectures or conversations (Fu \ufffd gen et al., 2007; Bangalore et al., 2012). Unlike the typical machine translation task (MT), where translation quality is central, simultaneous translation requires a balance between translation quality and time delay to ensure that users receive translated content in an accelerated manner (Mieno et al., 2015). A number of methods have been proposed to solve this problem, largely in the context of phrase-based machine translation. These methods are based on a segment receiving a word to send it to an MT system that translates each1code and data to replicate our experiments."}, {"heading": "2 Problem Definition", "text": "Suppose we have a buffer of input words X = {x1,..., xTs} that need to be translated in real time. We define the simultaneous translation task in such a way that we make two intertwined decisions one after the other: Read or WRITE. Specifically, the translator reads a source word x\u03b7 from the input buffer in chronological order as a translation context, or WRITES a translated word y\u0442 onto the output buffer, resulting in an output set Y = {y1,..., yTt}, and the action sequence A = {a1,..., aT} consists of Ts READs and Tt WRITEs, i.e. T = Ts + T.Similar to standard MT, we have a Q (Y) measure to evaluate the translation quality, such as BLEU score (Papineni et al.,..., 2002)."}, {"heading": "3 Simultaneous Translation with Neural Machine Translation", "text": "The proposed frame is shown in Fig. 2 and can be naturally divided into two parts: environment (Fig. 3.1) and agent (Fig. 3.2)."}, {"heading": "3.1 Environment", "text": "Encoder: READ The first element of the NMT system is the encoder that converts input words X = {x1,..., xTs} to context vectors H = {h1,..., hTs}. Standard NMT uses bidirectional RNNs as encoders (Bahdanau et al., 2014), but this is not suitable for simultaneous processing, since using an inverted encoder requires the last word of the sentence before processing begins. Therefore, we use a simple unidirectional RNN from left to right as encoder. In contrast, we only refer to the words read from input when each target word is generated."}, {"heading": "3.2 Agent", "text": "A trainable agent is designed to make decisions A = {a1,.., aT}, at least sequentially, based on observations O = {o1,..., oT}, and then control the translation environment accordingly. Observation As shown in Figure 2, we link the current context vector c\u03b7\u03c4, the current decoder state z\u03b7\u03c4, and the embedding vector of the candidate word y\u03b7\u0442 as continuous observation, o\u0442 + \u03b7 = [c\u043e\u0442; z \u043e; E (y \u03b7)], to represent the current state of the environment. Action Similar to previous work (Grissom II et al., 2014), we define the following catalogue of measures: \u2022 READ: The agent rejects the candidate and waits to encode the next word from the input buffer; \u2022 WRITE: The agent accepts the candidate and sends it as a prediction into the starting buffer; Policy How the agent selects the action based on the ITA and waits for the candidate to accept the next word from the observation and the next E."}, {"heading": "4 Optimization", "text": "The proposed framework can be trained using reinforcement learning. Specifically, we use the Policy Gradient Algorithm along with Variance Reduction and Regulatory Techniques. Algorithm 1: NMT system \u03c6, Policy Gradient Algorithm, Input Buffer X, Output Buffer Y, State Buffer S. 1: Init x1 \u0445 X, h1 DL ENC (x1), H1 \u2190 {h1} 2: z0 DL (H1), y0 < s > 3: Success Buffer Y, State Buffer S. 1: Meanwhile < ts < p? p? p; ts < p p p? p p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p? p?"}, {"heading": "4.1 Pre-training", "text": "We need an NMT environment that the agent can explore and use to generate translations. In this case, we simply train the NMT encoder decoder on full sentence pairs with maximum probability and assume that the pre-trained model is still capable of producing reasonable translations even for incomplete sentences. Although this is likely to be suboptimal, our NMT environment, based on unidirectional RNNNs, can handle incomplete sentences much like shorter sentences and has the potential to translate them more or less correctly."}, {"heading": "4.2 Reward Function", "text": "To evaluate a good simultaneous machine translation, a reward must take into account both quality and lag. Quality We evaluate the translation quality using automated metrics such as BLEU (Papineni et al., 2002). BLEU rating is defined as the weighted geometric average of the modified Ngram - accuracy BLEU0 multiplied by the abbreviation penalty BP to punish a short translation. BLEU (Y, Y) = BP \u00b7 BLEU0 (Y, Y), (5) where the Y rating is the reference and Y the output. We decompose BLEU and use the difference of the partial BLEU values as a reward, that is: rQt = {S BLEU0 (Y, t) t < TBLEU (Y, T) t = T (6), where the accumulated BLEU score is we."}, {"heading": "4.3 Reinforcement Learning", "text": "The political gradient maximizes the following prospective cumulative future rewards, J = E\u03c0\u03b8 [T \u2211 t = 1 Rt]. (10) Its gradient is then the cumulative gradient for current observations and actions. (10) Its gradient is the cumulative future reward for current observations and actions. (In practice, Eq. 11 is estimated by sampling several action paths from current policies, collecting the corresponding rewards. (11) Tk = t [rQk + r D] is the cumulative future reward for current observations and actions. (11) The gradient is estimated by sampling several action paths from current policies, collecting the corresponding rewards. The reduction of variance directly using the political gradient suffers from high variance, making learning unstable and inefficient."}, {"heading": "5 Simultaneous Beam Search", "text": "In previous sections, we have described a simultaneous greedy decoding algorithm. In the standard NMT, it was shown that the bar search, in which the decoder holds a beam of k translation curves, significantly improves translation quality (Sutskever et al., 2014). As shown in Fig. 3 (A), the final output is determined by selecting the best translation from the entire beam. It is not trivial to apply the bar search directly to the simultaneous machine translation, since the bar search waits until the last source word to write the translation down. We can perform a simultaneous bar search if the agent chooses to select WRITE one after the other: keep several bars of the translation curves in a temporary buffer and output the best way when the agent switches to read. As shown in Fig. 3 (B) & (C), he tries to seek a relatively better way, maintaining the delay unchanged."}, {"heading": "6 Related Work", "text": "Researchers often consider the problem of simultaneous machine translation in the scenario of real-time language interpretation (Fu \ufffd gen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013). In this approach, the language stream to be translated is initially recognized and segmented using an automatic language recognition system (ASR), and the translation model then operates independently on each of these segments, potentially limiting the quality of the translation. To avoid the use of a fixed segmentation algorithm, Oda et al. (2014) have introduced a traceable segmentation component into their system, so that segmentation leads to better translation quality. Grissom II et al. (2014) proposed a similar framework, but based on reinforcement learning. All of these methods are still based on the independent translation of each segment, without considering the proposed shortening step."}, {"heading": "7 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Preliminary Work", "text": "Dataset To thoroughly study the proposed simultaneous translation model, we train and evaluate it on two different language pairs: \"English-German (EN-DE)\" and \"English-Russian (ENRU)\" in both directions per pair. We use the parallel corpora available from WMT '152 for both pre-training the NMT environment and learning policy. We use newstest-2013 as a validation set to evaluate the proposed algorithm. Both the training set and the validation set are tokenized and divided into subordinate units with byte-pair encryption (BPE) (Sennrich et al., 2015). We only use sentence pairs in which both sides have less than 50 BPE subordinate symbols for the training.Environment & Agent Settings We have the NMT environments for both language pairs and both directions according to the same setting (Cho and Epova, 2016)."}, {"heading": "7.2 Quantitative Analysis", "text": "To judge the effectiveness of our enhanced learning algorithms with different reward functions, we vary the target delay d * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *"}, {"heading": "7.3 Qualitative Analysis", "text": "In fact, it is such that it is a matter of a way in which people are in a world, in which they are in a world, in which they are in a world, in which they are in a world, in which they live in a world, in which they live in a world, in which they live in a world, in which they live in a world, in which they live in a world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live,"}, {"heading": "8 Conclusion", "text": "We propose a unified framework for simultaneous neural machine translation. In order to balance quality and delay, we examine extensively different targets for delay and design a method for beamfinding that is applicable in simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the effectiveness both quantitatively and qualitatively."}, {"heading": "Acknowledgments", "text": "KC appreciates the support of Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016). GN appreciates the support of the Microsoft CORE program. In part, this work has also been supported by Samsung Advanced Institute of Technology (Neural Machine Translation) and Samsung Electronics (Project: \"Development and Application of Larger-Context Neural Machine Translation\")."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.0473", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Real-time incremental speech-to-speech translation of dialogs", "author": ["Vivek Kumar Rangarajan Sridhar", "Prakash Kolan", "Ladan Golipour", "Aura Jimenez"], "venue": "In Proceedings of the 2012 Conference of the North", "citeRegEx": "Bangalore et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bangalore et al\\.", "year": 2012}, {"title": "Can neural machine translation do simultaneous translation? arXiv preprint arXiv:1606.02012", "author": ["Cho", "Esipova2016] Kyunghyun Cho", "Masha Esipova"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2016}, {"title": "Simultaneous translation of lectures and speeches", "author": ["Alex Waibel", "Muntsin Kolss"], "venue": "Machine Translation,", "citeRegEx": "F\u00fcgen et al\\.,? \\Q2007\\E", "shortCiteRegEx": "F\u00fcgen et al\\.", "year": 2007}, {"title": "Simple, lexicalized choice of translation timing for simultaneous speech translation", "author": ["Fujita et al.2013] Tomoki Fujita", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura"], "venue": "In INTERSPEECH", "citeRegEx": "Fujita et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Fujita et al\\.", "year": 2013}, {"title": "Dont until the final verb wait: Reinforcement learning for simultaneous machine translation", "author": ["He He", "Jordan Boyd-Graber", "John Morgan", "Hal Daum\u00e9 III"], "venue": "In Proceedings of the 2014 Conference", "citeRegEx": "II et al\\.,? \\Q2014\\E", "shortCiteRegEx": "II et al\\.", "year": 2014}, {"title": "Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980", "author": ["Kingma", "Ba2014] Diederik Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Speed or accuracy? a study in evaluation of simultaneous speech translation", "author": ["Mieno et al.2015] Takashi Mieno", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura"], "venue": "In INTERSPEECH", "citeRegEx": "Mieno et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mieno et al\\.", "year": 2015}, {"title": "Neural variational inference and learning in belief networks. arXiv preprint arXiv:1402.0030", "author": ["Mnih", "Gregor2014] Andriy Mnih", "Karol Gregor"], "venue": null, "citeRegEx": "Mnih et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mnih et al\\.", "year": 2014}, {"title": "Optimizing segmentation strategies for simultaneous speech translation", "author": ["Oda et al.2014] Yusuke Oda", "Graham Neubig", "Sakriani Sakti", "Tomoki Toda", "Satoshi Nakamura"], "venue": "In Proceedings of the 52nd Annual Meeting of the Association for Computa-", "citeRegEx": "Oda et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Oda et al\\.", "year": 2014}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th annual meeting on association for computational linguistics,", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Segmentation strategies for streaming speech translation", "author": ["John Chen", "Srinivas Bangalore", "Andrej Ljolje", "Rathinavelu Chengalvarayan"], "venue": "In Proceedings of the 2013 Conference", "citeRegEx": "Sridhar et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Sridhar et al\\.", "year": 2013}, {"title": "Simultaneous machine translation using deep reinforcement learning. Abstraction in Reinforcement Learning Workshop, ICML2016", "author": ["Satija", "Pineau2016] Harsh Satija", "Joelle Pineau"], "venue": null, "citeRegEx": "Satija et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Satija et al\\.", "year": 2016}, {"title": "Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909", "author": ["Barry Haddow", "Alexandra Birch"], "venue": null, "citeRegEx": "Sennrich et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104\u20133112", "author": ["Oriol Vinyals", "Quoc V Le"], "venue": null, "citeRegEx": "Sutskever et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["Ronald J Williams"], "venue": "Machine learning,", "citeRegEx": "Williams.,? \\Q1992\\E", "shortCiteRegEx": "Williams.", "year": 1992}, {"title": "Incremental segmentation and decoding strategies for simultaneous translation", "author": ["Vivek Kumar Rangarajan Sridhar", "Srinivas Bangalore", "Baskaran Sankaran"], "venue": "In IJCNLP,", "citeRegEx": "Yarmohammadi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Yarmohammadi et al\\.", "year": 2013}], "referenceMentions": [{"referenceID": 3, "context": "Simultaneous translation, the task of translating content in real-time as it is produced, is an important tool for real-time understanding of spoken lectures or conversations (F\u00fcgen et al., 2007; Bangalore et al., 2012).", "startOffset": 175, "endOffset": 219}, {"referenceID": 1, "context": "Simultaneous translation, the task of translating content in real-time as it is produced, is an important tool for real-time understanding of spoken lectures or conversations (F\u00fcgen et al., 2007; Bangalore et al., 2012).", "startOffset": 175, "endOffset": 219}, {"referenceID": 7, "context": "Different from the typical machine translation (MT) task, in which translation quality is paramount, simultaneous translation requires balancing the trade-off between translation quality and time delay to ensure that users receive translated content in an expeditious manner (Mieno et al., 2015).", "startOffset": 275, "endOffset": 295}, {"referenceID": 9, "context": "segment independently (Oda et al., 2014) or with a minimal amount of language model context (Bangalore et al.", "startOffset": 22, "endOffset": 40}, {"referenceID": 1, "context": ", 2014) or with a minimal amount of language model context (Bangalore et al., 2012).", "startOffset": 59, "endOffset": 83}, {"referenceID": 14, "context": "Independently of simultaneous translation, accuracy of standard MT systems has greatly improved with the introduction of neural-networkbased MT systems (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 158, "endOffset": 205}, {"referenceID": 0, "context": "Independently of simultaneous translation, accuracy of standard MT systems has greatly improved with the introduction of neural-networkbased MT systems (NMT) (Sutskever et al., 2014; Bahdanau et al., 2014).", "startOffset": 158, "endOffset": 205}, {"referenceID": 10, "context": "Similar to standard MT, we have a measure Q(Y ) to evaluate the translation quality, such as BLEU score (Papineni et al., 2002).", "startOffset": 104, "endOffset": 127}, {"referenceID": 0, "context": "Standard NMT uses bi-directional RNNs as encoders (Bahdanau et al., 2014), but this is not suitable for simultaneous processing as using a reverse-order encoder requires knowing the final word of the sentence before beginning processing.", "startOffset": 50, "endOffset": 73}, {"referenceID": 10, "context": "Quality We evaluate the translation quality using automated metrics such as BLEU (Papineni et al., 2002).", "startOffset": 81, "endOffset": 104}, {"referenceID": 15, "context": "Policy Gradient We freeze the pre-trained parameters of an NMT model, and train the agent using the policy gradient (Williams, 1992).", "startOffset": 116, "endOffset": 132}, {"referenceID": 14, "context": "In standard NMT it has been shown that beam search, where the decoder keeps a beam of k translation trajectories, greatly improves translation quality (Sutskever et al., 2014).", "startOffset": 151, "endOffset": 175}, {"referenceID": 3, "context": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F\u00fcgen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 129, "endOffset": 254}, {"referenceID": 1, "context": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F\u00fcgen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 129, "endOffset": 254}, {"referenceID": 4, "context": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F\u00fcgen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 129, "endOffset": 254}, {"referenceID": 16, "context": "Researchers commonly consider the problem of simultaneous machine translation in the scenario of real-time speech interpretation (F\u00fcgen et al., 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013).", "startOffset": 129, "endOffset": 254}, {"referenceID": 1, "context": ", 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013). In this approach, the incoming speech stream required to be translated are first recognized and segmented based on an automatic speech recognition (ASR) system. The translation model then works independently based on each of these segments, potentially limiting the quality of translation. To avoid using a fixed segmentation algorithm, Oda et al. (2014) introduced a trainable segmentation component into their system, so that the segmentation leads to better translation quality.", "startOffset": 8, "endOffset": 469}, {"referenceID": 1, "context": ", 2007; Bangalore et al., 2012; Fujita et al., 2013; Rangarajan Sridhar et al., 2013; Yarmohammadi et al., 2013). In this approach, the incoming speech stream required to be translated are first recognized and segmented based on an automatic speech recognition (ASR) system. The translation model then works independently based on each of these segments, potentially limiting the quality of translation. To avoid using a fixed segmentation algorithm, Oda et al. (2014) introduced a trainable segmentation component into their system, so that the segmentation leads to better translation quality. Grissom II et al. (2014) proposed a similar framework, however, based on reinforcement learning.", "startOffset": 8, "endOffset": 621}, {"referenceID": 13, "context": "Both the training set and the validation set are tokenized and segmented into sub-word units with byte-pair encoding (BPE) (Sennrich et al., 2015).", "startOffset": 123, "endOffset": 146}, {"referenceID": 9, "context": "\u2022 Segmentation-based (SEG) (Oda et al., 2014): a state-of-the-art segmentation-based algorithm based on optimizing segmentation to achieve the highest quality score.", "startOffset": 27, "endOffset": 45}, {"referenceID": 9, "context": "We also compared against Oda et al. (2014)\u2019s state-of-the-art segmentation algorithm (SEG).", "startOffset": 25, "endOffset": 43}], "year": 2016, "abstractText": "Translating in real-time, a.k.a. simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively. 1", "creator": "LaTeX with hyperref package"}}}