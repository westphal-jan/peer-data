{"id": "1610.00552", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "30-Sep-2016", "title": "FPGA-Based Low-Power Speech Recognition with Recurrent Neural Networks", "abstract": "In this paper, a neural network based real-time speech recognition (SR) system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks (RNNs); one is a speech-to-character RNN for acoustic modeling (AM) and the other is for character-level language modeling (LM). The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple N-best search algorithm instead of the hidden Markov model (HMM) based network. The RNNs are implemented using massively parallel processing elements (PEs) for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time.", "histories": [["v1", "Fri, 30 Sep 2016 10:44:32 GMT  (471kb,D)", "http://arxiv.org/abs/1610.00552v1", "Accepted to SiPS 2016"]], "COMMENTS": "Accepted to SiPS 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG cs.SD", "authors": ["minjae lee", "kyuyeon hwang", "jinhwan park", "sungwook choi", "sungho shin", "wonyong sung"], "accepted": false, "id": "1610.00552"}, "pdf": {"name": "1610.00552.pdf", "metadata": {"source": "CRF", "title": "FPGA-based Low-power Speech Recognition with Recurrent Neural Networks", "authors": ["Minjae Lee", "Kyuyeon Hwang", "Jinhwan Park", "Sungwook Choi", "Sungho Shin", "Wonyong Sung"], "emails": ["shshin}@dsp.snu.ac.kr,", "wysung@snu.ac.kr"], "sections": [{"heading": null, "text": "In fact, it is so that most of them will be able to show themselves that they are able to survive themselves, and that they are able to survive themselves, \"he said in an interview with the\" New York Times, \"in which he said the role of the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" New York Times, \"the\" the \"the\" New York Times, \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" New York Times, \"the\" the \"the\" the \"New York Times,\" the \"the\" the \"the\" the \"New York Times,\" the \"the\" the \""}, {"heading": "II. RELATED WORKS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Large Vocabulary Continuous Speech Recognition", "text": "Most state-of-the-art large vocabulary continuous speech recognition (LVCSR) systems employ a DNN-HMM hybrid acoustic model [3] or a weighted finite state transducer (WFST) decoder [2].The WFST network is enhanced by integrating the HMM acoustic model, a pronunciation-lexicon model and a word-level n-gram-back-off speech model.ar Xiv: 161 0.00 552v 1 [cs.C L] 30 Sep 2016Therefore, the resulting decoding network becomes huge, which usually exceeds a few hundred megabytes [4] and hinders small footprints low-power implementations.A traditional LVCSR performs viterbi decoding [15] on the WFST network using senone level likelihoods compustic model computeries."}, {"heading": "B. FPGA-Based Neural Network Implementation", "text": "Neural networks require many multiplication and add operations, but are hardware-friendly due to their massive parallelism. However, many previous implementations only store the network parameters on an external DRAM, as the networks usually require more than millions of parameters. Note that the weights for fully connected layers or recurrent neural networks are only used once when downloaded, which means that their accesses have a very low time locality. [19] Efforts have been made to reduce the size of the parameters by quantization [18]. The bit width of DNNs can be reduced to just two bits by retraining the quantified parameters with a modified back propagation algorithm. [12] This approach has been successfully applied to CNNs and RNNNNs. RNNNs also require a large number of parameters, so it is helpful to quantify the parameters in low bits. A study on the weighting of NNNNNNs based on an architecture [19] was also presented."}, {"heading": "III. SPEECH RECOGNITION WITHOUT HMM", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Algorithm Overview", "text": "The speech recognition algorithm implemented in this paper consists of an RNN for acoustic modeling (AM), an RNN for the character level (LM) and a statistical word level (LM) as shown in Fig. 1. RNN AM uses the online CTC algorithm [22] and generates the probabilities of the characters by analyzing each frame of input expression. The character level RNN LM prints the probabilities of the following characters, while the statistical word-character back-off LM shows the following words. The information generated from these three modules is integrated to find the best hypothesis using an N-best search algorithm. The acoustic model has a deep LSTM network structure and is trained end-to-end with online CTC algorithm [22]. Although some of the most recent RNN-based end-to-end speech recognition algorithms (LM) are compliant."}, {"heading": "B. Beam Search Algorithm", "text": "In this work, the decoding of the bar search is performed using a simple prefix tree structure. N-best hypotheses are generated using the RNN AM and RNN for character level LM and rescorded by the statistical word level LM on the leaflet. Let L be the set of all output labels in the RNN AM except for the CTC space. The input feature vector from time 1 to t is called x1: t. Given the x1: t decoding, the goal of the bar search is to find the label sequence with the maximum posterior probability that the RNN AM has. The hypotheses are represented by a simple tree, with each node in the tree representing labels in L. To deal with CTC state transitions, state-based networks represented by CTC states, L \u2032 = L \u00b2 = CTC spaces, are inserted at a low level, where a tree node corresponds to one CTC state in a CTC state; a CTC state is placed in a CTC state."}, {"heading": "C. Retraining Based Fixed-Point Optimization", "text": "Since the LSTM-RNN contains millions of weights, an FPGA-based implementation requires large memory space on the chip to store the parameters. It is not efficient to store the weights on the external DRAM, since the collected weights are used only once for each output calculation. In our implementation, the retraining-based method [12], [19] is used to reduce the word length of the weights. The algorithm groups the weights and signals by layers, applies direct quantization to each group and retrains the entire network in the quantified domain. In our work, the weights and internal signals are quantified to 6 and 8 bits, respectively. We note that the internal LSTM cells require high precision and are therefore represented in 16 bits."}, {"heading": "IV. FPGA-BASED IMPLEMENTATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Overview of the FPGA System", "text": "The proposed algorithm is implemented on a Xilinx ZC706 evaluation board equipped with an XC7Z045 FPGA. In addition to configurable logic circuits, the FPGA embeds an ARM CPU. Fig. 2 shows the hardware architecture for implementing RNNs. Although the SR algorithm uses two RNN algorithms, our FPGA design implements only one LSTM tile and one output tile, which work intermittently when the control signal is given. Note that the RNN operation is required for the acoustic model only once per input language frame, which is normally 10 ms long, but the character level LM works much more frequently to generate N-best hypotheses for different search paths."}, {"heading": "B. Architecture and Algorithm", "text": "The vector b stands for the bias. The vector b stands for the bias. The vector is the current record in which t contains the data from the previous time step. W is the vector i, f and o is the vector-vector-vector-vector-multiplications and the LSTM-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector-vector"}, {"heading": "C. Throughput of the LSTM tile", "text": "As shown in Fig. 4, there are two PE arrays in the PE array block. As there are eight matrix vector multiplications, one RNN layer requires four matrix vector multiplication cycles. Each PE array has 256 PEs and performs a matrix vector multiplication using the outer product method. Thus, the processing time of the LSTM depends on the dimension of the input vector, since the outer product method provides an input element at each clock. The input size of the first level RNN AM is 123 and that of the next layers is 256. Thus, the first layer processing of the RNN AM requires 246 (= 123 \u00d7 4 \u04452) and 512 (= 256 \u00d7 4 \u04452) clock cycles to perform matrix vector multiplications connected to xt and ht \u2212 1. Thus, the number of clock cycles for the next layer is 246 (= 124 \u04452) and 512 (= 256 \u00d7 4 \u04452) clock cycles to perform matrix vector multiplications connected to xt and ht \u2212 1."}, {"heading": "V. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Recognition Performance", "text": "This year it is more than ever before in the history of the city."}, {"heading": "B. FPGA Implementation Performance", "text": "In our implementation, the programmable hardware operates at 100 MHz and the CPU runs at an 800 MHz clock speed to perform the N-best search. However, the result of the FPGA resource usage is in TABLE II. The implemented system requires one RNN AM operation per 10 ms voice processing (100 times per second). However, the RNN for character level LM is only needed when a character transition occurs, the frequency of which in our experiments is usually no more than 30 times per second. Suppose 128 beams of AM processing mean about 3,840 RNN LM operations per second. Therefore, the number of clock cycles to achieve a real-time LM with conservative estimate of DPU performance is about 6.4 M (= 100 x 2, 806 + 3, 840 x 1, 596)."}, {"heading": "VI. CONCLUDING REMARKS", "text": "In this paper, an RNN-based real-time speech recognition system is implemented on an FPGA. The algorithm uses the RNNs for acoustic modeling and sign language modeling, and is optimized for real-time operations using unidirectional RNNs. The vocabulary size of speech recognition is unlimited, as character-level RNN can dictate words from the vocabulary. To improve recognition performance, a statistical speech model is also used at word level. The models are integrated using a simple tree-based search algorithm, without using a hidden Markov model or weighted finite-state converter. The weights of the RNNNs are quantified to 6 bits. The RNNNs are implemented using a series of processing elements for high-throughput matrix vector multiplications."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was partially supported by the Brain Korea 21 Plus Project and the Korean Government-funded National Research Foundation of Korea (NRF) (No. 2015R1A2A1A10056051)."}], "references": [{"title": "Foreword By-Reddy, Spoken language processing: A guide to theory, algorithm, and system development", "author": ["X. Huang", "A. Acero", "H.-W. Hon"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2001}, {"title": "Weighted finite-state transducers in speech recognition", "author": ["M. Mohri", "F. Pereira", "M. Riley"], "venue": "Computer Speech & Language, vol. 16, no. 1, pp. 69\u201388, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups", "author": ["G. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "A.-r. Mohamed", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "T.N. Sainath"], "venue": "IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel scalability in speech recognition", "author": ["K. You", "J. Chong", "Y. Yi", "E. Gonina", "C.J. Hughes", "Y.-K. Chen", "W. Sung", "K. Keutzer"], "venue": "IEEE Signal Processing Magazine, vol. 26, no. 6, pp. 124\u2013135, 2009.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2009}, {"title": "Lexicon-free conversational speech recognition with neural networks", "author": ["A.L. Maas", "Z. Xie", "D. Jurafsky", "A.Y. Ng"], "venue": "The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT), 2015, pp. 345\u2013354.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Character-level incremental speech recognition with recurrent neural networks", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 5335\u2013 5339.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Connectionist Temporal Classification: Labelling unsegmented sequence data with recurrent neural networks", "author": ["A. Graves", "S. Fern\u00e1ndez", "F. Gomez", "J. Schmidhuber"], "venue": "International Conference on Machine Learning (ICML), 2006, pp. 369\u2013376.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2006}, {"title": "From feedforward to recurrent LSTM neural networks for language modeling", "author": ["M. Sundermeyer", "H. Ney", "R. Schl\u00fcter"], "venue": "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 23, no. 3, pp. 517\u2013529, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep speech 2: End-to-end speech recognition in english and mandarin", "author": ["D. Amodei", "R. Anubhai", "E. Battenberg", "C. Case", "J. Casper", "B. Catanzaro", "J. Chen", "M. Chrzanowski", "A. Coates", "G. Diamos"], "venue": "International Conference on Machine Learning (ICML), 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "EESEN: End-to-end speech recognition using deep rnn models and wfst-based decoding", "author": ["Y. Miao", "M. Gowayyed", "F. Metze"], "venue": "IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2015, pp. 167\u2013174.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 1997}, {"title": "Fixed-point feedforward deep neural network design using weights +1, 0, and -1", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE Workshop on Signal Processing Systems (SiPS), 2014, pp. 1\u20136.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "1.1 computing\u2019s energy problem (and what we can do about it)", "author": ["M. Horowitz"], "venue": "IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC), 2014, pp. 10\u201314.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "EIE: efficient inference engine on compressed deep neural network", "author": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "venue": "arXiv preprint arXiv:1602.01528, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "The Viterbi algorithm", "author": ["G.D. Forney Jr"], "venue": "Proceedings of the IEEE, vol. 61, no. 3, pp. 268\u2013278, 1973.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1973}, {"title": "An FPGA implementation of speech recognition with weighted finite state transducers", "author": ["J. Choi", "K. You", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2010, pp. 1602\u20131605.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Towards end-to-end speech recognition with recurrent neural networks", "author": ["A. Graves", "N. Jaitly"], "venue": "International Conference on Machine Learning (ICML), 2014, pp. 1764\u20131772.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Fixed point optimization of deep convolutional neural networks for object recognition", "author": ["S. Anwar", "K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 1131\u20131135.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Fixed-point performance analysis of recurrent neural networks", "author": ["S. Shin", "K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 976\u2013980.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks", "author": ["J. Kim", "K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 7510\u20137514.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Fpga based implementation of deep neural networks using on-chip memory only", "author": ["J. Park", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 1011\u2013 1015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to sequence training of ctc-rnns with partial windowing", "author": ["K. Hwang", "W. Sung"], "venue": "International Conference on Machine Learning (ICML), 2016, pp. 2178\u20132187.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Generating text with recurrent neural networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton"], "venue": "International Conference on Machine Learning (ICML), 2011, pp. 1017\u20131024.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Improved backing-off for m-gram language modeling", "author": ["R. Kneser", "H. Ney"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), vol. 1, 1995, pp. 181\u2013184.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Single stream parallelization of generalized LSTM-like RNNs on a GPU", "author": ["K. Hwang", "W. Sung"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 1047\u2013 1051.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "ADADELTA: An adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "IRSTLM: an open source toolkit for handling large scale language models.", "author": ["M. Federico", "N. Bertoldi", "M. Cettolo"], "venue": "in Interspeech,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2008}, {"title": "The Kaldi speech recognition toolkit", "author": ["D. Povey", "A. Ghoshal", "G. Boulianne", "L. Burget", "O. Glembek", "N. Goel", "M. Hannemann", "P. Motlicek", "Y. Qian", "P. Schwarz"], "venue": "IEEE workshop on automatic speech recognition and understanding, no. EPFL-CONF-192584, 2011.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1925}, {"title": "Xilinx ultrascale architecture for high-performance, smarter systems", "author": ["N. Mehta"], "venue": "Xilinx White Paper WP434, 2013.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Speech recognition has long been studied, and most of the algorithms employ hidden Markov models (HMMs) or its variants as inference and information combining tools [1], [2].", "startOffset": 165, "endOffset": 168}, {"referenceID": 1, "context": "Speech recognition has long been studied, and most of the algorithms employ hidden Markov models (HMMs) or its variants as inference and information combining tools [1], [2].", "startOffset": 170, "endOffset": 173}, {"referenceID": 2, "context": "Recently, deep neural networks are employed for acoustic modeling (AM) of state of the art speech recognition systems which, however, are not free from the HMM [3].", "startOffset": 160, "endOffset": 163}, {"referenceID": 3, "context": "HMM modeling for speech recognition demands a vast amount of memory access operations on a large size network, whose memory capacity usually exceeds a few hundred megabytes [4].", "startOffset": 173, "endOffset": 176}, {"referenceID": 4, "context": "Recently, fully neural recurrent network based speech recognition algorithms are actively investigated [5], [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 5, "context": "Recently, fully neural recurrent network based speech recognition algorithms are actively investigated [5], [6].", "startOffset": 108, "endOffset": 111}, {"referenceID": 6, "context": "The RNN is end-to-end trained with connectionist temporal classification (CTC) [7] to directly transcribe the input utterance to characters.", "startOffset": 79, "endOffset": 82}, {"referenceID": 7, "context": "The RNN has also been used for language modeling (LM), which shows much better capability than tri-gram based statistical algorithms [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 4, "context": "Recently, complete speech recognition algorithms have been developed by combining the CTC RNN and the RNN LM [5], [6].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "Recently, complete speech recognition algorithms have been developed by combining the CTC RNN and the RNN LM [5], [6].", "startOffset": 114, "endOffset": 117}, {"referenceID": 8, "context": "However, neural network algorithms, including RNNs, demand a very large number of arithmetic operations, thus they are mostly implemented using GPUs [9], [10].", "startOffset": 149, "endOffset": 152}, {"referenceID": 9, "context": "However, neural network algorithms, including RNNs, demand a very large number of arithmetic operations, thus they are mostly implemented using GPUs [9], [10].", "startOffset": 154, "endOffset": 158}, {"referenceID": 10, "context": "The developed system employs two long-short term memory (LSTM) RNNs [11]; one for acoustic modeling and the other for character-level language modeling.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "To store all the weights of the RNNs in the on-chip memory, the weights are quantized to 6 bits using the retraining based fixed-point optimization algorithm [12].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "All of the weights and the contexts are stored in the on-chip memory of the FPGA, and thus the RNNs do not need DRAM accesses which require a large amount of energy [13], [14].", "startOffset": 165, "endOffset": 169}, {"referenceID": 13, "context": "All of the weights and the contexts are stored in the on-chip memory of the FPGA, and thus the RNNs do not need DRAM accesses which require a large amount of energy [13], [14].", "startOffset": 171, "endOffset": 175}, {"referenceID": 2, "context": "Most state-of-the-art large vocabulary continuous speech recognition (LVCSR) systems employ a DNN-HMM hybrid acoustic model [3] or a weighted finite state transducer (WFST) decoder [2].", "startOffset": 124, "endOffset": 127}, {"referenceID": 1, "context": "Most state-of-the-art large vocabulary continuous speech recognition (LVCSR) systems employ a DNN-HMM hybrid acoustic model [3] or a weighted finite state transducer (WFST) decoder [2].", "startOffset": 181, "endOffset": 184}, {"referenceID": 3, "context": "Therefore, the resulting decoding network becomes huge, which is usually over a few hundred megabytes [4], and hinders small-footprint low-power implementations.", "startOffset": 102, "endOffset": 105}, {"referenceID": 14, "context": "A traditional LVCSR performs Viterbi decoding [15] on the WFST network using senone-level likelihoods computed by the acoustic model.", "startOffset": 46, "endOffset": 50}, {"referenceID": 15, "context": "Efficient hardware based implementation of the LVCSR [16] is difficult because of the large amount of search operations needed for Viterbi decoding.", "startOffset": 53, "endOffset": 57}, {"referenceID": 12, "context": "The energy cost of a DRAM access is large since static power is required to keep the I/O active and data must travel a long distance [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 16, "context": "Recently, several RNN based end-to-end speech recognizers have been developed [17], [9], [10].", "startOffset": 78, "endOffset": 82}, {"referenceID": 8, "context": "Recently, several RNN based end-to-end speech recognizers have been developed [17], [9], [10].", "startOffset": 84, "endOffset": 87}, {"referenceID": 9, "context": "Recently, several RNN based end-to-end speech recognizers have been developed [17], [9], [10].", "startOffset": 89, "endOffset": 93}, {"referenceID": 9, "context": "A phoneme-level CTCtrained RNN for acoustic modeling can reduce the size of a WFST network to about a half of that needed for DNNHMM hybrid models [10].", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "Also, character-level RNN language models and prefix beam search decoding greatly reduce the complexity of the decoding stage [5], [6].", "startOffset": 126, "endOffset": 129}, {"referenceID": 5, "context": "Also, character-level RNN language models and prefix beam search decoding greatly reduce the complexity of the decoding stage [5], [6].", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "Especially, a tree-based online decoding algorithm is proposed for lowlatency speech recognition [6].", "startOffset": 97, "endOffset": 100}, {"referenceID": 11, "context": "The bit-width of DNNs can be reduced to only two bits by retraining the quantized parameters with a modified backpropagation algorithm [12].", "startOffset": 135, "endOffset": 139}, {"referenceID": 17, "context": "This approach was successfully applied to CNNs and RNNs [18], [19].", "startOffset": 56, "endOffset": 60}, {"referenceID": 18, "context": "This approach was successfully applied to CNNs and RNNs [18], [19].", "startOffset": 62, "endOffset": 66}, {"referenceID": 18, "context": "A study on weight quantization of RNNs was presented in [19].", "startOffset": 56, "endOffset": 60}, {"referenceID": 19, "context": "The retrain-based quantization method led to an efficient VLSI implementation of DNNs that store all the quantized parameters on the on-chip SRAM [20].", "startOffset": 146, "endOffset": 150}, {"referenceID": 20, "context": "Also, a similar architecture was employed for a DNN implementation on an FPGA [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 21, "context": "The RNN AM employs the online CTC algorithm [22] and generates the probabilities of characters by analyzing each frame of input utterance.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "The acoustic model has a deep LSTM network structure and is end-to-end trained with online CTC algorithm [22].", "startOffset": 105, "endOffset": 109}, {"referenceID": 16, "context": "Although some recent RNN-based end-to-end speech recognition algorithms [17], [9], [10] employ the bidirectional structure for recognition performance improvement, we use a unidirectional structure for real-time operation, where it is not allowed to access the future contexts.", "startOffset": 72, "endOffset": 76}, {"referenceID": 8, "context": "Although some recent RNN-based end-to-end speech recognition algorithms [17], [9], [10] employ the bidirectional structure for recognition performance improvement, we use a unidirectional structure for real-time operation, where it is not allowed to access the future contexts.", "startOffset": 78, "endOffset": 81}, {"referenceID": 9, "context": "Although some recent RNN-based end-to-end speech recognition algorithms [17], [9], [10] employ the bidirectional structure for recognition performance improvement, we use a unidirectional structure for real-time operation, where it is not allowed to access the future contexts.", "startOffset": 83, "endOffset": 87}, {"referenceID": 22, "context": "The proposed SR system also employs a deep unidirectional LSTM RNN for character-level LM [23].", "startOffset": 90, "endOffset": 94}, {"referenceID": 5, "context": "The structures of the RNNs for the AM and character-level LM are described in [6].", "startOffset": 78, "endOffset": 81}, {"referenceID": 23, "context": "For better backing-off, we use improved Kneser-Ney smoothing [24].", "startOffset": 61, "endOffset": 65}, {"referenceID": 5, "context": "The word-level LM is integrated for the N -best beam search in a similar manner as the character-level LM [6], except that the rescoring is performed on the fly, only when the active node represents a blank or the end of sentence (EOS) symbol.", "startOffset": 106, "endOffset": 109}, {"referenceID": 5, "context": "The tree is pruned both in depth and width as explained in [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 11, "context": "In our implementation, the retraining based method [12], [19] is applied to reduce the word-length of weights.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "In our implementation, the retraining based method [12], [19] is applied to reduce the word-length of weights.", "startOffset": 57, "endOffset": 61}, {"referenceID": 20, "context": "5 multiplies the input Din with the weight W and adds the result with the partial sum stored in the accumulator where the bias values are preloaded [21].", "startOffset": 148, "endOffset": 152}, {"referenceID": 0, "context": "PE0[1] Din W Bias Dout", "startOffset": 3, "endOffset": 6}, {"referenceID": 0, "context": "PE1[1] Din W Bias Dout", "startOffset": 3, "endOffset": 6}, {"referenceID": 20, "context": "The output tile is a fully connected layer that employs the same structure in [21].", "startOffset": 78, "endOffset": 82}, {"referenceID": 24, "context": "The RNN AM is trained using the stochastic gradient descent (SGD) with 8 parallel input streams on a GPU [25].", "startOffset": 105, "endOffset": 109}, {"referenceID": 25, "context": "The RNN LM is trained with AdaDelta [26] based SGD.", "startOffset": 36, "endOffset": 40}, {"referenceID": 26, "context": "The statistical tri-gram LM is generated with the IRSTLM [27] toolkit included in the KALDI speech recognition tool [28].", "startOffset": 57, "endOffset": 61}, {"referenceID": 27, "context": "The statistical tri-gram LM is generated with the IRSTLM [27] toolkit included in the KALDI speech recognition tool [28].", "startOffset": 116, "endOffset": 120}, {"referenceID": 23, "context": "sh and compile-lm in IRSTLM toolkit is used to generate a standard advanced research project agency (ARPA) file while applying the improved Kneser-Ney method [24] for higher performance.", "startOffset": 158, "endOffset": 162}, {"referenceID": 9, "context": "34 % [10], but ours supports delay free real-time SR.", "startOffset": 5, "endOffset": 9}, {"referenceID": 9, "context": "Note that the algorithm in [10] is not for real-time speech recognition task, and employs a bidirectional structure that shows better performance over the unidirectional structure.", "startOffset": 27, "endOffset": 31}, {"referenceID": 28, "context": "Note that the large-model based system can be implemented using an ultra-scale FPGA [29].", "startOffset": 84, "endOffset": 88}], "year": 2016, "abstractText": "In this paper, a neural network based real-time speech recognition (SR) system is developed using an FPGA for very low-power operation. The implemented system employs two recurrent neural networks (RNNs); one is a speech-tocharacter RNN for acoustic modeling (AM) and the other is for character-level language modeling (LM). The system also employs a statistical word-level LM to improve the recognition accuracy. The results of the AM, the character-level LM, and the word-level LM are combined using a fairly simple N -best search algorithm instead of the hidden Markov model (HMM) based network. The RNNs are implemented using massively parallel processing elements (PEs) for low latency and high throughput. The weights are quantized to 6 bits to store all of them in the on-chip memory of an FPGA. The proposed algorithm is implemented on a Xilinx XC7Z045, and the system can operate much faster than real-time.", "creator": "LaTeX with hyperref package"}}}