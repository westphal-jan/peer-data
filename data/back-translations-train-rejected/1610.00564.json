{"id": "1610.00564", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural Networks", "abstract": "We investigate sequence machine learning techniques on raw radio signal time-series data. By applying deep recurrent neural networks we learn to discriminate between several application layer traffic types on top of a constant envelope modulation without using an expert demodulation algorithm. We show that complex protocol sequences can be learned and used for both classification and generation tasks using this approach.", "histories": [["v1", "Mon, 3 Oct 2016 14:22:19 GMT  (2957kb,D)", "http://arxiv.org/abs/1610.00564v1", null]], "reviews": [], "SUBJECTS": "cs.LG cs.NI", "authors": ["timothy j o'shea", "seth hitefield", "johnathan corgan"], "accepted": false, "id": "1610.00564"}, "pdf": {"name": "1610.00564.pdf", "metadata": {"source": "CRF", "title": "End-to-End Radio Traffic Sequence Recognition with Deep Recurrent Neural Networks", "authors": ["Timothy J. O\u2019Shea", "Seth Hitefield", "Johnathan Corgan"], "emails": ["oshea@vt.edu", "hitefield@vt.edu", "johnathan@corganlabs.com"], "sections": [{"heading": null, "text": "Keywords - Machine Learning, Software Radio, Protocol Recognition, Recurrent Neural Networks, LSTM, Protocol Learning, Traffic Classification, Cognitive Radio, Deep LearningI. INTRODUCTIONTraffic Analysis and Deep Packet Inspection are important tools in ensuring service quality (QoS), network security, and proper billing and routing within wired and wireless networks. Systems and algorithms exist today to differentiate between different protocols and applications for this reason, but new methods offer great potential for improvisation. Today's techniques often involve the use of numerous protocol parsers that need to analyze a combinatorial large number of different network and application protocols, which limits the capabilities of analyzing known protocols whose parsers we have manually implemented, potentially with parser implementation vulnerabilities that require parameterization, paraphrasing, or ocoding of other signals."}, {"heading": "A. Recurrent Networks in Natural Language", "text": "Recurrent neural network approaches to learning temporal sequences are nothing new, they have been very successful in recent years in translating natural language, embedding information gathering or mapping tasks, and in automatic speech recognition fields, among others. In each of these tokens, sequences of characters or phonemes are encoded using recursive neural networks such as Long-Term Short-Term Memory [3] (LSTM). Recurrent neural networks based on the simple recursive unit, LSTM, and the Gated Recurrent Unit (GRU) [5] are all widely used, and their ability to learn sequences is quite impressive, which is evident in a task as simple as displaying natural text characters in such a system [9]. The transmission function and structure of the LSTM basic neuron unit is illustrated in Figure 1. Many applications have also successfully used recurrent networks for translation between sequence areas (such as the preparation of different languages)."}, {"heading": "B. Background on Radio Sequence Motivations", "text": "In radio communications, radio transmitters and receivers consist of a series of sequences for sequencing translation routines [1], which translate between sequences of log data bits, error-corrected encoded bits, random and white bits, framed bits, and finally modulated and encoded symbols that cross the radio channel directly. Instead of implementing expert algorithms for each of these bits, we can try to learn these sequence translation mappings by presenting the data to a suitable machine learning architecture. Ideally, we learn to consume radio symbols, process idle patterns, data frame patterns, and data usage patterns, all directly from the sample data sequences presented to the learning algorithms, rather than relying on any amount of expert descriptions to encode and decode algorithms."}, {"heading": "II. SUPERVISED TRAFFIC TYPE LEARNING", "text": "In our network, we train a multi-layer LSTM-based sequence learner network on a sequence of parts of our modulated radio signal to perform a supervised classification into one of 11 different protocol traffic classes. We are an architecture where LSTM units operate directly on complex baseband I / Q signal representations, where I and Q components are treated as separate and independent channels, followed by fully connected layers with linear rectifiers and softmax activation on the final output layer."}, {"heading": "A. Dataset Generation", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "B. Model Data Ingest", "text": "For training models on this large time series, we need to choose how to present the data to the RNN model. There are two considerations here, firstly, how to divide a sequence into time steps to present it to the sequence model, and secondly, how to divide the data into regions of training and test data on a macro level. In this case, we are looking at a time series x (n) in which we want to create examples from linear subsequences. In this case, we extract N windows of size L in one step of M to form a three-dimensional sample vector. In this case, the dimensions are expressed in the form of a real tensor of form Nx2xL, in which the first dimension is above the window, the second is above the I / Q dimension, and the third over time within each window. Each tensor example is then formed from L + (N \u2212 1) tensor examples, with Tensor-M vilifying complex samples in the original time series."}, {"heading": "C. Discriminative Model Training", "text": "This year, we will be in a position to take the lead, \"he said in an interview with the Taiwanese daily La."}, {"heading": "D. Generative Model Training", "text": "We use a simple first order generative model shown in Figure 13, which predicts the next timeframe given to N previous timeframes as a regression task. We train network parameters using the mean square error (MSE) of real output sample values with linear activation of the output layer. Fig.11. The best LSTM256 prediction of the IRC sequenceFig. 12. the best LSTM256 prediction of the Spotify sequenceIn Fig.11, we show a modulated radio data signal where the first half is the truth from an IRC sequence example and the second half of the samples from a generative model is predicted using the recursive neural network model described here. In visual comparison of the predicted samples with those from the base example, we can see it correctly, says the HDLC idle pattern, the same wide framing pattern, the generative pattern, and certain similarities within the predicted data are generative."}, {"heading": "III. CONCLUSION", "text": "We have shown in this paper that recurrent neural network models can readily be used for high-level radio protocol sequence detection from pre-demodulated radio signal data for both discriminatory identification and generative emulation tasks. We have demonstrated basic performance for both tasks that works quite well under ideal conditions (high SNR, no sampling frequency offset).However, the introduction of realistic channel effects makes the task much more difficult and significantly reduces model performance.The channel variations introduced via a wireless channel into sampling frequency offset, frequency offset, and channel delay distribution complicate the learning of sequence models from raw data, but there are a number of ideas that can help alleviate this problem, e.g. by cononizing channel effects through attention models [15] and introducing strong channel regulation during training, as described in [14] these cognitive sequencing and numerous results."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank the Bradley Department of Electrical and Computer Engineering at Virginia Polytechnic Institute and State University, the Hume Center and DARPA for their generous support in this work, which was developed with the support of the MTO Office of the Defense Advanced Research Projects Agency (DARPA) as part of the grant HR0011-16-1-0002. The views, opinions and / or results expressed are those of the author and should not be interpreted to represent the official views or strategies of the Department of Defense or the U.S. government."}], "references": [{"title": "Wireless communications: principles and practice", "author": ["T.S. Rappaport"], "venue": "Prentice Hall PTR New Jersey,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1996}, {"title": "Gnu radio: tools for exploring the radio frequency spectrum", "author": ["E. Blossom"], "venue": "Linux journal, vol. 2004, no. 122, p. 4, 2004.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2004}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["A. Graves", "J. Schmidhuber"], "venue": "Neural Networks, vol. 18, no. 5, pp. 602\u2013610, 2005.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning sequence neighbourhood metrics", "author": ["J. Bayer", "C. Osendorfer", "P. Van Der Smagt"], "venue": "International Conference on Artificial Neural Networks, Springer, 2012, pp. 531\u2013538.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2012}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "venue": "arXiv preprint arXiv:1412.3555, 2014.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Generative adversarial nets", "author": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, 2014, pp. 2672\u20132680.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "Advances in neural information processing systems, 2014, pp. 3104\u2013 3112.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Char-rnn: multi-layer recurrent neural networks (lstm, gru, rnn) for character-level language models in torch", "author": ["A. Karpathy"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Understanding lstm networks", "author": ["C. Olah"], "venue": "Net: http://colah. github. io/posts/2015-08-Understanding- LSTMs, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning the speech front-end with raw waveform cldnns", "author": ["T.N. Sainath"], "venue": "Proc. Interspeech, 2015.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2015}, {"title": "Wavenet: a generative model for raw audio", "author": ["S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "venue": "arXiv preprint arXiv:1609.03499, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Convolutional radio modulation recognition networks", "author": ["T.J. O\u2019Shea", "J. Corgan", "T.C. Clancy"], "venue": "arXiv preprint arXiv:1602.04105, 2016.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to communicate: channel auto-encoders, domain specific regularizers, and attention", "author": ["T.J. O\u2019Shea", "K. Karra", "T.C. Clancy"], "venue": "arXiv preprint arXiv:1608.06409, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Radio transformer networks: attention models for learning to synchronize in wireless systems", "author": ["T.J. O\u2019Shea", "L. Pemula", "D. Batra", "T.C. Clancy"], "venue": "arXiv preprint arXiv:1605.00716, 2016.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team"], "venue": "arXiv e-prints, vol. abs/1605.02688, May 2016. [Online]. Available: http://arxiv.org/abs/1605.02688.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Seqgan: sequence generative adversarial nets with policy gradient", "author": ["L. Yu", "W. Zhang", "J. Wang", "Y. Yu"], "venue": "arXiv preprint arXiv:1609.05473, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 11, "context": "We have previously demonstrated [13] that this class of approach using deep neural networks to learn a radio discrimination task on low level modulations can be highly effective, but in this work we show that this potential also spans up the stack to higher layer traffic types as well.", "startOffset": 32, "endOffset": 36}, {"referenceID": 2, "context": "In each of these, sequences of tokens, either characters or phonemes are encoded using recurrent neural networks such as the long short-term memory [3] (LSTM).", "startOffset": 148, "endOffset": 151}, {"referenceID": 4, "context": "Recurrent neural networks based on the simple recurrent unit, the LSTM, and the Gated Recurrent Unit (GRU) [5] are all widely used and their capacity for sequence learning is quite impressive as is visable in a task as simple as presenting natural language text characters to such a system [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 7, "context": "Recurrent neural networks based on the simple recurrent unit, the LSTM, and the Gated Recurrent Unit (GRU) [5] are all widely used and their capacity for sequence learning is quite impressive as is visable in a task as simple as presenting natural language text characters to such a system [9].", "startOffset": 290, "endOffset": 293}, {"referenceID": 8, "context": "Basic LSTM Unit Transfer Function Diagram from [10]", "startOffset": 47, "endOffset": 51}, {"referenceID": 6, "context": "Many applications have also successfully employed recurrent networks for translation between sequence domains (such as different languages) [7] based on embeddings, mapping from sequences to discrete classes [4], and many other sequence related tasks.", "startOffset": 140, "endOffset": 143}, {"referenceID": 3, "context": "Many applications have also successfully employed recurrent networks for translation between sequence domains (such as different languages) [7] based on embeddings, mapping from sequences to discrete classes [4], and many other sequence related tasks.", "startOffset": 208, "endOffset": 211}, {"referenceID": 0, "context": "In radio communications, the radio transmitter and receiver are comprised of a number of sequence to sequence translation routines [1].", "startOffset": 131, "endOffset": 134}, {"referenceID": 1, "context": "The transmitter we use is a GNU Radio [2] flow-graph that uses High-Level Data Link Control (HDLC) for framing and a Quadrature Phase Shift Keying (QPSK) for modulation without any error correction or randomization (shown in Figure II-A.", "startOffset": 38, "endOffset": 41}, {"referenceID": 14, "context": "We perform this slicing using pythonnumpy and ingest tensor data into Keras [8] and Theano [16] for model training.", "startOffset": 91, "endOffset": 95}, {"referenceID": 9, "context": "We implement both a CLDNN [11], or a network formed by a sequence of convolutional layers, LSTM layers, and finally fully-connected layers, as well as a LSTM followed by fully connected layers.", "startOffset": 26, "endOffset": 30}, {"referenceID": 5, "context": "In future work we plan to use a Generative Adversarial Network (GAN) architecture [6] approach to improving our generative model realism by introducing a critic/discriminator model.", "startOffset": 82, "endOffset": 85}, {"referenceID": 15, "context": "Two extremely promising recent approaches to time-series generation we believe are extremely applicable here for future work are presented in [17] and [12].", "startOffset": 142, "endOffset": 146}, {"referenceID": 10, "context": "Two extremely promising recent approaches to time-series generation we believe are extremely applicable here for future work are presented in [17] and [12].", "startOffset": 151, "endOffset": 155}, {"referenceID": 13, "context": "The channel variations to the sequence introduced over a wireless channel in sample rate offset, frequency offset, and channel delay spread make learning sequence models from raw data difficult, but a number of ideas exist which may help alleviate this problem such as allowing attention models to cononicalize the channel effects out [15] and the introduction of heavy channel regularization during training as described in [14].", "startOffset": 335, "endOffset": 339}, {"referenceID": 12, "context": "The channel variations to the sequence introduced over a wireless channel in sample rate offset, frequency offset, and channel delay spread make learning sequence models from raw data difficult, but a number of ideas exist which may help alleviate this problem such as allowing attention models to cononicalize the channel effects out [15] and the introduction of heavy channel regularization during training as described in [14].", "startOffset": 425, "endOffset": 429}], "year": 2016, "abstractText": "We investigate sequence machine learning techniques on raw radio signal time-series data. By applying deep recurrent neural networks we learn to discriminate between several application layer traffic types on top of a constant envelope modulation without using an expert demodulation algorithm. We show that complex protocol sequences can be learned and used for both classification and generation tasks using this approach. Keywords\u2014Machine Learning, Software Radio, Protocol Recognition, Recurrent Neural Networks, LSTM, Protocol Learning, Traffic Classification, Cognitive Radio, Deep Learning", "creator": "LaTeX with hyperref package"}}}