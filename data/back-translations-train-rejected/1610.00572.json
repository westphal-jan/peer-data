{"id": "1610.00572", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "An Arabic-Hebrew parallel corpus of TED talks", "abstract": "We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT3, the Web inventory that repurposes the original content of the TED website in a way which is more convenient for MT researchers. The benchmark consists of about 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately aligned and rearranged in sentences, for a total of about 3.5M tokens per language. Talks have been partitioned in train, development and test sets similarly in all respects to the MT tasks of the IWSLT 2016 evaluation campaign. In addition to describing the benchmark, we list the problems encountered in preparing it and the novel methods designed to solve them. Baseline MT results and some measures on sentence length are provided as an extrinsic evaluation of the quality of the benchmark.", "histories": [["v1", "Mon, 3 Oct 2016 14:44:58 GMT  (119kb,D)", "http://arxiv.org/abs/1610.00572v1", "To appear in Proceedings of the AMTA 2016 Workshop on Semitic Machine Translation (SeMaT)"]], "COMMENTS": "To appear in Proceedings of the AMTA 2016 Workshop on Semitic Machine Translation (SeMaT)", "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["mauro cettolo"], "accepted": false, "id": "1610.00572"}, "pdf": {"name": "1610.00572.pdf", "metadata": {"source": "CRF", "title": "An Arabic-Hebrew parallel corpus of TED talks", "authors": ["Mauro Cettolo"], "emails": ["cettolo@fbk.eu"], "sections": [{"heading": null, "text": "The benchmark consists of approximately 2,000 presentations, subtitled accurately in Arabic and Hebrew and rearranged into sentences, totaling approximately 3.5 million characters per language. The presentations have been divided into train, development and test kits, which are in all respects similar to the MT tasks of the IWSLT evaluation campaign 2016. In addition to describing the benchmark, we list the problems encountered in its creation and the novel methods for solving them. As an extrinsic assessment of the quality of the benchmark, the baseline MT results and some measures of sentence length are provided."}, {"heading": "1 Introduction", "text": "TED is a non-profit organization that \"invites the world's most fascinating thinkers and lecturers to talk about their lives.\" Their website1 provides video recordings of the best TED conversations under a Creative Commons license. All conversations have English captions, which have also been translated into many languages by volunteers around the world. WIT3 (Cettolo et al., 2012) 2 is a web inventory that provides access to a collection of TED conversations, with the original TED site content through1www.ted.com 2wit3.fbk.euyears publications specifically designed to provide train, development and test data for participants in MT and SLT tracks of the evaluation campaign, organized by the International Workshop on Spoken Language Translation (IWSLT)."}, {"heading": "2 Related Work", "text": "To our knowledge, the most comprehensive collection of publicly available Arabic-Hebrew parallel corpora to date is part of the OPUS project; 3 in total, it provides more than 110 million tokens per language, divided into 5 corpora, with OpenSubtitles2016 being by far the largest. OpenSubtitles2016 (Lison and Tiedemann, 2016) 4 provides parallel subtitles of movies and television programs provided by the Open multilingual subtitle databases.5 The size of this corpus makes it exceptionally valuable; nevertheless, the translation of such subtitles is often less good than in other areas (even TED), which is likely to compromise the accuracy of the fully automated processing implemented for parallelizing the Arabic and Hebrew subtitles. Another Arabic-Hebrew corpus we know of is that it is prepared manually by Shilon et al. (2012) for development and evaluation purposes; no statistics are provided on its size, nor are some of them publicly available."}, {"heading": "3 Parallel Corpus Creation", "text": "English subtitles of TED talks are segmented based on the recorded speech, for example in the form of pauses, and adapt to the limited subtitle space; therefore, the individual caption generally does not correspond to a sentence. The natural translation unit considered by human translators is the caption as defined by the original transcript. While translators can look at the context of each caption, such an arrangement of each NLP task - especially the MT - would make it particularly difficult, especially when word reordering occurs between successive captions. Therefore, we aim to rebuild the original sentences, making the NLP / MT tasks more realistic."}, {"heading": "3.1 Collection of talks", "text": "WIT3 distributes a single XML file for each language, containing all talks with subtitles in that language; the XML format is defined in a3opus.lingfil.uu.se 4opus.lingfil.uu.se / OpenSubtitles2016.php 5www.opensubtitles.org DTD.6, so we did not have to search data, as we could download the three XML files in Arabic, Hebrew and English available at wit3.fbk.eu / mono.php? release = XML versions."}, {"heading": "3.2 Alignment issues", "text": "Even if the translators volunteering for TED translated the aforementioned English captions, they sometimes did not adhere to the source segmentation. For example, in Speech No. 2357,7 the English subtitle is: French sign language was brought to America at the beginning of the 19th century, is set between timestamps 53851 and 59091, while the corresponding Arabic translation is divided into two subtitles: A \u00be K Q\u00d3 @ \u00fa; HY\u00d2 J \"@ \u00e9J Q \u00ae \u0445 @ \u00e8PA B @ \u00e9 \u0435 Q\u00e5.\" \u00a9 A J\u043e @ \u00e0Q \u00ae @ \u00c9 K @ \u00fa, \"which cover the sound recording from 53851 to 56091 and from 56091 to 59091 and literally\" French sign language was brought to America \"and\" in the early 19th century \"mean that these differences each affect a relevant number of conversations (9% of the following), all of which are differently segmented and subordinated."}, {"heading": "3.3 Sentence rebuilding issues", "text": "Unfortunately, Arabic spelling is often contradictory with respect to punctuation, as both Arabic UTF8 symbols and English ASCII punctuation symbols are used. Worse, in both Arabic and Hebrew translations, the original English punctuation is often ignored. An extreme case is Speech No. 14438, where 97% of the dots at the end of English subtitles do not appear in Hebrew translations. The initial subtitles of this lecture are shown in Figure 1.6wit3.fbk.eu / archive / XML releases / wit3.dtd 7www.ted.com / talks / christine sun kim charming music of sign language 8www.ted.com / talks / joshua foer tricks of memory that anyone can perform seriously."}, {"heading": "3.4 Pivot-based alignment", "text": "The alignment of the Arabic and Hebrew subtitles is done using the algorithm outlined in Figure 2.Starting point are the XML files with subtitles in the three languages. English is aligned with Arabic and Hebrew (Step 1 in the figure) using two independent runs of Gargantua, a sentence compensation described in Section 3.2. As discussed in Section 3.2, the two resulting English pages can be desynchronized, as is actually the case: in one third of the conversations, the number of subtitles in the two alignments differs. Then, Gargantua is run again to align the two desynchronized English pages (Step 2); now, the two maps are used from English to English to rearrange the Arabic and Hebrew sides aligned at this point (Step 3). The automatic procedure outlined above is not error-proof; while measuring errors in Steps 1 and 3 of the English side of the algorithms is possible, two small steps are not perfectly aligned, each of 2 000."}, {"heading": "3.5 Pivot-based sentence rebuilding", "text": "The final step in creating the Arabic-Hebrew parallel corpus is to restore sentences from the matching subtitles. As discussed in Section 3.3, we cannot rely on strong punctuation in the texts of these two languages. Again, the English side is an advantage. In fact, the procedure presented in Section 3.4 provides perfectly synchronized lists of Arabic, Hebrew and English subtitles. As punctuation is reliable on the English side, sentences in the three languages are regenerated by concatenating successive captions until a correct punctuation mark is recognized on the English side."}, {"heading": "4 Data Partitioning and Statistics", "text": "In April 2016, WIT3 will distribute the English transcriptions of 2085 TED conversations, of which the Arabic translation will be available in 2029, while the Hebrew translation will be available in 2065. Common conversations for the three languages (2023) were processed using the alignment / sentence reconstruction process described in the previous section, and were arranged in-train / development / test sets according to the same subdivision that was adopted in the MT tasks of the IWSLT evaluation campaign 2016. Following the IWSLT practice, the conversations contained in evaluation sets of each past evaluation campaign based on TED conversations were removed from the train sets, even if they did not appear in the dev / test sets of this Arabic-Hebrew publication. Therefore, the publication shows a total number of aligned conversations (1908) smaller than 2023 Tables 1 and 2, statistics on monolingual and bilingual corporations of the Arabic Hebrew publication."}, {"heading": "5 Extrinsic Quality Assessment", "text": "The most reliable intrinsic assessment of the quality of the benchmark would be to ask human experts in the two languages to assess the degree of parallelism of a statistically significant set of randomly selected requests. Unable to afford it, 9alt.qcri.org / tools / arabic-normalizer / carried out a series of extrinsic checks based on both MT runs and measurements of train sets."}, {"heading": "5.1 MT baseline performance", "text": "The performance of baseline MT systems on two sets of tests has been measured. The assumption behind this indirect verification is that the better the MT performance, the higher the quality of the train data (and by extending the overall benchmark).SMT systems were developed using the MMT toolkit, 10, which is based on the Moses decoder (Koehn et al., 2007), IRSTLM (Federico et al., 2008) and fast aligned (Dyer et al., 2013).The Baseline MT engine (called Pivot) was estimated on the train data of the benchmark; for comparison purposes, two additional MT systems were based on two Arabic Hebrew bittexts built on the same train, TED speaks of our benchmark, but processed differently; in both subtitles were aligned directly without moving through English; in one case, the original MT systems were kept as they are, i.e. without any sentence reconstruction (none); in another case, the leverage was used by Google, without the actual arrangement being the three."}, {"heading": "5.2 Measurements on the train sets", "text": "Table 4 summarizes the values of the original subtitles (none) and the sentences generated by the strngP and pivot methods. We see that the variability of the sentence length in the pivot version corresponds to those of the original subtitles that can be used as a reference, while the length of the strngP sentences varies much more. Furthermore, in the pivot case, the number of sentences that are normally unmanageable / useless in the standard processing is four / five times less than in strngP. Finally, Table 5 indicates the mean and the default deviation of the difference in the number of tokens between Arabic and Hebrew subtitles. Again, the statistics on the original subtitles (none) can be taken as a gold reference, and again, the pivot version is preferable to the strngP version."}, {"heading": "5.3 Example", "text": "Here we show how the three methods no, strngP and pivot process the example of Figure 1. For the sake of legibility, only the English translation is available. All methods align the original captions correctly. Differences arise from the sentence modification. By definition, none of the five original Ar / He subtitles retains: I would like to invite you to close your eyes. Imagine you are standing at the front door of your house. I would like you to notice the color of the door, the material it consists of. strngP, misled by the lack of strong punctuation on the Hebrew side, joins the five subtitles (and many more) into a long \"sentence\": I would like to invite you [...] that it is off. [...] Instead, the pivot point is able to correctly reconstruct sentences from the original captions: I would like to invite you to close your eyes. Imagine you standing [...] at the door of your house."}, {"heading": "6 Summary", "text": "In this article, we described an Arabic-Hebrew benchmark that builds on the data provided by WIT3. Arabic and Hebrew subtitles of approximately 2,000 TED conversations have been precisely rearranged and aligned into sentences using a novel and effective process that uses English as the lynchpin. Conversations comprise a total of 225,000 sentences and 3.5 million tokens per language and were divided into train, development, and test kits according to the breakdown of the MT tasks of the 2016 IWSLT evaluation campaign."}, {"heading": "Acknowledgments", "text": "This work was partially supported by the CRACKER project, which was funded under the European Union's Horizon 2020 research and innovation programme under grant number 645357. The author thanks Yonatan Belinkov for providing valuable input in the development of the benchmark."}], "references": [{"title": "Large-scale Machine Translation between Arabic and Hebrew: Available Corpora and Initial Results", "author": ["Belinkov", "Glass2016] Yonatan Belinkov", "James Glass"], "venue": "In Proc. of SeMaT,", "citeRegEx": "Belinkov et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Belinkov et al\\.", "year": 2016}, {"title": "Improved Unsupervised Sentence Alignment for Symmetrical and Asymmetrical Parallel Corpora", "author": ["Braune", "Fraser2010] Fabienne Braune", "Alexander Fraser"], "venue": "In Proc. of Coling 2010: Posters,", "citeRegEx": "Braune et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Braune et al\\.", "year": 2010}, {"title": "WIT: Web Inventory of Transcribed and Translated Talks", "author": ["Christian Girardi", "Marcello Federico"], "venue": "In Proc. of EAMT,", "citeRegEx": "Cettolo et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Cettolo et al\\.", "year": 2012}, {"title": "A Simple, Fast, and Effective Reparameterization of IBM Model 2", "author": ["Dyer et al.2013] Chris Dyer", "Victor Chahuneau", "Noah A. Smith"], "venue": "In Proc. of NAACL,", "citeRegEx": "Dyer et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2013}, {"title": "Morphological Constraints for Phrase Pivot Statistical Machine Translation", "author": ["El Kholy", "Habash2015] Ahmed El Kholy", "Nizar Habash"], "venue": "In Proc. of MT Summit XV,", "citeRegEx": "Kholy et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kholy et al\\.", "year": 2015}, {"title": "IRSTLM: an Open Source Toolkit for Handling Large Scale Language Models", "author": ["Nicola Bertoldi", "Mauro Cettolo"], "venue": "In Proc. of Interspeech,", "citeRegEx": "Federico et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Federico et al\\.", "year": 2008}, {"title": "Europarl: A Parallel Corpus for Statistical Machine Translation", "author": ["Philipp Koehn"], "venue": "In Proc. of MT Summit X,", "citeRegEx": "Koehn.,? \\Q2005\\E", "shortCiteRegEx": "Koehn.", "year": 2005}, {"title": "Opensubtitles2016: Extracting Large Parallel Corpora from Movie and TV Subtitles", "author": ["Lison", "Tiedemann2016] Pierre Lison", "J\u00f6rg Tiedemann"], "venue": "In Proc. of LREC,", "citeRegEx": "Lison et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Lison et al\\.", "year": 2016}, {"title": "Machine Translation between Hebrew and Arabic", "author": ["Shilon et al.2012] Reshef Shilon", "Nizar Habash", "Alon Lavie", "Shuly Wintner"], "venue": "Machine Translation,", "citeRegEx": "Shilon et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Shilon et al\\.", "year": 2012}], "referenceMentions": [{"referenceID": 2, "context": "WIT3 (Cettolo et al., 2012)2 is a Web inventory that offers access to a collection of TED talks, redistributing the original TED website contents through", "startOffset": 5, "endOffset": 27}, {"referenceID": 8, "context": "Another Arabic-Hebrew corpus we are aware of is that manually prepared by Shilon et al. (2012) for development and evaluation purposes; no statistics on its size is provided in the paper, nor it is publicly available; according to El Kholy and Habash (2015), it consists of some hundred of sentences, definitely less than those included in our benchmark.", "startOffset": 74, "endOffset": 95}, {"referenceID": 8, "context": "Another Arabic-Hebrew corpus we are aware of is that manually prepared by Shilon et al. (2012) for development and evaluation purposes; no statistics on its size is provided in the paper, nor it is publicly available; according to El Kholy and Habash (2015), it consists of some hundred of sentences, definitely less than those included in our benchmark.", "startOffset": 74, "endOffset": 258}, {"referenceID": 6, "context": "The standard tokenization via the tokenizer script released with the Europarl corpus (Koehn, 2005) was applied to English and Hebrew languages, while Arabic was normalized and tokenized by means of the QCRI Arabic Normalizer 3.", "startOffset": 85, "endOffset": 98}, {"referenceID": 5, "context": ", 2007), IRSTLM (Federico et al., 2008) and fast align (Dyer et al.", "startOffset": 16, "endOffset": 39}, {"referenceID": 3, "context": ", 2008) and fast align (Dyer et al., 2013).", "startOffset": 23, "endOffset": 42}], "year": 2016, "abstractText": "We describe an Arabic-Hebrew parallel corpus of TED talks built upon WIT, the Web inventory that repurposes the original content of the TED website in a way which is more convenient for MT researchers. The benchmark consists of about 2,000 talks, whose subtitles in Arabic and Hebrew have been accurately aligned and rearranged in sentences, for a total of about 3.5M tokens per language. Talks have been partitioned in train, development and test sets similarly in all respects to the MT tasks of the IWSLT 2016 evaluation campaign. In addition to describing the benchmark, we list the problems encountered in preparing it and the novel methods designed to solve them. Baseline MT results and some measures on sentence length are provided as an extrinsic evaluation of the quality of the benchmark.", "creator": "LaTeX with hyperref package"}}}