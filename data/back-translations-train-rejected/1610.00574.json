{"id": "1610.00574", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "14-Sep-2016", "title": "Cosine Similarity Search with Multi Index Hashing", "abstract": "Due to rapid development of the Internet, recent years have witnessed an explosion in the rate of data generation. Dealing with data at current scales brings up unprecedented challenges. From the algorithmic view point, executing existing linear algorithms in information retrieval and machine learning on such tremendous amounts of data incur intolerable computational and storage costs. To address this issue, there is a growing interest to map data points in large-scale datasets to binary codes. This can significantly reduce the storage complexity of large-scale datasets. However, one of the most compelling reasons for using binary codes or any discrete representation is that they can be used as direct indices into a hash table. Incorporating hash table offers fast query execution; one can look up the nearby buckets in a hash table populated with binary codes to retrieve similar items. Nonetheless, if binary codes are compared in terms of the cosine similarity rather than the Hamming distance, there is no fast exact sequential procedure to find the $K$ closest items to the query other than the exhaustive search. Given a large dataset of binary codes and a binary query, the problem that we address is to efficiently find $K$ closest codes in the dataset that yield the largest cosine similarities to the query. To handle this issue, we first elaborate on the relation between the Hamming distance and the cosine similarity. This allows finding the sequence of buckets to check in the hash table. Having this sequence, we propose a multi-index hashing approach that can increase the search speed up to orders of magnitude in comparison to the exhaustive search and even approximation methods such as LSH. We empirically evaluate the performance of the proposed algorithm on real world datasets.", "histories": [["v1", "Wed, 14 Sep 2016 23:16:37 GMT  (286kb)", "http://arxiv.org/abs/1610.00574v1", null]], "reviews": [], "SUBJECTS": "cs.DB cs.DS cs.IR cs.LG", "authors": ["sepehr eghbali", "ladan tahvildari"], "accepted": false, "id": "1610.00574"}, "pdf": {"name": "1610.00574.pdf", "metadata": {"source": "CRF", "title": "Cosine Similarity Search with Multi-Index Hashing", "authors": ["Sepehr Eghbali", "Ladan Tahvildari"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 0.00 574v 1 [cs.D B] 14 Sep 2016 1Index Terms - Nearest neighborhood search, binary codes, large area retrieval, cosmic similarity"}, {"heading": "1 INTRODUCTION", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "2 MOTIVATING EXAMPLE", "text": "One approach to solving this problem is to include text-based analysis. For example, in order to find images of the Eiffel Tower rather than searching through the visual content of photos, images are selected that appear on websites with the keyword \"Eiffel Tower.\" This can often lead to a bad result because no image analysis is performed. To solve this problem, Google uses the VisualRank algorithm [25]. The general idea of the VisualRank algorithm is to first find all images that correspond to the query by analyzing their respective tags, metadata, image names and other related features, using the PageRank algorithm [37]. This leads to the creation of a pool of candidate results. The next step is to calculate the similarity between such candidates and the query. Since it is mathematically impracticable to measure distances to all actual images, hashing methods are included to shorten the search time by a similar item is available."}, {"heading": "3 RELATED WORKS", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "4 DEFINITIONS AND PROBLEM STATEMENT", "text": "In the neeisn eeisrmngne\u00fceBnn nvo the eeisrsn eeisrmngne\u00fceBnn nvo the eeisrmnlrrgne\u00fceBnn rf\u00fc ide eeisrmnlrrteeeeeeeeeeBnn rf\u00fc ide eeisrmnlrgneeeeeeBnn rf\u00fc the eeisrmnlrteeeeeeeeeeeeBnn rf\u00fc the eeisrmnlrteeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"}, {"heading": "5 FAST COSINE SIMILARITY SEARCH", "text": "That is, the cosmic similarity between xi and xj is as close as possible to the cosmic similarity between W (xi) and W (xj). The proposed technique uses the binary codes in B to populate a hash table in which each binary code is entered as a direct index into a hash bucket. The problem we want to address is the search for the closest binary codes (in terms of cosmic similarity) in B to populate a hash table in which each binary code is treated as a direct index."}, {"heading": "6 ANGULAR MULTI-INDEX HASHING", "text": "This year it is more than ever before."}, {"heading": "6.2 Cost Analysis", "text": "The cost analysis follows directly from the analysis of the MIH in [36]. As proposed in [36], we assume that we shall calculate the cost of the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the buckets, which we paid for the money, which we paid for the money, which we paid for the money, which we paid for the money, which we paid for the money, which we paid for the money we paid for the money, which we paid for the money, and which we paid for the money we paid for the money we paid for the money we paid for the money, and which we paid for the money we paid for the money we paid for the money."}, {"heading": "7 EXPERIMENT AND RESULTS", "text": "We implemented the AMIH in C + +, in addition to the MIH implementation by the authors of [36] (all codes were compiled with GCC 4.8.4), and the experiments were conducted on a 2.0 GHz Xeon CPU with 256 gigabytes of RAM. Note that different results are obtained with a single core to make the time cost comparisons meaningful. We first compare the performance of the LSH with the linear scan, and then provide an implementation-independent comparison between AMIH and LSH."}, {"heading": "7.1 Datasets", "text": "In our experiments, we used 2 non-synthetic datasets: SIFT: The ANN SIFT1B dataset [39] consists of SIFT descriptors. The available dataset was originally divided into 109 items as a base set, 104 items as a query set, and 108 items as a learning set. Each dataset is a 128-dimensional SIFT vector. TRC2: The TRC2 (Thomas Reuters Text Research Collection 2) consists of 1,800,370 messages covering a period of one year. We used 5 x 105 messages as a learning set, 11106 messages as a base set, and the rest as a query set. We pre-processed the data by removing common stopwords, stems, and then only considering the 2000 most common words in the learning set. Thus, each message is represented as a vector composed of 2000 words."}, {"heading": "7.2 AMIH and Linear Scan Comparison", "text": "The results of linear scanning technique are compared with AMIH in terms of search technology. [The linear scanning algorithms for preserving binary codes in the U.S. and other countries of the world are able to confuse the speed of linear scanning technique with all possible norm values.] The norm of any binary code in the way in which the data is from 0 to 7 in the way in which they increase the speed of linear scanning technique, we can initialize a table with all possible norm values. Moreover, as the term \"in the denominator of (4) is, it is independent of bi, there is no need to consider the value of linear scanning technique. Figure 5 shows the average search speed per query using linear scanning technique for different database sizes. The action shows that the search time is almost independent of K (number of closest neighbors)."}, {"heading": "7.3 AMIH and LSH Comparison", "text": "This year it is so far that it will be able to erenie.n the aforementioned lcihsrcnlrVo"}, {"heading": "8 CONCLUSION AND FUTURE WORK", "text": "This paper proposes a new algorithm for solving the square KNN problem on large-area binary code datasets. By treating binary codes as memory addresses, our proposed algorithm can find similar binary codes in terms of cosmic similarity at a time that grows sublinearly with the size of the dataset. To achieve this, we first established a relationship between the hamming distance and the cosmic similarity, which in turn is used to solve the square KNN problem for applications where binary codes are used as the memory addresses of a hash table. However, the use of a hash table for long codes is often worse than the linear scan due to the large number of empty buckets. To address this problem, as a second contribution, we have proposed the AMIH technique, the multi-indexing approach to reduce both computing and storage costs."}], "references": [{"title": "Practical and optimal lsh for angular distance", "author": ["Alexandr Andoni", "Piotr Indyk", "Thijs Laarhoven", "Ilya Razenshteyn", "Ludwig Schmidt"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "The inverted multi-index", "author": ["Artem Babenko", "Victor Lempitsky"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["Jon Louis Bentley"], "venue": "Communications of the ACM,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1975}, {"title": "Transform coding for fast approximate nearest neighbor search in high dimensions", "author": ["Jonathan Brandt"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2010}, {"title": "Efficient large-scale sequence comparison by locality-sensitive", "author": ["Jeremy Buhler"], "venue": "hashing. Bioinformatics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2001}, {"title": "Hashing with binary autoencoders", "author": ["Miguel A. Carreira-Perpinan", "Ramin Raziperchikolaei"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Support vector machines for histogram-based image classification", "author": ["Olivier Chapelle", "Patrick Haffner", "Vladimir N Vapnik"], "venue": "IEEE Transactions on Neural Networks,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1999}, {"title": "Similarity estimation techniques from rounding algorithms", "author": ["Moses S Charikar"], "venue": "ACM Symposium on Theory of Computing,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "author": ["Mayur Datar", "Nicole Immorlica", "Piotr Indyk", "Vahab S Mirrokni"], "venue": "Annual Symposium on Computational Geometry,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2004}, {"title": "Imagenet: crowdsourcing, benchmarking & other cool things", "author": ["L Fei-Fei"], "venue": "CMU VASC Seminar,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2010}, {"title": "Parameterized Complexity Theory", "author": ["J. Flum", "M. Grohe"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "An algorithm for finding best matches in logarithmic expected time", "author": ["Jerome H. Friedman", "Jon Louis Bentley", "Raphael Ari Finkel"], "venue": "ACM Transactions on Mathmatical Software,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 1977}, {"title": "Similarity search in high dimensions via hashing", "author": ["Aristides Gionis", "Piotr Indyk", "Rajeev Motwani"], "venue": "International Conference on Very Large Data Bases,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1999}, {"title": "Angular quantization-based binary codes for fast similarity search", "author": ["Yunchao Gong", "Sanjiv Kumar", "Vishal Verma", "Svetlana Lazebnik"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2012}, {"title": "Iterative quantization: A procrustean approach to learning binary codes for large-scale image retrieval", "author": ["Yunchao Gong", "Svetlana Lazebnik", "Albert Gordo", "Florent Perronnin"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Asymmetric distances for binary embeddings", "author": ["Albert Gordo", "Florent Perronnin", "Yunchao Gong", "Svetlana Lazebnik"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2014}, {"title": "Multi-index hashing for information retrieval", "author": ["Dan Greene", "Michal Parnas", "Frances Yao"], "venue": "Annual Symposium on Foundations of Computer Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1994}, {"title": "Spatial query integrity with voronoi neighbors", "author": ["Ling Hu", "Wei-Shinn Ku", "Spiridon Bakiras", "Cyrus Shahabi"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2013}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "author": ["Piotr Indyk", "Rajeev Motwani"], "venue": "Annual ACM Symposium on Theory of Computing,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 1998}, {"title": "What is the most efficientway to select nearest neighbor candidates for fast approximate nearest neighbor search", "author": ["Mikio Iwamura", "Takao Sato", "Kenji Kise"], "venue": "IEEE International Conference on Computer Vision,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Big data and its technical challenges", "author": ["HV Jagadish", "Johannes Gehrke", "Alexandros Labrinidis", "Yannis Papakonstantinou", "Jignesh M Patel", "Raghu Ramakrishnan", "Cyrus Shahabi"], "venue": "Communications of the ACM,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2014}, {"title": "Product quantization for nearest neighbor search", "author": ["H. Jegou", "M. Douze", "C. Schmid"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2011}, {"title": "Towards optimal bag-of-features for object categorization and semantic video retrieval", "author": ["Yu-Gang Jiang", "Chong-Wah Ngo", "Jun Yang"], "venue": "International Conference on Image and Video Retrieval,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Visualrank: Applying pagerank to large-scale image search", "author": ["Yushi Jing", "Shumeet Baluja"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2008}, {"title": "Continuous knearest neighbor queries in spatial network databases", "author": ["Mohammad R Kolahdouzan", "Cyrus Shahabi"], "venue": "Spatio- Temporal Database Management,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2004}, {"title": "Learning to hash with binary reconstructive embeddings", "author": ["Brian Kulis", "Trevor Darrell"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2009}, {"title": "Multi-probe lsh: efficient indexing for high-dimensional similarity search", "author": ["Qin Lv", "William Josephson", "ZheWang", "Moses Charikar", "Kai Li"], "venue": "International Conference on Very Large Data Bases,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2007}, {"title": "Multimodal similarity-preserving hashing", "author": ["Jonathan Masci", "Michael M Bronstein", "Alexander Bronstein", "Jurgen Schmidhuber"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Aziawa. Pqtable: Fast exact asymmetric distance neighbor search for product quantization using hash tables", "author": ["Yusuke Matusi", "Toshihiko Yamasaki", "Kiyoharu"], "venue": "International Conference on Computer Vision,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Quantitative analysis of culture using millions of digitized", "author": ["Jean-Baptiste Michel", "Yuan Kui Shen", "Aviva Presser Aiden", "Adrian Veres", "Matthew K Gray", "Joseph P Pickett", "Dale Hoiberg", "Dan Clancy", "Peter Norvig", "Jon Orwant"], "venue": null, "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2011}, {"title": "The power of asymmetry in binary hashing", "author": ["Behnam Neyshabur", "Nati Srebro", "Ruslan R Salakhutdinov", "Yury Makarychev", "Payman Yadollahpour"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2013}, {"title": "Minimal loss hashing for compact binary codes", "author": ["Mohammad Norouzi", "David M Blei"], "venue": "International Conference on Machine Learning,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2011}, {"title": "Hamming distance metric learning", "author": ["Mohammad Norouzi", "David M Blei", "Ruslan R Salakhutdinov"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2012}, {"title": "Fast search in hamming space with multi-index hashing", "author": ["Mohammad Norouzi", "Ali Punjani", "David J Fleet"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Fast exact search in hamming space with multi-index hashing", "author": ["Mohammad Norouzi", "Ali Punjani", "David J Fleet"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2014}, {"title": "The pagerank citation ranking: Bringing order to the web", "author": ["Lawrence Page", "Sergey Brin", "Rajeev Motwani", "Terry Winograd"], "venue": "Technical Report 1999-66,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "Entropy based nearest neighbor search in high dimensions", "author": ["Rina Panigrahy"], "venue": "Annual ACM-SIAM symposium on Discrete algorithm,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2006}, {"title": "Locality sensitive hashing: A comparison of hash function types and querying mechanisms", "author": ["Lo\u0131\u0308c Paulev\u00e9", "Herv\u00e9 J\u00e9gou", "Laurent Amsaleg"], "venue": "Pattern Recognition Letters,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2010}, {"title": "Foundations of multidimensional and metric data structures", "author": ["Hanan Samet"], "venue": null, "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2006}, {"title": "Bayesian locality sensitive hashing for fast similarity search", "author": ["Venu Satuluri", "Srinivasan Parthasarathy"], "venue": "International Conference on Very Large Data Bases,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2012}, {"title": "Sparse spectral hashing", "author": ["Jian Shao", "Fei Wu", "Chuanfei Ouyang", "Xiao Zhang"], "venue": "Pattern Recognition Letters,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2012}, {"title": "In defense of minhash over simhash", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2014}, {"title": "Asymmetric minwise hashing for indexing binary inner products and set containment", "author": ["Anshumali Shrivastava", "Ping Li"], "venue": "International Conference on World Wide Web,", "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["Antonio Torralba", "Rob Fergus", "William T Freeman"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "citeRegEx": "46", "shortCiteRegEx": "46", "year": 1958}, {"title": "Small codes and large image databases for recognition", "author": ["Antonio Torralba", "Rob Fergus", "Yair Weiss"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2008}, {"title": "Sequential projection learning for hashing with compact codes", "author": ["Jun Wang", "Sanjiv Kumar", "Shih-Fu Chang"], "venue": "International Conference on Machine Learning,", "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2010}, {"title": "A quantitative analysis and performance study for similarity-search methods in high-dimensional spaces", "author": ["Roger Weber", "Hans-J\u00f6rg Schek", "Stephen Blott"], "venue": "International Conference on Very Large Data bases,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 1998}, {"title": "Spectral hashing", "author": ["Yair Weiss", "Antonio Torralba", "Rob Fergus"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2009}, {"title": "Binary code ranking with weighted hamming distance", "author": ["Lei Zhang", "Yongdong Zhang", "Jinhu Tang", "Ke Lu", "Qi Tian"], "venue": "IEEE 15 Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Topology preserving hashing for similarity search", "author": ["Lei Zhang", "Yongdong Zhang", "Jinhui Tang", "Xiaoguang Gu", "Jintao Li", "Qi Tian"], "venue": "International Conference on Multimedia,", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2013}], "referenceMentions": [{"referenceID": 29, "context": "For example, in text mining, Google provides N-gram data obtained from over 5 million books [31].", "startOffset": 92, "endOffset": 96}, {"referenceID": 9, "context": "Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks.", "startOffset": 27, "endOffset": 31}, {"referenceID": 43, "context": "Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks.", "startOffset": 45, "endOffset": 49}, {"referenceID": 21, "context": "Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks.", "startOffset": 61, "endOffset": 65}, {"referenceID": 37, "context": "Resources such as ImageNet [10], Tiny Images [46] and Texmex [23], [39] with 14 millions, 80 millions and 1 billion images, respectively, are typical benchmarks for many machine vision tasks.", "startOffset": 67, "endOffset": 71}, {"referenceID": 20, "context": "Dealing with such large-scale datasets brings up unprecedented challenges, in turn necessitating new software tools, algorithms and databases for data acquisition, storage, transmission and retrieval [22].", "startOffset": 200, "endOffset": 204}, {"referenceID": 14, "context": "We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26].", "startOffset": 173, "endOffset": 177}, {"referenceID": 21, "context": "We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26].", "startOffset": 179, "endOffset": 183}, {"referenceID": 17, "context": "We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26].", "startOffset": 237, "endOffset": 241}, {"referenceID": 24, "context": "We mainly focus on efficiently solving the KNN query which is an important problem with wide range of applications in many fields such as data mining, information retrieval [15], [23] and handling multimedia as well as spacial databases [19], [26].", "startOffset": 243, "endOffset": 247}, {"referenceID": 13, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 110, "endOffset": 114}, {"referenceID": 27, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 122, "endOffset": 126}, {"referenceID": 45, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 128, "endOffset": 132}, {"referenceID": 47, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 134, "endOffset": 138}, {"referenceID": 49, "context": "The aim is to find a mapping such that similar items in the original dataset are mapped to close binary codes [14], [18], [29], [48], [50], [52].", "startOffset": 140, "endOffset": 144}, {"referenceID": 6, "context": "For example, in the Bag of Words (BoW) representation of the documents and images, the presence or absence of information, captured in terms of a binary variable, quantifies the items [7], [24].", "startOffset": 184, "endOffset": 187}, {"referenceID": 22, "context": "For example, in the Bag of Words (BoW) representation of the documents and images, the presence or absence of information, captured in terms of a binary variable, quantifies the items [7], [24].", "startOffset": 189, "endOffset": 193}, {"referenceID": 13, "context": "In practice, instead of using a hash table, researchers resort either to the brute force search [14], or to approximate similarity search techniques, such as LSH [20], to solve the angular KNN problem.", "startOffset": 96, "endOffset": 100}, {"referenceID": 18, "context": "In practice, instead of using a hash table, researchers resort either to the brute force search [14], or to approximate similarity search techniques, such as LSH [20], to solve the angular KNN problem.", "startOffset": 162, "endOffset": 166}, {"referenceID": 16, "context": "Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue.", "startOffset": 26, "endOffset": 30}, {"referenceID": 28, "context": "Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue.", "startOffset": 32, "endOffset": 36}, {"referenceID": 33, "context": "Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue.", "startOffset": 38, "endOffset": 42}, {"referenceID": 34, "context": "Multi-Index Hashing (MIH) [17], [30], [35], [36] is a powerful technique to address this issue.", "startOffset": 44, "endOffset": 48}, {"referenceID": 33, "context": "Motivated by the MIH technique proposed in [35], we develop the Angular Multi-Index Hashing technique to realize similar advantages.", "startOffset": 43, "endOffset": 47}, {"referenceID": 0, "context": "A handful of sublinear techniques are known that can provide approximate solutions to the angular KNN problem [1], [8], [44].", "startOffset": 110, "endOffset": 113}, {"referenceID": 7, "context": "A handful of sublinear techniques are known that can provide approximate solutions to the angular KNN problem [1], [8], [44].", "startOffset": 115, "endOffset": 118}, {"referenceID": 41, "context": "A handful of sublinear techniques are known that can provide approximate solutions to the angular KNN problem [1], [8], [44].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Indeed, to address this issue, Google uses the VisualRank algorithm [25].", "startOffset": 68, "endOffset": 72}, {"referenceID": 35, "context": "The general idea of the VisualRank algorithm is to first find all the images that match the query by analyzing their corresponding tags, meta data, image names, and other related features, using the PageRank algorithm [37].", "startOffset": 218, "endOffset": 222}, {"referenceID": 2, "context": "Perhaps the most known example of such techniques is the kd-tree [3] with a worst-case search time of O(dn).", "startOffset": 65, "endOffset": 68}, {"referenceID": 2, "context": "Following [3], many other tree-based indexing algorithms are proposed, such as R-tree, R*-tree, SS-tree, SRtree, and X-tree (see [41] for an overview).", "startOffset": 10, "endOffset": 13}, {"referenceID": 38, "context": "Following [3], many other tree-based indexing algorithms are proposed, such as R-tree, R*-tree, SS-tree, SRtree, and X-tree (see [41] for an overview).", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "Typically, the kd-tree and its variants are efficient for dimensions less than 20 [12], [49].", "startOffset": 82, "endOffset": 86}, {"referenceID": 46, "context": "Typically, the kd-tree and its variants are efficient for dimensions less than 20 [12], [49].", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known.", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known.", "startOffset": 86, "endOffset": 89}, {"referenceID": 12, "context": "Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known.", "startOffset": 91, "endOffset": 95}, {"referenceID": 39, "context": "Among hashing techniques, the Locality Sensitive Hashing (LSH) [20], and its variants [9], [13], [42], are well-known.", "startOffset": 97, "endOffset": 101}, {"referenceID": 47, "context": "To achieve an acceptable retrieval accuracy, the number of hash tables, as well as the number of buckets in each hash table, should be relatively large [50], [51].", "startOffset": 152, "endOffset": 156}, {"referenceID": 48, "context": "To achieve an acceptable retrieval accuracy, the number of hash tables, as well as the number of buckets in each hash table, should be relatively large [50], [51].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "Experimental studies indicate that LSH-based techniques need over a hundred [13] and sometimes several hundreds hash tables [5].", "startOffset": 76, "endOffset": 80}, {"referenceID": 4, "context": "Experimental studies indicate that LSH-based techniques need over a hundred [13] and sometimes several hundreds hash tables [5].", "startOffset": 124, "endOffset": 127}, {"referenceID": 36, "context": "Entropy-based LSH [38] and Mutli-Prob LSH [28] are two well-known techniques to reduce the number of hash tables.", "startOffset": 18, "endOffset": 22}, {"referenceID": 26, "context": "Entropy-based LSH [38] and Mutli-Prob LSH [28] are two well-known techniques to reduce the number of hash tables.", "startOffset": 42, "endOffset": 46}, {"referenceID": 47, "context": "[50] have proposed an eigenvector formulation which aims at finding short similarity preserving binary codes, such that bits are uncorrelated and balanced.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "Binary Reconstructive Embedding (BRE) [27] uses a loss function that penalizes the difference between the Euclidean distance in the input space and the Hamming distance in the binary space.", "startOffset": 38, "endOffset": 42}, {"referenceID": 31, "context": "[33] have used a hinge-like loss function to learn the codes.", "startOffset": 0, "endOffset": 4}, {"referenceID": 32, "context": "In a follow up work, Norouzi et al [34] have used a triple objective function to learn the binary codes for supervised problems.", "startOffset": 35, "endOffset": 39}, {"referenceID": 5, "context": "More recently, Carreira-Perpinan and Raziperchikolaei [6] have used autoencoder to learn binary codes in order to minimize the reconstruction error.", "startOffset": 54, "endOffset": 57}, {"referenceID": 3, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 87, "endOffset": 90}, {"referenceID": 13, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 92, "endOffset": 96}, {"referenceID": 27, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 104, "endOffset": 108}, {"referenceID": 40, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 110, "endOffset": 114}, {"referenceID": 45, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 116, "endOffset": 120}, {"referenceID": 47, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 122, "endOffset": 126}, {"referenceID": 49, "context": "Current literature abounds with techniques for learning similarity preserving mappings [4], [14], [18], [29], [43], [48], [50], [52].", "startOffset": 128, "endOffset": 132}, {"referenceID": 14, "context": "For unlabeled data, if the Euclidean distance is chosen as the measure of similarity, binary codes are usually compared in terms of the Hamming distance (which is equal to the squared Euclidean distance in the binary space) [15], [27].", "startOffset": 224, "endOffset": 228}, {"referenceID": 25, "context": "For unlabeled data, if the Euclidean distance is chosen as the measure of similarity, binary codes are usually compared in terms of the Hamming distance (which is equal to the squared Euclidean distance in the binary space) [15], [27].", "startOffset": 230, "endOffset": 234}, {"referenceID": 13, "context": "In such applications, input vectors are compared in terms of the cosine similarity, and consequently, some techniques use the cosine similarity of binary codes as the similarity measure in the binary space [14], [45].", "startOffset": 206, "endOffset": 210}, {"referenceID": 42, "context": "In such applications, input vectors are compared in terms of the cosine similarity, and consequently, some techniques use the cosine similarity of binary codes as the similarity measure in the binary space [14], [45].", "startOffset": 212, "endOffset": 216}, {"referenceID": 1, "context": "[2] have proposed the inverted multi-index technique to solve the ANN problem where data points are estimated with codewords of multiple codebooks.", "startOffset": 0, "endOffset": 3}, {"referenceID": 28, "context": "[30] have used multiple hash tables to reduce the search time, whereby the distance between items are approximated by codewords of multiple codebooks.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21] have proposed a non-exhaustive search algorithm based on the branch and bound paradigm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "To overcome this problem, recent studies have often resorted to linear search for binary codes longer than 64 bits [14], [40], [47].", "startOffset": 115, "endOffset": 119}, {"referenceID": 44, "context": "To overcome this problem, recent studies have often resorted to linear search for binary codes longer than 64 bits [14], [40], [47].", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "Multi-Index Hashing (MIH) [17], and its variants [35], [36], are elegant approaches for reducing storage and computational costs of the R-near neighbour search for binary codes.", "startOffset": 26, "endOffset": 30}, {"referenceID": 33, "context": "Multi-Index Hashing (MIH) [17], and its variants [35], [36], are elegant approaches for reducing storage and computational costs of the R-near neighbour search for binary codes.", "startOffset": 49, "endOffset": 53}, {"referenceID": 34, "context": "Multi-Index Hashing (MIH) [17], and its variants [35], [36], are elegant approaches for reducing storage and computational costs of the R-near neighbour search for binary codes.", "startOffset": 55, "endOffset": 59}, {"referenceID": 34, "context": "Interested readers are referred to [36] for further details and cost analysis.", "startOffset": 35, "endOffset": 39}, {"referenceID": 34, "context": "The cost analysis directly follows the performance analysis of MIH in [36].", "startOffset": 70, "endOffset": 74}, {"referenceID": 34, "context": "As suggested in [36], we assume that \u230a p log2 n\u230b \u2264 m \u2264 \u2308 p log2 n \u2309.", "startOffset": 16, "endOffset": 20}, {"referenceID": 10, "context": "Assuming that r1+r2 p \u2264 1/2, we can use the following bound on the sum of the binomial coefficients [11]: For any n \u2265 1 and 0 < \u03b1 \u2264 1/2, we have:", "startOffset": 100, "endOffset": 104}, {"referenceID": 34, "context": "We implemented the AMIH in C++ on top of the MIH implementation provided by the authors of [36] (all codes are compiled with GCC 4.", "startOffset": 91, "endOffset": 95}, {"referenceID": 37, "context": "In our experiments, we have used 2 non-synthetic datasets: SIFT: The ANN SIFT1B dataset [39] consists of SIFT descriptors.", "startOffset": 88, "endOffset": 92}, {"referenceID": 13, "context": "We have used the angular-preserving mapping method called Angular Quantization-based Binary Codes (AQBC) proposed in [14], to create the dataset of binary codes.", "startOffset": 117, "endOffset": 121}, {"referenceID": 13, "context": "The MATLAB code for this method has been made publicly available by the authors of [14].", "startOffset": 83, "endOffset": 87}, {"referenceID": 16, "context": "multi-index hashing technique in all experiments is set to p log2 n , following [17], [30], [36].", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "multi-index hashing technique in all experiments is set to p log2 n , following [17], [30], [36].", "startOffset": 86, "endOffset": 90}, {"referenceID": 34, "context": "multi-index hashing technique in all experiments is set to p log2 n , following [17], [30], [36].", "startOffset": 92, "endOffset": 96}, {"referenceID": 7, "context": "One of the popular hash functions for the cosine similarity measure is the Simhash function [8] (also known as the hyperplane LSH) which uses sign random projection.", "startOffset": 92, "endOffset": 95}, {"referenceID": 18, "context": "We have implemented the standard (G,L) parametrized LSH [20] algorithm with the simhash as the hash function.", "startOffset": 56, "endOffset": 60}, {"referenceID": 13, "context": "Both techniques are applied on 1 million binary codes; the original points are chosen randomly from the SIFT dataset, then AQBC algorithm [14] is applied to all points in the query and the base set.", "startOffset": 138, "endOffset": 142}, {"referenceID": 48, "context": "One potential avenue for future work is to find fast search algorithms for other measures of similarity such as the spherical Hamming distance [18] and the weighted Hamming distance [51].", "startOffset": 182, "endOffset": 186}, {"referenceID": 34, "context": "Also, the value of m in our experiments is chosen based on the value proposed for MIH in [36].", "startOffset": 89, "endOffset": 93}, {"referenceID": 15, "context": "It has been shown that using asymmetric distance measures, such as the asymmetric Hamming distance, can boost the retrieval accuracy [16], [32].", "startOffset": 133, "endOffset": 137}, {"referenceID": 30, "context": "It has been shown that using asymmetric distance measures, such as the asymmetric Hamming distance, can boost the retrieval accuracy [16], [32].", "startOffset": 139, "endOffset": 143}], "year": 2016, "abstractText": "Due to rapid development of the Internet, recent years have witnessed an explosion in the rate of data generation. Dealing with data at current scales brings up unprecedented challenges. From the algorithmic view point, executing existing linear algorithms in information retrieval and machine learning on such tremendous amounts of data incur intolerable computational and storage costs. To address this issue, there is a growing interest to map data points in large-scale datasets to binary codes. This can significantly reduce the storage complexity of large-scale datasets. However, one of the most compelling reasons for using binary codes or any discrete representation is that they can be used as direct indices into a hash table. Incorporating hash table offers fast query execution; one can look up the nearby buckets in a hash table populated with binary codes to retrieve similar items. Nonetheless, if binary codes are compared in terms of the cosine similarity rather than the Hamming distance, there is no fast exact sequential procedure to find the K closest items to the query other than the exhaustive search. Given a large dataset of binary codes and a binary query, the problem that we address is to efficiently find K closest codes in the dataset that yield the largest cosine similarities to the query. To handle this issue, we first elaborate on the relation between the Hamming distance and the cosine similarity. This allows finding the sequence of buckets to check in the hash table. Having this sequence, we propose a multi-index hashing approach that can increase the search speed up to orders of magnitude in comparison to the exhaustive search and even approximation methods such as LSH. We empirically evaluate the performance of the proposed algorithm on real world datasets.", "creator": "LaTeX with hyperref package"}}}