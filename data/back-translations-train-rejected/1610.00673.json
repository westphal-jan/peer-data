{"id": "1610.00673", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search", "abstract": "In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of real-world conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.", "histories": [["v1", "Mon, 3 Oct 2016 18:54:00 GMT  (4289kb,D)", "http://arxiv.org/abs/1610.00673v1", "Submitted to the IEEE International Conference on Robotics and Automation 2017"]], "COMMENTS": "Submitted to the IEEE International Conference on Robotics and Automation 2017", "reviews": [], "SUBJECTS": "cs.LG cs.AI cs.RO", "authors": ["ali yahya", "adrian li", "mrinal kalakrishnan", "yevgen chebotar", "sergey levine"], "accepted": false, "id": "1610.00673"}, "pdf": {"name": "1610.00673.pdf", "metadata": {"source": "CRF", "title": "Collective Robot Reinforcement Learning with Distributed Asynchronous Guided Policy Search", "authors": ["Ali Yahya", "Adrian Li", "Mrinal Kalakrishnan", "Yevgen Chebotar", "Sergey Levine"], "emails": ["alive@x.team", "alhli@x.team", "kalakris@x.team"], "sections": [{"heading": null, "text": "This year, it will be able to take the measures mentioned in order to defuse the situation."}, {"heading": "II. RELATED WORK", "text": "In fact, most motor skill successes involve significant manual representation design to enable effective generalization; for example, the well-known dynamic movement of primitive representation is widely used to generalize acquired skills, but it limits the learning process related to tractor creation."}, {"heading": "III. PRELIMINARIES ON GUIDED POLICY SEARCH", "text": "In this section, we will define the problem formulation and briefly summarize the basic policies (GPS), specifically highlighting computer-aided bottlenecks that can be mitigated by asynchrony and parallelism. However, a more complete description of the theoretical foundations of the method can be found in previous work [7]. The goal of policy search methods is to optimize the parameters of a policy approach (ut | xt) that defines a probability distribution through robot actions that are conditioned in each step of the execution of a task. Let's be [x1, u1, u1,., uT) a trajectory of states and measures that define a cost function l (xt, ut), let's define the cost of trajectories l (T = 1 l, ut)."}, {"heading": "A. Local Policy Optimization", "text": "In this paper, we consider two possible methods for local policy optimization: c) LQR with local models: In order to take a model-based approach to optimizing time-varying linear-Gaussian local strategies, we can observe that under time-varying linear-Gaussian dynamics, local strategies can be optimized analytically using the LQR method or the iterative LQR method in the case of non-square costs. However, this approach requires a linearization of system dynamics that is generally not known for complex robotic manipulation tasks. As described in previous work, we can still use LQR if we match a time-varying linear-Gaussian model to the examples of linear-Gaussian regression. In this approach, the samples of the algorithms generated on Line 1 are used."}, {"heading": "IV. ASYNCHRONOUS DISTRIBUTED GUIDED POLICY SEARCH", "text": "This training regime presents two challenges: (1) there is considerable downtime for the robot, while politics is both asynchronous and distributed (see Figure 3); in our asynchronous distributed GPS (ADGPS) method, the algorithm is decoupled into global and local labor; to overcome these challenges, we propose a modification of global politics that is both asynchronous and distributed (see Figure 3); in our asynchronously distributed GPS method, the collected data is decoupled into global and local labor; and global workers are responsible for continuously optimizing global politics using an experience buffer called replay memory; local workers execute the current controllers on their respective robots by adding the collected data to the replay memory; and thelocal workers are also responsible for updating local policies."}, {"heading": "V. EXPERIMENTAL EVALUATION", "text": "Our experimental evaluation aims to answer two questions about our proposed asynchronous learning system: (1) accelerates the formation of complex, nonlinear neural network strategies, and (2) improves training within an ensemble of robots to generalize the resulting strategies. The answers to these questions are not trivial: While it may seem natural that parallelizing experience acquisition should accelerate learning, it is by no means clear whether the additional distortion introduced by asynchronous training would not outweigh the benefits of a larger amount of data, or whether the amount of data is actually the limiting factor."}, {"heading": "A. Simulated Evaluation", "text": "In the simulation, we can systematically vary the number of robots to assess how training times scale with the number of workers, as well as study the effects of asynchronous training. We simulate multiple 7-DoF arms with parallel simulators, each running in real time, to mimic execution times that would be observed on real robots. Arms are controlled by speed control to perform a simple Cartesian achievement task that requires placing the end effector at a command-determined position. The robot-state vector consists of common angles and speeds, as well as its end effector pose and speed. We use a 9-DoF parameterization of the pose, which represents the positions of the robot end effector in the base frame."}, {"heading": "B. Real-World Evaluation", "text": "This year, more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a region, and in which it is a country, in which it is a country, in which it is not a country, but in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country, in a country,"}, {"heading": "VI. DISCUSSION AND FUTURE WORK", "text": "We have presented a system for distributed asynchronous political learning between multiple robots that can work together to learn a single generalizable motor skill, represented by a deep neural network. Our method extends the guided political search algorithm to the asynchronous environment in which maximum robot use is achieved through parallelization of political training and experience acquisition; the robots continually gain new experience and add it to a replay buffer that is used to train global politics for neural networks; at the same time, each robot individually improves its local politics to succeed in its own example of the task. Our simulated experiments show that this approach can shorten training times, while our assessment in the real world shows that a policy trained on multiple cases of different doors can improve the generalization capability of vision-based policies. Our method also assumes that each robot can execute the same policies that implicitly imply assume that the robots are physically similar or identical."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We would like to thank Peter Pastor and Kurt Konolige for additional engineering, robot maintenance and technical discussions, and Ryan Walker and Gary Vosters for developing custom hardware for this project."}], "references": [{"title": "Stochastic policy gradient reinforcement learning on a simple 3d biped", "author": ["R. Tedrake", "T.W. Zhang", "H.S. Seung"], "venue": "In IROS,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2004}, {"title": "Relative entropy policy search", "author": ["J. Peters", "K. M\u00fclling", "Y. Altun"], "venue": "In AAAI,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2010}, {"title": "A generalized path integral control approach to reinforcement learning", "author": ["E. Theodorou", "J. Buchli", "S. Schaal"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "A survey on policy search for robotics", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "Foundations and Trends in Robotics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Trust region policy optimization", "author": ["J. Schulman", "S. Levine", "P. Moritz", "M. Jordan", "P. Abbeel"], "venue": "In ICML,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "In International Conference on Learning Representations (ICLR),", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2016}, {"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2016}, {"title": "Movement imitation with nonlinear dynamical systems in humanoid robots", "author": ["J.A. Ijspeert", "J. Nakanishi", "S. Schaal"], "venue": "In ICRA,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2002}, {"title": "Deep learning", "author": ["Yann Lecun", "Yoshua Bengio", "Geoffrey Hinton"], "venue": "Nature, 521(7553):436\u2013444,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Acquiring visual servoing reaching and grasping skills using neural reinforcement learning", "author": ["Thomas Lampe", "Martin Riedmiller"], "venue": "In IEEE International Joint Conference on Neural Networks (IJCNN", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "A platform for robotics research based on the remote-brained robot approach", "author": ["M. Inaba", "S. Kagami", "F. Kanehiro", "Y. Hoshino"], "venue": "International Journal of Robotics Research,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2000}, {"title": "Cloud-enabled humanoid robots", "author": ["J. Kuffner"], "venue": "In IEEE-RAS International Conference on Humanoid Robotics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Cloud-based robot grasping with the google object recognition engine", "author": ["B. Kehoe", "A. Matsukawa", "S. Candido", "J. Kuffner", "K. Goldberg"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "A survey of research on cloud robotics and automation", "author": ["B. Kehoe", "S. Patil", "P. Abbeel", "K. Goldberg"], "venue": "IEEE Transactions on Automation Science and Engineering,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Parallelized stochastic gradient descent", "author": ["Martin Zinkevich", "Markus Weimer", "Lihong Li", "Alex J. Smola"], "venue": "In Neural Information Processing Systems (NIPS),", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2010}, {"title": "Interactive control of diverse complex characters with neural networks", "author": ["Igor Mordatch", "Kendall Lowrey", "Galen Andrew", "Zoran Popovic", "Emanuel V Todorov"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2015}, {"title": "Guided policy search as approximate mirror descent", "author": ["W. Montgomery", "S. Levine"], "venue": "In NIPS,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Learning neural network policies with guided policy search under unknown dynamics", "author": ["S. Levine", "P. Abbeel"], "venue": "In NIPS,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Path integral guided policy search", "author": ["Y. Chebotar", "M. Kalakrishnan", "A. Yahya", "A. Li", "S. Schaal", "S. Levine"], "venue": "Technical Report,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2017}, {"title": "Bregman alternating direction method of multipliers", "author": ["H. Wang", "A. Banerjee"], "venue": "In NIPS,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Hierarchical relative entropy policy search", "author": ["Christian Daniel", "Gerhard Neumann", "Jan Peters"], "venue": "In AISTATS,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "Iterative linear quadratic regulator design for nonlinear biological movement systems", "author": ["W. Li", "E. Todorov"], "venue": "In ICINCO,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2004}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "Revisiting distributed synchronous sgd", "author": ["Jianmin Chen", "Rajat Monga", "Samy Bengio", "Rafal Jozefowicz"], "venue": "In International Conference on Learning Representations Workshop Track,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous distributed systems, 2015. URL http:// tensorflow.org", "author": ["Mart\u0131\u0301n Abadi", "Ashish Agarwal", "Paul Barham", "Eugene Brevdo", "Zhifeng Chen", "Craig Citro", "Greg S. Corrado", "Andy Davis", "Jeffrey Dean", "Matthieu Devin", "Sanjay Ghemawat", "Ian Goodfellow", "Andrew Harp", "Geoffrey Irving", "Michael Isard", "Yangqing Jia", "Rafal Jozefowicz", "Lukasz Kaiser", "Manjunath Kudlur", "Josh Levenberg", "Dan Man\u00e9", "Rajat Monga", "Sherry Moore", "Derek Murray", "Chris Olah", "Mike Schuster", "Jonathon Shlens", "Benoit Steiner", "Ilya Sutskever", "Kunal Talwar", "Paul Tucker", "Vincent Vanhoucke", "Vijay Vasudevan", "Fernanda Vi\u00e9gas", "Oriol Vinyals", "Pete Warden", "Martin Wattenberg", "Martin Wicke", "Yuan Yu", "Xiaoqiang Zheng"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Joint training of a convolutional network and a graphical model for human pose estimation", "author": ["J. Tompson", "A. Jain", "Y. LeCun", "C. Bregler"], "venue": "In NIPS,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Going further with point pair features", "author": ["S. Hinterstoisser", "V. Lepetit", "N. Rajkumar", "K. Konolige"], "venue": "In ECCV,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Policy search techniques show promising ability to learn feedback control policies for robotic tasks with highdimensional sensory inputs through trial and error [1, 2, 3, 4].", "startOffset": 161, "endOffset": 173}, {"referenceID": 1, "context": "Policy search techniques show promising ability to learn feedback control policies for robotic tasks with highdimensional sensory inputs through trial and error [1, 2, 3, 4].", "startOffset": 161, "endOffset": 173}, {"referenceID": 2, "context": "Policy search techniques show promising ability to learn feedback control policies for robotic tasks with highdimensional sensory inputs through trial and error [1, 2, 3, 4].", "startOffset": 161, "endOffset": 173}, {"referenceID": 3, "context": "Policy search techniques show promising ability to learn feedback control policies for robotic tasks with highdimensional sensory inputs through trial and error [1, 2, 3, 4].", "startOffset": 161, "endOffset": 173}, {"referenceID": 4, "context": "Recently, deep reinforcement learning (RL) methods have been used to show that policies for complex tasks can be trained end-to-end, directly from raw sensory inputs (like images [5, 6]) to actions.", "startOffset": 179, "endOffset": 185}, {"referenceID": 5, "context": "Recently, deep reinforcement learning (RL) methods have been used to show that policies for complex tasks can be trained end-to-end, directly from raw sensory inputs (like images [5, 6]) to actions.", "startOffset": 179, "endOffset": 185}, {"referenceID": 6, "context": "Methods based on Guided Policy Search (GPS) [7], which convert the policy search problem into a supervised learning problem, with a local trajectory-centric RL algorithm acting as a teacher, reduce sample complexity and thereby help make said applications tractable.", "startOffset": 44, "endOffset": 47}, {"referenceID": 0, "context": "motion skills [1, 2, 3, 4].", "startOffset": 14, "endOffset": 26}, {"referenceID": 1, "context": "motion skills [1, 2, 3, 4].", "startOffset": 14, "endOffset": 26}, {"referenceID": 2, "context": "motion skills [1, 2, 3, 4].", "startOffset": 14, "endOffset": 26}, {"referenceID": 3, "context": "motion skills [1, 2, 3, 4].", "startOffset": 14, "endOffset": 26}, {"referenceID": 7, "context": "For example, the well-known dynamic movement primitive representation [8] has been widely used to generalize learned skills by adapting the goal state, but it inherently restricts the learning process to trajectory-centric behaviors.", "startOffset": 70, "endOffset": 73}, {"referenceID": 8, "context": "Recent years have seen improvement in the generalizability of passive perception systems, in domains such as computer vision, natural language processing, and speech recognition through the use of deep learning techniques [9].", "startOffset": 222, "endOffset": 225}, {"referenceID": 4, "context": "While several works have extended deep learning methods to simulated [5, 6] and real-world [7, 10] robotic tasks, the kind of generalization exhibited by deep learning in passive perception domains has not yet been demonstrated for robotic skill learning.", "startOffset": 69, "endOffset": 75}, {"referenceID": 5, "context": "While several works have extended deep learning methods to simulated [5, 6] and real-world [7, 10] robotic tasks, the kind of generalization exhibited by deep learning in passive perception domains has not yet been demonstrated for robotic skill learning.", "startOffset": 69, "endOffset": 75}, {"referenceID": 6, "context": "While several works have extended deep learning methods to simulated [5, 6] and real-world [7, 10] robotic tasks, the kind of generalization exhibited by deep learning in passive perception domains has not yet been demonstrated for robotic skill learning.", "startOffset": 91, "endOffset": 98}, {"referenceID": 9, "context": "While several works have extended deep learning methods to simulated [5, 6] and real-world [7, 10] robotic tasks, the kind of generalization exhibited by deep learning in passive perception domains has not yet been demonstrated for robotic skill learning.", "startOffset": 91, "endOffset": 98}, {"referenceID": 10, "context": "The ability of robotic systems to learn more quickly and effectively by pooling their collective experience has long been recognized in the domain of cloud robotics, where it is typically referred to as collective robotic learning [11, 12, 13, 14].", "startOffset": 231, "endOffset": 247}, {"referenceID": 11, "context": "The ability of robotic systems to learn more quickly and effectively by pooling their collective experience has long been recognized in the domain of cloud robotics, where it is typically referred to as collective robotic learning [11, 12, 13, 14].", "startOffset": 231, "endOffset": 247}, {"referenceID": 12, "context": "The ability of robotic systems to learn more quickly and effectively by pooling their collective experience has long been recognized in the domain of cloud robotics, where it is typically referred to as collective robotic learning [11, 12, 13, 14].", "startOffset": 231, "endOffset": 247}, {"referenceID": 13, "context": "The ability of robotic systems to learn more quickly and effectively by pooling their collective experience has long been recognized in the domain of cloud robotics, where it is typically referred to as collective robotic learning [11, 12, 13, 14].", "startOffset": 231, "endOffset": 247}, {"referenceID": 14, "context": "Distributed systems have long been an important subject in deep learning [15].", "startOffset": 73, "endOffset": 77}, {"referenceID": 15, "context": "While distributed asynchronous architectures have previously been used to optimize controllers for simulated characters [16], our work is, to the best of our knowledge, the first to experimentally explore distributed asynchronous training of deep neural network policies for real-world robotic control.", "startOffset": 120, "endOffset": 124}, {"referenceID": 6, "context": "A more complete description of the theoretical underpinnings of the method can be found in prior work [7].", "startOffset": 102, "endOffset": 105}, {"referenceID": 3, "context": "However, this kind of direct model-free method can quickly become intractable for very high-dimensional policies, such as the large neural network policies considered in this work [4].", "startOffset": 180, "endOffset": 183}, {"referenceID": 6, "context": "In this work, we employ time-varying linear-Gaussian controllers of the form pi(ut|xt) = N (Ktxt+kt,Ct) to represent these local policies, following prior work [7].", "startOffset": 160, "endOffset": 163}, {"referenceID": 6, "context": "This allows the global policy to predict actions from raw observations at test time [7].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "In this work, we will examine a general asynchronous framework for guided policy search algorithms, and we will show how this framework can be instantiated to extend two prior guided policy search methods: BADMM-based guided policy search [7] and mirror descent guided policy search", "startOffset": 239, "endOffset": 242}, {"referenceID": 16, "context": "(MDGPS) [17].", "startOffset": 8, "endOffset": 12}, {"referenceID": 17, "context": "structure, with alternating optimization of the local policies via trajectory-centric RL, which in the case of our system is either a model-based algorithm based on LQR [18] or a model-free algorithm based on PI [3], and optimization of the global policy via supervised learning through stochastic gradient descent (SGD).", "startOffset": 169, "endOffset": 173}, {"referenceID": 2, "context": "structure, with alternating optimization of the local policies via trajectory-centric RL, which in the case of our system is either a model-based algorithm based on LQR [18] or a model-free algorithm based on PI [3], and optimization of the global policy via supervised learning through stochastic gradient descent (SGD).", "startOffset": 212, "endOffset": 215}, {"referenceID": 18, "context": "The adaptation of PI to guided policy search is described in detail in a companion paper [19].", "startOffset": 89, "endOffset": 93}, {"referenceID": 6, "context": "a) BADMM-based GPS: In BADMM-based guided policy search [7], the alternating optimization is formalized as a constrained optimization of the form", "startOffset": 56, "endOffset": 59}, {"referenceID": 19, "context": "The constrained optimization is then solved using the Bregman ADMM algorithm [20], which augments the objective for both the local and global policies with Lagrange multipliers that keep them similar in terms of KL-divergence.", "startOffset": 77, "endOffset": 81}, {"referenceID": 1, "context": "The constraint ensures that the local policies only change by a small amount at each iteration, to prevent divergence of the trajectory-centric RL algorithm, analogously to other recent RL methods [2, 21].", "startOffset": 197, "endOffset": 204}, {"referenceID": 20, "context": "The constraint ensures that the local policies only change by a small amount at each iteration, to prevent divergence of the trajectory-centric RL algorithm, analogously to other recent RL methods [2, 21].", "startOffset": 197, "endOffset": 204}, {"referenceID": 6, "context": "The derivations of \u03c6i(\u03c4, \u03b8,xt) and \u03c6\u03b8(pi, \u03b8,xt) are provided in prior work [7].", "startOffset": 75, "endOffset": 78}, {"referenceID": 16, "context": "b) MDGPS: In MDGPS [17], the local policies are optimized with respect to", "startOffset": 19, "endOffset": 23}, {"referenceID": 21, "context": "c) LQR with local models: To take a model-based approach to optimization of time-varying linear-Gaussian local policies, we can observe that, under time-varying linearGaussian dynamics, the local policies can be optimized analytically using the LQR method, or the iterative LQR method in the case of non-quadratic costs [22].", "startOffset": 320, "endOffset": 324}, {"referenceID": 17, "context": "As described in prior work [18], we can still use LQR if we fit a time-varying linear-Gaussian model to the samples using linear regression.", "startOffset": 27, "endOffset": 31}, {"referenceID": 17, "context": "in Equation (2) or (3) with a simple modification that uses LQR within a dual gradient descent loop [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 2, "context": "d) PI: Policy Improvement with Path Integrals (PI) is a model-free policy search method based on the principles of stochastic optimal control [3].", "startOffset": 142, "endOffset": 145}, {"referenceID": 18, "context": "In this work, we employ PI to learn feedforward commands kt of time-varying linear-Gaussian controllers as described in [19].", "startOffset": 120, "endOffset": 124}, {"referenceID": 1, "context": "In this work, we use relative entropy optimization [2] to determine the temperature \u03b7 at each time step independently, based on a KL-divergence constraint between policy updates.", "startOffset": 51, "endOffset": 54}, {"referenceID": 16, "context": "In this case, new initial states can be sampled at each iteration of the algorithm, with new local policies instantiated for each one [17].", "startOffset": 134, "endOffset": 138}, {"referenceID": 22, "context": "stored on a parameter server [23], allowing multiple robots to concurrently collect data while multiple machines concurrently apply updates to the same global policy.", "startOffset": 29, "endOffset": 33}, {"referenceID": 23, "context": "batch, we found that distributed training actually improved stability when aggregating gradients across multiple robots and machines [24].", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "This entire system, which we call asynchronous distributed guided policy search (ADGPS), was implemented in the distributed machine learning framework TensorFlow [25], and is summarized in Algorithms 2 and 3.", "startOffset": 162, "endOffset": 166}, {"referenceID": 6, "context": "Our architecture resembles prior work [7], with the visual features represented by feature points produced via a spatial softmax applied to the last convolutional response maps.", "startOffset": 38, "endOffset": 41}, {"referenceID": 6, "context": "Unlike in [7], our convolutional network includes multiple stages of pooling and skip connections, which allows the visual features to incorporate information at various scales: low-level, high-resolution, local features as well as higher-level features with larger spatial context.", "startOffset": 10, "endOffset": 13}, {"referenceID": 26, "context": "a geometry-based pose estimator based on the point pair feature (PPF) algorithm [27].", "startOffset": 80, "endOffset": 84}, {"referenceID": 25, "context": "The outputs of these 5 layers are recombined by passing each of them into a 1x1 convolution, converting them to a size of 125x157 by using nearest-neighbor upscaling, and summation (similar to [26]).", "startOffset": 193, "endOffset": 197}, {"referenceID": 6, "context": "The spatial soft-argmax operator [7] computes the expected 2D image coordinates of each feature.", "startOffset": 33, "endOffset": 36}, {"referenceID": 6, "context": "In future work, it would be straightforward to also fine-tune the convolutional layers end-to-end with guided policy search as in prior work [7], but we found that we could obtain satisfactory performance without end-to-end training of the vision layers on this task.", "startOffset": 141, "endOffset": 144}, {"referenceID": 18, "context": "In comparison, a successful policy that was trained on only a single robot and a single door using GPS with PI as in [19] fails to generalize to either an unseen door or different camera positions.", "startOffset": 117, "endOffset": 121}], "year": 2016, "abstractText": "In principle, reinforcement learning and policy search methods can enable robots to learn highly complex and general skills that may allow them to function amid the complexity and diversity of the real world. However, training a policy that generalizes well across a wide range of realworld conditions requires far greater quantity and diversity of experience than is practical to collect with a single robot. Fortunately, it is possible for multiple robots to share their experience with one another, and thereby, learn a policy collectively. In this work, we explore distributed and asynchronous policy learning as a means to achieve generalization and improved training times on challenging, real-world manipulation tasks. We propose a distributed and asynchronous version of Guided Policy Search and use it to demonstrate collective policy learning on a vision-based door opening task using four robots. We show that it achieves better generalization, utilization, and training times than the single robot alternative.", "creator": "LaTeX with hyperref package"}}}