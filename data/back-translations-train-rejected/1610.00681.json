{"id": "1610.00681", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Team-Optimal Distributed MMSE Estimation in General and Tree Networks", "abstract": "We construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation.", "histories": [["v1", "Mon, 3 Oct 2016 19:09:01 GMT  (2369kb)", "http://arxiv.org/abs/1610.00681v1", "Submitted to Digital Signal Processing"], ["v2", "Sun, 8 Jan 2017 19:28:02 GMT  (410kb,D)", "http://arxiv.org/abs/1610.00681v2", "Submitted to Digital Signal Processing"]], "COMMENTS": "Submitted to Digital Signal Processing", "reviews": [], "SUBJECTS": "cs.SY cs.LG", "authors": ["muhammed o sayin", "suleyman s kozat", "tamer ba\\c{s}ar"], "accepted": false, "id": "1610.00681"}, "pdf": {"name": "1610.00681.pdf", "metadata": {"source": "CRF", "title": "Network Structures and Fast Distributed MMSE Estimation", "authors": ["Muhammed O. Sayina", "Suleyman S. Kozatb"], "emails": ["sayin2@illinois.edu", "kozat@ee.bilkent.edu.tr"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.00 681v 1 [cs.S Y] 3O ct2 01We construct optimal estimation algorithms over distributed networks to estimate the state in the sense of the Middle Square (MSE). Here we have a distributed collection of agents with processing and cooperation skills. These agents continuously observe a noisy version of a desired state of nature through a linear model and try to learn this state through interaction with each other. Although this problem has attracted significant attention and has been extensively investigated in various areas, including machine learning theory on signal processing, all known strategies achieve sub-optimal learning performance in the sense of MSE. To this end, we provide algorithms that achieve distributed minimal MSE (MMSE) performance via an arbitrary network topology based on the aggregation of information among each agent. This approach differs from the diffusion of information over the network, i.e. the exchange of local estimates that is important per unit of time."}, {"heading": "1. Introduction", "text": "In fact, most of us are able to survive on our own."}, {"heading": "2. Distributed-MMSE Estimation Framework", "text": "Consider a distributed network of M-agents with processing and communication capacities. In Fig. 1, we illustrate this network using an undirected diagram in which the depressions and edges correspond to the agents and communication connections in the network, describing the amount of agents whose information could be received at least according to k-hops, i.e., k-hopp communication, as agents randomly obtained. (k) We assume that N (0) and N (k) i), that the agents (k) i (k) i (k) i (k) i (k) i (k) i (n) i (k) i (n) i (k) i (n) i (k) i (n) i (n) i (n) i (n) i (k) i (n) i (n) i (n) i (n) i (n) i (n) i. Here, the agents observe a noisy version of a time-invariant and unknown state vector, which is a realization of the process sian-vector."}, {"heading": "2.1. ODOL Algorithm", "text": "Since the state and the observation noise are independent Gauss random parameters, we propose the ODOL algorithm, which iteratively constructs the distributed MMSE valuer (3). For this purpose, we collect all received information at a time t aszi, t = col {yi, t, yi1, t \u2212 1,..., y j1, t \u2212 \u03bai,..., y j \u03c0 (\u03bai) i, t \u2212 \u0445i}. Subsequently, the iterations of the ODOL algorithm of Ki, t = \u03a3, t \u2212 1H, T i (H-i\u03a3, t \u2212 1H-T i + \u0445ni) \u2212 1, x-i, t = (I-Ki, tH-i, tH-pump, tH-i, tH-i, Pump, Pump, Pump, Pump, Pump, Pump, Pump, Pump, Pump, Pump, Pump, and, Pump, Pump, Pump, Pump, P, Pump, Pump, Pump, observation noise are constructed iteratively."}, {"heading": "3. Distributed-MMSE Estimation with Disclosure of Local Estimate", "text": "Since the underlying state and the observation noises are independent Gaussian random parameters, the conditional expectation of the state in view of the observations, i.e. the MMSE estimate, is an affinity combination of the observations. Therefore, in this section, we aim to exploit the affinity transformation to reduce the amount of information disclosed so that each agent exchanges the necessary information in an efficient manner. We point out that, as shown in the following example, the disclosure of local estimates is not sufficient to achieve the distributed MMSE performance in general. Consider a cycle network of 4 Agent-1 in which Agent-1 is directly connected to Agent-2 and 3."}, {"heading": "3.1. Tree Networks", "text": "A network has a \"tree structure\" if its corresponding graph is a tree, i.e., connected and undirected without any cycles (26). As an example, there are numerous distributed algorithms for a minimum tree construction (27, 28, 30, 31). It is important that the following theorem shows that we can achieve the performance of the oracle algorithm through tree networks by disclosing local estimates. Theorem 3.1: Consider the distributed estimation frame over a tree network. Then the distributed MSE estimator (3) could also be achieved by ourselves."}, {"heading": "3.2. OEDOL Algorithm", "text": "In order to extract the new information, we must eliminate the previously obtained information about the neighboring agents at any time. This leads to additional computational complexity. On the contrary, agents can simply disclose the new information (i.e., z-i, t, after removing all previously exchanged information. Since we condition the linear combinations of conditioned variables without causing their space, i.e., z-i, t, the new information is not compatible with x-i, z-t, but vice versa, we can still achieve MSE performance by reduced computational load, yet.At time t, agent i observes yi, t and receives z-i, t = col {z-j1, z-j1, z-j1, z-j1, z-j1)."}, {"heading": "3.3. Tree Networks Involving Cell Structures", "text": "We define a \"cell structure\" as a sub-network in which all agents are connected to each other. \u2022 Intuitively, we consider a cell structure as a \"single\" agent, we can include the cell (i.e. all agents in the cell) in the tree, so that we can still achieve distributed MMSE performance by disclosing local estimates (although we may have loops in the cell). \u2022 A cell structure consists of at least 2 agents. \u2022 An agent can belong to more than one cell. \u2022 Two different agents cannot belong to more than one cell. \u2022 All agents belong to at least one cell in a connected network. \u2022 Each agent also has knowledge of the cells of the other agents. \u2022 Each agent identifies its cells from its own and its first order."}, {"heading": "4. Sub-optimal Approaches", "text": "Minimizing the cost (2) requires relatively excessive calculations. We aim to mitigate the problem suboptimally (1) and at the same time achieve a performance comparable to the optimum case. \u2212 \u2212 \u2212 Then we can estimate the cost measurement (2) using time windows as follows (2): (1): (1): (1): (1): (1): (1): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): (2): 3): (2): 3: (2): 3: (2: 3): (4: 4:) (4: (4:) (4) (4: (4:) (4) (4) (4: (4:) (4) (4) (4) (4): (4) (4): (4) (4): (4): (4: 4: 4): (4) (4: 4) (4): (4): (4): (4: 4): (4) (4) (4: (4): (4): (4: 4): (4): (4: (4): (4): (4: (4)) (4: (4: 4) (4: (4)) (4: (4): (4: 4): (4: (4)) (4: (4: (4)) (4: (4: (4)): (4: (4: (4))) (4: (4: (4: (4))) (4: (4: (4: (4))): (4: 4: (4: (4))) (4: 4: (4: (4))) (4: (4: 4: (4: (4))) (4: (4: (4: (4)))) (4: (4: (4: (4) 4: (4)"}, {"heading": "5. Illustrative Examples", "text": "In this section, we examine the performance of the presented strategies under various scenarios. Consider a network of m = 20 agents, in which each agent has an underlying state x-Rp by yi, t = hTi x + ni, t =. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S \".\" S \".\" S \".\" S \".\" S \".\" S \".\" S \".\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S. \"S.\" S \"S.\" S \"S\" S \"S.\" S \"S.\" S. \"S.\" S. \"S.\" S \"S.\" S \"S\" S. \"S.\" S. \"S\" S. \"S\" S. \"S.\" S. \"S\" S. \"S\" S. \"S\" S. \"S\" S \"S.\" S \"S.\" S \"S.\" S. \"S\" S. \"S.\" S. \"S\" S. \"S\" S. \"S\" S. \"S\" S. \"S\" S. \"S\" S \"S.\" S \"S\" S. \"S\" S \"S.\" S \"S\" S. \"S\" S \"S\" S \"S.\" S \"S\" S. \"S\" S \"S\" S. \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S. S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S.\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S \"S\" S"}, {"heading": "6. Conclusion", "text": "The distributed algorithms have attracted considerable attention due to their widespread applicability to highly complex structures ranging from biological systems to social and economic networks. However, there are still challenges for the disclosure and use of information between agents. We are introducing the ODOL algorithm that achieves MMSE performance for Gaussian state and noise statistics and LMMSE performance for arbitrary statistical profiles. The ODOL algorithm has consensual iterations for streaming data over any network topology via the aggregation of information to each agent and not the dissemination of information through the disclosure of local estimates. It is important that the disclosure of the local estimate is sufficient only via certain network topologies, e.g. via the introduced tree network, which includes cell structures. Furthermore, the aggregation of the information-based approach may require an excessive communication load. Therefore, in order to reduce the communication load, we are using the OEDL to achieve optimal performance through the combination of tree-based MSE, as well as the OEDL to the combination of the local MSE."}, {"heading": "7. Acknowledgment", "text": "This work is partly supported by the Outstanding Researcher Programme of the Turkish Academy of Sciences and the TUBITAK projects, contract numbers: 112E161 and 113E517."}], "references": [{"title": "A", "author": ["M.K. Banavar", "J.J. Zhang", "B. Chakraborty", "H. Kwon", "Y. Li", "H. Jiang", "A. Spanias", "C. Tepedelenlioglu", "C. Chakrabarti"], "venue": "P.-Suppappola, An overview of recent advances on distributed and agile sensing algorithms and implementation, Digital Signal Processing 39 ", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Diffusion strategies for adaptation and learning over networks: An examination of distributed strategies and network behavior", "author": ["A.H. Sayed", "S.-Y. Tu", "J. Chen", "X. Zhao", "Z.J. Towfic"], "venue": "IEEE Signal Processing Magazine 30 (3) ", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed estimation in diffusion networks using affine least-squares combiners", "author": ["J. Fernandez-Bes", "L.A. Azpicueta-Ruiz", "J. Arenas-Garcia", "M.T.M. Silva"], "venue": "Digital Signal Processing 36 ", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Decentralized activation in sensor networks-global games and adaptive filtering games", "author": ["V. Krishnamurthy"], "venue": "Digital Signal Processing 21 (5) ", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2011}, {"title": "Non-Bayesian social learning", "author": ["A. Jadbabaie", "P. Molavi", "A. Sandroni", "A. Tahbaz-Salehi"], "venue": "Games and Economic Behavior 76 (1) ", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Dynamic games and applications", "author": ["D. Acemoglu", "A. Ozdaglar"], "venue": "Opinion dynamics and learning in social networks 1 (1) ", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Social and Economic Networks", "author": ["M. Jackson"], "venue": "Princeton University Press, Princeton, N.J.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion least-mean squares over adaptive networks: Formulation and performance analysis", "author": ["C.G. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 56 (7) ", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion strategies for distributed Kalman filtering and smoothing", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "IEEE Transactions on Automatic Control 55 (9) ", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Online learning of dynamic parameters in social networks", "author": ["S. Shahrampour", "A. Rakhlin", "A. Jadbabaie"], "venue": "in: Neural Information Processing Systems", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed LMS for consensus-based in-network adaptive processing", "author": ["I. Schizas", "G. Mateos", "G. Giannakis"], "venue": "IEEE Transactions on Signal Processing 57 (6) ", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Compressive diffusion strategies over distributed networks for reduced communication load", "author": ["M.O. Sayin", "S.S. Kozat"], "venue": "IEEE Transactions on Signal Processing 62 (20) ", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2014}, {"title": "Chemical Kinetics and Dynamics", "author": ["J.I. Steinfeld", "J.S. Francisco", "W.L. Hase"], "venue": "Pearson", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "Single bit and reduced dimension diffusion strategies over distributed networks", "author": ["M.O. Sayin", "S.S. Kozat"], "venue": "IEEE Signal Processing Letters 20 (10) ", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed kalman filtering for sensor networks", "author": ["R. Olfati-Saber"], "venue": "in: Proceedings of 46th IEEE Conference on Decision and Control", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2007}, {"title": "SOI: Distributed Kalman filtering with low-cost communications using the sign of innovations", "author": ["A. Ribeiro", "G.B. Giannakis", "S.I. Roumeliotis"], "venue": "IEEE Transactions on Signal Processing 54 (12) ", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2006}, {"title": "Distributed Kalman filtering: a bibliographic review", "author": ["M.S. Mahmoud", "H.M. Khalid"], "venue": "IET Control Theory and Applications 7 (4) ", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Optimal decentralized Kalman filter and Lainiotis filter", "author": ["N. Assimakis", "M. Adam", "M. Koziri", "S. Voliotis", "K. Asimakis"], "venue": "Digital Signal Processing 23 (1) ", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2013}, {"title": "Agreeing to disagree", "author": ["R.J. Aumann"], "venue": "The Annals of Statistics 4 (6) ", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1976}, {"title": "Convergence in multiagent coordination", "author": ["V.D. Blondel", "J.M. Hendrickx", "A. Olshevsky", "J.N. Tsitsiklis"], "venue": "consensus, and flocking, in: Proceedings of 44th IEEE Conference on Decision and Control (CDC-ECC)", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2005}, {"title": "Locally constructed algorithms for distributed computations in ad-hoc networks", "author": ["D.S. Scherber", "H.C. Papadopoulos"], "venue": "in: Proceedings of Information Processing Sensor Networks (IPSN)", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2004}, {"title": "Equation of state calculations by fast computing machines", "author": ["N. Metropolis", "A.W. Rosenbluth", "M.N. Rosenbluth", "A.H. Teller", "E. Teller"], "venue": "The Journal of Chemical Physics 21 (6) ", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1953}, {"title": "Fundamentals of Adaptive Filtering", "author": ["A.H. Sayed"], "venue": "Wiley, New York", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2003}, {"title": "Reaching a consensus", "author": ["M.H. DeGroot"], "venue": "Journal of the American Statistical Association 69 (345) ", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1974}, {"title": "Optimal Filtering", "author": ["B.D.O. Anderson", "J.B. Moore"], "venue": "Prentice-Hall, Inc., Englewood Cliffs, NJ", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1979}, {"title": "Implementing Discrete Mathematics: Combinatorics and Graph Theory with Mathematica", "author": ["S. Skiena"], "venue": "Addison- Wesley, Reading, MA", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1990}, {"title": "Spanning Trees and Optimization Problems (Discrete Mathematics and Its Applications)", "author": ["B.Y. Wu", "K.-M. Chao"], "venue": "CRC Press, New York", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "A distributed algorithm for minimum-weight spanning trees", "author": ["R.G. Gallager", "P.A. Humblet", "P.M. Spira"], "venue": "ACM Transactions on Programming Languages and Systems 5 (1) ", "citeRegEx": "28", "shortCiteRegEx": null, "year": 1983}, {"title": "Distributed Computing: A Locality-Sensitive Approach", "author": ["D. Peleg"], "venue": "SIAM, Philadelphia, PA", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2000}, {"title": "Unconditional lower bounds on the time-approximation tradeoffs for the distributed minimum spanning tree problem", "author": ["M. Elkin"], "venue": "in: Proceedings of 36th ACM Symposium on Theory of Computing (STOC)", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2004}, {"title": "Distributed algorithms for constructing approximate minimum spanning trees in wireless sensor networks", "author": ["M. Khan", "G. Pandurangan", "V.S.A. Kumar"], "venue": "IEEE Transactions on Parallel and Distributed Systems 20 (1) ", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2009}, {"title": "Optimal combination rules for adaptation and learning over networks", "author": ["J. Chen", "A.H. Sayed"], "venue": "in: Proceedings of 50th Annual Allerton Conference on Communication, Control, and Computing (Allerton)", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2012}, {"title": "Diffusion recursive least-squares for distributed estimation over adaptive networks", "author": ["F.S. Cattivelli", "C.G. Lopes", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 56 (5) ", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Diffusion strategies outperform consensus strategies for distributed estimation over adaptive networks", "author": ["S.Y. Tu", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 60 (12) ", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Diffusion LMS for distributed estimation over adaptive networks", "author": ["F.S. Cattivelli", "A.H. Sayed"], "venue": "IEEE Transactions on Signal Processing 58 (3) ", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2010}, {"title": "Optimal combination rules for adaptation and learning over networks", "author": ["S.-Y. Tu", "A.H. Sayed"], "venue": "in: Proceedings of 4th IEEE International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP)", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2011}, {"title": "Consensus problems in networks of agents with switching topology and time-delays", "author": ["R. Olfati-Saber", "R.M. Murray"], "venue": "IEEE Transactions on Automatic Control 49 (9) ", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2004}], "referenceMentions": [{"referenceID": 0, "context": ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].", "startOffset": 114, "endOffset": 123}, {"referenceID": 1, "context": ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].", "startOffset": 114, "endOffset": 123}, {"referenceID": 2, "context": ", fast response time, relative to the centralized networks by distributing the processing power over the networks [1, 2, 3].", "startOffset": 114, "endOffset": 123}, {"referenceID": 3, "context": "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].", "startOffset": 131, "endOffset": 143}, {"referenceID": 4, "context": "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].", "startOffset": 131, "endOffset": 143}, {"referenceID": 5, "context": "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].", "startOffset": 131, "endOffset": 143}, {"referenceID": 6, "context": "This framework is conveniently used to model highly complex structures from defense applications to social and economical networks [4, 5, 6, 7].", "startOffset": 131, "endOffset": 143}, {"referenceID": 1, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 7, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 8, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 9, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 4, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 5, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 6, "context": ", [2, 8, 9, 10, 5, 6, 7] and references therein, we still have significant and yet unexplored problems for disclosure and utilization of information among agents.", "startOffset": 2, "endOffset": 24}, {"referenceID": 1, "context": ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.", "startOffset": 53, "endOffset": 67}, {"referenceID": 7, "context": ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.", "startOffset": 53, "endOffset": 67}, {"referenceID": 10, "context": ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.", "startOffset": 53, "endOffset": 67}, {"referenceID": 11, "context": ", diffusion or consensus based estimation algorithms [2, 8, 11, 12], due to processing power related practical concerns.", "startOffset": 53, "endOffset": 67}, {"referenceID": 12, "context": "In chemical kinetics, this phenomena is called as the rate determining step (RDS) such that speed of a chain of reactions can be approximately determined by the speed of the slowest reaction [13].", "startOffset": 191, "endOffset": 195}, {"referenceID": 3, "context": "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].", "startOffset": 157, "endOffset": 168}, {"referenceID": 13, "context": "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].", "startOffset": 157, "endOffset": 168}, {"referenceID": 11, "context": "To this end, in the design of distributed networks, we should also consider the trade-off in terms of the communication power and the estimation performance [4, 14, 12].", "startOffset": 157, "endOffset": 168}, {"referenceID": 13, "context": "In [14, 12], the authors study this problem by compressing the data before the transmission and extracting the compressed information adaptively.", "startOffset": 3, "endOffset": 11}, {"referenceID": 11, "context": "In [14, 12], the authors study this problem by compressing the data before the transmission and extracting the compressed information adaptively.", "startOffset": 3, "endOffset": 11}, {"referenceID": 14, "context": "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].", "startOffset": 90, "endOffset": 106}, {"referenceID": 15, "context": "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].", "startOffset": 90, "endOffset": 106}, {"referenceID": 16, "context": "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].", "startOffset": 90, "endOffset": 106}, {"referenceID": 17, "context": "Correspondingly, there have been abundant attempts to formulate distributed Kalman filter [15, 16, 17, 18].", "startOffset": 90, "endOffset": 106}, {"referenceID": 18, "context": "As explained in [19], once agent-1 transmits an information to agent-2, this information becomes common knowledge between the agents.", "startOffset": 16, "endOffset": 20}, {"referenceID": 19, "context": ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].", "startOffset": 19, "endOffset": 23}, {"referenceID": 20, "context": ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": ", the uniform rule [20], the Laplacian rule [21] or the Metropolis rule [22].", "startOffset": 72, "endOffset": 76}, {"referenceID": 1, "context": ", each agent observes diverse signal-to-noise ratios, by ignoring the variation in noise, these rules yield severe decline in the estimation performance [2].", "startOffset": 153, "endOffset": 156}, {"referenceID": 1, "context": "In such cases the agents can even perform better without cooperation [2].", "startOffset": 69, "endOffset": 72}, {"referenceID": 1, "context": "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].", "startOffset": 221, "endOffset": 231}, {"referenceID": 7, "context": "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].", "startOffset": 221, "endOffset": 231}, {"referenceID": 10, "context": "Notably, the ODOL algorithm utilizes the aggregation of information at each agent via time-stamped information exchange and is different from the conventional algorithms that benefit from the diffusion of local estimates [2, 8, 11].", "startOffset": 221, "endOffset": 231}, {"referenceID": 22, "context": "We assume that the variance of the noise signals are known, which can also be readily estimated from the data [23].", "startOffset": 110, "endOffset": 114}, {"referenceID": 22, "context": "Naturally, the agents can learn the true state, under certain regularity conditions [23], irrespective of cooperation among them, provided that their own measurements are not biased and sufficient to estimate the true state in time.", "startOffset": 84, "endOffset": 88}, {"referenceID": 1, "context": "However, through the cooperation of agents, we can increase the learning rate significantly [2].", "startOffset": 92, "endOffset": 95}, {"referenceID": 11, "context": "Since the communication load is crucial for the applicability of the distributed learning algorithms [12, 14], we also seek to reduce the communication load yet achieve the MMSE performance.", "startOffset": 101, "endOffset": 109}, {"referenceID": 13, "context": "Since the communication load is crucial for the applicability of the distributed learning algorithms [12, 14], we also seek to reduce the communication load yet achieve the MMSE performance.", "startOffset": 101, "endOffset": 109}, {"referenceID": 1, "context": "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].", "startOffset": 142, "endOffset": 156}, {"referenceID": 23, "context": "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].", "startOffset": 142, "endOffset": 156}, {"referenceID": 10, "context": "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].", "startOffset": 142, "endOffset": 156}, {"referenceID": 7, "context": "In the consensus and diffusion implementations, the agents disclose the local estimates, which requires relatively reduced communication load [2, 24, 11, 8].", "startOffset": 142, "endOffset": 156}, {"referenceID": 24, "context": "2If the inverse fails to exist, a pseudo inverse can replace the inverse [25].", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": ", connected and undirected without any cycles [26].", "startOffset": 46, "endOffset": 50}, {"referenceID": 26, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 27, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 28, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 29, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 30, "context": "In the literature, there exists numerous distributed algorithms for minimum spanning tree construction [27, 28, 29, 30, 31].", "startOffset": 103, "endOffset": 123}, {"referenceID": 31, "context": "Similarly, in [32, 33, 34], the authors show that the performance of the diffusion based algorithms could approach the performance of a fully connected network under certain regularity conditions.", "startOffset": 14, "endOffset": 26}, {"referenceID": 8, "context": "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].", "startOffset": 119, "endOffset": 122}, {"referenceID": 7, "context": "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].", "startOffset": 160, "endOffset": 163}, {"referenceID": 32, "context": "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].", "startOffset": 207, "endOffset": 211}, {"referenceID": 33, "context": "We compare the performance of the introduced algorithms with the diffusion implementation of Kalman updates (D-Kalman) [9], diffusion least mean square (D-LMS) [8], diffusion recursive least squares (D-RLS) [35], and consensus algorithms [36].", "startOffset": 238, "endOffset": 242}, {"referenceID": 19, "context": "In the uniform combination rule [20], the combination weights given by agent i for neighbor j are chosen as \u03bbi, j = { 1/(\u03c0i + 1) if j \u2208 Ni \u222a {i} 0 otherwise since Ni excludes i.", "startOffset": 32, "endOffset": 36}, {"referenceID": 34, "context": "Correspondingly, in the relative variance rule [37, 38], \u03bbi, j is given by", "startOffset": 47, "endOffset": 55}, {"referenceID": 35, "context": "Correspondingly, in the relative variance rule [37, 38], \u03bbi, j is given by", "startOffset": 47, "endOffset": 55}, {"referenceID": 36, "context": "In the D-RLS algorithm, we use the Laplacian combination rule [39] for the incremental update and the Metropolis combination rule for the spatial update.", "startOffset": 62, "endOffset": 66}, {"referenceID": 26, "context": ", i = arg max j \u03c0 j, and then we eliminate the multipaths [27].", "startOffset": 58, "endOffset": 62}], "year": 2017, "abstractText": "We construct optimal estimation algorithms over distributed networks for state estimation in the mean-square error (MSE) sense. Here, we have a distributed collection of agents with processing and cooperation capabilities. These agents continually observe a noisy version of a desired state of the nature through a linear model and seek to learn this state by interacting with each other. Although this problem has attracted significant attention and extensively been studied in several different fields including machine learning theory to signal processing, all the well-known strategies achieve suboptimal learning performance in the MSE sense. To this end, we provide algorithms that achieve distributed minimum MSE (MMSE) performance over an arbitrary network topology based on the aggregation of information at each agent. This approach differs from the diffusion of information across network, i.e., exchange of local estimates per time instance. Importantly, we show that exchange of local estimates is sufficient only over the certain network topologies. By inspecting these network structures, we also propose strategies that achieve the distributed MMSE performance also through the diffusion of information such that we can substantially reduce the communication load while achieving the best possible MSE performance. For practical implementations we provide approaches to reduce the complexity of the algorithms through the time-windowing of the observations. Finally, in the numerical examples, we demonstrate the superior performance of the introduced algorithms in the MSE sense due to optimal estimation.", "creator": "LaTeX with hyperref package"}}}