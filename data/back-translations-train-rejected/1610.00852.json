{"id": "1610.00852", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Ensemble Maximum Entropy Classification and Linear Regression for Author Age Prediction", "abstract": "The evolution of the internet has created an abundance of unstructured data on the web, a significant part of which is textual. The task of author profiling seeks to find the demographics of people solely from their linguistic and content-based features in text. The ability to describe traits of authors clearly has applications in fields such as security and forensics, as well as marketing. Instead of seeing age as just a classification problem, we also frame age as a regression one, but use an ensemble chain method that incorporates the power of both classification and regression to learn the authors exact age.", "histories": [["v1", "Tue, 4 Oct 2016 06:01:03 GMT  (187kb)", "http://arxiv.org/abs/1610.00852v1", "6 pages, 4 figures"]], "COMMENTS": "6 pages, 4 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CL", "authors": ["joey hong", "chris mattmann", "paul ramirez"], "accepted": false, "id": "1610.00852"}, "pdf": {"name": "1610.00852.pdf", "metadata": {"source": "CRF", "title": "Ensemble Maximum Entropy Classification and Linear Regression for Author Age Prediction", "authors": ["Joey Hong", "Chris Mattmann", "Paul Ramirez"], "emails": ["jhhong@caltech.edu"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.00 852v 1 [cs.L G] 4O ct2 01Keywords Author Profiling, age prediction, maximum entropy, regression, processing natural language"}, {"heading": "1. INTRODUCTION", "text": "The rapid expansion of the web is associated with an astounding increase in communication between members of the world via blogs, e-mails, forums and social media platforms. Thus, it is of increasing interest to analyze the authorship of Internet content, and to be able to use this massive data to predict characteristics. In the task of the author profile, such characteristics can vary along the dimensions of age, gender and personality type, although we focus on age, which is the most difficult. Able to do so with high predictive power, these will range from law enforcement to focal advertising. Thus, such technology could be used to evaluate the veracity of the self-mentioned."}, {"heading": "2. RELATED WORK", "text": "In selecting feature films, Houvardas and Stamatos showed in 2006 the extraction of N-grams as an effective feature for classification in the author profile area [8]. In 2007, Estival et al. experimented with linguistic feature selection in author profiling and discovered POS frequencies as a practicable feature [6]. Calix et al. conducted author comparisons from e-mails, using 55 different stylistic features, along with a K-next-door implementation, to achieve an accuracy of 76.72% [3]. Specifically in age determination, Pennebaker and Stone LIWC used to find a relationship between language usage as a person in age [14]. However, the problem was modeled almost exclusively as a classification problem, with a variety of machine learning methods ranging from Bayesian methods to support for vector lines being accurate in 2011."}, {"heading": "3. MAXIMUM ENTROPY CLASSIFIERS", "text": "We will derive the basics of the MaxEnt model in NLP (c, d), as well as the generalized iterative scaling algorithm used by Apache OpenNLP to solve the model. We will leave p (c, d) the empirical probability of a training sample (c, d) with document vector d and category c and N the size of the training set. We will also introduce a function for each feature fi, j in our document vector representation so that: fi, j (c, d) = N (wi), l N (wl), if c = cj, and d contains, c c, otherwise.d Where cj denotes the possible categories, and wi the possible context tokens."}, {"heading": "3.1 Generalized Iterative Scaling", "text": "The determination of the optimal \u03bb parameters can easily be done with an iterative scaling algorithm. OpenNLP uses the simple GIS algorithm to do this. It guarantees that no local maxima exist in the probability interface, so hillclimbing algorithms will find the global maximum entropy solution from each random initial distribution. We use the log probability as in Della Pietra et al. in 1997 [2]. Let \u03bb be the parameter vector of all \u03bbi, j values and p\u03bb be the resulting probability model. The log probability is specified by, (\u03bb) = log probability (c | d) = log value i, j\u03bbi, jfi, j (c, j sum) \u2212 dlog value cexp: i, jxi value, jxi value, jfi, jfi, jfi, jfi, j (c, d). As in any hillclimbing algorithm, we strive for the value, jc, jxi value, jfi-xi value, jfi-xi value, jfi-xi-xi value (jp)."}, {"heading": "4. DATA", "text": "We use marked data from four important datasets, divided into a training and test set via a 90-10 split. Figure 1 outlines the distribution of the three datasets with exact ages, and Table 1 shows the distribution of the PAN16 dataset with only categorized age groups. Posts were not counted for the first three corporas because all posts from a single user have been aggregated and can vary greatly in the length of posts, while the PAN16 dataset, which contains only Twitter data, has relatively consistent postal lengths."}, {"heading": "4.1 Blog Authorship Corpus", "text": "The corpus is the result of crawled blogs (blogger.com) by Schler et al. in 2004. [1] The corpus consists of posts by 19,320 bloggers and 681,288 posts, tagged with Blogger ID and self-stated gender and age, as well as industry and / or astrological signs when available. Blogs are formatted in XML format, and we extracted all the text from a user's files and aggregated it into a single flagged post."}, {"heading": "4.2 Fisher English Transcripts", "text": "The Fisher CALLHOME dataset contains transcripts of telephone conversations between random pairs of people, in which a truth file was provided with characteristics such as the age and gender of each speaker. [5] Each phone call concerned a predetermined topic that differed greatly from all recorded conversations. We analyzed the truth file and used a script to aggregate the entire text between the transcripts belonging to the individual into a single sample."}, {"heading": "4.3 Cancer Forum", "text": "Contributions and user profiles of an online forum for people with breast cancer (community.breastcancer.org) were searched in 2011 by Nguyen et al. [10]. Users \"ages were determined either by context in their posts or by manually commenting on their profiles. Contributions from a total of 1,996 users were recorded. Again, we aggregated all contributions from an individual user into one tagged post per user."}, {"heading": "4.4 PAN Dataset", "text": "The PAN16 dataset consists of tweets from Twitter users and an annotated truth set on age and gender. The Twitter corpus was compiled by looking at LinkedIn profiles and finding those with a corresponding Twitter account. Age was determined either by a date of birth given in the profile or by an estimate with the start date of the education degree [15]. Due to the Twitter Terms of Use, only the URLs for the tweets of each profile were provided, and a script was used to download the tweets, and another was written to match downloaded tweets with the truth set provided. Age is marked by categories: 18-24 | 25-34 | 35-49 | 50- 64 | 65-x. For this reason, the PAN16 dataset was only used in the training data for our classification task."}, {"heading": "5. FEATURES", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Content-based Features", "text": "It can be assumed that, since people of different ages talk about different topics, the use of words between the ages can be a distinguishing feature in their written text. As the content can be represented as a N-gram characteristic, we have compiled uni and bigrams from each set. For uni-grams, we have replaced all words with a list of 319 English stopwords and removed all occurrences of stopwords that do not provide meaningful content. For classification and regression, we have calculated the frequency of occurrence in the document for each of the characteristics, and used p-normalized counting in the L1 space when creating a vector for each document."}, {"heading": "5.2 Stylistic Features", "text": "s POS tagger for a POS countdown. We also counted the occurrence of words belonging to LIWC word classes [13], including personal words (\"I,\" \"I,\" \"me,\" \"me,\" \"me,\" etc.), positive (\"cheerful,\" \"bliss,\" \"joy,\" etc.) and negative (\"abandoned,\" \"bad,\" \"hurt,\" etc.), mood words and quantifying words (\"many,\" \"few,\" \"both,\" etc.). Overall, word lists for 9 word classes were compiled manually, whereas originally all words in the document used lemmatibility and stammerging to maintain styling functions so that they were not based on the content."}, {"heading": "5.3 Normalization", "text": "In order to reduce the number of features and thus keep the training time of the model within a practicable range, we implemented a Chi-Squared Selector that works alongside the OpenNLP data indexer and transforms the input functions into a reduced feature pace. We used a one-on-many transformation in which the number of features for each category was calculated relative to all other age categories."}, {"heading": "6. ARCHITECTURE", "text": "Figure 2 outlines the pipeline of the algorithm, which we will describe in detail. First, the text data from all data sources is aggregated by the user. As most data sets have been publicly published, no further external work was required except for a reformatting that meets the input requirements of our classifier and regression trainer."}, {"heading": "6.1 Data Preprocessing", "text": "As I said, most records did not require significant cleaning. However, the Twitter data from PAN16 was raw XML text and required cleaning labels and removal of non-encoded words. We used regex filters to replace all outlinks with \"Urllink,\" and removed all hashtags and mentions from the data. The cleaned data can then be formatted into the same input requests. Since the compiled records had a bias toward people in the 1920s, we used oversampling to duplicate all posts from rare age groups and correct bias."}, {"heading": "6.2 Feature Selection", "text": "To extract stylistic features from the formatted data, we used Apache OpenNLP's POS tagging and sentence recognition tools. Our custom tokenizer separated by both spaces and punctuation, as well as by sentence boundaries. The tokenized text was indexed into a counter vector to extract features such as the number of sentences in the document, words in the sentence, and punctuation classes. Using the tokenized text, unigrams and bigrams were extracted. Unigrams were fed into a stopword filter, where all tokens that match with stopwords in a preset list were removed. We also counted the occurrence of unigram words in each of our manually created LIWC word classes, and calculated frequency distribution. We then piped the tokens into the POS tagger to generate POS unigrams and bigrams, and a frequency was retrieved with the tags."}, {"heading": "6.3 Classification", "text": "For classification, all features were used, but filtered using statistical feature selection methods. We only selected features that appeared at least 10 times in the training materials. Our normalized document vectors were also subjected to a chi-square feature selection mechanism, so only features with 90% reliability were selected. We used Apache OpenNLP's Generalized Iterative Scaling method to solve the parameters for our MaxEnt classification model."}, {"heading": "6.4 Regression", "text": "We did not use text bigrams for the regression, because the linear regression with a large number of features was not well scaled. As the PAN16 tweets were labeled with age group only, we omitted the sentence from the training data.We passed the age categories as another feature in our regression model trainer. Only features that exist in at least 5 different documents were selected. For regression, we use the L1 regularization to train a model with LASSO. To optimize our feature weights, but be economical. To match the regression parameters, we tested several learning rates and regulation values (0.5, 0.25, 0.125, 0.0625, 0.03125, 0.015625).To perform our concatenated author age prediction, we can use our classification model, which first evaluates the top age category and then inserts the category attribute into the document vector of our ASSO model."}, {"heading": "7. EXPERIMENT", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7.1 Classification", "text": "We experimented with different feature vectors with the MaxEnt classifier, using the desired feature generators that were enabled in our implementation. We obtained the following models: 1. UNIGRAM: model trains on the two training corpus with only unigrams; 2. NGRAM: model trains on common training corpus with unigrams and stylistic characteristics (set, number of words, punctuation, POS, LIWC, etc.); 4. GLOBAL: model trains on common training corpus with all the above features. We measured the accuracy of each individual model on a common test set. We also calculated precision, memory and F1 score for the most powerful model among all age categories. As mentioned above, we performed a 90-10 split on the entire training corpus to create the training and test data sets."}, {"heading": "7.2 Regression", "text": "We have the following two models for regression: 1. DEFAULT: model trained without entering the result of the classification; 2. ENSEMBLE: model trained with concatenated method. We used the GLOBAL classifier in the compilation. We measured the mean absolute error as well as the Pearson correlation coefficient for each model and created a report on predicted vs. expected ages, which is displayed in a scatter plot."}, {"heading": "8. RESULTS AND DISCUSSIONS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Classification", "text": "As expected, the GLOBAL model performed best, albeit marginally, with all the features in Table 2. Only the selection of unigrams and bigrams, as in the NGRAM model, yielded almost as good predictive accuracy. Although the accuracy is not very high, all models performed better than the baseline of 0.33, which is calculated by always predicting the most common category, namely xx-17, based on the frequency of age groups among the test data. Table 3 highlights the fine-grained report for the most powerful GLOBAL model. It is clear that the model is progressively performing worse as the age group increases, most likely due to the lack of training data in these ages.For the younger age groups, the performance of GLOBAL was generally good. However, for the 18-24 year olds, many items in this group were predicted more in the xx-17 age group, which led to poor memory."}, {"heading": "8.2 Regression", "text": "Both regression models suffered from the uneven distribution of data. Although we used oversample samples to mitigate bias, both models had small coefficients for their characteristics and predicted relatively close to their respective sections. During training, the MAE remained relatively low for both models, the correlation in DEFAULT of 0.608 yielding r2 = 0.37, which means that only 37% of the variance can be explained. In ENSEMBLE, the r2 increases to 0.49. This is perhaps due to the distortion in training distribution, loud data, and the fact that most age groups are close together, especially after applying oversample data. The MAE of about 10 for ENSEMBLE means that it predicts an average of 10 years away from the actual age. While this is high, the mean is distorted by the huge underpredictions for age groups over 50. It may be that language usage does not change much after this age, although the reason for our lack of older training data is more plausible."}, {"heading": "9. CONCLUSION AND FUTURE WORK", "text": "Our dataset used a collection of annotated corpora associated with forum posts, phone conversations, blogs and tweets in which data from older age groups were duplicated to reduce prejudice. We approached the problem of age prediction both through classification, where age was divided into age groups, and regression, where age is treated as a continuous variable. In classification, we obtained predictive accuracy of at best 52%, about 20% above baseline, by training a MaxEnt classifier with both content and stylistic characteristics. However, precision and recall decreased dramatically for older age groups. For the future, we can set better boundaries for users, perhaps through significant life phases rather than arbitrary 10-year age groups. Even instead of normalized frequencies, we can experiment with tf-idf weights for characteristics."}, {"heading": "10. REFERENCES", "text": "[1] S. Argamon, M. Koppel, J. W. Pennebaker, andJ. Rose Pacific. Workshop [Effects of age and gender on blogging. 2007. [2] A. L. Berger, S. A. D. Pietra, and V. J. D. Pietra. A maximum entropy approach to natural language processing. COMPUTATIONAL LINGUISTICS, 22: 39-71, 1996. [3] K. Calix, M. Connors, D. Levy, H. Manzar, G. Mccabe, and S. Westcott for e-mail author identification and authentication. In Day, Pace Univ, 2008. [4] J. R. Curran and S. Clark. Investigating gis and smoothing for maximum entropy taggers. pages 91-98, 2003. [5] C. David, D. Miller, and K. Walker. The fisher corpus: a resource for the next generations of speech-to-text."}], "references": [{"title": "Effects of age and gender on blogging", "author": ["S. Argamon", "M. Koppel", "J.W. Pennebaker", "J. Schler"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2007}, {"title": "A maximum entropy approach to natural language processing", "author": ["A.L. Berger", "S.A.D. Pietra", "V.J.D. Pietra"], "venue": "COMPUTATIONAL LINGUISTICS,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "Stylometry for e-mail author identification and authentication", "author": ["K. Calix", "M. Connors", "D. Levy", "H. Manzar", "G. Mccabe", "S. Westcott"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Investigating gis and smoothing for maximum entropy taggers", "author": ["J.R. Curran", "S. Clark"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2003}, {"title": "The fisher corpus: a resource for the next generations of speech-to-text", "author": ["C.C. David", "D. Miller", "K. Walker"], "venue": "In in Proceedings 4th International Conference on Language Resources and Evaluation,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2004}, {"title": "Author profiling for english emails", "author": ["D. Estival", "T. Gaustad", "B. Hutchinson", "S.B. Pham", "W. Radford"], "venue": "Proceedings of the 10th Conference of the Pacific Association for Computational Linguistics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2007}, {"title": "Stylometric analysis of bloggers", "author": ["S. Goswami", "S. Sarkar", "M. Rustagi"], "venue": "age and gender,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2009}, {"title": "N-Gram Feature Selection for Authorship Identification, pages 77\u201386", "author": ["J. Houvardas", "E. Stamatatos"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2006}, {"title": "how old do you think i am?\u201d a study of language and age in twitter", "author": ["D. Nguyen", "R. Gravel", "D. Trieschnigg", "T. Meder"], "venue": "In ICWSM,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2013}, {"title": "Author age prediction from text using linear regression", "author": ["D. Nguyen", "N.A. Smith", "C.P. Ros\u00e9"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Using maximum entropy for text classification", "author": ["K. Nigam"], "venue": "Workshop on Machine Learning for Information Filtering,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1999}, {"title": "Thumbs up?: Sentiment classification using machine learning techniques", "author": ["B. Pang", "L. Lee", "S. Vaithyanathan"], "venue": "In Proceedings of the ACL-02 Conference on Empirical Methods in Natural Language Processing - Volume 10,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Linguistic inquiry and word count (liwc): A computerized text analysis program", "author": ["J.W. Pennebaker", "R.J. Booth", "M.E. Francis"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2001}, {"title": "Words of wisdom: Language use over the life span", "author": ["J.W. Pennebaker", "L.D. Stone"], "venue": "Journal of Personality and Social Psychology, 85(2):291\u2013301,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2003}, {"title": "Overview of the 4th author profiling task at pan 2016: Cross-genre evaluations", "author": ["F. Rangel", "P. Rosso", "B. Verhoeven", "W. Daelemans", "M. Potthast", "B. Stein"], "venue": "In Working Notes Papers of the CLEF 2016 Evaluation Labs. CEUR Workshop Proceedings,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}], "referenceMentions": [{"referenceID": 14, "context": "In the task of author profiling, such characteristics can vary along dimensions of age, gender, and personality type, though we will focus on age, which has conventionally been the most difficult [15].", "startOffset": 196, "endOffset": 200}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "In feature selection, Houvardas and Stamatatos in 2006 showed extracting N-grams as an effective feature for classifying in the domain of author profiling [8].", "startOffset": 155, "endOffset": 158}, {"referenceID": 5, "context": "experimented with linguistic feature selection in author profiling, and revealed POS frequencies as a viable feature [6].", "startOffset": 117, "endOffset": 120}, {"referenceID": 2, "context": "72% accuracy [3].", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "Specifically on age identification, Pennebaker and Stone used LIWC to find a relation between the use of language as a person ages [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 9, "context": "were the first to treat age as a continuous variable via linear regression, and experimented with regression on various individual corpora [10].", "startOffset": 139, "endOffset": 143}, {"referenceID": 1, "context": "If we set up and solve the Lagrangian [2], we get the multiclass version of the sigmoid function in logistic regression as our optimal solution,", "startOffset": 38, "endOffset": 41}, {"referenceID": 1, "context": "in 1997 [2].", "startOffset": 8, "endOffset": 11}, {"referenceID": 10, "context": "From Jensen\u2019s inequality, we can lower bound the difference by an auxiliary function B [11],", "startOffset": 87, "endOffset": 91}, {"referenceID": 0, "context": "in 2004 [1].", "startOffset": 8, "endOffset": 11}, {"referenceID": 4, "context": "The Fisher CALLHOME dataset contains transcripts of telephone conversations between random pairs of people, where a truth file was provided with characteristics such as age and gender of each speaker [5].", "startOffset": 200, "endOffset": 203}, {"referenceID": 9, "context": "in 2011 [10].", "startOffset": 8, "endOffset": 12}, {"referenceID": 14, "context": "Age was determined either by a provided birth date in the profile, or via an estimation with the degree starting date in the education section [15].", "startOffset": 143, "endOffset": 147}, {"referenceID": 12, "context": "We also counted occurrences of words that belong to LIWC word classes [13], including personal words (\u201cI\u201d,\u201cme\u201d,\u201cmine\u201d, etc.", "startOffset": 70, "endOffset": 74}], "year": 2016, "abstractText": "The evolution of the internet has created an abundance of unstructured data on the web, a significant part of which is textual. The task of author profiling seeks to find the demographics of people solely from their linguistic and contentbased features in text. The ability to describe traits of authors clearly has applications in fields such as security and forensics, as well as marketing. Instead of seeing age as just a classification problem, we also frame age as a regression one, but use an ensemble chain method that incorporates the power of both classification and regression to learn the author\u2019s exact age.", "creator": "LaTeX with hyperref package"}}}