{"id": "1610.00956", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension", "abstract": "There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children's Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.", "histories": [["v1", "Tue, 4 Oct 2016 12:48:51 GMT  (238kb,D)", "http://arxiv.org/abs/1610.00956v1", "The first two authors contributed equally to this work. Submitted to EACL 2017. Code and dataset are publicly available"]], "COMMENTS": "The first two authors contributed equally to this work. Submitted to EACL 2017. Code and dataset are publicly available", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["ondrej bajgar", "rudolf kadlec", "jan kleindienst"], "accepted": false, "id": "1610.00956"}, "pdf": {"name": "1610.00956.pdf", "metadata": {"source": "CRF", "title": "Embracing data abundance: BookTest Dataset for Reading Comprehension", "authors": ["Ondrej Bajgar", "Rudolf Kadlec", "Jan Kleindienst"], "emails": ["kadlec@cz.ibm.com", "jankle@cz.ibm.com"], "sections": [{"heading": null, "text": "We show that training with the new data improves the accuracy of our Attention Sum Reader model on the original CBT test data many times over than many current attempts to improve the model architecture. On one version of the data set, our ensemble even exceeds the human baseline provided by Facebook. In our own study, we then show that there is room for further improvement."}, {"heading": "1 Introduction", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "2 Task Description", "text": "Therefore, the task we are trying to solve is to answer a clozestyle question, the answer of which depends on understanding a context document provided with the question. Also, the model contains a number of possible answers from which the correct one should be selected.2 This can be formalized as follows: The training data consists of tuples (q, d, a, A), where q is a question, d is a document containing the answer to question q, A is a set of possible answers, and a \u00b2 A is the answer to the basic truth.Both q and d are word sequences from Vocabulary1Therefore, they are more than 16 times larger than the dataset.2To obtain an open domain question, we can provide the entire vocabulary as a list of candidates."}, {"heading": "3 Current Landscape", "text": "We will now briefly review which data sets for understanding text have been published and look at models that have recently been used to solve the problem just described."}, {"heading": "3.1 Datasets", "text": "A key prerequisite for the application of deep learning techniques is the availability of an enormous amount of data for training. In terms of answering questions, this specifically means that a large number of question-and-answer triples are available. While an unlimited amount of text is available, the creation of relevant questions and the corresponding answers can be extremely labor-intensive when performed by human commentators. Efforts have been made to provide such human-generated data sets, such as Microsoft's MCTest (Richardson et al., 2013), but their scope is not suitable for deep learning without prior training in relation to other data (Trischler et al., 2016a) (e.g. by using pre-trained word-embedding vectors). Google DeepMind has succeeded in avoiding this scale problem through its way of automatically generating question-and-answer triples, closely followed by Facebook using a similar method. Let us now briefly present the two resulting data sets, whose characteristics are summarized in Table 1."}, {"heading": "3.1.1 CNN & Daily Mail datasets", "text": "These two sets of data (Hermann et al., 2015) take advantage of a useful feature of online news articles - many articles include a short summary sentence at the top of the page. Since all the information in the summary sentence is also presented in the article body, we get a nice cloze-style question about the3Note that distinguishes this vocabulary from the model's own dictionary. Thus, the model can replace certain rare words with generic unword tags, thus reducing its own dictionary size by removing a word from the short summary. The authors of the data set have also replaced all the named units in the data set with anonymous tokens that are further shuffled for each new group, forcing the model to rely solely on information from the context document and not be able to transfer the meaning of the named entities between the documents. This limits the task to a certain aspect of the context-dependent question, which can be useful, but the answer to the real-world context of this application can continue to shift the half of the scenario where we can."}, {"heading": "3.1.2 Children\u2019s Book Test", "text": "The Children's Book Test (Hill et al., 2015) uses a different source - books that are freely available thanks to the Gutenberg4 project. Since no abstract is available, each example consists of a context document composed of 20 consecutive sentences from history and a question from the following sentence. The data set occurs in four different variations depending on which word type is omitted from the questionnaire. Based on human evaluation in (Hill et al., 2015), it appears that named entities (NEs) and common nouns (CNs) are more context-dependent than the other two types - prepositions and verbs. Therefore, we (and all recent publications) focus only on these two word types. 4https: / / www.gutenberg.org /"}, {"heading": "3.1.3 Recent additions", "text": "The LAMBADA dataset (Paperno et al., 2016) is designed to measure progress in understanding common sense questions about short stories that can be easily answered by humans but cannot be answered by current standard machine learning models (e.g., plain LSTM language models).This dataset is useful for measuring the gap between humans and machine learning algorithms. Unlike our BookTest datasets, it will not allow us to track progress in terms of baseline system performance or examples in which machine learning can demonstrate super-human performance. Also, LAMBADA is only a diagnostic dataset and does not offer ready-to-use questions, but only a text-corpus database that can include progress."}, {"heading": "3.2 Machine Learning Models", "text": "A first major work in which depth learning techniques were applied to understanding text was Hermann et al. (Hermann et al., 2015), which was followed by the application of memory networks to the same task (Hill et al., 2015). Later, three models emerged around the same time (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016), including our Attention Sum Reader (AS Reader) model (Kadlec et al., 2016). AS Reader inspired several subsequent models that use it as a subcomponent in a diverse ensemble (Trischler et al., 2016b); expand it to include a hierarchical structure (Sordoni et al., 2016; Shen et al., 2016; Dhingra et al., 2016); calculate the attention paid to the context document for each word in the query (Cui et al., 2016b) or use issided contexts for each word in the context of attention and context."}, {"heading": "3.3 Possible Directions for Improvements", "text": "Current state-of-the-art models (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016a) improve the accuracy of AS Reader's CBT-NE and CN datasets by 1-2 percent in absolute terms, suggesting that current techniques have limited scope for improvement on the algorithmic side. Simply, the other way to increase performance is to use more training data. Mercer's oft-quoted statement: \"There is no data like more data.\" 5 The observation that more data is often more important than better algorithms has been frequently emphasized since then (Banko and Brill, 2001; Halevy et al., 2009).As a step toward exploiting the potential of more data in the field of text comprehension, we have created new CBT datasets that are much more similar to the widely used CBT test kits."}, {"heading": "4 BookTest", "text": "We used 3555 non-copyrighted books to extract CN examples and 10507 books for non-ferrous examples, for comparison, the CBT data set was extracted from only 108 books. In creating our data set, we followed the same procedure that was used to create the CBT data set (Hill et al., 2015), that is, we determined whether each sentence contained either a named entity or a common noun that had already occurred in one of the previous twenty sentences, and this word was then replaced by a gap tag (XXXXX) in that sentence, which is therefore turned into a clozestyle question. The previous 20 sentences are used as context documents, for ordinary noun5Quote attributed to Robert Mercer by Fred Jelinek (Jelinek, 2004)."}, {"heading": "5 Baselines", "text": "We will now use our AS reader model to evaluate the performance gain by enlarging the data set."}, {"heading": "5.1 AS Reader", "text": "In (Kadlec et al., 2016) we introduced the AS Reader8, which at the time of publication significantly exceeded all other architectures on the CNN, DM and CBT datasets. This model was designed so that the answer is a single word from the context document. Similar to many other models, it uses attention to the document - intuitively a measure of how relevant each word is to answering the question. However, while most previous models used this attention as a weighting to calculate a mixed representation of the answer, we simply add up the attention to all occurrences of each word and then simply select the word with the highest sum as the final answer. Although this trick is simple, it seems to both improve accuracy and speed up training. It was developed by 7We use version 3.6.0 of both NER and POS taggers. For POS tagging, we used the model by default 2015-12-09 / wsj-bidirectionaltagsity.18."}, {"heading": "5.1.1 Basic structure", "text": "The words from the document and the question are first converted into vector embedding using a look-up matrix V. The document is then read by a bi-directional GRU network (Cho et al., 2014). A concatenation of the hidden states of the forward and backward GRUs on each word is then used as a contextual embedding of that word, which intuitively represents the context in which the word appears. We can also understand it to represent the series of questions to which that word can be an answer. Similarly, the question is read by a bi-directional GRU, but in this case only the last hidden states are concatenated to embed the question. Attention on each word in the context is then calculated as the point product of its contextual embedding with the question. This attention is then normalized by the Softmax function and summed up over all occurrences of each answer candidate."}, {"heading": "5.1.2 Out-of-vocabulary words", "text": "During our previous experiments with CNN, DM, and CBT datasets (Kadlec et al., 2016), each unique word from the training, validation, and test datasets had its series in the search matrix V. However, by radically increasing the size of the dataset, this would result in an extremely large number of model parameters, so we decided to limit the vocabulary size to 200,000 most common words. Each unique word from the vocabulary is now mapped to one of 1,000 anonymous tokens randomly initialized and not trained."}, {"heading": "5.1.3 Query-initiated encoder", "text": "We tried to initialize the hidden state of the context encoder GRU by letting the encoder read the question before proceeding to read the context document. Intuitively, the encoder can know in advance what to look out for when reading the context document. Including models of this kind in the ensemble helped improve performance."}, {"heading": "5.2 Results", "text": "Table 2 shows the accuracy of the AS reader and other architectures based on the CBT validation and test data. The last two lines show the performance of the AS reader trained on the BookTest dataset; all other models were trained on the original CBT training data. If we take the best AS reader ensemble trained on the CBT as a starting point, improving the model architecture as in (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016; Weissenborn et al., 2016; Cui et al., 2016b; Cui et al., 2016a) by continuing to use the original CBT training data leads to improvements of 1% and 2.1% absolutely on named units and common nouns, respectively. In contrast, inflating the training dataset resulted in a boost of 7.4-14.8% using the same model at the same time. The interaction of our models on Facebook (Nasal) even surpassed the ones on Hill in 2015."}, {"heading": "6 Discussion", "text": "In fact, it is that we are able to assert ourselves, that we are able, that we are able to achieve our goals, and that we are able to achieve our goals."}, {"heading": "7 Human Study", "text": "After adding more data, we have boosted the performance of the CBT validation and test data sets. However, is there still potential for much further growth beyond the results we observed? We decided to explore the remaining room for improvement over the CBT by testing people using a random subset of 50 named entities and 50 common noun validation questions that the AS Reader Ensemble was unable to answer correctly. These questions were answered by 10 non-native English speakers from our research laboratory, each based on a subset of questions. Participants had unlimited time to answer the questions, and were told that these questions were not answered correctly by a machine, which provided additional motivation to prove that they were better than computers. The results of the human study are summarized in Table 3. They show that the majority of the questions that our system has not been able to answer to date are actually answerable, and this suggests that 1) the original human baseline may still be answered by both, but that there may still be some of the human baseline examples of BT, but that some of the two may still be underestimated."}, {"heading": "8 Conclusion", "text": "Few methods for improving model performance are as firmly established as using more training data. However, we believe that this principle has been somewhat neglected in recent research on text comprehension. Although there is virtually unlimited data available in this area, most research has been done on unnecessarily small data. As a gentle reminder to the community, we have shown that simply injecting a model with more data can lead to performance improvements of up to 14.8%, with multiple attempts to improve the model architecture on the same training data resulting in gains of no more than 2.1% compared to our best end result. Yes, experiments with small data sets can certainly bring useful insights, but we believe that the community should also embrace the real-world scenario of data overload. The BookTest dataset we propose gives the reading comprehension community the opportunity to take a step in this direction."}, {"heading": "Appendix A Training Details", "text": "The training is similar to that in (Kadlec et al., 2016), but we do not have it complete here. The best learning rate in our experiments was 0.0005. We have objectively minimized negative log probability like the training. Initial weights in the word embedding were pulled out of the interval. [...] The weights in the GRU networks were initiated by random orthogonal matrices. [...]"}], "references": [{"title": "Scaling to Very Very Large Corpora for Natural Language Disambiguation", "author": ["Banko", "Brill2001] Michele Banko", "Eric Brill"], "venue": "ACL \u201901 Proceedings of the 39th Annual Meeting on Association for Computational Linguistics,", "citeRegEx": "Banko et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Banko et al\\.", "year": 2001}, {"title": "Theano: new features and speed improvements", "author": ["Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio"], "venue": null, "citeRegEx": "Bastien et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "One billion word benchmark for measuring progress in statistical language modeling", "author": ["Tomas Mikolov", "Mike Schuster", "Qi Ge", "Thorsten Brants", "Phillipp Koehn", "Tony Robinson"], "venue": "Proceedings of the Annual Con-", "citeRegEx": "Chelba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chelba et al\\.", "year": 2014}, {"title": "A Thorough Examination of the CNN / Daily Mail Reading Comprehension Task. In Association for Computational Linguistics (ACL)", "author": ["Chen et al.2016] Danqi Chen", "Jason Bolton", "Christopher D. Manning"], "venue": null, "citeRegEx": "Chen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2016}, {"title": "Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Transla", "author": ["Cho et al.2014] Kyunghyun Cho", "Bart van Merrienboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": null, "citeRegEx": "Cho et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "2016a. Attention-over-Attention Neural Networks for Reading Comprehension", "author": ["Cui et al.2016a] Yiming Cui", "Zhipeng Chen", "Si Wei", "Shijin Wang", "Ting Liu", "Guoping Hu"], "venue": null, "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Consensus Attention-based Neural Networks for Chinese Reading Comprehension", "author": ["Cui et al.2016b] Yiming Cui", "Ting Liu", "Zhipeng Chen", "Shijin Wang", "Guoping Hu"], "venue": null, "citeRegEx": "Cui et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Cui et al\\.", "year": 2016}, {"title": "Gated-Attention Readers for Text Comprehension", "author": ["Hanxiao Liu", "William W. Cohen", "Ruslan Salakhutdinov"], "venue": null, "citeRegEx": "Dhingra et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Dhingra et al\\.", "year": 2016}, {"title": "Incorporating nonlocal information into information extraction systems by Gibbs sampling", "author": ["Trond Grenager", "Christopher Manning"], "venue": "Proceedings of the 43rd Annual Meeting on Association", "citeRegEx": "Finkel et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2005}, {"title": "The unreasonable effectiveness of data", "author": ["Halevy et al.2009] Alon Halevy", "Peter Norvig", "Fernando Pereira"], "venue": "IEEE Intelligent Systems,", "citeRegEx": "Halevy et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Halevy et al\\.", "year": 2009}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "WIKI READING : A Novel Large-scale Language Understanding Task over Wikipedia", "author": ["Alexandre Lacoste", "Llion Jones", "Illia Polosukhin", "Andrew Fandrianto", "Jay Han", "Matthew Kelcey", "David Berthelot"], "venue": "Acl", "citeRegEx": "Hewlett et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Hewlett et al\\.", "year": 2016}, {"title": "The goldilocks principle: Reading children\u2019s books with explicit memory representations. arXiv preprint arXiv:1511.02301", "author": ["Hill et al.2015] Felix Hill", "Antoine Bordes", "Sumit Chopra", "Jason Weston"], "venue": null, "citeRegEx": "Hill et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Some of my Best Friends are Linguists Applying Information Theoretic Methods : Evaluation of Grammar Quality", "author": ["Frederick Jelinek"], "venue": null, "citeRegEx": "Jelinek.,? \\Q2004\\E", "shortCiteRegEx": "Jelinek.", "year": 2004}, {"title": "Exploring the Limits of Language Modeling", "author": ["Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu"], "venue": "International Conference on Machine Learning (ICML)", "citeRegEx": "Jozefowicz et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Jozefowicz et al\\.", "year": 2016}, {"title": "Neural Text Understanding with Attention Sum Reader", "author": ["Kadlec et al.2016] Rudolf Kadlec", "Martin Schmid", "Ondej Bajgar", "Jan Kleindienst"], "venue": "Proceedings of ACL", "citeRegEx": "Kadlec et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kadlec et al\\.", "year": 2016}, {"title": "Adam: a Method for Stochastic Optimization", "author": ["Kingma", "Ba2015] Diederik P. Kingma", "Jimmy Lei Ba"], "venue": "International Conference on Learning Representations,", "citeRegEx": "Kingma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2015}, {"title": "Dynamic Entity Representation with Max-pooling Improves Machine Reading", "author": ["Ran Tian", "Naoaki Okazaki", "Kentaro Inui"], "venue": "Proceedings of the North American Chapter of the Association", "citeRegEx": "Kobayashi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kobayashi et al\\.", "year": 2016}, {"title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain Factoid Question Answering", "author": ["Li et al.2016] Peng Li", "Wei Li", "Zhengyan He", "Xuguang Wang", "Ying Cao", "Jie Zhou", "Wei Xu"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Li et al\\.", "year": 2016}, {"title": "Never-ending learning", "author": ["R. Wang", "D. Wijaya", "A. Gupta", "X. Chen", "A. Saparov", "M. Greaves", "J. Welling."], "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15).", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A Corpus and Evaluation Framework for Deeper Understanding", "author": ["Nathanael Chambers", "Xiaodong He", "Devi Parikh", "Dhruv Batra", "Lucy Vanderwende", "Pushmeet Kohli", "James Allen"], "venue": null, "citeRegEx": "Mostafazadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Who did What: A Large-Scale Person-Centered Cloze Dataset", "author": ["Hai Wang", "Mohit Bansal", "Kevin Gimpel", "David McAllester"], "venue": null, "citeRegEx": "Onishi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Onishi et al\\.", "year": 2016}, {"title": "The LAMBADA dataset : Word prediction requiring a broad discourse", "author": ["Angeliki Lazaridou", "Quan Ngoc Pham", "Raffaella Bernardi", "Sandro Pezzelle", "Marco Baroni", "Gemma Boleda", "Raquel Fern"], "venue": null, "citeRegEx": "Paperno et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Paperno et al\\.", "year": 2016}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Tomas Mikolov", "Yoshua Bengio"], "venue": "Proceedings of The 30th International Conference on Machine Learning,", "citeRegEx": "Pascanu et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Pascanu et al\\.", "year": 2012}, {"title": "SQuAD: 100,000+ Questions for Machine Comprehension of Text", "author": ["Jian Zhang", "Konstantin Lopyrev", "Percy Liang"], "venue": null, "citeRegEx": "Rajpurkar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rajpurkar et al\\.", "year": 2016}, {"title": "MCTest: A Challenge Dataset for the Open-Domain Machine Comprehension of Text", "author": ["Christopher J C Burges", "Erin Renshaw"], "venue": "Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Richardson et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Richardson et al\\.", "year": 2013}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Saxe et al.2014] Andrew M Saxe", "James L Mcclelland", "Surya Ganguli"], "venue": "International Conference on Learning Representations", "citeRegEx": "Saxe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Saxe et al\\.", "year": 2014}, {"title": "ReasoNet: Learning to Stop Reading in Machine Comprehension", "author": ["Shen et al.2016] Yelong Shen", "Po-Sen Huang", "Jianfeng Gao", "Weizhu Chen"], "venue": null, "citeRegEx": "Shen et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shen et al\\.", "year": 2016}, {"title": "Iterative Alternating Neural Attention for Machine Reading", "author": ["Phillip Bachman", "Yoshua Bengio"], "venue": null, "citeRegEx": "Sordoni et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Sordoni et al\\.", "year": 2016}, {"title": "Cloze procedure: a new tool for measuring readability", "author": ["Wilson L Taylor"], "venue": "Journalism and Mass Communication", "citeRegEx": "Taylor.,? \\Q1953\\E", "shortCiteRegEx": "Taylor.", "year": 1953}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["Dan Klein", "Christopher D Manning"], "venue": "In Proceedings of the 2003 Conference of the North American Chapter of the Association", "citeRegEx": "Toutanova et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Toutanova et al\\.", "year": 2003}, {"title": "2016a. A Parallel-Hierarchical Model for Machine Comprehension on Sparse Data", "author": ["Zheng Ye", "Xingdi Yuan", "Jing He", "Phillip Bachman", "Kaheer Suleman"], "venue": "Proceedings of ACL", "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Natural Language Comprehension with the EpiReader", "author": ["Zheng Ye", "Xingdi Yuan", "Kaheer Suleman"], "venue": null, "citeRegEx": "Trischler et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Trischler et al\\.", "year": 2016}, {"title": "Separating Answers from Queries for Neural Reading Comprehension", "author": ["Dirk Weissenborn"], "venue": null, "citeRegEx": "Weissenborn.,? \\Q2016\\E", "shortCiteRegEx": "Weissenborn.", "year": 2016}], "referenceMentions": [{"referenceID": 29, "context": "These are cloze-style questions (Taylor, 1953) which require the reader to fill in a missing word in a sentence.", "startOffset": 32, "endOffset": 46}, {"referenceID": 10, "context": "Two such large-scale datasets have recently been proposed by researchers from Google DeepMind and Facebook AI: the CNN/Daily Mail dataset (Hermann et al., 2015) and the Children\u2019s Book Test (CBT) (Hill et al.", "startOffset": 138, "endOffset": 160}, {"referenceID": 12, "context": ", 2015) and the Children\u2019s Book Test (CBT) (Hill et al., 2015) respectively.", "startOffset": 43, "endOffset": 62}, {"referenceID": 9, "context": "it is widely accepted that more data can significantly improve performance of most models (Banko and Brill, 2001; Halevy et al., 2009);", "startOffset": 90, "endOffset": 134}, {"referenceID": 2, "context": "Then the large-scale One Billion Word corpus dataset appeared (Chelba et al., 2014) and it allowed Jozefowicz et al.", "startOffset": 62, "endOffset": 83}, {"referenceID": 14, "context": "to train much larger LSTM models (Jozefowicz et al., 2016) that almost halved the state-of-the-art perplexity on this dataset.", "startOffset": 33, "endOffset": 58}, {"referenceID": 12, "context": "On the named-entity version of CBT this brings the ensemble of our models to the level of human baseline as reported by Facebook (Hill et al., 2015).", "startOffset": 129, "endOffset": 148}, {"referenceID": 12, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 15, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 28, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 7, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 18, "context": "This is exploited by many machine learning models (Hill et al., 2015; Kadlec et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016b; Cui et al., 2016a; Li et al., 2016), however some do not explicitly depend on this property (Hermann et al.", "startOffset": 50, "endOffset": 214}, {"referenceID": 10, "context": ", 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)", "startOffset": 64, "endOffset": 148}, {"referenceID": 17, "context": ", 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)", "startOffset": 64, "endOffset": 148}, {"referenceID": 3, "context": ", 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)", "startOffset": 64, "endOffset": 148}, {"referenceID": 33, "context": ", 2016), however some do not explicitly depend on this property (Hermann et al., 2015; Kobayashi et al., 2016; Chen et al., 2016; Weissenborn, 2016)", "startOffset": 64, "endOffset": 148}, {"referenceID": 25, "context": "MCTest (Richardson et al., 2013), however their scale is not suitable for deep learning without pretraining on other data (Trischler et al.", "startOffset": 7, "endOffset": 32}, {"referenceID": 10, "context": "These two datasets (Hermann et al., 2015) exploit a useful feature of online news articles \u2013 many articles include a short summarizing sentence near the top of the page.", "startOffset": 19, "endOffset": 41}, {"referenceID": 10, "context": "Statistics were taken from (Hermann et al., 2015) and the statistics provided with the CBT data set.", "startOffset": 27, "endOffset": 49}, {"referenceID": 3, "context": "(Chen et al., 2016) have suggested that this can make about 17% of the questions unanswerable even by humans.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "The Children\u2019s Book Test (Hill et al., 2015) uses a different source - books freely available thanks to Project Gutenberg4.", "startOffset": 25, "endOffset": 44}, {"referenceID": 12, "context": "Based on human evaluation done in (Hill et al., 2015) it seems that named entities (NEs) and common nouns (CNs) are more context dependent than the other two types \u2013 prepositions and verbs.", "startOffset": 34, "endOffset": 53}, {"referenceID": 22, "context": "The LAMBADA dataset (Paperno et al., 2016) is designed to measure progress in understanding common-sense questions about short stories that can be easily answered by humans but cannot be answered by current standard machine-learning models (e.", "startOffset": 20, "endOffset": 42}, {"referenceID": 24, "context": "The SQuAD dataset (Rajpurkar et al., 2016) based on Wikipedia and the Who-did-What dataset (Onishi et al.", "startOffset": 18, "endOffset": 42}, {"referenceID": 21, "context": ", 2016) based on Wikipedia and the Who-did-What dataset (Onishi et al., 2016) based on Gigaword news articles are factoid question-answering datasets where a multi-word answer should be extracted from a context document.", "startOffset": 56, "endOffset": 77}, {"referenceID": 20, "context": "The Story Cloze Test (Mostafazadeh et al., 2016)", "startOffset": 21, "endOffset": 48}, {"referenceID": 11, "context": "In the WikiReading (Hewlett et al., 2016) dataset the context document is formed from a Wikipedia article and the question-answer pair is taken from the corresponding WikiData page.", "startOffset": 19, "endOffset": 41}, {"referenceID": 10, "context": "(Hermann et al., 2015).", "startOffset": 0, "endOffset": 22}, {"referenceID": 12, "context": "This work was followed by the application of Memory Networks to the same task (Hill et al., 2015).", "startOffset": 78, "endOffset": 97}, {"referenceID": 17, "context": "Later three models emerged around the same time (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al.", "startOffset": 48, "endOffset": 112}, {"referenceID": 15, "context": "Later three models emerged around the same time (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al.", "startOffset": 48, "endOffset": 112}, {"referenceID": 3, "context": "Later three models emerged around the same time (Kobayashi et al., 2016; Kadlec et al., 2016; Chen et al., 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al.", "startOffset": 48, "endOffset": 112}, {"referenceID": 15, "context": ", 2016) including our Attention Sum Reader (AS Reader) model (Kadlec et al., 2016).", "startOffset": 61, "endOffset": 82}, {"referenceID": 28, "context": ", 2016b); extend it with a hierarchical structure (Sordoni et al., 2016; Shen et al., 2016; Dhingra et al., 2016); compute attention over the context document for every word in the query (Cui et al.", "startOffset": 50, "endOffset": 113}, {"referenceID": 27, "context": ", 2016b); extend it with a hierarchical structure (Sordoni et al., 2016; Shen et al., 2016; Dhingra et al., 2016); compute attention over the context document for every word in the query (Cui et al.", "startOffset": 50, "endOffset": 113}, {"referenceID": 7, "context": ", 2016b); extend it with a hierarchical structure (Sordoni et al., 2016; Shen et al., 2016; Dhingra et al., 2016); compute attention over the context document for every word in the query (Cui et al.", "startOffset": 50, "endOffset": 113}, {"referenceID": 33, "context": "Other neural approaches to text comprehension are explored in (Weissenborn, 2016; Li et al., 2016).", "startOffset": 62, "endOffset": 98}, {"referenceID": 18, "context": "Other neural approaches to text comprehension are explored in (Weissenborn, 2016; Li et al., 2016).", "startOffset": 62, "endOffset": 98}, {"referenceID": 28, "context": "Current state of the art models (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016a) improve over AS Reader\u2019s accuracy on CBT NE and CN datasets by 1-2 percent absolute.", "startOffset": 32, "endOffset": 120}, {"referenceID": 7, "context": "Current state of the art models (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Cui et al., 2016a) improve over AS Reader\u2019s accuracy on CBT NE and CN datasets by 1-2 percent absolute.", "startOffset": 32, "endOffset": 120}, {"referenceID": 9, "context": "then (Banko and Brill, 2001; Halevy et al., 2009).", "startOffset": 5, "endOffset": 49}, {"referenceID": 12, "context": "When creating our dataset we follow the same procedure as was used to create the CBT dataset (Hill et al., 2015).", "startOffset": 93, "endOffset": 112}, {"referenceID": 13, "context": "Quote attributed to Robert Mercer by Fred Jelinek (Jelinek, 2004) BookTest dataset can be downloaded from https:// ibm.", "startOffset": 50, "endOffset": 65}, {"referenceID": 30, "context": "and named entity detection we use the Stanford POS tagger (Toutanova et al., 2003) and Stanford NER (Finkel et al.", "startOffset": 58, "endOffset": 82}, {"referenceID": 8, "context": ", 2003) and Stanford NER (Finkel et al., 2005)7.", "startOffset": 25, "endOffset": 46}, {"referenceID": 12, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 15, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 3, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 28, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 7, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 33, "context": "Therefore the models trained on the BookTest corpus can be evaluated on the original CBT data and they can be compared with recent text-comprehension models utilizing this dataset (Hill et al., 2015; Kadlec et al., 2016; Chen et al., 2016; Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a).", "startOffset": 180, "endOffset": 365}, {"referenceID": 15, "context": "In (Kadlec et al., 2016) we introduced the AS Reader8, which at the time of publication significantly outperformed all other architectures on the CNN, DM and CBT datasets.", "startOffset": 3, "endOffset": 24}, {"referenceID": 28, "context": "rkadlec/asreader many subsequent models (Trischler et al., 2016b; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b; Cui et al., 2016a; Shen et al., 2016).", "startOffset": 40, "endOffset": 166}, {"referenceID": 7, "context": "rkadlec/asreader many subsequent models (Trischler et al., 2016b; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b; Cui et al., 2016a; Shen et al., 2016).", "startOffset": 40, "endOffset": 166}, {"referenceID": 27, "context": "rkadlec/asreader many subsequent models (Trischler et al., 2016b; Sordoni et al., 2016; Dhingra et al., 2016; Cui et al., 2016b; Cui et al., 2016a; Shen et al., 2016).", "startOffset": 40, "endOffset": 166}, {"referenceID": 4, "context": "The document is then read by a bidirectional GRU network (Cho et al., 2014).", "startOffset": 57, "endOffset": 75}, {"referenceID": 15, "context": "For a more detailed description of the model including equations check (Kadlec et al., 2016).", "startOffset": 71, "endOffset": 92}, {"referenceID": 15, "context": "During our past experiments on the CNN, DM and CBT datasets (Kadlec et al., 2016) each unique word from the training, validation and test datasets had its row in the look-up matrix V.", "startOffset": 60, "endOffset": 81}, {"referenceID": 28, "context": "If we take the best AS Reader ensemble trained on CBT as a baseline, improving the model architecture as in (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a), continuing to use the original CBT training data, lead to improvements of 1% and 2.", "startOffset": 108, "endOffset": 234}, {"referenceID": 7, "context": "If we take the best AS Reader ensemble trained on CBT as a baseline, improving the model architecture as in (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a), continuing to use the original CBT training data, lead to improvements of 1% and 2.", "startOffset": 108, "endOffset": 234}, {"referenceID": 33, "context": "If we take the best AS Reader ensemble trained on CBT as a baseline, improving the model architecture as in (Sordoni et al., 2016; Dhingra et al., 2016; Trischler et al., 2016b; Weissenborn, 2016; Cui et al., 2016b; Cui et al., 2016a), continuing to use the original CBT training data, lead to improvements of 1% and 2.", "startOffset": 108, "endOffset": 234}, {"referenceID": 12, "context": "The ensemble of our models even exceeded the human baseline provided by Facebook (Hill et al., 2015) on the Common Noun dataset.", "startOffset": 81, "endOffset": 100}, {"referenceID": 3, "context": "(Chen et al., 2016).", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Humans (query) (Hill et al., 2015) NA 52.", "startOffset": 15, "endOffset": 34}, {"referenceID": 12, "context": "4 Humans (context+query) (Hill et al., 2015) NA 81.", "startOffset": 25, "endOffset": 44}, {"referenceID": 12, "context": "LSTMs (context+query) (Hill et al., 2015) 51.", "startOffset": 22, "endOffset": 41}, {"referenceID": 12, "context": "0 \uf8fc\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fd\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8fe CBT training data Memory Networks (Hill et al., 2015) 70.", "startOffset": 61, "endOffset": 80}, {"referenceID": 7, "context": "GA Reader (ensemble) (Dhingra et al., 2016) 75.", "startOffset": 21, "endOffset": 43}, {"referenceID": 28, "context": "6 IA Reader (ensemble) (Sordoni et al., 2016) 76.", "startOffset": 23, "endOffset": 45}], "year": 2016, "abstractText": "There is a practically unlimited amount of natural language data available. Still, recent work in text comprehension has focused on datasets which are small relative to current computing possibilities. This article is making a case for the community to move to larger data and as a step in that direction it is proposing the BookTest, a new dataset similar to the popular Children\u2019s Book Test (CBT), however more than 60 times larger. We show that training on the new data improves the accuracy of our Attention-Sum Reader model on the original CBT test data by a much larger margin than many recent attempts to improve the model architecture. On one version of the dataset our ensemble even exceeds the human baseline provided by Facebook. We then show in our own human study that there is still space for further improvement.", "creator": "LaTeX with hyperref package"}}}