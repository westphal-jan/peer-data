{"id": "1610.01030", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Applications of Online Deep Learning for Crisis Response Using Social Media Information", "abstract": "During natural or man-made disasters, humanitarian response organizations look for useful information to support their decision-making processes. Social media platforms such as Twitter have been considered as a vital source of useful information for disaster response and management. Despite advances in natural language processing techniques, processing short and informal Twitter messages is a challenging task. In this paper, we propose to use Deep Neural Network (DNN) to address two types of information needs of response organizations: 1) identifying informative tweets and 2) classifying them into topical classes. DNNs use distributed representation of words and learn the representation as well as higher level features automatically for the classification task. We propose a new online algorithm based on stochastic gradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset.", "histories": [["v1", "Tue, 4 Oct 2016 14:53:51 GMT  (1301kb,D)", "https://arxiv.org/abs/1610.01030v1", "Accepted at SWDM co-located with CIKM 2016. 6 pages, 2 figures. arXiv admin note: text overlap witharXiv:1608.03902"], ["v2", "Wed, 5 Oct 2016 09:50:15 GMT  (1302kb,D)", "http://arxiv.org/abs/1610.01030v2", "Accepted at SWDM co-located with CIKM 2016. 6 pages, 2 figures. arXiv admin note: text overlap witharXiv:1608.03902"]], "COMMENTS": "Accepted at SWDM co-located with CIKM 2016. 6 pages, 2 figures. arXiv admin note: text overlap witharXiv:1608.03902", "reviews": [], "SUBJECTS": "cs.CL cs.CY cs.LG", "authors": ["dat tien nguyen", "shafiq joty", "muhammad imran", "hassan sajjad", "prasenjit mitra"], "accepted": false, "id": "1610.01030"}, "pdf": {"name": "1610.01030.pdf", "metadata": {"source": "CRF", "title": "Applications of Online Deep Learning for Crisis Response Using Social Media Information", "authors": ["Dat Tien Nguyen", "Shafiq Joty", "Muhammad Imran", "Hassan Sajjad", "Prasenjit Mitra"], "emails": ["pmitra}@qf.org.qa"], "sections": [{"heading": null, "text": "Tags Deep Learning, Supervised Classification, Twitter, Text Classification, Crisis Response"}, {"heading": "1. INTRODUCTION", "text": "In fact, most of them are able to survive on their own."}, {"heading": "2. DEEP NEURAL NETWORK", "text": "As previously argued, deep neural networks (DNNs) can be very effective in classifying tweets during a disaster situation due to their distributed representation of words and their automatic ability to learn features. Furthermore, DNNs are usually trained using online algorithms, which is well suited to the needs of a crisis response situation. Our main hypothesis is that a classification model to effectively classify tweets that are short and informative should learn the most important features at different levels of abstraction. To this end, we use a Convolutional Neural Network (CNN) that has proven effective for sentence-level classification tasks [13]."}, {"heading": "2.1 Convolutional Neural Network", "text": "We show how a CNN with an example tweet.2 Each word in Vocabulary V is represented by a D-dimensional vector in a common selection table. (D) We can convert any word we convert into a sequence into a ready-made word sequence. (D) We can convert any word we embed in a 2The HTTP tag in the example to a different level. (D) The output layer then generates an input vector xt xt for each token that is passed through a sequence of evolution and pooling operations to learn high-level abstractions. (D) We have to apply a filter u RL.D to a window of L-words to create a new one. (D)"}, {"heading": "2.2 Online Learning", "text": "DNNs are usually trained using best-in-class online methods such as stochastic gradient descent (SGD), which brings a crucial advantage in crisis situations where retraining the entire model is impractical every time a small batch of marked data arrives. Algorithm 1 shows how our CNN model can be trained in a pure online environment. First, we initialize the model parameters \u03b80 (line 1), which can be a trained model from other disaster events, or it can be randomly initialized to start from zero. As a new batch of labeled tweets, we first calculate the log losses (cross-entropy) in Equation 6 forBt taking into account the current parameters successt (line 2a). Then, we use backpropagation to calculate the gradients f \u2032 (Northt) of the loss taking into account the current parameters (line 2b). Finally, we update the parameters with the correct learning rate suggested by the ADS (succesle 2st) and take the ADNs into practice."}, {"heading": "2.3 Word Embedding and Fine-tuning", "text": "As already mentioned, we can randomly initialize word embeddings L and learn them as part of model parameters by propagating the errors back to the search plane. Random initialization can cause the training algorithm to get stuck in a local minimum. However, the readily available embeddings from external sources (e.g. Google embeddings [16]) can be incorporated into the neural network model and used as features without further task-specific tunings. However, the latter approach does not take advantage of the automatic learning capability of DNN models, which is one of the main motivations for their use. In our work, we use pre-formed word embeddings (see below) to better initialize our models, and we tailor them to our task, which proves beneficial. Mikolov et al. [16] offer two loglinear models for calculating word embeddings from large (unlabeled) bodies."}, {"heading": "3. DATASET AND EXPERIMENTAL SETTINGS", "text": "In this section, we describe the records used for the classification tasks and the settings for CNN and online learning."}, {"heading": "3.1 Dataset and Preprocessing", "text": "We use CrisisNLP [9] labeled data sets. CNN models were trained online using a labeled data set related to the Nepal Earthquake5 of 2015, and the rest of the data sets are used to train an initial model (success0 in Algorithm 1) on which online learning is conducted. The Nepal Earthquakes dataset consists of about 12k labeled tweets collected during the event using various keywords such as NepalEarthquake from Twitter. Of all labeled tweets, 9k are labeled during the actual event on the AIDR platform [8] by trained volunteers, and the remaining 3k tweets are crowdsourced using the Crowdflower7 platform. The data set is divided into different informative classes (e.g. individuals affected, infrastructure damage, donations, etc.) and an \"unrelated\" or \"irrelevant\" tweet. \""}, {"heading": "3.2 Online Training Settings", "text": "In our case, we train the initial model using all data sets from CrisisNLP except for the Nepal earthquake. For the online training, we sort the data labeled with Nepal based on the timestamp of the tweets. This brings the tweets in their posting order. Next, the data set D is split into each time interval {} {}, and a test set (20%) is defined as: D = \u2211 T = 1 dt, where dt = 200. For each time interval, we divide the available data sets into a train set (70%), dev set (10%), and a test set (20%) using the module of skimmers [19], which ensures that the class distribution in each subset remains reasonably balanced. Based on the above-mentioned data splitting strategy, we start learning online to train a binary and a multi-class classifier."}, {"heading": "4. RESULTS", "text": "In this section we present our results for binary and multi-level classification tasks."}, {"heading": "4.1 Binary Classification", "text": "Figure 2 shows the results for the \"informative\" vs. \"non-informative\" binary classification task using online learning. The performance of the model is quite contradictory as the size of the training data varies. We initially observed an improvement in performance. However, if the training size is between 2200 and 3900 tweets, the results decreased. We examined this strange result and found that this could be due to inconsistencies in the annotation process and data sources. In our training data for the case (Nepal Earthquake), the first 3000 tweets were from CrowdFlower and the rest from AIDR. Tweets in CrowdFlower were commented by paid workers, where AIDR tweets are commented by volunteers. We speculate that these inconsistencies may affect performance at the outset, but as the model sees more AIDR data (4000 +), performance stabilizes."}, {"heading": "4.2 Multi-Class Classification", "text": "Figure 3 summarizes the results of the online training for the multi-class classification task. As the multi-class classification is a more difficult task than binary classification, the first training run delivers very low accuracy and the results continue to decline until a good number of training examples are available, which in this case is about 2,200 tagged tweets. As in the case of binary classification, the performance of the classifier improves once over 3,000 tweets are available and remains stable thereafter. The advantage of using online learning methods such as CNN compared to offline learning methods used in classifiers such as SVM, Naive Bayes and Logistic Regression is online training. The tagged data comes in stacks and each time requires a new training of a model on the complete data together with the addition of newly tagged data. Online training methods learn in small quantities, which is perfectly suited to the situation in hand. Another advantage of neural networking methods is the automatic feature extraction, which does not require any manual engineering."}, {"heading": "4.3 Discussion", "text": "The rapid analysis of social media posts in time-critical situations is important for humanitarian agencies to make timely decisions and initiate relief efforts. This work proposes solutions to two major challenges that humanitarian organizations face while incorporating social media data into crisis response. Firstly, how to filter out loud and irrelevant messages from big crisis data, and secondly, how to classify informative messages into different interest groups. By using tagged data from past crises, we show the performance of DNNs trained using the proposed online learning algorithm for binary and multi-level classification tasks. We observe that in the past, tagged data helps when event-specific data is not available in the early hours of a crisis."}, {"heading": "5. RELATED WORK", "text": "A number of systems have been developed to classify, extract and summarize crisis-related information from social media [21]; for a detailed survey see [7]. Cameron, et al., describe a platform for emergency awareness [2]. They classify interesting tweets using an SVM classifier. Verma, et al., use Naive Bayes and MaxEnt classifiers to find situation-related tweets from multiple crises [25]. Imran, et al., implemented AIDR to classify a Twitter data stream during crises [8]. They use a random forest classifier in an offline setting. After receiving each mini-batch of 50 training examples, they replace the older model with a new one. In [10], the authors show the performance of a number of non-neural network classifiers trained on marked data from past crisis events."}, {"heading": "6. CONCLUSIONS", "text": "We introduced an online learning model, the Convolutional Neural Network, for classifying tweets in a disaster response scenario. We proposed a new online learning algorithm for training CNNs in online mode. We demonstrated that the online training of the model fits perfectly with the disaster response situation. We believe that a basic model exists that was trained on the basis of earlier crisis-marked data, and that the event-specific marked data arrives in small batches that are used to conduct online learning. Neural network models offer an additive advantage of automatic feature extraction, which facilitates the training process compared to offline learning methods such as SVM, logistic regression. The model uses only tagged tweets for training and automatically learns features from them. We re-ported the results of two classification tasks (i.e. binary and multiclass). In addition, we provide the research community with source code for CNN online learning models for further enhancements."}, {"heading": "7. REFERENCES", "text": "[1] A. Acar and Y. Muraki. Twitter for crisis communication: lessons learned from japan's tsunami disaster. International Journal of Web Based Communities, 7 (3): 392-402, 2011. [2] M. A. Cameron, R. Power, B. Robinson, and J. Yin. Emergency situation awareness from twitter for crisis management. In Proceedings of the 21st international conference companion on on on on World Wide Web, pages 695-698. ACM, 2012. [3] C. Caragea, A. Silvescu, and A. H. Tapia. Identifying informative messages in disaster events using convolutional neural networks. International Conference on Confederations of Crisis Response and Management, 2016. C. Castillo. Big Crisis Data. Cambridge University Press, 2016. [5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. Natural language processing (almost) from scratch."}], "references": [{"title": "Twitter for crisis communication: lessons learned from japan\u2019s tsunami disaster", "author": ["A. Acar", "Y. Muraki"], "venue": "International Journal of Web Based Communities, 7(3):392\u2013402", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2011}, {"title": "Emergency situation awareness from twitter for crisis management", "author": ["M.A. Cameron", "R. Power", "B. Robinson", "J. Yin"], "venue": "Proceedings of the 21st international conference companion on World Wide Web, pages 695\u2013698. ACM", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2012}, {"title": "Identifying informative messages in disaster events using convolutional neural networks", "author": ["C. Caragea", "A. Silvescu", "A.H. Tapia"], "venue": "International Conference on Information Systems for Crisis Response and Management", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Big Crisis Data", "author": ["C. Castillo"], "venue": "Cambridge University Press", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Natural language processing (almost) from scratch", "author": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "The Journal of Machine Learning Research, 12:2493\u20132537", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "B", "author": ["K. Gimpel", "N. Schneider"], "venue": "O\u2019Connor, D. Das, D. Mills, J. Eisenstein, M. Heilman, D. Yogatama, J. Flanigan, and N. A. Smith. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 42\u201347. Association for Computational Linguistics", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "Processing social media messages in mass emergency: a survey", "author": ["M. Imran", "C. Castillo", "F. Diaz", "S. Vieweg"], "venue": "ACM Computing Surveys (CSUR), 47(4):67", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "AIDR: Artificial intelligence for disaster response", "author": ["M. Imran", "C. Castillo", "J. Lucas", "P. Meier", "S. Vieweg"], "venue": "Proceedings of the companion publication of the 23rd international conference on World wide web companion, pages 159\u2013162. International World Wide Web Conferences Steering Committee", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Twitter as a lifeline: Human-annotated twitter corpora for NLP of crisis-related messages", "author": ["M. Imran", "P. Mitra", "C. Castillo"], "venue": "Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC)", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Cross-language domain adaptation for classifying crisis-related short messages", "author": ["M. Imran", "P. Mitra", "J. Srivastava"], "venue": "International Conference on Information Systems for Crisis Response and Management", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "Speech act modeling of written asynchronous conversations with task-specific embeddings and conditional structured models", "author": ["S.R. Joty", "E. Hoque"], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "A convolutional neural network for modelling sentences. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)", "author": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Y. Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2014}, {"title": "Adam: A method for stochastic optimization", "author": ["D.P. Kingma", "J. Ba"], "venue": "CoRR, abs/1412.6980", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Fine-grained opinion mining with recurrent neural networks and word embeddings", "author": ["P. Liu", "S. Joty", "H. Meng"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "venue": "Advances in Neural Information Processing Systems, pages 3111\u20133119", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2013}, {"title": "Rapid classification of crisis-related data on social networks using convolutional neural networks", "author": ["D.T. Nguyen", "K.A.A. Mannai", "S. Joty", "H. Sajjad", "M. Imran", "P. Mitra"], "venue": "arXiv preprint arXiv:1608.03902", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "Journal of Machine Learning Research, 12:2825\u20132830", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2011}, {"title": "Glove: Global  vectors for word representation", "author": ["J. Pennington", "R. Socher", "C. Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2014}, {"title": "Summarizing situational tweets in crisis scenario", "author": ["K. Rudra", "S. Banerjee", "N. Ganguly", "P. Goyal", "M. Imran", "P. Mitra"], "venue": "Proceedings of the 27th ACM Conference on Hypertext and Social Media, pages 137\u2013147. ACM", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2016}, {"title": "Earthquake shakes twitter users: real-time event detection by social sensors", "author": ["T. Sakaki", "M. Okazaki", "Y. Matsuo"], "venue": "Proceedings of the 19th international conference on World wide web, pages 851\u2013860. ACM", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, 15:1929\u20131958", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Aid is out there: Looking for help from tweets during a large scale disaster", "author": ["I. Varga", "M. Sano", "K. Torisawa", "C. Hashimoto", "K. Ohtake", "T. Kawai", "J.-H. Oh", "S. De Saeger"], "venue": "ACL (1), pages 1619\u20131629", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2013}, {"title": "Natural language processing to the rescue? extracting\" situational awareness\" tweets during mass emergency", "author": ["S. Verma", "S. Vieweg", "W.J. Corvey", "L. Palen", "J.H. Martin", "M. Palmer", "A. Schram", "K.M. Anderson"], "venue": "ICWSM. Citeseer", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2011}, {"title": "Integrating social media communications into the rapid assessment of sudden onset disasters", "author": ["S. Vieweg", "C. Castillo", "M. Imran"], "venue": "Social Informatics, pages 444\u2013461. Springer", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "CoRR, abs/1212.5701", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 3, "context": "However, information scarcity during time-critical situations hinders decision-making processes and delays response efforts [4, 7].", "startOffset": 124, "endOffset": 130}, {"referenceID": 6, "context": "However, information scarcity during time-critical situations hinders decision-making processes and delays response efforts [4, 7].", "startOffset": 124, "endOffset": 130}, {"referenceID": 25, "context": ", on social media platforms like Twitter [26].", "startOffset": 41, "endOffset": 45}, {"referenceID": 6, "context": "Traditional classification approaches rely on manually engineered features like cue words and TF-IDF vectors for learning [7].", "startOffset": 122, "endOffset": 125}, {"referenceID": 12, "context": "To this end, we use a Convolutional Neural Network (CNN), which has been shown to be effective for sentence-level classification tasks [13].", "startOffset": 135, "endOffset": 139}, {"referenceID": 15, "context": "We can initialize L randomly or using pretrained word embedding vectors like word2vec [16].", "startOffset": 86, "endOffset": 90}, {"referenceID": 11, "context": "We use a wide convolution [12] (as opposed to narrow), which ensures that the filters reach the entire sentence, including the boundary words.", "startOffset": 26, "endOffset": 30}, {"referenceID": 26, "context": "Several adaptive methods such as ADADELTA [27], ADAM [14], etc.", "startOffset": 42, "endOffset": 46}, {"referenceID": 13, "context": "Several adaptive methods such as ADADELTA [27], ADAM [14], etc.", "startOffset": 53, "endOffset": 57}, {"referenceID": 15, "context": ", Google embeddings [16]) in the neural network model and use them as features without further task-specific tuning.", "startOffset": 20, "endOffset": 24}, {"referenceID": 15, "context": "[16] propose two log-linear models for computing word embeddings from large (unlabeled) corpuses efficiently: (i) a bag-of-words model CBOW that predicts the current word based on the context words, and (ii) a skip-gram model that predicts surrounding words given the current word.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "com/p/word2vec/ Since we work on disaster related tweets, which are quite different from news, we have trained domain-specific embeddings of 300-dimensions (vocabulary size 20 million) using the Skip-gram model of word2vec tool [17] from a large corpus of disaster related tweets.", "startOffset": 228, "endOffset": 232}, {"referenceID": 8, "context": "We use CrisisNLP [9] labeled datasets.", "startOffset": 17, "endOffset": 20}, {"referenceID": 7, "context": "Of all the labeled tweets, 9k are labeled by trained volunteers during the actual event using the AIDR platform [8] and the remaining 3k tweets are labeled using the Crowdflower crowdsourcing platform.", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "We further tokenize the tweets using the CMU TweetNLP tool [6].", "startOffset": 59, "endOffset": 62}, {"referenceID": 18, "context": "For each time interval t, we divide the available labeled dataset into a train set (70%), dev set (10%), and a test set (20%) using ski-learn toolkit\u2019s module [19], which ensured that the class distribution remains reasonably balanced in each subset.", "startOffset": 159, "endOffset": 163}, {"referenceID": 26, "context": "We train CNN models by optimizing the cross entropy in Equation 4 using the gradient-based online learning algorithm ADADELTA [27].", "startOffset": 126, "endOffset": 130}, {"referenceID": 22, "context": "To avoid overfitting, we use dropout [23] of hidden units and early stop-", "startOffset": 37, "endOffset": 41}, {"referenceID": 0, "context": "Recent studies have shown the usefulness of crisis-related data on social media for disaster response and management [1, 22, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 21, "context": "Recent studies have shown the usefulness of crisis-related data on social media for disaster response and management [1, 22, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 23, "context": "Recent studies have shown the usefulness of crisis-related data on social media for disaster response and management [1, 22, 24].", "startOffset": 117, "endOffset": 128}, {"referenceID": 20, "context": "A number of systems have been developed to classify, extract, and summarize [21] crisis-relevant information from social media; for a detailed survey see [7].", "startOffset": 76, "endOffset": 80}, {"referenceID": 6, "context": "A number of systems have been developed to classify, extract, and summarize [21] crisis-relevant information from social media; for a detailed survey see [7].", "startOffset": 154, "endOffset": 157}, {"referenceID": 1, "context": ", describe a platform for emergency situation awareness [2].", "startOffset": 56, "endOffset": 59}, {"referenceID": 24, "context": ", use Naive Bayes and MaxEnt classifiers to find situational awareness tweets from several crises [25].", "startOffset": 98, "endOffset": 102}, {"referenceID": 7, "context": ", implemented AIDR to classify a Twitter data stream during crises [8].", "startOffset": 67, "endOffset": 70}, {"referenceID": 9, "context": "In [10], the authors show the performance of a number of non-neural network classifiers trained on labeled data from past crisis events.", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "DNNs and word embeddings have been applied successfully to address NLP problems [5, 3, 18, 15, 11].", "startOffset": 80, "endOffset": 98}, {"referenceID": 2, "context": "DNNs and word embeddings have been applied successfully to address NLP problems [5, 3, 18, 15, 11].", "startOffset": 80, "endOffset": 98}, {"referenceID": 17, "context": "DNNs and word embeddings have been applied successfully to address NLP problems [5, 3, 18, 15, 11].", "startOffset": 80, "endOffset": 98}, {"referenceID": 14, "context": "DNNs and word embeddings have been applied successfully to address NLP problems [5, 3, 18, 15, 11].", "startOffset": 80, "endOffset": 98}, {"referenceID": 10, "context": "DNNs and word embeddings have been applied successfully to address NLP problems [5, 3, 18, 15, 11].", "startOffset": 80, "endOffset": 98}, {"referenceID": 16, "context": "The emergence of tools such as word2vec [17] and GloVe [20] have enabled NLP researchers to learn word embeddings efficiently and use them to train better models.", "startOffset": 40, "endOffset": 44}, {"referenceID": 19, "context": "The emergence of tools such as word2vec [17] and GloVe [20] have enabled NLP researchers to learn word embeddings efficiently and use them to train better models.", "startOffset": 55, "endOffset": 59}, {"referenceID": 4, "context": "[5] presented a unified DNN architecture for solving various NLP tasks including part-of-speech tagging, chunking, named entity recognition and semantic role labeling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 12, "context": "Kim [13] and Kalchbrenner et al.", "startOffset": 4, "endOffset": 8}, {"referenceID": 11, "context": "[12] used convolutional neural networks (CNN) for sentence-level classification tasks (e.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "Caragea, Silvescu, and Tapia used CNNs to identify informative messages during disasters [3].", "startOffset": 89, "endOffset": 92}], "year": 2016, "abstractText": "During natural or man-made disasters, humanitarian response organizations look for useful information to support their decisionmaking processes. Social media platforms such as Twitter have been considered as a vital source of useful information for disaster response and management. Despite advances in natural language processing techniques, processing short and informal Twitter messages is a challenging task. In this paper, we propose to use Deep Neural Network (DNN) to address two types of information needs of response organizations: (i) identifying informative tweets and (ii) classifying them into topical classes. DNNs use distributed representation of words and learn the representation as well as higher level features automatically for the classification task. We propose a new online algorithm based on stochastic gradient descent to train DNNs in an online fashion during disaster situations. We test our models using a crisis-related real-world Twitter dataset.", "creator": "LaTeX with hyperref package"}}}