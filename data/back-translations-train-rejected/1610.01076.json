{"id": "1610.01076", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Tutorial on Answering Questions about Images with Deep Learning", "abstract": "Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task.", "histories": [["v1", "Tue, 4 Oct 2016 16:29:28 GMT  (1299kb,D)", "http://arxiv.org/abs/1610.01076v1", "The tutorial was presented at '2nd Summer School on Integrating Vision and Language: Deep Learning' in Malta, 2016"]], "COMMENTS": "The tutorial was presented at '2nd Summer School on Integrating Vision and Language: Deep Learning' in Malta, 2016", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL cs.LG cs.NE", "authors": ["mateusz malinowski", "mario fritz"], "accepted": false, "id": "1610.01076"}, "pdf": {"name": "1610.01076.pdf", "metadata": {"source": "CRF", "title": "Tutorial on Answering Questions about Images with Deep Learning", "authors": ["Mateusz Malinowski", "Mario Fritz"], "emails": ["mmalinow@mpi-inf.mpg.de", "mfritz@mpi-inf.mpg.de"], "sections": [{"heading": "1 Preface", "text": "This year, it will be ready to leave the country in which it is located."}, {"heading": "In [ ]: #TODO: Execute the following procedure (Shift+Enter in the Notebook)", "text": "From kraino.utils import data _ providerdp = data _ provider.select ['daquar-triples'] train _ text _ representation = dp ['text'] (train _ or _ test = 'train') This view defines how questions are terminated (\"?\"), answers are terminated (\".\"), answer words are delimited (DAQUAR sometimes has a series of answer words as an answer, e.g. \"knife, fork\" can be an answer), but most important is that it has questions (key \"x\"), answers (key \"y\") and names of the corresponding images (key \"img name\"). In []: # let us check some entries of the text representation n _ elements = 10 print ('= Questions:' = Questions: ') print _ list (train _ text _ representation [' print '] [: print _ elements] [: n _ elements]) print print (train _ text _ representation ['), print _ representation ['[' print _ representation]: n: article)."}, {"heading": "3 Textual Features", "text": "This year it has come to the point where it has never gone as far as it did this year."}, {"heading": "4 Language Only Models", "text": "The formula for weight updates is: w: = w \u2212 \u03b1 \"(x, y; w) with \u03b1 that we call the learning rate, and\" that is a gradient wrt. the weights w. \"The learning rate is a hyper-parameter that must be specified in advance. The rule shown above is called an SGD update, but other variants are also possible. In fact, we use their variant called ADAM [Kingma and Ba, 2014].We put the question in a classification box as an answer to the problem, so that we classify an input x into a class that represents an answer word, but other variants are also possible. In fact, we use their variant called ADAM [Kingma and Ba, 2014].We put the question in a classification box as an answer to the problem, so that we classify an input x into an answer word. Therefore, we use it, often used in classification, logistic regression as the goal:\" (x; y; w: the answer to the model x. \""}, {"heading": "In [ ]: import theano", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "In [ ]: #== Model definition", "text": "This year, it is only a matter of time before a result is reached: \"It is still too early to realise that such an outcome will be achieved.\""}, {"heading": "In [ ]: #== Model definition", "text": "This year, the number of newly discovered, newly discovered and newly discovered viruses has tripled compared to the previous year, so that the number of newly discovered viruses has tripled in recent years. \"We have never found as many new viruses as this year,\" he said."}, {"heading": "5 Evaluation Measures", "text": "In order to monitor progress on a task, we need to find ways to evaluate architectures on the task. Otherwise, we would not know how to evaluate architectures, or worse, we would not even know what the goal is. In addition, we should also aim at automatic rating standards to make it easier to evaluate, and the valuation costs are high. It is still difficult to evaluate architectures because of the ambiguities that occur in the answers. We have ambiguities in naming objects, sometimes because of synonyms, but sometimes because of fuzziness. For example, \"chair\" = \"armchair\" or \"chair.\""}, {"heading": "6 New Predictions", "text": "In fact, the majority of people who are in a position to move, to fight back, to fight back, to fight back, to fight back, to fight back, to fight back, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight,"}, {"heading": "7 Visual Features", "text": "As shown in Figure 6, a fairly common practice is: 1. Take a previously trained CNN; often the pre-training is done in a large-scale classification task such as ImageNet [Russakovsky et al., 2014]. 2. \"Cut\" a CNN representation after a layer. We use reactions of this layer as visual features. In this tutorial, we use features extracted from the penultimate 4096-dimensional layer of the VGG network [Simonyan and Zisserman, 2014]. We have previously extracted features with Caffe [Jia et al., 2014] - another excellent framework for deep learning, especially good for CNNs. The following code gives visual features that are consistent with textual features."}, {"heading": "In [ ]: # this contains a list of the image names of our interest;", "text": "# it also ensures that visual and textual attributes are aligned accordingly train _ image _ names = train _ text _ representation ['img _ name'] # the name for visual attributes we use # CNN _ NAME = 'vgg _ net' # CNN _ NAME = 'googlenet' CNN _ NAME = 'fb _ resnet' # the layer in CNN used to extract attributes # PERCEPTION _ LAYER = 'fc7' # PERCEPTION _ LAYER = 'pool5-7x7 _ s1' # PERCEPTION _ LAYER ='res5c-152' # l2 prefix because there are l2-normalized visual attributes PERCEPTION _ LAYER = 'l2 _ res5c-152' train _ visual _ features = dp ['sensation'] (train _ or test = train _ form = image _ parts = No _ extractor _ PLAER _ parts = No image _ parts = No image _ parts = No _ representation _."}, {"heading": "8 Vision+Language", "text": "As we can see in Figure 1, it is difficult to answer questions correctly without seeing images. Let's create an input as a pair of textual and visual features using the following code.In []: train _ input = [train _ x, train _ visual _ features] We will examine two approaches to answering questions: an orderly BOW and an RNN.8.0.15 BOW + VisionSimilar to our blind model, we will begin with a BOW encoding of a question. Here, we will explore two ways to combine both modalities (circle with \"C\" in Figure 7): concatenation and piecemeal multiplication. For simplicity, we will refine the visual representation (dashed line symbolizes the barrier that blocks backpropagation in Figure 7)."}, {"heading": "In [ ]: #== Model definition", "text": "First of all, we define a model with keras / krainofrom keras.models import Sequential from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.core import Layer from keras.layers.core import Merge from keras.layers.core import TimeDistributedMerge from keras.layers.core import TimeDistributedMerge from keras.layers.core import (keras.layers.core import) import TimeDistributedMerge _ from keras.layers.core.core import # image import AbstractSingleAnswer from krainocore.image image image image image modelimage image image _ image _ image _ moctSequo import AbstractSequel _ image _ image _ mod _ image _ image _ mod _ image _ image _ mod _ image _ image _ image _ image _ image _ config.model _ mod _ mod _ mod _ mod _ mod _ image _ image _ image _ image _ image _ image _ image mod _ image mod _ image mod _ image mod _ image mod _ image _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ mod _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ mod _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ image mod _ mod _ image mod _ image mod _ image mod _ image mod _ mod _ image mod _ mod _ mod _ image _ mod _ image mod _ mod _ mod _ mod _ image mod _ mod _ mod _ mod _ image mod _ image mod _ mod _ image _ mod _ mod _ image mod _ mod _ mod _ mod _ image mod _ mod _ mod _ mod _ mod _"}, {"heading": "In [ ]: # dimensionality of embeddings", "text": "EMBEDDING _ DIM = 500 # Type of multimodal fusion (ave, concat, mul, sum) MULTIMODAL _ MERGE _ MODE = 'concat' model _ config = Config (textual _ embedding _ dim = EMBEDDING _ DIM, visual _ embedding _ dim = 0, multimodal _ merge _ mode = MULTIMODAL _ MERGE _ MODE, input _ dim = len (word2index _ x.keys ())), output _ dim = len (word2index _ y.keys (), visual _ dim = train _ visual _ funcures.shape [1]) model = VisionLanguageBOW (model _ config) model.create () model.compile () (loss = 'categorical _ crossentropy', optimizer = 'adam')."}, {"heading": "In [ ]: #== Model definition", "text": "First of all we define a model with keras / kraino from keras.models import Sequential from keras.layers.core import Activation from keras.layers.core import Dense from keras.layers.core import Dropout from keras.layers.core import Layer from keras.layers.core import Merge from keras.layers.core import TimeDistributedMerge from keras.layers.core import Dropout Import Embeddingfrom kraino.core.model _ zoo import AbstractSequentialModel from kraino.core.model _ zoo import AbstractSingleAnswer from kraino.core.model _ zoo import AbstractSequentiales.model _ modeleScript _ modeleScript _.model _.dmodeleScript _ Scriptv _ Scriptmov _ Scriptmomov _ Scriptmov _ Scriptmov _ Scriptmov _ Scriptmov _ Scriptmomov _ Scriptmov _ Script _ Script _ Script _ modeleScript _ Script _ Scripmov _ Scriptmomov _ Scriptmov _ momov _ mov _ Scriptv"}, {"heading": "In [ ]: # dimensionality of embeddings", "text": "EMBEDDING _ DIM = 500 # Type of multimodal fusion (ave, concat, mul, sum) MULTIMODAL _ MERGE _ MODE ='mul' model _ config = Config (textual _ embedding _ dim = EMBEDDDING _ DIM, visual _ embedding _ dim = EMBEDDING _ DIM, multimodal _ merge _ mode = MULTIMODAL _ MERGE _ MODE, input _ dim = len (word2index _ x.keys ()), output _ dim = len (word2index _ y.keys (), visual _ dim = train _ visual _ features.shape [1]) = VisionLanguageBOW (model _ config) model.create () model.compile _ (loss = 'categorical _ crossentropy _ amp', optimizer = 'adam' text _ ons.posion.If it (model _ modu) modelely.create (modelu =.modelu =.modelu = qu)."}, {"heading": "In [ ]: #== Model definition", "text": "\"It is very important for us that we are able to solve the problems of the world,\" he said in an interview with \"Welt am Sonntag\": \"It is very important for us that we are able to solve the problems of the world.\" \"It is very important that we get a grip on the problems of the world,\" he said in an interview with \"Welt am Sonntag.\" (sid) \"It is very important that the world gets a grip on the world,\" he said in an interview with \"Welt am Sonntag.\" (sid) \"It is very important that the world gets a grip on the world.\" (sid) \"It is very important that the world gets a grip on the world,\" he said in an interview with \"Welt am Sonntag.\" It is very important that the world gets a grip on the world, the world in the world of the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the world, the, the world, the world, the world, the world, the, the, the world, the, the world: \"he said in"}, {"heading": "9 New Predictions with Vision+Language", "text": "9.0.18 Predictions (Features) In []: test _ image _ names = test _ text _ representation ['img _ name'] test _ visual _ features = dp ['perception'] (train _ or _ test = 'test', names _ list = test _ image _ names, parts _ extractor = None, max _ parts = None, perception = CNN _ NAME, layer = PERCEPTION _ LAYER, second _ layer = None) test _ visual _ features.shapeIn []: test _ input = [test _ x, test _ visual _ features] 9.0.19 Predictions (Bow with Vision) In []: from kraino.core.model _ zoo import word _ generator # we first need to _ word _ config # (we could have this before, test _ indexindex _ index.19 Predictions (Bow with Vision) index In []: from kraino.could e.model _ zoo import word _ generator # first config we could use indexword as indexed code # index19 before we could do this (indexort)."}, {"heading": "10 VQA", "text": "Consider a recently introduced large dataset called VQA [Antol et al., 2015]. In this section, we train and evaluate VQA models. As the reader should already be familiar with all the parts, we quickly switch to encoding. For convenience, we only use BOW architectures. Since VQA hides the test data for challenge, we use the publicly available validation set to evaluate the architectures.10.0.21 VQA Language Features"}, {"heading": "In [ ]: #TODO: Execute the following procedure (Shift+Enter)", "text": "In fact, most people who are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to move, to move, to move, to move, to move, to move, to move, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to move, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to dance, to"}, {"heading": "11 New Research Opportunities", "text": "The task of testing the machines on the content of the images is a brand new research direction that has recently gained popularity, so there are many possibilities available. We finish the tutorial by pointing out a few possible directions. \u2022 Global Representation In this tutorial we use a global, complete representation of the images. Such representation can destroy too much information. Therefore, there seems to be a fine-grained alternative that uses valid options. Perhaps we should use recognition features, or object suggestions (e.g. Ilievski et al.) use a question, and Mokarian Forooshani et al. [2016] we use object suggestions to provide a visual representation that is fairly successfully converted into images."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Together with the development of more accurate methods in Computer Vision and Natural Language Understanding, holistic architectures that answer on questions about the content of real-world images have emerged. In this tutorial, we build a neural-based approach to answer questions about images. We base our tutorial on two datasets: (mostly on) DAQUAR, and (a bit on) VQA. With small tweaks the models that we present here can achieve a competitive performance on both datasets, in fact, they are among the best methods that use a combination of LSTM with a global, full frame CNN representation of an image. We hope that after reading this tutorial, the reader will be able to use Deep Learning frameworks, such as Keras and introduced Kraino, to build various architectures that will lead to a further performance improvement on this challenging task. 1 Preface In this tutorial1 we build a few architectures that can answer questions about images. The architectures are based on our two papers on this topic: Malinowski et al. [2015] and Malinowski et al. [2016]; and more broadly, on our project towards a Visual Turing Test2. In particular, an encoder-decoder perspective of Malinowski et al. [2016] allows us to effectively experiment with various design choices. For the sake of simplicity, we only consider a classification-based approach to answer questions about images, although an approach that generate answers word-by-word is also studied in the community [Malinowski et al., 2015]. In the tutorial, we mainly focus on the DAQUAR dataset [Malinowski and Fritz, 2014a], but a few possible directions to apply learnt techniques to VQA [Antol et al., 2015] are also pointed. First, we will get familiar with the task of answering questions about images, and a dataset that implements the task (due to a small size, we mainly use DAQUAR as it better serves an educational purpose that we aim at this tutorial). Next, we build a few blind models that answer questions about images without actually seeing such images. Such models already exhibit a reasonable performance as they can effectively learn various biases that exist in a dataset, which we also interpret as learning a common sense knowledge [Malinowski et al., 2015, 2016]. Subsequently, we build a few language+vision models that answer questions based on both a textual and a visual inputs. Finally, we leave the tutorial with a few possible research directions. Technical aspects The tutorial is originally written using Python Notebook, which the reader is welcome to download and use through the tutorial. Instructions necessary to run the Notebook version of this tutorial are provided in the following: https://github.com/mateuszmalinowski/visual_turing_test-tutorial. In this tutorial, we heavily use a Python code, and therefore it is expected the reader either already knows this language, or can quickly learn it. However, we made an effort to make this tutorial approachable to a wider audience. We use Kraino that is a framework prepared for this tutorial in order to simplify the development of the question answering architectures. Under the hood, it uses Theano4 [Bastien et al., 2012] and Keras5 [Chollet, 2015] \u2013 two frameworks to build Deep Learning models. We also use various CNNs representations extracted from images that can be downloaded as explained at the beginning of our Notebook tutorial. We also highlight exercises that a curious reader may attempt to solve. This tutorial was presented for the first time during the 2nd Summer School on Integrating Vision and Language: Deep Learning. http://mpii.de/visual_turing_test https://github.com/mateuszmalinowski/visual_turing_test-tutorial/blob/master/visual_ turing_test.ipynb http://deeplearning.net/software/theano/ https://keras.io 1 ar X iv :1 61 0. 01 07 6v 1 [ cs .C V ] 4 O ct 2 01 6", "creator": "LaTeX with hyperref package"}}}