{"id": "1610.01108", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "4-Oct-2016", "title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "abstract": "In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentence-aligned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios.", "histories": [["v1", "Tue, 4 Oct 2016 17:52:42 GMT  (137kb,D)", "http://arxiv.org/abs/1610.01108v1", null], ["v2", "Tue, 11 Oct 2016 07:35:47 GMT  (146kb,D)", "http://arxiv.org/abs/1610.01108v2", null], ["v3", "Wed, 30 Nov 2016 18:37:18 GMT  (147kb,D)", "http://arxiv.org/abs/1610.01108v3", "Accepted for presentation at IWSLT 2016, Seattle"]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["marcin junczys-dowmunt", "tomasz dwojak", "hieu hoang"], "accepted": false, "id": "1610.01108"}, "pdf": {"name": "1610.01108.pdf", "metadata": {"source": "META", "title": "Is Neural Machine Translation Ready for Deployment? A Case Study on 30 Translation Directions", "authors": ["Marcin Junczys-Dowmunt", "Tomasz Dwojak", "Hieu Hoang"], "emails": ["junczys@amu.edu.pl", "t.dwojak@amu.edu.pl", "hieu@hoang.co.uk"], "sections": [{"heading": "1 Introduction", "text": "Ziemski et al. (2016) have recently published the United Nations Parallel Corpus v1.0. which contains a sub-corpus of about 11M sentences, fully translated into six languages (Arabic, Chinese, English, French, Russian, and Spanish) and includes official development and testing kits, making it an ideal resource for multi-language experiments. It is also a compelling example of use for indomain translations with large bilingual in-house resources. We provide BLEU values for the entire translation matrix for all official languages from the fully aligned sub-corpus. We also introduce AmuNMT1, our efficient neural machine translation decoder, and daemon 1https: / / github.com / emjotde / amunmtstrate that the current configuration would already be available instead of a multiplier for translating Moses in GU."}, {"heading": "2 Training data", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 The UN corpus", "text": "The United Nations Parallel Corpus v1.0 (Ziemski et al., 2016) consists of humanly translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages Arabic, Chinese, English, French, Russian and Spanish. Apart from the pairs of documents, a fully identical sub-corpus is distributed for the six official UN languages. This sub-corpus consists of sentences that uniformly match the English primary documents in all languages. Statistics for the data are provided below: documents published in 2015 (except the main corpus) were used to create official development and test kits for machine translation tasks. Development data were randomly selected from documents published in the first quarter of 2015, and test data were selected from the second quarter of 2015. Both sentences comprise 4,000 sentences that are 1: 1-1 matches in all official languages. As in the case of the fully identical sub-corpus, any translation can be evaluated."}, {"heading": "2.2 Preprocessing", "text": "Sentences longer than 100 words were discarded. We lowered the training data as in Ziemski et al. (2016); the data were tokenizedar Xiv: 161 0.01 108v 1 [cs.C L] 4O ct2 016 with the Moses tokenizer. For Chinese segmentation, we used Jieba2 before using the Moses tokenizer."}, {"heading": "2.3 Subword units", "text": "To avoid the major vocabulary problem in NMT models (Luong et al., 2015), we use byte pair coding (BPE) to achieve an open vocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2015). For all languages, we set the number of subword units to 30,000. Subword units are segmented after every other pre-processing step. During evaluation, subwords are linked to complete tokens."}, {"heading": "3 Phrase-based SMT baselines", "text": "In Ziemski et al. (2016), we have provided BLEU baseline values for Moses configurations (Koehn et al., 2007) trained on the 6-way subcorpus. We repeat the system description here: To accelerate word alignment, we divide the training corpus into four equal-sized parts trained on MGIZA + + (Gao and Vogel, 2008), running 5 iterations of the Model 1 and the HMM model on each part.3 We use a 5-gram language model trained from the parallel target data, truncating 3 grams or a higher order if they occur only once. Apart from the standard configuration with a lexical conversion model, we add a 5-gram operation sequence model (Durrani et al., 2013) (all n-grams printed if they occur only once) and a 9-gram word class model (2g of the language groups) in 2013."}, {"heading": "4 Neural translation systems", "text": "The neural machine translation system is an attentive encoder decoder (Bahdanau et al., 2014) trained with Nematus. We used mini-batches of size 40, a maximum sentence length of 100, word embedding of size 500, and hidden layers of size 1024. We cut the gradient standard to 1.0 (Pascanu et al., 2013). Models were trained with Adadelta (Zeiler, 2012), which reshuffles the training corpus between epochs. Models were trained for 1.2 million iterations (one iteration is equivalent to a mini-batch), saving all 30,000 iterations. On our NVidia GTX 1080, this corresponds to approximately 4 epochs and 8 days of training time. Models with English as source or target date were trained for a further 1.2 million iterations (another 2 epochs, 8 days) to test the impact of the extended training time."}, {"heading": "5 Phrase-based vs. NMT \u2013 full matrix", "text": "In Figure 1, we present the results for all thirty language pairs in the United Nations parallel corpus for the officially included test set. Here, we compare with NMT models trained for 4 epochs or 1.2 million iterations. With the exception of fr-es and ru-en, the neural system is always comparable or better than the phrase-based system. The differences in which NMT is worse are very small. Especially, in cases where Chinese is one of the languages in a language pair, the improvement in NMT over PB-SMT is dramatic with 7 to 9 BLEU points. This seems to confirm the big gains Wu et al. (2016) report for Chinese-English. However, it also seems to suggest that this is a feature of the language pair, not so much of its composition. We also see big improvements in translations from and into Arabic. At this point, it should be noted that no special pre-processing was applied for Arabic."}, {"heading": "6 Phrase-based vs. Hiero vs. NMT \u2013 language pairs with English", "text": "The particularly impressive results for each Chinese-related translation direction prompted us to experiment with hierarchical phrase-based machine translation (Hiero) as well. Hiero confirmed that it is better for the Chinese-English language pair than phrase-based SMT. We also decided to extend our experiment to all language pairs that include English, as these are the main translation guidelines with which the United Nations works. For these ten translation directions, we created a hierarchical PB-SMT system with the same pre-processing settings as the PB-SMT system. We also continued to train our neural systems for another four eras or 1.2 million iterations (a total of 2.4 million), increasing the training time per neural system to a total of 16 days. Figure 2 summarizes the results better than PB-SMT as expected."}, {"heading": "7 Efficient decoding with AmuNMT", "text": "AmuNMT4 is a basic implementation of the neural MT toolkit developed in C + +. It currently consists of an efficient beam search engine for models trained with Nematus (Sennrich et al., 2016; Bahdanau et al., 2014). We focused mainly on efficiency and ease of use. Characteristics of the AmuNMT decoder include: \u2022 low latency CPU-based decoding with intra-sentence multithreading (one sentence uses multiple threads during matrix operations) and sentence threads (different sentences are decrypted in different threads); \u2022 Multi-GPU support for GPU translation; \u2022 full compatibility with NMT models trained with Nematus (Sennrich et al., 2016); \u2022 similarity of similar models; \u2022 Memorization of models with the same initial vocabulary but different inputs."}, {"heading": "7.1 Beam size vs. Speed and Quality", "text": "The beam size has a great influence on the decoding speed. In Figure 3 we show the influence of the beam size on the decoding speed (in words per second) and the translation quality (in BLEU) for the English-French model. The English part of the UN test set consists of approx. 120,000 tokens, the total test sets have been translated for each experiment. Beam sizes above 5-7, as can be seen, do not lead to significant improvements, as the quality for a beam size of 5 is only 0.1 BLEU below the maximum. However, the decoding speed is significantly slower. Therefore, we choose a beam size of 5 for our next experiments."}, {"heading": "7.2 Comparison of AmunNMT to Moses and Nematus", "text": "Although the models from Wu et al. (2016) are more complex than our own, we treat the reported numbers as a reference for ready-to-use performance for NMT. We conducted our experiments on an Intel Xeon E52620 2.40 GHz server with four NVIDIA GeForce GTX 1080 GPUs. Phrase-based parameters are described in Section 3, which is guided by best practices to achieve a reasonable speed over a high-quality trade-off (Pouliquen et al., 2013). Neural MT models are described as in the previous section. We present the word-per-second ratio for our NMT models with AmunNMT and Nematus, executed on the CPU and GPU, Figure 4b. For the CPU version, we use 16 threads, translating one sentence per thread."}, {"heading": "7.3 Low-latency translation", "text": "So far, we have evaluated the mass throughput, which is equivalent to a high translation load in settings with long documents that need to be translated continuously, and the translation speed has been averaged across entire documents and multiple cores. However, there are settings where the latency for a single sentence can be more important, such as predictive translation (Knowles and Koehn, 2016). Many other settings will fall between these extremes - low-latency translation and mass translation - such as the use of MT in CAT systems, another instance of interactive translation. To compare the latency per sentence, we translate our test sentence with all tools in an order and use at most one CPU thread or process. We do not seek full GPU saturation, as this would not improve latency."}, {"heading": "8 Conclusions", "text": "We have evaluated the performance of neural machine translation on all thirty translation directions of the United Nations Parallel Corpus v1. We have shown that for almost all translation directions, NMT is either on par with or above phrase-based SMT. There does not appear to be a risk of potential quality loss due to the switch to NMT. For some language pairs, the gains are substantial, as measured by BLEU, including all pairs that have Chinese as the source or target language. Very respectable gains can also be observed for Arabic. However, for other language pairs, in general, there are at least some improvements. Although NMT systems are known to generalize better than phrase-based systems for out-of-domain data, it was unclear how they operate in a purely domain-based environment that is of interest to any major organization with significant resources of its own data, such as the UN or other government agencies. Although NMT systems are known to calculate performance on pseudo-based domain data, such as domain-based systems for instance, it is unclear for larger domain-based systems such as out-of-domain."}, {"heading": "Acknowledgments", "text": "This project was funded within the framework of the research and innovation programme Horizon 2020 of the European Union under the funding agreement 688139 (SUMMA)."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio."], "venue": "CoRR, abs/1409.0473.", "citeRegEx": "Bahdanau et al\\.,? 2014", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Can Markov models over minimal translation units help phrasebased SMT? In ACL, pages 399\u2013405", "author": ["Nadir Durrani", "Alexander Fraser", "Helmut Schmid", "Hieu Hoang", "Philipp Koehn."], "venue": "The Association for Computer Linguistics.", "citeRegEx": "Durrani et al\\.,? 2013", "shortCiteRegEx": "Durrani et al\\.", "year": 2013}, {"title": "Parallel implementations of word alignment tool", "author": ["Qin Gao", "Stephan Vogel."], "venue": "Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49\u201357. ACL.", "citeRegEx": "Gao and Vogel.,? 2008", "shortCiteRegEx": "Gao and Vogel.", "year": 2008}, {"title": "Scalable modified Kneser-Ney language model estimation", "author": ["Kenneth Heafield", "Ivan Pouzyrevsky", "Jonathan H. Clark", "Philipp Koehn."], "venue": "Proceedings of the 51st Annual Meeting of the ACL, pages 690\u2013696.", "citeRegEx": "Heafield et al\\.,? 2013", "shortCiteRegEx": "Heafield et al\\.", "year": 2013}, {"title": "Improving translation quality by discarding most of the phrasetable", "author": ["J Howard Johnson", "Joel Martin", "George Forst", "Roland Kuhn."], "venue": "Proceedings of EMNLP-CoNLL\u201907, pages 967\u2013975.", "citeRegEx": "Johnson et al\\.,? 2007", "shortCiteRegEx": "Johnson et al\\.", "year": 2007}, {"title": "Log-linear combinations of monolingual and bilingual neural machine translation models for automatic post-editing", "author": ["Marcin Junczys-Dowmunt", "Roman Grundkiewicz."], "venue": "CoRR, abs/1605.04800.", "citeRegEx": "Junczys.Dowmunt and Grundkiewicz.,? 2016", "shortCiteRegEx": "Junczys.Dowmunt and Grundkiewicz.", "year": 2016}, {"title": "Phrasal RankEncoding: Exploiting phrase redundancy and translational relations for phrase table compression", "author": ["Marcin Junczys-Dowmunt."], "venue": "Prague Bull. Math. Linguistics, 98:63\u201374.", "citeRegEx": "Junczys.Dowmunt.,? 2012", "shortCiteRegEx": "Junczys.Dowmunt.", "year": 2012}, {"title": "Neural interactive translation prediction", "author": ["Rebecca Knowles", "Philipp Koehn."], "venue": "Proceedings of AMTA 2016, Austin, USA, October.", "citeRegEx": "Knowles and Koehn.,? 2016", "shortCiteRegEx": "Knowles and Koehn.", "year": 2016}, {"title": "Addressing the rare word problem in neural machine translation", "author": ["Thang Luong", "Ilya Sutskever", "Quoc V. Le", "Oriol Vinyals", "Wojciech Zaremba."], "venue": "ACL.", "citeRegEx": "Luong et al\\.,? 2015", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "CoRR, abs/1301.3781.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "On the difficulty of training recurrent neural networks", "author": ["Razvan Pascanu", "Tomas Mikolov", "Yoshua Bengio."], "venue": "Proceedings of the 30th International Conference on Machine Learning, ICML 2013, pages 1310\u20131318, , Atlanta, GA, USA.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "Large-scale multiple language translation accelerator at the United Nations", "author": ["Bruno Pouliquen", "Cecilia Elizalde", "Marcin JunczysDowmunt", "Christophe Mazenc", "Jos\u00e9 Garc\u0131\u0301aVerdugo"], "venue": "In MTSummit XIV,", "citeRegEx": "Pouliquen et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Pouliquen et al\\.", "year": 2013}, {"title": "Neural machine translation of rare words with subword units", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "CoRR, abs/1508.07909.", "citeRegEx": "Sennrich et al\\.,? 2015", "shortCiteRegEx": "Sennrich et al\\.", "year": 2015}, {"title": "Edinburgh neural machine translation systems for WMT 16", "author": ["Rico Sennrich", "Barry Haddow", "Alexandra Birch."], "venue": "Proceedings of the First Conference on Machine Translation, WMT 2016, colocated with ACL 2016, August 11-12, Berlin, Ger-", "citeRegEx": "Sennrich et al\\.,? 2016", "shortCiteRegEx": "Sennrich et al\\.", "year": 2016}, {"title": "ADADELTA: An Adaptive Learning Rate Method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "The united nations parallel corpus v1.0", "author": ["Micha\u0142 Ziemski", "Marcin Junczys-Dowmunt", "Bruno Pouliquen"], "venue": null, "citeRegEx": "Ziemski et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ziemski et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 15, "context": "Ziemski et al. (2016) recently published the United Nations Parallel Corpus v1.", "startOffset": 0, "endOffset": 22}, {"referenceID": 15, "context": "0 (Ziemski et al., 2016) consists of human translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish.", "startOffset": 2, "endOffset": 24}, {"referenceID": 15, "context": "We lowercased the training data as it was done in Ziemski et al. (2016); the data was tokenized ar X iv :1 61 0.", "startOffset": 50, "endOffset": 72}, {"referenceID": 8, "context": "To avoid the large-vocabulary problem in NMT models (Luong et al., 2015), we use byte-pairencoding (BPE) to achieve open-vocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al.", "startOffset": 52, "endOffset": 72}, {"referenceID": 12, "context": ", 2015), we use byte-pairencoding (BPE) to achieve open-vocabulary translation with a fixed vocabulary of subword symbols (Sennrich et al., 2015).", "startOffset": 122, "endOffset": 145}, {"referenceID": 2, "context": "We repeat the system description here: To speed up the word alignment procedure, we split the training corpora into four equally sized parts that are aligned with MGIZA++ (Gao and Vogel, 2008), running 5 iterations of Model 1 and the HMM model on each part.", "startOffset": 171, "endOffset": 192}, {"referenceID": 1, "context": "Apart from the default configuration with a lexical reordering model, we add a 5-gram operation sequence model (Durrani et al., 2013) (all n-grams pruned if they occur only once) and a 9-gram word-class language model with word-classes produced by word2vec (Mikolov et al.", "startOffset": 111, "endOffset": 133}, {"referenceID": 9, "context": ", 2013) (all n-grams pruned if they occur only once) and a 9-gram word-class language model with word-classes produced by word2vec (Mikolov et al., 2013) (3-grams and 4-grams are pruned if they occur only once, 5-grams and 6-grams if they occur only twice, etc.", "startOffset": 131, "endOffset": 153}, {"referenceID": 3, "context": "), both trained using KenLM (Heafield et al., 2013).", "startOffset": 28, "endOffset": 51}, {"referenceID": 4, "context": "To reduce the phrasetable size, we apply significance pruning (Johnson et al., 2007) and use the compact phrase-table and reordering data structures (Junczys-Dowmunt, 2012).", "startOffset": 62, "endOffset": 84}, {"referenceID": 6, "context": ", 2007) and use the compact phrase-table and reordering data structures (Junczys-Dowmunt, 2012).", "startOffset": 72, "endOffset": 95}, {"referenceID": 9, "context": "In Ziemski et al. (2016), we provided baseline BLEU scores for Moses (Koehn et al.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "The neural machine translation system is an attentional encoder-decoder (Bahdanau et al., 2014), which has been trained with Nematus.", "startOffset": 72, "endOffset": 95}, {"referenceID": 10, "context": "0 (Pascanu et al., 2013).", "startOffset": 2, "endOffset": 24}, {"referenceID": 14, "context": "Models were trained with Adadelta (Zeiler, 2012), reshuffling the training corpus between epochs.", "startOffset": 34, "endOffset": 48}, {"referenceID": 13, "context": "It currently consist of an efficient beam-search inference engine for models trained with Nematus (Sennrich et al., 2016; Bahdanau et al., 2014).", "startOffset": 98, "endOffset": 144}, {"referenceID": 0, "context": "It currently consist of an efficient beam-search inference engine for models trained with Nematus (Sennrich et al., 2016; Bahdanau et al., 2014).", "startOffset": 98, "endOffset": 144}, {"referenceID": 13, "context": "\u2022 Low-latency CPU-based decoding with intra-sentence multi-threading (one sentence makes use of multiple threads during matrix operations) and sentence-wise threads (different sentences are decoded in different threads); \u2022 Multi-GPU support for sentence-wise translation per GPU; \u2022 Full compatibility with NMT models trained with Nematus (Sennrich et al., 2016); \u2022 Ensembling of similar models; \u2022 Ensembling of models with the same output vocabulary but different inputs.", "startOffset": 338, "endOffset": 361}, {"referenceID": 5, "context": "com/emjotde/amunmt similar configuration was used in the winning automatic post-editing shared task at WMT2016 (Junczys-Dowmunt and Grundkiewicz, 2016).", "startOffset": 111, "endOffset": 151}, {"referenceID": 12, "context": "\u2022 YAML vocabularies and configuration files; \u2022 Integrated segmentation into subword units and de-segmentation (Sennrich et al., 2015); \u2022 A clean and documented C++ code-base.", "startOffset": 110, "endOffset": 133}, {"referenceID": 11, "context": "quality trade-off (Pouliquen et al., 2013).", "startOffset": 18, "endOffset": 42}, {"referenceID": 7, "context": "However, there are settings where the latency for a single sentence may be more important, one example would be predictive translation (Knowles and Koehn, 2016).", "startOffset": 135, "endOffset": 160}], "year": 2016, "abstractText": "In this paper we provide the largest published comparison of translation quality for phrase-based SMT and neural machine translation across 30 translation directions. For ten directions we also include hierarchical phrase-based MT. Experiments are performed for the recently published United Nations Parallel Corpus v1.0 and its large six-way sentencealigned subcorpus. In the second part of the paper we investigate aspects of translation speed, introducing AmuNMT, our efficient neural machine translation decoder. We demonstrate that current neural machine translation could already be used for in-production systems when comparing words-per-second ratios.", "creator": "LaTeX with hyperref package"}}}