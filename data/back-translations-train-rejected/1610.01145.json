{"id": "1610.01145", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Error bounds for approximations with deep ReLU networks", "abstract": "We study how approximation errors of neural networks with ReLU activation functions depend on the depth of the network. We establish rigorous error bounds showing that deep ReLU networks are significantly more expressive than shallow ones as long as approximations of smooth functions are concerned. At the same time, we show that on a set of functions constrained only by their degree of smoothness, a ReLU network architecture cannot in general achieve approximation accuracy with better than a power law dependence on the network size, regardless of its depth.", "histories": [["v1", "Mon, 3 Oct 2016 23:08:22 GMT  (134kb,D)", "https://arxiv.org/abs/1610.01145v1", null], ["v2", "Mon, 7 Nov 2016 16:57:35 GMT  (162kb,D)", "http://arxiv.org/abs/1610.01145v2", "16 pages; Theorem 3 added in v2"], ["v3", "Mon, 1 May 2017 14:01:32 GMT  (314kb,D)", "http://arxiv.org/abs/1610.01145v3", "31 pages; major revision in v3; submitted to Neural Networks"]], "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["dmitry yarotsky"], "accepted": false, "id": "1610.01145"}, "pdf": {"name": "1610.01145.pdf", "metadata": {"source": "CRF", "title": "Error bounds for approximations with deep ReLU networks", "authors": ["Dmitry Yarotsky"], "emails": ["d.yarotsky@skoltech.ru"], "sections": [{"heading": "1 Introduction", "text": "Recently, there have been several successful applications of deep neural networks to pattern recognition problems (Schmidhuber [2015], LeCun et al. [2015], we have revived active interest in the theoretical properties of such networks, particularly their expressiveness. It has been argued that deep networks can be more meaningful than flat networks of comparable size (see, for example, Delalleau and Bengio [2011], Raghu et al. [2016], Montufar et al. [2014], Bianchini and Scarselli [2014], Telgarsky [2015])). Unlike a flat network, a deep network can be considered as a long sequence of non-commutative transformations that represent a natural environment for high expressiveness (cf. the well-known Solovay-Kitaev theorem on rapid approximation of arbitrary quantum operations through sequences of non-commutative gates, see Kitaev et al."}, {"heading": "2 The ReLU network model", "text": "The network consists of several input units, an output unit and a number of \"hidden\" computing units. Each hidden unit performs an operation of the formy unit, but without nonlinearity, i.e. it calculates y = 1 wkxk + b) with some weights (adjustable parameters) (wk) N k = 1 and b depending on the unit. The output unit is also a computing unit, but without nonlinearity, i.e. it calculates y = 1 wkxk + b. The units are grouped in layers, and the inputs (xk) N k = 1 of a computing unit in a given layer are outputs of some of the preceding layers (see Fig. 1)."}, {"heading": "3 Upper bounds", "text": "Throughout the essay we will be interested in approximating the functions f: [0, 1] d \u2192 R by ReLU networks. Given the function f: [0, 1] d \u2192 R and its approximation f \u0442, by approximation error we always mean the uniform maximum error between f and f = max x [0,1] d | f (x) \u2212 f (x) |."}, {"heading": "3.1 Fast deep approximation of squaring and multiplication", "text": "Our first key result shows that ReLU networks with unrestricted depth very efficiently use the function f (x) = 2 m = 2 sqm = 2 sqm (more efficient than any other network with fixed depth, as we will see in Section 4.4). Our design uses the \"Sawtooth\" function, which has previously appeared in the thesis Telgarsky [2015]. Function f (x) = 2 on the segment [0, 1] can handle any error [0, 1] through a ReLU network with the depth and number of weight and calculation units O (ln (1 /). Proof. Consider the \"tooth\" (or \"mirror\") function g: [0, 1], g (x) = {2, x < 12 (1 \u2212 x), x (1 \u2212 2), and the iterated functions gs (x)."}, {"heading": "3.2 Fast deep approximation of general smooth functions", "text": "To formulate our general result, let us consider the functional ranges of Wn, [0, 1] d) with n = 1, 2,. - Remember that Wn, [0, 1] d) can be defined as the space of functions on [0, 1] d, which is in L \u00b2 together with their weak derivatives (1). - The norm in Wn, [0, 1] d) can be defined by [0, 1] d (0,1] d) d) = max n: | n | n, n [1] d [0,1] d [0,1] d (x) |, where n = (n1,., nd)."}, {"heading": "3.3 Faster approximations using adaptive network architectures", "text": "We expect, of course, that this would reduce the complexity of the resulting architectures in general (at the cost of the need to find the appropriate architecture).In this section we show that we can actually achieve better upper limits in this scenario. For simplicity, we will consider only the case d = n = 1. Then, Wn [0, 1] d is the space of the Lipschitz functions on the segment [0, 1].The set F1,1 consists of functions f that include both f and f the Lipschitz constant. Theorem 1 provides an upper bound O (ln) for the number of weights and compression units, but in this case it is better."}, {"heading": "4 Lower bounds", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Continuous nonlinear widths", "text": "The method of continuous nonlinear widths (DeVore et al. [1989]) is a very general approach to the analysis of parameterized nonlinear approximations, based on the assumption of a continuous selection of their parameters. We are interested in the following lower limit for the complexity of approximations in Wn, \u221e ([0, 1] d).Theorem 3 (DeVore et al. [1989], theorem 4.2).Fix d, n. Let W be a positive integer and make sure that RW \u2192 C ([0, 1] d) will be some correlation between space RW and space C (0, 1] d).Suppose that there is a continuous map M: Fd, n \u2192 RW so that there is a positive integer (M (f))."}, {"heading": "4.2 Bounds based on VC-dimension", "text": "In this section, we will consider the constellation in which the same network architecture is used to perform all functions f > Fd, n, but the dependence of weights on f is not necessarily assumed to be continuous. In this setup, some lower limits on network complexity can be achieved as a result of existing upper limits on the VC dimension of networks with partial polynomial activation and the Boolean outputs (Anthony and Bartlett [2009]). In the next theorem, part a) is a more general but weaker limit, while part b) is more strongly bound assuming limited growth of the network depth.Theorem 4. Fix d, n.a) For any other type of network architecture that is able to approach any function f, n with error must have at least c \u2212 d / (2n) weights, with any constant c = 0.b)."}, {"heading": "4.3 Adaptive network architectures", "text": "Our goal in this section is to obtain a lower limit for approximate complexity in the scenario in which the network architecture may depend on the approximate function. (This lower limit is thus a counterpart to the upper limit of Section 3.3.To specify this result, we define the complexity of the approximate function f (f,) so that N (f,) is not o (\u2212 d / (9n) as the minimum number of hidden units in a ReLU network that provides such an approximate number of approximate units. (\u2212 d) The proof is based on the following lemma.Lemma 3. Fix d, n. For all sufficiently small > 0, there is f-Fd, n such that N (f,) n is N (8n), with some constant c1 = c1 (d, n).Proof."}, {"heading": "4.4 Slow approximation of smooth functions by shallow networks", "text": "In this section, we show that, unlike deep ReLU networks, flat ReLU networks are relatively inefficient and not linear enough (C2).We note that Liang and Srikant can produce a similar result in 2016 if global convexity is replaced by smoothness and nonlinearity (C2).We note that f & # 252; for each fixed L & # 160; d can be a nonlinear function (i.e., not the form f & # 160; x1,., xd & # 160; s overall f & # 252; for each fixed L & # 160; d, a deeper L & # 160; reLU network, the f & # 160; f & # 160; f & # 160; r errors (0, 1) at least c & # 160; 1 (L & # 160; 2)."}, {"heading": "5 Discussion", "text": "We discuss some implications of the obtained limits. Deep vs. shallow ReLU approximations of smooth functions. Our results clearly show that deep ReLU networks can be expressed more efficiently than flat ReLU networks. However, from theorem 1, functions from the Sobolev space Wn, \u221e ([0, 1] d) cannot be limited by ReLU networks with depth O (ln) and the number of computational units O (\u2212 d / n ln (1 /)). In contrast, a nonlinear function of C2 ([0, 1] d) is not by a ReLU network of fixed depth L with the number of units less than c \u2212 1 / (2). In particular, it follows that in relation to the required number of compilation units unlimited approximations of the functions of Wn in which we (0, 1] d are asymptotically more efficient than approximations with a fixed depth."}, {"heading": "Acknowledgments", "text": "The author thanks Matus Telgarsky and the anonymous arbitrators for the numerous helpful comments on the preliminary versions of the paper, which was funded by the Skolkovo Institute of Science and Technology."}], "references": [{"title": "Neural network learning: Theoretical foundations", "author": ["Martin Anthony", "Peter L Bartlett"], "venue": "Cambridge university press,", "citeRegEx": "Anthony and Bartlett.,? \\Q2009\\E", "shortCiteRegEx": "Anthony and Bartlett.", "year": 2009}, {"title": "Almost linear VC-dimension bounds for piecewise polynomial networks", "author": ["Peter L Bartlett", "Vitaly Maiorov", "Ron Meir"], "venue": "Neural computation,", "citeRegEx": "Bartlett et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Bartlett et al\\.", "year": 1998}, {"title": "On the complexity of neural network classifiers: A comparison between shallow and deep architectures", "author": ["Monica Bianchini", "Franco Scarselli"], "venue": "IEEE transactions on neural networks and learning systems,", "citeRegEx": "Bianchini and Scarselli.,? \\Q2014\\E", "shortCiteRegEx": "Bianchini and Scarselli.", "year": 2014}, {"title": "Why deep neural networks", "author": ["Nadav Cohen", "Or Sharir", "Amnon Shashua"], "venue": "arXiv preprint arXiv:1509.05009,", "citeRegEx": "Cohen et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cohen et al\\.", "year": 2015}, {"title": "The Solovay-Kitaev algorithm", "author": ["CM Dawson", "MA Nielsen"], "venue": "Quantum Information and Computation,", "citeRegEx": "Dawson and Nielsen.,? \\Q2006\\E", "shortCiteRegEx": "Dawson and Nielsen.", "year": 2006}, {"title": "Shallow vs. deep sum-product networks", "author": ["Olivier Delalleau", "Yoshua Bengio"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Delalleau and Bengio.,? \\Q2011\\E", "shortCiteRegEx": "Delalleau and Bengio.", "year": 2011}, {"title": "Optimal nonlinear approximation", "author": ["Ronald A DeVore", "Ralph Howard", "Charles Micchelli"], "venue": "Manuscripta mathematica,", "citeRegEx": "DeVore et al\\.,? \\Q1989\\E", "shortCiteRegEx": "DeVore et al\\.", "year": 1989}, {"title": "Bounding the Vapnik-Chervonenkis dimension of concept classes parameterized by real numbers", "author": ["Paul W Goldberg", "Mark R Jerrum"], "venue": "Machine Learning,", "citeRegEx": "Goldberg and Jerrum.,? \\Q1995\\E", "shortCiteRegEx": "Goldberg and Jerrum.", "year": 1995}, {"title": "Approximation by neural networks is not", "author": ["Paul C Kainen", "V\u011bra K\u016frkov\u00e1", "Andrew Vogt"], "venue": "continuous. Neurocomputing,", "citeRegEx": "Kainen et al\\.,? \\Q1999\\E", "shortCiteRegEx": "Kainen et al\\.", "year": 1999}, {"title": "Efficient distribution-free learning of probabilistic concepts", "author": ["Michael J Kearns", "Robert E Schapire"], "venue": "In Foundations of Computer Science,", "citeRegEx": "Kearns and Schapire.,? \\Q1990\\E", "shortCiteRegEx": "Kearns and Schapire.", "year": 1990}, {"title": "Classical and Quantum Computation", "author": ["A. Yu. Kitaev", "A.H. Shen", "M.N. Vyalyi"], "venue": "American Mathematical Society,", "citeRegEx": "Kitaev et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Kitaev et al\\.", "year": 2002}, {"title": "Why deep neural networks", "author": ["Shiyu Liang", "R. Srikant"], "venue": "arXiv preprint arXiv:1610.04161,", "citeRegEx": "Liang and Srikant.,? \\Q2016\\E", "shortCiteRegEx": "Liang and Srikant.", "year": 2016}, {"title": "On the near optimality of the stochastic approximation of smooth functions by neural networks", "author": ["VE Maiorov", "Ron Meir"], "venue": "Advances in Computational Mathematics,", "citeRegEx": "Maiorov and Meir.,? \\Q2000\\E", "shortCiteRegEx": "Maiorov and Meir.", "year": 2000}, {"title": "Neural networks for optimal approximation of smooth and analytic functions", "author": ["HN Mhaskar"], "venue": "Neural Computation,", "citeRegEx": "Mhaskar.,? \\Q1996\\E", "shortCiteRegEx": "Mhaskar.", "year": 1996}, {"title": "Deep vs. shallow networks: An approximation theory perspective", "author": ["Hrushikesh Mhaskar", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1608.03287,", "citeRegEx": "Mhaskar and Poggio.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar and Poggio.", "year": 2016}, {"title": "Learning real and boolean functions: When is deep better than shallow", "author": ["Hrushikesh Mhaskar", "Qianli Liao", "Tomaso Poggio"], "venue": "arXiv preprint arXiv:1603.00988,", "citeRegEx": "Mhaskar et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mhaskar et al\\.", "year": 2016}, {"title": "On the number of linear regions of deep neural networks", "author": ["Guido F Montufar", "Razvan Pascanu", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Montufar et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Montufar et al\\.", "year": 2014}, {"title": "Approximation theory of the MLP model in neural networks", "author": ["Allan Pinkus"], "venue": "Acta Numerica,", "citeRegEx": "Pinkus.,? \\Q1999\\E", "shortCiteRegEx": "Pinkus.", "year": 1999}, {"title": "On the expressive power of deep neural networks", "author": ["Maithra Raghu", "Ben Poole", "Jon Kleinberg", "Surya Ganguli", "Jascha Sohl-Dickstein"], "venue": "arXiv preprint arXiv:1606.05336,", "citeRegEx": "Raghu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Raghu et al\\.", "year": 2016}, {"title": "Tight Bounds for the VC-Dimension of Piecewise Polynomial Networks", "author": ["Akito Sakurai"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Sakurai.,? \\Q1999\\E", "shortCiteRegEx": "Sakurai.", "year": 1999}, {"title": "Deep learning in neural networks: An overview", "author": ["J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Schmidhuber.,? \\Q2015\\E", "shortCiteRegEx": "Schmidhuber.", "year": 2015}, {"title": "The synthesis of two-terminal switching circuits", "author": ["Claude Shannon"], "venue": "Bell Labs Technical Journal,", "citeRegEx": "Shannon.,? \\Q1949\\E", "shortCiteRegEx": "Shannon.", "year": 1949}, {"title": "Representation benefits of deep feedforward networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1509.08101,", "citeRegEx": "Telgarsky.,? \\Q2015\\E", "shortCiteRegEx": "Telgarsky.", "year": 2015}, {"title": "Benefits of depth in neural networks", "author": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "citeRegEx": "Telgarsky.,? \\Q2016\\E", "shortCiteRegEx": "Telgarsky.", "year": 2016}, {"title": "On the uniform convergence of relative frequencies of events to their probabilities", "author": ["Vladimir N Vapnik", "A Ya Chervonenkis"], "venue": "In Measures of Complexity,", "citeRegEx": "Vapnik and Chervonenkis.,? \\Q2015\\E", "shortCiteRegEx": "Vapnik and Chervonenkis.", "year": 2015}, {"title": "Lower bounds for approximation by nonlinear manifolds", "author": ["Hugh E Warren"], "venue": "Transactions of the American Mathematical Society,", "citeRegEx": "Warren.,? \\Q1968\\E", "shortCiteRegEx": "Warren.", "year": 1968}], "referenceMentions": [{"referenceID": 14, "context": "Recently, multiple successful applications of deep neural networks to pattern recognition problems (Schmidhuber [2015], LeCun et al.", "startOffset": 100, "endOffset": 119}, {"referenceID": 14, "context": "Recently, multiple successful applications of deep neural networks to pattern recognition problems (Schmidhuber [2015], LeCun et al. [2015]) have revived active interest in theoretical properties of such networks, in particular their expressive power.", "startOffset": 100, "endOffset": 140}, {"referenceID": 3, "context": ", Delalleau and Bengio [2011], Raghu et al.", "startOffset": 2, "endOffset": 30}, {"referenceID": 3, "context": ", Delalleau and Bengio [2011], Raghu et al. [2016], Montufar et al.", "startOffset": 2, "endOffset": 51}, {"referenceID": 3, "context": ", Delalleau and Bengio [2011], Raghu et al. [2016], Montufar et al. [2014], Bianchini and Scarselli [2014], Telgarsky [2015]).", "startOffset": 2, "endOffset": 75}, {"referenceID": 2, "context": "[2014], Bianchini and Scarselli [2014], Telgarsky [2015]).", "startOffset": 8, "endOffset": 39}, {"referenceID": 2, "context": "[2014], Bianchini and Scarselli [2014], Telgarsky [2015]).", "startOffset": 8, "endOffset": 57}, {"referenceID": 2, "context": "[2014], Bianchini and Scarselli [2014], Telgarsky [2015]). In contrast to a shallow network, a deep one can be viewed as a long sequence of non-commutative transformations, which is a natural setting for high expressiveness (cf. the well-known Solovay-Kitaev theorem on fast approximation of arbitrary quantum operations by sequences of non-commutative gates, see Kitaev et al. [2002], Dawson and Nielsen [2006]).", "startOffset": 8, "endOffset": 385}, {"referenceID": 2, "context": "[2014], Bianchini and Scarselli [2014], Telgarsky [2015]). In contrast to a shallow network, a deep one can be viewed as a long sequence of non-commutative transformations, which is a natural setting for high expressiveness (cf. the well-known Solovay-Kitaev theorem on fast approximation of arbitrary quantum operations by sequences of non-commutative gates, see Kitaev et al. [2002], Dawson and Nielsen [2006]).", "startOffset": 8, "endOffset": 412}, {"referenceID": 0, "context": "Bianchini and Scarselli 2014 give bounds for Betti numbers characterizing topological properties of functions represented by networks. Telgarsky 2015, 2016 provides specific examples of classification problems where deep networks are provably more efficient than shallow ones. In the context of classification problems, a general and standard approach to characterizing expressiveness is based on the notion of the Vapnik-Chervonenkis dimension (Vapnik and Chervonenkis [2015]).", "startOffset": 0, "endOffset": 477}, {"referenceID": 0, "context": "There exist several bounds for VC-dimension of deep networks with piece-wise polynomial activation functions that go back to geometric techniques of Goldberg and Jerrum 1995 and earlier results of Warren 1968; see Bartlett et al. [1998], Sakurai [1999] and the book Anthony and Bartlett [2009].", "startOffset": 214, "endOffset": 237}, {"referenceID": 0, "context": "There exist several bounds for VC-dimension of deep networks with piece-wise polynomial activation functions that go back to geometric techniques of Goldberg and Jerrum 1995 and earlier results of Warren 1968; see Bartlett et al. [1998], Sakurai [1999] and the book Anthony and Bartlett [2009].", "startOffset": 214, "endOffset": 253}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]).", "startOffset": 36, "endOffset": 64}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]).", "startOffset": 36, "endOffset": 190}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]).", "startOffset": 36, "endOffset": 219}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]). A very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function. In this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a C-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result).", "startOffset": 36, "endOffset": 1051}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]). A very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function. In this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a C-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result).", "startOffset": 36, "endOffset": 1066}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]). A very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function. In this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a C-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result). Much less seems to be known about deep networks in this setting, though Mhaskar et al. 2016, 2016 have recently introduced functional spaces constructed using deep dependency graphs and obtained expressiveness bounds for related deep networks. We will focus our attention on networks with the ReLU activation function \u03c3(x) = max(0, x), which, despite its utter simplicity, seems to be the most popular choice in practical applications LeCun et al. [2015]. We will consider L\u221e-error of approximation of functions belonging to the Sobolev spacesWn,\u221e([0, 1]) (without any assumptions of hierarchical structure).", "startOffset": 36, "endOffset": 1621}, {"referenceID": 0, "context": "[1998], Sakurai [1999] and the book Anthony and Bartlett [2009]. There is a related concept, the fat-shattering dimension, for real-valued approximation problems (Kearns and Schapire [1990], Anthony and Bartlett [2009]). A very general approach to expressiveness in the context of approximation is the method of nonlinear widths by DeVore et al. 1989 that concerns approximation of a family of functions under assumption of a continuous dependence of the model on the approximated function. In this paper we examine the problem of shallow-vs-deep expressiveness from the perspective of approximation theory and general spaces of functions having derivatives up to certain order (Sobolev-type spaces). In this framework, the problem of expressiveness is very well studied in the case of shallow networks with a single hidden layer, where it is known, in particular, that to approximate a C-function on a d-dimensional set with infinitesimal error one needs a network of size about \u2212d/n, assuming a smooth activation function (see, e.g., Mhaskar [1996], Pinkus [1999] for a number of related rigorous upper and lower bounds and further qualifications of this result). Much less seems to be known about deep networks in this setting, though Mhaskar et al. 2016, 2016 have recently introduced functional spaces constructed using deep dependency graphs and obtained expressiveness bounds for related deep networks. We will focus our attention on networks with the ReLU activation function \u03c3(x) = max(0, x), which, despite its utter simplicity, seems to be the most popular choice in practical applications LeCun et al. [2015]. We will consider L\u221e-error of approximation of functions belonging to the Sobolev spacesWn,\u221e([0, 1]) (without any assumptions of hierarchical structure). We will often consider families of approximations, as the approximated function runs over the unit ball Fd,n in Wn,\u221e([0, 1]). In such cases we will distinguish scenarios of fixed and adaptive network architectures. Our goal is to obtain lower and upper bounds on the expressiveness of deep and shallow networks in different scenarios. We measure complexity of networks in a conventional way, by counting the number of their weights and computation units (cf. Anthony and Bartlett [2009]).", "startOffset": 36, "endOffset": 2262}, {"referenceID": 11, "context": "The arXiv preprint of the first version of the present work appeared almost simultaneously with the work of Liang and Srikant Liang and Srikant [2016] containing results partly overlapping with our results in Subsections 3.", "startOffset": 108, "endOffset": 151}, {"referenceID": 0, "context": "The depth of the network, the number of units and the total number of weights are standard measures of network complexity (Anthony and Bartlett [2009]).", "startOffset": 123, "endOffset": 151}, {"referenceID": 22, "context": "Our construction uses the \u201csawtooth\u201d function that has previously appeared in the paper Telgarsky [2015]. Proposition 2.", "startOffset": 88, "endOffset": 105}, {"referenceID": 21, "context": "This construction is inspired by a similar argument used to prove the O(2/n) upper bound for the complexity of Boolean circuits implementing n-ary functions Shannon [1949]. The proof becomes simpler if, in addition to the ReLU function \u03c3, we are allowed to use the activation function", "startOffset": 157, "endOffset": 172}, {"referenceID": 6, "context": "The method of continuous nonlinear widths (DeVore et al. [1989]) is a very general approach to the analysis of parameterized nonlinear approximations, based on the assumption of continuous selection of their parameters.", "startOffset": 43, "endOffset": 64}, {"referenceID": 6, "context": "Theorem 3 (DeVore et al. [1989], Theorem 4.", "startOffset": 11, "endOffset": 32}, {"referenceID": 6, "context": "Theorem 3 (DeVore et al. [1989], Theorem 4.2). Fix d, n. Let W be a positive integer and \u03b7 : R \u2192 C([0, 1]) be any mapping between the space R and the space C([0, 1]). Suppose that there is a continuous map M : Fd,n \u2192 R such that \u2016f \u2212 \u03b7(M(f))\u2016\u221e \u2264 for all f \u2208 Fd,n. Then W \u2265 c \u2212d/n, with some constant c depending only on n. We apply this theorem by taking \u03b7 to be some ReLU network architecture, and R the corresponding weight space. It follows that if a ReLU network architecture is capable of expressing any function from Fd,n with error , then, under the hypothesis of continuous weight selection, the network must have at least c \u2212d/n weights. The number of connections is then lower bounded by c 2 \u2212d/n (since the number of weights is not larger than the sum of the number of computation units and the number of connections, and there are at least as many connections as units). The hypothesis of continuous weight selection is crucial in Theorem 3. By examining our proof of the counterpart upper bound O( \u2212d/n ln(1/ )) in Theorem 1, the weights are selected there in a continuous manner, so this upper bound asymptotically lies above c \u2212d/n in agreement with Theorem 3. We remark, however, that the optimal choice of the network weights (minimizing the error) is known to be discontinuous in general, even for shallow networks (Kainen et al. [1999]).", "startOffset": 11, "endOffset": 1355}, {"referenceID": 0, "context": "In this setup, some lower bounds on the network complexity can be obtained as a consequence of existing upper bounds on VC-dimension of networks with piece-wise polynomial activation functions and Boolean outputs (Anthony and Bartlett [2009]).", "startOffset": 214, "endOffset": 242}, {"referenceID": 0, "context": ", Anthony and Bartlett [2009], Section 3.", "startOffset": 2, "endOffset": 30}, {"referenceID": 0, "context": ", Anthony and Bartlett [2009], Section 3.3). We are interested in the case when H is the family of functions obtained by applying thresholds 1(x > a) to a ReLU network with fixed architecture but variable weights. In this case Theorem 8.7 of Anthony and Bartlett [2009] implies that VCdim(H) \u2264 c3W , (33)", "startOffset": 2, "endOffset": 270}, {"referenceID": 22, "context": "1 of Telgarsky [2015]. Precisely, Telgarsky\u2019s lemma states that if a network has a single input, connections only between neighboring layers, at most m units in a layer, and a piece-wise linear activation function with t pieces, then the number of linear pieces in the output of the network is not greater than (tm).", "startOffset": 5, "endOffset": 22}, {"referenceID": 11, "context": "Liang and Srikant describe in Liang and Srikant [2016] some conditions on the approximated function (resembling conditions of local analyticity) under which complexity of deep -approximation is O(ln(1/ )) with a constant power c.", "startOffset": 0, "endOffset": 55}, {"referenceID": 20, "context": "This bound is achieved by finite-depth (depth-6) ReLU networks using the idea of reused subnetworks familiar from the theory of Boolean circuits Shannon [1949]. In the case of fixed architecture, we have not established any evidence of complexity improvement for unconstrained weight selection compared to continuous weight selection.", "startOffset": 145, "endOffset": 160}, {"referenceID": 8, "context": "We remark however that, already for approximations with depth-3 networks, the optimal weights are known to discontinuously depend, in general, on the approximated function (Kainen et al. [1999]).", "startOffset": 173, "endOffset": 194}, {"referenceID": 12, "context": "Upper and lower approximation complexity bounds for these networks (Mhaskar [1996], Maiorov and Meir [2000]) show that complexity scales as \u223c \u2212d/n up to some ln(1/ ) factors.", "startOffset": 68, "endOffset": 83}, {"referenceID": 12, "context": "Upper and lower approximation complexity bounds for these networks (Mhaskar [1996], Maiorov and Meir [2000]) show that complexity scales as \u223c \u2212d/n up to some ln(1/ ) factors.", "startOffset": 84, "endOffset": 108}, {"referenceID": 12, "context": "this gap, in particular, graph-based hierarchical approximations are studied in Mhaskar et al. [2016], Mhaskar and Poggio [2016] and convolutional arithmetic circuits in Cohen et al.", "startOffset": 80, "endOffset": 102}, {"referenceID": 12, "context": "this gap, in particular, graph-based hierarchical approximations are studied in Mhaskar et al. [2016], Mhaskar and Poggio [2016] and convolutional arithmetic circuits in Cohen et al.", "startOffset": 80, "endOffset": 129}, {"referenceID": 3, "context": "[2016], Mhaskar and Poggio [2016] and convolutional arithmetic circuits in Cohen et al. [2015]. Theoretical analysis of expressiveness of deep networks taking into account such properties of real data seems to be an important and promising direction of future research.", "startOffset": 75, "endOffset": 95}], "year": 2017, "abstractText": "We study expressive power of shallow and deep neural networks with piece-wise linear activation functions. We establish new rigorous upper and lower bounds for the network complexity in the setting of approximations in Sobolev spaces. In particular, we prove that deep ReLU networks more efficiently approximate smooth functions than shallow networks. In the case of approximations of 1D Lipschitz functions we describe adaptive depth-6 network architectures more efficient than the standard shallow architecture.", "creator": "LaTeX with hyperref package"}}}