{"id": "1610.01178", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "1-Oct-2016", "title": "A Tour of TensorFlow", "abstract": "Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released $\\textit{TensorFlow}$, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.", "histories": [["v1", "Sat, 1 Oct 2016 11:32:03 GMT  (436kb,D)", "http://arxiv.org/abs/1610.01178v1", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["peter goldsborough"], "accepted": false, "id": "1610.01178"}, "pdf": {"name": "1610.01178.pdf", "metadata": {"source": "CRF", "title": "A Tour of TensorFlow", "authors": [], "emails": ["peter.goldsborough@in.tum.de"], "sections": [{"heading": null, "text": "In fact, most of them are able to decide for themselves what they want."}, {"heading": "II. HISTORY OF MACHINE LEARNING LIBRARIES", "text": "In this section, we want to give a brief overview and important milestones in the history of machine learning software libraries, starting with a review of libraries suitable for a wide range of machine learning and data analysis, going back more than 20 years, and then conducting a more focused examination of recent programming frameworks that are particularly suited to the task of deep learning. Figure 1 illustrates this section in a timeline. We would like to emphasize that this section in Noar Xiv: 161 0.01 178v 1 [cs.L G] 1O ct2 016way compare TensorFlow, as we have dedicated Section VI to this specific purpose."}, {"heading": "A. General Machine Learning", "text": "In fact, most of them will be able to move to another world, in which they are able to move, and in which they are able to change the world."}, {"heading": "B. Deep Learning", "text": "The software libraries mentioned in the previous section are useful for a wide variety of different machine learning and statistical analysis. http http: / / www.http: / / www.http: / / www.http: / / www.http: / / www.http: / / www.http: / / www.http: / / www.http: / / www.http: / / / www.http: / / / www.http: / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / / www.http: / / / / www.http: / / / / / www.http: / / / / www.http: / / / / www.http: / / / www.http: / / / / www.http: / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / www.http: / / / / / www.http: / / / / / www.http: / / / / www.http: / / / / / www.http: / / / / / / www.http: / / / / / / www.http: / / / / / / / www.http: / / / / / / / www.http: / / / / / / www.http: / / / / / www.http: / / / / / www.http: / / / / www.http: / / / / / / www.http: / / / / www.http: / / / / / / www.http: / / / / www.http: / / / / / / / www.http: / / / / / / www.http: / / / / www.http: / / / / / / / / www.http / / / / www.http: / / / / / / / / / / www.http / / / / / / / / www.http: / / / / / / / / / http / / / / / / / / / http / / / / / / / / http / / / / / / / / / / / http / / / / / / / / / http / / http / / / / / / / / / / /"}, {"heading": "III. THE TENSORFLOW PROGRAMMING MODEL", "text": "In this section, we will discuss in detail the abstract calculation principles underlying the TensorFlow software library. We will begin by thoroughly examining the basic structural and architectural decisions of the TensorFlow development team and explain how machine learning algorithms can be expressed in the language of data flow diagrams. We will then examine the TensorFlow execution model and provide insights into the mapping of TensorFlow diagrams to available hardware units in both a local and a distributed environment. We will then examine the various optimizations that have been integrated into TensorFlow to improve both software and hardware efficiency. Finally, we will list extensions of the basic programming model that help the user with both computer-based and logistical aspects of training a machine learning model with TensorFlow."}, {"heading": "A. Computational Graph Architecture", "text": "This year is the highest in the history of the country."}, {"heading": "B. Execution Model", "text": "For execution, there are two \"versions\" of TensorFlow, one for local execution on a single machine (but possibly distributed) many different groups: the client, the master, a set of workers, and finally a number of devices. If the client requests the evaluation of a TensorFlow diagram via a running routine of a session, this query is sent to the master process, which in turn delegates the task to one or more work processes and coordinates their execution. Each worker is then responsible for monitoring one or more devices for which the physical processing units of an operation are implemented. Within this model, there are two degrees of scalability. The first degree refers to the scaling of the machines on which a graph is executed. On each machine, there can then be more than one device, such as five independent GPUs and / or three CPUs. For this reason, there are two \"versions\" of TensorFlow, one for local execution on a single machine."}, {"heading": "C. Optimizations", "text": "In order to maximize the efficiency and performance of the TensorFlow execution model, a number of optimizations are built into the library. In this subsection, we examine three such improvements: elimination of subgraphs, execution scheduling, and finally lossy compression. The result is then stored in a temporary variable and reused where it was previously recalculated. Likewise, a compiler can replace the calculation of an identical value with a single instance of this calculation two or more times. As a result, it is stored in a temporary variable in which it was previously recalculated. In a TensorFlow graph, it can happen that the same operation is performed on identical inputs, which can be inefficient if compression is expensive."}, {"heading": "D. Additions to the Basic Programming Model", "text": "\"After discussing the basic paradigms and execution models of TensorFlow, we will now review three more advanced topics that we consider highly relevant to anyone who wants to use TensorFlow to create machine learning algorithms. First, we will discuss how TensorFlow handles back propagation, an essential concept for many deep learning applications. Then, we will examine how TensorFlow supports graphics for control flow. Finally, we will briefly touch on the issue of checkpoints, as they are very useful for maintaining large models.1) Back propagation nodes: In a large number of deep learning and other machine learning algorithms, it is necessary to calculate the gradients of specific nodes in relation to one or many other nodes. For example, we can calculate the cost of the model in a neural network by passing this example through a series of non-linear transformations."}, {"heading": "IV. THE TENSORFLOW PROGRAMMING INTERFACE", "text": "After explaining the abstract concepts of TensorFlow's computational model in Section III, we will now concretize these ideas and talk to the TensorFlow programming interface. We will begin with a brief discussion of the language interfaces available, and then take a more hands-on look at TensorFlow's Python API by going through a simple practical example. Finally, we will give an insight into what higher-level abstractions exist for TensorFlow's API, which are particularly beneficial for rapid prototyping of machine learning models. InterfacesThere are currently two programming interfaces, C + + and Python, that allow interaction with the TensorFlow backend. The Python API has a very rich set of features for creating and executing computer-based diagrams. As this font is based on Python, the C + + interface (which is actually just the core backend) is a much more constrained API."}, {"heading": "B. Walkthrough", "text": "This year it is more than ever before."}, {"heading": "C. Abstractions", "text": "This year it has come to the point that it is only a narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow narrow"}, {"heading": "A. TensorBoard Features", "text": "The core feature of TensorBoard is the clear visualization of computational curves, illustrated in Figure 8a. Diagrams with complex topologies and many levels can be presented in a clear and organized way, allowing the user to understand exactly how data flows through them. TensorBoard's idea of name scopes is particularly useful, where nodes or entire subgraphs can be grouped into a visual block, such as a single neural network layer. Such name scopes can then be interactively expanded to display the grouped units in more detail. Figure 8b shows the expansion of one of the name scopes in Figure 8a.In addition, TensorBoard allows the user to track the development of individual tensor values over time by adding two types of summary operations to nodes of the computational task: scalar summaries and histograms as summaries."}, {"heading": "B. TensorBoard in Practice", "text": "To integrate TensorBoard into your TensorFlow code, three steps are required: First, it is advisable to group nodes into name scopes, then to add scalar and histogram summaries to your operations, and finally, to instantiate a SummaryWriter object and pass it the tensors generated by the summary nodes in a session context whenever you want to save new summaries. Finally, instead of retrieving individual summaries, it is also possible to combine all summary nodes using the tf.merge _ all _ summaries () operation.with tf.name _ scope (\"variables\"): x = tf.train.summaryWriter with tf.constant (2.0) tf.scalar _ summary ('z', x + y) merged = tf.merge _ all _ summaries () with tf.name _ scope (\"variables\")."}, {"heading": "VI. COMPARISON WITH OTHER DEEP LEARNING FRAMEWORKS", "text": "In addition to TensorFlow, there are a number of other open source deep learning software libraries, of which Theano, Torch and Caffe are the most popular. In this section, we examine the qualitative and quantitative differences between TensorFlow and each of these alternatives. We start with a \"high level\" qualitative comparison and examine where TensorFlow differs or overlaps conceptually or architecturally."}, {"heading": "A. Qualitative Comparison", "text": "In fact, the fact is that we will be able to go in search of a solution that is capable, that we are able, that we are able to find a solution that is capable of finding a solution, \"he said."}, {"heading": "B. Quantitative Comparison", "text": "We will now produce three sources of quantitative comparisons between TensorFlow and other deep learning units for important training outcomes, which will provide a summary of the most important results of each paper. Furthermore, we will briefly discuss the general trend of these benchmarks. The first work, [20], written by the Bosch Research and Technology Center in late March 2016, compares the performance of TensorFlow, Torch, Theano, and Caffe (among others) in relation to various neural network architectures. Their setup includes Ubuntu 14.04 running on an Intel Xeon E5-1650 v2 CPU @ 3.50 GHz and an NVIDIA GeForce GTX Titan X / PCIe / SSE2 GPU architectures. A benchmark we find remarkable tests of the relative performance of each library on a slightly modified reproduction of the LeNet CNN model [21]. More specifically, the authors measure the propagation time they make relevant to the use of the model, and act back."}, {"heading": "VII. USE CASES OF TENSORFLOW TODAY", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "VIII. CONCLUSION", "text": "In fact, it is as if it were some kind of tantrum that would have been able to retaliate, \"he said in an interview.\" It is very important that we are able to change the world, \"he said.\" It is very important that we are able to change the world, \"he said.\" It is very important that we are able to change the world, \"he said.\" It is very important that we are able to change the world, \"he said.\" It is very important that we are able to change the world, \"he said."}], "references": [{"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, May 2015. [Online]. Available: http://dx.doi.org/10.1038/nature14539", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Rectified linear units improve restricted boltzmann machines", "author": ["V. Nair", "G.E. Hinton"], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML-10), J. F\u00c3ijrnkranz and T. Joachims, Eds. Omnipress, 2010, pp. 807\u2013814. [Online]. Available: http://www.icml2010.org/papers/432.pdf", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research, vol. 15, pp. 1929\u20131958, 2014. [Online]. Available: http://jmlr.org/papers/v15/srivastava14a.html", "citeRegEx": "3", "shortCiteRegEx": null, "year": 1929}, {"title": "Tensorflow: Biology\u2019s gateway to deep learning?", "author": ["L. Rampasek", "A. Goldenberg"], "venue": "Cell Systems,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["The Theano Development Team", "R. Al-Rfou", "G. Alain", "A. Almahairi", "C. Angermueller", "D. Bahdanau", "N. Ballas", "F. Bastien", "J. Bayer", "A. Belikov", "A. Belopolsky", "Y. Bengio", "A. Bergeron", "J. Bergstra", "V. Bisson", "J. Bleecher Snyder", "N. Bouchard", "N. Boulanger-Lewandowski", "X. Bouthillier", "A. de Br\u00e9bisson", "O. Breuleux", "P.-L. Carrier", "K. Cho", "J. Chorowski", "P. Christiano", "T. Cooijmans", "M.-A. C\u00f4t\u00e9", "M. C\u00f4t\u00e9", "A. Courville", "Y.N. Dauphin", "O. Delalleau", "J. Demouth", "G. Desjardins", "S. Dieleman", "L. Dinh", "M. Ducoffe", "V. Dumoulin", "S. Ebrahimi Kahou", "D. Erhan", "Z. Fan", "O. Firat", "M. Germain", "X. Glorot", "I. Goodfellow", "M. Graham", "C. Gulcehre", "P. Hamel", "I. Harlouchet", "J.-P. Heng", "B. Hidasi", "S. Honari", "A. Jain", "S. Jean", "K. Jia", "M. Korobov", "V. Kulkarni", "A. Lamb", "P. Lamblin", "E. Larsen", "C. Laurent", "S. Lee", "S. Lefrancois", "S. Lemieux", "N. L\u00e9onard", "Z. Lin", "J.A. Livezey", "C. Lorenz", "J. Lowin", "Q. Ma", "P.-A.  Manzagol", "O. Mastropietro", "R.T. McGibbon", "R. Memisevic", "B. van Merri\u00ebnboer", "V. Michalski", "M. Mirza", "A. Orlandi", "C. Pal", "R. Pascanu", "M. Pezeshki", "C. Raffel", "D. Renshaw", "M. Rocklin", "A. Romero", "M. Roth", "P. Sadowski", "J. Salvatier", "F. Savard", "J. Schl\u00fcter", "J. Schulman", "G. Schwartz", "I. Vlad Serban", "D. Serdyuk", "S. Shabanian", "\u00c9. Simon", "S. Spieckermann", "S. Ramana Subramanyam", "J. Sygnowski", "J. Tanguay", "G. van Tulder", "J. Turian", "S. Urban", "P. Vincent", "F. Visin", "H. de Vries", "D. Warde-Farley", "D.J. Webb", "M. Willson", "K. Xu", "L. Xue", "L. Yao", "S. Zhang", "Y. Zhang"], "venue": "ArXiv e-prints, May 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Torch: A modular machine learning software library", "author": ["R. Collobert", "S. Bengio", "J. Marithoz"], "venue": "2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "venue": "J. Mach. Learn. Res., vol. 12, pp. 2825\u20132830, Nov. 2011. [Online]. Available: http://dl.acm.org/citation.cfm?id=1953048.2078195", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "TensorFlow: Large-scale machine learning on heterogeneous systems", "author": ["M. Abadi", "A. Agarwal", "P. Barham", "E. Brevdo", "Z. Chen", "C. Citro", "G.S. Corrado", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "I. Goodfellow", "A. Harp", "G. Irving", "M. Isard", "Y. Jia", "R. Jozefowicz", "L. Kaiser", "M. Kudlur", "J. Levenberg", "D. Man\u00e9", "R. Monga", "S. Moore", "D. Murray", "C. Olah", "M. Schuster", "J. Shlens", "B. Steiner", "I. Sutskever", "K. Talwar", "P. Tucker", "V. Vanhoucke", "V. Vasudevan", "F. Vi\u00e9gas", "O. Vinyals", "P. Warden", "M. Wattenberg", "M. Wicke", "Y. Yu", "X. Zheng"], "venue": "2015, software available from tensorflow.org. [Online]. Available: http://tensorflow.org/", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Data mining using mscr; lscr; cscr;++ a machine learning library in c++", "author": ["R. Kohavi", "D. Sommerfield", "J. Dougherty"], "venue": "Tools with Artificial Intelligence, 1996., Proceedings Eighth IEEE International Conference on, Nov 1996, pp. 234\u2013245.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1996}, {"title": "The opencv library", "author": ["G. Bradski"], "venue": "Doctor Dobbs Journal, vol. 25, no. 11, pp. 120\u2013126, 2000.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2000}, {"title": "A tutorial on principal component analysis with the accord.net framework", "author": ["C.R. de Souza"], "venue": "CoRR, vol. abs/1210.7463, 2012. [Online]. Available: http://arxiv.org/abs/1210.7463", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2012}, {"title": "Moa: Massive online analysis, a framework for stream classification and clustering", "author": ["A. Bifet", "G. Holmes", "B. Pfahringer", "P. Kranen", "H. Kremer", "T. Jansen", "T. Seidl"], "venue": "Journal of Machine Learning Research (JMLR) Workshop and Conference Proceedings, Volume 11: Workshop on Applications of Pattern Analysis. Journal of Machine Learning Research, 2010, pp. 44\u201350.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Spark: Cluster computing with working sets", "author": ["M. Zaharia", "M. Chowdhury", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "Proceedings of the 2Nd USENIX Conference on Hot Topics in Cloud Computing, ser. HotCloud\u201910. Berkeley, CA, USA: USENIX Association, 2010, pp. 10\u201310. [Online]. Available: http://dl.acm.org/citation.cfm?id=1863103. 1863113", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Mllib: Machine learning in apache spark", "author": ["X. Meng", "J.K. Bradley", "B. Yavuz", "E.R. Sparks", "S. Venkataraman", "D. Liu", "J. Freeman", "D.B. Tsai", "M. Amde", "S. Owen", "D. Xin", "R. Xin", "M.J. Franklin", "R. Zadeh", "M. Zaharia", "A. Talwalkar"], "venue": "CoRR, vol. abs/1505.06807, 2015. [Online]. Available: http://arxiv.org/abs/1505.06807", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R.B. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "CoRR, vol. abs/1408.5093, 2014. [Online]. Available: http://arxiv.org/abs/1408.5093", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2014}, {"title": "Announcing tensorflow 0.8 \u00e2\u0102\u015e now with distributed computing support!", "author": ["D. Murray"], "venue": "Google Research Blog, April 2016 (accessed May 22,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Google supercharges machine learning tasks with tpu custom chip", "author": ["N. Jouppi"], "venue": "Google Cloud Platform Blog, May 2016 (accessed May 22, 2016), https://cloudplatform.googleblog.com/2016/05/Googlesupercharges-machine-learning-tasks-with-custom-chip.html.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning", "author": ["I.G.Y. Bengio", "A. Courville"], "venue": "2016, book in preparation for MIT Press. [Online]. Available: http://www. deeplearningbook.org", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Going Deeper with Convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "ArXiv e-prints, Sep. 2014.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "Comparative  Study of Deep Learning Software Frameworks", "author": ["S. Bahrampour", "N. Ramakrishnan", "L. Schott", "M. Shah"], "venue": "ArXiv e-prints, Nov. 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. Lecun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov 1998.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1998}, {"title": "convnet-benchmarks", "author": ["S. Chintala"], "venue": "GitHub, April 2016 (accessed May 24, 2016), https://github.com/soumith/convnet-benchmarks.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems, 2012, p. 2012.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2012}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["M.P. Marcus", "M.A. Marcinkiewicz", "B. Santorini"], "venue": "Comput. Linguist., vol. 19, no. 2, pp. 313\u2013330, Jun. 1993. [Online]. Available: http://dl.acm.org/citation.cfm?id=972470.972475", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1993}, {"title": "Massively Multitask Networks for Drug Discovery", "author": ["B. Ramsundar", "S. Kearnes", "P. Riley", "D. Webster", "D. Konerding", "V. Pande"], "venue": "ArXiv e-prints, Feb. 2015.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "How google translate squeezes deep learning onto a phone", "author": ["O. Good"], "venue": "Google Research Blog, Jul. 2015 (accessed: May 25, 2016), http://googleresearch.blogspot.de/2015/07/how-google-translatesqueezes-deep.html.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Computer, respond to this email.", "author": ["G. Corrado"], "venue": "Google Research Blog, Nov. 2015 (accessed: May 25,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Deepmind moves to tensorflow", "author": ["K. Kavukcuoglu"], "venue": "Google Research Blog, Apr. 2016 (accessed May 24, 2016), http://googleresearch.blogspot.de/2016/04/deepmind-moves-totensorflow.html.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning", "author": ["C. Szegedy", "S. Ioffe", "V. Vanhoucke"], "venue": "ArXiv e-prints, Feb. 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "Using recurrent neural networks to optimize dynamical decoupling for quantum memory", "author": ["M. August", "X. Ni"], "venue": "arXiv.org, vol. quantph, no. arXiv:1604.00279. Technical University of Munich, Max Planck Institute for Quantum Optics, Apr. 2016. [Online]. Available: http://arxiv.org/pdf/1604.00279v1.pdf", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Character-Level Neural Translation for Multilingual Media Monitoring in the SUMMA Project", "author": ["G. Barzdins", "S. Renals", "D. Gosko"], "venue": "ArXiv e-prints, Apr. 2016.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2016}, {"title": "The pagerank citation ranking: Bringing order to the web.", "author": ["L. Page", "S. Brin", "R. Motwani", "T. Winograd"], "venue": "Stanford InfoLab, Technical Report 1999-66,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1999}, {"title": "Google turning its lucrative web search over to ai machines", "author": ["J. Clark"], "venue": "Bloomberg Technology, Oct. 2015 (accessed: May 25, 2016), http://www.bloomberg.com/news/articles/2015-10-26/googleturning-its-lucrative-web-search-over-to-ai-machines.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Given enough such transformation modules, very complex functions may be modeled to solve classification, regression, transcription and numerous other learning tasks [1].", "startOffset": 165, "endOffset": 168}, {"referenceID": 0, "context": "It is noteworthy that the rise in popularity of deep learning can be traced back to only the last few years, enabled primarily by the greater availability of large data sets, containing more training examples; the efficient use of graphical processing units (GPUs) and massively parallel commodity hardware to train deep learning models on these equally massive data sets as well as the discovery of new methods such as the rectified linear unit (ReLU) activation function or dropout as a regularization technique [1]\u2013[4].", "startOffset": 514, "endOffset": 517}, {"referenceID": 3, "context": "It is noteworthy that the rise in popularity of deep learning can be traced back to only the last few years, enabled primarily by the greater availability of large data sets, containing more training examples; the efficient use of graphical processing units (GPUs) and massively parallel commodity hardware to train deep learning models on these equally massive data sets as well as the discovery of new methods such as the rectified linear unit (ReLU) activation function or dropout as a regularization technique [1]\u2013[4].", "startOffset": 518, "endOffset": 521}, {"referenceID": 4, "context": "Among these are Theano [5], Torch [6], scikitlearn [7] and many more, which we review in further detail in Section II of this paper.", "startOffset": 23, "endOffset": 26}, {"referenceID": 5, "context": "Among these are Theano [5], Torch [6], scikitlearn [7] and many more, which we review in further detail in Section II of this paper.", "startOffset": 34, "endOffset": 37}, {"referenceID": 6, "context": "Among these are Theano [5], Torch [6], scikitlearn [7] and many more, which we review in further detail in Section II of this paper.", "startOffset": 51, "endOffset": 54}, {"referenceID": 7, "context": "In November 2015, this list was extended by TensorFlow, a novel machine learning software library released by Google [8].", "startOffset": 117, "endOffset": 120}, {"referenceID": 7, "context": "] on heterogeneous distributed systems\u201d [8].", "startOffset": 40, "endOffset": 43}, {"referenceID": 8, "context": "We begin our review with a library published 21 years before TensorFlow: MLC++ [9].", "startOffset": 79, "endOffset": 82}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Another machine learning library we wish to mention is scikit-learn3 [7].", "startOffset": 69, "endOffset": 72}, {"referenceID": 10, "context": "Released in 2008, it is composed not only of a variety of machine learning algorithms, but also signal processing modules for speech and image recognition [11].", "startOffset": 155, "endOffset": 159}, {"referenceID": 11, "context": "It was conceived in 2010 [12].", "startOffset": 25, "endOffset": 29}, {"referenceID": 12, "context": "Lastly, Spark MLlib11 is an open source machine learning and data analysis platform released in 2015 and built on top of the Apache Spark12 project [13], a fast cluster computing system.", "startOffset": 148, "endOffset": 152}, {"referenceID": 13, "context": "For this, it includes classification, regression, clustering and other machine learning algorithms [14].", "startOffset": 99, "endOffset": 103}, {"referenceID": 5, "context": "The first and oldest framework in our list suited to the development and training of deep neural networks is Torch13, released already in 2002 [6].", "startOffset": 143, "endOffset": 146}, {"referenceID": 4, "context": "Theano15, released in 2008 [5], is another noteworthy deep learning library.", "startOffset": 27, "endOffset": 30}, {"referenceID": 4, "context": "As such, [5] labels Theano a \u201cmathematical compiler\u201d.", "startOffset": 9, "endOffset": 12}, {"referenceID": 14, "context": "It was released in 2014 under a BSD-License [15].", "startOffset": 44, "endOffset": 48}, {"referenceID": 7, "context": "In TensorFlow, nodes represent operations, which in turn express the combination or transformation of data flowing through the graph [8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 7, "context": "In [8] such an implementation is referred to as the operation\u2019s kernel.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "TABLE I: Examples for TensorFlow operations [8].", "startOffset": 44, "endOffset": 47}, {"referenceID": 7, "context": "Moreover, an optional mapping from arbitrary nodes in the graph to respective replacement values \u2014 referred to as feed nodes \u2014 may be supplied to run as well [8].", "startOffset": 158, "endOffset": 161}, {"referenceID": 15, "context": "While the initial release of TensorFlow supported only single-machine execution, the distributed version was open-sourced on April 13, 2016 [16].", "startOffset": 140, "endOffset": 144}, {"referenceID": 16, "context": "For example, in May 2016, Google announced its Tensor Processing Unit (TPU), a custom built ASIC (application-specific-integrated-circuit) optimized specifically for fast tensor computations [17].", "startOffset": 191, "endOffset": 195}, {"referenceID": 7, "context": "The authors of [8] note that this is especially vital on devices such as GPUs, where memory resources are scarce.", "startOffset": 15, "endOffset": 18}, {"referenceID": 7, "context": "On the receiving end, the truncated representation is converted back to 32 bits simply by filling in zeros, rather than rounding [8].", "startOffset": 129, "endOffset": 132}, {"referenceID": 17, "context": "In [18], two approaches for back-propagating gradients through a computational graph are described.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "Another approach, more relevant to TensorFlow, is what [18] calls symbol-to-symbol derivatives and [8] terms automatic gradient computation.", "startOffset": 55, "endOffset": 59}, {"referenceID": 7, "context": "Another approach, more relevant to TensorFlow, is what [18] calls symbol-to-symbol derivatives and [8] terms automatic gradient computation.", "startOffset": 99, "endOffset": 102}, {"referenceID": 7, "context": "In [8] it is noted that symbol-to-symbol derivatives may incur a considerable performance cost and especially result in increased memory overhead.", "startOffset": 3, "endOffset": 6}, {"referenceID": 7, "context": "According to [8], TensorFlow currently employs the first approach.", "startOffset": 13, "endOffset": 16}, {"referenceID": 7, "context": "For this reason, in [8] the development team of TensorFlow states that recomputing certain tensors rather than keeping them in memory may be a possible performance improvement for the future.", "startOffset": 20, "endOffset": 23}, {"referenceID": 4, "context": "If the number of iterations for of a loop would be fixed and known at graph compile-time, its body could be unrolled into an acyclic sequence of computations, one per loop iteration [5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 7, "context": "However, to support a variable amount of iterations, TensorFlow is forced to jump through an additional set of hoops, as described in [8].", "startOffset": 134, "endOffset": 137}, {"referenceID": 4, "context": "This technique of stepping through a loop in reverse to compute the gradients is referred to as back-propagation through time in [5].", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Also, checkpoints are a vital element to ensuring fault tolerance in a distributed environment [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "they need neither be \u2208 [0, 1] nor sum to one.", "startOffset": 23, "endOffset": 29}, {"referenceID": 9, "context": "random_uniform([784, 10]))", "startOffset": 15, "endOffset": 24}, {"referenceID": 9, "context": "1, shape=[10]))", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "log(estimates), [1]) loss = tf.", "startOffset": 16, "endOffset": 19}, {"referenceID": 18, "context": "For example, [19] reports of deep convolutional network based on the Google Inception model with more than 36,000 individual units, while [8] states that certain long short-term memory (LSTM) architectures can span over 15,000 nodes.", "startOffset": 13, "endOffset": 17}, {"referenceID": 7, "context": "For example, [19] reports of deep convolutional network based on the Google Inception model with more than 36,000 individual units, while [8] states that certain long short-term memory (LSTM) architectures can span over 15,000 nodes.", "startOffset": 138, "endOffset": 141}, {"referenceID": 7, "context": "This can be useful to show the images sampled for each mini-batch of an image classification task, or to visualize the kernel filters of a convolutional neural network [8].", "startOffset": 168, "endOffset": 171}, {"referenceID": 4, "context": "However, Theano is known to have very long graph compile times as it translates Python code to C++/CUDA [5].", "startOffset": 104, "endOffset": 107}, {"referenceID": 4, "context": "In part, this is due to the fact that Theano applies a number of more advanced graph optimization algorithms [5], while TensorFlow currently only performs common subgraph elimination.", "startOffset": 109, "endOffset": 112}, {"referenceID": 15, "context": "8, released in April 2016 [16].", "startOffset": 26, "endOffset": 30}, {"referenceID": 19, "context": "The first work, [20], authored by the Bosch Research and Technology Center in late March 2016, compares the performance of TensorFlow, Torch, Theano and Caffe (among others) with respect to various neural network architectures.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "One benchmark we find noteworthy tests the relative performance of each library on a slightly modified reproduction of the LeNet CNN model [21].", "startOffset": 139, "endOffset": 143}, {"referenceID": 19, "context": "The authors of [20] note that one reason for this may be that they used the NVIDIA cuDNN v2 library for their GPU implementation with TensorFlow while using cuDNN v3 for the others.", "startOffset": 15, "endOffset": 19}, {"referenceID": 19, "context": "TABLE III: This table shows the benchmarks performed by [20], where TensorFlow, Torch, Caffe and Theano are compared on a LeNet model reproduction [21].", "startOffset": 56, "endOffset": 60}, {"referenceID": 20, "context": "TABLE III: This table shows the benchmarks performed by [20], where TensorFlow, Torch, Caffe and Theano are compared on a LeNet model reproduction [21].", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "TABLE IV: The result of Soumith Chintala\u2019s benchmarks for TensorFlow, Torch and Caffe (not Theano) on an AlexNet ConvNet model [22], [23].", "startOffset": 127, "endOffset": 131}, {"referenceID": 22, "context": "TABLE IV: The result of Soumith Chintala\u2019s benchmarks for TensorFlow, Torch and Caffe (not Theano) on an AlexNet ConvNet model [22], [23].", "startOffset": 133, "endOffset": 137}, {"referenceID": 21, "context": "The second source in our collection is the convnetbenchmarks repository on GitHub by Soumith Chintala [22], an artificial intelligence research engineer at Facebook.", "startOffset": 102, "endOffset": 106}, {"referenceID": 22, "context": "Inter alia, Chintala gives the forward and backwardpropagation time of TensorFlow, Torch and Caffe for the AlexNet CNN model [23].", "startOffset": 125, "endOffset": 129}, {"referenceID": 4, "context": "Lastly, we review the results of [5], published by the Theano development team on May 9, 2016.", "startOffset": 33, "endOffset": 36}, {"referenceID": 19, "context": "8, released in April 2016 and thus after the publication of [20], TensorFlow now supports cuDNN v4, which promises better performance on GPUs than cuDNN v3 and especially cuDNN v2.", "startOffset": 60, "endOffset": 64}, {"referenceID": 4, "context": "9: The results of [5], comparing TensorFlow, Theano and Torch on an LSTM model for the Penn Treebank dataset [24].", "startOffset": 18, "endOffset": 21}, {"referenceID": 23, "context": "9: The results of [5], comparing TensorFlow, Theano and Torch on an LSTM model for the Penn Treebank dataset [24].", "startOffset": 109, "endOffset": 113}, {"referenceID": 23, "context": "dataset [24].", "startOffset": 8, "endOffset": 12}, {"referenceID": 4, "context": "In [5] also a medium-sized model is tested, which we ignore for our review.", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "Table 9 shows these results, taken from [5].", "startOffset": 40, "endOffset": 43}, {"referenceID": 19, "context": "The earliest of the three sources, [20], published in late March 2016, ranks TensorFlow consistently uncompetitive compared to Theano, Torch and Caffe.", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "Released almost two months later, [22] ranks TensorFlow comparatively better.", "startOffset": 34, "endOffset": 38}, {"referenceID": 4, "context": "The latest work reviewed, [5], then places TensorFlow in first or second place for LSTMs and also other architectures discussed by the authors.", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "We state that one reason for this upward trend is that [5] uses TensorFlow with cuDNN v4 for its GPU experiments, whereas [20] still used cuDNN v2.", "startOffset": 55, "endOffset": 58}, {"referenceID": 19, "context": "We state that one reason for this upward trend is that [5] uses TensorFlow with cuDNN v4 for its GPU experiments, whereas [20] still used cuDNN v2.", "startOffset": 122, "endOffset": 126}, {"referenceID": 18, "context": "The one exception is, of course, Google, which has already deployed TensorFlow for a variety of learning tasks [19], [25]\u2013[28].", "startOffset": 111, "endOffset": 115}, {"referenceID": 24, "context": "The one exception is, of course, Google, which has already deployed TensorFlow for a variety of learning tasks [19], [25]\u2013[28].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "The one exception is, of course, Google, which has already deployed TensorFlow for a variety of learning tasks [19], [25]\u2013[28].", "startOffset": 122, "endOffset": 126}, {"referenceID": 28, "context": "The first noteworthy mention of TensorFlow is [29], published by Szegedy, Ioffe and Vanhoucke of the Google Brain Team in February 2016.", "startOffset": 46, "endOffset": 50}, {"referenceID": 18, "context": "In their work, the authors use TensorFlow to improve on the Inception model [19], which achieved best performance at the 2014 ImageNet classification challenge.", "startOffset": 76, "endOffset": 80}, {"referenceID": 24, "context": "In [25], Ramsundar et al.", "startOffset": 3, "endOffset": 7}, {"referenceID": 29, "context": "August and Ni apply TensorFlow to create recurrent neural networks for optimizing dynamic decoupling, a technique for suppressing errors in quantum memory [30].", "startOffset": 155, "endOffset": 159}, {"referenceID": 30, "context": "Lastly, [31] investigates the use of sequence to sequence neural translation models for natural language processing of multilingual media sources.", "startOffset": 8, "endOffset": 12}, {"referenceID": 31, "context": "Recently, Google has begun augmenting its core search service and accompanying PageRank algorithm [32] with a system called RankBrain [33], which makes use of TensorFlow.", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "Recently, Google has begun augmenting its core search service and accompanying PageRank algorithm [32] with a system called RankBrain [33], which makes use of TensorFlow.", "startOffset": 134, "endOffset": 138}, {"referenceID": 32, "context": "According to [33], more than 15 percent of all search queries received on www.", "startOffset": 13, "endOffset": 17}, {"referenceID": 26, "context": "Another area where Google applies deep learning with TensorFlow is smart email replies [27].", "startOffset": 87, "endOffset": 91}, {"referenceID": 25, "context": "In [26] it is reported how Google employs convolutional neural networks for image recognition and automatic text translation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 25, "context": "[26] notes especially the challenge of deploying such a system onto low-end phones with slow network connections.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "Lastly, we make note of the decision of Google DeepMind, an AI division within Google, to move from Torch7 to TensorFlow [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 16, "context": "A related source, [17], states that DeepMind made use of TensorFlow for its AlphaGo29 model, alongside Google\u2019s newly developed Tensor Processing Unit (TPU), which was built to integrate especially well with TensorFlow.", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "1, shape=[10]))", "startOffset": 9, "endOffset": 13}, {"referenceID": 0, "context": "reduction_indices=[1])", "startOffset": 18, "endOffset": 21}], "year": 2016, "abstractText": "Deep learning is a branch of artificial intelligence employing deep neural network architectures that has significantly advanced the state-of-the-art in computer vision, speech recognition, natural language processing and other domains. In November 2015, Google released TensorFlow, an open source deep learning software library for defining, training and deploying machine learning models. In this paper, we review TensorFlow and put it in context of modern deep learning concepts and software. We discuss its basic computational paradigms and distributed execution model, its programming interface as well as accompanying visualization toolkits. We then compare TensorFlow to alternative libraries such as Theano, Torch or Caffe on a qualitative as well as quantitative basis and finally comment on observed use-cases of TensorFlow in academia and industry.", "creator": "LaTeX with hyperref package"}}}