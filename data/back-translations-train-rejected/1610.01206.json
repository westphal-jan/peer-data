{"id": "1610.01206", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "3-Oct-2016", "title": "Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods", "abstract": "Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper first reviews the root methods and theories on multi-view representation learning, especially on canonical correlation analysis (CCA) and its several extensions. And then we investigate the advancement of multi-view representation learning that ranges from shallow methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to deep methods including multi-modal restricted Boltzmann machines, multi-modal autoencoders, and multi-modal recurrent neural networks. Further, we also provide an important perspective from manifold alignment for multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical basis and current developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.", "histories": [["v1", "Mon, 3 Oct 2016 17:14:15 GMT  (1036kb,D)", "http://arxiv.org/abs/1610.01206v1", "27 pages, 10 figures. arXiv admin note: text overlap witharXiv:1206.5538,arXiv:1304.5634by other authors"], ["v2", "Sun, 27 Nov 2016 03:11:53 GMT  (1032kb,D)", "http://arxiv.org/abs/1610.01206v2", "20pages, 9 figures"], ["v3", "Thu, 24 Aug 2017 08:08:22 GMT  (812kb,D)", "http://arxiv.org/abs/1610.01206v3", "20pages, 9 figures. arXiv admin note: text overlap witharXiv:1206.5538by other authors"], ["v4", "Fri, 1 Sep 2017 05:52:06 GMT  (812kb,D)", "http://arxiv.org/abs/1610.01206v4", "20pages, 9 figures"]], "COMMENTS": "27 pages, 10 figures. arXiv admin note: text overlap witharXiv:1206.5538,arXiv:1304.5634by other authors", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.IR", "authors": ["yingming li", "ming yang", "zhongfei zhang"], "accepted": false, "id": "1610.01206"}, "pdf": {"name": "1610.01206.pdf", "metadata": {"source": "CRF", "title": "Multi-View Representation Learning: A Survey from Shallow Methods to Deep Methods", "authors": ["Yingming Li", "Ming Yang", "Zhongfei (Mark) Zhang"], "emails": ["zhongfei}@zju.edu.cn"], "sections": [{"heading": null, "text": "Multi-View Representation Learning: A Survey from Shallow Methods to Deep MethodsYingming Li, Ming Yang, Zhongfei (Mark) Zhang, Senior Member, IEEEAbstract - Recently, Multiview Representation Learning has become a rapidly growing field in the field of machine learning and data mining. In this paper, we first examine the basic methods and theories of multiview representation learning, particularly canonical correlation analysis (CCA) and its various extensions, and then explore the evolution of multiview representation learning, which ranges from flat methods such as multimodal topic learning, multiview spare coding and multiview latent space-Markov networks to deep methods such as multimodal restricted Boltzmann machines, multimodal autoencoders and multimodal recursive neural networks. In addition, we offer an important perspective from the diverse orientation of multiview representation learning. Overall, this multi-modal survey aims to provide an overview of the most appropriate insights for specific networks and applications."}, {"heading": "1 INTRODUCTION", "text": "This year it has come to the point that it will only be a matter of time before it will happen, until it does."}, {"heading": "2 CANONICAL CORRELATION ANALYSIS AND ITS EXTENSIONS", "text": "In this section, we will examine the roots of the techniques of multiview representation acquisition, canonical correlation analysis (CCA) and their extensions, ranging from probabilistic graphical modeling to nonlinear deep embedding. In particular, we will examine the related work of probabilistic CCA, sparse CCA, kernel CCA and deep CCA to illustrate the probabilistic, sparse and nonlinear views of multiview representation learning theories."}, {"heading": "2.1 Canonical Correlation Analysis", "text": "From the perspective of multiview learning, CCA is considered a common embedding of both sets of variables by maximizing the correlations between the variables between the two groups. Specifically, CCA is widely used in multi-view learning tasks to generate low-dimensional feature representations [25-41], etc. Improved generalization performance has been observed in areas including dimension reduction [28-30], clustering [31-33], regression [34, 35], word embeddings [36-38], discriminatory learning [39-41], etc. Figure 1 shows a fascinating cross-modality application of CCA in multimedia retrievals.Given a pair of datasets X = [x1, xn] and Y =."}, {"heading": "2.2 Probabilistic CCA", "text": "This formulation of CCA as a probabilistic model is regarded by most people in the world as a kind of terminology in which it is a matter of a kind of terminology, in which most terms and concepts in relation to the manner in which the terms and concepts in relation to the concepts and concepts in relation to the concepts and concepts in relation to the concepts and concepts in relation to the concepts and concepts in relation to the concepts and concepts in relation to the concepts and concepts in relation to the concepts and concepts in relation to the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts in relation to the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts in relation to the concepts, the concepts and concepts, the concepts and concepts in relation to the concepts, the concepts and concepts and concepts in relation to the concepts, the"}, {"heading": "2.3 Sparse CCA", "text": "In recent years, there has been a growing interest in frugal representations of data. Accordingly, the problem of frugal CCA has also received a lot of attention in the multiview representation of learning; the pursuit of frugality can be motivated by several aspects; firstly, the ability to take into account the predicted outcomes; the big picture is usually based on a small number of crucial variables, with details that are allowed for variations; the second motivation for frugality is regulation and stability; Reasonable regularization plays an important role in eliminating the influence of noirish data and reducing the sensitivity of CCA to a small number of observations; and, in addition, frugal CCA can be formulated as a subset selection scheme that reduces the dimensionality of the vectors and makes a stable solution.The problem of frugal CCA acquisition can be considered as a search for a pair of linear combinations of wx and wy with prescribed cardinality that maximizes correlation."}, {"heading": "2.4 Kernel CCA", "text": "The central idea of Kernel CCA is to embed the data in a higher dimensional space: X \u2192 H, where Hx produces the real problem, and statistics [2, 101, 102]. The central idea of Kernel CCA is to embed the data in a higher dimensional space."}, {"heading": "2.5 Deep CCA", "text": "CCA-like correlation analysis (NLCCA) can of course be applied to neural networks to capture the relationship between two different viewpoints. In the early work, assuming that different parts of the perception input have common causes in the external world, Becker and Hinton [119] presented a multi-layered nonlinear extension of the canonical correlation by maximizing normalized covariation between the results of two neural network modules. [121, 122] explores the idea of maximizing mutual information between the outputs of different network modules to extract features of higher order from coherence. Later, Lai and Fyfe [122] investigate a neural network implementation of CCA and maximize the mutual correlation between the outputs of the networks for each view. Hsieh [123] formulates a nonlinear canonical correlation analysis (NLCCA) using three forward neural networks."}, {"heading": "3 SHALLOW METHODS ON MULTI-VIEW REPRESENTATION LEARNING", "text": "In this section we will first examine the superficial methods for learning representation from the perspective of probabilistic modelling and then examine the associated methods from the perspective of directly parameterised learning of representation."}, {"heading": "3.1 Probabilistic Models", "text": "From the perspective of probabilistic modeling, the problem of multiview feature learning can be interpreted as an attempt to restore a compact set of latent random variables that describe a distribution over the observed multi-view data. We can express p (x, y, h) as a probabilistic model over the common space of the latent variable h and observed two-view data x, y. Feature values are determined by the posterior probability p (h | x, y)."}, {"heading": "3.1.1 Multi-View Probabilistic Latent Semantic Analysis", "text": "Probabilistic latent semantic analysis (PLSA) [129] is a statistical variant of Latent Semantic Analysis (LSA) [130] and a significant step to probabilistic modelling of text. It performs a probabilistic decomposition by modelling each word in a document as an example from a blending model in which the blending components are multifunctional random variables, which are used as representations of \"subjects\" z = {z1,., zK}. Each topic is associated with relative estimates P (ti | zk) for each term in the corpus. A document dj is then called a convex combination of factors with mixed weights P = {z1, i.e., the predictable probabilities for terms in a particular document have the form P (ti | dj)."}, {"heading": "3.1.2 Multi-Modal Latent Dirichlet Allocation", "text": "The latent dirichlet mapping (LDA) [142] is a generative probabilistic model for collections of a corpus. It goes beyond PLSA by providing a generative model at the word and document level at the same time. Specifically, LDA is a three-level hierarchical LDA model that presents each document as a finite mixture of an underlying set of topics. As a generative model, LDA is expandable to multiview data. Barnard et al. [10] presents a mixture of multimodal LDA models (MoM-LDA) that describes the following generative process for the multimodal data: Each document (consisting of visual and textual information) has a distribution for a fixed number of topics (mixing components), and given a specific topic, the visual characteristics and textual words are generated. Furthermore, the probability distribution of the topics is different for each image-text pair that is achieved by implementing a dilet prior to the distribution of topics."}, {"heading": "3.1.3 Shared Gaussian Process Latent Variable Model", "text": "Gaussian Processes (GPs) [154, 155] are generalizations of Gaussian distributions = each is defined over infinite index sets. GPs have become powerful models for classification and regression that subsume numerous classes of function approximators, as single hidden-layer neural networks and RBF networks. Lawrence [156] suggests the Gaussian Process Latent Variable Model (GPLVM) as a non-linear dimensionality reduction technique. [157] present the shared GPLVM (SGPLVM) model as a generalization of GPLVM that can handle multiple observation spaces, where each set of observations is parameterized by a different set of kernel parameters. This shared GPLVM model can be considered as a nonlinear extension to CCA.Let X, Y be matrices of observations of dimensionality dX, dY or Z be a latent space of dimensionality."}, {"heading": "3.1.4 Collective Matrix Factorization", "text": "These matrices are not expected to have the same factor matrices as they do in the same sequential dimension and at the same time on the basis of formXi = UV > i for all i (22).These matrices are assumed to share the same factor matrix factor, and each matrix has its own charge matrix relationships. We can also integrate bias for different factors and charges [170].To use the CMF, we need to multiply the factors and charges."}, {"heading": "3.1.5 Multi-View Sparse Coding", "text": "Like the CMF, the CMF can see a less likely encoding, like the CMF can see a less likely and less likely interpretation. (mz) Like the CMF can see a less likely encoding, the CMF can see a less likely and less likely interpretation. (mz) Like the CMF can see a less likely encoding, like the CMF can see a less likely encoding, to ensure a high degree of correlation with the input. (mz) Like the CMF can see a less likely encoding, like the CMF can see a less likely encoding, to ensure a sparse activation of the h. (Given a pair of datasets {X-Rn \u00d7 dx, Y-Rn \u00d7) like the CMF can see a less likely encoding, like the CMF can see a less likely encoding."}, {"heading": "3.1.6 Multi-View Latent Space Markov Networks", "text": "In this dual wing model, which consists of two modalities of the input units X = 1, Y = 1 and a series of hidden units, we are each connected to hidden units."}, {"heading": "3.2 Directly Learning A Parametric Embedding from", "text": "Within the framework of the multi-view probability models discussed in the previous section, we can see that the learned representation is usually associated with common latent variables, especially with their posterior distribution given the multi-view input observed. Furthermore, this posterior distribution is often complicated and insoluble if the designed models have hierarchical structures, and must rely on random samples or approximate inference techniques to endure the associated arithmetic and approximation errors. Furthermore, a posterior distribution via shared latent variables is not yet a reasonable feature vector that can be fed directly to the classifier. Therefore, if we want to obtain stable deterministic characteristic values, an alternative non-probable embedding of the learning functions in Multiview is the directly parameterized feature or the representational functions. The common perspective between these methods is that they learn direct coding for input with multiple views, so we will review the follow-up work from this section in this section."}, {"heading": "3.2.1 Partial Least Squares", "text": "The PLS-problematic can now be described as follows: \"The PLS-problematic is not only a problem, but also a problem.\" (\"The PLS-problematic\") \"The PLS-problematic.\" (\"The PLS-problematic\") \"The PLS-problematic.\" (\"The PLS-problematic\") \"The PLS-problematic.\" \"(\" The PLS-problematic \")\" The PLS-problematic. \"(\" The PLS-problematic \")\" The PLS-problematic. \"\" (\"The PLS-problematic.\") \"The PLS-problematic.\" (\"The PLS-problematic.\") \"(The PLS-problematic.\" (The PLS-problematic.) \"(The PLS-problematic.)\" (The PLS-problematic.) \"(The PLS-problematic.)\" (The PLS-problematic.) \"(The PLS-problematic.)\" (The PLS-problematic.) \"(The PLS-problematic.\" (The PLS-problematic.)"}, {"heading": "3.2.2 Multi-View Discriminant Analysis", "text": "CCA and its core enhancements are used to adjust sets of characteristics by maximizing within sets \u03b2 correlation and minimizing correlation between sets that are uncontrolled scenarios. A number of multiview analysis approaches [205-207] have also been proposed, taking into account the scenarios in which the data have several different views, along with monitored information.Faced with a data pair that selects the different views of sets X = [x1] and Y = [y1,., yn] with the appropriate labels l = (alpha),., ln), the two-view regulated Fisher discriminant analysis (FDA) [205] selects two sets of weights wx and wy to solve the following optimization problem."}, {"heading": "3.2.3 Cross-Modal Ranking", "text": "Motivated by the inclusion of ranking information in multimodal embedding of learning processes, cross-modal ranking q = > q q = 1 is attracting a lot of attention in the literature [212-214]. [212] represents a supervised semantic indexing (SSI) model that defines a class of non-linear models that are discriminatively trained to convert multimodal input pairs into ranking scores. In contrast to CCA and CFA, SSI is trained by a supervised manner to perform the ranking task of interest, so that it maintains discriminatory information. Specifically, SSI attempts to learn a similarity function f (q, d) between a textquery q and an image d according to a predefined ranking loss. The learned function f is directly based on a ranking image pair based on its semantic relevance. In the face of a text query q-Rm and an image d, d intends to find a measurement function of linear relevance to SSI."}, {"heading": "3.2.4 Cross-Modal Hashing", "text": "It is a promising way to accelerate the search for multimodal similarity in multimodal data [222-231]. The principle of multimodal hashing methods is to convert the high-dimensional multimodal multimodal data into common hash codes so that similar cross-modal data objects will have the same or similar hash codes.Bronstein et al. [222] propose a hashing-based model called cross-modal similarity (CMSSH), which approaches the cross-modal similarity problem by embedding the multimodal data in a common metric space. Similarity is parameterized by embedding itself."}, {"heading": "4 DEEP METHODS ON MULTI-VIEW REPRESENTATION LEARNING", "text": "Inspired by the success of deep neural networks [5, 6, 124], a variety of methods have been proposed to capture the high-level correlation between data from multiple views. In this section, the deep representation models from multiple views are considered from probabilistic and directly embedded perspectives."}, {"heading": "4.1 Probabilistic Models", "text": "The model consists of 1 (1), 2), 2 (2), 2 (2), 3 (2), 4 (2), 4 (2), 4 (2), 5 (2), 5 (2), 5 (2), 5 (2), 5 (2), 6 (2), 6 (2), 7 (2), 7 (2), 8 (2), 8 (2), 8 (2), 8 (2), 8 (2), 8 (2), 8 (2), 8 (8), 8 (2), 8 (8), 8 (2), 8 (8), 8 (2), 8 (8), 8 (2), 8 (8), 8 (2), 8 (8), 8 (8), 8 (2), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (2), 8 (2), 8 (2), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (2, 8), 8 (8), 8 (8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8 (8), 8 (8), 8 (8 (8), 8 (8), 8), 8 (8 (8), 8 (8"}, {"heading": "4.2.1 Multi-Modal Deep Autoencoders", "text": "Although RBMs achieve great success in learning a common representation, there are still limitations to them, e.g. there is no explicit goal for the models to discover correlations between modalities, so that some hidden units are tuned for one modality only, while others are tuned for the other. [18] Multimodal deep autoencoders [18, 249-251] gradually propose good alternatives for learning a common representation between modalities due to the sufficient flexibility of their objectives. [18] They are supported by denoising autoencoders [5], Ngiam et al. [18] They propose to learn a common representation between modalities (Figure 5) by augmented but noisy dataset with additional examples having only one single modality as input. The key idea is to use greedy layer-wise training with an extension to RBMs with sparsity."}, {"heading": "4.2.2 Deep Cross-View Embedding Models", "text": "Unlike other visual networks, which connect two visual networks through multimodal mapping, the visual transformation to represent the visual model at the top of the visual core model is embedded in the learned vector representations by the neural language model [259] and a pre-trained deep visual-semantic model [124]. This results in a linear transformation to represent the representation at the top of the visual core model in the learned vector representations by the neural language model.After setting up the loss function in [212], DeViSE uses a combination of dot-product similarity and rank loss, so that the model has the ability to produce a higher dot-product similarity between the visual representation."}, {"heading": "4.2.3 Deep Multi-Modal Hashing", "text": "It is not only the way in which the different ways of learning and learning differ, but also the way in which the different ways of learning and learning are interwoven with each other. (...) It is also the way in which the different ways of learning and learning are interwoven with each other. (...) It is the way in which the different ways of learning and learning are interwoven with each other. (...) It is the way in which the different ways of learning and learning are interwoven with each other. (...) It is the way in which the different ways of learning are interwoven with each other. (...) It is the way in which the different ways of learning and learning are interwoven with each other. (...) It is the way in which the the the different ways and the ways are interwoven with each other. (...) It is the way in which the way the different ways and the ways are interwoven. (...) It is the way in which the different ways are interwoven with each other. (...) It is the way in which the way in which the different ways and the different ways are interwoven."}, {"heading": "4.2.4 Multi-Modal Recurrent Neural Network", "text": "A recurrent neural network (RNN) [275] is a neural network that determines a variable length sequence x = 1 (x1,., xT) by a hidden state representation h.At any time, the hidden state of the RNN function can be estimated (ht \u2212 1, xt) (63), where f is a nonlinear activation function and selected based on the requirement of data modeling. For example, a simple case can be a common elemental logistic sigmoid function and a complex case that can be a long-term short-term memory (LSTM).An RNN is known for its ability to learn a probability distribution over a sequence by being trained to predict the next symbol in a sequence. In this training, the prediction at any time is determined by the conditional distribution p (xt \u2212 1, x1)."}, {"heading": "5 MULTI-VIEW REPRESENTATION LEARNING AS MANIFOLD ALIGNMENT", "text": "The basic idea of this approach is to embed inputs from different domains in a new latent common space that simultaneously preserves the topology of each input domain. Its premise is the multiple hypothesis that high-dimensional data from the real world are expected to be concentrated near a low-dimensional multiplicity embedded in a high-dimensional input domain. In particular, this approach is well suited for datasets such as images, sounds, and texts, as this information is not natural signals. As most data sources can be modeled through multiplicity, the multiple alignment of the underlying structures can be used across different data views. From this perspective, the multivisionary representation learning task can be viewed as finding the relationship between the structures of multiplicity from different views of the data."}, {"heading": "6 CONCLUSION", "text": "This paper first examines the basic methods and theories of multiview representation learning, in particular canonical correlation analysis (CCA) and its extensions. We then examine the progress of multiview representation learning, which ranges from flat methods such as multimodal theme-based learning, sparse multi-view coding and latent multi-view Markov networks, to in-depth methods such as multimodal restricted Boltzmann machines, multimodal autoencoders and multimodal recursive neural networks. In addition, we offer an important perspective for multiview representation learning from a wide range of perspectives. The aim of this survey is to provide an insightful picture of the theoretical foundations and current developments in the field of multiview representation learning and to help researchers find the most appropriate methods for specific applications."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is partially supported by China's National Basic Research Program (2012CB316400), Zhejiang University - Alibaba Financial Joint lab, and the Zhejiang Provincial Engineering Center on Media Data Cloud Processing and Analysis. ZZ is also partially supported by the US NSF (IIS-0812114, CCF-1017828)."}], "references": [{"title": "Relations between two sets of variates", "author": ["H. Hotelling"], "venue": "Biometrika, vol. 28, no. 3/4, pp. 321\u2013377, 1936.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1936}, {"title": "Kernel independent component analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 1\u201348, 2002.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2002}, {"title": "Canonical correlation analysis: An overview with application to learning methods", "author": ["D.R. Hardoon", "S.R. Szedmak", "J.R. Shawe-taylor"], "venue": "Neural Comput., vol. 16, no. 12, pp. 2639\u20132664, Dec. 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "A survey of multi-view machine learning", "author": ["S. Sun"], "venue": "Neural Computing and Applications, vol. 23, no. 7-8, pp. 2031\u2013 2038, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["P. Vincent", "H. Larochelle", "Y. Bengio", "P.-A. Manzagol"], "venue": "ICML, 2008, pp. 1096\u20131103.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2008}, {"title": "Deep boltzmann machines.", "author": ["R. Salakhutdinov", "G.E. Hinton"], "venue": "in AISTATS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 35, no. 8, pp. 1798\u20131828, 2013.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1828}, {"title": "Deep canonical correlation analysis", "author": ["G. Andrew", "R. Arora", "J.A. Bilmes", "K. Livescu"], "venue": "ICML, 2013, pp. 1247\u2013 1255.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "The missing link - A probabilistic model of document content and hypertext connectivity", "author": ["D.A. Cohn", "T. Hofmann"], "venue": "NIPS, 2000, pp. 430\u2013436.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2000}, {"title": "Matching words and pictures", "author": ["K. Barnard", "P. Duygulu", "D. Forsyth", "N. de Freitas", "D.M. Blei", "M.I. Jordan"], "venue": "J. Mach. Learn. Res., vol. 3, pp. 1107\u20131135, Mar. 2003.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2003}, {"title": "Modeling annotated data", "author": ["D.M. Blei", "M.I. Jordan"], "venue": "SIGIR, 2003, pp. 127\u2013134.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2003}, {"title": "Factorized latent spaces with structured sparsity", "author": ["Y. Jia", "M. Salzmann", "T. Darrell"], "venue": "NIPS, 2010, pp. 982\u2013 990.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "Robust multimodal dictionary learning", "author": ["T. Cao", "V. Jojic", "S. Modla", "D. Powell", "K. Czymmek", "M. Niethammer"], "venue": "MICCAI, 2013, pp. 259\u2013266.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Multiview hessian discriminative sparse coding for image annotation", "author": ["W. Liu", "D. Tao", "J. Cheng", "Y. Tang"], "venue": "Computer Vision and Image Understanding, vol. 118, pp. 50\u201360, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Mining associated text and images with dual-wing harmoniums", "author": ["E.P. Xing", "R. Yan", "A.G. Hauptmann"], "venue": "UAI \u201905, Proceedings of the 21st Conference in Uncertainty in Artificial Intelligence, Edinburgh, Scotland, July 26-29, 2005, 2005, pp. 633\u2013641.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Predictive subspace learning for multi-view data: a large margin approach", "author": ["N. Chen", "J. Zhu", "E.P. Xing"], "venue": "NIPS, 2010, pp. 361\u2013369.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal learning with deep boltzmann machines", "author": ["N. Srivastava", "R. Salakhutdinov"], "venue": "NIPS, 2012, pp. 2231\u2013 2239.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Multimodal deep learning", "author": ["J. Ngiam", "A. Khosla", "M. Kim", "J. Nam", "H. Lee", "A.Y. Ng"], "venue": "ICML, 2011, pp. 689\u2013 696.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2011}, {"title": "Cross-modal retrieval with correspondence autoencoder", "author": ["F. Feng", "X. Wang", "R. Li"], "venue": "ACM Multimedia, 2014, pp. 7\u201316.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2014}, {"title": "On deep multi-view representation learning", "author": ["W. Wang", "R. Arora", "K. Livescu", "J.A. Bilmes"], "venue": "ICML, 2015, pp. 1083\u20131092.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "F. Li"], "venue": "CoRR, vol. abs/1412.2306, 2014.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "arXiv preprint arXiv:1412.6632, 2014.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L. Anne Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, 2015, pp. 2625\u2013 2634.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Semisupervised alignment of manifolds", "author": ["J. Ham", "D. Lee", "L. Saul"], "venue": "10th International Workshop on Artificial Intelligence and Statistics, 2005, pp. 120\u2013127.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2005}, {"title": "A new approach to cross-modal multimedia retrieval", "author": ["N. Rasiwasia", "J. Costa Pereira", "E. Coviello", "G. Doyle", "G.R. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "ACM Multimedia, 2010, pp. 251\u2013260.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2010}, {"title": "Pixels that sound.", "author": ["E. Kidron", "Y.Y. Schechner", "M. Elad"], "venue": "IEEE Computer Society,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2005}, {"title": "A least squares formulation for canonical correlation analysis", "author": ["L. Sun", "S. Ji", "J. Ye"], "venue": "ICML, 2008, pp. 1024\u2013 1031.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-view dimensionality reduction via canonical correlation analysis", "author": ["D.P. Foster", "R. Johnson", "T. Zhang"], "venue": "Tech. Rep., 2008.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2008}, {"title": "A scalable two-stage approach for a class of dimensionality reduction techniques", "author": ["L. Sun", "B. Ceran", "J. Ye"], "venue": "KDD, 2010, pp. 313\u2013322.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "ICML, 2013, pp. 347\u2013355.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Correlation clustering for learning mixtures of canonical correlation models", "author": ["X.Z. Fern", "C.E. Brodley", "M.A. Friedl"], "venue": "SDM, 2005, pp. 439\u2013448.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2005}, {"title": "Correlational spectral clustering", "author": ["M.B. Blaschko", "C.H. Lampert"], "venue": "CVPR, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-view clustering via canonical correlation analysis", "author": ["K. Chaudhuri", "S.M. Kakade", "K. Livescu", "K. Sridharan"], "venue": "ICML, 2009, pp. 129\u2013136.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-view regression via canonical correlation analysis", "author": ["S.M. Kakade", "D.P. Foster"], "venue": "COLT, 2007, pp. 82\u201396.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2007}, {"title": "Correlated random features for fast semi-supervised learning", "author": ["B. McWilliams", "D. Balduzzi", "J.M. Buhmann"], "venue": "NIPS, 2013, pp. 440\u2013448.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Multi-view learning of word embeddings via CCA", "author": ["P.S. Dhillon", "D.P. Foster", "L.H. Ungar"], "venue": "NIPS, 2011, pp. 199\u2013207.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2011}, {"title": "Using CCA to improve CCA: A new spectral method for estimating vector models of words", "author": ["P.S. Dhillon", "J. Rodu", "D.P. Foster", "L.H. Ungar"], "venue": "ICML, 2012.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2012}, {"title": "A multi-view embedding space for modeling internet images, tags, and their semantics", "author": ["Y. Gong", "Q. Ke", "M. Isard", "S. Lazebnik"], "venue": "International Journal of Computer Vision, vol. 106, no. 2, pp. 210\u2013233, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Discriminative learning and recognition of image set classes using canonical corre-  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  21 lations", "author": ["T. Kim", "J. Kittler", "R. Cipolla"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 6, pp. 1005\u20131018, 2007.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2007}, {"title": "Multi-label output codes using canonical correlation analysis.", "author": ["Y. Zhang", "J.G. Schneider"], "venue": "in AISTATS,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2011}, {"title": "Discriminant learning through multiple principal angles for visual recognition", "author": ["Y. Su", "Y. Fu", "X. Gao", "Q. Tian"], "venue": "IEEE Trans. Image Processing, vol. 21, no. 3, pp. 1381\u2013 1390, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "The canonical correlations of matrix pairs and their numerical computation", "author": ["G.H. Golub", "H. Zha"], "venue": "Stanford, CA, USA, Tech. Rep., 1992.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 1992}, {"title": "Canonical analysis of several sets of variables", "author": ["J.R. Kettenring"], "venue": "Biometrika, vol. 58, no. 3, pp. 433\u2013451, 1971.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1971}, {"title": "Efficient dimensionality reduction for canonical correlation analysis", "author": ["H. Avron", "C. Boutsidis", "S. Toledo", "A. Zouzias"], "venue": "SIAM J. Scientific Computing, vol. 36, no. 5, 2014.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2014}, {"title": "Improved analysis of the subsampled randomized hadamard transform", "author": ["J.A. Tropp"], "venue": "CoRR, vol. abs/1011.1595, 2010.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "large scale canonical correlation analysis with iterative least squares", "author": ["Y. Lu", "D.P. Foster"], "venue": "NIPS, 2014, pp. 91\u201399.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2014}, {"title": "Stochastic optimization for PCA and PLS", "author": ["R. Arora", "A. Cotter", "K. Livescu", "N. Srebro"], "venue": "50th Annual Allerton Conference on Communication, Control, and Computing, Allerton 2012, Allerton Park & Retreat Center, Monticello, IL, USA, October 1-5, 2012, 2012, pp. 861\u2013868.", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2012}, {"title": "Finding linear structure in large datasets with scalable canonical correlation analysis", "author": ["Z. Ma", "Y. Lu", "D.P. Foster"], "venue": "ICML, 2015, pp. 169\u2013178.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2015}, {"title": "Globally convergent stochastic optimization for canonical correlation analysis", "author": ["W. Wang", "J. Wang", "N. Srebro"], "venue": "CoRR, vol. abs/1604.01870, 2016.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 1870}, {"title": "Efficient algorithms for large-scale generalized eigenvector computation and canonical correlation analysis", "author": ["R. Ge", "C. Jin", "S.M. Kakade", "P. Netrapalli", "A. Sidford"], "venue": "CoRR, vol. abs/1604.03930, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "A probabilistic interpretation of canonical correlation analysis", "author": ["F.R. Bach", "M.I. Jordan"], "venue": "Department of Statistics, University of California, Berkeley, Tech. Rep., 2005.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2005}, {"title": "The maximum-likelihood solution in interbattery factor analysis", "author": ["M.W. Browne"], "venue": "British Journal of Mathematical and Statistical Psychology, vol. 32, no. 1, pp. 75\u201386, 1979.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 1979}, {"title": "Robust probabilistic projections", "author": ["C. Archambeau", "N. Delannay", "M. Verleysen"], "venue": "(ICML, 2006, pp. 33\u201340.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2006}, {"title": "Stochastic processes for canonical correlation analysis", "author": ["C. Fyfe", "G. Leen"], "venue": "ESANN, 2006, pp. 245\u2013250.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2006}, {"title": "Local dependent components", "author": ["A. Klami", "S. Kaski"], "venue": "ICML, 2007, pp. 425\u2013432.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 2007}, {"title": "Variational bayesian approach to canonical correlation analysis", "author": ["C. Wang"], "venue": "IEEE Trans. Neural Networks, vol. 18, no. 3, pp. 905\u2013910, 2007.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2007}, {"title": "Variational bayesian mixture of robust CCA models", "author": ["J. Viinikanoja", "A. Klami", "S. Kaski"], "venue": "ECML PKDD, 2010, pp. 370\u2013385.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2010}, {"title": "A generalization of principal components analysis to the exponential family", "author": ["M. Collins", "S. Dasgupta", "R.E. Schapire"], "venue": "NIPS, 2001, pp. 617\u2013624.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2001}, {"title": "Bayesian  exponential family PCA", "author": ["S. Mohamed", "K.A. Heller", "Z. Ghahramani"], "venue": "NIPS, 2008, pp. 1089\u20131096.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2008}, {"title": "Bayesian exponential family projections for coupled data sources", "author": ["A. Klami", "S. Virtanen", "S. Kaski"], "venue": "UAI, 2010, pp. 286\u2013293.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2010}, {"title": "Sparse bayesian multi-task learning", "author": ["C. Archambeau", "S. Guo", "O. Zoeter"], "venue": "NIPS, 2011, pp. 1755\u20131763.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2011}, {"title": "Robust bayesian matrix factorisation", "author": ["B. Lakshminarayanan", "G. Bouchard", "C. Archambeau"], "venue": "AISTATS, 2011, pp. 425\u2013433.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2011}, {"title": "Probabilistic partial canonical correlation analysis", "author": ["Y. Mukuta", "T. Harada"], "venue": "ICML, 2014, pp. 1449\u20131457.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2014}, {"title": "Partial canonical correlations", "author": ["RAO B.R."], "venue": "vol. 20, no. 2, 1969, pp. 211\u2013219.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 1969}, {"title": "Probabilistic semi-canonical correlation analysis", "author": ["C. Kamada", "A. Kanezaki", "T. Harada"], "venue": "ACM Multimedia, 2015, pp. 1131\u20131134.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2015}, {"title": "Two-way analysis of high-dimensional collinear data", "author": ["I. Huopaniemi", "T. Suvitaival", "J. Nikkil\u00e4", "M. Oresic", "S. Kaski"], "venue": "Data Min. Knowl. Discov., vol. 19, no. 2, pp. 261\u2013 276, 2009.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2009}, {"title": "Multivariate multi-way analysis of multi-source data", "author": ["\u2014\u2014"], "venue": "Bioinformatics, vol. 26, no. 12, pp. 391\u2013398, 2010.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2010}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society (Series B), vol. 58, pp. 267\u2013288, 1996.", "citeRegEx": "68", "shortCiteRegEx": null, "year": 1996}, {"title": "Sparse canonical correlation analysis", "author": ["D.R. Hardoon", "J. Shawe-Taylor"], "venue": "Department of Computer Science, University College London, Tech. Rep., 2007.", "citeRegEx": "69", "shortCiteRegEx": null, "year": 2007}, {"title": "Quantifying the association between gene expressions and DNA-markers by penalized canonical correlation analysis.", "author": ["S. Waaijenborg", "P.C. Verselewel de Witt Hamer", "A.H. Zwinderman"], "venue": "Statistical applications in genetics and molecular biology,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2008}, {"title": "Least angle regression", "author": ["B. Efron", "T. Hastie", "I. Johnstone", "R. Tibshirani"], "venue": "Annals of Statistics, vol. 32, pp. 407\u2013499, 2004.", "citeRegEx": "71", "shortCiteRegEx": null, "year": 2004}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "J. Mach. Learn. Res., vol. 7, pp. 2399\u20132434, Dec. 2006.", "citeRegEx": "72", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparse eigen methods by D.C. programming", "author": ["B.K. Sriperumbudur", "D.A. Torres", "G.R.G. Lanckriet"], "venue": "ICML, 2007, pp. 831\u2013838.", "citeRegEx": "73", "shortCiteRegEx": null, "year": 2007}, {"title": "Full regularization path for sparse principal component analysis", "author": ["A. d\u2019Aspremont", "F.R. Bach", "L.E. Ghaoui"], "venue": "ICML, 2007, pp. 177\u2013184.", "citeRegEx": "74", "shortCiteRegEx": null, "year": 2007}, {"title": "A direct formulation for sparse PCA using semidefinite programming", "author": ["A. d\u2019Aspremont", "L.E. Ghaoui", "M.I. Jordan", "G.R.G. Lanckriet"], "venue": "SIAM Review, vol. 49, no. 3, pp. 434\u2013448, 2007.", "citeRegEx": "75", "shortCiteRegEx": null, "year": 2007}, {"title": "Finding musically meaningful words by sparse cca", "author": ["D. Torres", "D. Turnbull", "L. Barrington", "B. Sriperumbudur", "G. Lanckriet"], "venue": "NIPS WMBC \u201907, December 2007.", "citeRegEx": "76", "shortCiteRegEx": null, "year": 2007}, {"title": "A greedy approach to sparse canonical correlation analysis", "author": ["A. Wiesel", "M. Kliger", "III A.O. Hero"], "venue": "ArXiv e-prints, 2008.", "citeRegEx": "77", "shortCiteRegEx": null, "year": 2008}, {"title": "A penalized matrix decomposition, with applications to sparse principal components and canonical correlation analysis", "author": ["D.M. Witten", "T. Hastie", "R. Tibshirani"], "venue": "Biostatistics, 2009.", "citeRegEx": "78", "shortCiteRegEx": null, "year": 2009}, {"title": "Structured sparse  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  22 canonical correlation analysis", "author": ["X. Chen", "H. Liu", "J.G. Carbonell"], "venue": "AISTATS, 2012, pp. 199\u2013 207.", "citeRegEx": "79", "shortCiteRegEx": null, "year": 2012}, {"title": "Two methods for sparsifying probabilistic canonical correlation analysis", "author": ["C. Fyfe", "G. Leen"], "venue": "ICONIP, 2006, pp. 361\u2013370.", "citeRegEx": "80", "shortCiteRegEx": null, "year": 2006}, {"title": "Sparse probabilistic projections", "author": ["C. Archambeau", "F.R. Bach"], "venue": "NIPS, 2008, pp. 73\u201380.", "citeRegEx": "81", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-label prediction via sparse infinite CCA", "author": ["P. Rai", "H.D. III"], "venue": "NIPS, 2009, pp. 1518\u20131526.", "citeRegEx": "82", "shortCiteRegEx": null, "year": 2009}, {"title": "Estimating image bases for visual image reconstruction from human brain activity", "author": ["Y. Fujiwara", "Y. Miyawaki", "Y. Kamitani"], "venue": "NIPS, 2009, pp. 576\u2013584.", "citeRegEx": "83", "shortCiteRegEx": null, "year": 2009}, {"title": "A nonparametric bayesian approach to modeling overlapping clusters", "author": ["K.A. Heller", "Z. Ghahramani"], "venue": "AISTATS, 2007, pp. 187\u2013194.", "citeRegEx": "84", "shortCiteRegEx": null, "year": 2007}, {"title": "An inter-battery method of factor analysis", "author": ["L.R. Tucker"], "venue": "Psychometrika, vol. 23, no. 2, pp. 111\u2013136, 1958.", "citeRegEx": "85", "shortCiteRegEx": null, "year": 1958}, {"title": "Bayesian canonical correlation analysis", "author": ["A. Klami", "S. Virtanen", "S. Kaski"], "venue": "Journal of Machine Learning Research, vol. 14, no. 1, pp. 965\u20131003, 2013.", "citeRegEx": "86", "shortCiteRegEx": null, "year": 2013}, {"title": "Three common factor models for groups of variables", "author": ["R. McDonald"], "venue": "Psychometrika, vol. 37, no. 1, pp. 173\u2013178, 1970.", "citeRegEx": "87", "shortCiteRegEx": null, "year": 1970}, {"title": "Factor analysis of multiple batteries by maximum likelihood", "author": ["M.W. Browne"], "venue": "British Journal of Mathematical and Statistical Psychology, vol. 33, pp. 184\u2013199, 1979.", "citeRegEx": "88", "shortCiteRegEx": null, "year": 1979}, {"title": "Group factor analysis", "author": ["A. Klami", "S. Virtanen", "E. Lepp\u00e4aho", "S. Kaski"], "venue": "IEEE Trans. Neural Netw. Learning Syst., vol. 26, no. 9, pp. 2136\u20132147, 2015.", "citeRegEx": "89", "shortCiteRegEx": null, "year": 2015}, {"title": "A survey on multi-view learning", "author": ["C. Xu", "D. Tao", "C. Xu"], "venue": "arXiv preprint arXiv:1304.5634, 2013.", "citeRegEx": "90", "shortCiteRegEx": null, "year": 2013}, {"title": "Kernel and nonlinear canonical correlation analysis", "author": ["P.L. Lai", "C. Fyfe"], "venue": "IJCNN (4), 2000, p. 614.", "citeRegEx": "91", "shortCiteRegEx": null, "year": 2000}, {"title": "A kernel method for canonical correlation analysis", "author": ["S. Akaho"], "venue": "International Meeting of the Psychometric Society, 2001.", "citeRegEx": "92", "shortCiteRegEx": null, "year": 2001}, {"title": "Nonlinear feature extraction using generalized canonical correlation analysis", "author": ["T. Melzer", "M. Reiter", "H. Bischof"], "venue": "ICANN, 2001, pp. 353\u2013360.", "citeRegEx": "93", "shortCiteRegEx": null, "year": 2001}, {"title": "Connecting modalities: Semisupervised segmentation and annotation of images using unaligned text corpora", "author": ["R. Socher", "F. Li"], "venue": "CVPR, 2010, pp. 966\u2013973.", "citeRegEx": "94", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning the relative importance of objects from tagged images for retrieval and crossmodal search.", "author": ["S.J. Hwang", "K. Grauman"], "venue": "International Journal of Computer Vision,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 2012}, {"title": "Extraction of correlated gene clusters from multiple genomic data by generalized kernel canonical correlation analysis", "author": ["Y. Yamanishi", "J. Vert", "A. Nakaya", "M. Kanehisa"], "venue": "ICISMB, 2003, pp. 323\u2013330.", "citeRegEx": "96", "shortCiteRegEx": null, "year": 2003}, {"title": "Semi-supervised kernel canonical correlation analysis with application to human fmri", "author": ["M.B. Blaschko", "J.A. Shelton", "A. Bartels", "C.H. Lampert", "A. Gretton"], "venue": "Pattern Recognition Letters, vol. 32, no. 11, pp. 1572\u20131583, 2011.", "citeRegEx": "97", "shortCiteRegEx": null, "year": 2011}, {"title": "Exploiting tag and word correlations for improved webpage clustering", "author": ["A. Trivedi", "P. Rai", "S.L. DuVall", "III H. Daum\u00e9"], "venue": "Proceedings of the 2Nd International Workshop on Search and Mining User-generated Contents, ser. SMUC \u201910, 2010, pp. 3\u201312.", "citeRegEx": "98", "shortCiteRegEx": null, "year": 2010}, {"title": "Kernel CCA for multi-view learning of acoustic features using articulatory measure-  ments", "author": ["R. Arora", "K. Livescu"], "venue": "MLSLP, 2012, pp. 34\u201337.", "citeRegEx": "99", "shortCiteRegEx": null, "year": 2012}, {"title": "Multi-view cca-based acoustic features for phonetic recognition across speakers and domains", "author": ["\u2014\u2014"], "venue": "ICASSP, 2013, pp. 7135\u20137139.", "citeRegEx": "100", "shortCiteRegEx": null, "year": 2013}, {"title": "The kernel mutual information", "author": ["A. Gretton", "R. Herbrich", "A.J. Smola"], "venue": "ICASSP, 2003, pp. 880\u2013884.", "citeRegEx": "101", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernel methods for measuring independence", "author": ["A. Gretton", "R. Herbrich", "A.J. Smola", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research, vol. 6, pp. 2075\u20132129, 2005.", "citeRegEx": "102", "shortCiteRegEx": null, "year": 2005}, {"title": "Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond", "author": ["B. Sch\u00f6lkopf", "A.J. Smola"], "venue": null, "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2001}, {"title": "The geometry of kernel canonical correlation analysis", "author": ["M. Kuss", "T. Graepel"], "venue": "Max Planck Institute for Biological Cybernetics, T\u00fcbingen, Germany, Tech. Rep. 108, may 2003.", "citeRegEx": "104", "shortCiteRegEx": null, "year": 2003}, {"title": "Statistical consistency of kernel canonical correlation analysis", "author": ["K. Fukumizu", "F.R. Bach", "A. Gretton"], "venue": "Journal of Machine Learning Research, vol. 8, pp. 361\u2013383, 2007.", "citeRegEx": "105", "shortCiteRegEx": null, "year": 2007}, {"title": "Convergence analysis of kernel canonical correlation analysis: theory and practice", "author": ["D.R. Hardoon", "J. Shawe-Taylor"], "venue": "Machine Learning, vol. 74, no. 1, pp. 23\u201338, 2009.", "citeRegEx": "106", "shortCiteRegEx": null, "year": 2009}, {"title": "Convergence rate of kernel canonical correlation analysis", "author": ["J. Cai", "H. Sun"], "venue": "Science China Mathematics, vol. 54, no. 10, pp. 2161\u20132170, 2011.", "citeRegEx": "107", "shortCiteRegEx": null, "year": 2011}, {"title": "Incremental singular value decomposition of uncertain data with missing values", "author": ["M. Brand"], "venue": "ECCV, 2002, pp. 707\u2013720.", "citeRegEx": "108", "shortCiteRegEx": null, "year": 2002}, {"title": "Using the nystr\u00f6m method to speed up kernel machines", "author": ["C.K.I. Williams", "M.W. Seeger"], "venue": "NIPS, 2000, pp. 682\u2013688.", "citeRegEx": "109", "shortCiteRegEx": null, "year": 2000}, {"title": "Nystr\u00f6m method vs random fourier features: A theoretical and empirical comparison", "author": ["T. Yang", "Y. Li", "M. Mahdavi", "R. Jin", "Z. Zhou"], "venue": "NIPS, 2012, pp. 485\u2013493.", "citeRegEx": "110", "shortCiteRegEx": null, "year": 2012}, {"title": "Fastfood - computing hilbert space expansions in loglinear time", "author": ["Q.V. Le", "T. Sarl\u00f3s", "A.J. Smola"], "venue": "ICML, 2013, pp. 244\u2013252.", "citeRegEx": "111", "shortCiteRegEx": null, "year": 2013}, {"title": "Randomized nonlinear component analysis", "author": ["D. Lopez-Paz", "S. Sra", "A.J. Smola", "Z. Ghahramani", "B. Sch\u00f6lkopf"], "venue": "ICML, 2014, pp. 1359\u20131367.", "citeRegEx": "112", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale approximate kernel canonical correlation analysis", "author": ["W. Wang", "K. Livescu"], "venue": "CoRR, vol. abs/1511.04773, 2015.", "citeRegEx": "113", "shortCiteRegEx": null, "year": 2015}, {"title": "Using KCCA for japaneseenglish cross-language information retrieval and document classification", "author": ["Y. Li", "J. Shawe-Taylor"], "venue": "J. Intell. Inf. Syst., vol. 27, no. 2, pp. 117\u2013 133, 2006.", "citeRegEx": "114", "shortCiteRegEx": null, "year": 2006}, {"title": "Two view learning: Svm-2k, theory and practice", "author": ["J.D.R. Farquhar", "D.R. Hardoon", "H. Meng", "J. Shawe- Taylor", "S. Szedm\u00e1k"], "venue": "NIPS, 2005, pp. 355\u2013362.", "citeRegEx": "115", "shortCiteRegEx": null, "year": 2005}, {"title": "A correlation approach for automatic image annotation", "author": ["D.R. Hardoon", "C. Saunders", "S. Szedm\u00e1k", "J. Shawe- Taylor"], "venue": "ADMA, 2006, pp. 681\u2013692.", "citeRegEx": "116", "shortCiteRegEx": null, "year": 2006}, {"title": "Framing image description as a ranking task: Data, models and evaluation metrics", "author": ["M. Hodosh", "P. Young", "J. Hockenmaier"], "venue": "J. Artif. Intell. Res. (JAIR), vol. 47, pp. 853\u2013899, 2013.", "citeRegEx": "117", "shortCiteRegEx": null, "year": 2013}, {"title": "Manifold regularization: A geometric framework for learning from labeled and unlabeled examples", "author": ["M. Belkin", "P. Niyogi", "V. Sindhwani"], "venue": "Journal of Machine Learning  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  23 Research, vol. 7, pp. 2399\u20132434, 2006.", "citeRegEx": "118", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-organizing neural network that discovers surfaces in random-dot stereograms", "author": ["S. Becker", "G.E. Hinton"], "venue": "Nature, vol. 355, no. 6356, pp. 161\u2013163, 1992.", "citeRegEx": "119", "shortCiteRegEx": null, "year": 1992}, {"title": "Mutual information maximization: Models of cortical self-organization.", "author": ["S. Becker"], "venue": "Network : Computation in Neural Systems,", "citeRegEx": "120", "shortCiteRegEx": "120", "year": 1996}, {"title": "Canonical correlation analysis using artificial neural networks", "author": ["P.L. Lai", "C. Fyfe"], "venue": "ESANN, 1998, pp. 363\u2013368.", "citeRegEx": "121", "shortCiteRegEx": null, "year": 1998}, {"title": "A neural implementation of canonical correlation analysis", "author": ["\u2014\u2014"], "venue": "Neural Networks, vol. 12, no. 10, pp. 1391\u20131397, 1999.", "citeRegEx": "122", "shortCiteRegEx": null, "year": 1999}, {"title": "Nonlinear canonical correlation analysis by neural networks", "author": ["W.W. Hsieh"], "venue": "Neural Networks, vol. 13, no. 10, pp. 1095\u20131105, 2000.", "citeRegEx": "123", "shortCiteRegEx": null, "year": 2000}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS, 2012, pp. 1097\u20131105.", "citeRegEx": "124", "shortCiteRegEx": null, "year": 2012}, {"title": "Unsupervised learning of acoustic features via deep canonical correlation analysis", "author": ["W. Wang", "R. Arora", "K. Livescu", "J.A. Bilmes"], "venue": "ICASSP, 2015, pp. 4590\u20134594.", "citeRegEx": "125", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep multilingual correlation for improved word embeddings", "author": ["A. Lu", "W. Wang", "M. Bansal", "K. Gimpel", "K. Livescu"], "venue": "HLT-NAACL, 2015, pp. 250\u2013256.", "citeRegEx": "126", "shortCiteRegEx": null, "year": 2015}, {"title": "Stochastic optimization for deep CCA via nonlinear orthogonal iterations", "author": ["W. Wang", "R. Arora", "K. Livescu", "N. Srebro"], "venue": "Allerton, 2015, pp. 688\u2013695.", "citeRegEx": "127", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep correlation for matching images and text", "author": ["F. Yan", "K. Mikolajczyk"], "venue": "CVPR, 2015, pp. 3441\u20133450.", "citeRegEx": "128", "shortCiteRegEx": null, "year": 2015}, {"title": "Probabilistic latent semantic analysis", "author": ["T. Hofmann"], "venue": "UAI, 1999, pp. 289\u2013296.", "citeRegEx": "129", "shortCiteRegEx": null, "year": 1999}, {"title": "Indexing by latent semantic analysis", "author": ["S. Deerwester", "S.T. Dumais", "G.W. Furnas", "T.K. Landauer", "R. Harshman"], "venue": "Journal of the American society for information science, vol. 41, no. 6, p. 391, 1990.", "citeRegEx": "130", "shortCiteRegEx": null, "year": 1990}, {"title": "Learning to probabilistically identify authoritative documents", "author": ["D. Cohn", "H. Chang"], "venue": "ICML, 2000, pp. 167\u2013 174.", "citeRegEx": "131", "shortCiteRegEx": null, "year": 2000}, {"title": "Plsa-based image autoannotation: Constraining the latent space", "author": ["F. Monay", "D. Gatica-Perez"], "venue": "ACM Multimedia, 2004, pp. 348\u2013351.", "citeRegEx": "132", "shortCiteRegEx": null, "year": 2004}, {"title": "A probabilistic semantic model for image annotation and multi-modal image retrieva", "author": ["R. Zhang", "Z.M. Zhang", "M. Li", "W. Ma", "H. Zhang"], "venue": "ICCV, 2005, pp. 846\u2013851.", "citeRegEx": "133", "shortCiteRegEx": null, "year": 2005}, {"title": "Modeling semantic aspects for cross-media image indexing", "author": ["F. Monay", "D. Gatica-Perez"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 10, pp. 1802\u20131817, 2007.", "citeRegEx": "134", "shortCiteRegEx": null, "year": 1802}, {"title": "Semisupervised topic modeling for image annotation", "author": ["Y. Shao", "Y. Zhou", "X. He", "D. Cai", "H. Bao"], "venue": "ACM Multimedia, 2009, pp. 521\u2013524.", "citeRegEx": "135", "shortCiteRegEx": null, "year": 2009}, {"title": "Multilayer plsa for multimodal image retrieval", "author": ["R. Lienhart", "S. Romberg", "E. H\u00f6rster"], "venue": "ACM CIVR, 2009.", "citeRegEx": "136", "shortCiteRegEx": null, "year": 2009}, {"title": "Heterogeneous transfer learning for image clustering via the socialweb", "author": ["Q. Yang", "Y. Chen", "G. Xue", "W. Dai", "Y. Yu"], "venue": "ACL, 2009, pp. 1\u20139.", "citeRegEx": "137", "shortCiteRegEx": null, "year": 2009}, {"title": "A feature-word-topic model for image annotation", "author": ["C. Nguyen", "N. Kaothanthong", "X.H. Phan", "T. Tokuyama"], "venue": "ACM CIKM, 2010, pp. 1481\u20131484.", "citeRegEx": "138", "shortCiteRegEx": null, "year": 2010}, {"title": "Automatic image annotation with continuous PLSA", "author": ["Z. Li", "Z. Shi", "X. Liu", "Z. Shi"], "venue": "ICASSP, 2010, pp. 806\u2013809.", "citeRegEx": "139", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi modal semantic indexing for image retrieval", "author": ["C. Pulla", "C.V. Jawahar"], "venue": "ACM CIVR, 2010, pp. 342\u2013349.", "citeRegEx": "140", "shortCiteRegEx": null, "year": 2010}, {"title": "Multi-feature plsa for combining visual features in image annotation", "author": ["R. Zhang", "L. Zhang", "X. Wang", "L. Guan"], "venue": "ACM Multimedia, 2011, pp. 1513\u20131516.", "citeRegEx": "141", "shortCiteRegEx": null, "year": 2011}, {"title": "Latent dirichlet allocation", "author": ["D.M. Blei", "A.Y. Ng", "M.I. Jordan"], "venue": "Journal of Machine Learning Research, vol. 3, pp. 993\u20131022, 2003.", "citeRegEx": "142", "shortCiteRegEx": null, "year": 2003}, {"title": "Hierarchical dirichlet processes", "author": ["Y.W. Teh", "M.I. Jordan", "M.J. Beal", "D.M. Blei"], "venue": "Journal of the American Statistical Association, vol. 101, 2004.", "citeRegEx": "143", "shortCiteRegEx": null, "year": 2004}, {"title": "Multi-modal hierarchical dirichlet process model for predicting image annotation and image-object label correspondence", "author": ["O. Yakhnenko", "V. Honavar"], "venue": "SDM, 2009, pp. 283\u2013293.", "citeRegEx": "144", "shortCiteRegEx": null, "year": 2009}, {"title": "Gibbs sampling methods for stick-breaking priors", "author": ["H. Ishwaran", "L.F. James"], "venue": "Journal of the American Statistical Association, vol. 96, no. 453, pp. 161\u2013173, 2001.", "citeRegEx": "145", "shortCiteRegEx": null, "year": 2001}, {"title": "Mining partially annotated images", "author": ["Z. Qi", "M. Yang", "Z.M. Zhang", "Z. Zhang"], "venue": "KDD, 2011, pp. 1199\u20131207.", "citeRegEx": "146", "shortCiteRegEx": null, "year": 2011}, {"title": "Large margin learning of upstream scene understanding models", "author": ["J. Zhu", "L. Li", "F. Li", "E.P. Xing"], "venue": "NIPS, 2010, pp. 2586\u20132594.", "citeRegEx": "147", "shortCiteRegEx": null, "year": 2010}, {"title": "Simultaneous image classification and annotation", "author": ["C. Wang", "D.M. Blei", "F. Li"], "venue": "CVPR, 2009, pp. 1903\u2013 1910.", "citeRegEx": "148", "shortCiteRegEx": null, "year": 2009}, {"title": "Supervised topic models", "author": ["D.M. Blei", "J.D. McAuliffe"], "venue": "NIPS, 2007, pp. 121\u2013128.", "citeRegEx": "149", "shortCiteRegEx": null, "year": 2007}, {"title": "What, where and who? classifying events by scene and object recognition", "author": ["L. Li", "F. Li"], "venue": "ICCV, 2007, pp. 1\u20138.", "citeRegEx": "150", "shortCiteRegEx": null, "year": 2007}, {"title": "Spatially coherent latent topic model for concurrent segmentation and classification of objects and scenes", "author": ["L. Cao", "F. Li"], "venue": "ICCV, 2007, pp. 1\u20138.", "citeRegEx": "151", "shortCiteRegEx": null, "year": 2007}, {"title": "Towards total scene understanding: Classification, annotation and segmentation in an automatic framework", "author": ["L. Li", "R. Socher", "F. Li"], "venue": "CVPR, 2009, pp. 2036\u20132043.", "citeRegEx": "152", "shortCiteRegEx": null, "year": 2009}, {"title": "Multi-modal image annotation with multi-instance multi-label LDA", "author": ["C. Nguyen", "D. Zhan", "Z. Zhou"], "venue": "IJCAI, 2013.", "citeRegEx": "153", "shortCiteRegEx": null, "year": 2013}, {"title": "Gaussian processes for machine learning.", "author": ["C.E. Rasmussen"], "venue": null, "citeRegEx": "154", "shortCiteRegEx": "154", "year": 2006}, {"title": "Gaussian process latent variable models for human pose estimation", "author": ["C.H. Ek", "P.H.S. Torr", "N.D. Lawrence"], "venue": "MLMI, 2007, pp. 132\u2013143.", "citeRegEx": "155", "shortCiteRegEx": null, "year": 2007}, {"title": "Gaussian process latent variable models for visualisation of high dimensional data", "author": ["N.D. Lawrence"], "venue": "NIPS, 2003, pp. 329\u2013336.", "citeRegEx": "156", "shortCiteRegEx": null, "year": 2003}, {"title": "Learning shared latent structure for image synthesis and robotic imitation", "author": ["A.P. Shon", "K. Grochow", "A. Hertzmann", "R.P.N. Rao"], "venue": "NIPS, 2005, pp. 1233\u20131240.", "citeRegEx": "157", "shortCiteRegEx": null, "year": 2005}, {"title": "Discriminative gaussian process latent variable model for classification", "author": ["R. Urtasun", "T. Darrell"], "venue": "ICML, 2007, pp. 927\u2013934.", "citeRegEx": "158", "shortCiteRegEx": null, "year": 2007}, {"title": "Discriminative shared gaussian processes for multiview and viewinvariant facial expression recognition", "author": ["S. Eleftheriadis", "O. Rudovic", "M. Pantic"], "venue": "IEEE Trans. Image Processing, vol. 24, no. 1, pp. 189\u2013204, 2015.", "citeRegEx": "159", "shortCiteRegEx": null, "year": 2015}, {"title": "Large-margin multiview gaussian process for image classification", "author": ["C. Xu", "D. Tao", "Y. Li", "C. Xu"], "venue": "ICIMCS, 2013, pp. 7\u201312.", "citeRegEx": "160", "shortCiteRegEx": null, "year": 2013}, {"title": "Similarity  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  24 gaussian process latent variable model for multi-modal data analysis", "author": ["G. Song", "S. Wang", "Q. Huang", "Q. Tian"], "venue": "ICCV, 2015, pp. 4050\u20134058.", "citeRegEx": "161", "shortCiteRegEx": null, "year": 2015}, {"title": "Relational learning via collective matrix factorization", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "SIGKDD, 2008, pp. 650\u2013 658.", "citeRegEx": "162", "shortCiteRegEx": null, "year": 2008}, {"title": "Multi-label informed latent semantic indexing", "author": ["K. Yu", "S. Yu", "V. Tresp"], "venue": "SIGIR, 2005, pp. 258\u2013265.", "citeRegEx": "163", "shortCiteRegEx": null, "year": 2005}, {"title": "Spectral clustering for multi-type relational data", "author": ["B. Long", "Z.M. Zhang", "X. Wu", "P.S. Yu"], "venue": "ICML, 2006, pp. 585\u2013592.", "citeRegEx": "164", "shortCiteRegEx": null, "year": 2006}, {"title": "A probabilistic framework for relational clustering", "author": ["B. Long", "Z.M. Zhang", "P.S. Yu"], "venue": "KDD, 2007, pp. 470\u2013479.", "citeRegEx": "165", "shortCiteRegEx": null, "year": 2007}, {"title": "Relation-Prediction in Multi-Relational Domains using Matrix-Factorization", "author": ["C. Lippert", "S.H. Weber", "Y. Huang", "V. Tresp", "M. Schubert", "H.-P. Kriegel"], "venue": "NIPS 2008 Workshop: Structured Input - Structured Output, 2008.", "citeRegEx": "166", "shortCiteRegEx": null, "year": 2008}, {"title": "Group-sparse embeddings in collective matrix factorization", "author": ["A. Klami", "G. Bouchard", "A. Tripathi"], "venue": "CoRR, vol. abs/1312.5921, 2013.", "citeRegEx": "167", "shortCiteRegEx": null, "year": 2013}, {"title": "Convex collective matrix factorization", "author": ["G. Bouchard", "D. Yin", "S. Guo"], "venue": "AISTATS, 2013, pp. 144\u2013152.", "citeRegEx": "168", "shortCiteRegEx": null, "year": 2013}, {"title": "Consistent collective matrix completion under joint low rank structure", "author": ["S. Gunasekar", "M. Yamada", "D. Yin", "Y. Chang"], "venue": "AISTATS, 2015.", "citeRegEx": "169", "shortCiteRegEx": null, "year": 2015}, {"title": "Partial collective matrix factorization and its pac bound", "author": ["C. Lan", "X. Li", "Y. Deng", "J. Huan"], "venue": "2016.", "citeRegEx": "170", "shortCiteRegEx": null, "year": 2016}, {"title": "Image annotation using multi-correlation probabilistic matrix factorization", "author": ["Z. Li", "J. Liu", "X. Zhu", "T. Liu", "H. Lu"], "venue": "ACM Multimedia, 2010, pp. 1187\u20131190.", "citeRegEx": "171", "shortCiteRegEx": null, "year": 2010}, {"title": "Combining content and link for classification using matrix factorization", "author": ["S. Zhu", "K. Yu", "Y. Chi", "Y. Gong"], "venue": "SIGIR, 2007, pp. 487\u2013494.", "citeRegEx": "172", "shortCiteRegEx": null, "year": 2007}, {"title": "A bayesian matrix factorization model for relational data", "author": ["A.P. Singh", "G.J. Gordon"], "venue": "CoRR, vol. abs/1203.3517, 2012.", "citeRegEx": "173", "shortCiteRegEx": null, "year": 2012}, {"title": "A probabilistic model for multimodal hash function learning", "author": ["Y. Zhen", "D. Yeung"], "venue": "SIGKDD, 2012, pp. 940\u2013 948.", "citeRegEx": "174", "shortCiteRegEx": null, "year": 2012}, {"title": "Collective matrix factorization hashing for multimodal data", "author": ["G. Ding", "Y. Guo", "J. Zhou"], "venue": "CVPR, 2014, pp. 2083\u20132090.", "citeRegEx": "175", "shortCiteRegEx": null, "year": 2014}, {"title": "Non-negative Matrix Factorization in Multimodality Data for Segmentation and Label Prediction", "author": ["Z. Akata", "C. Thurau", "C. Bauckhage"], "venue": "16th Computer Vision Winter Workshop, 2011.", "citeRegEx": "176", "shortCiteRegEx": null, "year": 2011}, {"title": "Multi-view clustering via joint nonnegative matrix factorization", "author": ["J. Liu", "C. Wang", "J. Gao", "J. Han"], "venue": "SDM, 2013, pp. 252\u2013260.", "citeRegEx": "177", "shortCiteRegEx": null, "year": 2013}, {"title": "Data fusion by matrix factorization", "author": ["M. Zitnik", "B. Zupan"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 37, no. 1, pp. 41\u201353, 2015.", "citeRegEx": "178", "shortCiteRegEx": null, "year": 2015}, {"title": "Robust and non-negative collective matrix factorization for text-to-image transfer learning", "author": ["L. Yang", "L. Jing", "M.K. Ng"], "venue": "IEEE Trans. Image Processing, vol. 24, no. 12, pp. 4701\u20134714, 2015.", "citeRegEx": "179", "shortCiteRegEx": null, "year": 2015}, {"title": "Pattern recognition and machine", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "180", "shortCiteRegEx": "180", "year": 2006}, {"title": "Sparse unsupervised dimensionality reduction for multiple view data", "author": ["Y. Han", "F. Wu", "D. Tao", "J. Shao", "Y. Zhuang", "J. Jiang"], "venue": "IEEE Trans. Circuits Syst. Video Techn., vol. 22,  no. 10, pp. 1485\u20131496, 2012.", "citeRegEx": "181", "shortCiteRegEx": null, "year": 2012}, {"title": "Click prediction for web image reranking using multimodal sparse coding", "author": ["J. Yu", "Y. Rui", "D. Tao"], "venue": "IEEE Transactions on Image Processing, vol. 23, no. 5, pp. 2019\u2013 2032, 2014.", "citeRegEx": "182", "shortCiteRegEx": null, "year": 2019}, {"title": "Supervised coupled dictionary learning with group structures for multi-modal retrieval", "author": ["Y. Zhuang", "Y. Wang", "F. Wu", "Y. Zhang", "W. Lu"], "venue": "AAAI, 2013.", "citeRegEx": "183", "shortCiteRegEx": null, "year": 2013}, {"title": "Sparse multi-modal hashing", "author": ["F. Wu", "Z. Yu", "Y. Yang", "S. Tang", "Y. Zhang", "Y. Zhuang"], "venue": "IEEE Trans. Multimedia, vol. 16, no. 2, pp. 427\u2013439, 2014.", "citeRegEx": "184", "shortCiteRegEx": null, "year": 2014}, {"title": "Exponential family harmoniums with an application to information retrieval", "author": ["M. Welling", "M. Rosen-Zvi", "G.E. Hinton"], "venue": "NIPS, 2004, pp. 1481\u20131488.", "citeRegEx": "185", "shortCiteRegEx": null, "year": 2004}, {"title": "Training products of experts by minimizing contrastive divergence", "author": ["G.E. Hinton"], "venue": "Neural Computation, vol. 14, no. 8, pp. 1771\u20131800, 2002.", "citeRegEx": "186", "shortCiteRegEx": null, "year": 2002}, {"title": "Multi-modal distance metric learning", "author": ["P. Xie", "E.P. Xing"], "venue": "IJCAI, 2013, pp. 1806\u20131812.", "citeRegEx": "187", "shortCiteRegEx": null, "year": 2013}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "A.Y. Ng", "M.I. Jordan", "S.J. Russell"], "venue": "NIPS, 2002, pp. 505\u2013512.", "citeRegEx": "188", "shortCiteRegEx": null, "year": 2002}, {"title": "Soft modeling: the basic design and some extensions", "author": ["H. Wold"], "venue": "Systems under indirect observation, vol. 2, pp. 589\u2013591, 1982.", "citeRegEx": "189", "shortCiteRegEx": null, "year": 1982}, {"title": "Human detection using partial least squares analysis", "author": ["W.R. Schwartz", "A. Kembhavi", "D. Harwood", "L.S. Davis"], "venue": "ICCV, 2009, pp. 24\u201331.", "citeRegEx": "191", "shortCiteRegEx": null, "year": 2009}, {"title": "Pls-regression: a basic tool of chemometrics", "author": ["S. Wold", "M. Sjstrm", "L. Eriksson"], "venue": "Chemometrics and Intelligent Laboratory Systems, vol. 58, pp. 109\u2013130, 2001.", "citeRegEx": "192", "shortCiteRegEx": null, "year": 2001}, {"title": "Partial least squares for discrimination", "author": ["M. Barker", "W. Rayens"], "venue": "Journal of Chemometrics, vol. 17, no. 3, pp. 166 \u2013 173, 2003.", "citeRegEx": "193", "shortCiteRegEx": null, "year": 2003}, {"title": "Joint estimation of age, gender and ethnicity: CCA vs. PLS", "author": ["G. Guo", "G. Mu"], "venue": "FG, 2013, pp. 1\u20136.", "citeRegEx": "194", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimedia content processing through cross-modal association", "author": ["D. Li", "N. Dimitrova", "M. Li", "I.K. Sethi"], "venue": "ACM Multimedia, 2003, pp. 604\u2013611.", "citeRegEx": "195", "shortCiteRegEx": null, "year": 2003}, {"title": "Kernel cross-modal factor analysis for information fusion with application to bimodal emotion recognition", "author": ["Y. Wang", "L. Guan", "A.N. Venetsanopoulos"], "venue": "IEEE Transactions on Multimedia, vol. 14, no. 3-1, pp. 597\u2013607, 2012.", "citeRegEx": "196", "shortCiteRegEx": null, "year": 2012}, {"title": "On the role of correlation and abstraction in cross-modal multimedia retrieval", "author": ["J.C. Pereira", "E. Coviello", "G. Doyle", "N. Rasiwasia", "G.R.G. Lanckriet", "R. Levy", "N. Vasconcelos"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 3, pp. 521\u2013535, 2014.", "citeRegEx": "197", "shortCiteRegEx": null, "year": 2014}, {"title": "Supervised cross-modal factor analysis", "author": ["J. Wang", "H. Wang", "Y. Tu", "K. Duan", "Z. Zhan", "S. Chekuri"], "venue": "CoRR, vol. abs/1502.05134, 2015.", "citeRegEx": "198", "shortCiteRegEx": null, "year": 2015}, {"title": "Characterizing the response of PET and fMRI data using multivariate linear models", "author": ["K. Worsley", "J.-B. Poline", "K. Friston", "A. Evans"], "venue": "Neuroimage, vol. 6, no. 4, pp. 305\u2013319, 1997.", "citeRegEx": "199", "shortCiteRegEx": null, "year": 1997}, {"title": "Efficient kernel orthonormalized PLS for remote sensing applications", "author": ["J. Arenas-Garc\u0131\u0301a", "G. Camps-Valls"], "venue": "IEEE Trans. Geoscience and Remote Sensing, vol. 46, no. 10, pp. 2872\u20132881, 2008.  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  25", "citeRegEx": "200", "shortCiteRegEx": null, "year": 2008}, {"title": "Sparse and kernel opls feature extraction based on eigenvalue problem solving", "author": ["S. Mu\u00f1oz Romero", "J. Arenas-Garc\u0131\u0301a", "V. G\u00f3mez- Verdejo"], "venue": "Pattern Recognition, vol. 48, no. 5, pp. 1797\u20131811, May 2015.", "citeRegEx": "201", "shortCiteRegEx": null, "year": 1811}, {"title": "On the equivalence between canonical correlation analysis and orthonormalized partial least squares", "author": ["L. Sun", "S. Ji", "S. Yu", "J. Ye"], "venue": "IJCAI, 2009, pp. 1230\u20131235.", "citeRegEx": "202", "shortCiteRegEx": null, "year": 2009}, {"title": "Eigenspectra, a robust regression method for multiplexed raman spectra analysis", "author": ["S. Li", "J. Gao", "J.O. Nyagilo", "D.P. Dave"], "venue": "BIBM, 2010, pp. 525\u2013530.", "citeRegEx": "203", "shortCiteRegEx": null, "year": 2010}, {"title": "Probabilistic partial least square regression: A robust model for quantitative analysis of raman spectroscopy data", "author": ["\u2014\u2014"], "venue": "BIBM, 2011, pp. 526\u2013531.", "citeRegEx": "204", "shortCiteRegEx": null, "year": 2011}, {"title": "Multiview fisher discriminant analysis", "author": ["T. Diethe", "D.R. Hardoon", "J. Shawe-taylor"], "venue": "In NIPS Workshop on Learning from Multiple Sources, 2008.", "citeRegEx": "205", "shortCiteRegEx": null, "year": 2008}, {"title": "Constructing nonlinear discriminants from multiple data views", "author": ["T. Diethe", "D.R. Hardoon", "J. Shawe-Taylor"], "venue": "ECML PKDD, 2010, pp. 328\u2013343.", "citeRegEx": "206", "shortCiteRegEx": null, "year": 2010}, {"title": "Multiview discriminant analysis", "author": ["M. Kan", "S. Shan", "H. Zhang", "S. Lao", "X. Chen"], "venue": "ECCV, 2012, pp. 808\u2013821.", "citeRegEx": "207", "shortCiteRegEx": null, "year": 2012}, {"title": "An improved training algorithm for kernel fisher discriminants", "author": ["S. Mika", "A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "AIS- TATS, 2001.", "citeRegEx": "208", "shortCiteRegEx": null, "year": 2001}, {"title": "Separating style and content with bilinear models", "author": ["J.B. Tenenbaum", "W.T. Freeman"], "venue": "Neural Computation, vol. 12, no. 6, pp. 1247\u20131283, 2000.", "citeRegEx": "209", "shortCiteRegEx": null, "year": 2000}, {"title": "Graph embedding and extensions: A general framework for dimensionality reduction", "author": ["S. Yan", "D. Xu", "B. Zhang", "H. Zhang", "Q. Yang", "S. Lin"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 1, pp. 40\u201351, 2007.", "citeRegEx": "210", "shortCiteRegEx": null, "year": 2007}, {"title": "Generalized multiview analysis: A discriminative latent space", "author": ["A. Sharma", "A. Kumar", "H.D. III", "D.W. Jacobs"], "venue": "CVPR, 2012, pp. 2160\u20132167.", "citeRegEx": "211", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning to rank with (a lot of) word features", "author": ["B. Bai", "J. Weston", "D. Grangier", "R. Collobert", "K. Sadamasa", "Y. Qi", "O. Chapelle", "K.Q. Weinberger"], "venue": "Inf. Retr., vol. 13, no. 3, pp. 291\u2013314, 2010.", "citeRegEx": "212", "shortCiteRegEx": null, "year": 2010}, {"title": "Large scale image annotation: learning to rank with joint word-image embeddings", "author": ["J. Weston", "S. Bengio", "N. Usunier"], "venue": "Machine Learning, vol. 81, no. 1, pp. 21\u201335, 2010.", "citeRegEx": "213", "shortCiteRegEx": null, "year": 2010}, {"title": "A discriminative kernel-based approach to rank images from text queries", "author": ["D. Grangier", "S. Bengio"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 8, pp. 1371\u20131384, 2008.", "citeRegEx": "214", "shortCiteRegEx": null, "year": 2008}, {"title": "Large margin rank boundaries for ordinal regression", "author": ["R. Herbrich", "T. Graepel", "K. Obermayer"], "venue": "Advances in Large Margin Classifiers. Cambridge, MA: MIT Press, 2000.", "citeRegEx": "215", "shortCiteRegEx": null, "year": 2000}, {"title": "Learning to rank using gradient descent", "author": ["C.J.C. Burges", "T. Shaked", "E. Renshaw", "A. Lazier", "M. Deeds", "N. Hamilton", "G.N. Hullender"], "venue": "ICML, 2005, pp. 89\u201396.", "citeRegEx": "216", "shortCiteRegEx": null, "year": 2005}, {"title": "New ranking algorithms for parsing and tagging: Kernels over discrete structures, and the voted perceptron", "author": ["M. Collins", "N. Duffy"], "venue": "ACL, 2002, pp. 263\u2013270.", "citeRegEx": "217", "shortCiteRegEx": null, "year": 2002}, {"title": "Links between perceptrons, mlps and svms", "author": ["R. Collobert", "S. Bengio"], "venue": "ICML, 2004.", "citeRegEx": "218", "shortCiteRegEx": null, "year": 2004}, {"title": "A low rank structural large margin method for cross-modal ranking", "author": ["X. Lu", "F. Wu", "S. Tang", "Z. Zhang", "X. He", "Y. Zhuang"], "venue": "SIGIR, 2013, pp. 433\u2013442.", "citeRegEx": "219", "shortCiteRegEx": null, "year": 2013}, {"title": "Cross-modal learning to rank via latent joint representation", "author": ["F. Wu", "X. Jiang", "X. Li", "S. Tang", "W. Lu", "Z. Zhang", "Y. Zhuang"], "venue": "IEEE Trans. Image Processing, vol. 24, no. 5, pp. 1497\u20131509, 2015.", "citeRegEx": "221", "shortCiteRegEx": null, "year": 2015}, {"title": "Data fusion through cross-modality metric learning using similarity-sensitive hashing", "author": ["M.M. Bronstein", "A.M. Bronstein", "F. Michel", "N. Paragios"], "venue": "CVPR, 2010, pp. 3594\u20133601.", "citeRegEx": "222", "shortCiteRegEx": null, "year": 2010}, {"title": "Learning hash functions for cross-view similarity search", "author": ["S. Kumar", "R. Udupa"], "venue": "IJCAI, 2011, pp. 1360\u2013 1365.", "citeRegEx": "223", "shortCiteRegEx": null, "year": 2011}, {"title": "Iterative quantization: A procrustean approach to learning binary codes", "author": ["Y. Gong", "S. Lazebnik"], "venue": "CVPR, 2011, pp. 817\u2013824.", "citeRegEx": "224", "shortCiteRegEx": null, "year": 2011}, {"title": "Co-regularized hashing for multimodal data", "author": ["Y. Zhen", "D. Yeung"], "venue": "NIPS, 2012, pp. 1385\u20131393.", "citeRegEx": "225", "shortCiteRegEx": null, "year": 2012}, {"title": "Linear cross-modal hashing for efficient multimedia search", "author": ["X. Zhu", "Z. Huang", "H.T. Shen", "X. Zhao"], "venue": "ACM Multimedia Conference, MM \u201913, Barcelona, Spain, October 21-25, 2013, 2013, pp. 143\u2013152.", "citeRegEx": "226", "shortCiteRegEx": null, "year": 2013}, {"title": "Parametric local multimodal hashing for cross-view similarity search", "author": ["D. Zhai", "H. Chang", "Y. Zhen", "X. Liu", "X. Chen", "W. Gao"], "venue": "IJCAI, 2013, pp. 2754\u20132760.", "citeRegEx": "227", "shortCiteRegEx": null, "year": 2013}, {"title": "Collaborative hashing", "author": ["X. Liu", "J. He", "C. Deng", "B. Lang"], "venue": "CVPR, 2014, pp. 2147\u20132154.", "citeRegEx": "228", "shortCiteRegEx": null, "year": 2014}, {"title": "Large-scale supervised multimodal hashing with semantic correlation maximization", "author": ["D. Zhang", "W. Li"], "venue": "AAAI, 2014, pp. 2177\u20132183.", "citeRegEx": "229", "shortCiteRegEx": null, "year": 2014}, {"title": "Semanticspreserving hashing for cross-view retrieval", "author": ["Z. Lin", "G. Ding", "M. Hu", "J. Wang"], "venue": "CVPR, 2015, pp. 3864\u20133872.", "citeRegEx": "230", "shortCiteRegEx": null, "year": 2015}, {"title": "Spectral multimodal hashing and its application to multimedia retrieval", "author": ["Y. Zhen", "Y. Gao", "D. Yeung", "H. Zha", "X. Li"], "venue": "IEEE Trans. Cybernetics, vol. 46, no. 1, pp. 27\u201338, 2016.", "citeRegEx": "231", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning task-specific similarity", "author": ["G. Shakhnarovich"], "venue": "Ph.D. dissertation, Cambridge, MA, USA, 2005.", "citeRegEx": "232", "shortCiteRegEx": null, "year": 2005}, {"title": "A decision-theoretic generalization of on-line learning and an application to boosting", "author": ["Y. Freund", "R.E. Schapire"], "venue": "J. Comput. Syst. Sci., vol. 55, no. 1, pp. 119\u2013139, 1997.", "citeRegEx": "233", "shortCiteRegEx": null, "year": 1997}, {"title": "Discriminative coupled dictionary hashing for fast crossmedia retrieval", "author": ["Z. Yu", "F. Wu", "Y. Yang", "Q. Tian", "J. Luo", "Y. Zhuang"], "venue": "SIGIR, 2014, pp. 395\u2013404.", "citeRegEx": "234", "shortCiteRegEx": null, "year": 2014}, {"title": "Spectral hashing", "author": ["Y. Weiss", "A. Torralba", "R. Fergus"], "venue": "NIPS, 2008, pp. 1753\u20131760.", "citeRegEx": "235", "shortCiteRegEx": null, "year": 2008}, {"title": "Hashing with graphs", "author": ["W. Liu", "J. Wang", "S. Kumar", "S. Chang"], "venue": "ICML, 2011, pp. 1\u20138.", "citeRegEx": "236", "shortCiteRegEx": null, "year": 2011}, {"title": "Intermedia hashing for large-scale retrieval from heterogeneous data sources", "author": ["J. Song", "Y. Yang", "Y. Yang", "Z. Huang", "H.T. Shen"], "venue": "SIGMOD, 2013, pp. 785\u2013796.", "citeRegEx": "237", "shortCiteRegEx": null, "year": 2013}, {"title": "Latent semantic sparse hashing for cross-modal similarity search", "author": ["J. Zhou", "G. Ding", "Y. Guo"], "venue": "SIGIR, 2014, pp. 415\u2013424.", "citeRegEx": "238", "shortCiteRegEx": null, "year": 2014}, {"title": "Quantized correlation hashing for fast cross-modal search", "author": ["B. Wu", "Q. Yang", "W. Zheng", "Y. Wang", "J. Wang"], "venue": "IJCAI, 2015, pp. 3946\u20133952.", "citeRegEx": "239", "shortCiteRegEx": null, "year": 2015}, {"title": "Replicated softmax: an undirected topic model", "author": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "NIPS, 2009, pp. 1607\u20131614.", "citeRegEx": "241", "shortCiteRegEx": null, "year": 2009}, {"title": "Audio-visual deep learning for noise robust speech recognition", "author": ["J. Huang", "B. Kingsbury"], "venue": "ICASSP, 2013, pp. 7596\u20137599.", "citeRegEx": "242", "shortCiteRegEx": null, "year": 2013}, {"title": "Multimodal dbn for predicting high-quality answers in cqa portals.", "author": ["H. Hu", "B. Liu", "B. Wang", "M. Liu", "X. Wang"], "venue": null, "citeRegEx": "243", "shortCiteRegEx": "243", "year": 2013}, {"title": "Multi-source deep learning for information trustworthiness estimation", "author": ["L. Ge", "J. Gao", "X. Li", "A. Zhang"], "venue": "KDD, 2013, pp. 766\u2013774.", "citeRegEx": "244", "shortCiteRegEx": null, "year": 2013}, {"title": "Mutlimodal learning with deep boltzmann machine for emotion prediction in user generated videos", "author": ["L. Pang", "C.-W. Ngo"], "venue": "ACM ICMR, 2015, pp. 619\u2013622.", "citeRegEx": "245", "shortCiteRegEx": null, "year": 2015}, {"title": "Improved multimodal deep learning with variation of information", "author": ["K. Sohn", "W. Shang", "H. Lee"], "venue": "NIPS, 2014, pp. 2141\u20132149.", "citeRegEx": "246", "shortCiteRegEx": null, "year": 2014}, {"title": "Facial attributes classification using multi-task representation learning", "author": ["M. Ehrlich", "T.J. Shields", "T. Almaev", "M.R. Amer"], "venue": "CVPR Workshops, 2016, pp. 47\u201355.", "citeRegEx": "247", "shortCiteRegEx": null, "year": 2016}, {"title": "Classification using discriminative restricted boltzmann machines", "author": ["H. Larochelle", "Y. Bengio"], "venue": "ICML, 2008, pp. 536\u2013543.", "citeRegEx": "248", "shortCiteRegEx": null, "year": 2008}, {"title": "Multimodal deep autoencoder for human pose recovery", "author": ["C. Hong", "J. Yu", "J. Wan", "D. Tao", "M. Wang"], "venue": "IEEE Transactions on Image Processing, vol. 24, no. 12, pp. 5659\u20135670, 2015.", "citeRegEx": "249", "shortCiteRegEx": null, "year": 2015}, {"title": "Coupled auto-associative neural networks for heterogeneous face recognition", "author": ["B.S. Riggan", "C. Reale", "N.M. Nasrabadi"], "venue": "IEEE Access, vol. 3, pp. 1620\u20131632, 2015.", "citeRegEx": "250", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning text pair similarity with context-sensitive autoencoders", "author": ["H. Amiri", "P. Resnik", "J. Boyd-Graber", "H.D. III"], "venue": "2016.", "citeRegEx": "251", "shortCiteRegEx": null, "year": 2016}, {"title": "Sparse deep belief net model for visual area v2", "author": ["H. Lee", "C. Ekanadham", "A.Y. Ng"], "venue": "NIPS, 2008, pp. 873\u2013880.", "citeRegEx": "252", "shortCiteRegEx": null, "year": 2008}, {"title": "Effective multi-modal retrieval based on stacked autoencoders", "author": ["W. Wang", "B.C. Ooi", "X. Yang", "D. Zhang", "Y. Zhuang"], "venue": "Proceedings of the VLDB Endowment, vol. 7, no. 8, pp. 649\u2013660, 2014.", "citeRegEx": "253", "shortCiteRegEx": null, "year": 2014}, {"title": "Mdl-cw: A multimodal deep learning framework with cross weights", "author": ["S. Rastegar", "M. Soleymani", "H.R. Rabiee", "S. Mohsen Shojaee"], "venue": "CVPR, 2016, pp. 2601\u2013 2609.", "citeRegEx": "254", "shortCiteRegEx": null, "year": 2016}, {"title": "Devise: A deep visual-semantic embedding model", "author": ["A. Frome", "G.S. Corrado", "J. Shlens", "S. Bengio", "J. Dean", "T. Mikolov"], "venue": "NIPS, 2013, pp. 2121\u20132129.", "citeRegEx": "255", "shortCiteRegEx": null, "year": 2013}, {"title": "Word2visualvec: Crossmedia retrieval by visual feature prediction", "author": ["J. Dong", "X. Li", "C.G. Snoek"], "venue": "arXiv preprint arXiv:1604.06838, 2016.", "citeRegEx": "256", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal neural language models.", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "in ICML,", "citeRegEx": "257", "shortCiteRegEx": "257", "year": 2014}, {"title": "Combining language and vision with a multimodal skip-gram model", "author": ["A. Lazaridou", "N.T. Pham", "M. Baroni"], "venue": "arXiv preprint arXiv:1501.02598, 2015.", "citeRegEx": "258", "shortCiteRegEx": null, "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781, 2013.", "citeRegEx": "259", "shortCiteRegEx": null, "year": 2013}, {"title": "Zero-shot learning by convex combination of semantic embeddings", "author": ["M. Norouzi", "T. Mikolov", "S. Bengio", "Y. Singer", "J. Shlens", "A. Frome", "G.S. Corrado", "J. Dean"], "venue": "arXiv preprint arXiv:1312.5650, 2013.", "citeRegEx": "261", "shortCiteRegEx": null, "year": 2013}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "CVPR, 2015, pp. 1473\u20131482.", "citeRegEx": "262", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual information in semantic representation", "author": ["Y. Feng", "M. Lapata"], "venue": "HLT-NAACL, 2010, pp. 91\u201399.", "citeRegEx": "263", "shortCiteRegEx": null, "year": 2010}, {"title": "Multimodal distributional semantics.", "author": ["E. Bruni", "N.-K. Tran", "M. Baroni"], "venue": "J. Artif. Intell. Res.(JAIR),", "citeRegEx": "264", "shortCiteRegEx": "264", "year": 2014}, {"title": "Learning image embeddings using convolutional neural networks for improved multi-modal semantics.", "author": ["D. Kiela", "L. Bottou"], "venue": "in EMNLP,", "citeRegEx": "265", "shortCiteRegEx": "265", "year": 2014}, {"title": "Jointly modeling deep video and compositional text to bridge vision and language in a unified framework.", "author": ["R. Xu", "C. Xiong", "W. Chen", "J.J. Corso"], "venue": null, "citeRegEx": "266", "shortCiteRegEx": "266", "year": 2015}, {"title": "Learning cross space mapping via dnn using large scale click-through logs", "author": ["W. Yu", "K. Yang", "Y. Bai", "H. Yao", "Y. Rui"], "venue": "IEEE Transactions on Multimedia, vol. 17, no. 11, pp. 2000\u20132007, 2015.", "citeRegEx": "267", "shortCiteRegEx": null, "year": 2000}, {"title": "Deep compositional cross-modal learning to rank via local-global alignment", "author": ["X. Jiang", "F. Wu", "X. Li", "Z. Zhao", "W. Lu", "S. Tang", "Y. Zhuang"], "venue": "ACM Multimedia, 2015, pp. 69\u201378.", "citeRegEx": "268", "shortCiteRegEx": null, "year": 2015}, {"title": "Cross-modal retrieval with cnn visual features: A new baseline", "author": ["Y. Wei", "Y. Zhao", "C. Lu", "S. Wei", "L. Liu", "Z. Zhu", "S. Yan"], "venue": "2016.", "citeRegEx": "269", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning of multimodal representations with random walks on the click graph", "author": ["F. Wu", "X. Lu", "J. Song", "S. Yan", "Z.M. Zhang", "Y. Rui", "Y. Zhuang"], "venue": "IEEE Transactions on Image Processing, vol. 25, no. 2, pp. 630\u2013642, 2016.", "citeRegEx": "270", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning to hash with multiple representations", "author": ["Y. Kang", "S. Kim", "S. Choi"], "venue": "ICDM, 2012, pp. 930\u2013935.", "citeRegEx": "271", "shortCiteRegEx": null, "year": 2012}, {"title": "Cross-media hashing with neural networks", "author": ["Y. Zhuang", "Z. Yu", "W. Wang", "F. Wu", "S. Tang", "J. Shao"], "venue": "ACM Multimedia, 2014, pp. 901\u2013904.", "citeRegEx": "272", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep multimodal hashing with orthogonal regularization", "author": ["D. Wang", "P. Cui", "M. Ou", "W. Zhu"], "venue": "IJCAI, 2015.", "citeRegEx": "273", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep cross-modal hashing", "author": ["Q.-Y. Jiang", "W.-J. Li"], "venue": "arXiv preprint arXiv:1602.02255, 2016.", "citeRegEx": "274", "shortCiteRegEx": null, "year": 2016}, {"title": "Recurrent neural network based language model.", "author": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. Cernock\u1ef3", "S. Khudanpur"], "venue": "in Interspeech,", "citeRegEx": "275", "shortCiteRegEx": "275", "year": 2010}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput., vol. 9, no. 8, pp. 1735\u20131780, Nov. 1997.", "citeRegEx": "276", "shortCiteRegEx": null, "year": 1997}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": "EMNLP, 2014, pp. 1724\u20131734.", "citeRegEx": "277", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence learning with neural networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "venue": "NIPS, 2014, pp. 3104\u20133112.", "citeRegEx": "278", "shortCiteRegEx": null, "year": 2014}, {"title": "Learning longterm dependencies with gradient descent is difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi"], "venue": "IEEE  JOURNAL OF  LTEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015  27 Transactions on Neural Networks, vol. 5, no. 2, pp. 157\u2013 166, 1994.", "citeRegEx": "279", "shortCiteRegEx": null, "year": 2015}, {"title": "Unifying visual-semantic embeddings with multimodal neural language models", "author": ["R. Kiros", "R. Salakhutdinov", "R.S. Zemel"], "venue": "arXiv preprint arXiv:1411.2539, 2014.", "citeRegEx": "280", "shortCiteRegEx": null, "year": 2014}, {"title": "Translating videos to natural language using deep recurrent neural networks", "author": ["S. Venugopalan", "H. Xu", "J. Donahue", "M. Rohrbach", "R. Mooney", "K. Saenko"], "venue": "arXiv preprint arXiv:1412.4729, 2014.", "citeRegEx": "281", "shortCiteRegEx": null, "year": 2014}, {"title": "Sequence to sequence-video to text", "author": ["S. Venugopalan", "M. Rohrbach", "J. Donahue", "R. Mooney", "T. Darrell", "K. Saenko"], "venue": "ICCV, 2015, pp. 4534\u20134542.", "citeRegEx": "282", "shortCiteRegEx": null, "year": 2015}, {"title": "Vqa: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV, 2015, pp. 2425\u20132433.", "citeRegEx": "283", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning a recurrent visual representation for image caption generation", "author": ["X. Chen", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1411.5654, 2014.", "citeRegEx": "284", "shortCiteRegEx": null, "year": 2014}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR, vol. abs/1409.0473, 2014.", "citeRegEx": "285", "shortCiteRegEx": null, "year": 2014}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "CoRR, vol. abs/1412.7755, 2014.", "citeRegEx": "286", "shortCiteRegEx": null, "year": 2014}, {"title": "Recurrent models of visual attention", "author": ["V. Mnih", "N. Heess", "A. Graves", "K. Kavukcuoglu"], "venue": "CoRR, vol. abs/1406.6247, 2014.", "citeRegEx": "287", "shortCiteRegEx": null, "year": 2014}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": "CoRR, vol. abs/1502.03044, 2015.", "citeRegEx": "288", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel field alignment for cross media retrieval", "author": ["X. Mao", "B. Lin", "D. Cai", "X. He", "J. Pei"], "venue": "ACM Multimedia, 2013, pp. 897\u2013906.", "citeRegEx": "289", "shortCiteRegEx": null, "year": 2013}, {"title": "Ranking on data manifolds", "author": ["D. Zhou", "J. Weston", "A. Gretton", "O. Bousquet", "B. Sch\u00f6lkopf"], "venue": "NIPS, 2003, pp. 169\u2013176.", "citeRegEx": "290", "shortCiteRegEx": null, "year": 2003}, {"title": "Data fusion and multicue data matching by diffusion maps", "author": ["S. Lafon", "Y. Keller", "R.R. Coifman"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 11, pp. 1784\u20131797, 2006.", "citeRegEx": "291", "shortCiteRegEx": null, "year": 2006}, {"title": "Manifold alignment using procrustes analysis", "author": ["C. Wang", "S. Mahadevan"], "venue": "ICML, 2008, pp. 1120\u20131127.", "citeRegEx": "292", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 0, "context": "Canonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning.", "startOffset": 37, "endOffset": 40}, {"referenceID": 1, "context": "Canonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning.", "startOffset": 67, "endOffset": 72}, {"referenceID": 2, "context": "Canonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning.", "startOffset": 67, "endOffset": 72}, {"referenceID": 3, "context": "Canonical correlation analysis (CCA) [1] and its kernel extensions [2\u20134] are representative techniques in early studies of multi-view representation learning.", "startOffset": 67, "endOffset": 72}, {"referenceID": 6, "context": "Inspired by the success of deep neural networks [5\u2013 7], deep CCAs [8] have been proposed to this problem, with a common strategy to learn a joint representation that is coupled between multiple views at higher level after learning several layers of view-specific features in the lower layers.", "startOffset": 48, "endOffset": 54}, {"referenceID": 7, "context": "Inspired by the success of deep neural networks [5\u2013 7], deep CCAs [8] have been proposed to this problem, with a common strategy to learn a joint representation that is coupled between multiple views at higher level after learning several layers of view-specific features in the lower layers.", "startOffset": 66, "endOffset": 69}, {"referenceID": 8, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 132, "endOffset": 138}, {"referenceID": 9, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 132, "endOffset": 138}, {"referenceID": 10, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 132, "endOffset": 138}, {"referenceID": 11, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 165, "endOffset": 172}, {"referenceID": 12, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 165, "endOffset": 172}, {"referenceID": 13, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 165, "endOffset": 172}, {"referenceID": 14, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 217, "endOffset": 225}, {"referenceID": 15, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 217, "endOffset": 225}, {"referenceID": 16, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 295, "endOffset": 299}, {"referenceID": 17, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 325, "endOffset": 332}, {"referenceID": 18, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 325, "endOffset": 332}, {"referenceID": 19, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 325, "endOffset": 332}, {"referenceID": 20, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 376, "endOffset": 383}, {"referenceID": 21, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 376, "endOffset": 383}, {"referenceID": 22, "context": "For example, the advancement of multi-view representation learning ranges from shallow methods including multi-modal topic learning [9\u201311], multi-view sparse coding [12\u201314], and multiview latent space Markov networks [15, 16], to deep methods including multi-modal restricted Boltzmann machines [17], multimodal autoencoders [18\u201320], and multi-modal recurrent neural networks [21\u201323].", "startOffset": 376, "endOffset": 383}, {"referenceID": 23, "context": "Further, manifold alignment also provides an important perspective for multi-view representation learning [24].", "startOffset": 106, "endOffset": 110}, {"referenceID": 24, "context": "An illustrative example application of CCA in cross-modal retrieval (adapted from [25]).", "startOffset": 82, "endOffset": 86}, {"referenceID": 0, "context": "Canonical Correlation Analysis [1] has become increasingly popular for its capability of effectively modeling the relationship between two or more sets of variables.", "startOffset": 31, "endOffset": 34}, {"referenceID": 26, "context": "More specifically, CCA has been widely used in multi-view learning tasks to generate low dimensional feature representations [25\u2013 27].", "startOffset": 125, "endOffset": 133}, {"referenceID": 27, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 99, "endOffset": 106}, {"referenceID": 28, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 99, "endOffset": 106}, {"referenceID": 29, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 99, "endOffset": 106}, {"referenceID": 32, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 119, "endOffset": 127}, {"referenceID": 33, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 140, "endOffset": 148}, {"referenceID": 34, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 140, "endOffset": 148}, {"referenceID": 35, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 166, "endOffset": 173}, {"referenceID": 36, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 166, "endOffset": 173}, {"referenceID": 37, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 166, "endOffset": 173}, {"referenceID": 38, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 197, "endOffset": 204}, {"referenceID": 39, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 197, "endOffset": 204}, {"referenceID": 40, "context": "Improved generalization performance has been witnessed in areas including dimensionality reduction [28\u201330], clustering [31\u2013 33], regression [34, 35], word embeddings [36\u201338], discriminant learning [39\u201341], etc.", "startOffset": 197, "endOffset": 204}, {"referenceID": 2, "context": "(3) is equivalent to solving a pair of generalized eigenvalue problems [3],", "startOffset": 71, "endOffset": 74}, {"referenceID": 41, "context": "Besides the above definition of CCA, there are also other different ways to define the canonical correlations of a pair of matrices, and all these ways are shown to be equivalent [42].", "startOffset": 179, "endOffset": 183}, {"referenceID": 42, "context": "In particular, Kettenring [43] show that CCA is equivalent to a constrained least-square optimization problem.", "startOffset": 26, "endOffset": 30}, {"referenceID": 41, "context": "Further, Golub and Zha [42] also provide a classical algorithm for computing CCA by first QR decomposition of the data matrices which whitens the data and then an SVD of the whitened covariance matrix.", "startOffset": 23, "endOffset": 27}, {"referenceID": 29, "context": "[30, 44] propose a fast algorithm for CCA with a pair of tall-and-thin matrices using subsampled randomized Walsh-Hadamard transform [45], which only subsamples a small proportion of the training data points to approximate the matrix product.", "startOffset": 0, "endOffset": 8}, {"referenceID": 43, "context": "[30, 44] propose a fast algorithm for CCA with a pair of tall-and-thin matrices using subsampled randomized Walsh-Hadamard transform [45], which only subsamples a small proportion of the training data points to approximate the matrix product.", "startOffset": 0, "endOffset": 8}, {"referenceID": 44, "context": "[30, 44] propose a fast algorithm for CCA with a pair of tall-and-thin matrices using subsampled randomized Walsh-Hadamard transform [45], which only subsamples a small proportion of the training data points to approximate the matrix product.", "startOffset": 133, "endOffset": 137}, {"referenceID": 45, "context": "Further, Lu and Foster [46] consider sparse design matrices and introduce an efficient iterative regression algorithm for large scale CCA.", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": "Another line of research for large scale CCA considers stochastic optimization algorithm for CCA [47].", "startOffset": 97, "endOffset": 101}, {"referenceID": 47, "context": "[48] introduce an augmented approximate gradient scheme and further extend it to a stochastic optimization regime.", "startOffset": 0, "endOffset": 4}, {"referenceID": 48, "context": "Recent work [49, 50] attempts to transform the original problem of CCA into sequences of least squares problems and solve these problems with accelerated gradient descent (AGD).", "startOffset": 12, "endOffset": 20}, {"referenceID": 49, "context": "Recent work [49, 50] attempts to transform the original problem of CCA into sequences of least squares problems and solve these problems with accelerated gradient descent (AGD).", "startOffset": 12, "endOffset": 20}, {"referenceID": 30, "context": "[31] propose to use a mixture of local linear canonical correlation models to find correlation clusters from the data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] formulate CCA as a least squares problem in multi-label classification.", "startOffset": 0, "endOffset": 4}, {"referenceID": 50, "context": "This formulation of CCA as a probabilistic model is proposed by Bach and Jordan [51].", "startOffset": 80, "endOffset": 84}, {"referenceID": 50, "context": "Consequently, Bach and Jordan [51] provide a detailed connection between CCA and probabilistic CCA based on the result of the maximum likelihood estimation of Eq.", "startOffset": 30, "endOffset": 34}, {"referenceID": 51, "context": "In addition, Browne [52] also proves that the maximum likelihood solution of Eq.", "startOffset": 20, "endOffset": 24}, {"referenceID": 52, "context": "[53] introduce robust canonical correlation analysis by replacing Gaussian distributions with Student-t distributions, which constructs mixtures of robust CCA and can deal with missing data quite easily.", "startOffset": 0, "endOffset": 4}, {"referenceID": 53, "context": "Fyfe and Leen [54] consider a Dirichlet process method for performing a non-Bayesian probabilistic mixture CCA.", "startOffset": 14, "endOffset": 18}, {"referenceID": 54, "context": "By treating the mixture case in length, Klami and Kaski [55] present a full Bayesian treatment of probabilistic canonical analyzer.", "startOffset": 56, "endOffset": 60}, {"referenceID": 55, "context": "Further, Wang [56] applies a hierarchical Bayesian model to probabilistic CCA and learn this model by variational approximation.", "startOffset": 14, "endOffset": 18}, {"referenceID": 54, "context": "Both [55] and [56] exploit the inverse Wishart matrix distribution as a prior for the covariance matrices \u03a8x(y) in Eq.", "startOffset": 5, "endOffset": 9}, {"referenceID": 55, "context": "Both [55] and [56] exploit the inverse Wishart matrix distribution as a prior for the covariance matrices \u03a8x(y) in Eq.", "startOffset": 14, "endOffset": 18}, {"referenceID": 56, "context": "[57] introduce a mixture of robust canonical correlation analyzers and provide a variational Bayesian inference for learning from noisy data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 57, "context": "Based on exponential family extensions of principal component analysis [58, 59], Klami et al.", "startOffset": 71, "endOffset": 79}, {"referenceID": 58, "context": "Based on exponential family extensions of principal component analysis [58, 59], Klami et al.", "startOffset": 71, "endOffset": 79}, {"referenceID": 59, "context": "[60] extend Bayesian CCA to the exponential family by generalizing the Gaussian noise assumption to noise with any distribution in the exponential family.", "startOffset": 0, "endOffset": 4}, {"referenceID": 60, "context": "Using the natural parameter formulation of exponential family distributions, certain common choices can be incorporated as special cases, which leading to an efficient way of exploiting the robustness of exponential family distribution in practical models [61, 62].", "startOffset": 256, "endOffset": 264}, {"referenceID": 61, "context": "Using the natural parameter formulation of exponential family distributions, certain common choices can be incorporated as special cases, which leading to an efficient way of exploiting the robustness of exponential family distribution in practical models [61, 62].", "startOffset": 256, "endOffset": 264}, {"referenceID": 62, "context": "Recently, Mukuta and Harada [63] follow Wang\u2019s approach [56] and introduce a Bayesian extension of partial CCA [64].", "startOffset": 28, "endOffset": 32}, {"referenceID": 55, "context": "Recently, Mukuta and Harada [63] follow Wang\u2019s approach [56] and introduce a Bayesian extension of partial CCA [64].", "startOffset": 56, "endOffset": 60}, {"referenceID": 63, "context": "Recently, Mukuta and Harada [63] follow Wang\u2019s approach [56] and introduce a Bayesian extension of partial CCA [64].", "startOffset": 111, "endOffset": 115}, {"referenceID": 64, "context": "[65] propose a probabilistic semi-CCA by incorporating the mechanism of data missing.", "startOffset": 0, "endOffset": 4}, {"referenceID": 54, "context": "Most of the practical applications of Bayesian CCA in the earlier efforts focus on relatively low-dimensional data [55, 56].", "startOffset": 115, "endOffset": 123}, {"referenceID": 55, "context": "Most of the practical applications of Bayesian CCA in the earlier efforts focus on relatively low-dimensional data [55, 56].", "startOffset": 115, "endOffset": 123}, {"referenceID": 65, "context": "[66, 67] propose to use multi-way analysis setup by exploiting dimensionality reduction techniques.", "startOffset": 0, "endOffset": 8}, {"referenceID": 66, "context": "[66, 67] propose to use multi-way analysis setup by exploiting dimensionality reduction techniques.", "startOffset": 0, "endOffset": 8}, {"referenceID": 67, "context": "Most of the approaches to sparse CCA are based on the well known LASSO trick [68] which is a shrinkage and selection method for linear regression.", "startOffset": 77, "endOffset": 81}, {"referenceID": 68, "context": "By formulating CCA as two constrained simultaneous regression problems, Hardoon and ShaweTaylor [69] propose to approximate the non-convex constraints with \u221e-norm.", "startOffset": 96, "endOffset": 100}, {"referenceID": 69, "context": "[70] propose to use the elastic net type regression.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[27] introduce a sparse CCA by formulating CCA as a least squares problem in multi-label classification and directly computing it with the Least Angle Regression algorithm (LARS) [71].", "startOffset": 0, "endOffset": 4}, {"referenceID": 70, "context": "[27] introduce a sparse CCA by formulating CCA as a least squares problem in multi-label classification and directly computing it with the Least Angle Regression algorithm (LARS) [71].", "startOffset": 179, "endOffset": 183}, {"referenceID": 71, "context": "For example, graph laplacian [72] can be used in this framework to tackle with the unlabeled data.", "startOffset": 29, "endOffset": 33}, {"referenceID": 72, "context": "In fact, the development of sparse CCA is intimately related to the advance of sparse PCA [73\u201375].", "startOffset": 90, "endOffset": 97}, {"referenceID": 73, "context": "In fact, the development of sparse CCA is intimately related to the advance of sparse PCA [73\u201375].", "startOffset": 90, "endOffset": 97}, {"referenceID": 74, "context": "In fact, the development of sparse CCA is intimately related to the advance of sparse PCA [73\u201375].", "startOffset": 90, "endOffset": 97}, {"referenceID": 75, "context": "The classical solutions to generalized eigenvalue problem with sparse PCA can be easily extended to that of sparse CCA [76, 77].", "startOffset": 119, "endOffset": 127}, {"referenceID": 76, "context": "The classical solutions to generalized eigenvalue problem with sparse PCA can be easily extended to that of sparse CCA [76, 77].", "startOffset": 119, "endOffset": 127}, {"referenceID": 75, "context": "[76] derive a sparse CCA algorithm by extending an approach for solving sparse eigenvalue problems using D.", "startOffset": 0, "endOffset": 4}, {"referenceID": 73, "context": "Based on the sparse PCA algorithm in [74], Wiesel et al.", "startOffset": 37, "endOffset": 41}, {"referenceID": 76, "context": "[77] propose a backward greedy approach to sparse CCA by bounding the correlation at each stage.", "startOffset": 0, "endOffset": 4}, {"referenceID": 77, "context": "[78] propose to apply a penalized matrix decomposition to the covariance matrix Cxy , which results in a method for penalized sparse CCA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 78, "context": "Consequently, structured sparse CCA has been proposed by extending the penalized CCA with structured sparsity inducing penalty [79].", "startOffset": 127, "endOffset": 131}, {"referenceID": 79, "context": "Another line of research for sparse CCA is based on sparse Bayesian learning [80\u201382].", "startOffset": 77, "endOffset": 84}, {"referenceID": 80, "context": "Another line of research for sparse CCA is based on sparse Bayesian learning [80\u201382].", "startOffset": 77, "endOffset": 84}, {"referenceID": 81, "context": "Another line of research for sparse CCA is based on sparse Bayesian learning [80\u201382].", "startOffset": 77, "endOffset": 84}, {"referenceID": 50, "context": "In particular, they are built on the probabilistic interpretation of CCA outlined by [51].", "startOffset": 85, "endOffset": 89}, {"referenceID": 79, "context": "Fyfe and Leen [80] investigate two methods for sparsifying probabilistic CCA.", "startOffset": 14, "endOffset": 18}, {"referenceID": 82, "context": "[83] present a variant of sparse Bayesian CCA, using an element-wise automatic relevance determination (ARD) prior configuration, where non-effective parameters are automatically driven to zero.", "startOffset": 0, "endOffset": 4}, {"referenceID": 81, "context": "Rai and Daum\u00e9 III [82] propose a nonparametric, fully Bayesian framework that can automatically select the number of correlation components and capture the sparsity underlying the projections.", "startOffset": 18, "endOffset": 22}, {"referenceID": 83, "context": "This framework exploits the Indian Buffet Process [84] to discover latent feature representation of a set of observations and to control the overall complexity of the model.", "startOffset": 50, "endOffset": 54}, {"referenceID": 51, "context": "Recently, some authors have used probabilistic inter-battery factor analysis (IBFA) [52, 85], which can be considered as an extended CCA model which complements CCA by providing a certain variation not captured by the correlation components, to build more complex hierarchical sparse Bayesian CCA models.", "startOffset": 84, "endOffset": 92}, {"referenceID": 84, "context": "Recently, some authors have used probabilistic inter-battery factor analysis (IBFA) [52, 85], which can be considered as an extended CCA model which complements CCA by providing a certain variation not captured by the correlation components, to build more complex hierarchical sparse Bayesian CCA models.", "startOffset": 84, "endOffset": 92}, {"referenceID": 85, "context": "[86] introduce a Bayesian treatment of the IBFA model, which describes not only the correlations between data sets but also provides components explaining the linear structure within each of the data sets.", "startOffset": 0, "endOffset": 4}, {"referenceID": 86, "context": "Built on multi-battery factor analysis (MBFA) by McDonald [87] and Browne [88] as a generalization of IBFA, Klami et al.", "startOffset": 58, "endOffset": 62}, {"referenceID": 87, "context": "Built on multi-battery factor analysis (MBFA) by McDonald [87] and Browne [88] as a generalization of IBFA, Klami et al.", "startOffset": 74, "endOffset": 78}, {"referenceID": 88, "context": "[89] introduce a problem formulation of group factor analysis, which extends CCA to more than two sets with structural sparsity, resulting in that it is more flexible than the previous extensions [86].", "startOffset": 0, "endOffset": 4}, {"referenceID": 85, "context": "[89] introduce a problem formulation of group factor analysis, which extends CCA to more than two sets with structural sparsity, resulting in that it is more flexible than the previous extensions [86].", "startOffset": 196, "endOffset": 200}, {"referenceID": 89, "context": "Canonical Correlation Analysis is a linear multi-view representation learning algorithm, but for many scenarios of real-world multi-view data revealing nonlinearities, it is impossible for a linear embedding to capture all the properties of the multi-view data [90].", "startOffset": 261, "endOffset": 265}, {"referenceID": 90, "context": "Since kernerlization is a principled trick for introducing non-linearity into linear methods, kernel CCA [91\u201393] provides an alternative solution.", "startOffset": 105, "endOffset": 112}, {"referenceID": 91, "context": "Since kernerlization is a principled trick for introducing non-linearity into linear methods, kernel CCA [91\u201393] provides an alternative solution.", "startOffset": 105, "endOffset": 112}, {"referenceID": 92, "context": "Since kernerlization is a principled trick for introducing non-linearity into linear methods, kernel CCA [91\u201393] provides an alternative solution.", "startOffset": 105, "endOffset": 112}, {"referenceID": 1, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 136, "endOffset": 139}, {"referenceID": 2, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 175, "endOffset": 186}, {"referenceID": 93, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 175, "endOffset": 186}, {"referenceID": 94, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 175, "endOffset": 186}, {"referenceID": 2, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 210, "endOffset": 221}, {"referenceID": 95, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 210, "endOffset": 221}, {"referenceID": 96, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 210, "endOffset": 221}, {"referenceID": 31, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 244, "endOffset": 252}, {"referenceID": 97, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 244, "endOffset": 252}, {"referenceID": 98, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 280, "endOffset": 289}, {"referenceID": 99, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 280, "endOffset": 289}, {"referenceID": 1, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 306, "endOffset": 319}, {"referenceID": 100, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 306, "endOffset": 319}, {"referenceID": 101, "context": "As a non-linear extension of CCA, kernel CCA has been successfully applied in many situations, including independent component analysis [2], cross-media information retrieval [3, 94, 95], computational biology [3, 96, 97], multiview clustering [32, 98], acoustic feature learning [99, 100], and statistics [2, 101, 102].", "startOffset": 306, "endOffset": 319}, {"referenceID": 102, "context": "By adapting the representer theorem [103] to the case of multiview data to state that the following minimization problem,", "startOffset": 36, "endOffset": 41}, {"referenceID": 2, "context": "As discussed in [3], the above optimization leads to degenerate solutions when either Kx or Ky is invertible.", "startOffset": 16, "endOffset": 19}, {"referenceID": 2, "context": "(13) is also equivalent to solving a pair of generalized eigenproblems [3],", "startOffset": 71, "endOffset": 74}, {"referenceID": 1, "context": "Consequently, the statistical properties of KCCA have been investigated from several aspects [2, 104].", "startOffset": 93, "endOffset": 101}, {"referenceID": 103, "context": "Consequently, the statistical properties of KCCA have been investigated from several aspects [2, 104].", "startOffset": 93, "endOffset": 101}, {"referenceID": 104, "context": "[105] introduce a mathematical proof of the statistical convergence of kernel CCA by providing rates for the regularization parameters.", "startOffset": 0, "endOffset": 5}, {"referenceID": 105, "context": "Later Hardoon and Shawe-Taylor [106] provide a detailed theoretical analysis of KCCA and propose a finite sample statistical analysis of KCCA by using a regression formulation.", "startOffset": 31, "endOffset": 36}, {"referenceID": 106, "context": "Cai and Sun [107] also provide a convergence rate analysis of kernel CCA under an approximation assumption.", "startOffset": 12, "endOffset": 17}, {"referenceID": 1, "context": "Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108].", "startOffset": 163, "endOffset": 166}, {"referenceID": 2, "context": "Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108].", "startOffset": 207, "endOffset": 210}, {"referenceID": 98, "context": "Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108].", "startOffset": 238, "endOffset": 247}, {"referenceID": 107, "context": "Thus, various approximation methods have been proposed by constructing low-rank approximations of the kernel matrices, including incomplete Cholesky decomposition [2], partial Gram-Schmidt orthogonolisation [3], and block incremental SVD [99, 108].", "startOffset": 238, "endOffset": 247}, {"referenceID": 108, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 32, "endOffset": 37}, {"referenceID": 109, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 85, "endOffset": 94}, {"referenceID": 110, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 85, "endOffset": 94}, {"referenceID": 111, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 85, "endOffset": 94}, {"referenceID": 112, "context": "In addition, the Nystr\u00f6m method [109] is widely used to speed up the kernel machines [110\u2013113].", "startOffset": 85, "endOffset": 94}, {"referenceID": 113, "context": "In early applications, KCCA has shown its ability as a feature preprocessing step that improves subsequent analysis in, for example, classification using a support vector machine [114].", "startOffset": 179, "endOffset": 184}, {"referenceID": 114, "context": "[115] investigate the possibility of combining the two distinct stages of KCCA and SVM into a single optimization named as SVM-2K, which has been widely used in classification tasks where two views of the same phenomenon are available.", "startOffset": 0, "endOffset": 5}, {"referenceID": 2, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 93, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 94, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 115, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 116, "context": "With the development of image understanding, kernel CCA has already been successfully used to associate images or image regions with different kinds of captions, including individual words, sets of tag, and even sentences [3, 94, 95, 116, 117].", "startOffset": 222, "endOffset": 243}, {"referenceID": 2, "context": "[3] first apply KCCA to cross-modality retrieval task, in which images are retrieved by a given multiple text query and without using any label information around the retrieved images.", "startOffset": 0, "endOffset": 3}, {"referenceID": 93, "context": "Consequently, KCCA is exploited by Socher and Li [94] to learn a mapping between textual words and visual words so that both modalities are connected by a shared, low dimensional feature space.", "startOffset": 49, "endOffset": 53}, {"referenceID": 116, "context": "[117] make use of KCCA in a stringent task of associating images with natural language sentences that describe what is depicted.", "startOffset": 0, "endOffset": 5}, {"referenceID": 31, "context": "Moreover, kernel CCA also has attracted much attention in multi-view clustering [32, 98].", "startOffset": 80, "endOffset": 88}, {"referenceID": 97, "context": "Moreover, kernel CCA also has attracted much attention in multi-view clustering [32, 98].", "startOffset": 80, "endOffset": 88}, {"referenceID": 31, "context": "Blaschko and Lampert [32] propose a correlational spectral clustering method, which exploits KCCA to do unsupervised clustering of images and text in latent meaning space.", "startOffset": 21, "endOffset": 25}, {"referenceID": 97, "context": "[98] use a regularized variant of the KCCA algorithm to learn a lower dimensional subspace from heterogeneous data sources.", "startOffset": 0, "endOffset": 4}, {"referenceID": 96, "context": "[97] propose to modify the objective of KCCA with semi-supervised Laplacian regularization [118] to favor directions that lie along the data manifold [118].", "startOffset": 0, "endOffset": 4}, {"referenceID": 117, "context": "[97] propose to modify the objective of KCCA with semi-supervised Laplacian regularization [118] to favor directions that lie along the data manifold [118].", "startOffset": 91, "endOffset": 96}, {"referenceID": 117, "context": "[97] propose to modify the objective of KCCA with semi-supervised Laplacian regularization [118] to favor directions that lie along the data manifold [118].", "startOffset": 150, "endOffset": 155}, {"referenceID": 118, "context": "In the early work, by assuming that different parts of perceptual input have common causes in the external world, Becker and Hinton [119] present a multilayer nonlinear extension of canonical correlation by maximizing the normalized covariation between the outputs from two neural network modules.", "startOffset": 132, "endOffset": 137}, {"referenceID": 119, "context": "Further, Becker [120] explores the idea of maximizing the mutual information between the outputs of different network modules to extract higher order features from coherence inputs.", "startOffset": 16, "endOffset": 21}, {"referenceID": 120, "context": "Later Lai and Fyfe [121, 122] investigate a neural network implementation of CCA and maximize the correlation (rather than canonical correlation) between the outputs of the networks for each view.", "startOffset": 19, "endOffset": 29}, {"referenceID": 121, "context": "Later Lai and Fyfe [121, 122] investigate a neural network implementation of CCA and maximize the correlation (rather than canonical correlation) between the outputs of the networks for each view.", "startOffset": 19, "endOffset": 29}, {"referenceID": 122, "context": "Hsieh [123] formulates a nonlinear canonical correlation analysis (NLCCA) method using three feedforward neural networks.", "startOffset": 6, "endOffset": 11}, {"referenceID": 7, "context": "The framework of deep CCA (adapted from [8]), in which the output layers of two deep networks are maximally correlated.", "startOffset": 40, "endOffset": 43}, {"referenceID": 7, "context": "[8].", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "Inspired by the recent success of deep neural networks [6, 124], Andrew et al.", "startOffset": 55, "endOffset": 63}, {"referenceID": 123, "context": "Inspired by the recent success of deep neural networks [6, 124], Andrew et al.", "startOffset": 55, "endOffset": 63}, {"referenceID": 7, "context": "[8] introduce deep CCA to learn deep nonlinear mappings between two views {X,Y } which are maximally correlated.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Thus, the parameters (\u03b8\u2217 x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].", "startOffset": 176, "endOffset": 179}, {"referenceID": 124, "context": "Thus, the parameters (\u03b8\u2217 x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].", "startOffset": 225, "endOffset": 234}, {"referenceID": 125, "context": "Thus, the parameters (\u03b8\u2217 x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].", "startOffset": 225, "endOffset": 234}, {"referenceID": 126, "context": "Thus, the parameters (\u03b8\u2217 x, \u03b8 \u2217 y) are also estimated on the training data by following the gradient of the correlation objective, with batchbased algorithms like L-BFGS as in [8] or stochastic optimization with mini-batches [125\u2013127].", "startOffset": 225, "endOffset": 234}, {"referenceID": 127, "context": "Yan and Mikolajczyk [128] learn a joint latent space for matching images and captions with a deep CCA framework, which adopts a GPU implementation and could deal with overfitting.", "startOffset": 20, "endOffset": 25}, {"referenceID": 125, "context": "[126] learn deep non-linear embeddings of two languages using the deep CCA.", "startOffset": 0, "endOffset": 5}, {"referenceID": 19, "context": "Recently, a deep canonically correlated autoencoder (DCCAE) [20] is proposed by combining the advantages of the deep CCA and autoencoder-based approaches.", "startOffset": 60, "endOffset": 64}, {"referenceID": 128, "context": "Probabilistic latent semantic analysis (PLSA) [129] is a statistical variant of Latent Semantic Analysis (LSA) [130] and a significant step toward probabilistic modeling of text.", "startOffset": 46, "endOffset": 51}, {"referenceID": 129, "context": "Probabilistic latent semantic analysis (PLSA) [129] is a statistical variant of Latent Semantic Analysis (LSA) [130] and a significant step toward probabilistic modeling of text.", "startOffset": 111, "endOffset": 116}, {"referenceID": 8, "context": "In the original LinkPLSA model [9], both term-based PLSA [129] and citationbased PHITS [131] are merged into a joint probabilistic model, explaining terms and citations in terms of a common set of underlying factors.", "startOffset": 31, "endOffset": 34}, {"referenceID": 128, "context": "In the original LinkPLSA model [9], both term-based PLSA [129] and citationbased PHITS [131] are merged into a joint probabilistic model, explaining terms and citations in terms of a common set of underlying factors.", "startOffset": 57, "endOffset": 62}, {"referenceID": 130, "context": "In the original LinkPLSA model [9], both term-based PLSA [129] and citationbased PHITS [131] are merged into a joint probabilistic model, explaining terms and citations in terms of a common set of underlying factors.", "startOffset": 87, "endOffset": 92}, {"referenceID": 131, "context": "[132] present a multi-view PLSA-based approach called PLSA-words.", "startOffset": 0, "endOffset": 5}, {"referenceID": 132, "context": "[133] propose a PLSA based probabilistic multi-modal semantic model, in which the visual features and the textual words are fully connected via a hidden layer to allow much more mutual dependence between each other than PLSA-words.", "startOffset": 0, "endOffset": 5}, {"referenceID": 133, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 134, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 135, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 136, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 137, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 138, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 139, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 140, "context": "Considering that multi-view PLSA methods have good performance on image annotations, extensions are designed to conduct image annotation or clustering tasks [134\u2013141].", "startOffset": 157, "endOffset": 166}, {"referenceID": 135, "context": "[136] propose a multilayer multimodal PLSA model, which can handle different modalities as well as different features within a mode.", "startOffset": 0, "endOffset": 5}, {"referenceID": 141, "context": "Latent Dirichlet allocation (LDA) [142] is a generative probabilistic model for collections of a corpus.", "startOffset": 34, "endOffset": 39}, {"referenceID": 9, "context": "[10] present a mixture of multi-modal LDA model (MoM-LDA), which describes the following generative process for the multi-modal data: each document (consisting of visual and textual information) has a distribution for a fixed number of topics (mixture components), and given a specific topic the visual features and textual words are generated.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Consequently, Blei and Jordan [11] propose a correspondence LDA (Corr-LDA) model, which not only allows simultaneous dimensionality reduction in the representation of region descriptions and words, but also models the conditional correspondence between their respectively reduced representations.", "startOffset": 30, "endOffset": 34}, {"referenceID": 10, "context": "The graphical model representation of the Corr-LDA model (adapted from [11]).", "startOffset": 71, "endOffset": 75}, {"referenceID": 142, "context": "Based on hierarchical Dirichlet process (HDP) [143], Yakhnenko and Honavar [144] introduce a multi-modal hierarchical Dirichlet process model (MoM-HDP), which is an extension of MoM-LDA with an infinite number of mixture components.", "startOffset": 46, "endOffset": 51}, {"referenceID": 143, "context": "Based on hierarchical Dirichlet process (HDP) [143], Yakhnenko and Honavar [144] introduce a multi-modal hierarchical Dirichlet process model (MoM-HDP), which is an extension of MoM-LDA with an infinite number of mixture components.", "startOffset": 75, "endOffset": 80}, {"referenceID": 144, "context": "Note that in practice, the Dirichlet process is approximated by truncating it [145].", "startOffset": 78, "endOffset": 83}, {"referenceID": 145, "context": "[146] also present a correspondence hierarchical Dirichlet process model (Corr-HDP), which is an extension of the Corr-LDA model using a hierarchical Dirichlet process instead.", "startOffset": 0, "endOffset": 5}, {"referenceID": 146, "context": "Following the supervised LDA algorithms [147], supervised multi-modal LDA models are subsequently proposed to make effective use of the discriminative information.", "startOffset": 40, "endOffset": 45}, {"referenceID": 146, "context": "Since supervised topic models can be classified into two categories: downstream models and upstream models [147], supervised multi-modal LDA models can also be catergorized accordingly.", "startOffset": 107, "endOffset": 112}, {"referenceID": 147, "context": "[148] develop a multi-modal probabilistic model for jointly modeling the image, its class label, and its annotations, called multi-class supervised LDA with annotations, which treats the class label as a global description of the image, and treats annotation terms as local descriptions of parts of the image.", "startOffset": 0, "endOffset": 5}, {"referenceID": 148, "context": "Its underlying assumptions naturally integrate the multi-modal data with their discriminative information so that it takes advantage of the merits of Corr-LDA and supervised LDA [149] simultaneously.", "startOffset": 178, "endOffset": 183}, {"referenceID": 149, "context": "For an upstream supervised multi-modal model, the response variables directly or indirectly generate latent topic variables [150\u2013152].", "startOffset": 124, "endOffset": 133}, {"referenceID": 150, "context": "For an upstream supervised multi-modal model, the response variables directly or indirectly generate latent topic variables [150\u2013152].", "startOffset": 124, "endOffset": 133}, {"referenceID": 151, "context": "For an upstream supervised multi-modal model, the response variables directly or indirectly generate latent topic variables [150\u2013152].", "startOffset": 124, "endOffset": 133}, {"referenceID": 150, "context": "[151] propose a spatially coherent latent topic model (Spatial-LTM), which represents an image containing objects in two different modalities: appearance features and salient image patches.", "startOffset": 0, "endOffset": 5}, {"referenceID": 152, "context": "[153] propose a Multi-modal Multi-instance Multi-Label LDA model (M3LDA), in which the model consists of a visual-label part, a textual-label part, and a label-topic part.", "startOffset": 0, "endOffset": 5}, {"referenceID": 153, "context": "Gaussian Processes (GPs) [154, 155] are generalizations of Gaussian distributions defined over infinite index sets.", "startOffset": 25, "endOffset": 35}, {"referenceID": 154, "context": "Gaussian Processes (GPs) [154, 155] are generalizations of Gaussian distributions defined over infinite index sets.", "startOffset": 25, "endOffset": 35}, {"referenceID": 155, "context": "Lawrence [156] proposes the Gaussian Process Latent Variable Model (GPLVM) as a non-linear dimensionality reduction technique.", "startOffset": 9, "endOffset": 14}, {"referenceID": 156, "context": "[157] present the shared GPLVM (SGPLVM) model as a generalization of GPLVM that can handle multiple observation spaces, where each set of observations is parameterized by a different set of kernel parameters.", "startOffset": 0, "endOffset": 5}, {"referenceID": 157, "context": "Following the idea of discriminative Gaussian process latent variable model (DGPLVM) [158], Eleftheriadis et al.", "startOffset": 85, "endOffset": 90}, {"referenceID": 158, "context": "[159] propose a discriminative shared Gaussian process latent variable model (DS-GPLVM) for multi-view analysis by exploiting a shared discriminative manifold by multi-view data.", "startOffset": 0, "endOffset": 5}, {"referenceID": 159, "context": "[160] propose a large margin multi-view Gaussian process via an integration of Gaussian process and large-margin principle.", "startOffset": 0, "endOffset": 5}, {"referenceID": 160, "context": "[161] propose a multi-modal regularized similarity Gaussian Process variable model to enforce that the semantically similar/dissimilar crossmodal observations are also similar/dissimilar in the latent space, which maximizes the cross-modal semantic consistency.", "startOffset": 0, "endOffset": 5}, {"referenceID": 161, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 162, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 163, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 164, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 165, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 166, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 167, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 168, "context": "Collective Matrix factorization (CMF) have become an important tool for boosting the overall factorization quality in the case of multiple view related matrices [162\u2013169].", "startOffset": 161, "endOffset": 170}, {"referenceID": 169, "context": "Bias for different factors and loadings can also be incorporated [170].", "startOffset": 65, "endOffset": 70}, {"referenceID": 170, "context": "This is the probabilistic interpretation of CMF that other complex models build on [171].", "startOffset": 83, "endOffset": 88}, {"referenceID": 162, "context": "[163] propose a multi-label informed latent semantic indexing algorithm which exploits the idea of joint matrix factorization to learn a shared latent semantic factor.", "startOffset": 0, "endOffset": 5}, {"referenceID": 171, "context": "[172] introduce a joint matrix factorization model to combine the information of content and links for web page analysis.", "startOffset": 0, "endOffset": 5}, {"referenceID": 161, "context": "Later Singh and Gordon [162] propose collective matrix factorization in which several matrices are factorized simultaneously, sharing parameters among factors.", "startOffset": 23, "endOffset": 28}, {"referenceID": 172, "context": "Further, a hierarchical Bayesian variant [173] is proposed to provide an estimate of uncertainty during the relational modeling process.", "startOffset": 41, "endOffset": 46}, {"referenceID": 173, "context": "Further, CMF has been applied for multi-view hashing [174, 175].", "startOffset": 53, "endOffset": 63}, {"referenceID": 174, "context": "Further, CMF has been applied for multi-view hashing [174, 175].", "startOffset": 53, "endOffset": 63}, {"referenceID": 173, "context": "A multimodal latent binary embedding (MLBE) method is proposed by Zhen and Yang [174] to learn hash functions from a probabilistic generative perspective.", "startOffset": 80, "endOffset": 85}, {"referenceID": 174, "context": "[175] put forward collective matrix factorization hashing to learn unified hash codes by CMF with a latent factor model from different modalities of one instance.", "startOffset": 0, "endOffset": 5}, {"referenceID": 175, "context": "In addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179].", "startOffset": 105, "endOffset": 114}, {"referenceID": 176, "context": "In addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179].", "startOffset": 105, "endOffset": 114}, {"referenceID": 177, "context": "In addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179].", "startOffset": 105, "endOffset": 114}, {"referenceID": 178, "context": "In addition, joint nonnegative matrix factorization (NMF) also has been proposed for multi-view analysis [176\u2013179].", "startOffset": 105, "endOffset": 114}, {"referenceID": 176, "context": "[177] introduce a joint NMF algorithm to learn a factorization that gives compatible clustering solutions across multiple views.", "startOffset": 0, "endOffset": 5}, {"referenceID": 11, "context": "Multi-view sparse coding [12\u201314] relates a latent representation (either a vector of random variables or a feature vector, depending on the interpretation) to the multi-view data through a set of linear mappings, which we refer to as the dictionaries.", "startOffset": 25, "endOffset": 32}, {"referenceID": 12, "context": "Multi-view sparse coding [12\u201314] relates a latent representation (either a vector of random variables or a feature vector, depending on the interpretation) to the multi-view data through a set of linear mappings, which we refer to as the dictionaries.", "startOffset": 25, "endOffset": 32}, {"referenceID": 13, "context": "Multi-view sparse coding [12\u201314] relates a latent representation (either a vector of random variables or a feature vector, depending on the interpretation) to the multi-view data through a set of linear mappings, which we refer to as the dictionaries.", "startOffset": 25, "endOffset": 32}, {"referenceID": 179, "context": "This property is owing to the explaining away effect which aries naturally in directed graphical models [180].", "startOffset": 104, "endOffset": 109}, {"referenceID": 11, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 132, "endOffset": 136}, {"referenceID": 180, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 159, "endOffset": 164}, {"referenceID": 181, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 182, "endOffset": 187}, {"referenceID": 182, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 222, "endOffset": 232}, {"referenceID": 183, "context": "There are numerous examples of its successful applications as a multi-view feature learning scheme, including human pose estimation [12], image classification [181], web data mining [182], as well as cross-media retrieval [183, 184].", "startOffset": 222, "endOffset": 232}, {"referenceID": 184, "context": "Undirected graphical models, also called Markov random fields that have many special cases, including the exponential family Harmonium [185] and restricted Boltzmann machine [186].", "startOffset": 135, "endOffset": 140}, {"referenceID": 185, "context": "Undirected graphical models, also called Markov random fields that have many special cases, including the exponential family Harmonium [185] and restricted Boltzmann machine [186].", "startOffset": 174, "endOffset": 179}, {"referenceID": 14, "context": "[15] first introduce a particular form of multi-view latent space Markov network model called multi-wing harmonium model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "This model can be viewed as an undirected counterpart of the aforementioned directed aspect models such as multi-modal LDA [11], with the advantages that inference is fast due to the conditional independence of the hidden units and that topic mixing can be achieved by document- and feature-specific combination of aspects.", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "[16] present a multi-view latent space Markov network and its large-margin extension that satisfies a weak conditional independence assumption that data from different views and the response variables are conditionally independent given a set of latent variables.", "startOffset": 0, "endOffset": 4}, {"referenceID": 186, "context": "In addition, Xie and Xing [187] propose a multi-modal distance metric learning (MMDML) framework based on the multi-wing harmonium model and metric learning method by [188].", "startOffset": 26, "endOffset": 31}, {"referenceID": 187, "context": "In addition, Xie and Xing [187] propose a multi-modal distance metric learning (MMDML) framework based on the multi-wing harmonium model and metric learning method by [188].", "startOffset": 167, "endOffset": 172}, {"referenceID": 188, "context": "Partial Least Squares (PLS) [189\u2013191] is a wide class of methods for modeling relations between sets of observed variables.", "startOffset": 28, "endOffset": 37}, {"referenceID": 189, "context": "Partial Least Squares (PLS) [189\u2013191] is a wide class of methods for modeling relations between sets of observed variables.", "startOffset": 28, "endOffset": 37}, {"referenceID": 190, "context": "It has been a popular tool for regression and classification as well as dimensionality reduction, especially in the field of chemometrics [192, 193].", "startOffset": 138, "endOffset": 148}, {"referenceID": 191, "context": "It has been a popular tool for regression and classification as well as dimensionality reduction, especially in the field of chemometrics [192, 193].", "startOffset": 138, "endOffset": 148}, {"referenceID": 46, "context": "be parameterized by a pair of matrices Wx \u2208 Rdx\u00d7k and Wy \u2208 Rdy\u00d7k [47].", "startOffset": 65, "endOffset": 69}, {"referenceID": 191, "context": "It has been shown that there is close connections between PLS and CCA in several aspects [190, 193].", "startOffset": 89, "endOffset": 99}, {"referenceID": 192, "context": "Guo and Mu [194] investigate the CCA based methods, including linear CCA, regularized CCA, and kernel CCA, and compare to the PLS models in solving the joint estimation problem.", "startOffset": 11, "endOffset": 16}, {"referenceID": 193, "context": "[195] introduce a least square form of PLS, called cross-modal factor analysis (CFA).", "startOffset": 0, "endOffset": 5}, {"referenceID": 196, "context": "Several extensions of CFA are presented by incorporating non-linearity and supervised information [196\u2013 198].", "startOffset": 98, "endOffset": 108}, {"referenceID": 197, "context": "Among the variants of PLS, Orthonormalized PLS (OPLS) [199\u2013201] is a popular representative.", "startOffset": 54, "endOffset": 63}, {"referenceID": 198, "context": "Among the variants of PLS, Orthonormalized PLS (OPLS) [199\u2013201] is a popular representative.", "startOffset": 54, "endOffset": 63}, {"referenceID": 199, "context": "Among the variants of PLS, Orthonormalized PLS (OPLS) [199\u2013201] is a popular representative.", "startOffset": 54, "endOffset": 63}, {"referenceID": 200, "context": "[202] investigate the equivalence relationship between CCA and OPLS and show that the difference between the CCA solution and the OPLS solution is a mere orthogonal transformation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 190, "context": "Another commonly used variant of PLS is PLS Regression (PLSR), which has been proven to be an efficient robust method to do quantity analysis for signal processing [192, 203].", "startOffset": 164, "endOffset": 174}, {"referenceID": 201, "context": "Another commonly used variant of PLS is PLS Regression (PLSR), which has been proven to be an efficient robust method to do quantity analysis for signal processing [192, 203].", "startOffset": 164, "endOffset": 174}, {"referenceID": 190, "context": "PLSR uses the two-block predictive PLS model to capture the relationship between X and Y [192].", "startOffset": 89, "endOffset": 94}, {"referenceID": 202, "context": "[204] present a probabilistic PLS to explain PLSR from a probabilistic viewpoint and describes the physical meaning of PLSR model.", "startOffset": 0, "endOffset": 5}, {"referenceID": 203, "context": "A number of multiview analysis approaches [205\u2013207] have also been proposed by considering the scenarios in which the data have multiple different views, along with supervised information.", "startOffset": 42, "endOffset": 51}, {"referenceID": 204, "context": "A number of multiview analysis approaches [205\u2013207] have also been proposed by considering the scenarios in which the data have multiple different views, along with supervised information.", "startOffset": 42, "endOffset": 51}, {"referenceID": 205, "context": "A number of multiview analysis approaches [205\u2013207] have also been proposed by considering the scenarios in which the data have multiple different views, along with supervised information.", "startOffset": 42, "endOffset": 51}, {"referenceID": 203, "context": ", ln), the two-view regularized Fisher discriminant analysis (FDA) [205] chooses two sets of weights wx and wy to solve the following optimization problem,", "startOffset": 67, "endOffset": 72}, {"referenceID": 206, "context": "In addition, based on the approach outlined in [208], both regularized two-view FDA and its kernel extension can be casted as equivalent disciplined convex optimization problems.", "startOffset": 47, "endOffset": 52}, {"referenceID": 204, "context": "[206] introduce Multiview Fisher Discriminant Analysis (MFDA) that learns classifiers in multiple views, by minimizing the variance of the data along the projection while maximizing the distance between the average outputs for classes over all of the views.", "startOffset": 0, "endOffset": 5}, {"referenceID": 205, "context": "In [207], Kan et al.", "startOffset": 3, "endOffset": 8}, {"referenceID": 207, "context": "Later based on bilinear models [209] and general graph embedding framework [210], Sharma et al.", "startOffset": 31, "endOffset": 36}, {"referenceID": 208, "context": "Later based on bilinear models [209] and general graph embedding framework [210], Sharma et al.", "startOffset": 75, "endOffset": 80}, {"referenceID": 209, "context": "[211] introduce Generalized Multi-view Analysis (GMA).", "startOffset": 0, "endOffset": 5}, {"referenceID": 210, "context": "Motivated by incorporating ranking information into multi-modal embedding learning, cross-modal ranking has attracted much attention in the literature [212\u2013214].", "startOffset": 151, "endOffset": 160}, {"referenceID": 211, "context": "Motivated by incorporating ranking information into multi-modal embedding learning, cross-modal ranking has attracted much attention in the literature [212\u2013214].", "startOffset": 151, "endOffset": 160}, {"referenceID": 212, "context": "Motivated by incorporating ranking information into multi-modal embedding learning, cross-modal ranking has attracted much attention in the literature [212\u2013214].", "startOffset": 151, "endOffset": 160}, {"referenceID": 210, "context": "[212] present a supervised semantic indexing (SSI) model which defines a class of non-linear models that are discriminatively trained to map multimodal input pairs into ranking scores.", "startOffset": 0, "endOffset": 5}, {"referenceID": 213, "context": "For this purpose, SSI exploits the margin ranking loss [215] which has already been widely used in information retrieval, and minimizes: \u2211", "startOffset": 55, "endOffset": 60}, {"referenceID": 214, "context": "This optimization problem can be solved through stochastic gradient descent [216],", "startOffset": 76, "endOffset": 81}, {"referenceID": 215, "context": "In fact, this method is a special margin ranking perceptron [217], which has been shown to be equivalent to SVM [218].", "startOffset": 60, "endOffset": 65}, {"referenceID": 216, "context": "In fact, this method is a special margin ranking perceptron [217], which has been shown to be equivalent to SVM [218].", "startOffset": 112, "endOffset": 117}, {"referenceID": 212, "context": "To exploit the advantage of online learning of kernel-based classifiers, Grangier and Bengio [214] propose a discriminative crossmodal ranking model called Passive-Aggressive Model for Image Retrieval (PAMIR), which not only adopts a learning criterion related to the final retrieval performance, but also considers different image kernels.", "startOffset": 93, "endOffset": 98}, {"referenceID": 217, "context": "Further, several recent efforts [219\u2013221] incorporate latent semantic representation learning into cross-modal ranking.", "startOffset": 32, "endOffset": 41}, {"referenceID": 218, "context": "Further, several recent efforts [219\u2013221] incorporate latent semantic representation learning into cross-modal ranking.", "startOffset": 32, "endOffset": 41}, {"referenceID": 217, "context": "[219] propose a Latent Semantic Cross-Modal Ranking (LSCMR) algorithm, which regards the cross-modal retrieval as a list-wise ranking problem and optimizes the list-wise ranking loss with a low-rank embedding setup.", "startOffset": 0, "endOffset": 5}, {"referenceID": 24, "context": "CCA and its extensions haven been widely applied to conduct cross-view similarity search [25, 38, 95].", "startOffset": 89, "endOffset": 101}, {"referenceID": 37, "context": "CCA and its extensions haven been widely applied to conduct cross-view similarity search [25, 38, 95].", "startOffset": 89, "endOffset": 101}, {"referenceID": 94, "context": "CCA and its extensions haven been widely applied to conduct cross-view similarity search [25, 38, 95].", "startOffset": 89, "endOffset": 101}, {"referenceID": 219, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 220, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 221, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 222, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 223, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 224, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 225, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 226, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 227, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 228, "context": "Hence, several multi-modal hashing methods have been proposed for fast similarity search in multi-modal data [222\u2013231].", "startOffset": 109, "endOffset": 118}, {"referenceID": 219, "context": "[222] propose a hashing-based model, called cross-modal similarity sensitive hashing (CMSSH), which approaches the cross-modality similarity learning problem by embedding the multi-modal data into a common metric space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 229, "context": "Following the greedy approach [232], the Hamming metric can be constructed sequentially as a superposition of weak binary classifiers on pairs of data points,", "startOffset": 30, "endOffset": 35}, {"referenceID": 230, "context": "Observing the resemblance with sequentially binary classifiers, boosted cross-modality similarity learning algorithms are introduced based on the standard AdaBoost procedure [233].", "startOffset": 174, "endOffset": 179}, {"referenceID": 231, "context": "However, CMSSH only considers the inter-view correlation but ignores the intra-view similarity [234].", "startOffset": 95, "endOffset": 100}, {"referenceID": 220, "context": "Kumar and Udupa [223] extend Spectral Hashing [235] from the single view setting to the multi-view scenario and present cross view hashing (CVH), which attempts to find hash functions that map similar objects to similar codewords over all the views so that inter-view and intraview similarities are both preserved.", "startOffset": 16, "endOffset": 21}, {"referenceID": 232, "context": "Kumar and Udupa [223] extend Spectral Hashing [235] from the single view setting to the multi-view scenario and present cross view hashing (CVH), which attempts to find hash functions that map similar objects to similar codewords over all the views so that inter-view and intraview similarities are both preserved.", "startOffset": 46, "endOffset": 51}, {"referenceID": 221, "context": "Gong and Lazebink [224] combine iterative quantization with CCA to exploit cross-modal embeddings for learning similarity preserving binary codes.", "startOffset": 18, "endOffset": 23}, {"referenceID": 222, "context": "Consequently, Zhen and Yang [225] present co-regularized hashing (CRH) for multi-modal data based on a boosted co-regularization framework.", "startOffset": 28, "endOffset": 33}, {"referenceID": 223, "context": "[226] introduce linear cross-modal hashing (LCMH), which is adopts a \u201ctwo-stage\u201d strategy to learn cross-view hashing functions.", "startOffset": 0, "endOffset": 5}, {"referenceID": 233, "context": "The data within each modality are first encoded into a low-rank representation using the idea of anchor graph [236] and then hash functions for each modality are learned to map each modality\u2019s low-rank space into a shared Hamming space.", "startOffset": 110, "endOffset": 115}, {"referenceID": 234, "context": "[237] introduce an inter-media hashing (IMH) model by jointly capturing inter-media and intra-media consistency.", "startOffset": 0, "endOffset": 5}, {"referenceID": 183, "context": "[184] present sparse multi-modal hashing by introducing dictionary learning into multi-view hashing, where the intra-modality and inter-modality similarities are modeled with a hypergraph.", "startOffset": 0, "endOffset": 5}, {"referenceID": 231, "context": "[234] additionally exploit discriminative information when learning coupled dictionaries to make the shared dictionary space interpretable.", "startOffset": 0, "endOffset": 5}, {"referenceID": 235, "context": "[238] present a latent semantic sparse hashing model by unifying sparse coding and matrix factorization.", "startOffset": 0, "endOffset": 5}, {"referenceID": 236, "context": "[239] present quantized correlation hashing (QCH).", "startOffset": 0, "endOffset": 5}, {"referenceID": 4, "context": "Inspired by the success of deep neural networks [5, 6, 124], a variety of deep multi-view feature learning methods have been proposed to capture the high-level correlation between multi-view data.", "startOffset": 48, "endOffset": 59}, {"referenceID": 5, "context": "Inspired by the success of deep neural networks [5, 6, 124], a variety of deep multi-view feature learning methods have been proposed to capture the high-level correlation between multi-view data.", "startOffset": 48, "endOffset": 59}, {"referenceID": 123, "context": "Inspired by the success of deep neural networks [5, 6, 124], a variety of deep multi-view feature learning methods have been proposed to capture the high-level correlation between multi-view data.", "startOffset": 48, "endOffset": 59}, {"referenceID": 16, "context": "The graphical model of deep multi-modal RBM (adapted from [17]), which models the joint distribution over image and text inputs.", "startOffset": 58, "endOffset": 62}, {"referenceID": 184, "context": ", Gaussian RBM [185] and replicated softmax RBM [241].", "startOffset": 15, "endOffset": 20}, {"referenceID": 237, "context": ", Gaussian RBM [185] and replicated softmax RBM [241].", "startOffset": 48, "endOffset": 53}, {"referenceID": 16, "context": "By extending the setup of the DBM, Srivastava and Salakhutdinov [17] propose a deep multi-modal RBM to model the relationship between image and text.", "startOffset": 64, "endOffset": 68}, {"referenceID": 5, "context": "Like RBM, exact maximum likelihood learning in this model is also intractable, while efficient approximate learning can be implemented by using mean-field inference to estimate datadependent expectations, and an MCMC based stochastic approximation procedure to approximate the model\u2019s expected sufficient statistics [6].", "startOffset": 316, "endOffset": 319}, {"referenceID": 238, "context": "Multi-modal DBM has been widely used for multi-view representation learning [242\u2013244].", "startOffset": 76, "endOffset": 85}, {"referenceID": 239, "context": "Multi-modal DBM has been widely used for multi-view representation learning [242\u2013244].", "startOffset": 76, "endOffset": 85}, {"referenceID": 240, "context": "Multi-modal DBM has been widely used for multi-view representation learning [242\u2013244].", "startOffset": 76, "endOffset": 85}, {"referenceID": 239, "context": "[243] employ the multimodal DBM to learn joint representation for predicting answers in cQA portal.", "startOffset": 0, "endOffset": 5}, {"referenceID": 240, "context": "[244] apply the multi-modal RBM to determining information trustworthiness, in which the learned joint representation denotes the consistent latent reasons that underline users\u2019 ratings from multiple sources.", "startOffset": 0, "endOffset": 5}, {"referenceID": 241, "context": "Pang and Ngo [245] propose to learn a joint density model for emotion prediction in usergenerated videos with a deep multi-modal Boltzmann machine.", "startOffset": 13, "endOffset": 18}, {"referenceID": 242, "context": "[246] investigate an improved multi-modal RBM framework via minimizing the variation of information between data modalities through the shared latent representation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 243, "context": "[247] present a multi-task multi-modal RBM (MTM-RBM) approach for facial attribute classification.", "startOffset": 0, "endOffset": 5}, {"referenceID": 244, "context": "In particular, a multi-task RBM is proposed by extending the formulation of discriminative RBM [248] to account for multiple tasks.", "startOffset": 95, "endOffset": 100}, {"referenceID": 17, "context": "The bimodal deep autoencoder (adapted from [18]).", "startOffset": 43, "endOffset": 47}, {"referenceID": 17, "context": "Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.", "startOffset": 30, "endOffset": 43}, {"referenceID": 245, "context": "Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.", "startOffset": 30, "endOffset": 43}, {"referenceID": 246, "context": "Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.", "startOffset": 30, "endOffset": 43}, {"referenceID": 247, "context": "Multi-modal deep autoencoders [18, 249\u2013251] gradually become good alternatives for learning a shared representation between modalities due to the sufficient flexibility of their objectives.", "startOffset": 30, "endOffset": 43}, {"referenceID": 4, "context": "Inspired by denoising autoencoders [5], Ngiam et al.", "startOffset": 35, "endOffset": 38}, {"referenceID": 17, "context": "[18] propose to extract shared representations via training a bimodal deep autoencoder (Figure 5) using an augmented but noisy dataset with additional examples that have only a single modality as input.", "startOffset": 0, "endOffset": 4}, {"referenceID": 248, "context": "The key idea is to use greedy layer-wise training with an extension to RBMs with sparsity [252] followed by fine-tuning.", "startOffset": 90, "endOffset": 95}, {"referenceID": 18, "context": "[19] propose a correspondence autoencoder (Corr-AE) via constructing correlations between hidden representations of two uni-modal deep autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The correspondence deep autoencoder (adapted from [19]).", "startOffset": 50, "endOffset": 54}, {"referenceID": 17, "context": "Besides, based on two other multi-modal autoencoders [18], Corr-AE is extended to two other correspondence deep models, called CorrCross-AE and Corr-Full-AE.", "startOffset": 53, "endOffset": 57}, {"referenceID": 249, "context": "[253] propose an effective mapping mechanism based on stacked autoencoder for multimodal retrieval.", "startOffset": 0, "endOffset": 5}, {"referenceID": 19, "context": "[20] propose deep canonically correlated autoencoders that also consist of two autoencoders and optimize the combination of canonical correlations between the learned bottleneck representations and the reconstruction errors of the autoencoders.", "startOffset": 0, "endOffset": 4}, {"referenceID": 250, "context": "[254] suggest to exploit the cross weights between representations of modalities for gradually learning interactions of the modalities in a multi-modal deep autoencoder network.", "startOffset": 0, "endOffset": 5}, {"referenceID": 251, "context": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258].", "startOffset": 118, "endOffset": 128}, {"referenceID": 252, "context": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258].", "startOffset": 118, "endOffset": 128}, {"referenceID": 253, "context": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258].", "startOffset": 178, "endOffset": 188}, {"referenceID": 254, "context": "Deep cross-view embedding models have become increasingly popular in the applications including cross-media retrieval [255, 256] and multi-modal distributional semantic learning [257, 258].", "startOffset": 178, "endOffset": 188}, {"referenceID": 251, "context": "[255] propose a deep visual-semantic embedding model (DeViSE), which connects two deep neural networks by a cross-modal mapping.", "startOffset": 0, "endOffset": 5}, {"referenceID": 251, "context": "The DeViSE model (adapted from [255]), which is initialized with parameters pre-trained at the lower layers of the visual object categorization network and the skip-gram language model.", "startOffset": 31, "endOffset": 36}, {"referenceID": 255, "context": "initialized with a pre-trained neural network language model [259] and a pre-trained deep visual-semantic model [124].", "startOffset": 61, "endOffset": 66}, {"referenceID": 123, "context": "initialized with a pre-trained neural network language model [259] and a pre-trained deep visual-semantic model [124].", "startOffset": 112, "endOffset": 117}, {"referenceID": 210, "context": "Following the setup of loss function in [212], DeViSE employs a combination of dot-product similarity and hinge rank loss so that the model has the ability of producing a higher dot-product similarity between the visual model output and the vector representation of the correct label than between the visual output and the other randomly chosen text terms.", "startOffset": 40, "endOffset": 45}, {"referenceID": 256, "context": "[261] propose a convex combination of semantic embedding model (ConSE) for mapping images into continuous semantic embedding spaces.", "startOffset": 0, "endOffset": 5}, {"referenceID": 257, "context": "[262] develop a deep multi-modal similarity model that learns two neural networks to map images and text fragments to a common vector representation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 258, "context": "With the development of multi-modal distributional semantic models [263\u2013265], deep cross-modal mapping is naturally exploited to learn the improved multimodal distributed semantic representation.", "startOffset": 67, "endOffset": 76}, {"referenceID": 259, "context": "With the development of multi-modal distributional semantic models [263\u2013265], deep cross-modal mapping is naturally exploited to learn the improved multimodal distributed semantic representation.", "startOffset": 67, "endOffset": 76}, {"referenceID": 260, "context": "With the development of multi-modal distributional semantic models [263\u2013265], deep cross-modal mapping is naturally exploited to learn the improved multimodal distributed semantic representation.", "startOffset": 67, "endOffset": 76}, {"referenceID": 254, "context": "[258] introduce multimodal skipgram models to extend the skip-gram model of [259] by taking visual information into account.", "startOffset": 0, "endOffset": 5}, {"referenceID": 255, "context": "[258] introduce multimodal skipgram models to extend the skip-gram model of [259] by taking visual information into account.", "startOffset": 76, "endOffset": 81}, {"referenceID": 252, "context": "[256] present Word2VisualVec, a deep neural network architecture that learns to predict a deep visual encoding of textual input, and thus enables cross-media retrieval in a visual space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 261, "context": "[266] propose a unified framework that jointly models video and the corresponding text sentence.", "startOffset": 0, "endOffset": 5}, {"referenceID": 262, "context": "[267] present a unified deep neural network model called cross space mapping, in which the image and query are mapped to a common vector space via a convolution part and a query-embedding part, respectively.", "startOffset": 0, "endOffset": 5}, {"referenceID": 263, "context": "[268] also introduce a deep cross-modal retrieval method, called deep compositional cross-modal learning to rank (CMLR).", "startOffset": 0, "endOffset": 5}, {"referenceID": 264, "context": "[269] introduce a deep semantic matching method, in which two independent deep networks are learned to map image and text into a common semantic space with a high level abstraction.", "startOffset": 0, "endOffset": 5}, {"referenceID": 265, "context": "[270] consider learning multi-modal representation from the perspective of encoding the explicit/implicit relevance relationship between the vertices in the click graph, in which vertices are images/text queries and edges indicate the clicks between an image and a query.", "startOffset": 0, "endOffset": 5}, {"referenceID": 16, "context": "Motivated by the recent advance of deep learning for multi-modal data [17, 18, 255], deep neural networks have gradually been exploited to learn hash functions.", "startOffset": 70, "endOffset": 83}, {"referenceID": 17, "context": "Motivated by the recent advance of deep learning for multi-modal data [17, 18, 255], deep neural networks have gradually been exploited to learn hash functions.", "startOffset": 70, "endOffset": 83}, {"referenceID": 251, "context": "Motivated by the recent advance of deep learning for multi-modal data [17, 18, 255], deep neural networks have gradually been exploited to learn hash functions.", "startOffset": 70, "endOffset": 83}, {"referenceID": 266, "context": "[271] introduce a deep multi-view hashing method in which each layer of hidden nodes consists of view-specific and shared hidden nodes to learn individual and shared hidden spaces from multiple views of data.", "startOffset": 0, "endOffset": 5}, {"referenceID": 267, "context": "[272] propose a cross-media hashing approach based on a correspondence multi-modal neural network, referred as Cross-Media Neural Network Hashing (CMNNH).", "startOffset": 0, "endOffset": 5}, {"referenceID": 267, "context": "The illustration of cross-media neural network hashing architecture (adapted from [272]).", "startOffset": 82, "endOffset": 87}, {"referenceID": 219, "context": "By following the setup of most of the existing hashing methods [222, 235], the sign function can be removed at the hash function learning stage and added at the testing stage.", "startOffset": 63, "endOffset": 73}, {"referenceID": 232, "context": "By following the setup of most of the existing hashing methods [222, 235], the sign function can be removed at the hash function learning stage and added at the testing stage.", "startOffset": 63, "endOffset": 73}, {"referenceID": 268, "context": "[273] introduce a deep multi-modal hashing model with orthogonal regularization for mapping multi-modal data into a common hamming space.", "startOffset": 0, "endOffset": 5}, {"referenceID": 272, "context": "The illustration of the RNN encoder-decoder (adapted from [277]).", "startOffset": 58, "endOffset": 63}, {"referenceID": 269, "context": "Recently, Jiang and Li [274] propose a deep cross-modal hashing method (DCMH) by integrating feature learning and hashcode learning into the same framework.", "startOffset": 23, "endOffset": 28}, {"referenceID": 18, "context": "present a fully correlation autoencoder hashing method by extending the correspondence autoencoder by [19].", "startOffset": 102, "endOffset": 106}, {"referenceID": 270, "context": "A recurrent neural network (RNN) [275] is a neural network which processes a variable-length sequence x = (x1, .", "startOffset": 33, "endOffset": 38}, {"referenceID": 271, "context": "For example, a simple case may be a common element-wise logistic sigmoid function and a complex case may be a long short-term memory (LSTM) unit [276].", "startOffset": 145, "endOffset": 150}, {"referenceID": 272, "context": "[277] propose a RNN encoder-decoder model by exploiting RNN to connect multi-modal sequence.", "startOffset": 0, "endOffset": 5}, {"referenceID": 273, "context": "[278] also present a general endto-end approach for multi-modal sequence to sequence learning based on deep LSTM networks, which is very useful for learning problems with long range temporal dependencies [276, 279].", "startOffset": 0, "endOffset": 5}, {"referenceID": 271, "context": "[278] also present a general endto-end approach for multi-modal sequence to sequence learning based on deep LSTM networks, which is very useful for learning problems with long range temporal dependencies [276, 279].", "startOffset": 204, "endOffset": 214}, {"referenceID": 274, "context": "[278] also present a general endto-end approach for multi-modal sequence to sequence learning based on deep LSTM networks, which is very useful for learning problems with long range temporal dependencies [276, 279].", "startOffset": 204, "endOffset": 214}, {"referenceID": 272, "context": "Similar to [277], the conditional probability is computed by first obtaining the fixed dimensional representation v of the input sequence (x1, .", "startOffset": 11, "endOffset": 16}, {"referenceID": 20, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 71, "endOffset": 84}, {"referenceID": 21, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 71, "endOffset": 84}, {"referenceID": 275, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 71, "endOffset": 84}, {"referenceID": 22, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 104, "endOffset": 118}, {"referenceID": 276, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 104, "endOffset": 118}, {"referenceID": 277, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 104, "endOffset": 118}, {"referenceID": 278, "context": "Besides, multi-modal RNNs have been widely applied in image captioning [21, 22, 280], videos captioning [23, 281, 282], and visual question answering [283].", "startOffset": 150, "endOffset": 155}, {"referenceID": 20, "context": "Karpathy and Li [21] propose a multi-modal recurrent neural network architecture to generate new descriptions of image regions.", "startOffset": 16, "endOffset": 20}, {"referenceID": 279, "context": "Chen and Zitnick [284] explore the bi-directional mapping between images and their sentencebased descriptions with RNNs.", "startOffset": 17, "endOffset": 22}, {"referenceID": 277, "context": "[282] introduce an end-to-end sequence model to generate captions for videos.", "startOffset": 0, "endOffset": 5}, {"referenceID": 280, "context": "By applying attention mechanism [285] to visual recognition [286, 287], Xu et al.", "startOffset": 32, "endOffset": 37}, {"referenceID": 281, "context": "By applying attention mechanism [285] to visual recognition [286, 287], Xu et al.", "startOffset": 60, "endOffset": 70}, {"referenceID": 282, "context": "By applying attention mechanism [285] to visual recognition [286, 287], Xu et al.", "startOffset": 60, "endOffset": 70}, {"referenceID": 283, "context": "[288] introduce an attention based multimodal RNN model, which trains the multi-modal RNN in a deterministic manner using the standard back propagation.", "startOffset": 0, "endOffset": 5}, {"referenceID": 23, "context": "[24].", "startOffset": 0, "endOffset": 4}, {"referenceID": 285, "context": "This semi-supervised alignment is similar to manifold ranking [290], which learns to rank on data manifolds.", "startOffset": 62, "endOffset": 67}, {"referenceID": 284, "context": "[289] extend the original semi-supervised alignment and proposed a method called parallel field alignment retrieval (PFAR), which investigates alignment framework from the perspective of parallel vector fields.", "startOffset": 0, "endOffset": 5}, {"referenceID": 284, "context": "An illustrative example of parallel field alignment (adapted from [289]).", "startOffset": 66, "endOffset": 71}, {"referenceID": 23, "context": "[24] also introduce manifold alignment with pairwise correspondence, which builds connections between multiple data sets by aligning their underlying manifolds.", "startOffset": 0, "endOffset": 4}, {"referenceID": 286, "context": "[291] propose a two-step manifold alignment to solve this problem.", "startOffset": 0, "endOffset": 5}, {"referenceID": 287, "context": "Further, Wang and Mahadevan [292] also introduce a twostage approach for manifold alignment using procrustes analysis.", "startOffset": 28, "endOffset": 33}], "year": 2016, "abstractText": "Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper first reviews the root methods and theories on multi-view representation learning, especially on canonical correlation analysis (CCA) and its several extensions. And then we investigate the advancement of multi-view representation learning that ranges from shallow methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to deep methods including multi-modal restricted Boltzmann machines, multi-modal autoencoders, and multi-modal recurrent neural networks. Further, we also provide an important perspective from manifold alignment for multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical basis and current developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.", "creator": "LaTeX with hyperref package"}}}