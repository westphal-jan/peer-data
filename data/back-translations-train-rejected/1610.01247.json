{"id": "1610.01247", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "ECAT: Event Capture Annotation Tool", "abstract": "This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft's Kinect(R) hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT's object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool's workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT's output to VoxML will also be addressed.", "histories": [["v1", "Wed, 5 Oct 2016 01:24:42 GMT  (2904kb,D)", "http://arxiv.org/abs/1610.01247v1", "4 pages, 4 figures, ISA workshop 2015"]], "COMMENTS": "4 pages, 4 figures, ISA workshop 2015", "reviews": [], "SUBJECTS": "cs.CL cs.CV", "authors": ["tuan do", "nikhil krishnaswamy", "james pustejovsky"], "accepted": false, "id": "1610.01247"}, "pdf": {"name": "1610.01247.pdf", "metadata": {"source": "CRF", "title": "ECAT: Event Capture Annotation Tool", "authors": ["Tuan Do", "Nikhil Krishnaswamy", "James Pustejovsky"], "emails": ["tuandn@brandeis.edu", "nkrishna@brandeis.edu", "jamesp@brandeis.edu"], "sections": [{"heading": null, "text": "Keywords: Event capture, Event note, Motion capture"}, {"heading": "1. Introduction", "text": "Many existing work in the field of video annotation focused on capturing objects from the video in a purely two-dimensional format (i.e. tracking pixels) as in (Goldman et al., 2008), among other things, or on capturing the positioning of the human body in 3D for pose and gesture recognition (Tilting et al., 2014).We try to combine these two types of capabilities by extracting the positions and orientations of objects and human body rigs in videos recorded by Microsoft Kinectr. These objects can be commented on as participants in a recorded motion event, and this designated data can then be used to create a corpus of multimodal semantic simulations of these events that can model object-object, object-agent, and agent-interactions through the duration of said events. This library of simulated motion events can serve as a novel resource of direct linkages from natural language to visualization of events, relying on the ability of the object to generate the motion of the object and the Kinect in the body."}, {"heading": "2. Functionality", "text": "We use Kinect Sensor v2 for Windows, which supports resolutions up to 1920px x x 1080px (RGB video) and 512px x 424px x x 8 meters (depth)."}, {"heading": "2.1. Capture and Input", "text": "For ECAT, we have developed our own capture and compression capabilities, rather than using the standard Kinect SDK functionality due to the large size of the resulting raw files. Kinect capture automatically detects human bodies. Other objects can be marked manually with annotations, and once a video is recorded and loaded, annotations can play back and edit it, which can include removing an incorrectly detected human body rig from the scene or cutting out the video clip. the video clip can contain frames that go beyond the interval of the captured event. Default RGB color image and depth stream data are stored as separate video files. Body tracking data is stored along with a schema file that specifies the name and index of each detected joint in Board1, how they are connected and how they can be projected onto the RGB video. In addition, users can import a property scheme file that allows them to comment on any set of objects that they can modify."}, {"heading": "2.2. User Interface", "text": "Figure 1 shows the ECAT GUI. The various components are listed below. 1. Project Management Panel. Each project may include several recorded sessions. 2. Video display. To display color video or grayscale depth field video, and to locate objects of interest in the scene - for example, the table outlined in green in Figure 1.3. Object annotation controller. Yellow timelines show when each tracked object appears in the video. 1. A human body is always a rooted tree, its nodes and edges roughly shaped like a human stick figure.ar Xiv: 161 0.01 247v 1 [cs.C L] 5O ct2 016Black ticks mark boxes in which a Remarkable polygon has been drawn around the object using the object property (Element 5). Link the selected object to another by associating it with a different spatial configuration."}, {"heading": "2.3. Object Annotation", "text": "ECAT supports two ways of marking objects in a video: one is to import objects that have been automatically tracked using other libraries, such as human body rigs detected by Kinect SDK; the other is to comment on the positions of objects in the RGB video stream; annotators mark the positions of objects at the beginning and end of an interval; and ECAT provides semi-automatic tracking using depth field data and the iterative method of the closest point (Besl and McKay, 1992) to track the three-dimensional position of the object; the tracking algorithm output can be either a point cloud or a parametric format if the shape of the object can be approximated as a simple geometry (e.g. an orange or an apple could be modeled as a spheroid, with the tracking output being just the position of the object center and a radius)."}, {"heading": "2.4. Event Annotation", "text": "In principle, there are at least two ways of annotating an event associated with a video or video sub-interval: (a) identifying an event type from an existing ontology or semantic resource such as FrameNet (Baker et al., 1998); or (b) describing the event in natural language. We currently use the latter approach to fill the text field of an event, but are working to link ontologies to the event-tag information addressed in Section 4. As mentioned in Section 2.2, ECAT allows the annotation of event-consistent relationships. Therefore, an overall event can be annotated as set, but it contains the substances Grab, Hold, Move, and Unshave, which may overlap with some sub-sections of the main event and with each other."}, {"heading": "3. Links to VoxML", "text": "Entities modeled in VoxML can be objects, programs, attributes, relationships, or functions. VoxML OBJECT is used to model nouns, while PROGRAM is used to model events. The semantic field of an object captured in ECAT, which is filled with free NL input, can be associated with objects commented on in VoxML if objects with the specified label exist in the VoxML-based lexicon. An object of semanticType = block can be associated in a 3D scene with a VoxML object designated by the Lexeme block, and the captured object with all the ontological and semantic data provided by the VoxML-based lexicon (e.g. an object marked with semanticType = stack can be associated in a 3D scene seticmantic block designated by the Lexeme block, and the captured object with all the ontological and semantic data provided by the VoxML-based lexicon (e.g. an object marked with semanticType = stack can be associated with a VoxMack, \"a VoxMack event that is associated with a VoxMack event that is associated with an event in the ECX-MAT)."}, {"heading": "4. Output", "text": "Body rigs are stored as objects with semanticType = body rig. They are ID'ed (id = o1 as shown in fig. 2) and can be given an alias for ease of use (here John). Commented objects are treated similarly, get an ID, a name and a semantic type. here o2 is the shell logo block from fig. 1. Object locations and relative spatial relationships can be commented after frame. frame 1 has o2 on the table (o3), while frame 50 has it set to the other block (o4), so the corresponding LinkTo tags are On (o2, o3) or On (o2, o4). By default, ECAT supports the relationships On, In, Part of, and Attach To, where an object is in a parent-child relationship with another object (o4)."}, {"heading": "5. Conclusions", "text": "Recognition and recognition of events and actions in video is gaining increasing attention in the scientific community due to their relevance to a variety of applications (Ballan et al., 2011) and calls have been made for an annotation infrastructure, including video (Ide, 2013).We have presented a tool here that provides a user-friendly interface for video annotations that is capable of capturing a depth of detail not provided by most existing video annotation tools, links to existing linguistic infrastructures, and is well suited for building a corpus of event-annotated multimodal simulations for use in spatial and motion semantics (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013).For future annotation capabilities, we plan to introduce links to existing semantic lexical resources, such as FrameNet, as event sontologies. More importantly, we are expanding the ECAT environment to include multiple events, or summarizing multiple events."}, {"heading": "Acknowledgements", "text": "This work is supported by the DARPA Contract W911NF-15C-0238 with the US Defense Advanced Research Projects Agency (DARPA) and the Army Research Office (ARO). Approved for Public Release, Distribution Unlimited. The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the US Government. All errors and errors are, of course, the responsibility of the authors."}, {"heading": "6. Bibliographical References", "text": "Baker, C. F., Fillmore, C. J., and Lowe, J. B. (1998). Theberkeley framenet project. In Proceedings of the 17th International Conference on Computational linguisticsVolume 1, pp. 86-90. Association for Computational Linguistics.Ballan, L., Bertini, M., Del Bimbo, A., Seidenari, L., and Serra, G. (2011). Event detection and recognition for semantic annotation of video. Multimedia Tools and Applications, 51 (1): 279-302.Besl, P. J. and McKay, N. D. (1992). Method for registration of 3-d shapes. In Robotics-DL tentative, pp. 586- 606. International Society for Optics and Photonics.Goldman, D. B., Gonterman, C., C. and C. and McKay, N. D., Pudelic."}], "references": [{"title": "The berkeley framenet project", "author": ["C.F. Baker", "C.J. Fillmore", "J.B. Lowe"], "venue": "In Proceedings of the 17th international conference on Computational linguisticsVolume", "citeRegEx": "Baker et al\\.,? \\Q1998\\E", "shortCiteRegEx": "Baker et al\\.", "year": 1998}, {"title": "Event detection and recognition for semantic annotation of video", "author": ["L. Ballan", "M. Bertini", "A. Del Bimbo", "L. Seidenari", "G. Serra"], "venue": "Multimedia Tools and Applications,", "citeRegEx": "Ballan et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Ballan et al\\.", "year": 2011}, {"title": "Method for registration of 3-d shapes", "author": ["P.J. Besl", "N.D. McKay"], "venue": "In Robotics-DL tentative,", "citeRegEx": "Besl and McKay,? \\Q1992\\E", "shortCiteRegEx": "Besl and McKay", "year": 1992}, {"title": "Video object annotation, navigation, and composition", "author": ["D.B. Goldman", "C. Gonterman", "B. Curless", "D. Salesin", "S.M. Seitz"], "venue": "In Proceedings of the 21st annual ACM symposium on User interface software and technology,", "citeRegEx": "Goldman et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Goldman et al\\.", "year": 2008}, {"title": "An open linguistic infrastructure for annotated corpora", "author": ["N. Ide"], "venue": null, "citeRegEx": "Ide,? \\Q2013\\E", "shortCiteRegEx": "Ide", "year": 2013}, {"title": "Single-person and multi-party 3d visualizations for nonverbal communication analysis", "author": ["M. Springer. Kipp", "L.F. von Hollen", "M.C. Hrstka", "F. Zamponi"], "venue": "Web Meets NLP,", "citeRegEx": "Kipp et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kipp et al\\.", "year": 2014}, {"title": "Performance measurements for the microsoft kinect skeleton", "author": ["M.A. Livingston", "J. Sebastian", "Z. Ai", "J.W. Decker"], "venue": "In Virtual Reality Short Papers and Posters (VRW),", "citeRegEx": "Livingston et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Livingston et al\\.", "year": 2012}, {"title": "Voxml: A visual object modeling language", "author": ["J. Pustejovsky", "N. Krishnaswamy"], "venue": "Proceedings of LREC", "citeRegEx": "Pustejovsky and Krishnaswamy,? \\Q2016\\E", "shortCiteRegEx": "Pustejovsky and Krishnaswamy", "year": 2016}, {"title": "The qualitative spatial dynamics of motion", "author": ["J. Pustejovsky", "J. Moszkowicz"], "venue": "The Journal of Spatial Cognition and Computation", "citeRegEx": "Pustejovsky and Moszkowicz,? \\Q2011\\E", "shortCiteRegEx": "Pustejovsky and Moszkowicz", "year": 2011}, {"title": "Dynamic event structure and habitat theory", "author": ["J. Pustejovsky"], "venue": "In Proceedings of the 6th International Conference on Generative Approaches to the Lexicon", "citeRegEx": "Pustejovsky,? \\Q2013\\E", "shortCiteRegEx": "Pustejovsky", "year": 2013}], "referenceMentions": [{"referenceID": 7, "context": "The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT\u2019s object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances.", "startOffset": 28, "endOffset": 64}, {"referenceID": 3, "context": "tracking pixels) as in (Goldman et al., 2008), among others, or in capturing human body positioning in 3D for pose and gesture recognition (Kipp et al.", "startOffset": 23, "endOffset": 45}, {"referenceID": 5, "context": ", 2008), among others, or in capturing human body positioning in 3D for pose and gesture recognition (Kipp et al., 2014).", "startOffset": 101, "endOffset": 120}, {"referenceID": 6, "context": "The Kinect\u2019s depth field stream facilitates improved tracking of human movement, as reflected in the Kinect SDK\u2019s skeleton and face tracking performance (Livingston et al., 2012).", "startOffset": 153, "endOffset": 178}, {"referenceID": 2, "context": "Annotators mark the locations of objects at the beginning and end of an interval, and ECAT provides semi-automatic tracking using the depth field data and the iterative closest point method (Besl and McKay, 1992) to track the object\u2019s three-dimensional location.", "startOffset": 190, "endOffset": 212}, {"referenceID": 0, "context": "In principle, there are at least two ways to annotate an event associated with a video or video subinterval: (a) IDing an event type from an existing ontology or semantic resource, such as FrameNet (Baker et al., 1998); or (b) describing the event in natural language.", "startOffset": 198, "endOffset": 218}, {"referenceID": 1, "context": "Event and action detection and recognition in video is receiving increasing attention in the scientific community, due to its relevance to a wide variety of applications (Ballan et al., 2011) and there have been calls for annotation infrastructure that includes video (Ide, 2013).", "startOffset": 170, "endOffset": 191}, {"referenceID": 4, "context": ", 2011) and there have been calls for annotation infrastructure that includes video (Ide, 2013).", "startOffset": 84, "endOffset": 95}, {"referenceID": 8, "context": "We have presented here a tool that provides a user-friendly interface for video annotation that is able to capture a level of detail not provided by most existing video annotation tools, provides links to existing linguistic infrastructures, and is well suited for building a corpus of event-annotated multimodal simulations for use in the study of spatial and motion semantics (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013).", "startOffset": 378, "endOffset": 431}, {"referenceID": 9, "context": "We have presented here a tool that provides a user-friendly interface for video annotation that is able to capture a level of detail not provided by most existing video annotation tools, provides links to existing linguistic infrastructures, and is well suited for building a corpus of event-annotated multimodal simulations for use in the study of spatial and motion semantics (Pustejovsky and Moszkowicz, 2011; Pustejovsky, 2013).", "startOffset": 378, "endOffset": 431}], "year": 2016, "abstractText": "This paper introduces the Event Capture Annotation Tool (ECAT), a user-friendly, open-source interface tool for annotating events and their participants in video, capable of extracting the 3D positions and orientations of objects in video captured by Microsoft\u2019s Kinectr hardware. The modeling language VoxML (Pustejovsky and Krishnaswamy, 2016) underlies ECAT\u2019s object, program, and attribute representations, although ECAT uses its own spec for explicit labeling of motion instances. The demonstration will show the tool\u2019s workflow and the options available for capturing event-participant relations and browsing visual data. Mapping ECAT\u2019s output to VoxML will also be addressed.", "creator": "LaTeX with hyperref package"}}}