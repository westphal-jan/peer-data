{"id": "1610.01367", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Monaural Multi-Talker Speech Recognition using Factorial Speech Processing Models", "abstract": "A Pascal challenge entitled monaural multi-talker speech recognition was developed, targeting the problem of robust automatic speech recognition against speech like noises which significantly degrades the performance of automatic speech recognition systems. In this challenge, two competing speakers say a simple command simultaneously and the objective is to recognize speech of the target speaker. Surprisingly during the challenge, a team from IBM research, could achieve a performance better than human listeners on this task. The proposed method of the IBM team, consist of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task of this challenge based on gain adapted factorial speech processing models. It develops a joint-token passing algorithm for direct utterance decoding of both target and masker speakers, simultaneously. Comparing it to the challenge winner, it uses maximum uncertainty during the decoding which cannot be used in the past two-phased method. It provides detailed derivation of inference on these models based on general inference procedures of probabilistic graphical models. As another improvement, it uses deep neural networks for joint-speaker identification and gain estimation which makes these two steps easier than before producing competitive results for these steps. The proposed method of this work outperforms past super-human results and even the results were achieved recently by Microsoft research, using deep neural networks. It achieved 5.5% absolute task performance improvement compared to the first super-human system and 2.7% absolute task performance improvement compared to its recent competitor.", "histories": [["v1", "Wed, 5 Oct 2016 11:34:36 GMT  (1171kb)", "http://arxiv.org/abs/1610.01367v1", null]], "reviews": [], "SUBJECTS": "cs.CL cs.SD", "authors": ["mahdi khademian", "mohammad mehdi homayounpour"], "accepted": false, "id": "1610.01367"}, "pdf": {"name": "1610.01367.pdf", "metadata": {"source": "CRF", "title": "Monaural Multi-Talker Speech Recognition using Factorial Speech Processing Models", "authors": ["Mahdi Khademian", "Mohammad Mehdi Homayounpour"], "emails": ["homayoun@aut.ac.ir"], "sections": [{"heading": null, "text": "A Pascal challenge called Monaural Multi-Talker Speech Recognition was developed to address the problem of robust automatic speech recognition against speech-like sounds that significantly worsen the performance of automatic speech recognition systems. In this challenge, two competing speakers simultaneously say a simple command and the goal is to recognize the target speaker's language. Surprisingly, an IBM research team was able to perform better than human listeners in this task. The IBM team's proposed method consists of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task based on customized factorial speech processing models, developing a common token distribution algorithm for direct decoding of target and mask speakers at the same time. Comparing it with the winner of the challenge, it uses maximum uncertainty during the decryption of this task, which cannot be used in the previous two phases."}, {"heading": "1. Introduction", "text": "In fact, the truth is that most of them are not just a suicide attack, but a conspiracy, which is a conspiracy."}, {"heading": "2. Factorial speech processing models for single channel speech recognition", "text": "The aim of monaural speech separation and speech recognition challenge is to identify some keywords of a target speaker from a mixed language of the target speaker and a mask speaker (Cooke et al., 2010). Mixed speech signals for this task are artificially generated from speech materials of the grid body (Cooke et al., 2006). This corpus contains simple six-word slot commands from 34 different speakers. Each command is a sequence of command words, color, preposition, a letter, a digit and an advertisement, which is generated in Fig. 1. Mixed speech signals are generated by selecting two expressions from the grid body, one for the target speaker and the other for the mask player. The target speaker always uses \"white\" as the command color and the mask does not. This is the indication of discrimination of the target and mask voices. Two speech signals are mixed by the following time domain relationship:"}, {"heading": "2.1. Factorial speech processing models", "text": "This year, it has come to the point that it has never come as far in the United States as it has this year."}, {"heading": "2.2. Inference", "text": "The aim of the conclusion in models in the form of Fig. 2 is to find the most probable states of the sources within a period of time, taking into account the observed characteristic vectors from that period: 1:, 1:, (1:, 1:) (3) However, the main objective of the conclusion in our task is to find the order of the spoken words of each speaker, or more precisely, the letter and number of the target speaker mentioned. Decoding the most probable acoustic states in the order of the spoken words is done by a joint decoder. In this subsection, acoustic and temporal conclusions on the factor model are described and in the next subsection, the deciphering process is discussed."}, {"heading": "2.2.1. Acoustic inference", "text": "The graphical model of Fig. 2 is not used directly in the applications. In fact, in this step three CPDs are merged, including (,), (,) and (,, (,) in state conditional probabilities (,,,),"}, {"heading": "2.2.2. VTS based acoustic inference", "text": "In fact, most of them are able to determine for themselves what they want to do and what they want to do."}, {"heading": "2.2.3. Temporal inference", "text": "It is not the first time that EU member states have found themselves in such a situation."}, {"heading": "2.3. Joint-decoding", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "3. Determining and adapting source models", "text": "The use of speaker models in the factor models for the task of \"monaural speech separation and recognition\" significantly improves recognition performance, since discriminatory features of speaker voices are also indications of improvement in acoustic inference over time, rather than dynamic constraints that apply to the entire utterance. Furthermore, any adjustment related to the amplification effect of the expression (1) to synthesize the mixed speech signal also significantly improves recognition performance. In the task of mixed voice speech recognition, statements from two speakers out of 34 speakers in the GRID dataset are randomly selected to synthesize each mixed voice signal. During the test phase, the use of speaker labels from the test files is prohibited, and therefore, the identity of speakers for each utterance is unknown to the detection device. On the other hand, TMR is also unknown to the detection device."}, {"heading": "3.1. Speaker identification", "text": "In the challenge, model-based methods such as the IBM system use speaker conditional probabilities for calculating the speaker id posteriors over the frame of the input mixed language signal. Then the speaker is identified by tuning high confidence, which results in relatively accurate speaker identification. In the proposed method we simply use a deep neural network for common speaker identification, which also provides competing results with relative simplicity and shorter test time."}, {"heading": "3.2. Gain estimation and model adaptation", "text": "This year, it has come to the point where we see ourselves in a position to put ourselves at the top, \"he said in an interview with the Deutsche Presse-Agentur.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said,\" but we have to put ourselves at the top. \""}, {"heading": "4. Experiments", "text": "The challenge of monaural speech separation and recognition is based on the GRID dataset (Cooke et al., 2006). The language material of this task consists of 1000 utterances from 34 speakers, which are evenly distributed for the training and test phase. Synthesized mixed speech signals from randomly selected common speakers are extracted from 500 utterances of the test part for each speaker in 6 different TMRs, including 6, 3, 0, -3, -6 and -9 dBs. For each TMR, 600 or 300 mixed speech signals are synthesized for the test and development set. While the identification of desktop speakers and the mixed speaker can be understood by the file names and folders, it cannot be used in the test phase. In this task, the recognition of the letter and number of the target speaker is the goal and the evaluation is done through the scoring scripts provided by the organizers."}, {"heading": "4.1. Source Models, Grammar and Lexicon", "text": "The models are first initialized by the TIMIT dataset (Garofolo et al., 1993), then re-evaluated by training the speech expressions of 34 Challenge Corpus speakers; the number of mixing components is increased from 8 to 32 for each monophonic state; the models are then adjusted for each speaker and the number of mixing components is reduced to 8 and 4 for the experiments; the four component mixing components are used for selecting the characteristics and adjusting the phase factors; the eight component models are used for the main experiments; the MFCC features are used for acoustic modeling; and the feature extraction is done through the voicebox toolbox (Brookes, 1997); the feature extraction is done using 27-scale filters and framing is done by 10ms image shifting and 25ms frame length; the number of 13 to 23-MFCs accelerations are made efficient (EC1)."}, {"heading": "4.2. Feature selection and phase factor determination", "text": "Before starting the main experiments, the selection of suitable characteristics for this task and the adjustment of the phase factor in the mismatch function of (9) for performing the acoustic inference must be made. In this step, MFCC characteristics and their derivatives of first and second order are selected as the characteristic type. Now, various static phase factors are selected for the test on the development list. Configuration of the feature extraction is similar to the task foundation system except that the MFCC coefficient 0th is used in characteristics instead of the logarithm of the image energy and 27 filters are used in the filter bank of the Mel scale. In this step, no speaker detection and adjustment of the loudspeakers is performed and the identity of the loudspeakers is considered known during the tests. As no amplification adjustment is performed in this step, only the performance of the system in almost zero TMRs for phase factor selection (alpha) is taken into account."}, {"heading": "4.3. Speaker identification results", "text": "The network consists of 5 layers, including 4 hidden layers (each layer has 2500 logistic sigmoid neurons) and an output layer consisting of 34 neurons to identify common speakers; the network is fed with a concatenation of 21 high-resolution power spectral characteristics (= 10), which are extracted by installing a dense Mel scale filter bank on the frame power spectrum (110 filters are included in the filter bank); pre-training of the network is done in layers using RBMs and then refined by the DeeBNet toolbox with the speaker labels (Keyvanrad and Homayounpour, 2014); approximately 7000 training expressions are synthesized by using target and mask loudspeaker expressions extracted from the training data in various TMR values similar to the test data; the associated speaker labels are associated with the training data for the network."}, {"heading": "4.4. Joint-decoding by gain adapted models, pushing forward past super-human results", "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and in which it is a country."}, {"heading": "5. Conclusion", "text": "This paper presents factorial speech processing models through the language of probabilistic graphical models, describes the probability distributions of these models, in particular the detailed derivation of their centric CPD, the CPD, which combines the source audio characteristics, and derives the inference algorithm via these models using factor graphs. In this paper, the idea of token transfer is expanded to support the decoding of the task of monaural speech separation and recognition. Therefore, a common token transmission algorithm based on the idea of token transfer is developed to perform a common decoding of the factorial speech processing models using the two-dimensional Viterbi algorithm. Furthermore, a set of specific deep neural networks is proposed and used for common loudspeaker identification in this paper."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Mr. Mohammad Ali Keyvanrad for his valuable arguments and support of the DeeBNet toolbox in this work."}], "references": [{"title": "Developments and directions in speech recognition and understanding, Part 1 [DSP Education", "author": ["J. Baker", "L. Deng", "J. Glass", "S. Khudanpur", "C. Lee", "N. Morgan", "D. O\u2019Shaughnessy"], "venue": "IEEE Signal Process. Mag", "citeRegEx": "Baker et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Baker et al\\.", "year": 2009}, {"title": "Voicebox: Speech Processing Toolbox for MATLAB. Imperial College, Exhibition Road, London", "author": ["M. Brookes"], "venue": null, "citeRegEx": "Brookes,? \\Q1997\\E", "shortCiteRegEx": "Brookes", "year": 1997}, {"title": "An audio-visual corpus for speech perception and automatic speech recognition", "author": ["M. Cooke", "J. Barker", "S. Cunningham", "X. Shao"], "venue": "J. Acoust. Soc. Am", "citeRegEx": "Cooke et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Cooke et al\\.", "year": 2006}, {"title": "Monaural speech separation and recognition challenge", "author": ["M. Cooke", "J.R. Hershey", "S.J. Rennie"], "venue": "Comput. Speech Lang", "citeRegEx": "Cooke et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Cooke et al\\.", "year": 2010}, {"title": "Robust continuous speech recognition using parallel model combination", "author": ["M.J.F. Gales", "S.J. Young"], "venue": "IEEE Trans. Speech Audio Process", "citeRegEx": "Gales and Young,? \\Q1996\\E", "shortCiteRegEx": "Gales and Young", "year": 1996}, {"title": "The Cocktail Party Problem", "author": ["S. Haykin", "Z. Chen"], "venue": "Neural Comput", "citeRegEx": "Haykin and Chen,? \\Q2005\\E", "shortCiteRegEx": "Haykin and Chen", "year": 2005}, {"title": "Factorial Models for Noise", "author": ["J.R. Hershey", "S.J. Rennie", "J. Le Roux"], "venue": "Robust Speech Recognition,", "citeRegEx": "Hershey et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2012}, {"title": "Super-human multi-talker speech recognition: A graphical modeling approach", "author": ["J.R. Hershey", "S.J. Rennie", "P.A. Olsen", "T.T. Kristjansson"], "venue": "Comput. Speech Lang", "citeRegEx": "Hershey et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Hershey et al\\.", "year": 2010}, {"title": "A brief survey on deep belief networks and introducing a new object oriented toolbox (DeeBNet)", "author": ["M.A. Keyvanrad", "M.M. Homayounpour"], "venue": null, "citeRegEx": "Keyvanrad and Homayounpour,? \\Q2014\\E", "shortCiteRegEx": "Keyvanrad and Homayounpour", "year": 2014}, {"title": "Probabilistic graphical models: principles and techniques", "author": ["D. Koller", "N. Friedman"], "venue": null, "citeRegEx": "Koller and Friedman,? \\Q2009\\E", "shortCiteRegEx": "Koller and Friedman", "year": 2009}, {"title": "An analytic derivation of a phase-sensitive observation model for noise robust speech recognition, in: Interspeech", "author": ["V. Leutnant", "R. Haeb-Umbach"], "venue": null, "citeRegEx": "Leutnant and Haeb.Umbach,? \\Q2009\\E", "shortCiteRegEx": "Leutnant and Haeb.Umbach", "year": 2009}, {"title": "An Overview of Noise-Robust Automatic Speech Recognition", "author": ["J. Li", "L. Deng", "Y. Gong", "R. Haeb-Umbach"], "venue": "IEEEACM Trans. Audio Speech Lang. Process", "citeRegEx": "Li et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Li et al\\.", "year": 2014}, {"title": "A unified framework of HMM adaptation with joint compensation of additive and convolutive distortions", "author": ["J. Li", "L. Deng", "D. Yu", "Y. Gong", "A. Acero"], "venue": "Comput. Speech Lang", "citeRegEx": "Li et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Li et al\\.", "year": 2009}, {"title": "A vector Taylor series approach for environmentindependent speech recognition", "author": ["P.J. Moreno", "B. Raj", "R.M. Stern"], "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing,", "citeRegEx": "Moreno et al\\.,? \\Q1996\\E", "shortCiteRegEx": "Moreno et al\\.", "year": 1996}, {"title": "Dynamic bayesian networks: representation, inference and learning (Ph.D. Thesis)", "author": ["K.P. Murphy"], "venue": "University of California", "citeRegEx": "Murphy,? \\Q2002\\E", "shortCiteRegEx": "Murphy", "year": 2002}, {"title": "Single-Channel Multitalker Speech Recognition", "author": ["S.J. Rennie", "J.R. Hershey", "P.A. Olsen"], "venue": "IEEE Signal Process. Mag. 27,", "citeRegEx": "Rennie et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rennie et al\\.", "year": 2010}, {"title": "BEEP dictionary [WWW Document", "author": ["T. Robinson"], "venue": null, "citeRegEx": "Robinson,? \\Q1997\\E", "shortCiteRegEx": "Robinson", "year": 1997}, {"title": "Factorial models and refiltering for speech separation and denoising", "author": ["S.T. Roweis"], "venue": "in: Eighth European Conference on Speech Communication and Technology", "citeRegEx": "Roweis,? \\Q2003\\E", "shortCiteRegEx": "Roweis", "year": 2003}, {"title": "Statistical models for noise-robust speech recognition (Ph.D. Thesis)", "author": ["R.C. Van Dalen"], "venue": null, "citeRegEx": "Dalen,? \\Q2011\\E", "shortCiteRegEx": "Dalen", "year": 2011}, {"title": "Deep Neural Networks for Single-Channel Multi-Talker Speech Recognition", "author": ["C. Weng", "D. Yu", "M.L. Seltzer", "J. Droppo"], "venue": "IEEEACM Trans. Audio Speech Lang. Process", "citeRegEx": "Weng et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Weng et al\\.", "year": 2015}, {"title": "A review of large-vocabulary continuous-speech", "author": ["S. Young"], "venue": "IEEE Signal Process. Mag. 13,", "citeRegEx": "Young,? \\Q1996\\E", "shortCiteRegEx": "Young", "year": 1996}], "referenceMentions": [{"referenceID": 0, "context": "Robustness of automatic speech recognition systems (ASR) against diverse speech processing environments and adverse disturbing noises still remains as one of the important research areas in speech recognition systems (Baker et al., 2009; Li et al., 2014).", "startOffset": 217, "endOffset": 254}, {"referenceID": 11, "context": "Robustness of automatic speech recognition systems (ASR) against diverse speech processing environments and adverse disturbing noises still remains as one of the important research areas in speech recognition systems (Baker et al., 2009; Li et al., 2014).", "startOffset": 217, "endOffset": 254}, {"referenceID": 5, "context": "This problem is known as the cocktail party problem (Haykin and Chen, 2005) in which a person (or a system) wants to focus and follow the conversation of a speaker in a place where some people talk simultaneously.", "startOffset": 52, "endOffset": 75}, {"referenceID": 3, "context": "An interesting competition entitled \u201cMonaural speech separation and recognition challenge\u201d, addressing the challenges related to the second group of approaches was developed in 2006 (Cooke et al., 2010).", "startOffset": 182, "endOffset": 202}, {"referenceID": 3, "context": "Several teams attended this competition with different techniques for handling the problem (Cooke et al., 2010).", "startOffset": 91, "endOffset": 111}, {"referenceID": 7, "context": "Among the competitors, surprisingly, a team from IBM research presented a technique that outperforms the other techniques and even human listeners (Hershey et al., 2010).", "startOffset": 147, "endOffset": 169}, {"referenceID": 7, "context": "In the third step, two separated speeches are decoded using a single-talker recognition system (Hershey et al., 2010).", "startOffset": 95, "endOffset": 117}, {"referenceID": 19, "context": "This work incorporates a pair of Deep Neural Networks for acoustic inference over semi-joint hidden Markov models (HMM) (Weng et al., 2015); a network for the generation of senone posteriors of high energy utterances (or instantaneous high energy frames) and a network for low energy utterances.", "startOffset": 120, "endOffset": 139}, {"referenceID": 3, "context": "The objective of the monaural speech separation and recognition challenge is to recognize some keywords of a target speaker from a mixed-speech of the target and a masker speaker (Cooke et al., 2010).", "startOffset": 179, "endOffset": 199}, {"referenceID": 2, "context": "Mixed-speech signals of this task are artificially created from speech materials of the Grid corpus (Cooke et al., 2006).", "startOffset": 100, "endOffset": 120}, {"referenceID": 6, "context": "It is applicable for robust-ASR systems (Hershey et al., 2012) and is tailored exactly for the challenge of this paper.", "startOffset": 40, "endOffset": 62}, {"referenceID": 9, "context": "This model is based on factorial hidden Markov models which are used for modeling processes with multiple independent underlying Markov chains (Koller and Friedman, 2009).", "startOffset": 143, "endOffset": 170}, {"referenceID": 20, "context": "Each chain of a factorial speech processing model contains an HMM for modeling its audio source which is known as acoustic modeling (Young, 1996) in conventional speech recognition applications.", "startOffset": 132, "endOffset": 145}, {"referenceID": 9, "context": "By the acoustic interaction function we can infer the posterior distribution of source features and then the posterior distribution of source states in an \u201cevidential reasoning\u201d pattern (Koller and Friedman, 2009).", "startOffset": 186, "endOffset": 213}, {"referenceID": 6, "context": "This form of marginalization is seen before in noise-robust automatic speech recognition when one audio source is clean speech and the other is the disturbing noise (Hershey et al., 2012).", "startOffset": 165, "endOffset": 187}, {"referenceID": 17, "context": "For example, for high resolution power spectral features, the max-model (Roweis, 2003) is used for marginalizing-out source feature vectors.", "startOffset": 72, "endOffset": 86}, {"referenceID": 4, "context": "By using an appropriate mismatch function (Gales and Young, 1996) which combines source features into the observed feature vector, we can approximate each state conditional distribution by one Gaussian.", "startOffset": 42, "endOffset": 65}, {"referenceID": 13, "context": "This technique is known as approximation by the vector Taylor series (Moreno et al., 1996), VTS, which will be discussed later.", "startOffset": 69, "endOffset": 90}, {"referenceID": 4, "context": "In the terminology of noise-robust ASR, this expression is called \u201cmismatch function\u201d when one of the source signals is the disturbing noise (Gales and Young, 1996).", "startOffset": 141, "endOffset": 164}, {"referenceID": 10, "context": "Moreover \u03b1 can be considered independent from source feature vectors by an additional simplifying assumption (Leutnant and Haeb-Umbach, 2009).", "startOffset": 109, "endOffset": 141}, {"referenceID": 12, "context": "It is traditionally ignored in many applications (Van Dalen, 2011) or considered as a constant for all frequency bins (Li et al., 2009).", "startOffset": 118, "endOffset": 135}, {"referenceID": 12, "context": "10 For a detailed derivation, the reader is referred to the appendix of (Li et al., 2009).", "startOffset": 72, "endOffset": 89}, {"referenceID": 7, "context": "This algorithm which is derived here using general inference procedures of probabilistic graphical models is called the two-dimensional Viterbi algorithm (Hershey et al., 2010).", "startOffset": 154, "endOffset": 176}, {"referenceID": 9, "context": "For more details about clique tree construction and general inference over the graphical models, the reader can refer to (Koller and Friedman, 2009; Murphy, 2002).", "startOffset": 121, "endOffset": 162}, {"referenceID": 14, "context": "For more details about clique tree construction and general inference over the graphical models, the reader can refer to (Koller and Friedman, 2009; Murphy, 2002).", "startOffset": 121, "endOffset": 162}, {"referenceID": 20, "context": "It is a conceptual framework for decoding in large vocabulary continuous speech recognition tasks (Young, 1996; Young et al., 2009).", "startOffset": 98, "endOffset": 131}, {"referenceID": 2, "context": "The monaural speech separation and recognition challenge is designed based on the GRID dataset (Cooke et al., 2006).", "startOffset": 95, "endOffset": 115}, {"referenceID": 1, "context": "MFCC features are used for acoustic modeling and feature extraction is done by the Voicebox toolbox (Brookes, 1997).", "startOffset": 100, "endOffset": 115}, {"referenceID": 16, "context": "We use the BEEP dictionary (Robinson, 1997) for the phonetic transcription of 51 different words of the challenge (see Fig.", "startOffset": 27, "endOffset": 43}, {"referenceID": 3, "context": "Additionally, the word lattice network is created based on the task grammar which is already provided in (Cooke et al., 2010) and also presented in Fig.", "startOffset": 105, "endOffset": 125}, {"referenceID": 12, "context": "Alpha candidates are selected almost similar to the work of (Li et al., 2009) and the selected alpha is 2.", "startOffset": 60, "endOffset": 77}, {"referenceID": 12, "context": "This alpha value is not valid regarding its support set (the reader is referred to (Li et al., 2009) or (Van Dalen, 2011) for explanations of this theoretical contradiction).", "startOffset": 83, "endOffset": 100}, {"referenceID": 8, "context": "Pre-training of the network is done layer-wise by RBMs and then it is fined-tuned with the speaker labels by the DeeBNet toolbox (Keyvanrad and Homayounpour, 2014).", "startOffset": 129, "endOffset": 163}, {"referenceID": 7, "context": "The first super-human system was the IBM system (Hershey et al., 2010; Rennie et al., 2010) and the second one was the recent Microsoft DNN based recognizer (Weng et al.", "startOffset": 48, "endOffset": 91}, {"referenceID": 15, "context": "The first super-human system was the IBM system (Hershey et al., 2010; Rennie et al., 2010) and the second one was the recent Microsoft DNN based recognizer (Weng et al.", "startOffset": 48, "endOffset": 91}, {"referenceID": 19, "context": ", 2010) and the second one was the recent Microsoft DNN based recognizer (Weng et al., 2015).", "startOffset": 73, "endOffset": 92}], "year": 2016, "abstractText": "A Pascal challenge entitled monaural multi-talker speech recognition was developed, targeting the problem of robust automatic speech recognition against speech like noises which significantly degrades the performance of automatic speech recognition systems. In this challenge, two competing speakers say a simple command simultaneously and the objective is to recognize speech of the target speaker. Surprisingly during the challenge, a team from IBM research, could achieve a performance better than human listeners on this task. The proposed method of the IBM team, consist of an intermediate speech separation and then a single-talker speech recognition. This paper reconsiders the task of this challenge based on gain adapted factorial speech processing models. It develops a joint-token passing algorithm for direct utterance decoding of both target and masker speakers, simultaneously. Comparing it to the challenge winner, it uses maximum uncertainty during the decoding which cannot be used in the past two-phased method. It provides detailed derivation of inference on these models based on general inference procedures of probabilistic graphical models. As another improvement, it uses deep neural networks for joint-speaker identification and gain estimation which makes these two steps easier than before producing competitive results for these steps. The proposed method of this work outperforms past superhuman results and even the results were achieved recently by Microsoft research, using deep neural networks. It achieved 5.5% absolute task performance improvement compared to the first superhuman system and 2.7% absolute task performance improvement compared to its recent competitor.", "creator": "Microsoft\u00ae Word 2013"}}}