{"id": "1610.01382", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC and Random Forest", "abstract": "Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches.", "histories": [["v1", "Wed, 5 Oct 2016 12:16:35 GMT  (516kb)", "http://arxiv.org/abs/1610.01382v1", "8 pages, conference paper, The 2nd International Integrated Conference &amp; Concert on Convergence (2016)"]], "COMMENTS": "8 pages, conference paper, The 2nd International Integrated Conference &amp; Concert on Convergence (2016)", "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["abdul malik badshah", "jamil ahmad", "mi young lee", "sung wook baik"], "accepted": false, "id": "1610.01382"}, "pdf": {"name": "1610.01382.pdf", "metadata": {"source": "CRF", "title": "Divide-and-Conquer based Ensemble to Spot Emotions in Speech using MFCC and Random Forest", "authors": ["Abdul Malik Badshah", "Jamil Ahmad", "Mi Young Lee", "Sung Wook Baik"], "emails": ["sbaik@sejong.ac.kr"], "sections": [{"heading": null, "text": "In addition to spoken words, speech signals also contain information about the gender, age and emotional state of the speaker, which can be used in a variety of speech analysis applications. In this paper, a strategy of sharing and mastering was proposed to identify emotions in language. Intrinsic hierarchy of emotions was used to construct an emotion tree that helped divide the task of emotion recognition into smaller subtasks. Finally, the proposed framework generates predictions in three phases. First, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the language is classified as emotional, it is further classified into positive and negative classes in the second phase. Finally, individual positive or negative emotions are identified based on the results of the previous phases. Several experiments were conducted using a widely used benchmark dataset. Compared to several other approaches, the proposed method achieved improved recognition rates. Keywords: language emotion, division and erosion, random classification, forest."}, {"heading": "1. Introduction", "text": "Body positions, facial expressions and language are the most common ways of expressing emotions [1]. However, certain emotions such as disgust and boredom cannot be identified by gestures or facial expressions, but can be identified by differences in energy, speech rate, linguistic and semantic information [2]. Speech emotion recognition (SER) has attracted increasing attention in recent decades due to the increasing use of emotions in affective computer systems and human computer interaction. It has played a huge role in changing the way we interact with computers [3]. Affective computer techniques use emotions to interact in a more natural way. Likewise, automatic speech response systems (AVR) can become more adaptable and user-friendly when affective states of users could be identified during interactions."}, {"heading": "2. Related Work", "text": "Some of the most recent work in this area is presented here.Vogt and Andre [8] proposed a gender-based emotion recognition model for recognizing emotions from language. They used 20 different traits, consisting of 17 MFCC, 2 energy coefficients and 1 pitch value for non-gender and 30 features, consisting of 22 MFCC, 3 energy and 5 pitch values for male and female emotions, to train naive Bayes classifiers. They evaluated their framework on two different data sets, including Berlin and SmartKom. They achieved an improvement in overall recognition rates for Berliner datasets and 4% for SmartKom datasets. Lalitha et al. [9] developed a system for recognizing emotions through loudspeakers to detect different emotions with a generalized range of functionality. They used and compared the results of two different classifiers, including continuous hidden Markov models (HMM) and supported emotion recognition rates, 33% of which were not suggested."}, {"heading": "3. Proposed Method", "text": "This section presents the overall architecture of the proposed framework, explains the method of feature extraction and highlights the most important design aspects of the classification scheme."}, {"heading": "3.1 Feature Extraction", "text": "The proposed method uses acoustic characteristics to detect emotions. Mel Frequency Cepstral Coefficients (MFCCs) are robust to noise, have significant representational capabilities and perform best with multiple speech analysis applications. Speech signals are divided into small frames of several milliseconds and 16 MFCC coefficients. These characteristics are then used to train different classifiers for emotion modeling. MFCC characteristics are well known and readers are referred to [13, 14] for more details. The proposed method uses DC rule to detect and classify seven different types of emotions in speech. The emotion recognition system is divided into three stages. The first stage detects whether the sound signal contains any emotions or not (neutral speech). The second stage classifies emotional signals into positive and negative emotions."}, {"heading": "4. Experiments and Results", "text": "This section presents the results of various experiments carried out with the language data set for emotion recognition and highlights the main strengths and weaknesses of the evaluation process."}, {"heading": "4.1 Database Description", "text": "The SAVEE database was recorded by four native English speakers at the University of Surrey. It consists of 7 different types of emotions, namely anger, disgust, fear, happiness, sadness, surprise and neutrality.The sound files vary from 2 to 7 seconds depending on the speaker's sentence. A total of 120 expressions per loudspeaker.The experiment was recorded on a desktop PC with Intel Core i7-3770 CPU @ 3.40 GHz, 16.0 GB RAM and 64-bit Windows 7 Professional. Feature extraction and classification modules were implemented using C + +. Experimental results analysis was conducted in Matlab 2015a. Essentially, two types of experiments were conducted: The first experiment was conducted to compare the performance of the DC-based approach and the single multiclass classification module."}, {"heading": "4.3 Stage-wise Classification Performance", "text": "Fig. 2 shows the classification performance of DT, SVM and RF. All these schemes achieved near perfect recognition rates for neutral language. However, the recognition rate of emotional language identification with DT and SVM was relatively low, with emotions reaching 81.11% and 80% accuracy respectively. RF classifier achieved an accuracy of 92.2% for the identification of emotional language. The overall accuracy achieved by RF was highest at this stage at 96.11%. In Level 2, classifiers are rated to identify emotional language from the previous phase as negative or positive. Accuracy rates are shown in Figure 3. SVM classifier has the lowest accuracy among classifiers used for both negative and positive emotions. Achieving accuracy by 70% and 76.67%. Decision Tree achieved better performance in identifying positive emotions."}, {"heading": "4.4 Comparison with other SER approaches", "text": "The results indicate that the proposed classification approach outperforms the other methods for identifying anger, disgust, neutral and sad emotions. In the case of anxiety, the proposed method performed better than [11], but was lower than the method described in [9]. Likewise, our method performed significantly better than [9] and slightly less accurately than [11] in the case of happy emotions. Overall, the proposed method outperformed both methods [9] and [11] by 23% and 17%, respectively."}, {"heading": "5. Conclusion", "text": "In this paper, we presented an approach to the recognition and classification of emotions from speech signals. MFCC characteristics are extracted from the speech signals used to train classifiers in three different stages of the proposed scheme. In the first stage, generic emotions are detected in language by using a simple binary classifier that separates neutral language from emotional language. In the second stage, emotions are further identified as positive or negative. Finally, individual emotions are identified in the final stage of the classification scheme. Three different classification schemes, including DT, SVM and RF-based emotion recognition scheme, achieve a high level of accuracy with RF classifier at each stage. In the future, we plan to incorporate the Dempster-shafer theory of fusion in order to combine the decisions of intermediate classifiers before a final decision.We hope that this fusion will help to further improve accuracy."}, {"heading": "Acknowledgements", "text": "a solution for situational awareness based on the analysis of language and ambient sounds)."}], "references": [{"title": "Facial and vocal expressions of emotion", "author": ["J.A. Russell", "J.-A. Bachorowski", "J.-M. Fernandez-Dols"], "venue": "Annual review of psychology, vol. 54, pp. 329-349, 2003.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2003}, {"title": "Emotion-specific effects of facial expressions and postures on emotional experience", "author": ["S.E. Duclos", "J.D. Laird", "E. Schneider", "M. Sexter", "L. Stern", "O. Van Lighten"], "venue": "Journal of Personality and Social Psychology, vol. 57, p. 100, 1989.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1989}, {"title": "Emotion recognition in human-computer interaction", "author": ["R. Cowie", "E. Douglas-Cowie", "N. Tsapatsoulis", "G. Votsis", "S. Kollias", "W. Fellenz"], "venue": "Signal Processing Magazine, IEEE, vol. 18, pp. 32-80, 2001.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2001}, {"title": "Dempster-Shafer Fusion Based Gender Recognition for Speech Analysis Applications", "author": ["J. Ahmad", "K. Muhammad", "S. i. Kwon", "S.W. Baik", "S. Rho"], "venue": "2016 International Conference on Platform Technology and Service (PlatCon), 2016, pp. 1-4.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "The nature of statistical learning theory: Springer", "author": ["V. Vapnik"], "venue": "Science & Business Media,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2013}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, vol. 45, pp. 5-32, 2001.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2001}, {"title": "Emotion recognition using a hierarchical binary decision tree approach", "author": ["C.-C. Lee", "E. Mower", "C. Busso", "S. Lee", "S. Narayanan"], "venue": "Speech Communication, vol. 53, pp. 1162-1171, 2011.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2011}, {"title": "Improving automatic emotion recognition from speech via gender differentiation", "author": ["T. Vogt", "E. Andr\u00e9"], "venue": "Proc. Language Resources and Evaluation Conference (LREC 2006), Genoa, 2006.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2006}, {"title": "Emotion Recognition through Speech Signal for Human-Computer Interaction", "author": ["S. Lalitha", "S. Patnaik", "T. Arvind", "V. Madhusudhan", "S. Tripathi"], "venue": "Electronic System Design (ISED), 2014 Fifth International Symposium on, 2014, pp. 217-218.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Speaker independent automatic emotion recognition from speech: a comparison of MFCCs and discrete  8  A.M. Badshah, J Ahmad, MY Lee, S.W. Baik wavelet transforms", "author": ["A. Firoz Shah", "V. Vimal Krishnan", "A. Raji Sukumar", "A. Jayakumar", "P. Babu Anto"], "venue": "Advances in Recent Technologies in Communication and Computing, 2009. ARTCom'09. International Conference on, 2009, pp. 528-531.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "Emotion recognition in speech using MFCC and wavelet features", "author": ["K. Krishna Kishore", "P. Krishna Satish"], "venue": "Advance Computing Conference (IACC), 2013 IEEE 3rd International, 2013, pp. 842- 847.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Survey on speech emotion recognition: Features, classification schemes, and databases", "author": ["M. El Ayadi", "M.S. Kamel", "F. Karray"], "venue": "Pattern Recognition, vol. 44, pp. 572-587, 2011.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2011}, {"title": "Comparison of different implementations of MFCC", "author": ["F. Zheng", "G. Zhang", "Z. Song"], "venue": "Journal of Computer Science and Technology, vol. 16, pp. 582-589, 2001.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Gender Identification using MFCC for Telephone Applications-A Comparative Study", "author": ["J. Ahmad", "M. Fiaz", "S.-i. Kwon", "M. Sodanil", "B. Vo", "S.W. Baik"], "venue": "arXiv preprint arXiv:1601.01577, 2016.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "Body poses, facial expressions, and speech are the most common ways to express emotions [1].", "startOffset": 88, "endOffset": 91}, {"referenceID": 1, "context": "However, certain emotions like disgust and boredom cannot be identified from gestures or facial expressions but could be effectively recognized from speech tone due to the difference in energy, speaking rate, speech linguistic and semantic information [2].", "startOffset": 252, "endOffset": 255}, {"referenceID": 2, "context": "It has played a tremendous role in changing the way we interact with computers over the last few years [3].", "startOffset": 103, "endOffset": 106}, {"referenceID": 3, "context": "Information regarding the emotions of a person can help in several speech analysis applications such as human computer interaction, humanoid robots, mobile communication, and call centers [4].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "Three classifiers namely support vector machine (SVM) [5], Random Forest (RF) [6] and Decision Tree (DT) [7] were evaluated.", "startOffset": 54, "endOffset": 57}, {"referenceID": 5, "context": "Three classifiers namely support vector machine (SVM) [5], Random Forest (RF) [6] and Decision Tree (DT) [7] were evaluated.", "startOffset": 78, "endOffset": 81}, {"referenceID": 6, "context": "Three classifiers namely support vector machine (SVM) [5], Random Forest (RF) [6] and Decision Tree (DT) [7] were evaluated.", "startOffset": 105, "endOffset": 108}, {"referenceID": 7, "context": "Vogt and Andre [8] proposed a gender-dependent emotion recognition model for emotion detection from speech.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "[9] developed a speaker emotion recognition system to recognize different emotions with a generalized feature set.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10] conducted an experiment using Discrete Wavelet Transforms (DWTs) and Mel Frequency Cepstral Coefficients (MFCCs) for speaker independent automatic emotion recognition from speech.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] also used MFCC and wavelet features.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Implementing SER for real world applications is a very challenging task which require careful design strategies [12].", "startOffset": 112, "endOffset": 116}, {"referenceID": 12, "context": "MFCC features are well known and the readers are referred to [13, 14] for more details.", "startOffset": 61, "endOffset": 69}, {"referenceID": 13, "context": "MFCC features are well known and the readers are referred to [13, 14] for more details.", "startOffset": 61, "endOffset": 69}, {"referenceID": 10, "context": "In case of fear, the proposed method performed better than [11], however, it was lower than the method described in [9].", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "In case of fear, the proposed method performed better than [11], however, it was lower than the method described in [9].", "startOffset": 116, "endOffset": 119}, {"referenceID": 8, "context": "Similarly, in case of happy emotion, our method achieved significantly better performance than [9] and slightly lower accuracy than [11].", "startOffset": 95, "endOffset": 98}, {"referenceID": 10, "context": "Similarly, in case of happy emotion, our method achieved significantly better performance than [9] and slightly lower accuracy than [11].", "startOffset": 132, "endOffset": 136}, {"referenceID": 8, "context": "Overall, the proposed method outperformed both methods [9] and [11] by 23% and 17%, respectively.", "startOffset": 55, "endOffset": 58}, {"referenceID": 10, "context": "Overall, the proposed method outperformed both methods [9] and [11] by 23% and 17%, respectively.", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "Emotions Emotion Recognition Accuracy Proposed Method Method [9] Method [11] Anger 100 91 90 Boredom 50 Disgust 73 23.", "startOffset": 61, "endOffset": 64}, {"referenceID": 10, "context": "Emotions Emotion Recognition Accuracy Proposed Method Method [9] Method [11] Anger 100 91 90 Boredom 50 Disgust 73 23.", "startOffset": 72, "endOffset": 76}], "year": 2016, "abstractText": "Besides spoken words, speech signals also carry information about speaker gender, age, and emotional state which can be used in a variety of speech analysis applications. In this paper, a divide and conquer strategy for ensemble classification has been proposed to recognize emotions in speech. Intrinsic hierarchy in emotions has been utilized to construct an emotions tree, which assisted in breaking down the emotion recognition task into smaller sub tasks. The proposed framework generates predictions in three phases. Firstly, emotions are detected in the input speech signal by classifying it as neutral or emotional. If the speech is classified as emotional, then in the second phase, it is further classified into positive and negative classes. Finally, individual positive or negative emotions are identified based on the outcomes of the previous stages. Several experiments have been performed on a widely used benchmark dataset. The proposed method was able to achieve improved recognition rates as compared to several other approaches.", "creator": "Microsoft\u00ae Word 2013"}}}