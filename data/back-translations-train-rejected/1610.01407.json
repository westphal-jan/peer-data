{"id": "1610.01407", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Towards semi-episodic learning for robot damage recovery", "abstract": "The recently introduced Intelligent Trial and Error algorithm (IT\\&amp;E) enables robots to creatively adapt to damage in a matter of minutes by combining an off-line evolutionary algorithm and an on-line learning algorithm based on Bayesian Optimization. We extend the IT\\&amp;E algorithm to allow for robots to learn to compensate for damages while executing their task(s). This leads to a semi-episodic learning scheme that increases the robot's lifetime autonomy and adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot locomotion task show promising results.", "histories": [["v1", "Wed, 5 Oct 2016 13:21:43 GMT  (136kb)", "http://arxiv.org/abs/1610.01407v1", "Workshop on AI for Long-Term Autonomy at the IEEE International Conference on Robotics and Automation (ICRA), May 2016, Stockholm, Sweden. 2016"]], "COMMENTS": "Workshop on AI for Long-Term Autonomy at the IEEE International Conference on Robotics and Automation (ICRA), May 2016, Stockholm, Sweden. 2016", "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.NE", "authors": ["konstantinos chatzilygeroudis", "antoine cully", "jean-baptiste mouret"], "accepted": false, "id": "1610.01407"}, "pdf": {"name": "1610.01407.pdf", "metadata": {"source": "CRF", "title": "Towards semi-episodic learning for robot damage recovery", "authors": ["Konstantinos Chatzilygeroudis", "Antoine Cully", "Jean-Baptiste Mouret"], "emails": ["jean-baptiste.mouret@inria.fr"], "sections": [{"heading": null, "text": "Most of the techniques in the first category require an anticipation of the situations that the robot must face; a problem can only be diagnosed if the right sensors are present, which are complex diagnostic systems. [cs.R O] 5O ct2 016I behaviors make the requirements more complex. INTRODUCTIONCurrent research on autonomous systems and robots have made important progress in increasing the autonomy of robots, which make it possible to operate robots for long periods of time in real scenarios. Nevertheless, they must face the inevitable fact that they will be damaged [3]. Current methods for restoring robot damage can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods most learning methods - Reinforcement Learning (RL) techniques [6], [7], [8] techniques in the first category require an anticipation of situations that the robot must face; a problem must be diagnosed only if sensors are present."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Bayesian Optimization with Gaussian Processes", "text": "Bayesian Optimization (BO) is a well-established strategy for finding the extreme of functions that are expensive to evaluate [11], [13]. It is applicable in cases where one does not have a closed-form expression for the objective function (the function is a \"black box\"), but where one can obtain observations (possibly disturbing) of this function. One of the characteristics of BO is that it constructs a probabilistic model for the objective function and then uses this model to make decisions about which point to evaluate next, while taking into account the uncertainty. There are two important decisions that need to be made when executing BO. First, one needs to select a previous hyperfunction that expresses assumptions about the optimized function. Second, one needs to choose an acquisition function (u x | D1: t) that is used to construct a supply function from the posterior model, which allows us to select the next point for evaluating the G.sian models (but many may be used for G.P. models)."}, {"heading": "III. APPROACH", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Generic Reward", "text": "In the original work of IT & E, the GP modeled the performance of each atomic behavior in the face of a task. In this work, we propose to learn a mapping of the behavior of the atoms to the resulting relative results. We call it a Generic Reward (GR) of the behavior of each robot. We use a GP for each dimension of the GR. For example, let's imagine we have a robot that moves in 2D space by using a continuous atomic behavior (direction to 0.1 step). A GR could be the relative position of the robot after performing a behavior - (x, y). So we need 2 GPs: GPx (\u03b8), GPy (\u03b8). If we query the GPs at the point \u03b80, we get a position, p1 = (GPx (\u04450), GPy (\u04450))), as a prediction."}, {"heading": "B. Specialized Reward Selection Layer", "text": "We also extend the proposed algorithm by adding a layer that is responsible for selecting the reward function defined above. We call it Specialized Reward Selection Layer (RSL). Because we model the result of each robotic behavior, not the actual performance (given task), we can change the reward function as often as necessary. That's true, because only the capture function requires an actual reward to select a new test point. We propose to update or select the reward function for each iteration of M-BOA. Consider, for example, the previous example of a mobile robot, a planner algorithm selects the closest point to be reached for each iteration. This point can then be used by the RSL to update the reward function so that it displays the Euclidean distance between the point selected by the planner and the GPs \"prediction."}, {"heading": "C. Semi-Episodic Learning Algorithm", "text": "Using the two proposed additions, we can now have a non-purely episodic version of the IT & E algorithm, which we call Semi-Episodic Learning Algorithm (SELA). The pseudo code is shown in Algorithm 2.Algorithm 2 Semi-Episodic Learning Algorithm1: Procedure SELA 2: Prior to the mission (in simulation with intact robots): 3: Create a Behavioural Performance Card using MAP elites 4: during mission 5: if significant performance decreases then 6: Adaptation Step (using SELA-ADAPT) 7: Procedure SELA-ADAPT 8: Create a Behavioural Performance Card: 9: p (f (x) | x) = N (P (x), k (x, x)))) 10: Interrupt criteria that are not met, do 11: Update Reward Function 12: xt + 1 = argmaxxxu (Reward (GPs +: 1: Daft (Daft) (+ 1) (1: 1) (Daft: 1) (Daft) (+: 1) (13: 1) (Daft) (+: 1): 1 (1)."}, {"heading": "IV. PRELIMINARY EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Toy Simulation", "text": "As a toy example, we consider the previously introduced mobile robot example. This mobile robot is a point (no dimensions, no orientation) and can take a 0.1 long step in any direction. We represent each atomic behavior by a scalar value, \u03b8: the direction of the corresponding behavior. This environment was inspired by Engel et al. [15]. The task of the robot is to reach a target point despite some damage. As the example is too simple, but also to show the effectiveness of our method without relying on simulated data, we did not generate a behavioral performance map. We used the exact model of the intact robot as an intermediate function. We also used the (x, y) relative end position of each behavior for the GR, for the reverse function of the euclidean distance between the next target and the prediction of the GPs and for the reward selection layer. To evaluate our technique, we used the following control experts (we randomly learn the problems of the robot imaging):"}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "We have introduced a semi-episodic learning scheme for the restoration of robot damage and a novel algorithm in this direction: Semi-Episodic Learning Algorithm. The intuition behind this scheme is that the robot can learn in a data-efficient way how to compensate damage while performing its task (s) by (1) reducing the search space by using simulated or calculated data as prior knowledge, and (2) by providing a general reward for the result of the robot's atomic behavior instead of its performance in place of a given task. 1https: / / www.youtube.com / watch? v = Gpf5h07pJFAFfuture work involves conducting further experiments with the real robot as well as experiments with different robots. In addition, BO can be replaced by other techniques that scale better. In addition, we have used a naive reward selection layer, but more efficient / can be used."}, {"heading": "A. BO with GPs", "text": "Acquistion function: UCB with \u03b1 = 0.05 Kernel: Exponential kernel with \u03c3 = 0.1 GP noise: \u03c32Noise = 0.001 Maximum iterations: N = 10 (toys), N = 15 (hexapod)"}, {"heading": "B. Learning with random babbling", "text": "Error threshold: Area model = 0.01 Maximum iterations: N = 15"}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported by the ERC project \"ResiBots\" funded by the European Research Council (grant agreement No. 637972)."}], "references": [{"title": "Emergency response to the nuclear accident at the Fukushima Daiichi Nuclear Power Plants using mobile rescue robots", "author": ["K. Nagatani", "S. Kiribayashi", "Y. Okada", "K. Otake", "K. Yoshida", "S. Tadokoro", "T. Nishimura", "T. Yoshida", "E. Koyanagi", "M. Fukushima"], "venue": "Journal of Field Robotics, vol. 30, no. 1, pp. 44\u201363, 2013.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "On autonomous navigation in a natural environment", "author": ["M. Devy", "R. Chatila", "P. Fillatreau", "S. Lacroix", "F. Nashashibi"], "venue": "Robotics and Autonomous Systems, vol. 16, no. 1, pp. 5 \u2013 16, 1995.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 1995}, {"title": "Follow-up analysis of mobile robot failures", "author": ["J. Carlson", "R.R. Murphy", "A. Nelson"], "venue": "ICRA. IEEE, 2004.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2004}, {"title": "How UGVs physically fail in the field", "author": ["J. Carlson", "R.R. Murphy"], "venue": "IEEE Transactions on Robotics, vol. 21, no. 3, pp. 423\u2013437, 2005.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2005}, {"title": "Real-time fault diagnosis", "author": ["V. Verma", "G. Gordon", "R. Simmons", "S. Thrun"], "venue": "Robotics & Automation Magazine, IEEE, vol. 11, no. 2, pp. 56\u201366, 2004.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2004}, {"title": "Online discovery of AUV control policies to overcome thruster failures", "author": ["S.R. Ahmadzadeh", "M. Leonetti", "A. Carrera", "M. Carreras", "P. Kormushev", "D.G. Caldwell"], "venue": "ICRA. IEEE, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Free gait generation with reinforcement learning for a six-legged robot", "author": ["M.S. Erden", "K. Leblebicio\u011flu"], "venue": "Robotics and Autonomous Systems, vol. 56, no. 3, pp. 199\u2013212, 2008.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2008}, {"title": "Robots that can adapt like animals", "author": ["A. Cully", "J. Clune", "D. Tarapore", "J.-B. Mouret"], "venue": "Nature, vol. 521, no. 7553, pp. 503\u2013507, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "MIT press,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 1998}, {"title": "PILCO: A model-based and data-efficient approach to policy search", "author": ["M.P. Deisenroth", "C.E. Rasmussen"], "venue": "ICML, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "A tutorial on Bayesian optimization of expensive cost functions, with application to active user modeling and hierarchical reinforcement learning", "author": ["E. Brochu", "V.M. Cora", "N. De Freitas"], "venue": "arXiv preprint arXiv:1012.2599, 2010.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2010}, {"title": "Creative adaptation through learning", "author": ["A. Cully"], "venue": "Ph.D. dissertation, Universit\u00e9 Pierre et Marie Curie, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Bayesian approach to global optimization: theory and applications", "author": ["J. Mockus"], "venue": "Kluwer Academic,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2013}, {"title": "Illuminating search spaces by mapping elites", "author": ["J.-B. Mouret", "J. Clune"], "venue": "arXiv preprint arXiv:1504.04909, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning with Gaussian processes", "author": ["Y. Engel", "S. Mannor", "R. Meir"], "venue": "ICML. ACM, 2005.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2005}, {"title": "Behavioral repertoire learning in robotics", "author": ["A. Cully", "J.-B. Mouret"], "venue": "GECCO. ACM, 2013.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].", "startOffset": 94, "endOffset": 97}, {"referenceID": 1, "context": "Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].", "startOffset": 120, "endOffset": 123}, {"referenceID": 2, "context": "Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].", "startOffset": 257, "endOffset": 260}, {"referenceID": 3, "context": "Nevertheless, as robots move from controlled and well-structured environments to more complex [1] and more natural ones [2], they must be able to react to unforeseen situations; in particular, they have to face the inevitable fact that they will be damaged [3], [4].", "startOffset": 262, "endOffset": 265}, {"referenceID": 4, "context": "Current methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods \u2014 mostly Reinforcement Learning (RL) techniques [6], [7], [8].", "startOffset": 109, "endOffset": 112}, {"referenceID": 5, "context": "Current methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods \u2014 mostly Reinforcement Learning (RL) techniques [6], [7], [8].", "startOffset": 187, "endOffset": 190}, {"referenceID": 6, "context": "Current methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods \u2014 mostly Reinforcement Learning (RL) techniques [6], [7], [8].", "startOffset": 192, "endOffset": 195}, {"referenceID": 7, "context": "Current methods for robot damage recovery can be divided into two categories: (1) diagnosis-based approaches [5], and (2) learning methods \u2014 mostly Reinforcement Learning (RL) techniques [6], [7], [8].", "startOffset": 197, "endOffset": 200}, {"referenceID": 8, "context": "For example, many RL approaches require tens if not hundreds or thousands of iterations to learn problems with low-dimensional state spaces and fairly benign dynamics, like the mountain car [9].", "startOffset": 190, "endOffset": 193}, {"referenceID": 9, "context": "The data efficiency of RL approaches is a critical aspect that limits their application in real-world robotics scenarios [10].", "startOffset": 121, "endOffset": 125}, {"referenceID": 7, "context": "A promising approach is the Intelligent Trial and Error algorithm (IT&E), a recently introduced algorithm [8].", "startOffset": 106, "endOffset": 109}, {"referenceID": 10, "context": "Bayesian Optimization [11], to find a compensatory behavior.", "startOffset": 22, "endOffset": 26}, {"referenceID": 7, "context": "The most recent results showed that IT&E can allow various types of robots (a 6legged robot and an 8-DOF manipulator) to compensate for many different types of injuries in a matter of minutes [8], [12].", "startOffset": 192, "endOffset": 195}, {"referenceID": 11, "context": "The most recent results showed that IT&E can allow various types of robots (a 6legged robot and an 8-DOF manipulator) to compensate for many different types of injuries in a matter of minutes [8], [12].", "startOffset": 197, "endOffset": 201}, {"referenceID": 10, "context": "Bayesian Optimization (BO) is a well-established strategy for finding the extrema of functions that are expensive to evaluate [11], [13].", "startOffset": 126, "endOffset": 130}, {"referenceID": 12, "context": "Bayesian Optimization (BO) is a well-established strategy for finding the extrema of functions that are expensive to evaluate [11], [13].", "startOffset": 132, "endOffset": 136}, {"referenceID": 10, "context": "Many models could be used for the BO prior, but Gaussian Process (GP) priors are the most common choice [11].", "startOffset": 104, "endOffset": 108}, {"referenceID": 10, "context": "A GP is an extension of the multivariate Gaussian distribution to an infinite-dimension stochastic process for which any finite combination of dimensions will be a Gaussian distribution [11].", "startOffset": 186, "endOffset": 190}, {"referenceID": 10, "context": "[11] for a more detailed explanation.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "An off-line evolutionary algorithm, MAP-Elites [14][8], that generates many thousands of potential good behaviors is followed by a trial and error on-line adaptation part, based on BO (M-BOA), in order to find a compensatory behavior.", "startOffset": 47, "endOffset": 51}, {"referenceID": 7, "context": "An off-line evolutionary algorithm, MAP-Elites [14][8], that generates many thousands of potential good behaviors is followed by a trial and error on-line adaptation part, based on BO (M-BOA), in order to find a compensatory behavior.", "startOffset": 51, "endOffset": 54}, {"referenceID": 14, "context": "[15].", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "See Figure 1b for the scenario and [8] for more details on the simulated hexapod.", "startOffset": 35, "endOffset": 38}, {"referenceID": 15, "context": "We evolved different atomic behaviors using the MAP-Elites algorithm with an 8D behavior descriptor (2 dimensions for space diversity + 6 dimensions for walking diversity), inspired by [16], [12].", "startOffset": 185, "endOffset": 189}, {"referenceID": 11, "context": "We evolved different atomic behaviors using the MAP-Elites algorithm with an 8D behavior descriptor (2 dimensions for space diversity + 6 dimensions for walking diversity), inspired by [16], [12].", "startOffset": 191, "endOffset": 195}], "year": 2016, "abstractText": "The recently introduced Intelligent Trial and Error algorithm (IT&E) enables robots to creatively adapt to damage in a matter of minutes by combining an off-line evolutionary algorithm and an on-line learning algorithm based on Bayesian Optimization. We extend the IT&E algorithm to allow for robots to learn to compensate for damages while executing their task(s). This leads to a semi-episodic learning scheme that increases the robot\u2019s life-time autonomy and adaptivity. Preliminary experiments on a toy simulation and a 6-legged robot locomotion task show promising results.", "creator": "LaTeX with hyperref package"}}}