{"id": "1610.01465", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "abstract": "Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, several additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.", "histories": [["v1", "Wed, 5 Oct 2016 14:58:36 GMT  (3353kb,D)", "http://arxiv.org/abs/1610.01465v1", null], ["v2", "Wed, 26 Oct 2016 01:39:40 GMT  (3353kb,D)", "http://arxiv.org/abs/1610.01465v2", null], ["v3", "Wed, 1 Mar 2017 05:39:21 GMT  (1766kb,D)", "http://arxiv.org/abs/1610.01465v3", null], ["v4", "Thu, 15 Jun 2017 01:52:59 GMT  (8046kb,D)", "http://arxiv.org/abs/1610.01465v4", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.CL", "authors": ["kushal kafle", "christopher kanan"], "accepted": false, "id": "1610.01465"}, "pdf": {"name": "1610.01465.pdf", "metadata": {"source": "CRF", "title": "Visual Question Answering: Datasets, Algorithms, and Future Challenges", "authors": ["Kushal Kafle", "Christopher Kanan"], "emails": [], "sections": [{"heading": null, "text": "Visual Question Answering (VQA) is a current problem in computer vision and natural language processing that has generated great interest in deep learning, computer vision and natural language processing. In VQA, an algorithm must answer text-based questions about images. Since the publication of the first VQA dataset in 2014, several additional datasets have been published and many algorithms proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics and algorithms. In particular, we discuss the limitations of current datasets in terms of their ability to properly train and evaluate VQA algorithms. Then, we thoroughly review existing algorithms for VQA. Finally, we discuss possible future directions for VQA research and understanding of images."}, {"heading": "1 Introduction", "text": "In fact, most of them are able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "2 Vision and Language Tasks", "text": "In fact, most of them are able to outdo themselves by putting themselves and themselves at the center. (...) Most of them are able to outdo themselves. (...) Most of them, however, are not able to outdo themselves. \"(...) Most of them, however, are not able to outdo themselves.\" (...) Most of them are able to outdo themselves. \"(...) Most of them are not able to outdo themselves.\" (...) Most of them are able to outdo themselves. \"(...) Most of them are able to outdo themselves.\" (...) Most of them are unable to outdo themselves. \"(...) Most of them are not able to outdo themselves.\" (...) Most of them are able to outdo themselves. (...) Most of them are not able to outdo themselves. (...) Most of them are able to outdo themselves. (...) Most of them are able to outdo themselves. (...) Most of them are not able to outdo themselves. (...) Most of them are able to outdo themselves."}, {"heading": "3 Datasets for VQA", "text": "The ideal VQA dataset must be sufficiently large to capture the variability within questions, images, and concepts that occur in real-world scenarios, and it should also have a fair evaluation scheme that is difficult to \"play\" and indicates that an algorithm can answer questions about images that have definite answers. If a dataset contains easily exploitable distortions in the distribution of questions or answers, it may be possible for an algorithm to work well on the dataset without actually solving the VQA problem. In this article, the most important datasets for VQA DAQUAR [30], COCO-QA [31], The VQA datasets for VQA [32], VQA-IQA [33], Visual Genome [34], and Visual Genome [35] datasets for the common datasets of VQUCO-1,000 images are included."}, {"heading": "3.1 DAQUAR", "text": "The DAtaset for QUestion QA-I QA [33] 1 Vua l704 g [32 O] 93 Q [30] was the first large VQA-7 dataset to be released. It is one of the smallest VQA datasets. It consists of 6795 QUAR-37 and 5673 QA pairs, which are shown on the images from the NYUPthV2 dataset [36].A The dataset is also available in an even smaller configuration, consisting of only 37 object categories, known as DAQUAR-37. DAQUAR-37 consists of only 3825 QA pairs and 297 QA pairs. In [37], additional ground correctness answers on DAQUAR page 4T are able 1: S tati stic sfo rV QA da tase ts.D AQ UA R [3 0 0 0 0 0 0] C OC O-Q A [3 1] OC OF-Q A QA [3 1] OC OC A-04 A A-A QI-04 A QI-A [7V] QI-33] QI-A [7V]"}, {"heading": "3.2 COCO-QA", "text": "In COCO-QA, QA pairs are created for images that use a Natural Language Processing (NLP) algorithm that derives them from the COCO images using the captions. For example, if you use the caption \"A boy plays Frisbee,\" you can create the question \"What does the boy play?\" with Frisbee as the answer. COCO-QA contains 78,736 training and 38,948 test quality pairs. Most questions are about the object in the image (69.84%), with the other questions referring to color (16.59%), counting (7.47%) and location (6.10%). All questions have a single word answer, and there are only 435 unique answers. These limitations of answers make the evaluation relatively simple. COCO-QA's biggest shortcoming is due to errors in the NLP algorithm used to generate the QA pairs. Longer sentences are used to simplify the grammar, but are not easily broken down in many cases."}, {"heading": "3.3 The VQA Dataset", "text": "The aforementioned persons are able to move, to be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "3.4 FM-IQA", "text": "The Freestyle Multilingual Image Question Answering (FM-IQA) dataset is another dataset based on COCO [33]. It contains man-made answers and questions. Originally, the dataset was collected in Chinese, but English translations were provided. Unlike COCO-QA and DAQUAR, this dataset also enabled full answers, making automatic evaluation using common metrics unfeasible. Therefore, the authors proposed to use human judges to evaluate whether or not the answer is given by a human, as well as the quality of an answer on a scale of 0-2. This approach is impracticable for most research groups and complicates the development of algorithms. This has led to limited use of FM-IQA, with the only exception being the original work of the dataset creators."}, {"heading": "3.5 Visual Genome", "text": "In this article, the Visual Genome is described as the largest VQA dataset recently introduced to capture the amount of data beyond the basics established by the authors. Visual Genome consists of six types of \"W\" questions: What, how, who, and why? Two different types of data collection have been used to increase the amount of data. In the freeform method, users are free to create an image."}, {"heading": "3.6 Visual7W", "text": "The Visual7W dataset is a subset of the Visual Genome. Visual7W contains 47,300 images from the Visual Genome, which are also present in the COCO. Visual7W is named after the seven question categories it contains: What, Where, How, When, Who, Why and What. The dataset consists of two different types of questions. The \"telling\" questions are identical to the questions of the Visual Genome, and the answer is text-based. The \"pointing\" questions are those that start with \"Which,\" and for these questions the algorithm must select the correct input field from the alternatives. An example of a pointing question is Figure 4b.Visual7W uses a multiple-choice answer frame as standard evaluation, with four possible answers provided to an algorithm during evaluation. To make the task challenging, the multiplex decisions consist of answers that are plausible for the given question."}, {"heading": "3.7 SHAPES", "text": "While the other VQA datasets contain either real or synthetic scenes, the SHAPES dataset [42] consists of shapes of different arrangements, types, and colors. Questions concern the attributes, relationships, and positions of the shapes. This approach allows for the creation of an enormous amount of data that is free from many distortions that plague other datasets in different degrees. SHAPES consists of 244 unique questions, with each question being asked about each of the 64 images in the dataset. Unlike other datasets, this means that it is perfectly balanced and free of bias. All questions are binary, with yes / no answers. Many of the questions require positional thinking about the layout and properties of the shapes. Although SHAPES cannot replace the use of scenes, the idea behind it is extremely valuable."}, {"heading": "4 Evaluation Metrics for VQA", "text": "In fact, most of us are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves, \"he told the Deutsche Presse-Agentur."}, {"heading": "5 Algorithms for VQA", "text": "Most of them formulate VQA as a classification problem and treat each answer as its own category, requiring an algorithm to extract image and question attributes. Most algorithms use CNNs pre-trained on ImageNet to extract image attributes, such as VGGNet [1], ResNet [2] and GoogLeNet [56]. There is a wider range of methods used to extract text attributes, including bag-of-words (BOW) [32, 44], long-term short-term memory (LSTM) encoders [32, 31, 37, 47, 33] and skipthought vectors [40, 45]. In this section, we briefly describe the algorithms used for VQA and break them down into common topics. Results on DAQUAR, COCO-QA and COCO-VQA for many of the most popular methods are listed in Table 2."}, {"heading": "5.1 Baseline Models", "text": "The simplest baseline algorithms for VQA consist of using linear or multi-layered perceptron (MLP) classifiers applied to a vector of image and text properties that are interconnected [32, 40, 44]. In [44], the authors used a sack of words to represent the question and CNN features of GoogLeNet for the vi-Page 11 sual features, and then fed these features into a multi-layered logistic regression classifier. Their approach worked remarkably well, surpassing the previous baseline for COCO-VQA [32], which used a more theoretically powerful model, an LSTM model, to represent the question. Similarly, [40] STard thought vectors [57] used to represent the questions, and Res152 to represent image features. They found that an MLP model trained with these two layers."}, {"heading": "5.2 Bayesian and Question-Aware Models", "text": "The authors used semantic segmentation to identify the objects in an image and their positions, and then trained a Bayesian algorithm to model the spatial relationships of the objects used to calculate the probability of each answer. [40] This was the earliest known algorithm for VQA, but its effectiveness is surpassed by simple base models, in part because it depends on the results of the semantic segmentation that was imperfect. [40] A very different Bayesian model was proposed, taking advantage of the fact that the nature of the answer can be predicted solely on the basis of the question, for example, \"What color does the flower have?\" would be assigned as a color question by the model, essentially turning the open problem into a multiplex image."}, {"heading": "5.3 Attention Based Models", "text": "The basic idea is that certain visual regions and certain words in a question are more informative than others in answering a given question. For example, for a system that answers \"What color is the screen?\" The image region that contains the screen is more informative than other image regions. Likewise, \"color\" and \"umbrella\" are the textual inputs that need to be addressed more directly than the others. Global image features, such as the last hidden layer of a CNN, and global text properties, such as editing text properties, are not granular enough to address region-specific issues. Similar attention models have shown great success in other visions and NLP tasks, such as labeling [20] and machine translation [58]. While several papers focus on the use of spatial visual attention for VQA."}, {"heading": "5.4 Other Noteworthy Models", "text": "In fact, most of them will be able to move to another world in which they are able, in which they are able to move, and in which they are able, in which they are able to move."}, {"heading": "6 Discussion", "text": "As shown in Figure 9, the performance of VQA algorithms has improved rapidly, but there is still a significant gap between the best methods and humans. It remains unclear whether the performance improvements stem from mechanisms built into later systems, such as attention, or whether they are due to other factors. In addition, it can be difficult to decouple the contributions of text and image data in isolation."}, {"heading": "6.1 Vision vs. Language in VQA", "text": "VQA consists of two distinct data streams that must be used correctly to ensure robust performance: images and questions. But, do current systems appropriately use both vision and language? Ablation studies [40, 32] have routinely shown that questions only perform models that are dramatically better than images, especially on open COCO-VQA. On COCO-QA, simple image-blind models that only use the question can achieve 50% accuracy, with the gain from using the image being comparatively modest [40]. In [40] it has also been shown that for DAQUAR37, using better language that uses an image-blind model, results are superior to previous work that used both images and questions. This is primarily due to two factors. First, the question limits the types of answers that are expected in many cases to essentially turn an open question into a multiple-choice system, e.g. questions about the color of an object as a prediction."}, {"heading": "6.2 How useful is attention for VQA?", "text": "It is difficult to determine how much attention VQA algorithms help. Currently, the best VQA model for COCO-VQA uses spatial visual attention [55], but simple models that do not use attention have shown that they outperform earlier models that used complex attention mechanisms. In [65], for example, some attention-based models performed very well in comparison, combining image and fragment functions by elemental multiplication and addition rather than linking them exclusively to each other. Combined with adjustment, this yielded significantly higher results than the complex attention-based models used in [47] and [51]. Similar results were obtained by other systems that do not use spatial attention [40, 55, 66]. In [67], the authors showed that methods that are commonly used to draw spatial attention to certain image functions do not cause QA models with the same regions to be challenged."}, {"heading": "6.3 Bias Impairs Method Evaluation", "text": "Many questions relate to the presence of objects or scene attributes. These questions tend to be well answered by CNN's and also have strong linguistic distortions. Difficult questions, such as those that begin with \"why,\" are comparatively rare, which has serious implications on page 18 of page 19 for evaluating performance. In COCO-VQA (Train and Validation Partions), a system that improves the accuracy of questions that begin with \"Is\" and \"Are\" by 15% increases overall accuracy by 5%. On the other hand, the same increase in both \"Why\" and \"Where\" only increases accuracy by 0.6%. Even if all \"Why\" and \"Where\" questions are correctly answered, the overall increase in accuracy is only 4.1%. On the other hand, the answer to all questions that begin with \"Is there\" is an accuracy of 85.2%."}, {"heading": "6.4 Are Binary Questions Sufficient?", "text": "The main argument against the use of binary questions is the lack of complex questions and the relative ease in answering the questions typically generated by human annotators. Visual Genome and Visual7W completely exclude binary questions, and the authors argued that this choice would encourage more complex questions from the annotators. On the other hand, binary questions are easy to evaluate, and these questions can theoretically include a huge variety of tasks. SHAPES data set [42] uses binary questions exclusively, but contains complex questions that involve spatial thinking, counting, and drawing conclusions (see Figure 6). [39] Using cartoons, these questions can also be particularly difficult for VQA algorithms when the dataset is balanced."}, {"heading": "6.5 Open Ended vs. Multiple Choice", "text": "Since it is difficult to evaluate open-ended multiword answers, multiple choice answers have been suggested as a way to evaluate VQA algorithms. As long as the alternatives are sufficiently difficult, a system could be evaluated in this way, but then used to answer open questions. Therefore, multiple choice is used to evaluate Visual7W, visual genomes, and a variant of the VQA dataset. Within this framework, an algorithm has access to a number of possible answers (e.g. 18 for COCO-VQA), along with the question and the image, and then has to select from possible selections.A major problem with multiple choice evaluation is that the problem can be reduced to determining which of the answers is correct rather than actually answering the question. In [66] for example, they formulated VQA as an answer-scoring task where the system was evaluated on the basis of the image, question, and potential answers. The answers themselves were fed into the system itself, which is called QA-System, the QA-System is sufficiently supplied with QCO-QW results."}, {"heading": "7 Recommendations for Future", "text": "In this section, we discuss future developments in VQA datasets that will make them better benchmarks for the problem. Future datasets need to be larger. While VQAPage 20 datasets have increased in size and diversity, algorithms do not have enough data for training and evaluation. We have conducted a small experiment in which we have built a simple MLP baseline model for VQA using ResNet152 image functions and skipped properties for the questions, and we evaluate performance as a function of the amount of training data available to COVQA. Results are shown in Figure 12, where it is clear that the curve has not begun to approach an asymptomatic view, suggesting that even on datasets that are distorted and increase the size of the datasets."}, {"heading": "8 Conclusions", "text": "An algorithm that can answer arbitrary questions about images would be a milestone in artificial intelligence. We believe that VQA should be a necessary part of any visual Turing test. In this essay, we critically reviewed existing datasets and algorithms for VQA. We discussed the challenges in evaluating answers generated by algorithms, especially multiword responses. We described how distortions and other problems plague existing datasets. This is a big problem, and the field needs a dataset that evaluates the important properties of a VQA algorithm, so that if an algorithm performs well on that dataset, it generally performs well on VQA."}, {"heading": "Acknowledgments", "text": "We thank Ronald Kemker for his helpful comments on an earlier draft of this paper. page 21"}], "references": [{"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "CVPR, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "You only look once: Unified, realtime object detection", "author": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "venue": "CVPR, 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Faster r-cnn: Towards real-time object detection with region proposal networks", "author": ["S. Ren", "K. He", "R. Girshick", "J. Sun"], "venue": "Advances in neural information processing systems, pp. 91\u201399, 2015.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Long-term recurrent convolutional networks for visual recognition and description", "author": ["J. Donahue", "L.A. Hendricks", "S. Guadarrama", "M. Rohrbach", "S. Venugopalan", "K. Saenko", "T. Darrell"], "venue": "CVPR, 2015.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Largescale video classification with convolutional neural networks", "author": ["A. Karpathy", "G. Toderici", "S. Shetty", "T. Leung", "R. Sukthankar", "L. Fei-Fei"], "venue": "CVPR, pp. 1725\u20131732, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Two-stream convolutional networks for action recognition in videos", "author": ["K. Simonyan", "A. Zisserman"], "venue": "NIPS, pp. 568\u2013576, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Visual turing test for computer vision systems", "author": ["D. Geman", "S. Geman", "N. Hallonquist", "L. Younes"], "venue": "Proceedings of the National Academy of Sciences, vol. 112, no. 12, pp. 3618\u2013 3623, 2015.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Towards a visual turing challenge", "author": ["M. Malinowski", "M. Fritz"], "venue": "arXiv preprint arXiv:1410.8027, 2014.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "ECCV, 2014.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep neural networks for object detection", "author": ["C. Szegedy", "A. Toshev", "D. Erhan"], "venue": "Advances in Neural Information Processing Systems, pp. 2553\u20132561, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR, pp. 3431\u20133440, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning deconvolution network for semantic segmentation", "author": ["H. Noh", "S. Hong", "B. Han"], "venue": "CVPR.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 0}, {"title": "Instance segmentation of indoor scenes using a coverage loss", "author": ["N. Silberman", "D. Sontag", "R. Fergus"], "venue": "ECCV, pp. 616\u2013631, Springer, 2014.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Monocular object instance segmentation and depth ordering with CNNs", "author": ["Z. Zhang", "A.G. Schwing", "S. Fidler", "R. Urtasun"], "venue": "CVPR, pp. 2614\u20132622, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Instancelevel segmentation with deep densely connected MRFs", "author": ["Z. Zhang", "S. Fidler", "R. Urtasun"], "venue": "CVPR, 2016.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep visualsemantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, 2015.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep captioning with multimodal recurrent neural networks (m-rnn)", "author": ["J. Mao", "W. Xu", "Y. Yang", "J. Wang", "Z. Huang", "A. Yuille"], "venue": "ICLR, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "A. Courville", "R. Salakhutdinov", "R. Zemel", "Y. Bengio"], "venue": "ICML, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "BLEU: a method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.-J. Zhu"], "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pp. 311\u2013318, Association for Computational Linguistics, 2002.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2002}, {"title": "Rouge: A package for automatic evaluation of summaries", "author": ["C.-Y. Lin"], "venue": "Text summarization branches out: Proceedings of the ACL-04 workshop, vol. 8, Barcelona, Spain, 2004.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2004}, {"title": "Meteor: An automatic metric for mt evaluation with improved correlation with human judgments", "author": ["S. Banerjee", "A. Lavie"], "venue": "Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, vol. 29, pp. 65\u201372, 2005. Page 22", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2005}, {"title": "Cider: Consensus-based image description evaluation", "author": ["R. Vedantam", "C. Lawrence Zitnick", "D. Parikh"], "venue": "CVPR, 2015.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2015}, {"title": "Re-evaluation the role of BLEU in machine translation research", "author": ["C. Callison-Burch", "M. Osborne", "P. Koehn"], "venue": "2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "From captions to visual concepts and back", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J. Platt"], "venue": "CVPR, 2015.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Automatic description generation from images: A survey of models, datasets, and evaluation measures", "author": ["R. Bernardi", "R. Cakici", "D. Elliott", "A. Erdem", "E. Erdem", "N. Ikizler-Cinbis", "F. Keller", "A. Muscat", "B. Plank"], "venue": "Journal of Artificial Intelligence Research, vol. 55, pp. 409\u2013 442, 2016.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2016}, {"title": "Exploring nearest neighbor approaches for image captioning", "author": ["J. Devlin", "S. Gupta", "R. Girshick", "M. Mitchell", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1505.04467, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR, 2016.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2016}, {"title": "A multi-world approach to question answering about real-world scenes based on uncertain input", "author": ["M. Malinowski", "M. Fritz"], "venue": "NIPS, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS, 2015.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "VQA: Visual question answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C.L. Zitnick", "D. Parikh"], "venue": "ICCV, 2015.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Are you talking to a machine? Dataset and methods for multilingual image question answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Y. Zhu", "O. Groth", "M. Bernstein", "L. Fei-Fei"], "venue": "CVPR, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "Visual Genome: Connecting language and vision using crowdsourced dense image annotations", "author": ["R. Krishna", "Y. Zhu", "O. Groth", "J. Johnson", "K. Hata", "J. Kravitz", "S. Chen", "Y. Kalantidis", "L.- J. Li", "D.A. Shamma", "M. Bernstein", "L. Fei- Fei"], "venue": "2016.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Indoor segmentation and support inference from rgbd images", "author": ["N. Silberman", "D. Hoiem", "P. Kohli", "R. Fergus"], "venue": "ECCV, 2012.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV, 2015.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Zeroshot learning via visual abstraction", "author": ["S. Antol", "C.L. Zitnick", "D. Parikh"], "venue": "European Conference on Computer Vision, pp. 401\u2013 416, 2014.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Yin and yang: Balancing and answering binary visual questions", "author": ["P. Zhang", "Y. Goyal", "D. Summers-Stay", "D. Batra", "D. Parikh"], "venue": "CVPR, 2016.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Answer-type prediction for visual question answering", "author": ["K. Kafle", "C. Kanan"], "venue": "CVPR, 2016.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2016}, {"title": "Yfcc100m: The new data in multimedia research", "author": ["B. Thomee", "D.A. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L.- J. Li"], "venue": "Communications of the ACM, vol. 59, no. 2, pp. 64\u201373, 2016.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2016}, {"title": "Neural module networks", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "CVPR, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}, {"title": "Verbs semantics and lexical selection", "author": ["Z. Wu", "M. Palmer"], "venue": "Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pp. 133\u2013138, Association for Computational Linguistics, 1994.", "citeRegEx": "43", "shortCiteRegEx": null, "year": 1994}, {"title": "Simple baseline for visual question answering", "author": ["B. Zhou", "Y. Tian", "S. Sukhbaatar", "A. Szlam", "R. Fergus"], "venue": "CoRR, vol. abs/1512.02167, 2015.", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2015}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["H. Noh", "P.H. Seo", "B. Han"], "venue": "CVPR, 2016.", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask, attend and answer: Exploring question-guided spatial attention for visual question answering", "author": ["H. Xu", "K. Saenko"], "venue": "arXiv preprint arXiv:1511.05234, 2015.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2015}, {"title": "Stacked attention networks for image question answering", "author": ["Z. Yang", "X. He", "J. Gao", "L. Deng", "A.J. Smola"], "venue": "CVPR, 2016. Page 23", "citeRegEx": "47", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask me anything: Free-form visual question answering based on knowledge from external sources", "author": ["Q. Wu", "P. Wang", "C. Shen", "A. van den Hengel", "A.R. Dick"], "venue": "CVPR, 2016.", "citeRegEx": "48", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning to compose neural networks for question answering", "author": ["J. Andreas", "M. Rohrbach", "T. Darrell", "D. Klein"], "venue": "arXiv preprint arXiv:1601.01705, 2016.", "citeRegEx": "49", "shortCiteRegEx": null, "year": 2016}, {"title": "A focused dynamic attention model for visual question answering", "author": ["I. Ilievski", "S. Yan", "J. Feng"], "venue": "arXiv preprint arXiv:1604.01485, 2016.", "citeRegEx": "50", "shortCiteRegEx": null, "year": 2016}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["C. Xiong", "S. Merity", "R. Socher"], "venue": "arXiv preprint arXiv:1603.01417, 2016.", "citeRegEx": "51", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal residual learning for visual qa", "author": ["J.-H. Kim", "S.-W. Lee", "D.-H. Kwak", "M.-O. Heo", "J. Kim", "J.-W. Ha", "B.-T. Zhang"], "venue": "arXiv preprint arXiv:1606.01455, 2016.", "citeRegEx": "52", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical question-image co-attention for visual question answering", "author": ["J. Lu", "J. Yang", "D. Batra", "D. Parikh"], "venue": "CoRR, vol. abs/1606.00061, 2016.", "citeRegEx": "53", "shortCiteRegEx": null, "year": 2016}, {"title": "Training recurrent answering units with joint loss minimization for VQA", "author": ["H. Noh", "B. Han"], "venue": "arXiv preprint arXiv:1606.03647, 2016.", "citeRegEx": "54", "shortCiteRegEx": null, "year": 2016}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["A. Fukui", "D.H. Park", "D. Yang", "A. Rohrbach", "T. Darrell", "M. Rohrbach"], "venue": "CoRR, vol. abs/1606.01847, 2016.", "citeRegEx": "55", "shortCiteRegEx": null, "year": 1847}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "CVPR, 2015.", "citeRegEx": "56", "shortCiteRegEx": null, "year": 2015}, {"title": "Skipthought vectors", "author": ["R. Kiros", "Y. Zhu", "R. Salakhutdinov", "R.S. Zemel", "A. Torralba", "R. Urtasun", "S. Fidler"], "venue": "NIPS, 2015.", "citeRegEx": "57", "shortCiteRegEx": null, "year": 2015}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["M.-T. Luong", "H. Pham", "C.D. Manning"], "venue": "arXiv preprint arXiv:1508.04025, 2015.", "citeRegEx": "58", "shortCiteRegEx": null, "year": 2015}, {"title": "Where to look: Focus regions for visual question answering", "author": ["K.J. Shih", "S. Singh", "D. Hoiem"], "venue": "CVPR, 2016.", "citeRegEx": "59", "shortCiteRegEx": null, "year": 2016}, {"title": "Edge boxes: Locating object proposals from edges", "author": ["C.L. Zitnick", "P. Doll\u00e1r"], "venue": "European Conference on Computer Vision, pp. 391\u2013405, Springer, 2014.", "citeRegEx": "60", "shortCiteRegEx": null, "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["T. Mikolov", "J. Dean"], "venue": "2013.", "citeRegEx": "61", "shortCiteRegEx": null, "year": 2013}, {"title": "Ask me anything: Dynamic memory networks for natural language processing", "author": ["A. Kumar", "O. Irsoy", "J. Su", "J. Bradbury", "R. English", "B. Pierce", "P. Ondruska", "I. Gulrajani", "R. Socher"], "venue": "ICML, 2016.", "citeRegEx": "62", "shortCiteRegEx": null, "year": 2016}, {"title": "DBpedia\u2013a large-scale, multilingual knowledge base extracted from Wikipedia", "author": ["J. Lehmann", "R. Isele", "M. Jakob", "A. Jentzsch", "D. Kontokostas", "P.N. Mendes", "S. Hellmann", "M. Morsey", "P. van Kleef", "S. Auer"], "venue": "Semantic Web, vol. 6, no. 2, pp. 167\u2013195, 2015.", "citeRegEx": "63", "shortCiteRegEx": null, "year": 2015}, {"title": "Analyzing the behavior of visual question answering models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "CoRR, vol. abs/1606.07356, 2016.", "citeRegEx": "64", "shortCiteRegEx": null, "year": 2016}, {"title": "Dualnet: Domain-invariant network for visual question answering", "author": ["K. Saito", "A. Shin", "Y. Ushiku", "T. Harada"], "venue": "arXiv preprint arXiv:1606.06108, 2016.", "citeRegEx": "65", "shortCiteRegEx": null, "year": 2016}, {"title": "Revisiting visual question answering baselines", "author": ["A. Jabri", "A. Joulin", "L. van der Maaten"], "venue": "arXiv preprint arXiv:1606.08390, 2016.", "citeRegEx": "66", "shortCiteRegEx": null, "year": 2016}, {"title": "Human attention in visual question answering: Do humans and deep networks look at the same regions", "author": ["A. Das", "H. Agrawal", "C.L. Zitnick", "D. Parikh", "D. Batra"], "venue": "arXiv preprint arXiv:1606.03556, 2016.", "citeRegEx": "67", "shortCiteRegEx": null, "year": 2016}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A. Efros"], "venue": "CVPR, 2011. Page 24", "citeRegEx": "68", "shortCiteRegEx": null, "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 157, "endOffset": 163}, {"referenceID": 1, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 157, "endOffset": 163}, {"referenceID": 2, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 182, "endOffset": 188}, {"referenceID": 3, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 182, "endOffset": 188}, {"referenceID": 4, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 215, "endOffset": 224}, {"referenceID": 5, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 215, "endOffset": 224}, {"referenceID": 6, "context": "Recent advancements in computer vision and deep learning research have enabled enormous progress in many computer vision tasks, such as image classification [1, 2], object detection [3, 4], and activity recognition [5, 6, 7].", "startOffset": 215, "endOffset": 224}, {"referenceID": 1, "context": "Given enough data, deep convolutional neural networks (CNNs) rival the abilities of humans to do image classification [2].", "startOffset": 118, "endOffset": 121}, {"referenceID": 7, "context": "sion problems, it can be considered a component of a Turing Test for image understanding [8, 9].", "startOffset": 89, "endOffset": 95}, {"referenceID": 8, "context": "sion problems, it can be considered a component of a Turing Test for image understanding [8, 9].", "startOffset": 89, "endOffset": 95}, {"referenceID": 7, "context": "A Visual Turing Test rigorously evaluates a computer vision system to assess whether it is capable of human-level semantic analysis of images [8, 9].", "startOffset": 142, "endOffset": 148}, {"referenceID": 8, "context": "A Visual Turing Test rigorously evaluates a computer vision system to assess whether it is capable of human-level semantic analysis of images [8, 9].", "startOffset": 142, "endOffset": 148}, {"referenceID": 1, "context": "The most successful of these is object recognition, where algorithms now rival humans in accuracy [2].", "startOffset": 98, "endOffset": 101}, {"referenceID": 10, "context": "The best object detection methods all use deep CNNs [11, 4, 3].", "startOffset": 52, "endOffset": 62}, {"referenceID": 3, "context": "The best object detection methods all use deep CNNs [11, 4, 3].", "startOffset": 52, "endOffset": 62}, {"referenceID": 2, "context": "The best object detection methods all use deep CNNs [11, 4, 3].", "startOffset": 52, "endOffset": 62}, {"referenceID": 11, "context": "Semantic segmentation takes the task of localization a step further by classifying each pixel as belonging to a particular semantic class [12, 13] .", "startOffset": 138, "endOffset": 146}, {"referenceID": 12, "context": "Semantic segmentation takes the task of localization a step further by classifying each pixel as belonging to a particular semantic class [12, 13] .", "startOffset": 138, "endOffset": 146}, {"referenceID": 13, "context": "Instance segmentation further improves upon localization by differentiating between separate instances of the same semantic class [14, 15, 16].", "startOffset": 130, "endOffset": 142}, {"referenceID": 14, "context": "Instance segmentation further improves upon localization by differentiating between separate instances of the same semantic class [14, 15, 16].", "startOffset": 130, "endOffset": 142}, {"referenceID": 15, "context": "Instance segmentation further improves upon localization by differentiating between separate instances of the same semantic class [14, 15, 16].", "startOffset": 130, "endOffset": 142}, {"referenceID": 16, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 4, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 17, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 18, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 19, "context": "One of the most studied is image captioning [17, 5, 18, 19, 20], in which an algorithm\u2019s goal is to produce a natural language description of a given image.", "startOffset": 44, "endOffset": 63}, {"referenceID": 20, "context": "Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24].", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24].", "startOffset": 61, "endOffset": 65}, {"referenceID": 22, "context": "Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "Most widely used evaluation schemes include BLEU [21], ROUGE [22], METEOR [23] and CIDEr [24].", "startOffset": 89, "endOffset": 93}, {"referenceID": 24, "context": "BLEU, the most widely used metric, is known to have the same score for large variations in sentence structure with largely varying semantic content [25].", "startOffset": 148, "endOffset": 152}, {"referenceID": 25, "context": "For captions generated in [26], BLEU scores ranked machine generated captions above human captions.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "This figure shows a semantic segmentation map from the COCO dataset [10].", "startOffset": 68, "endOffset": 72}, {"referenceID": 26, "context": "While other evaluation metrics, especially CIDEr and METEOR, show more robustness in terms of agreement with human judges, they still often rank automatically generated captions higher than human captions [27].", "startOffset": 205, "endOffset": 209}, {"referenceID": 27, "context": "In fact, a simple system that returns the caption of the training image with the most similar visual features using nearest neighbor yields relatively high scores using automatic evaluation metrics [28].", "startOffset": 198, "endOffset": 202}, {"referenceID": 28, "context": "Dense image captioning (DenseCap) avoids the generic caption problem by annotating an image densely with short visual descriptions pertaining to small, but salient, image regions [29].", "startOffset": 179, "endOffset": 183}, {"referenceID": 29, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 57, "endOffset": 61}, {"referenceID": 30, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 71, "endOffset": 75}, {"referenceID": 31, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 93, "endOffset": 97}, {"referenceID": 32, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 106, "endOffset": 110}, {"referenceID": 33, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 121, "endOffset": 125}, {"referenceID": 34, "context": "As of this article, the main datasets for VQA are DAQUAR [30], COCO-QA [31], The VQA Dataset [32], FM-IQA [33], Visual7W [34], and Visual Genome [35].", "startOffset": 145, "endOffset": 149}, {"referenceID": 9, "context": "With exception of DAQUAR, all of the datasets include images from the Microsoft Common Objects in Context (COCO) dataset [10], which consists of 328,000 images, 91 common object categories with over 2 million labeled instances, and an average of 5 captions per image.", "startOffset": 121, "endOffset": 125}, {"referenceID": 29, "context": "The DAtaset for QUestion Answering on Real-world images (DAQUAR) [30] was the first major VQA dataset to be released.", "startOffset": 65, "endOffset": 69}, {"referenceID": 35, "context": "It consists of 6795 training and 5673 testing QA pairs based on images from the NYUDepthV2 Dataset [36].", "startOffset": 99, "endOffset": 103}, {"referenceID": 36, "context": "In [37], additional ground truth answers were collected for DAQUAR to cre-", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 50, "endOffset": 56}, {"referenceID": 1, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 50, "endOffset": 56}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 68, "endOffset": 74}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 91, "endOffset": 97}, {"referenceID": 3, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 91, "endOffset": 97}, {"referenceID": 2, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 120, "endOffset": 126}, {"referenceID": 4, "context": "D A Q U A R [3 0 C O C O -Q A [3 1 C O C O -V Q A [3 2 ] F M -I Q A [3 3 ] 1 V is u a l7 W [3 4 ] V is u a l g en o m e [3 5 ]", "startOffset": 120, "endOffset": 126}, {"referenceID": 31, "context": "The VQA Dataset [32] consists of both real images from COCO and abstract cartoon images.", "startOffset": 16, "endOffset": 20}, {"referenceID": 37, "context": "The human models are the same as those used in [38], and they contain deformable limbs and eight different facial expressions.", "startOffset": 47, "endOffset": 51}, {"referenceID": 38, "context": "Yin and Yang [39] is a dataset built on top of SYNTH-VQA that tried to eliminate biases in the answers people have to questions.", "startOffset": 13, "endOffset": 17}, {"referenceID": 39, "context": "6% accuracy on COCO-VQA using the question alone [40].", "startOffset": 49, "endOffset": 53}, {"referenceID": 32, "context": "The Freestyle Multilingual Image Question Answering (FM-IQA) dataset is another dataset based on COCO [33].", "startOffset": 102, "endOffset": 106}, {"referenceID": 34, "context": "Visual Genome [35] consists of 108,249 images that occur in both YFCC100M [41] and COCO images.", "startOffset": 14, "endOffset": 18}, {"referenceID": 40, "context": "Visual Genome [35] consists of 108,249 images that occur in both YFCC100M [41] and COCO images.", "startOffset": 74, "endOffset": 78}, {"referenceID": 41, "context": "While the other VQA datasets contain either real or synthetic scenes, the SHAPES dataset [42] consists of shapes of varying arrangements, types, and colors.", "startOffset": 89, "endOffset": 93}, {"referenceID": 41, "context": "Questions in the SHAPES dataset [42] include counting (How many triangles are there?), spatial reasoning (Is there a red shape above a circle?), and inference (Is there a blue shape red?)", "startOffset": 32, "endOffset": 36}, {"referenceID": 34, "context": "This figure is taken from [35].", "startOffset": 26, "endOffset": 30}, {"referenceID": 33, "context": "Free form QA: What does the sky look like? Region based QA: What color is the horse? (b) Example of the pointing QA task in Visual7W [34].", "startOffset": 133, "endOffset": 137}, {"referenceID": 42, "context": "Wu-Palmer Similarity (WUPS) [43] was proposed as an alternative to accuracy in [30].", "startOffset": 28, "endOffset": 32}, {"referenceID": 29, "context": "Wu-Palmer Similarity (WUPS) [43] was proposed as an alternative to accuracy in [30].", "startOffset": 79, "endOffset": 83}, {"referenceID": 29, "context": "To remedy this, [30] proposed to threshold WUPS scores, where a score that is below a threshold will be scaled down by a factor.", "startOffset": 16, "endOffset": 20}, {"referenceID": 29, "context": "1 was suggested by [30].", "startOffset": 19, "endOffset": 23}, {"referenceID": 31, "context": "An alternative to relying on semantic similarity measures is to have multiple independently collected ground truth answers for each question, which was done for The VQA Dataset [32] and DAQUARconsensus [37].", "startOffset": 177, "endOffset": 181}, {"referenceID": 36, "context": "An alternative to relying on semantic similarity measures is to have multiple independently collected ground truth answers for each question, which was done for The VQA Dataset [32] and DAQUARconsensus [37].", "startOffset": 202, "endOffset": 206}, {"referenceID": 32, "context": "The creators of FM-IQA [33] suggested using human judges to assess multi-word answers, but this presents a number of problems.", "startOffset": 23, "endOffset": 27}, {"referenceID": 0, "context": "Most algorithms use CNNs that are pre-trained on ImageNet to extract image features, such as VGGNet [1], ResNet [2], and GoogLeNet [56].", "startOffset": 100, "endOffset": 103}, {"referenceID": 1, "context": "Most algorithms use CNNs that are pre-trained on ImageNet to extract image features, such as VGGNet [1], ResNet [2], and GoogLeNet [56].", "startOffset": 112, "endOffset": 115}, {"referenceID": 55, "context": "Most algorithms use CNNs that are pre-trained on ImageNet to extract image features, such as VGGNet [1], ResNet [2], and GoogLeNet [56].", "startOffset": 131, "endOffset": 135}, {"referenceID": 31, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 101, "endOffset": 109}, {"referenceID": 43, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 101, "endOffset": 109}, {"referenceID": 31, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 30, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 36, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 46, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 32, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 150, "endOffset": 170}, {"referenceID": 39, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 196, "endOffset": 204}, {"referenceID": 44, "context": "There have been a wider range of methods used to extract text features, including bag-of-words (BOW) [32, 44], long short term memory (LSTM) encoders [32, 31, 37, 47, 33], and skipthought vectors [40, 45].", "startOffset": 196, "endOffset": 204}, {"referenceID": 31, "context": "The simplest baseline algorithms for VQA consist of using linear or multi-layer perceptron (MLP) classifiers applied to a vector of image and text features concatenated to each other [32, 40, 44].", "startOffset": 183, "endOffset": 195}, {"referenceID": 39, "context": "The simplest baseline algorithms for VQA consist of using linear or multi-layer perceptron (MLP) classifiers applied to a vector of image and text features concatenated to each other [32, 40, 44].", "startOffset": 183, "endOffset": 195}, {"referenceID": 43, "context": "The simplest baseline algorithms for VQA consist of using linear or multi-layer perceptron (MLP) classifiers applied to a vector of image and text features concatenated to each other [32, 40, 44].", "startOffset": 183, "endOffset": 195}, {"referenceID": 43, "context": "In [44], the authors used a bag-of-words to represent the question and CNN features from GoogLeNet for the vi-", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "IMAGE-ONLY [40] 6.", "startOffset": 11, "endOffset": 15}, {"referenceID": 39, "context": "59 QUESTION-ONLY [40] 25.", "startOffset": 17, "endOffset": 21}, {"referenceID": 29, "context": "MULTI-WORLD [30] 7.", "startOffset": 12, "endOffset": 16}, {"referenceID": 36, "context": "73 - ASK-NEURON [37] 21.", "startOffset": 16, "endOffset": 20}, {"referenceID": 30, "context": "68 - ENSEMBLE [31] - 36.", "startOffset": 14, "endOffset": 18}, {"referenceID": 31, "context": "84 - LSTM Q+I [32] 54.", "startOffset": 14, "endOffset": 18}, {"referenceID": 43, "context": "17 iBOWIMG [44] 55.", "startOffset": 11, "endOffset": 15}, {"referenceID": 44, "context": "97 DPPNet [45] 28.", "startOffset": 10, "endOffset": 14}, {"referenceID": 45, "context": "69 SMem [46] - 40.", "startOffset": 8, "endOffset": 12}, {"referenceID": 46, "context": "24 SAN [47] 29.", "startOffset": 7, "endOffset": 11}, {"referenceID": 47, "context": "90 AMA [48] 69.", "startOffset": 7, "endOffset": 11}, {"referenceID": 41, "context": "40 NMN [42] 58.", "startOffset": 7, "endOffset": 11}, {"referenceID": 48, "context": "70 D-NMN [49] 59.", "startOffset": 9, "endOffset": 13}, {"referenceID": 49, "context": "40 FDA [50] 59.", "startOffset": 7, "endOffset": 11}, {"referenceID": 39, "context": "18 HYBRID [40] 28.", "startOffset": 10, "endOffset": 14}, {"referenceID": 50, "context": "06 DMN+ [51] 60.", "startOffset": 8, "endOffset": 12}, {"referenceID": 51, "context": "40 MRN [52] 61.", "startOffset": 7, "endOffset": 11}, {"referenceID": 52, "context": "33 HieCoAtten [53] 65.", "startOffset": 14, "endOffset": 18}, {"referenceID": 53, "context": "10 RAU ResNet [54] 63.", "startOffset": 14, "endOffset": 18}, {"referenceID": 54, "context": "30 MCB-ensemble [55] 66.", "startOffset": 16, "endOffset": 20}, {"referenceID": 31, "context": "Their approach worked remarkably well, surpassing the previous baseline on COCO-VQA from [32], which used a theoretically more powerful model, an LSTM, to represent the question.", "startOffset": 89, "endOffset": 93}, {"referenceID": 39, "context": "Similarly, [40] used skip-thought vectors [57] to represent the questions and ResNet152 to represent image features.", "startOffset": 11, "endOffset": 15}, {"referenceID": 56, "context": "Similarly, [40] used skip-thought vectors [57] to represent the questions and ResNet152 to represent image features.", "startOffset": 42, "endOffset": 46}, {"referenceID": 31, "context": "In [32], an LSTM encoder acting on a one-hot encoding of the sentence was used to represent question features, and GoogLeNet was used for image features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "In [37], an LSTM model was fed an embedding of each word sequentially with CNN features concatenated to it.", "startOffset": 3, "endOffset": 7}, {"referenceID": 30, "context": "A related approach was used in [31], where an LSTM was fed CNN features during the first and last time-steps, with word features in between.", "startOffset": 31, "endOffset": 35}, {"referenceID": 32, "context": "A similar approach was used in [33], but the CNN image features were only fed into the LSTM at the end of the question and instead of a classifier, another LSTM was used to generate the answer one word at a time.", "startOffset": 31, "endOffset": 35}, {"referenceID": 29, "context": "In [30], the first Bayesian framework for VQA was proposed.", "startOffset": 3, "endOffset": 7}, {"referenceID": 39, "context": "A very different Bayesian model was proposed in [40].", "startOffset": 48, "endOffset": 52}, {"referenceID": 19, "context": "Similar attention models have shown great success in other vision and NLP tasks, such as captioning [20] and machine translation [58].", "startOffset": 100, "endOffset": 104}, {"referenceID": 57, "context": "Similar attention models have shown great success in other vision and NLP tasks, such as captioning [20] and machine translation [58].", "startOffset": 129, "endOffset": 133}, {"referenceID": 58, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 46, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 50, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 45, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 52, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 49, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 54, "context": "While multiple papers have focused on using spatial visual attention for VQA [59, 47, 51, 46, 53, 50, 55], there are significant differences among these methods.", "startOffset": 77, "endOffset": 105}, {"referenceID": 58, "context": "The Focus Regions for VQA [59] and Focused Dynamic Attention (FDA) models [50] both used Edge Boxes [60] to generate bounding box region proposals for images.", "startOffset": 26, "endOffset": 30}, {"referenceID": 49, "context": "The Focus Regions for VQA [59] and Focused Dynamic Attention (FDA) models [50] both used Edge Boxes [60] to generate bounding box region proposals for images.", "startOffset": 74, "endOffset": 78}, {"referenceID": 59, "context": "The Focus Regions for VQA [59] and Focused Dynamic Attention (FDA) models [50] both used Edge Boxes [60] to generate bounding box region proposals for images.", "startOffset": 100, "endOffset": 104}, {"referenceID": 58, "context": "In [59], a CNN was used to extract features from each of these boxes.", "startOffset": 3, "endOffset": 7}, {"referenceID": 49, "context": "In FDA [50], the authors proposed to only use the region proposals that have the objects mentioned in the question.", "startOffset": 7, "endOffset": 11}, {"referenceID": 1, "context": "During test, the labels are obtained by classifying each bounding box using ResNet [2].", "startOffset": 83, "endOffset": 86}, {"referenceID": 60, "context": "Subsequently, word2vec [61] was used to compute the similarity between words in the question and the object labels assigned to each of the bounding boxes.", "startOffset": 23, "endOffset": 27}, {"referenceID": 46, "context": "In contrast to using region proposals, the Stacked Attention Network (SAN) [47] and the Dynamic Memory Network (DMN) [51] models both used visual features from the spatial grid of a CNN\u2019s feature maps (see Figure 8).", "startOffset": 75, "endOffset": 79}, {"referenceID": 50, "context": "In contrast to using region proposals, the Stacked Attention Network (SAN) [47] and the Dynamic Memory Network (DMN) [51] models both used visual features from the spatial grid of a CNN\u2019s feature maps (see Figure 8).", "startOffset": 117, "endOffset": 121}, {"referenceID": 46, "context": "Both [47] and [51] used the last convolutional layer from VGG-19 with 448 \u00d7 448 images to produce a 14 \u00d7 14 filter response map with 512 dimensional features at each grid location.", "startOffset": 5, "endOffset": 9}, {"referenceID": 50, "context": "Both [47] and [51] used the last convolutional layer from VGG-19 with 448 \u00d7 448 images to produce a 14 \u00d7 14 filter response map with 512 dimensional features at each grid location.", "startOffset": 14, "endOffset": 18}, {"referenceID": 46, "context": "In SAN [47], an attention layer is specified by a single layer of weights that uses the question and the CNN feature map with a softmax activation function to compute the attention distribution across image locations.", "startOffset": 7, "endOffset": 11}, {"referenceID": 45, "context": "A similar attentive mechanism was used in the Spatial Memory Network [46] model, where spatial", "startOffset": 69, "endOffset": 73}, {"referenceID": 50, "context": "Another approach that incorporated spatial attention using CNN feature maps is presented in [51].", "startOffset": 92, "endOffset": 96}, {"referenceID": 61, "context": "To do this, they used a modified Dynamic Memory Network (DMN) [62].", "startOffset": 62, "endOffset": 66}, {"referenceID": 52, "context": "The Hierarchical Co-Attention model [53] applies attention to both the image and question to jointly reason about the two different streams of information.", "startOffset": 36, "endOffset": 40}, {"referenceID": 45, "context": "to the method used in Spatial Memory Network [46].", "startOffset": 45, "endOffset": 49}, {"referenceID": 54, "context": "In [55], Multimodal Compact Bilinear (MCB) pooling was proposed as a method for combining image and text features in VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "In a variation of this model, a soft-attention mechanism, similar to the method in [47], was also used, with the only major change being the use of MCB for combining text and question features instead of element-wise multiplication in [47].", "startOffset": 83, "endOffset": 87}, {"referenceID": 46, "context": "In a variation of this model, a soft-attention mechanism, similar to the method in [47], was also used, with the only major change being the use of MCB for combining text and question features instead of element-wise multiplication in [47].", "startOffset": 235, "endOffset": 239}, {"referenceID": 53, "context": "In [54], the authors proposed breaking the answering process into abstract sub-tasks by using several complete answering units in a recurrent fashion.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "Each answering unit on the chain is equipped with an attentive mechanism derived from [47] and a classifier.", "startOffset": 86, "endOffset": 90}, {"referenceID": 44, "context": "Another notable model is provided by [45], which incorporates a Dynamic Parameter Prediction layer into the fully connected layers of a CNN.", "startOffset": 37, "endOffset": 41}, {"referenceID": 51, "context": "In [52], Multimodal Residual Networks (MRN) were proposed for VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Their system is a modification of ResNet [2] to use both visual and question features in the residual mapping.", "startOffset": 41, "endOffset": 44}, {"referenceID": 47, "context": "In [48], the authors explored using external knowledge sources to improve VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 62, "context": "The external knowledge bases were tailored to general information obtained from DBpedia [63], so it is possible that using a source tailored to VQA could yield greater improvement.", "startOffset": 88, "endOffset": 92}, {"referenceID": 41, "context": "Neural Module Networks (NMN) are an especially interesting approach to VQA [42, 49].", "startOffset": 75, "endOffset": 83}, {"referenceID": 48, "context": "Neural Module Networks (NMN) are an especially interesting approach to VQA [42, 49].", "startOffset": 75, "endOffset": 83}, {"referenceID": 39, "context": "But, do current systems adequately use both vision and language? Ablation studies [40, 32] have routinely shown that question only models perform drastically better than image only models, especially on open-ended COCO-VQA.", "startOffset": 82, "endOffset": 90}, {"referenceID": 31, "context": "But, do current systems adequately use both vision and language? Ablation studies [40, 32] have routinely shown that question only models perform drastically better than image only models, especially on open-ended COCO-VQA.", "startOffset": 82, "endOffset": 90}, {"referenceID": 39, "context": "On COCO-QA, simple image-blind models that use only the question can achieve 50% accuracy with the gain from using the image being comparatively modest [40].", "startOffset": 152, "endOffset": 156}, {"referenceID": 39, "context": "In [40], it was also shown that for DAQUAR37, using a better language embedding with an imageblind model produced results superior to earlier works employing both images and questions.", "startOffset": 3, "endOffset": 7}, {"referenceID": 63, "context": "In [64], the authors studied a model that had been trained using both image and question features.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "To further study the role of language and images in VQA, we used the model from [44].", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "We observed similar results when using the system in [32].", "startOffset": 53, "endOffset": 57}, {"referenceID": 38, "context": "In [39], bias in VQA was studied using synthetic cartoon images.", "startOffset": 3, "endOffset": 7}, {"referenceID": 43, "context": "The left example uses the system in [44] and the right example uses the system from [40].", "startOffset": 36, "endOffset": 40}, {"referenceID": 39, "context": "The left example uses the system in [44] and the right example uses the system from [40].", "startOffset": 84, "endOffset": 88}, {"referenceID": 54, "context": "The current best VQA model for COCO-VQA does employ spatial visual attention [55], but simple models that do not use attention have been shown to exceed earlier models that used complex attentive mechanisms.", "startOffset": 77, "endOffset": 81}, {"referenceID": 64, "context": "In [65], for example, an attention-free model that used multiple global image feature representations (VGG-19, ResNet-101, and ResNet-152), instead of a single CNN, performed very well compared some attentive models.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "Combined with ensembling, this yielded results significantly higher than the complex attention-based models used in [47] and [51].", "startOffset": 116, "endOffset": 120}, {"referenceID": 50, "context": "Combined with ensembling, this yielded results significantly higher than the complex attention-based models used in [47] and [51].", "startOffset": 125, "endOffset": 129}, {"referenceID": 39, "context": "Similar results have been obtained by other systems that do not employ spatial attention [40, 55, 66].", "startOffset": 89, "endOffset": 101}, {"referenceID": 54, "context": "Similar results have been obtained by other systems that do not employ spatial attention [40, 55, 66].", "startOffset": 89, "endOffset": 101}, {"referenceID": 65, "context": "Similar results have been obtained by other systems that do not employ spatial attention [40, 55, 66].", "startOffset": 89, "endOffset": 101}, {"referenceID": 66, "context": "In [67], the authors showed that methods commonly used to incorporate spatial attention to specific image features do not cause models to attend to the same regions as humans tasked with VQA.", "startOffset": 3, "endOffset": 7}, {"referenceID": 46, "context": "They made this observation using both the attentive mechanisms used in [47] and [53].", "startOffset": 71, "endOffset": 75}, {"referenceID": 52, "context": "They made this observation using both the attentive mechanisms used in [47] and [53].", "startOffset": 80, "endOffset": 84}, {"referenceID": 43, "context": "Figure 11: Using the system in [44], the answer score for the question \u2018Are there any people in the picture?\u2019 is roughly the same for \u2018yes\u2019 (8.", "startOffset": 31, "endOffset": 35}, {"referenceID": 41, "context": "The SHAPES dataset [42] uses binary questions exclusively but contains complex questions involving spatial reasoning, counting, and drawing inferences (see Figure 6).", "startOffset": 19, "endOffset": 23}, {"referenceID": 38, "context": "Using cartoon images, [39] also showed that these questions can be especially difficult for VQA algorithms when the dataset is balanced.", "startOffset": 22, "endOffset": 26}, {"referenceID": 65, "context": "For example, in [66], they formulated VQA as an answer scoring task, where the system was trained to produce a score based on the image, question, and potential answers.", "startOffset": 16, "endOffset": 20}, {"referenceID": 67, "context": "Bias has long been a problem in images used for computer vision datasets (for a review see [68]), and for VQA this problem is compounded by bias in the questions as well.", "startOffset": 91, "endOffset": 95}], "year": 2016, "abstractText": "Visual Question Answering (VQA) is a recent problem in computer vision and natural language processing that has garnered a large amount of interest from the deep learning, computer vision, and natural language processing communities. In VQA, an algorithm needs to answer text-based questions about images. Since the release of the first VQA dataset in 2014, several additional datasets have been released and many algorithms have been proposed. In this review, we critically examine the current state of VQA in terms of problem formulation, existing datasets, evaluation metrics, and algorithms. In particular, we discuss the limitations of current datasets with regard to their ability to properly train and assess VQA algorithms. We then exhaustively review existing algorithms for VQA. Finally, we discuss possible future directions for VQA and image understanding research.", "creator": "LaTeX with hyperref package"}}}