{"id": "1610.01476", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "$\\ell_1$ Regularized Gradient Temporal-Difference Learning", "abstract": "In this paper, we study the Temporal Difference (TD) learning with linear value function approximation. It is well known that most TD learning algorithms are unstable with linear function approximation and off-policy learning. Recent development of Gradient TD (GTD) algorithms has addressed this problem successfully. However, the success of GTD algorithms requires a set of well chosen features, which are not always available. When the number of features is huge, the GTD algorithms might face the problem of overfitting and being computationally expensive. To cope with this difficulty, regularization techniques, in particular $\\ell_1$ regularization, have attracted significant attentions in developing TD learning algorithms. The present work combines the GTD algorithms with $\\ell_1$ regularization. We propose a family of $\\ell_1$ regularized GTD algorithms, which employ the well known soft thresholding operator. We investigate convergence properties of the proposed algorithms, and depict their performance with several numerical experiments.", "histories": [["v1", "Wed, 5 Oct 2016 15:15:27 GMT  (2981kb,D)", "http://arxiv.org/abs/1610.01476v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.LG", "authors": ["dominik meyer", "hao shen", "klaus diepold"], "accepted": false, "id": "1610.01476"}, "pdf": {"name": "1610.01476.pdf", "metadata": {"source": "CRF", "title": "`1 Regularized Gradient Temporal-Difference Learning", "authors": ["Dominik Meyer", "Hao Shen", "Klaus Diepold", "Meyer Shen Diepold"], "emails": ["dominik.meyer@tum.de", "hao.shen@tum.de", "kldi@tum.de"], "sections": [{"heading": null, "text": "Keywords: Reinforcement Learning (RL), linear function alignment, Gradient TemporalDifference (GTD) learning, Iterative Soft Thresholding (IST)."}, {"heading": "1. Introduction", "text": "In fact, most of them will be able to move to another world where they are able to integrate, \"he said.\" But it's not that they are able to move to another world where they are able to integrate. \""}, {"heading": "2. Notations and Preliminaries", "text": "In this thesis, we consider a RL process as a Markov decision-making process (MDP), defined as a tuple (S, A, P, r, \u03b3), where S is a set of possible environmental states, A is a set of actions of the active substance, P: S \u00b7 A \u00b7 S \u2192 [0, 1] the conditional transition probabilities P (s, a, s \u00b2) versus the transitions from a state s to a state s \u00b2 in the face of an action a, r: S \u2192 R a reward function that assigns an immediate reward r to a state, and g \u00b2 [0, 1] a discount factor."}, {"heading": "2.1. TD Learning with Linear Function Approximation", "text": "The goal of an RL agent is to learn how to assign states to actions, i.e. a political reward: S \u2192 A, which maximizes the value function. (1) \"1 Regularized GTD LearningIt is well known that for a given political estimate the value function V \u03c0 (s) fulfills the Bellman equation, i.e. V \u03c0 (s) = r (s) + \u03b3 s \u2032 P (s) - p (s), s \u00b2 - p (s). (2) The right side of (2) is often referred to as the Bellman operator for political equation, i.e. V \u03c0 (s) = r (s) + p (s), p (s), p (s), s \u00b2 - p (s) - p (s)."}, {"heading": "2.2. Three Objective Functions for TD Learning", "text": "In order to find an optimal parameter, an optimization process must be used to define an appropriate objective function that accurately measures the correctness of the current value adjustment, i.e., how far the current adjustment is from the actual TD solution. In this subsection, we recall three popular objective functions for TD learning. Motivated by the fact that the value adjustment is the Bellman operator's fixed point for a given policy, the correctness of an adjustment can be measured simply by the TD error itself, i.e., J1: Rk \u2192 R, J1 (\u03b8 - T VTB): 2 D = 1 2 (E [Java]) 2, (6) where D-R | | S | is a diagonal matrix whose components represent a certain state distribution. This cost function is often referred to as the Mean Squared Bellman Error (MSBE). Ideally, the minimum of the MSBE function is a good value adjustment."}, {"heading": "3. Stochastic Gradient Algorithms for `1 Regularized TD Learning", "text": "In the first part of this section, we present a general framework of gradient algorithms to minimize '1 regulated TD lens functions. In the second section, two' 1 regulated stochastic gradient TD algorithms are developed in online setting and their convergence properties are examined from the perspective of stochastic optimization."}, {"heading": "3.1. `1 Regularized TD Learning", "text": "Applying a \"1 regularizer\" to the parameter \"1\" results in the following objective functionality (\"1\"): = Ji (\"1,\" \"2,\" \"1,\" \"1,\" \"1,\" \"1,\" \"1,\" \"1,\" \"1,\" \"1,\" \"1,\" \"1,\" \"1,\" \"1,\" \"\" 1, \"\" \"1,\" \"\" 1, \"\" \"1,\" \"\" 1, \"\" \"1,\" \"). The iterative soft threshold (\" IST \") algorithm is nowadays a classic algorithm for minimizing the cost function (10). It can be interpreted as an extension of the classical gradient algorithm. Due to its high popularity, we will skip the derivation of the IST algorithm,\" and refer to Zibulevsky and Elad (2010) and the references thereto for further reading."}, {"heading": "3.2. Stochastic GTD-IST Algorithms", "text": "The TD-IST algorithms introduced in the previous subsection ALLO ALALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO algorithms ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALLO ALgorithms (STD-IST algorithms) algorithms (STD-IST algorithms) algorithms (STD-IST algorithms) algorithms (STD-IST-IST algorithms) algorithms (STD-IST-IST algorithms) algorithms (STD-IST algorithms). (STD-IST-IST-IST algorithms) algorithms) algorithms (STD-IST algorithms). (STD-IST-IST-IST algorithms) algorithms). (STD-IST algorithms) algorithms (STD-IST algorithms). (STD-IST-IST algorithms) algorithms) algorithms (STD-STD-ALgorithms)."}, {"heading": "4. Numerical Experiments", "text": "In this section, we examine the performance of our proposed \"1 regulated GTD algorithms\" versus two existing \"1 regulated TD algorithms, both in policy settings and outside policy settings."}, {"heading": "4.1. Experiment One: On-Policy Learning", "text": "In this experiment, we apply our proposed algorithms to a random walk problem in the chain environment, which consists of seven states. There is only one action, and the probability of the transition to the right or left is the same. A reward of one is assigned only in the extreme right state, i.e. in the final state, while the rewards elsewhere are zero. Characteristics consist of a binary encoding of the states and ten additional \"loud\" characteristics, which are simply Gaussian noise. Within this framework, we conduct three different experiments."}, {"heading": "4.1.1. Regularized vs. Un-regularized", "text": "This experiment compares the performance of the proposed \"1 regulated GTD algorithms with their unregulated counterparts. Figure 1 shows the learning curves of three GTD learning algorithms, namely GTD, GTD2 and TDC, along with their regulated versions. It is obvious that IST-based GTD algorithms each exceed their original unregulated versions. Experimental results show the effectiveness of IST-based GTD learning algorithms."}, {"heading": "4.1.2. Unfavorable Initializations", "text": "The second experiment examines the recovery behavior and convergence rate of our proposed algorithms in the case of unfavorable initializations. Here, we consider only the simple GTD-IST algorithm. The parameter vector \u03b8 is initialized to have ones for all noisy characteristics and zeros for all \"good\" characteristics. In other words, our experiment begins with the initialization of the selection of all \"bad\" characteristics. The results in Figure 2 show that the \"1 Regulated GTD Learning\" 1 algorithm, i.e. the GTD-IST algorithm with a different parameter value \u03b7, converges more quickly to the correct selection of characteristics than the original GTD algorithm."}, {"heading": "4.1.3. GTD-IST Algorithms vs. Others", "text": "In the third experiment, we compare the GTD-IST algorithms with the L1TD algorithm of Painter-Wakefield and Parr (2012b) and the LARS-TD algorithm of Kolter and Ng (2009). Results in Figure 3 (a) and 3 (b) indicate that all three GTD-IST algorithms consistently outperform the L1TD algorithm with or without noise. A closer look at the result in the zoomed-in window in Figure 3 (c) shows that the LARS-TD algorithm performs best when noise is present, which may be due to the fact that the LARSTD algorithm updates every 20 episodes using all available samples. Nevertheless, a timing experiment in Table 1 shows that the LARS-TD algorithm performs significantly slower than the other online algorithms DieMeyer Sphen."}, {"heading": "4.2. Experiment Two: Off-Policy Learning", "text": "To test the performance of the GTD-IST algorithms against non-political learning, we use the well-known star example proposed in Baird (1995). It consists of seven states, one of which is considered to be the \"center.\" In each of the outer states, the actor can choose between two actions: either the \"solid\" action, which most likely leads him into the central state, or the \"dotted\" action, which equally likely leads him into one of the other states. Reward for all state transitions is zero and the states are represented by tabular characteristics, as described in the original setting. We add 20 noisy \"Gaussian\" characteristics to the representation of the state. Behavioral policy selects the \"solid\" action with the probability of 1 / 7 and the \"dotted\" otherwise, while estimation policy always chooses the \"dotted\" action. The learning curves in Figure 4 show that both the GTDIST and GT2 algorithms consistently outperform their original DIST algorithms."}, {"heading": "5. Conclusions", "text": "We are investigating the convergence properties of the proposed algorithms from the perspective of stochastic optimization. Preliminary experiments show that the proposed family of GTD-IST algorithms exceeds all their original equivalents and two existing '1-regulated TD algorithms. Aware of advanced developments in the sparse representation community, we plan to apply further state-of-the-art algorithms of sparse representation to RL. Thus, IST algorithms are usually known as slow compared to other advanced' 1 minimization algorithms."}, {"heading": "Acknowledgements", "text": "This work was partially supported by the International Graduate School of Science and Engineering (IGSSE) of the Technical University of Munich. The authors thank Christopher Painter-Wakefield for providing the Matlab implementation of the L1TD algorithm."}], "references": [{"title": "Residual algorithms: Reinforcement learning with function approximation", "author": ["L. Baird"], "venue": "In Proceeding of the 12th International Conference on Machine Learning,", "citeRegEx": "Baird.,? \\Q1995\\E", "shortCiteRegEx": "Baird.", "year": 1995}, {"title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "author": ["A. Beck", "M. Teboulle"], "venue": "SIAM Journal on Imaging Sciences,", "citeRegEx": "Beck and Teboulle.,? \\Q2009\\E", "shortCiteRegEx": "Beck and Teboulle.", "year": 2009}, {"title": "Linear least-squares algorithms for temporal difference learning", "author": ["S.J. Bradtke", "A.G. Barto"], "venue": "Maching Learning,", "citeRegEx": "Bradtke and Barto.,? \\Q1996\\E", "shortCiteRegEx": "Bradtke and Barto.", "year": 1996}, {"title": "Sparsity and incoherence in compressive sampling", "author": ["E.J. Cand\u00e9s", "J. Romberg"], "venue": "Inverse Problems,", "citeRegEx": "Cand\u00e9s and Romberg.,? \\Q2007\\E", "shortCiteRegEx": "Cand\u00e9s and Romberg.", "year": 2007}, {"title": "Efficient online and batch learning using forward backward splitting", "author": ["J. Duchi", "Y. Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi and Singer.,? \\Q2009\\E", "shortCiteRegEx": "Duchi and Singer.", "year": 2009}, {"title": "Regularized policy iteration", "author": ["A.M. Farahmand", "M. Ghavamzadeh", "C. Szepesv\u00e1ri", "S. Mannor"], "venue": "Advances in Neural Information Processing Systems 21,", "citeRegEx": "Farahmand et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Farahmand et al\\.", "year": 2008}, {"title": "`1-penalized projected bellman residual", "author": ["M. Geist", "B. Scherrer"], "venue": "Recent Advances in Reinforcement Learning,", "citeRegEx": "Geist and Scherrer.,? \\Q2012\\E", "shortCiteRegEx": "Geist and Scherrer.", "year": 2012}, {"title": "A Dantzig selector approach to temporal difference learning", "author": ["M. Geist", "B. Scherrer", "A. Lazaric", "M. Ghavamzadeh"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Geist et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Geist et al\\.", "year": 2012}, {"title": "Regularized least squares temporal difference learning with nested `2 and `1 penalization", "author": ["M.W. Hoffman", "A. Lazaric", "M. Ghavamzadeh", "R. Munos"], "venue": "Recent Advances in Reinforcement Learning,", "citeRegEx": "Hoffman et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Hoffman et al\\.", "year": 2012}, {"title": "Linear complementarity for regularized policy evaluation and improvement", "author": ["J. Johns", "C. Painter-Wakefield", "R. Parr"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Johns et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Johns et al\\.", "year": 2010}, {"title": "Automatic basis function construction for approximate dynamic programming and reinforcement learning", "author": ["P.W. Keller", "S. Mannor", "D. Precup"], "venue": "In Proceedings of the 23rd International Conference on Machine Learning", "citeRegEx": "Keller et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Keller et al\\.", "year": 2006}, {"title": "Regularization and feature selection in least-squares temporal difference learning", "author": ["J.Z. Kolter", "A.Y. Ng"], "venue": "In Proceedings of the 26th International Conference on Machine Learning (ICML", "citeRegEx": "Kolter and Ng.,? \\Q2009\\E", "shortCiteRegEx": "Kolter and Ng.", "year": 2009}, {"title": "Sparse temporal difference learning using lasso", "author": ["M. Loth", "M. Davy", "P. Preux"], "venue": "In Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning,", "citeRegEx": "Loth et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Loth et al\\.", "year": 2007}, {"title": "Greedy algorithms for sparse reinforcement learning", "author": ["C. Painter-Wakefield", "R. Parr"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "citeRegEx": "Painter.Wakefield and Parr.,? \\Q2012\\E", "shortCiteRegEx": "Painter.Wakefield and Parr.", "year": 2012}, {"title": "L1 regularized linear temporal difference learning", "author": ["C. Painter-Wakefield", "R. Parr"], "venue": "Technical report,", "citeRegEx": "Painter.Wakefield and Parr.,? \\Q2012\\E", "shortCiteRegEx": "Painter.Wakefield and Parr.", "year": 2012}, {"title": "Reinforcement Learning: An Introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "Sutton and Barto.,? \\Q1998\\E", "shortCiteRegEx": "Sutton and Barto.", "year": 1998}, {"title": "A convergent O(n) algorithm for offpolicy temporal-difference learning with linear function approximations", "author": ["R.S. Sutton", "Csaba Szepesv\u00e1ri", "H.R. Maei"], "venue": "Advances in Neural Information Processing Systems 21,", "citeRegEx": "Sutton et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2008}, {"title": "Fast gradient-descent methods for temporal-difference learning with linear function approximation", "author": ["R.S. Sutton", "H.R. Maei", "D. Precup", "S. Bhatnagar", "D. Silver", "C. Szepesv\u00e1ri", "E. Wiewiora"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Sutton et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Sutton et al\\.", "year": 2009}, {"title": "Kernelized value function approximation for reinforcement learning", "author": ["G. Taylor", "R. Parr"], "venue": "In Proceedings of the 26th International Conference on Machine Learning", "citeRegEx": "Taylor and Parr.,? \\Q2009\\E", "shortCiteRegEx": "Taylor and Parr.", "year": 2009}, {"title": "L1-L2 optimization in signal and image processing", "author": ["M. Zibulevsky", "M. Elad"], "venue": "IEEE Signal Processing Magazine,", "citeRegEx": "Zibulevsky and Elad.,? \\Q2010\\E", "shortCiteRegEx": "Zibulevsky and Elad.", "year": 2010}], "referenceMentions": [{"referenceID": 13, "context": "Sutton and Barto (1998). In the general setting with large or infinite state space, exact representation of the actual value function is often inhibitively computationally expensive or hardly possible.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Bradtke and Barto (1996); Keller et al.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Bradtke and Barto (1996); Keller et al. (2006), or kernel-based approaches, e.", "startOffset": 0, "endOffset": 47}, {"referenceID": 2, "context": "Bradtke and Barto (1996); Keller et al. (2006), or kernel-based approaches, e.g. Taylor and Parr (2009). A common approach generates firstly a vast number of features, which is often much larger than the number of available samples, and then chooses automatically relevant features to approximate the actual value function.", "startOffset": 0, "endOffset": 104}, {"referenceID": 3, "context": "Farahmand et al. (2008), in this work we focus on `1 regularization.", "startOffset": 0, "endOffset": 24}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.", "startOffset": 0, "endOffset": 25}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al.", "startOffset": 0, "endOffset": 174}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al.", "startOffset": 0, "endOffset": 196}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al.", "startOffset": 0, "endOffset": 217}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al.", "startOffset": 0, "endOffset": 244}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf.", "startOffset": 0, "endOffset": 267}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Cand\u00e9s and Romberg (2007); Zibulevsky and Elad (2010).", "startOffset": 0, "endOffset": 498}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Cand\u00e9s and Romberg (2007); Zibulevsky and Elad (2010). Very recently, two advanced `1 minimization algorithms have been adapted to the TD learning, i.", "startOffset": 0, "endOffset": 526}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Cand\u00e9s and Romberg (2007); Zibulevsky and Elad (2010). Very recently, two advanced `1 minimization algorithms have been adapted to the TD learning, i.e. the Dantzig selector based TD algorithm from Geist et al. (2012) and the orthogonal matching pursuit based TD algorithm developed in Painter-Wakefield and Parr (2012a).", "startOffset": 0, "endOffset": 690}, {"referenceID": 2, "context": "Bradtke and Barto (1996). Recent active researches on applying `1 regularization to TD learning have led to a various number of effective algorithms, e.g. Loth et al. (2007); Kolter and Ng (2009); Johns et al. (2010); Geist and Scherrer (2012); Hoffman et al. (2012). It is important to notice that `1 minimization has been extensively studied in the areas of compressed sensing and image processing, and many efficient `1 minimization algorithms have been developed, cf. Cand\u00e9s and Romberg (2007); Zibulevsky and Elad (2010). Very recently, two advanced `1 minimization algorithms have been adapted to the TD learning, i.e. the Dantzig selector based TD algorithm from Geist et al. (2012) and the orthogonal matching pursuit based TD algorithm developed in Painter-Wakefield and Parr (2012a). On the other hand, most TD learning algorithms are known to be unstable with linear value function approximation and off-policy learning.", "startOffset": 0, "endOffset": 793}, {"referenceID": 16, "context": "The function J3 is referred to as the Norm of Expected TD Update (NEU), which is used to derive the original GTD algorithm in Sutton et al. (2008).", "startOffset": 126, "endOffset": 147}, {"referenceID": 19, "context": "Due to its high popularity, we skip the derivation of the IST algorithm, and refer to Zibulevsky and Elad (2010) and the references therein for further reading.", "startOffset": 86, "endOffset": 113}, {"referenceID": 13, "context": "Note that IST has been employed in developing fixed point TD algorithms in Painter-Wakefield and Parr (2012b), whereas in this work we focus on developing intrinsic gradient TD algorithm.", "startOffset": 75, "endOffset": 110}, {"referenceID": 4, "context": "To investigate convergence properties of the proposed algorithms requires results from Duchi and Singer (2009), which develops a general framework for analyzing empirical loss minimization with regularizations.", "startOffset": 87, "endOffset": 111}, {"referenceID": 4, "context": "To investigate convergence properties of the proposed algorithms requires results from Duchi and Singer (2009), which develops a general framework for analyzing empirical loss minimization with regularizations. We adapt the result in corollary 10 from Duchi and Singer (2009) to our current setting as follows.", "startOffset": 87, "endOffset": 276}, {"referenceID": 4, "context": "To investigate convergence properties of the proposed algorithms requires results from Duchi and Singer (2009), which develops a general framework for analyzing empirical loss minimization with regularizations. We adapt the result in corollary 10 from Duchi and Singer (2009) to our current setting as follows. Theorem 1 Let the function J : Rk \u2192 R be smooth and strictly convex and \u03b8\u2217 \u2208 Rk be the global minimum of the function F (\u03b8) := J(\u03b8) + \u03b7\u2016\u03b8\u20161 with \u03b7 > 0. If the following three conditions hold: (1) \u03b8\u2217 fulfills \u2016\u03b8t \u2212 \u03b8\u20162 \u2264 d for some constant d > 0; (2) \u2016\u2207J(\u03b8t)\u20162 \u2264 g for some constant g > 0; and (3) a stochastic estimate of the gradient \u2207\u0303J(\u03b8t) fulfills E[\u2207\u0303J(\u03b8t)] = \u2207J(\u03b8t), then IST based stochastic algorithms converge with probability one to \u03b8\u2217. Let us look at the `1 regularized NEU function F3 first. Recall the approximate stochastic gradient update, developed in Sutton et al. (2008), as \u2207\u0303J3(\u03b8t) = (\u03c6t ut)(\u03b3\u03c6t \u2212 \u03c6t), (15)", "startOffset": 87, "endOffset": 901}, {"referenceID": 16, "context": "Sutton et al. (2008), we have", "startOffset": 0, "endOffset": 21}, {"referenceID": 16, "context": "In order to minimize the MSPBE function J2, two efficient GTD algorithms are developed in Sutton et al. (2009). Their approximate stochastic updates are defined as", "startOffset": 90, "endOffset": 111}, {"referenceID": 16, "context": "Sutton et al. (2009), we get", "startOffset": 0, "endOffset": 21}, {"referenceID": 12, "context": "Others In the third experiment, we compare the GTD-IST algorithms with the L1TD algorithm from Painter-Wakefield and Parr (2012b) and the LARS-TD algorithm from Kolter and Ng (2009).", "startOffset": 95, "endOffset": 130}, {"referenceID": 11, "context": "Others In the third experiment, we compare the GTD-IST algorithms with the L1TD algorithm from Painter-Wakefield and Parr (2012b) and the LARS-TD algorithm from Kolter and Ng (2009). Results in both Figure 3(a) and 3(b) imply that, with or without noise, all three GTD-IST algorithms outperforms the L1TD algorithm consistently.", "startOffset": 161, "endOffset": 182}, {"referenceID": 0, "context": "Experiment Two: Off-Policy Learning To test the performance of the GTD-IST algorithms on the off-policy learning, we employ the well-known star example, proposed in Baird (1995). It consists of seven states with one state being considered as the \u201ccenter\u201d.", "startOffset": 165, "endOffset": 178}, {"referenceID": 1, "context": "Applying more efficient `1 minimization algorithms, such as Beck and Teboulle (2009), to TD learning are of great interests as the future work.", "startOffset": 60, "endOffset": 85}], "year": 2016, "abstractText": "In this paper, we study the Temporal Difference (TD) learning with linear value function approximation. It is well known that most TD learning algorithms are unstable with linear function approximation and off-policy learning. Recent development of Gradient TD (GTD) algorithms has addressed this problem successfully. However, the success of GTD algorithms requires a set of well chosen features, which are not always available. When the number of features is huge, the GTD algorithms might face the problem of overfitting and being computationally expensive. To cope with this difficulty, regularization techniques, in particular `1 regularization, have attracted significant attentions in developing TD learning algorithms. The present work combines the GTD algorithms with `1 regularization. We propose a family of `1 regularized GTD algorithms, which employ the well known soft thresholding operator. We investigate convergence properties of the proposed algorithms, and depict their performance with several numerical experiments.", "creator": "LaTeX with hyperref package"}}}