{"id": "1610.01520", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database", "abstract": "Word embeddings have been extensively studied in large text datasets. However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies. In the present paper, we compare Skip-gram and LSA capabilities in this scenario, and we test both techniques to extract relevant semantic patterns in single-series dreams reports. LSA showed better performance than Skip-gram in small size training corpus in two semantic tests. As a study case, we show that LSA can capture relevant words associations in dream reports series, even in cases of small number of dreams or low-frequency words. We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this classic research area of psychology", "histories": [["v1", "Wed, 5 Oct 2016 16:47:17 GMT  (4567kb,D)", "http://arxiv.org/abs/1610.01520v1", null], ["v2", "Tue, 11 Apr 2017 15:43:33 GMT  (4567kb,D)", "http://arxiv.org/abs/1610.01520v2", null]], "reviews": [], "SUBJECTS": "cs.CL cs.IR", "authors": ["edgar altszyler", "mariano sigman", "sidarta ribeiro", "diego fern\\'andez slezak"], "accepted": false, "id": "1610.01520"}, "pdf": {"name": "1610.01520.pdf", "metadata": {"source": "CRF", "title": "Comparative study of LSA vs Word2vec embeddings in small corpora: a case study in dreams database", "authors": ["Edgar Altszyler", "Mariano Sigman", "Diego Fern\u00e1ndez Slezak"], "emails": ["ealtszyler@dc.uba.ar"], "sections": [{"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Methods", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Semantic representations", "text": "Both LSA and Word2vec were generated semantically using the Gensim Python library [28]. In the LSA implementation, a tf-idf transformation was applied prior to the truncated singular value decomposition, and the display dimensionality of the LSA was tuned to maximize performance in each case. No minimum frequency threshold was used in Word2vec (Skip-gram) implementations, and the window size, number of negative samples, and display dimensions were adjusted to maximize performance. All other Skip-gram parameters were set to Gensim values. In vector representation, the semantic similarity (S) of two words was calculated based on the cosmic similarity between their respective vector representations (v1, v2), S (v1, v2) = cos (v1, v2) and v1 (v2)."}, {"heading": "2.2 Semantic tests", "text": "To compare the semantic representation quality of LSA and Skip-gram, we perform two tests in two different corpora (TASA and UkWaC): (1) a semantic categorization test and (2) a word pair similarity test. For each test, we examined how the performance of LSA and Skip-gram embedding depends on the body size. To do this, we take 6 nested subsamples of the training corpus, in which documents were gradually eliminated according to [29, 30]. In both cases, the minimum body size contains only 600 documents. If one of the test words did not appear at least once in a subcorpus, a random one was replaced by one of the discarded documents."}, {"heading": "2.2.1 Semantic categorization test", "text": "In this test, we measured the model's ability to represent the semantic categories [31, 29] (such as drinks, countries, tools and clothing), and the test consisted of 53 categories of 10 words each. To measure how well the word i is vis\u00e0-grouped against the other words in its semantic category, we used the silhouette coefficients, s (i) [32], s (i) = b (i) \u2212 a (i) max {a (i), b (i)}, (2) where a (i) is the mean distance of the word i to all other words within the same category, and b (i) the minimum mean distance of the word i to any word within another category (i.e. the mean distance to the adjacent category). In other words, silhouette coefficients measure how close a word is to other words within the same category, compared with words of the nearest category. The silhouette score is calculated as the mean of all silhouette coefficients."}, {"heading": "2.2.2 Word-pairs similarity test", "text": "This test measures the model's ability to detect semantic similarities between concepts. We used the well-established test collection WordSim353 [33], which consists of 353 pairs of words (such as Maradona football or physics chemistry) associated with an average human-assigned similarity value. Each pair of words is rated on a scale from 0 (very different words) to 10 (very similar words), which is calculated as a Spearman correlation between human results and the semantic similarities of the model."}, {"heading": "2.3 Case study: Semantic association in dreams reports", "text": "In this case study, we analyze the models \"ability to grasp semantic word associations by testing whether the embedded models can capture the semantic neighborhood of a target word in a single subject's dream series (a collection of dream stories written by the same person). Specifically, we selected the word run as the target word and focused on recognizing its distance from the escape / persecution context. The rank distance of a given word\" w \"in terms of run was measured as the rank of\" w \"among the cosmic similarities between run and all other words in the vocabulary. If a word has a rank of 20, it means that of all words in the vocabulary, it is the 20th next word to use a cosmic similarity metric. Finally, we define the ranking distance of escape / chase concepts as the minimum value within the words escape / chase, chase concepts that we are within the ranks of escape * and chase valding."}, {"heading": "2.4 Corpora", "text": "In both tests, we used the TASA corpus [34] and a random sub-sample of the ukWaC corpus [35] as our training corpus. The TASA corpus is a commonly used linguistic corpus consisting of 37k educational texts with a body size of 5M words in its purified form. UkWaC consists of web site material from the.uk. The random sub-sample contains 140k documents with a body size of 57M words in its purified form. For the case study, we used the corpus of Dreambank reports [36, 23]. The DreamBank corpus consists of 19k dream reports from 59 subjects containing approximately 1.3 M words in its purified form. To clean the corpus, we performed word tokenization by discarding punctuation marks and symbols. We then converted each word into lowercase letters and stopwords, using the K in the stoplist."}, {"heading": "3 Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Corpus size analysis in the clustering test", "text": "As a first step for all analyses, we performed the optimization of the Skip-gram parameters for both tests (Table 1). The best values were selected to perform the body size analysis. In the case of the TASA corpus, neg = 15 was chosen because it showed a slightly better performance. To compare the quality of the embedding of LSA and Skip-gram into small corpus, we tested both methods in randomly nested subsamples of the TASA and ukWaC corpus (see Figure 1). Since the corresponding embedding dimensions depend on the body size [38], for each partial corpus, we performed the models with a wide range of dimensional values (7.15.25.50.100,200.400), using in each case the dimension that produces the best performance. Figure 1 shows that the Skip-gram word recognition rate tends to be greater than the SA as the LSA is coached with a smaller corpus, while we believe that this Skip-gram parameter is a better one than the number that we will produce."}, {"heading": "3.2 Case study: semantic association in dreams report", "text": "To verify the expected differences between the associations of the word run in dreams and wake life, we built LSA and Skip-gram embeddings trained each corpora, and we extracted the 25 words that resemble the run in each case. Rare words that appear less than 15 times were excluded. We found that word embeddings are able to identify differences in the patterns of use of the word between dreams and wake life. In TASA and ukWaC corpora, run is linked with words associated with a large variety of contexts, as sports, means of transport and programming, while in dreams, run is directly related with words with chase / escape situations. For example, with LSA trained in dreams we received words such as: chase, scream scream scream, escape, cry, cry, nazi, chased, yells, yells, safety, devil, evil, killing, slam and yell."}, {"heading": "3.3 Conclusion", "text": "To achieve this, we first tested the models \"ability to represent semantic categories (such as beverages, countries, tools, or clothing) in nested subsamples of a medium-sized body. We found that Word2vec embedding is better than LSA embedding when the models are trained with medium-size datasets [17, 18, 19]. However, we believe that when the body size is reduced, the performance of Word2vec greatly decreases, making LSA the more appropriate tool. This finding provides new insight into the discussion of predictive versus counter-based models [17, 18, 19]. We believe that the performance reduction of Word2vec in small businesses is based on the fact that predictive models require a lot of training data to adjust their high number of parameters."}, {"heading": "Acknowledgments", "text": "We would like to thank the teams behind the projects TASA [34], WaCky [35] and Dreambank [23] for providing us with the corpora and Eduardo Schmidt for helpful discussions. Conflicts of Interest The authors declare that there is no conflict of interest with regard to the publication of this paper."}], "references": [{"title": "Word Distributional structure", "author": ["Z. Harris"], "venue": "23(10):146\u2013162", "citeRegEx": "1", "shortCiteRegEx": null, "year": 1954}, {"title": "Semantic Compositionality through Recursive Matrix-Vector Spaces", "author": ["Richard Socher", "Brody Huval", "Christopher D Manning", "Andrew Y Ng"], "venue": "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2012}, {"title": "Automated analysis of free speech predicts psychosis onset in high-risk youths", "author": ["Gillinder Bedi", "Facundo Carrillo", "Guillermo A. Cecchi", "Diego Fern\u00e1ndez Slezak", "Mariano Sigman", "Nat\u00e1lia B. Mota", "Sidarta Ribeiro", "Daniel C. Javitt", "Mauro Copelli", "Cheryl M. Corcoran"], "venue": "npj Schizophrenia,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Identifying Issue Frames in Text", "author": ["Eyal Sagi", "Daniel Diermeier", "Stefan Kaufmann"], "venue": "PLoS ONE,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Scale-invariant transition probabilities in free word association trajectories", "author": ["Martin Elias Costa", "Flavia Bonomo", "Mariano Sigman"], "venue": "Frontiers in integrative neuroscience,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2009}, {"title": "A quantitative philology of introspection", "author": ["Carlos G. Diuk", "D. Fernandez Slezak", "I. Raskovsky", "M. Sigman", "G. a. Cecchi"], "venue": "Frontiers in Integrative Neuroscience,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Lsa as a theory of meaning", "author": ["Thomas K Landauer"], "venue": "Handbook of latent semantic analysis,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2007}, {"title": "Statistically significant detection of linguistic change", "author": ["Vivek Kulkarni", "Rami Al-Rfou", "Bryan Perozzi", "Steven Skiena"], "venue": "Proceedings of the 24th international conference on World Wide Web (WWW", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Indexing by latent semantic analysis", "author": ["Scott Deerwester", "Susan T Dumais", "Thomas Landauer", "George Furnas", "Richard. Harshman"], "venue": "JAsIs,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1990}, {"title": "A solution to Plato\u2019s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K. Landauer", "Susan T. Dumais"], "venue": "Psychological Review,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 1997}, {"title": "a Graesser", "author": ["X Hu", "Z Cai", "P Wiemer-Hastings"], "venue": "and D McNamara. Strengths, limitations, and extensions of LSA. Handbook of Latent Semantic Analysis, pages 401\u2013426", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Improving the retrieval of information from external sources", "author": ["Susan Dumais"], "venue": "Behavior Research Methods, Instruments, & Computers,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1991}, {"title": "A Unified Architecture for Natural Language Processing : Deep Neural Networks with Multitask Learning", "author": ["Ronan Collobert", "Jason Weston"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2008}, {"title": "Distributed Representations of Words and Phrases and their Compositionality", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "Nips, pages", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["Tomas Mikolov", "Greg Corrado", "Kai Chen", "Jeffrey Dean"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Don\u2019t count , predict ! A systematic comparison of context-counting vs . context-predicting semantic vectors", "author": ["Marco Baroni", "Georgiana Dinu", "German Kruszewski"], "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics.,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2014}, {"title": "Neural Word Embedding as Implicit Matrix Factorization", "author": ["Omer Levy", "Yoav Goldberg"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Improving Distributional Similarity with Lessons Learned from Word Embeddings", "author": ["Omer Levy", "Yoav Goldberg", "Ido Dagan"], "venue": "Transactions of the Association for Computational Linguistics,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Semantically aware time evolution tracking of communities in co-authorship networks", "author": ["Dionisios N Sotiropoulos", "Demitrios E. Pournarakis", "George M Giaglis"], "venue": "Proceedings of the 19th Panhellenic Conference on Informatics - PCI", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "The personality of a child molester: An analysis of dreams", "author": ["Alan Paul Bell", "Calvin Springer Hall"], "venue": "Transaction Publishers,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Using content analysis to study dreams: applications and implications for the humanities", "author": ["G.W. Domhoff"], "venue": "Bulkeley (Ed.), New York: Palgrave.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Studying dream content using the archive and search engine on DreamBank.net", "author": ["G. William Domhoff", "Adam Schneider"], "venue": "Consciousness and Cognition,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2008}, {"title": "Medication and dreams: Changes in dream content after drug treatment. Dreaming", "author": ["Nili T Kirschner"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Methods and measures for the study of dream content", "author": ["G William Domhoff"], "venue": "Principles and practices of sleep medicine, 3:463\u2013471", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2000}, {"title": "The typical dreams", "author": ["Tore A. Nielsen", "Antonio L. Zadra", "Valerie Simard", "Sebastien Saucier", "Philippe Stenstrom", "Carlyle Smith", "Don Kuiken"], "venue": "Canadian University students. Dreaming,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2003}, {"title": "The universality of typical dreams: Japanese vs. Americans", "author": ["Rm Griffith", "O Miyagi", "A Tago"], "venue": "American Anthropologist,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1958}, {"title": "Software Framework for Topic Modelling with Large Corpora", "author": ["Radim Rehuek", "Petr Sojka"], "venue": "In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2010}, {"title": "Extracting semantic representations from word co-occurrence statistics: a computational study", "author": ["John A Bullinaria", "Joseph P Levy"], "venue": "Behavior research methods,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2007}, {"title": "Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd", "author": ["John A Bullinaria", "Joseph P Levy"], "venue": "Behavior research methods,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2012}, {"title": "Extracting semantic representations from large text corpora", "author": ["Malti Patel", "John A. Bullinaria", "Joseph P Levy"], "venue": "Proceedings of the 4th Neural Computation and Psychology Workshop,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1997}, {"title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis", "author": ["Peter J. Rousseeuw"], "venue": "Journal of Computational and Applied Mathematics,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 1987}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin"], "venue": "In Proceedings of the 10th international conference on World Wide Web,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2001}, {"title": "and R", "author": ["S. Zeno", "S. Ivens"], "venue": "Millard, R.and Duvvuri. The educator\u2019s word frequency guide. Brewster", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1995}, {"title": "The wacky wide web: a collection of very large linguistically processed web-crawled corpora", "author": ["Marco Baroni", "Silvia Bernardini", "Adriano Ferraresi", "Eros Zanchetta"], "venue": "Language resources and evaluation,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2009}, {"title": "Dreambank", "author": ["A. Schneider", "G. William Domhoff"], "venue": "http://www.dreambank.net/, last accessed: Sep. 12, 2016", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Natural language processing with Python", "author": ["Steven Bird", "Ewan Klein", "Edward Loper"], "venue": null, "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Automatic estimation of the lsa dimension", "author": ["Jorge Fernandes", "Andreia Art\u00edfice", "Manuel J Fonseca"], "venue": "In KDIR,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2011}], "referenceMentions": [{"referenceID": 0, "context": "These methods rely in the idea that words with similar meanings tend to occur in similar contexts [1].", "startOffset": 98, "endOffset": 101}, {"referenceID": 1, "context": "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 2, "context": "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].", "startOffset": 123, "endOffset": 129}, {"referenceID": 4, "context": "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].", "startOffset": 123, "endOffset": 129}, {"referenceID": 5, "context": "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].", "startOffset": 141, "endOffset": 144}, {"referenceID": 6, "context": "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].", "startOffset": 164, "endOffset": 167}, {"referenceID": 7, "context": "Word embeddings has been used in a wide variety of applications such as sentiment analysis [2], psychiatry [3], psychology [4, 5], philology [6], cognitive science [7] and social science [8, 9].", "startOffset": 187, "endOffset": 193}, {"referenceID": 8, "context": "Latent Semantic Analysis (LSA) [10, 11, 12], is one of the most used methods for word meaning representation.", "startOffset": 31, "endOffset": 43}, {"referenceID": 9, "context": "Latent Semantic Analysis (LSA) [10, 11, 12], is one of the most used methods for word meaning representation.", "startOffset": 31, "endOffset": 43}, {"referenceID": 10, "context": "Latent Semantic Analysis (LSA) [10, 11, 12], is one of the most used methods for word meaning representation.", "startOffset": 31, "endOffset": 43}, {"referenceID": 11, "context": "Typically, normalization is applied to reduce the weight of uninformative high-frequency words in the words-documents matrix [13].", "startOffset": 125, "endOffset": 129}, {"referenceID": 12, "context": "More recently, neural-network language embeddings have received an increasing attention [14, 15], leaving aside classical word representation methods such as LSA.", "startOffset": 88, "endOffset": 96}, {"referenceID": 13, "context": "More recently, neural-network language embeddings have received an increasing attention [14, 15], leaving aside classical word representation methods such as LSA.", "startOffset": 88, "endOffset": 96}, {"referenceID": 13, "context": "In particular, Word2vec models [15, 16] have become especially popular in embeddings generation.", "startOffset": 31, "endOffset": 39}, {"referenceID": 14, "context": "In particular, Word2vec models [15, 16] have become especially popular in embeddings generation.", "startOffset": 31, "endOffset": 39}, {"referenceID": 14, "context": "In the present paper, we use Skip-gram model, which shows better performance in [16] semantic task.", "startOffset": 80, "endOffset": 84}, {"referenceID": 15, "context": "Although prediction-based models have strongly increased in popularity, it is not clear whether they outperform classical counter-based models [17, 18, 19].", "startOffset": 143, "endOffset": 155}, {"referenceID": 16, "context": "Although prediction-based models have strongly increased in popularity, it is not clear whether they outperform classical counter-based models [17, 18, 19].", "startOffset": 143, "endOffset": 155}, {"referenceID": 17, "context": "Although prediction-based models have strongly increased in popularity, it is not clear whether they outperform classical counter-based models [17, 18, 19].", "startOffset": 143, "endOffset": 155}, {"referenceID": 7, "context": "Moreover, this kind of approach is also relevant in sociological and linguistic research, in which linguistic patterns in word meaning networks are tracked along the time line, and small time chunks are needed [9, 20].", "startOffset": 210, "endOffset": 217}, {"referenceID": 18, "context": "Moreover, this kind of approach is also relevant in sociological and linguistic research, in which linguistic patterns in word meaning networks are tracked along the time line, and small time chunks are needed [9, 20].", "startOffset": 210, "endOffset": 217}, {"referenceID": 19, "context": "Dream content show gender and cultural differences, consistency over time of the dreams content, and concordance of dreaming features (such as activity and emotions) with waking-life experiences [21, 22, 23].", "startOffset": 195, "endOffset": 207}, {"referenceID": 20, "context": "Dream content show gender and cultural differences, consistency over time of the dreams content, and concordance of dreaming features (such as activity and emotions) with waking-life experiences [21, 22, 23].", "startOffset": 195, "endOffset": 207}, {"referenceID": 21, "context": "Dream content show gender and cultural differences, consistency over time of the dreams content, and concordance of dreaming features (such as activity and emotions) with waking-life experiences [21, 22, 23].", "startOffset": 195, "endOffset": 207}, {"referenceID": 22, "context": "Also, there is evidence of change in dreams contents after drug treatment [24] and shifts in content patterns in people with psychiatric disorders [25].", "startOffset": 74, "endOffset": 78}, {"referenceID": 23, "context": "Also, there is evidence of change in dreams contents after drug treatment [24] and shifts in content patterns in people with psychiatric disorders [25].", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "Most of the newest dreams content analysis methods are based on frequency wordcounting of predefined categories in dreams reports [23].", "startOffset": 130, "endOffset": 134}, {"referenceID": 24, "context": "For example, run may be associated to sports activities or with chase/escape situations, which is reported to be one of the most typical dreams [26, 27].", "startOffset": 144, "endOffset": 152}, {"referenceID": 25, "context": "For example, run may be associated to sports activities or with chase/escape situations, which is reported to be one of the most typical dreams [26, 27].", "startOffset": 144, "endOffset": 152}, {"referenceID": 26, "context": "1 Semantic representations Both, LSA and Word2vec semantic representations were generated with the Gensim Python library [28].", "startOffset": 121, "endOffset": 125}, {"referenceID": 27, "context": "To do this, we take 6 nested sub-samples of the training corpora, in which documents where progressively eliminated, following [29, 30].", "startOffset": 127, "endOffset": 135}, {"referenceID": 28, "context": "To do this, we take 6 nested sub-samples of the training corpora, in which documents where progressively eliminated, following [29, 30].", "startOffset": 127, "endOffset": 135}, {"referenceID": 29, "context": "In this test we measured the capabilities of the model to represent the semantic categories [31, 29] (such as, drinks, countries, tools and clothes).", "startOffset": 92, "endOffset": 100}, {"referenceID": 27, "context": "In this test we measured the capabilities of the model to represent the semantic categories [31, 29] (such as, drinks, countries, tools and clothes).", "startOffset": 92, "endOffset": 100}, {"referenceID": 30, "context": "In order to measure how well the word i is grouped vis\u00e0-vis the other words in its semantic category we used the Silhouette Coefficients, s(i) [32],", "startOffset": 143, "endOffset": 147}, {"referenceID": 31, "context": "We used the well established WordSim353 test collection [33], which consist of 353 word-pairs (such as Maradona-football or physics-chemistry) associated with a mean human-assigned similarity score.", "startOffset": 56, "endOffset": 60}, {"referenceID": 32, "context": "4 Corpora In both test, we use as training corpora the TASA corpus [34] and a random subsample of ukWaC corpus [35].", "startOffset": 67, "endOffset": 71}, {"referenceID": 33, "context": "4 Corpora In both test, we use as training corpora the TASA corpus [34] and a random subsample of ukWaC corpus [35].", "startOffset": 111, "endOffset": 115}, {"referenceID": 34, "context": "For the case study we use the Dreambank reports corpus [36, 23].", "startOffset": 55, "endOffset": 63}, {"referenceID": 21, "context": "For the case study we use the Dreambank reports corpus [36, 23].", "startOffset": 55, "endOffset": 63}, {"referenceID": 35, "context": "Then, we transformed each word to lowercase and eliminated stopwords, using the stoplist in NLTK Python package [37].", "startOffset": 112, "endOffset": 116}, {"referenceID": 36, "context": "Given that the appropriate embeddings dimensions depends on the corpus size [38], for each sub-corpus, we ran the models with a wide range of dimension values", "startOffset": 76, "endOffset": 80}, {"referenceID": 15, "context": "This finding gives a new insight into the prediction-based vs counter-based models discussion [17, 18, 19].", "startOffset": 94, "endOffset": 106}, {"referenceID": 16, "context": "This finding gives a new insight into the prediction-based vs counter-based models discussion [17, 18, 19].", "startOffset": 94, "endOffset": 106}, {"referenceID": 17, "context": "This finding gives a new insight into the prediction-based vs counter-based models discussion [17, 18, 19].", "startOffset": 94, "endOffset": 106}, {"referenceID": 19, "context": "This research field addresses questions such as \u201cwhat do we dream about?\u201d and \u201chow do gender, cultural background and waking life experiences shape the dreams content?\u201d [21, 25, 22, 23].", "startOffset": 169, "endOffset": 185}, {"referenceID": 23, "context": "This research field addresses questions such as \u201cwhat do we dream about?\u201d and \u201chow do gender, cultural background and waking life experiences shape the dreams content?\u201d [21, 25, 22, 23].", "startOffset": 169, "endOffset": 185}, {"referenceID": 20, "context": "This research field addresses questions such as \u201cwhat do we dream about?\u201d and \u201chow do gender, cultural background and waking life experiences shape the dreams content?\u201d [21, 25, 22, 23].", "startOffset": 169, "endOffset": 185}, {"referenceID": 21, "context": "This research field addresses questions such as \u201cwhat do we dream about?\u201d and \u201chow do gender, cultural background and waking life experiences shape the dreams content?\u201d [21, 25, 22, 23].", "startOffset": 169, "endOffset": 185}, {"referenceID": 32, "context": "We want to thank the teams behind the TASA [34], WaCky [35] and Dreambank [23] projects for providing us the corpora and Eduardo Schmidt for helpful discussions.", "startOffset": 43, "endOffset": 47}, {"referenceID": 33, "context": "We want to thank the teams behind the TASA [34], WaCky [35] and Dreambank [23] projects for providing us the corpora and Eduardo Schmidt for helpful discussions.", "startOffset": 55, "endOffset": 59}, {"referenceID": 21, "context": "We want to thank the teams behind the TASA [34], WaCky [35] and Dreambank [23] projects for providing us the corpora and Eduardo Schmidt for helpful discussions.", "startOffset": 74, "endOffset": 78}], "year": 2016, "abstractText": "Word embeddings have been extensively studied in large text datasets. However, only a few studies analyze semantic representations of small corpora, particularly relevant in single-person text production studies. In the present paper, we compare Skip-gram and LSA capabilities in this scenario, and we test both techniques to extract relevant semantic patterns in single-series dreams reports. LSA showed better performance than Skip-gram in small size training corpus in two semantic tests. As a study case, we show that LSA can capture relevant words associations in dream reports series, even in cases of small number of dreams or low-frequency words. We propose that LSA can be used to explore words associations in dreams reports, which could bring new insight into this classic research area of psychology", "creator": "LaTeX with hyperref package"}}}