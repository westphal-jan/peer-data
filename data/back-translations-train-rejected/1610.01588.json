{"id": "1610.01588", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "5-Oct-2016", "title": "Neural Structural Correspondence Learning for Domain Adaptation", "abstract": "Domain adaptation, adapting models from domains rich in labeled training data to domains poor in such data, is a fundamental NLP challenge. We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks. Particularly, our model is a three-layer neural network that learns to encode the nonpivot features of an input example into a low-dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. On the task of cross-domain product sentiment classification (Blitzer et al., 2007), consisting of 12 domain pairs, our model outperforms both the SCL and the marginalized stacked denoising autoencoder (MSDA, (Chen et al., 2012)) methods by 3.77% and 2.17% respectively, on average across domain pairs.", "histories": [["v1", "Wed, 5 Oct 2016 19:57:21 GMT  (204kb)", "https://arxiv.org/abs/1610.01588v1", null], ["v2", "Mon, 5 Jun 2017 19:03:00 GMT  (239kb)", "http://arxiv.org/abs/1610.01588v2", null], ["v3", "Sat, 17 Jun 2017 22:30:57 GMT  (236kb)", "http://arxiv.org/abs/1610.01588v3", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yftah ziser", "roi reichart"], "accepted": false, "id": "1610.01588"}, "pdf": {"name": "1610.01588.pdf", "metadata": {"source": "CRF", "title": null, "authors": [], "emails": ["syftah@campus.technion.ac.il,", "roiri@ie.technion.ac.il"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.01 588v 3 [cs.C L] 17 June 2017, which combines ideas from two prominent strands of research on domain adaptation through representational learning: structural correspondence learning (SCL, (Blitzer et al., 2006) and autoencoder neural networks (NNNs). Our model is a three-layered NN that learns to encode the non-pivot characteristics of an input example into a low-dimensional representation so that the existence of pivot characteristics (characteristics prevalent in both domains and conveying useful information for the NLP task) can be decoded in the example from this representation. Low-dimensional representation is then used in a learning algorithm for the task. Furthermore, we show how we inject pre-trained word embedding into our model to improve generalization across examples with similar pivot characteristics."}, {"heading": "1 Introduction", "text": "In fact, it is so that it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way in which it is about a way and a way and a way in which it is about which it is about a way in which it is about a way and a way in which it is about"}, {"heading": "2 Background and Contribution", "text": "In fact, it is such that most of us will be able to move into another world, in which they are able to move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they live."}, {"heading": "3 Neural SCL Models", "text": "We propose two models: the basic autoencoder SCL (AE-SCL, 3.2), which integrates ideas of autoencoders and SCL directly, and the sophisticated autoencoder SCL with Similarity Regularization (AE-SCL-SR, 3.3), which integrates pre-trained word embedding into the basic model."}, {"heading": "3.1 Definitions", "text": "In our problem, we designate the group of characteristics with f, the subset of pivot characteristics with fp {1,..., | f |}, and the subset of non-pivot characteristics with fnp {1,.., | f |} in such a way that fp-pivot characteristics of X are designated with xp, while the vector of non-pivot characteristics is designated with xnp. In order to learn a robust and compact characterization for X, we will endeavor to learn a nonlinear prediction function from xnp to xp. As discussed in Section 4, the task we are experimenting with is cross-domain sensing. Following previous work (e.g. Blitzer et al., 2006, Chen et al., 2012), our characterization consists of words responsible for the occurrences of the binary system and the occurrences of the binary system."}, {"heading": "3.2 Autoencoder SCL (AE-SCL)", "text": "To solve the prediction problem, we present an NN architecture inspired by auto-encoders (Figure 1). Faced with an input example X with a feature representation x, our basic idea is to encode from a non-pivot-specific feature representation, xnp, to an intermediate representation hwh (xnp), and finally to use a function rwr (hwh (x np)) to predict the occurrence of pivot characteristics, xp, in the example. As is usual in NN modeling, we introduce nonlinearity into the model through a non-linear activation function called a sigmoid function in our models. As a result, we get: hwh (x np) = ratio probability (whxnp) and rwr (hxnp) is a natural loss (hwp)."}, {"heading": "3.3 Autoencoder SCL with Similarity Regularization (AE-SCL-SR)", "text": "An important observation by Blitzer et al. (2007), is that some pivot characteristics are similar to each other as the plane, that they show the same information in relation to the classification task. (2007), is that some pivot characteristics are similar to each other as the plane, that they have the same characteristics in relation to the classification task. (For example, in the sentiment classification with word unigram characteristics, the words (unigrams) large and excellent are likely to serve as pivot characteristics, as the meaning of each of these characteristics is preserved across domains. (First, in many NLP tasks, the pivot characteristics can be embedded in a vector space where pivot with similar meaning have similar vectors. Second, the fp Xi set of pivot characteristics appearing in an exampleXi is typically much smaller than the set of fp Xi of pivot characteristics that do not appear in it."}, {"heading": "4 Experiments", "text": "This year, it has reached the point where it will be able to retaliate."}, {"heading": "5 Results", "text": "In this context, it should be noted that this project is based on the needs of the people."}, {"heading": "6 Conclusions and Future Work", "text": "We have shown how to encode information from pre-trained Word embeds to improve the generalization of our model using examples with semantically similar pivot characteristics. We have shown strong performance in cross-domain sentiment classification tasks with 16 domain pairs, and provided an initial qualitative analysis that supports the intuition behind our model. Our approach is general and applicable to a large number of NLP9We will consider an example pair from one of the five folds for each setup so that the dimensions of the hidden layers in both models are identical. Tasks (for AE-SCL-SR this is as long as the pivot characteristics can be embedded in a vector space). In the future, we would like to adapt our model to more general domain adjustments, such as where adjustments are made between source and target domains, and where some marked data from the target domain are (available)."}, {"heading": "A Hyperparameter Tuning", "text": "Some of these details appear in the full paper, but here we provide a detailed description. AE-SCL and AE-SCL-SR We matched the parameters of our two models in two steps. First, we randomly split the unlabeled data from both the source and target domains in an 80 / 20 manner, combining the large subsets to generate unlabeled training and validation sets. On these training / validation sets, we match the hyperparameters of stochastic gradient descent (SGD) that we use to train our networks: learning rate (0.9) and weight loss (10 \u2212 5). Note that these values are set to the completely unattended task of predicting pivot properties."}, {"heading": "B Experimental Choices", "text": "There are two publications of the datasets of Blitzer et al. (2007) cross-sectional product evaluation task. We use the ones from http: / / www.cs.jhu. edu / \u02dc mdredze / datasets / sentiment / index2.html, where the data is unbalanced and consists of more positive than negative ratings. We believe that our setup is more realistic than when collecting unlabeled data, it is difficult to get a balanced set. Note that Blitzer et al. (2007) used the Gensim package and trained the model on the unlabeled data from both the source and target domains of each customization setup (https: / / radimrehurek.com / gensim /).13 Results with 500 pivots were very similar. The other publication, where the unlabeled data consists of the same number of positive and negative ratings (2007), Blitzer et al. (2007) used only 400 target ratings for this year, we believe that the overall results are more robust and robust."}], "references": [{"title": "A framework for learning predictive structures frommultiple tasks and unlabeled data", "author": ["Rie Kubota Ando", "Tong Zhang."], "venue": "Journal of Machine Learning Research 6(Nov):1817\u20131853.", "citeRegEx": "Ando and Zhang.,? 2005", "shortCiteRegEx": "Ando and Zhang.", "year": 2005}, {"title": "A theory of learning from different domains", "author": ["Shai Ben-David", "John Blitzer", "Koby Crammer", "Alex Kulesza", "Fernando Pereira", "Jennifer Wortman Vaughan."], "venue": "Machine learning 79(1-2):151\u2013175.", "citeRegEx": "Ben.David et al\\.,? 2010", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Greedy layer-wise training of deep networks", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle."], "venue": "Proc. of NIPS.", "citeRegEx": "Bengio et al\\.,? 2007", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Biographies, bollywood, boom-boxes and blenders: Domain adaptation for sentiment classification", "author": ["John Blitzer", "Mark Dredze", "Fernando Pereira"], "venue": "In Proc. of ACL", "citeRegEx": "Blitzer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Blitzer et al\\.", "year": 2007}, {"title": "Domain adaptation with structural correspondence learning", "author": ["John Blitzer", "Ryan McDonald", "Fernando Pereira."], "venue": "Proc. of EMNLP.", "citeRegEx": "Blitzer et al\\.,? 2006", "shortCiteRegEx": "Blitzer et al\\.", "year": 2006}, {"title": "Unsupervised cross-domain word representation learning", "author": ["Danushka Bollegala", "Takanori Maehara", "Ken-ichi Kawarabayashi."], "venue": "Proc. of ACL.", "citeRegEx": "Bollegala et al\\.,? 2015", "shortCiteRegEx": "Bollegala et al\\.", "year": 2015}, {"title": "Relation adaptation: learning to extract novel relations with minimum supervision", "author": ["Danushka Bollegala", "Yutaka Matsuo", "Mitsuru Ishizuka."], "venue": "Proc. of IJCAI.", "citeRegEx": "Bollegala et al\\.,? 2011a", "shortCiteRegEx": "Bollegala et al\\.", "year": 2011}, {"title": "Using multiple sources to construct a sentiment sensitive thesaurus for cross-domain sentiment classification", "author": ["Danushka Bollegala", "David Weir", "John Carroll."], "venue": "Proc. of ACL.", "citeRegEx": "Bollegala et al\\.,? 2011b", "shortCiteRegEx": "Bollegala et al\\.", "year": 2011}, {"title": "Adaptation of maximum entropy capitalizer: Little data can help a lot", "author": ["Ciprian Chelba", "Alex Acero."], "venue": "Proc. of EMNLP.", "citeRegEx": "Chelba and Acero.,? 2004", "shortCiteRegEx": "Chelba and Acero.", "year": 2004}, {"title": "Automatic feature decomposition for single view co-training", "author": ["Minmin Chen", "Yixin Chen", "Kilian Q Weinberger."], "venue": "Proc. of ICML.", "citeRegEx": "Chen et al\\.,? 2011", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Marginalized denoising autoencoders for domain adaptation", "author": ["Minmin Chen", "Zhixiang Xu", "Kilian Weinberger", "Fei Sha."], "venue": "Proc. of ICML.", "citeRegEx": "Chen et al\\.,? 2012", "shortCiteRegEx": "Chen et al\\.", "year": 2012}, {"title": "A domain adaptation regularization for denoising autoencoders", "author": ["St\u00e9phane Clinchant", "Gabriela Csurka", "Boris Chidlovskii."], "venue": "Proc. of ACL (short papers).", "citeRegEx": "Clinchant et al\\.,? 2016", "shortCiteRegEx": "Clinchant et al\\.", "year": 2016}, {"title": "Frustratingly easy domain adaptation", "author": ["Hal Daum\u00e9 III."], "venue": "Proc. of ACL.", "citeRegEx": "III.,? 2007", "shortCiteRegEx": "III.", "year": 2007}, {"title": "Domain adaptation for statistical classifiers", "author": ["Hal Daume III", "Daniel Marcu."], "venue": "Journal of Artificial Intelligence Research 26:101\u2013126.", "citeRegEx": "III and Marcu.,? 2006", "shortCiteRegEx": "III and Marcu.", "year": 2006}, {"title": "Unsupervised domain adaptation by backpropagation", "author": ["Yaroslav Ganin", "Victor Lempitsky."], "venue": "Proc. of ICML.", "citeRegEx": "Ganin and Lempitsky.,? 2015", "shortCiteRegEx": "Ganin and Lempitsky.", "year": 2015}, {"title": "Domain-adversarial training of neural networks", "author": ["Yaroslav Ganin", "Evgeniya Ustinova", "Hana Ajakan", "Pascal Germain", "Hugo Larochelle", "Fran\u00e7ois Laviolette", "Mario Marchand", "Victor Lempitsky."], "venue": "Journal of Machine Learning Research", "citeRegEx": "Ganin et al\\.,? 2016", "shortCiteRegEx": "Ganin et al\\.", "year": 2016}, {"title": "Some statistical issues in the comparison of speech recognition algorithms", "author": ["Laurence Gillick", "Stephen J Cox."], "venue": "Proc. of ICASSP. IEEE.", "citeRegEx": "Gillick and Cox.,? 1989", "shortCiteRegEx": "Gillick and Cox.", "year": 1989}, {"title": "Domain adaptation for large-scale sentiment classification: A deep learning approach", "author": ["Xavier Glorot", "Antoine Bordes", "Yoshua Bengio."], "venue": "In proc. of ICML. pages 513\u2013520.", "citeRegEx": "Glorot et al\\.,? 2011", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Learning structural correspondences across different linguistic domains with synchronous neural language models", "author": ["Stephan Gouws", "GJ Van Rooyen", "MIH Medialab", "Yoshua Bengio."], "venue": "Proc. of the xLite Workshop on Cross-Lingual Technologies,", "citeRegEx": "Gouws et al\\.,? 2012", "shortCiteRegEx": "Gouws et al\\.", "year": 2012}, {"title": "Correcting sample selection bias by unlabeled data", "author": ["Jiayuan Huang", "Arthur Gretton", "Karsten M Borgwardt", "Bernhard Sch\u00f6lkopf", "Alex J Smola."], "venue": "Proc. of NIPS.", "citeRegEx": "Huang et al\\.,? 2007", "shortCiteRegEx": "Huang et al\\.", "year": 2007}, {"title": "Instance weighting for domain adaptation in nlp", "author": ["Jing Jiang", "ChengXiang Zhai."], "venue": "Proc. of ACL.", "citeRegEx": "Jiang and Zhai.,? 2007", "shortCiteRegEx": "Jiang and Zhai.", "year": 2007}, {"title": "Autoencoding variational bayes", "author": ["Diederik P Kingma", "Max Welling."], "venue": "Proc. of ICLR.", "citeRegEx": "Kingma and Welling.,? 2014", "shortCiteRegEx": "Kingma and Welling.", "year": 2014}, {"title": "The variational fair autoencoder", "author": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel"], "venue": null, "citeRegEx": "Louizos et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Louizos et al\\.", "year": 2016}, {"title": "Domain adaptation with multiple sources", "author": ["Yishay Mansour", "Mehryar Mohri", "Afshin Rostamizadeh."], "venue": "Proc. of NIPS.", "citeRegEx": "Mansour et al\\.,? 2009", "shortCiteRegEx": "Mansour et al\\.", "year": 2009}, {"title": "Automatic domain adaptation for parsing", "author": ["David McClosky", "Eugene Charniak", "Mark Johnson."], "venue": "Proc. of NAACL.", "citeRegEx": "McClosky et al\\.,? 2010", "shortCiteRegEx": "McClosky et al\\.", "year": 2010}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg Corrado", "Jeffrey Dean."], "venue": "Proc. of NIPS.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Cross-domain sentiment classification via spectral feature alignment", "author": ["Sinno Jialin Pan", "Xiaochuan Ni", "Jian-Tao Sun", "Qiang Yang", "Zheng Chen."], "venue": "Proceedings of the 19th international conference on World wide web. ACM, pages 751\u2013760.", "citeRegEx": "Pan et al\\.,? 2010", "shortCiteRegEx": "Pan et al\\.", "year": 2010}, {"title": "Self-training for enhancement and domain adaptation of statistical parsers trained on small datasets", "author": ["Roi Reichart", "Ari Rappoport."], "venue": "Proc. of ACL.", "citeRegEx": "Reichart and Rappoport.,? 2007", "shortCiteRegEx": "Reichart and Rappoport.", "year": 2007}, {"title": "Stochastic backpropagation and approximate inference in deep generative models", "author": ["Danilo Jimenez Rezende", "Shakir Mohamed", "Daan Wierstra."], "venue": "Proc. of ICML.", "citeRegEx": "Rezende et al\\.,? 2014", "shortCiteRegEx": "Rezende et al\\.", "year": 2014}, {"title": "Supervised and unsupervised pcfg adaptation to novel domains", "author": ["Brian Roark", "Michiel Bacchiani."], "venue": "Proc. of HLT-NAACL.", "citeRegEx": "Roark and Bacchiani.,? 2003", "shortCiteRegEx": "Roark and Bacchiani.", "year": 2003}, {"title": "Improved parsing and pos tagging using inter-sentence consistency constraints", "author": ["Alexander M Rush", "Roi Reichart", "Michael Collins", "Amir Globerson."], "venue": "Proc. of EMNLP-CoNLL.", "citeRegEx": "Rush et al\\.,? 2012", "shortCiteRegEx": "Rush et al\\.", "year": 2012}, {"title": "Towards robust cross-domain domain adaptation for part-ofspeech tagging", "author": ["Tobias Schnabel", "Hinrich Sch\u00fctze."], "venue": "Proc. of IJCNLP.", "citeRegEx": "Schnabel and Sch\u00fctze.,? 2013", "shortCiteRegEx": "Schnabel and Sch\u00fctze.", "year": 2013}, {"title": "Extracting and composing robust features with denoising autoencoders", "author": ["Pascal Vincent", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol."], "venue": "Proc. of ICML.", "citeRegEx": "Vincent et al\\.,? 2008", "shortCiteRegEx": "Vincent et al\\.", "year": 2008}, {"title": "Topic-bridged plsa for cross-domain text classification", "author": ["Gui-Rong Xue", "Wenyuan Dai", "Qiang Yang", "Yong Yu."], "venue": "Proc. of SIGIR.", "citeRegEx": "Xue et al\\.,? 2008", "shortCiteRegEx": "Xue et al\\.", "year": 2008}, {"title": "Fast easy unsupervised domain adaptationwith marginalized structured dropout", "author": ["Yi Yang", "Jacob Eisenstein."], "venue": "Proc. of ACL (short papers).", "citeRegEx": "Yang and Eisenstein.,? 2014", "shortCiteRegEx": "Yang and Eisenstein.", "year": 2014}, {"title": "Learning sentence embeddings with auxiliary tasks for cross-domain sentiment classification", "author": ["Jianfei Yu", "Jing Jiang."], "venue": "Proc. of EMNLP.", "citeRegEx": "Yu and Jiang.,? 2016", "shortCiteRegEx": "Yu and Jiang.", "year": 2016}], "referenceMentions": [{"referenceID": 4, "context": "We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks (NNs).", "startOffset": 198, "endOffset": 220}, {"referenceID": 1, "context": "Domain adaptation (Daum\u00e9 III, 2007; Ben-David et al., 2010), training an algorithm on", "startOffset": 18, "endOffset": 59}, {"referenceID": 7, "context": "Indeed, over the last decade domain adaptation methods have been proposed for tasks such as sentiment classification (Bollegala et al., 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al.", "startOffset": 117, "endOffset": 142}, {"referenceID": 31, "context": ", 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al.", "startOffset": 22, "endOffset": 50}, {"referenceID": 27, "context": ", 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al.", "startOffset": 70, "endOffset": 142}, {"referenceID": 24, "context": ", 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al.", "startOffset": 70, "endOffset": 142}, {"referenceID": 30, "context": ", 2011b), POS tagging (Schnabel and Sch\u00fctze, 2013), syntactic parsing (Reichart and Rappoport, 2007; McClosky et al., 2010; Rush et al., 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al.", "startOffset": 70, "endOffset": 142}, {"referenceID": 20, "context": ", 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name just a handful of applications and works.", "startOffset": 32, "endOffset": 79}, {"referenceID": 6, "context": ", 2012) and relation extraction (Jiang and Zhai, 2007; Bollegala et al., 2011a), if to name just a handful of applications and works.", "startOffset": 32, "endOffset": 79}, {"referenceID": 17, "context": "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012).", "startOffset": 123, "endOffset": 163}, {"referenceID": 10, "context": "Leading recent approaches to domain adaptation in NLP are based on Neural Networks (NNs), and particularly on autoencoders (Glorot et al., 2011; Chen et al., 2012).", "startOffset": 123, "endOffset": 163}, {"referenceID": 3, "context": "However, while excelling on benchmark domain adaptation tasks such as cross-domain product sentiment classification (Blitzer et al., 2007), the reasons to this success are not entirely understood.", "startOffset": 116, "endOffset": 138}, {"referenceID": 0, "context": "Following the auxiliary problems approach to semi-supervised learning (Ando and Zhang, 2005), this method identifies correspondences among features from different domains by modeling their correlations with pivot features: features that are frequent in both domains and are important for the NLP task.", "startOffset": 70, "endOffset": 92}, {"referenceID": 3, "context": "We experiment with the task of cross-domain product sentiment classification of (Blitzer et al., 2007), consisting of 4 domains (12 domain pairs) and further add an additional target domain, consisting of sentences extracted from social media blogs (total of 16 domain pairs).", "startOffset": 80, "endOffset": 102}, {"referenceID": 25, "context": "For pivot feature embedding in our advanced model, we employ the word2vec algorithm (Mikolov et al., 2013).", "startOffset": 84, "endOffset": 106}, {"referenceID": 10, "context": "Our models substantially outperform strong baselines: the SCL algorithm, the marginalized stacked denoising autoencoder (MSDA) model (Chen et al., 2012) and the MSDA-DAN model (Ganin et al.", "startOffset": 133, "endOffset": 152}, {"referenceID": 15, "context": ", 2012) and the MSDA-DAN model (Ganin et al., 2016) that combines the power of MSDA with a domain adversarial network (DAN).", "startOffset": 31, "endOffset": 51}, {"referenceID": 29, "context": "(Roark and Bacchiani, 2003; Chelba and Acero, 2004; Daume III and Marcu, 2006)).", "startOffset": 0, "endOffset": 78}, {"referenceID": 8, "context": "(Roark and Bacchiani, 2003; Chelba and Acero, 2004; Daume III and Marcu, 2006)).", "startOffset": 0, "endOffset": 78}, {"referenceID": 19, "context": "There are several approaches to domain adaptation in the machine learning literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al.", "startOffset": 117, "endOffset": 159}, {"referenceID": 23, "context": "There are several approaches to domain adaptation in the machine learning literature, including instance reweighting (Huang et al., 2007; Mansour et al., 2009), sub-sampling from both domains (Chen et al.", "startOffset": 117, "endOffset": 159}, {"referenceID": 9, "context": ", 2009), sub-sampling from both domains (Chen et al., 2011) and learning joint target and source feature representations (Blitzer et al.", "startOffset": 40, "endOffset": 59}, {"referenceID": 4, "context": ", 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daum\u00e9 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).", "startOffset": 69, "endOffset": 166}, {"referenceID": 33, "context": ", 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daum\u00e9 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).", "startOffset": 69, "endOffset": 166}, {"referenceID": 17, "context": ", 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daum\u00e9 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).", "startOffset": 69, "endOffset": 166}, {"referenceID": 10, "context": ", 2011) and learning joint target and source feature representations (Blitzer et al., 2006; Daum\u00e9 III, 2007; Xue et al., 2008; Glorot et al., 2011; Chen et al., 2012).", "startOffset": 69, "endOffset": 166}, {"referenceID": 15, "context": "We compare our models to one such method (MSDA-DAN, (Ganin et al., 2016)).", "startOffset": 52, "endOffset": 72}, {"referenceID": 26, "context": "A prominent method is spectral feature alignment (SFA, (Pan et al., 2010)).", "startOffset": 55, "endOffset": 73}, {"referenceID": 15, "context": "Recently, Gouws et al. (2012) and Bollegala et al.", "startOffset": 10, "endOffset": 30}, {"referenceID": 5, "context": "(2012) and Bollegala et al. (2015) implemented ideas related to those described here within an NN for cross-domain sentiment classification.", "startOffset": 11, "endOffset": 35}, {"referenceID": 5, "context": "(2012) and Bollegala et al. (2015) implemented ideas related to those described here within an NN for cross-domain sentiment classification. For example, the latter work trained a word embedding model so that for every document, regardless of its domain, pivots are good predictors of nonpivots, and the pivots\u2019 embeddings are similar across domains. Yu and Jiang (2016) presented", "startOffset": 11, "endOffset": 371}, {"referenceID": 5, "context": "That is, unlike Bollegala et al. (2015) we do not learn word embeddings and unlike Yu and Jiang (2016) we", "startOffset": 16, "endOffset": 40}, {"referenceID": 5, "context": "That is, unlike Bollegala et al. (2015) we do not learn word embeddings and unlike Yu and Jiang (2016) we", "startOffset": 16, "endOffset": 103}, {"referenceID": 2, "context": "Once an autoencoder has been trained, one can stack another autoencoder on top of it, by training a second model which sees the output of the first as its training data (Bengio et al., 2007).", "startOffset": 169, "endOffset": 190}, {"referenceID": 32, "context": "Recent prominent models for domain adaptation for sentiment classification are based on a variant of the autoencoder called Stacked Denoising Autoencoders (SDA, (Vincent et al., 2008)).", "startOffset": 161, "endOffset": 183}, {"referenceID": 15, "context": "SDA for crossdomain sentiment classification was implemented by Glorot et al. (2011). Later, Chen et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 9, "context": "Later, Chen et al. (2012) proposed the marginalized SDA (MSDA) model that is more computationally efficient and scalable to high-dimensional feature spaces than SDA.", "startOffset": 7, "endOffset": 26}, {"referenceID": 31, "context": "Yang and Eisenstein (2014) showed how to improve efficiency further by exploiting noising functions designed for structured feature spaces, which are common in NLP.", "startOffset": 0, "endOffset": 27}, {"referenceID": 11, "context": "More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al.", "startOffset": 15, "endOffset": 39}, {"referenceID": 11, "context": "More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al.", "startOffset": 15, "endOffset": 143}, {"referenceID": 11, "context": "More recently, Clinchant et al. (2016) proposed an unsupervised regularization method for MSDA based on the work of Ganin and Lempitsky (2015) and Ganin et al. (2016).", "startOffset": 15, "endOffset": 167}, {"referenceID": 21, "context": "There is a recent interest in models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014), for example the variational fair autoencoder model (Louizos et al.", "startOffset": 71, "endOffset": 119}, {"referenceID": 28, "context": "There is a recent interest in models based on variational autoencoders (Kingma and Welling, 2014; Rezende et al., 2014), for example the variational fair autoencoder model (Louizos et al.", "startOffset": 71, "endOffset": 119}, {"referenceID": 22, "context": ", 2014), for example the variational fair autoencoder model (Louizos et al., 2016), for domain adaptation.", "startOffset": 60, "endOffset": 82}, {"referenceID": 10, "context": "(Blitzer et al., 2006, 2007; Chen et al., 2012) our feature representation consists of binary indicators for the occurrence of word unigrams and bigrams in the represented document.", "startOffset": 0, "endOffset": 47}, {"referenceID": 3, "context": "An important observation of Blitzer et al. (2007), is that some pivot features are similar to each other to the level that they indicate the same information with respect to the classification task.", "startOffset": 28, "endOffset": 50}, {"referenceID": 3, "context": "Cross-domain Sentiment Classification To demonstrate the power of our models for domain adaptation we experiment with the task of crossdomain sentiment classification (Blitzer et al., 2007).", "startOffset": 167, "endOffset": 189}, {"referenceID": 3, "context": "The first baseline is SCL with pivot features selected using the mutual information criterion (SCL-MI, (Blitzer et al., 2007)).", "startOffset": 103, "endOffset": 125}, {"referenceID": 10, "context": "Our second baseline is hence the MSDA method (Chen et al., 2012), with code taken from the authors\u2019 web page.", "startOffset": 45, "endOffset": 64}, {"referenceID": 15, "context": "Among autoencoder models, SDA has shown by Glorot et al. (2011) to outperform SFA and SCL on cross-domain sentiment classification and later on Chen et al.", "startOffset": 43, "endOffset": 64}, {"referenceID": 9, "context": "(2011) to outperform SFA and SCL on cross-domain sentiment classification and later on Chen et al. (2012) demonstrated superior performance for MSDA over SDA and SCL on the same task.", "startOffset": 87, "endOffset": 106}, {"referenceID": 15, "context": "To consider a regularization scheme on top of MSDA representations we also experiment with the MSDA-DANmodel (Ganin et al., 2016) which employs a domain adversarial network (DAN) with the MSDA vectors as input.", "startOffset": 109, "endOffset": 129}, {"referenceID": 15, "context": "To consider a regularization scheme on top of MSDA representations we also experiment with the MSDA-DANmodel (Ganin et al., 2016) which employs a domain adversarial network (DAN) with the MSDA vectors as input. In Ganin et al. (2016) MSDA-DAN has shown to substantially outperform the DAN model when DAN is randomly initialized.", "startOffset": 110, "endOffset": 234}, {"referenceID": 5, "context": "org/stable/ We tried to compare to (Bollegala et al., 2015) but failed to replicate their results despite personal communication with the authors.", "startOffset": 35, "endOffset": 59}, {"referenceID": 3, "context": "We experiment with a 5-fold cross-validation on the source domain (Blitzer et al., 2007): 1600 reviews for training and 400 reviews for development.", "startOffset": 66, "endOffset": 88}, {"referenceID": 3, "context": "We experiment with a 5-fold cross-validation on the source domain (Blitzer et al., 2007): 1600 reviews for training and 400 reviews for development. The test set for each target domain of Blitzer et al. (2007) consists of all 2000 labeled reviews of that domain, and for the Blog domain it consists of the 7086 labeled sentences provided with the task dataset.", "startOffset": 67, "endOffset": 210}, {"referenceID": 25, "context": "For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (Mikolov et al., 2013).", "startOffset": 89, "endOffset": 111}, {"referenceID": 3, "context": "Baselines: For SCL-MI, following (Blitzer et al., 2007) we tuned the number of pivot features", "startOffset": 33, "endOffset": 55}, {"referenceID": 16, "context": "Statistical significance (with the McNemar paired test for labeling disagreements (Gillick and Cox, 1989; Blitzer et al., 2006), p < 0.", "startOffset": 82, "endOffset": 127}, {"referenceID": 4, "context": "Statistical significance (with the McNemar paired test for labeling disagreements (Gillick and Cox, 1989; Blitzer et al., 2006), p < 0.", "startOffset": 82, "endOffset": 127}, {"referenceID": 3, "context": "Table 1: Sentiment classification accuracy for the Blitzer et al. (2007) task (top tables), and for adaptation from the Blitzer\u2019s product review domains to the Blog domain (bottom table).", "startOffset": 51, "endOffset": 73}, {"referenceID": 15, "context": "For MSDA-DAN, we followed Ganin et al. (2016): the \u03bb adaptation parameter is chosen among 9 values between 10\u22122 and 1 on a logarithmic scale, the hidden layer size l is chosen among {50, 100, 200} and the learning rate \u03bc is 10\u22123.", "startOffset": 26, "endOffset": 46}, {"referenceID": 3, "context": "In the Blitzer et al. (2007) task (top tables), AE-SCL-SR is the best performing model in 9 of 12 setups and on a uni-", "startOffset": 7, "endOffset": 29}, {"referenceID": 3, "context": "Results are presented for the unified test set of the Blitzer et al. (2007) task.", "startOffset": 54, "endOffset": 76}, {"referenceID": 3, "context": "Table 3: Class based analysis for the unified test set of the Blitzer et al. (2007) task.", "startOffset": 62, "endOffset": 84}, {"referenceID": 25, "context": "For AE-SCL-SR, embeddings for the unigram and bigram features were learned with word2vec (Mikolov et al., 2013).", "startOffset": 89, "endOffset": 111}, {"referenceID": 3, "context": "SCL-MI Following (Blitzer et al., 2007) we used 1000 pivot features .", "startOffset": 17, "endOffset": 39}, {"referenceID": 10, "context": "For details on these hyper-parameters see (Chen et al., 2012).", "startOffset": 42, "endOffset": 61}, {"referenceID": 15, "context": "MSDA-DAN Following Ganin et al. (2016) we tuned the hyperparameters on the labeled development data as follows.", "startOffset": 19, "endOffset": 39}, {"referenceID": 3, "context": "Variants of the Product Review Data There are two releases of the datasets of the Blitzer et al. (2007) cross-domain product review task.", "startOffset": 82, "endOffset": 104}, {"referenceID": 3, "context": "Note that Blitzer et al. (2007) used", "startOffset": 10, "endOffset": 32}, {"referenceID": 3, "context": "Test Set Size While Blitzer et al. (2007) used only 400 target domain reviews for test, we use the entire set of 2000 reviews.", "startOffset": 20, "endOffset": 42}], "year": 2017, "abstractText": "We introduce a neural network model that marries together ideas from two prominent strands of research on domain adaptation through representation learning: structural correspondence learning (SCL, (Blitzer et al., 2006)) and autoencoder neural networks (NNs). Our model is a three-layer NN that learns to encode the non-pivot features of an input example into a low-dimensional representation, so that the existence of pivot features (features that are prominent in both domains and convey useful information for the NLP task) in the example can be decoded from that representation. The low-dimensional representation is then employed in a learning algorithm for the task. Moreover, we show how to inject pre-trained word embeddings into our model in order to improve generalization across examples with similar pivot features. We experiment with the task of cross-domain sentiment classification on 16 domain pairs and show substantial improvements over strong baselines.", "creator": "LaTeX with hyperref package"}}}