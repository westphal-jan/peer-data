{"id": "1610.01733", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Towards Cognitive Exploration through Deep Reinforcement Learning for Mobile Robots", "abstract": "Exploration in an unknown environment is the core functionality for mobile robots. Learning-based exploration methods, including convolutional neural networks, provide excellent strategies without human-designed logic for the feature extraction. But the conventional supervised learning algorithms cost lots of efforts on the labeling work of datasets inevitably. Scenes not included in the training set are mostly unrecognized either. We propose a deep reinforcement learning method for the exploration of mobile robots in an indoor environment with the depth information from an RGB-D sensor only. Based on the Deep Q-Network framework, the raw depth image is taken as the only input to estimate the Q values corresponding to all moving commands. The training of the network weights is end-to-end. In arbitrarily constructed simulation environments, we show that the robot can be quickly adapted to unfamiliar scenes without any man-made labeling. Besides, through analysis of receptive fields of feature representations, deep reinforcement learning motivates the convolutional networks to estimate the traversability of the scenes. The test results are compared with the exploration strategies separately based on deep learning or reinforcement learning. Even trained only in the simulated environment, experimental results in real-world environment demonstrate that the cognitive ability of robot controller is dramatically improved compared with the supervised method. We believe it is the first time that raw sensor information is used to build cognitive exploration strategy for mobile robots through end-to-end deep reinforcement learning.", "histories": [["v1", "Thu, 6 Oct 2016 05:08:21 GMT  (2212kb,D)", "http://arxiv.org/abs/1610.01733v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI", "authors": ["lei tai", "ming liu"], "accepted": false, "id": "1610.01733"}, "pdf": {"name": "1610.01733.pdf", "metadata": {"source": "CRF", "title": "Towards cognitive exploration through deep reinforcement learning for mobile robots", "authors": ["Lei Tai", "Ming Liu"], "emails": ["lei.tai@my.cityu.edu.hk", "eelium@ust.hk"], "sections": [{"heading": null, "text": "I. INTRODUCTION"}, {"heading": "A. Motivation", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "B. Contributions", "text": "We emphasize the following contributions and features of this work: \u2022 Through in-depth reinforcement learning, we demonstrate the evolutionary capability of a mobile robot in unknown environments. \u2022 We initialize the weights of the previous CNN model, which was trained with real-world sensory samples, and continuously train it in an end-to-end manner. \u2022 Performance is evaluated in both simulated and real-world environments. \u2022 The reinforcement learning model can quickly achieve obstacle avoidance capabilities in an indoor environment with several thousand training processes, without additional artificial upsampling or labeling work for datasets. \u2022 For evaluations of CNN, we use receptive fields in original inputs to establish the feasibility of the trained model. Receptive fields activated by the final feature representations are presented through visual upsampling."}, {"heading": "II. RELATED WORK", "text": "Traditional strategies for robotics research relied mainly on complicated control logic, with handmade functions extracted from environments [12]. Due to the development of mainframe hardware such as GPU, methods related to deep learning were considered to solve robotics-related problems, including research on robots."}, {"heading": "A. Deep learning in robotics exploration", "text": "Convolutionary neural networks (CNNs) were used to detect obstacles in the terrain [10] by using stereo images as input, and they also helped aerospace robotics to navigate through forest paths with a single monocular camera [9]. In our previous work, a three-layered revolutionary framework [1] was used to perceive an indoor environment for mobile robots. By taking raw images or depth images as inputs and using moving commands or steering angles as exits, weights of CNN-based models can be trained by backpropagation and stochastic slope descent. Except for robotics, capturing locations can be considered an object recognition problem [13], and CNN is the solution to this state-of-the-art problem. Note that all of these supervised learning methods mentioned above sorted boxes make great efforts to capture and label data values using high resolution objects [14]."}, {"heading": "B. Reinforcement learning in robotics", "text": "The main advantage of amplification learning is complete independence from humlabeling. Motivated by trial-and-error interaction with the environment, the assessment of the action-value function is self-directed, using the robot states as input to the model. Conventional amplification learning methods improved control performance in route planning of robot arms [16] and helicopter controls [17]. By viewing RGB or RGB-D images as states of robots, amplification learning can be used directly to achieve visual control guidelines. In our previous work [3], a Q-learning-based amplification learning controller was used to help a turtlebot navigate the simulation environment."}, {"heading": "C. Deep reinforcement learning", "text": "In this context, it should be noted that the solution to the problems is not a purely formal solution, but a purely formal one."}, {"heading": "A. Simulated environment", "text": "In our previous work [1], the training data sets were collected in structured corridor environments. Depth images in these data sets were labeled with real-time motion commands of human decisions. In order to expand the exploration capability of the mobile robot, we have created a more complicated indoor environment, as shown in Figure 1 in the Gazebo1 simulator. In addition to the corridor-like driveable areas, there are much more complicated scenes such as cylinders, sharp edges and multiple obstacles with different perceptual depths. These newly created scenes have never been used in the training of our previous supervised learning model [1]. We use a Turtlebot as the main agent in the simulated environment. A kinect RGBD camera is mounted on top1http: / / gazebosim.org / of the robot. We can obtain the real-time RGB-D raw image from the robot's field of view (FOV)."}, {"heading": "B. Deep reinforcement learning implementation", "text": "As a standard reinforcement of the learning structure, the environment mentioned in Section III-A is considered a complete combination. Q = Q = At each discrete time step, the agent selects an action from the defined action set. In this paper, the action set consists of five moving commands, namely left, half left, straight, half fear and right. Detailed assignments of speeds related to the moving commands are introduced in Section IV. The only perception by the robot is the depth effect taken by the Kinect camera after each execution of the action. In contrast, in Atari games, the reward rt is the change in the game result, the only feedback used as a reward, a binary state indicating whether the collision occurs or not. It is decided by recording the minimum distance by the kinect camera. As soon as the collision occurs, we set a negative reward to represent the ending. Conversely, we grant a positive motion to allow the movement to mutate."}, {"heading": "IV. EXPERIMENTS AND RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Training", "text": "At the beginning of the training, volume layers are initialized by copying the weights trained in [1] for the same layer structure. A simple political learning network structure has also been used separately in [3] with three movable commands as output. Compared to the gradually decreasing learning rate in the training of the supervised learning model [1], we are using a much lower fixed learning rate in end-to-end training for the deep reinforcement learning model. As the only feedback to motivate network convergence, the negative reward for the collision between the robot and obstacles must be very large as in [3]. Training parameters are detailed in Table I. All models are trained and tested with caffe [25] on a single NVIDIA GeForce GTX 690. Table II lists the mappings of speeds for the five output moving commands both in training and testing procedures."}, {"heading": "B. Analysis of exploration tests", "text": "In fact, most of us are able to move to a different world, where we do not want to get used to another world."}, {"heading": "C. Analysis of receptive fields in the cognitive process", "text": "This year it is more than ever before."}, {"heading": "V. CONCLUSION", "text": "This paper demonstrates the usefulness of the learning framework for exploring depth enhancement by robots using a previously trained model based on our previous work [1]. The depth enhancement learning model continuously expands the cognitive capabilities of mobile robots for more complex indoor environments in an efficient online learning process. Analysis of receptive fields shows the critical promotion of learning depth enhancement at the end of the day: feature representations extracted through revolutionary networks are essential to the mobile robots \"mobility in both simulated and real environments. There are many aspects that need to be developed in the future, such as building a more complicated, unstructured environment. To navigate in more complicated environments, even outdoors, the still raw RGB images should also be viewed as inputs, but not as a single depth image."}], "references": [{"title": "A Deep-network Solution Towords Modelless Obstacle Avoidence", "author": ["L. Tai", "S. Li", "M. Liu"], "venue": "IEEE/RSJ International Conference on Intelligent Robots and Systems, IROS 2016, 2016.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski"], "venue": "Nature, vol. 518, no. 7540, pp. 529\u2013533, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A robot exploration strategy based on q-learning network", "author": ["L. Tai", "M. Liu"], "venue": "Real-time Computing and Robotics (RCAR) 2016 IEEE International Conference on, Angkor Wat, Cambodia, June 2016.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "Integrating behavioral, perceptual, and world knowledge in reactive navigation", "author": ["R.C. Arkin"], "venue": "Robotics and autonomous systems, vol. 6, no. 1, pp. 105\u2013122, 1990.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1990}, {"title": "The vector field histogram-fast obstacle avoidance for mobile robots", "author": ["J. Borenstein", "Y. Koren"], "venue": "IEEE Transactions on Robotics and Automation, vol. 7, no. 3, pp. 278\u2013288, 1991.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 1991}, {"title": "Incremental topological segmentation for semi-structured environments using discretized gvg", "author": ["M. Liu", "F. Colas", "L. Oth", "R. Siegwart"], "venue": "Autonomous Robots, vol. 38, no. 2, pp. 143\u2013160, 2015.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A markov semisupervised clustering approach and its application in topological map extraction", "author": ["M. Liu", "F. Colas", "F. Pomerleau", "R. Siegwart"], "venue": "2012 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2012, pp. 4743\u20134748.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "Convolutional neural network-based image representation for visual loop closure detection", "author": ["Y. Hou", "H. Zhang", "S. Zhou"], "venue": "Information and Automation, 2015 IEEE International Conference on. IEEE, 2015, pp. 2238\u20132245.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A machine learning approach to visual perception of forest trails for mobile robots", "author": ["A. Giusti", "J. Guzzi", "D.C. Cire\u015fan", "F.-L. He", "J.P. Rodr\u0131\u0301guez", "F. Fontana", "M. Faessler", "C. Forster", "J. Schmidhuber", "G. Di Caro"], "venue": "IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 661\u2013667, 2016.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Offroad obstacle avoidance through end-to-end learning", "author": ["U. Muller", "J. Ben", "E. Cosatto", "B. Flepp", "Y.L. Cun"], "venue": "Advances in neural information processing systems, 2005, pp. 739\u2013746.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2005}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations", "author": ["B. Kuipers", "Y.-T. Byun"], "venue": "Robotics and autonomous systems, vol. 8, no. 1, pp. 47\u201363, 1991.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 1991}, {"title": "Deep learning for detecting robotic grasps", "author": ["I. Lenz", "H. Lee", "A. Saxena"], "venue": "The International Journal of Robotics Research, vol. 34, no. 4-5, pp. 705\u2013724, 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Traversability classification using unsupervised on-line visual learning for outdoor robot navigation", "author": ["D. Kim", "J. Sun", "S.M. Oh", "J.M. Rehg", "A.F. Bobick"], "venue": "Proceedings 2006 IEEE International Conference on Robotics and Automation, 2006. ICRA 2006. IEEE, 2006, pp. 518\u2013525.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "Semi-supervised online learning for efficient classification of objects in 3d data streams", "author": ["Y. Tao", "R. Triebel", "D. Cremers"], "venue": "Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on. IEEE, 2015, pp. 2904\u20132910.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Modelbased reinforcement learning with parametrized physical models and optimism-driven exploration", "author": ["C. Xie", "S. Patil", "T. Moldovan", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1509.06824, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Autonomous inverted helicopter flight via reinforcement learning", "author": ["A.Y. Ng", "A. Coates", "M. Diel", "V. Ganapathi", "J. Schulte", "B. Tse", "E. Berger", "E. Liang"], "venue": "Experimental Robotics IX. Springer, 2006, pp. 363\u2013372.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Benchmarking deep reinforcement learning for continuous control", "author": ["Y. Duan", "X. Chen", "R. Houthooft", "J. Schulman", "P. Abbeel"], "venue": "arXiv preprint arXiv:1604.06778, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint arXiv:1511.06581, 2015.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous control with deep reinforcement learning", "author": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "arXiv preprint arXiv:1509.02971, 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Continuous deep q-learning with model-based acceleration", "author": ["S. Gu", "T. Lillicrap", "I. Sutskever", "S. Levine"], "venue": "arXiv preprint arXiv:1603.00748, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Towards vision-based deep reinforcement learning for robotic motion control", "author": ["F. Zhang", "J. Leitner", "M. Milford", "B. Upcroft", "P. Corke"], "venue": "arXiv preprint arXiv:1511.03791, 2015.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "Dropout: a simple way to prevent neural networks from overfitting.", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "Caffe: Convolutional architecture for fast feature embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "Proceedings of the 22nd ACM international conference on Multimedia. ACM, 2014, pp. 675\u2013678.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 818\u2013833.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3431\u20133440.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "Learning-based exploration methods, including convolutional neural networks, provide excellent strategies without human-designed logic for the feature extraction [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 1, "context": "Based on the Deep Q-Network framework [2], the raw depth image is taken as the only input to estimate the Q values corresponding to all moving commands.", "startOffset": 38, "endOffset": 41}, {"referenceID": 0, "context": "The test results are compared with the exploration strategies separately based on deep learning [1] or reinforcement learning [3].", "startOffset": 96, "endOffset": 99}, {"referenceID": 2, "context": "The test results are compared with the exploration strategies separately based on deep learning [1] or reinforcement learning [3].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "Conventional exploration methods require heuristic control logic such as the front-wave exploration [4] and additional processes to deal with obstacles [5].", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "Conventional exploration methods require heuristic control logic such as the front-wave exploration [4] and additional processes to deal with obstacles [5].", "startOffset": 152, "endOffset": 155}, {"referenceID": 5, "context": "Aided by stereo vision systems or radar sensors, researchers often build the geometry or topological mapping of environments [6] [7] to make navigation decisions based", "startOffset": 125, "endOffset": 128}, {"referenceID": 6, "context": "Aided by stereo vision systems or radar sensors, researchers often build the geometry or topological mapping of environments [6] [7] to make navigation decisions based", "startOffset": 129, "endOffset": 132}, {"referenceID": 7, "context": "Successes of this hierarchical model also motivate robotic scientists to apply deep learning algorithms in conventional robotics problems like recognition [8] and obstacle avoidance [1] [9] [10].", "startOffset": 155, "endOffset": 158}, {"referenceID": 0, "context": "Successes of this hierarchical model also motivate robotic scientists to apply deep learning algorithms in conventional robotics problems like recognition [8] and obstacle avoidance [1] [9] [10].", "startOffset": 182, "endOffset": 185}, {"referenceID": 8, "context": "Successes of this hierarchical model also motivate robotic scientists to apply deep learning algorithms in conventional robotics problems like recognition [8] and obstacle avoidance [1] [9] [10].", "startOffset": 186, "endOffset": 189}, {"referenceID": 9, "context": "Successes of this hierarchical model also motivate robotic scientists to apply deep learning algorithms in conventional robotics problems like recognition [8] and obstacle avoidance [1] [9] [10].", "startOffset": 190, "endOffset": 194}, {"referenceID": 1, "context": "Through combining reinforcement learning and hierarchical sensory processing, deep reinforcement learning (DRL) [2]", "startOffset": 112, "endOffset": 115}, {"referenceID": 10, "context": "And it outperformed all of the previous artificial control algorithms in Atari games [11].", "startOffset": 85, "endOffset": 89}, {"referenceID": 0, "context": "In our previous work, we have proved the feasibility of the CNN-based supervised learning method for obstacle avoidance in the indoor environment [1] and the effectiveness of the conventional reinforcement learning method in the", "startOffset": 146, "endOffset": 149}, {"referenceID": 2, "context": "exploration policy estimation [3] through the feature representations extracted from the pre-trained CNN model in [1].", "startOffset": 30, "endOffset": 33}, {"referenceID": 0, "context": "exploration policy estimation [3] through the feature representations extracted from the pre-trained CNN model in [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 11, "context": "Conventional robot exploration strategies mainly depended on complicated control logics, with hand-crafted features extracted from environments [12].", "startOffset": 144, "endOffset": 148}, {"referenceID": 9, "context": "Convolutional neural networks (CNNs) have been applied to recognize off-road obstacles [10] by taking stereo images as input.", "startOffset": 87, "endOffset": 91}, {"referenceID": 8, "context": "It also helped aerial robotics to navigate along forest trails with a single monocular camera [9].", "startOffset": 94, "endOffset": 97}, {"referenceID": 0, "context": "In our previous work, a three-layer convolutional framework [1] was used to perceive an indoor corridor environment for mobile robots.", "startOffset": 60, "endOffset": 63}, {"referenceID": 12, "context": "Except for robotics exploration, grasping locations can be regarded as an object detection problem [13] and CNN is the state-of-the-art solution for this problem.", "startOffset": 99, "endOffset": 103}, {"referenceID": 13, "context": "[14] achieved the labeling result by using other sensors with higher resolution.", "startOffset": 0, "endOffset": 4}, {"referenceID": 14, "context": "[15] labeled the center sample of the clustering result for object classification as a semi-supervised method.", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "Conventional reinforcement learning methods improved the controller performances in path-planning of robot-arms [16] and controlling of helicopters [17].", "startOffset": 112, "endOffset": 116}, {"referenceID": 16, "context": "Conventional reinforcement learning methods improved the controller performances in path-planning of robot-arms [16] and controlling of helicopters [17].", "startOffset": 148, "endOffset": 152}, {"referenceID": 2, "context": "In our previous work [3], a Q-learning based reinforcement learning controller was used to help a turtlebot navigate in the simulation environment.", "startOffset": 21, "endOffset": 24}, {"referenceID": 17, "context": "Due to the potential of automating the design of data representations, deep reinforcement learning abstracted considerable attentions recently [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 1, "context": "Deep reinforcement learning was firstly applied on playing 2600 Atari games [2].", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "Based on the success of DQN [2], revised deep reinforcement learning methods appeared to improve the performance on various of applications.", "startOffset": 28, "endOffset": 31}, {"referenceID": 18, "context": "Not like DQN taking three continues images as input, DRQN [19] replaced several normal convolutional layers with recurrent neural networks (RNN) and long short term memory (LSTM) layers.", "startOffset": 58, "endOffset": 62}, {"referenceID": 19, "context": "Dueling network [20] separated the Q-value estimator to two independent network structures, one for the state value function and one for the advantage function.", "startOffset": 16, "endOffset": 20}, {"referenceID": 20, "context": "For robotics control, deep reinforcement learning also accomplished various simulated robotics control tasks [21].", "startOffset": 109, "endOffset": 113}, {"referenceID": 21, "context": "In the continues control domain [22], the same model-free algorithm robustly solved more than 20 simulated physics tasks.", "startOffset": 32, "endOffset": 36}, {"referenceID": 20, "context": "Considering the complexity of control problems, model-based reinforcement learning algorithm was proved to be able to accelerate the learning procedure [21] so that the deep reinforcement learning framework could handle more challenging problems.", "startOffset": 152, "endOffset": 156}, {"referenceID": 22, "context": "As in [23], the motion control of a Baxter robot motivated by deep reinforcement learning could make sense", "startOffset": 6, "endOffset": 10}, {"referenceID": 1, "context": "In this paper, we implement the same model-free deep reinforcement learning framework like DQN [2] in the simulation environment as well.", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "In the end-to-end training procedure, we set a small learning rate for the gradient descent of data representation structure compared with the learning rate used in the training of the supervised learning model [1].", "startOffset": 211, "endOffset": 214}, {"referenceID": 0, "context": "In our previous work [1], the training datasets were collected in structured corridor environments.", "startOffset": 21, "endOffset": 24}, {"referenceID": 0, "context": "These newly created scenes have never been used in the training of our previous supervised learning model [1].", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "They have been separately proven to be effective in our previous work [1] [3].", "startOffset": 70, "endOffset": 73}, {"referenceID": 2, "context": "They have been separately proven to be effective in our previous work [1] [3].", "startOffset": 74, "endOffset": 77}, {"referenceID": 1, "context": "The essential assumption in DQN [2] is the Bellman equation, which", "startOffset": 32, "endOffset": 35}, {"referenceID": 23, "context": "Note that dropout layers are eliminated in test procedure [24].", "startOffset": 58, "endOffset": 62}, {"referenceID": 1, "context": "Similar as [2], we use the memory replay method and the \u03b5-greedy training strategy to control the dynamic distribution of training samples.", "startOffset": 11, "endOffset": 14}, {"referenceID": 0, "context": "At the beginning of the training, Convolutional layers are initialized by copying the weights trained in [1] for the same layer structure.", "startOffset": 105, "endOffset": 108}, {"referenceID": 2, "context": "A simple policy learning networks structure was also separately proved in [3] with three moving commands as output.", "startOffset": 74, "endOffset": 77}, {"referenceID": 0, "context": "training of the supervised learning model [1], here we use a much smaller fixed learning rate in the end-to-end training for the deep reinforcement learning model.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "As the only feedback to motivate the network convergence, the negative reward for the collision between the robot and obstacles must be very large as in [3].", "startOffset": 153, "endOffset": 156}, {"referenceID": 24, "context": "All models are trained and tested with Caffe [25] on a single NVIDIA GeForce GTX 690.", "startOffset": 45, "endOffset": 49}, {"referenceID": 1, "context": "The value itself can limited present the sum of the future gains [2].", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "The trained supervised learning (SL) model from [1] and the reinforcement learning (RL) model from [3] are compared directly without any revising for the model structure or tuning for the weights.", "startOffset": 48, "endOffset": 51}, {"referenceID": 2, "context": "The trained supervised learning (SL) model from [1] and the reinforcement learning (RL) model from [3] are compared directly without any revising for the model structure or tuning for the weights.", "startOffset": 99, "endOffset": 102}, {"referenceID": 0, "context": "4 depicts the trajectory after normalizing the counts of trajectory points to [0, 1] in each map grid of the training environment.", "startOffset": 78, "endOffset": 84}, {"referenceID": 0, "context": "The counts of points in every map grid is normalized to [0,1].", "startOffset": 56, "endOffset": 61}, {"referenceID": 2, "context": "In the training of RL model [3], only the weights of the fully-connected network for policy iterations were updated iteratively.", "startOffset": 28, "endOffset": 31}, {"referenceID": 25, "context": "In [26], the strongest activation areas of the feature representations are presented by backtracking the receptive field in the source input.", "startOffset": 3, "endOffset": 7}, {"referenceID": 26, "context": "Multiply it with a convolutional kernel sizing 1\u00d7 15\u00d7 15, which is fixed with bilinear weights as the one used for upsampling of semantic segmentation in [27].", "startOffset": 154, "endOffset": 158}, {"referenceID": 0, "context": "from the SL model [1].", "startOffset": 18, "endOffset": 21}, {"referenceID": 0, "context": "Before transported to Softmax layer, feature representations of supervised learning model were firstly transformed to five values related to the five commands in [1].", "startOffset": 162, "endOffset": 165}, {"referenceID": 0, "context": "Notice that, these real world scenes are not included in the training datasets for the SL model [1] either.", "startOffset": 96, "endOffset": 99}, {"referenceID": 0, "context": "We initialized the weights of convolutional networks by a previously trained model based on our previous work [1].", "startOffset": 110, "endOffset": 113}, {"referenceID": 1, "context": "To navigate in more complicated even outdoor environments, the continues raw RGB images should also be considered as inputs like in [2] but not single depth image.", "startOffset": 132, "endOffset": 135}, {"referenceID": 26, "context": "The semantic extraction ability of CNN for RGB images has been fully proved [27].", "startOffset": 76, "endOffset": 80}, {"referenceID": 21, "context": "The revised deep reinforcement learning algorithms for continued control [22] should also be considered to improve the learning efficiency.", "startOffset": 73, "endOffset": 77}], "year": 2016, "abstractText": "Exploration in an unknown environment is the core functionality for mobile robots. Learning-based exploration methods, including convolutional neural networks, provide excellent strategies without human-designed logic for the feature extraction [1]. But the conventional supervised learning algorithms cost lots of efforts on the labeling work of datasets inevitably. Scenes not included in the training set are mostly unrecognized either. We propose a deep reinforcement learning method for the exploration of mobile robots in an indoor environment with the depth information from an RGB-D sensor only. Based on the Deep Q-Network framework [2], the raw depth image is taken as the only input to estimate the Q values corresponding to all moving commands. The training of the network weights is end-to-end. In arbitrarily constructed simulation environments, we show that the robot can be quickly adapted to unfamiliar scenes without any man-made labeling. Besides, through analysis of receptive fields of feature representations, deep reinforcement learning motivates the convolutional networks to estimate the traversability of the scenes. The test results are compared with the exploration strategies separately based on deep learning [1] or reinforcement learning [3]. Even trained only in the simulated environment, experimental results in real-world environment demonstrate that the cognitive ability of robot controller is dramatically improved compared with the supervised method. We believe it is the first time that raw sensor information is used to build cognitive exploration strategy for mobile robots through end-to-end deep reinforcement learning.", "creator": "LaTeX with hyperref package"}}}