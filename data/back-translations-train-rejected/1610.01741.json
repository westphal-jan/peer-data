{"id": "1610.01741", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Combining Generative and Discriminative Neural Networks for Sleep Stages Classification", "abstract": "Sleep stages pattern provides important clues in diagnosing the presence of sleep disorder. By analyzing sleep stages pattern and extracting its features from EEG, EOG, and EMG signals, we can classify sleep stages. This study presents a novel classification model for predicting sleep stages with a high accuracy. The main idea is to combine the generative capability of Deep Belief Network (DBN) with a discriminative ability and sequence pattern recognizing capability of Long Short-term Memory (LSTM). We use DBN that is treated as an automatic higher level features generator. The input to DBN is 28 \"handcrafted\" features as used in previous sleep stages studies. We compared our method with other techniques which combined DBN with Hidden Markov Model (HMM).In this study, we exploit the sequence or time series characteristics of sleep dataset. To the best of our knowledge, most of the present sleep analysis from polysomnogram relies only on single instanced label (nonsequence) for classification. In this study, we used two datasets: an open data set that is treated as a benchmark; the other dataset is our sleep stages dataset (available for download) to verify the results further. Our experiments showed that the combination of DBN with LSTM gives better overall accuracy 98.75\\% (Fscore=0.9875) for benchmark dataset and 98.94\\% (Fscore=0.9894) for MKG dataset. This result is better than the state of the art of sleep stages classification that was 91.31\\%.", "histories": [["v1", "Thu, 6 Oct 2016 06:05:16 GMT  (471kb,D)", "http://arxiv.org/abs/1610.01741v1", "Submitted to Computational Intelligence and Neuroscience (Hindawi Publishing). 13 pages"]], "COMMENTS": "Submitted to Computational Intelligence and Neuroscience (Hindawi Publishing). 13 pages", "reviews": [], "SUBJECTS": "cs.LG cs.NE", "authors": ["endang purnama giri", "mohamad ivan fanany", "aniati murni arymurthy"], "accepted": false, "id": "1610.01741"}, "pdf": {"name": "1610.01741.pdf", "metadata": {"source": "CRF", "title": "Combining Generative and Discriminative Neural Networks for Sleep Stages Classification", "authors": ["Endang Purnama Giri", "Mohamad Ivan Fanany", "Aniati Murni Arymurthy"], "emails": ["epgthebest@gmail.com"], "sections": [{"heading": null, "text": "This year, it will be able to find a solution that adapts to the needs of the people."}, {"heading": "1 Proposed Method", "text": "This section describes our proposed method compared to previous methods. Scheme for each method in Figure 2."}, {"heading": "1.1 Architecture of proposed method", "text": "As already mentioned, our goal in this study is to propose and evaluate a new hybrid deep architecture classification model for the automatic classification of sleep stages. To achieve this goal, we combine the generative excellence of DBN and the discriminatory ability of the intelligent time sequence of LSTM. The model started with DBN and was finalized by LSTM. The 28 data features as input of DBN are pre-trained with DBN. After we finish with DBN, the output of this stage is used as input for the LSTM. At the end with LSTM, the class label will be predictable. 1. DBN model architecture input-neuron for the DBN architecture is 28 neurons. For the pre-trained process, we use two layers DBN with some neurons for the visible unit 200 neurons and for hidden unit 200 neurons. Batch training size for each RBM 200 neuron and light-light unit is similar to DN for the light-light unit."}, {"heading": "1.2 Compared Techniques", "text": "In this study, we compare DBN + LSTM with the accuracy and F score of only DBN, only LSTM, and DBN with the Hidden Markov Model. We evaluate HMM to compare our proposed technique with the resulting study in [5]. To get an optimal value of the earlier sequence data for LSTM, we use three variations of input sequence size 5, 10, and 15."}, {"heading": "2 Experiment Setup", "text": "The experiment is divided into two sections. In the first experiment we use benchmark data sets for analysis and in the second experiment we use the MKG data set. All experiments use the feature extraction module DBN module and the HMM module, which is implemented on Matlab 15.a by [5]. The environment for operating these three modules is the Windows 8 operating system on a computer with Intel i7-4700HQ 2.6 GHz CPU and 4 GB RAM. On the other hand, LSTM implements module on KERAS of the Python library [2] with the Tensor flow backend. The environment for operating the LSTM module is the operating system Ubuntu 14.04 on a computer with 4xNVidia GTX Titanium and 128 GB RAM. First experiment: The data are five nights in which benchmark data is recorded, benchmark data sets are used for the experiment."}, {"heading": "3 Result", "text": "This section is divided into three sub-sections: result of the first experiment, result of the second experiment and runtime analysis."}, {"heading": "3.1 First Experiment Result", "text": "The model based on this experiment is \"only\" DBN \"(DBN + HMM),\" only LSTM \"(LSTM) and\" DBN + LSTM \"(LSTM). For the model using the LSTM module, three different sizes of sequence acquisition (5, 10, and 15) were used. Figure 3 shows the result of the average accuracy for each round (fold) using LSTM model. We have also tried to set the input sequence size equal to 20, but the accuracy is lower than the input sequence 15. From this result, we can conclude that the size of the input sequence 15 is optimal for the LSTM model."}, {"heading": "3.2 Second Experiment Result", "text": "Based on the result from the first experiment for this article, we will focus on the two top levels of accuracy and F-score (DBN + HMM and DBN + LSTM). The input sequence size for LSTM is 5, the best result on the first level of the experiment. Using MKG data set, we get the result of accuracy for each model, as shown in Table 5 and Figure 6. From the second, when data MKG was used, we get almost similarity result. DBN + LSTM we get an excellent result of accuracy, the delta between the accuracy of DBN + LSTM to the accuracy of DBN + HMM is about 0.36; this value is significant enough if we look at the variation of accuracy for each fold, the accuracy value of DBN + HMM are between 0.2011 (Fold 8) and the largest value 0.7654 (Fold 3) which is the domain of accuracy class."}, {"heading": "3.3 Running Time", "text": "How about runtime? From the experiment, we measure the greatest computing time is when the task of pretraining DBN is performed. So, for about the number of strokes, the computing time is 32,500 and the validation data is about 6,500, we need computing time between 42 minutes and 45 minutes. So, if we run for the general benchmark data set, the computing time is about four hours and for MKG data about 17 hours. On the other hand, the computing time for the LSTM process is very fast because we perform this task on Tensor Flow Python library, which supports parallel GPU processing. Use ten different sizes of number data (from 2,000 to 20,000) and three different sizes of input sequence (5, 10 and 15); we perform five-line measurements for each scenario. Figure 8 and Figure 9 show the result measurement on average. From the graph, we can see that the training time for 20,000 data is only about 10 / 10 minutes for the input size, 13 minutes for the input size, and 13 minutes for the execution size."}, {"heading": "4 Conclusion", "text": "This paper shows that the combined ability of the generative architecture of DBN (through unsupervised pre-training) and the superior discriminatory power of LSTM for sequence data can achieve a high degree of classification accuracy on the classification problem of sleep stages. In this study, we use the sequence or time series characteristics of the sleep data set. To our knowledge, most of the current sleep analysis from the polysomnogram is based on a single instance label (non-sequence) for classification only. From our experiment, we get a better accuracy of our model architecture compared to the state of the art in [9]. The state of the art in classification of sleep stages is 91.31%, while our proposed architecture model can achieve the best accuracy over 98.75% of the benchmark dataset and 98.94% of our MKG dataset use whether the analysis of the predictive data from our sleep stages classification is the second most inaccurate of the state system and the most difficult of the class 13."}, {"heading": "5 Acknowledgment", "text": "This work is supported by the Higher Education Center of Excellence Research Grant funded by the Indonesian Ministry of Research and Higher Education (Contract No. 1068 / UN2.R12 / HKP.05. 00 / 2016)."}, {"heading": "6 Conflict of Interests", "text": "The authors declare that there is no conflict of interest in the publication of this paper."}], "references": [{"title": "Monitoring and staging human sleep", "author": ["M.A. Carskadon", "A. Rechtschaffen"], "venue": "Principles and practice of sleep medicine, Saunders Elsevier, 4:265,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2005}, {"title": "Keras", "author": ["F. Chollet"], "venue": "https://github.com/fchollet/keras,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Sleep stage classification with ECG and respiratory effort", "author": ["P. Fonseca", "X. Long", "M. Radha", "R. Haakma", "R.M. Aarts", "J. Rolink"], "venue": "Physiol Meas.10.1088/0967-3334/36/10/2027. Epub 2015 Aug 19.PMID: 26289580, 36, issue10:1\u201315,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Sleep Stages Classification using Shallow Classifier", "author": ["E.P. Giri", "M. Fanany", "A.M. Arymurthy"], "venue": "International Conference on Advanced Computer Science and Information Systems ICACSIS, Proceeding of the IEEE Int Conf on Computer Sciences, pages 297 \u2013 301,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "Sleep Stage Classification Using Unsupervised Feature Learning", "author": ["M. Langkvist", "L. Karlsson", "A. Loutfi"], "venue": "Hindawi Publishing Corporation, Advances in Artificial Neural Systems, 2012, Article ID 107046:9 Pages,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Automatic sleep stage detection and classification: Distinguishing between patients with periodic limb movements, sleep apnea hypopnea syndrome, and healthy controls using electrooculography (eog) signals", "author": ["E. Malaekah", "S. Shahrbabaki", "D. Cvetkovic"], "venue": "Journal of Bioprocessing & Biotechniques, 14:1\u20136,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "A transition-constrained discrete hidden Markov model for automatic sleep staging", "author": ["S.-T. Pan", "C.-E. Kuo", "J.-H. Zeng", "S.-F. Liang"], "venue": "BioMedical Engineering OnLine 2012, pages 1\u201319,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2012}, {"title": "A manual of standardized terminology, techniques and scoring system for sleep stages of human subjects", "author": ["A. Rechtschaffen", "A. Kales"], "venue": "National Institutes of Health Publications, US Government Printing Office, 204:54,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1968}, {"title": "Automatic sleep stage classification based on sparse deep belief net and combination of multiple classifiers", "author": ["J. Zhang", "Y. Wu", "J. Bai", "F. Chen"], "venue": "Sage Journals, 14:1\u20139,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "and A", "author": ["L. Zoubek", "S. Charbonnier", "S. Lesecq"], "venue": "B. anf Florian Chapotot. A two-steps sleep/wake stages classifier taking into account artefacts in the polysomnographic signals. Proceedings of the 17th World Congress The International Federation of Automatic Control,Seoul, Korea, July 6-11, 2008, 2012, Article ID 107046,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2008}], "referenceMentions": [{"referenceID": 7, "context": "When a human fall asleep they can go through five sleep transitions or sleep stages that consist of wakefulness (W), sleep stages S1, S2, S3 or Rapid Eye Movement (REM) [8].", "startOffset": 169, "endOffset": 172}, {"referenceID": 0, "context": "By monitoring their proportion and distribution, the sleep stages of a person\u2019s sleep and sleep disorder can be diagnosed [1].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "Sleep stages study in [3] uses a feature that was extracted from ECG", "startOffset": 22, "endOffset": 25}, {"referenceID": 6, "context": "Typical polysomnographic recordings for each sleep stages slow-wave sleep (SWS/S3) associated with deep sleep, Each raw signal shows the electroencephalography (EEG), electrooculography (EOG) and electromyography (EMG) (source [7]).", "startOffset": 227, "endOffset": 230}, {"referenceID": 3, "context": "In [4] various shallow classifiers were evaluated to classify sleep stages based on features that were used in [5].", "startOffset": 3, "endOffset": 6}, {"referenceID": 4, "context": "In [4] various shallow classifiers were evaluated to classify sleep stages based on features that were used in [5].", "startOffset": 111, "endOffset": 114}, {"referenceID": 3, "context": "In our previous study [4], we found that it is potential to obtain performance improvement using the deep neural network techniques such as Deep Belief Network (DBN) and Long-Short Term of Memory (LSTM).", "startOffset": 22, "endOffset": 25}, {"referenceID": 9, "context": "In [10], using two-stage classification, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 5, "context": "Later, in 2015 [6] increased the accuracy level of the state the art to 90% while utilizing EOG channel and using k-nearest neighbor (KNN) as the classifier.", "startOffset": 15, "endOffset": 18}, {"referenceID": 8, "context": "Still in 2015, using deep belief net and the combination of multiple classifiers [9] the total accuracy of state the art increased to 91.", "startOffset": 81, "endOffset": 84}, {"referenceID": 4, "context": "We evaluate HMM to compare our proposed technique to the resulting study in [5].", "startOffset": 76, "endOffset": 79}, {"referenceID": 4, "context": "a by [5].", "startOffset": 5, "endOffset": 8}, {"referenceID": 1, "context": "On the other hand LSTM, module implement on KERAS of python library [2] with the tensor-flow backend.", "startOffset": 68, "endOffset": 71}, {"referenceID": 8, "context": "Compare to the result of the state of the art problem on [9] with value 91.", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "On the other hand, the differences are: we have feature extraction stage otherwise in [9] the approach directly process to the raw data, and for the classifier we use single classifier (LSTM) in the final stage otherwise in [9] use multiple classifiers to perform the discriminative process.", "startOffset": 86, "endOffset": 89}, {"referenceID": 8, "context": "On the other hand, the differences are: we have feature extraction stage otherwise in [9] the approach directly process to the raw data, and for the classifier we use single classifier (LSTM) in the final stage otherwise in [9] use multiple classifiers to perform the discriminative process.", "startOffset": 224, "endOffset": 227}, {"referenceID": 8, "context": "From our experiment, we get better performance of accuracy for our model architecture compared with the state of the art in [9].", "startOffset": 124, "endOffset": 127}], "year": 2016, "abstractText": "Sleep stages pattern provides important clues in diagnosing the presence of sleep disorder. By analyzing sleep stages pattern and extracting its features from EEG, EOG, and EMG signals, we can classify sleep stages. This study presents a novel classification model for predicting sleep stages with a high accuracy. The main idea is to combine the generative capability of Deep Belief Network (DBN) with a discriminative ability and sequence pattern recognizing capability of Long Short-term Memory (LSTM). We use DBN that is treated as an automatic higher level features generator. The input to DBN is 28 \u201dhandcrafted\u201d features as used in previous sleep stages studies. We compared our method with other techniques which combined DBN with Hidden Markov Model (HMM).In this study, we exploit the sequence or time series characteristics of sleep dataset. To the best of our knowledge, most of the present sleep analysis from polysomnogram relies only on single instanced label (nonsequence) for classification. In this study, we used two datasets: an open data set that is treated as a benchmark; the other dataset is our sleep stages dataset (available for download) to verify the results further. Our experiments showed that the combination of DBN with LSTM gives better overall accuracy 98.75% (Fscore=0.9875) for benchmark dataset and 98.94% (Fscore=0.9894) for MKG dataset. This result is better than the state of the art of sleep stages classification that was 91.31%i. Keywords\u2014 sleep stages classification, long short term memory, deep belief network, deep learning", "creator": "LaTeX with hyperref package"}}}