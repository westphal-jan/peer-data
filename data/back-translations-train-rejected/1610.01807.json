{"id": "1610.01807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Parallel Large-Scale Attribute Reduction on Cloud Systems", "abstract": "The rapid growth of emerging information technologies and application patterns in modern society, e.g., Internet, Internet of Things, Cloud Computing and Tri-network Convergence, has caused the advent of the era of big data. Big data contains huge values, however, mining knowledge from big data is a tremendously challenging task because of data uncertainty and inconsistency. Attribute reduction (also known as feature selection) can not only be used as an effective preprocessing step, but also exploits the data redundancy to reduce the uncertainty. However, existing solutions are designed 1) either for a single machine that means the entire data must fit in the main memory and the parallelism is limited; 2) or for the Hadoop platform which means that the data have to be loaded into the distributed memory frequently and therefore become inefficient. In this paper, we overcome these shortcomings for maximum efficiency possible, and propose a unified framework for Parallel Large-scale Attribute Reduction, termed PLAR, for big data analysis. PLAR consists of three components: 1) Granular Computing (GrC)-based initialization: it converts a decision table (i.e., original data representation) into a granularity representation which reduces the amount of space and hence can be easily cached in the distributed memory: 2) model-parallelism: it simultaneously evaluates all feature candidates and makes attribute reduction highly parallelizable; 3) data-parallelism: it computes the significance of an attribute in parallel using a MapReduce-style manner. We implement PLAR with four representative heuristic feature selection algorithms on Spark, and evaluate them on various huge datasets, including UCI and astronomical datasets, finding our method's advantages beyond existing solutions.", "histories": [["v1", "Thu, 6 Oct 2016 10:36:48 GMT  (2485kb,D)", "http://arxiv.org/abs/1610.01807v1", "14 pages, 10 figures"]], "COMMENTS": "14 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.DC cs.AI", "authors": ["junbo zhang", "tianrui li", "yi pan"], "accepted": false, "id": "1610.01807"}, "pdf": {"name": "1610.01807.pdf", "metadata": {"source": "CRF", "title": "Parallel Large-Scale Attribute Reduction on Cloud Systems", "authors": ["Junbo Zhang", "Tianrui", "Li", "Yi Pan"], "emails": ["jbzhang@my.swjtu.edu.cn,", "trli@swjtu.edu.cn);", "pan@cs.gsu.edu)."], "sections": [{"heading": null, "text": "Index Terms - Attribute Reduction, SPARK, Model Parallelism, Big Data, Cloud"}, {"heading": "1 INTRODUCTION", "text": "In fact, it is that we are able to assert ourselves, that we are able to change the world, and that we are able to change the world in order to change the world. \""}, {"heading": "2 PRELIMINARIES", "text": "In this section, we will first consider a unified framework for the selection of original features and four representative measures of significance of attributes, and then introduce the distributed data processing platform used in this essay."}, {"heading": "2.1 A Unified Framework for Feature Selection", "text": "Figure 1 shows a uniform framework for the sequential selection of characteristics. The first step is to generate multiple attribute subsets. The second step is to use an attribute significance measurement to evaluate all the generated candidates and output the current optimal attribute subset. The third step is to check whether the stop criterion (e.g. number of characteristics) is met, a) if yes, the output of the current optimal attribute subset as the final optimal subset; b) otherwise, it goes back to the first step, generating candidates until the selected attribute subset meets the stop criterion. Within this framework, the key component is the evaluation and almost the computational work comes from this step. Therefore, the efficiency of the feature selection depends on the evaluation. The most important contribution of this work is to expand this framework and propose a parallel framework to accelerate the entire evaluation process. [For the evaluation of characteristics, there are two main types of learning algorithms used]."}, {"heading": "2.1.1 Heuristic attribute reduction algorithm", "text": "In order to efficiently and effectively select an optimal feature subset, many heuristic attribute reduction algorithms were proposed during the insertion of two decades [33], [29], [34], [35], most of which employ a forward search strategy. In each forward heuristic feature subset, the feature reduction algorithm, starting with the attributes (called core) with the satisfied internal meaning (e.g. greater than a threshold), is iteratively included in the feature subgroup until the selected feature subgroup meets the holding criterion, and finally we can obtain an attribute reduction [6]. Let S = (U, C, D) be a decision table, B C. We iteratively designate the inner and outer meaning of a feature until the selected feature subgroup meets the holding criterion, and finally we can obtain an attribute reduction [6]. Let S = (U, C, Sign C, Sign B) be a sign subgroup."}, {"heading": "2.1.2 Representative significance measures of attributes", "text": "For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory (= PR = = = D = = PR = = D = = P = P = P = P = P = P = P = P (= P = P = P = P = P = P = P = P = P = P (= P = P = P = P = P = P = P))). Here, too, we focus only on these four representative attribute reduction methods. In view of a decision table S = (U, C, D), B C, the condition U / B = P (E1, E2, EE} and the decision U / D = D1, Dm}, a decision table S = (U, C, D), B C, the condition U / B (E2, EE) can be achieved. Through these assessments, we will briefly review four types of measurement of attributes as follows. (1) Positive Region-based Method Definition 2.3 (PR)."}, {"heading": "2.2 SPARK", "text": "We propose a unified framework for the parallel reduction of attributes distributed in nature. Therefore, it can in principle be implemented on all distributed data processing platforms (e.g. HADOOP, SPARK). In our implementation, SPARK [28] is chosen for its characteristics: a) SPARK has evolved into a full-fledged distributed computing engine for large-scale data processing and facilitates cluster computing, which is indispensable for iterative algorithms; b) it provides easy operations to build parallel, distributed and fault-tolerant applications; c) it is implemented in Scala, programming APIs in Scala, Java, and Python makes it a set of data scientists who have fast and full benefits from Spark technology."}, {"heading": "3 PARALLEL LARGE-SCALE ATTRIBUTE REDUCTION", "text": "In this section, we first propose a parallel hybrid attribute reduction framework, then present a standardized representation of evaluation functions, and finally a SPARK-based algorithm for large-scale attribute reduction."}, {"heading": "3.1 A Parallel Framework for Attribute Reduction", "text": "We provide a parallel attribute reduction framework here, which essentially consists of the following steps: 1) generating candidates using a specific search strategy (e.g. heuristic search in this essay). Generally, this step generates a pool of attributes. We propose to use a multi-processing method to parallelise the processing of multiple attributes. Since this method is model-related, we call the method Model-Parallelism (MP). 2) By assembling a pool of work processes, each of which is used to calculate the meaning of an attribute in SPARK. Here, R is the attribute that is reduced in the current loop. SPARK uses this processing to parallelise the processing of the meaning of an attribute. Therefore, it is called Data-Parallelism (DP). 3) After calculating the signing of all attributes, the optimal attribute is selected according to the best significance and inserted into the next loop."}, {"heading": "3.2 A Unified Representation of Evaluation Functions", "text": "In this section we first present the simplification and dismantling of evaluation functions, which represent a uniform view for the calculation of the value of evaluation functions."}, {"heading": "3.2.1 Simplification and Decomposition of Evaluation Functions", "text": "(D)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3.2.2 MapReduce-based Method", "text": "The detailed SPARK-based algorithm is introduced in Section 4. Note that the basic parallel idea is similar, but SPARK provides a much more comprehensive API that goes beyond the native MAP and REDUCE functions. \u2022 MAP Phase: Each map worker reads data divided Uk from a distributed file system (e.g. HDFS) and then maps its elements into a key-value pair (xB, xD). \u2022 SUM Phase: Finally, the main function collects the results from all workers whose key and value are the collected values (Ei, D). \u2022 REDUCE Phase: Each reduce worker receives a group of data whose key and value are EiB or Si = (Ei, xD), respectively. \u2022 SUM Phase: Finally, the main function collects the results from all workers whose key and value are the collected values (Ei, D)."}, {"heading": "3.3 Granularity Representation", "text": "En \"eG,\" according to Euler, \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \"Egg,\" \",\", \",\", \",\", \",\", \",\", \",\", \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\", \",\" \",\" \",\" \",\", \",\", \"\", \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\" \",\", \",\" \",\", \"\", \",\" \",\", \"\", \"\", \"\", \"\" \",\" \",\" \"\", \"\" \",\" \"\", \"\" \",\" \",\" \"\", \"\" \"\" \",\" \",\" \"\" \"\", \"\" \",\" \"\" \"\", \"\" \",\" \"\" \",\" \",\" \"\", \",\" \"\" \"\", \"\", \"\" \",\" \"\" \",\" \"\" \"\", \",\" \"\", \"\" \",\" \"\" \",\" \"\""}, {"heading": "4 IMPLEMENTATION", "text": "In this section we describe the implementation of our algorithms at the forefront of SPARK [28]. As introduced in section 3, the key components of the MDP system are to be understood in the first stage of processing data and their parallelism. (D) The first stage of measuring the GrC-based initialization (lines 1-2) consisting of 1) loading data from distributed file system (e.g. HDFS) via function sc.textFile (), 2) construct the granularity representation of the decision table; 3) caching the granularity representation of the decision table; 3) caching the granularity representation in distributed memory (line 2). It means that loading data and constructing the granularity representation of the decision table (3) we represent the granularity representation in distributed memory (line 2)."}, {"heading": "4.1 Complexity Analysis", "text": "Suppose that S = (U, A) is a decision table, therefore it requires O (| U | | A |) to store the data. In GrC-based initialization, the data is read only once on loading and only the characteristic vectors of the equivalence classes and the associated cardinality are stored. Thus, the spatial complexity is O (| U / A | (| A | + 1)) = O (| U / A | | A |), where | U / A | is the total number of equivalence classes relative to U w.r.t. A."}, {"heading": "5 EXPERIMENTAL RESULTS", "text": "In this section we evaluate PLAR experimentally. We first describe the experimental setup in Section 5.1. Then we report on the results in terms of effectiveness and efficiency in Section 5.2 and Section 5.3 respectively. We show the effect of the PLAR components in Section 5.4."}, {"heading": "5.1 Setup", "text": "Cluster. We run all experiments on a cluster of 19 machines, each with at least 8 core 2.0 GHz processors, more than 8 GB of RAM, with Cent OS 6.5. All machines are connected via a Gigabit network. We run on each computer in the SPARK version 1.x cluster and configure one computer as master node and the others as slaves. The total number of cores used by SPARK is 128. All of our algorithms are implemented in the Python and PySpark programming languages. Datasets. We tested on 13 benchmark datasets of different scales. The description of each dataset, including its samples and features, is in Table 5. Among these are datasets 1-9 small datasets downloaded from UCI dataset repository1; datasets 10 and 11 have millions of samples and are each derived from KDD CUP 19992 and selected by the WEKAulator dataset as the high-dimension.12 dataset is a selection of datasets."}, {"heading": "5.2 Effectiveness", "text": "To evaluate the effectiveness of PLAR, we compare it to two baselines, including HAR and FSPA, which are described below. \u2022 HAR is an original forward heuristic attribute reduction algorithm (see Algorithm 1). \u2022 FSPA [6] is a general feature selection based on positive approximation, which is a modern algorithm in a single machine.We evaluate PLAR using four representative significance measures of attributes, including PR, SCE, LCE and CCE (see details in Section 2.1).Since the calculation and storage of large datasets is impractical for traditional machine-based approaches, we use the datasets 1-9 of Table 5 for this test. The number of cores used and the model parallelism level are both set as 8 for PLAR.1. http: / / archive.dats.uci.ssu / ml / 2. Datasets are set to PLic.u5. Datasets are faster than the AR.8 / arch.uci... Datasets are set to 3."}, {"heading": "5.3 Efficiency", "text": "To measure the efficiency of PLAR, we evaluate it under four aspects: \u2022 Comparison with single machine algorithms: see Section 5.3.1. \u2022 Comparison with distributed algorithms: see Section 5.3.2. \u2022 Influence of parallelism level of models: see Section 5.4.2. \u2022 Speedup on large and high-dimensional data: see Section 5.3.3."}, {"heading": "5.3.1 Comparison with single-machine algorithms", "text": "In this section, we compare PLAR with the most advanced single machine algorithms on 9 small data sets. The data sets used and compared baseline algorithms are identical to those in Section 5.2. In addition, we are conducting an experiment with an algorithm called PLAR-DP, which is a simplified version of PLAR that uses only data parallelism without model parallelism. Since the HAR and FSPA algorithms can only be executed in a single machine, we are also executing PLAR and PLAR-DP on an 8-core machine. PLars model parallelism level is set to 8. Here, we are using speedup = runtime of a particular HAR algorithm to measure all algorithms. Figure 7 shows that FSPA uses other algorithms in the data sets tic-tac-toe, dermatology, breast cancer wisdom and backup large.test, whose samples are very few, better than duplicate and duplicate results."}, {"heading": "5.3.2 Comparison with distributed algorithms", "text": "In this experiment, we compare PLAR with some distributed algorithms, including HadoopAR, SparkAR, and PLAR-DP, which are described as follows. \u2022 HadoopAR [38] is a parallel attribute reduction algorithm implemented on the Hadoop6 platform. \u2022 SparkAR is a modified version of HadoopAR implemented on the Spark platform.6. http: / / hadoop.apache.org / In all of these experiments, we specify the same core number, i.e., 16. Figure 8 shows the performance comparison of these 4 distributed algorithms. It is easy to see that SparkAR is much faster than HadoopAR, because HadoopAR has to read the data from HDFS each time when evaluating the attribute. On the contrary, Spark-based algorithms always read the data once into distributed memory, making the same parallel data method up to 100 times faster than HadoopAR-based algorithms can always be hadopp-based."}, {"heading": "5.3.3 Speedup on large & high-dimensional data", "text": "In this experiment, we test our algorithms with different numbers of cores for the reduction of attributes on the SDSSin Table 5 dataset. Since the total runtime for this dataset is very large, we test two configurations: 32 cores and 128 cores. For example, if we include SCE and 128 cores, we record the runtime for each iteration, the first 5 iterations cost 7312, 6696, 6793, 7659 and 7035 seconds, respectively. For the first iteration, there are a total of 5201 feature candidates. Therefore, the average elapsed time for the evaluation of an attribute is about 1,406 seconds. If the same test is performed on 32 cores, the first iteration takes 24180 seconds, i.e. 4.649 seconds per attribute. Therefore, 4 times cores can reach 4.649 1.406 and 3.3 x speeds, respectively."}, {"heading": "5.4 Effect of components of PLAR", "text": "The key components of PLAR are GrC-based initialization, data parallelism and model parallelism. The effect of data parallelism has been extensively verified in Section 5.3. Here we mainly show the effects of GrC-based initialization and model parallelism."}, {"heading": "5.4.1 Effect of GrC-based initialization", "text": "In this experiment, we test the effect of GrC-based initialization on the KDD99 and WEKA15360 datasets from Table 5. We can observe that execution with GrC-based initialization on both the KDD99 and WEKA15360 datasets is extremely low using four different attribute significance measures, showing that GrC-based initialization can efficiently accelerate the entire processing of attribute selection."}, {"heading": "5.4.2 Effect of model parallelism level", "text": "In this section, we test our algorithms with different parallelism stages of the model for reducing the attributes on the DatasetGisette of Table 5. For this experiment, we use 64 cores and the measurement method of the attribute SCE. Gisette consists of 5000 characteristics and 6000 samples. Let us assume that the attribute core is empty, he evaluates successively 5000 characteristic candidates in the 1st iteration, 4999 in the 2nd iteration, 4998 in the 3rd iteration,..., which requires more model parallelism. We record the runtime of the first 5 iterations, which is in Table 12. If the parallelism stage of the model is equal to 1, Plar is degraded into Plar-DP (data parallelism only). We record the runtime of the first 5 iterations, which is shown in Table 12. If the parallelism stage of the Model 2 is plasma, Plar is twice as fast as plasma as plasma plasma plasma. Since the parallelism stage of the model increases lasma parallelism of plasma, we can increase the lasma level of the plasma to the plasma transit time of the plasma."}, {"heading": "6 CONCLUSIONS", "text": "GrC-based initialization transforms the original decision table into a granularity representation that reduces the spatial complexity of O (| U | | A |) to O (| U / A | | A |) and can efficiently accelerate calculation. Model parallelism is a natural parallelism strategy that means we can evaluate all feature candidates at the same time. It becomes much more efficient when there are thousands of features. Data parallelism means that we can calculate the meaning of a single feature in parallel, benefiting from our separation method for the evaluation functions. Extensive experimental results show that PLAR is more efficient and scalable than existing solutions to major problems."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work is supported by the National Science Foundation of China (No. 61573292, 61572406)."}], "references": [{"title": "The Principles and Methodologies of Big Data Mining\u2014-From the Perspectives of Granular Computing and Rough Sets", "author": ["T. Li", "C. Luo", "H. Chen", "J. Zhang"], "venue": "Science Press (In Chinese),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Pickt: A solution for big data analysis", "author": ["T. Li", "C. Luo", "H. Chen", "J. Zhang"], "venue": "International Conference on Rough Sets and Knowledge Technology. Springer, 2015, pp. 15\u201325.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Significance and challenges of big data research", "author": ["X. Jin", "B.W. Wah", "X. Cheng", "Y. Wang"], "venue": "Big Data Research, vol. 2, no. 2, pp. 59\u201364, 2015.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Consistency-based search in feature selection", "author": ["M. Dash", "H. Liu"], "venue": "Artificial Intelligence, vol. 151, no. 1, pp. 155\u2013176, 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Selecting discrete and continuous features based on neighborhood decision error minimization", "author": ["Q. Hu", "W. Pedrycz", "D. Yu", "J. Lang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), vol. 40, no. 1, pp. 137\u2013150, 2010.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Positive approximation: An accelerator for attribute reduction in rough set theory", "author": ["Y.H. Qian", "J.Y. Liang", "W. Pedrycz", "C.Y. Dang"], "venue": "Artificial Intelligence, vol. 174, no. 9-10, pp. 597\u2013618, Jun. 2010.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2010}, {"title": "Hybrid attribute reduction based on a novel fuzzy-rough model and information granulation", "author": ["Q. Hu", "Z. Xie", "D. Yu"], "venue": "Pattern Recognition, vol. 40, no. 12, pp. 3509\u20133521, 2007.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient accelerator for attribute reduction from incomplete data in rough set framework", "author": ["Y. Qian", "J. Liang", "W. Pedrycz", "C. Dang"], "venue": "Pattern Recognition, vol. 44, no. 8, pp. 1658\u20131670, 2011.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2011}, {"title": "Attribute reduction for massive data based on rough set theory and mapreduce", "author": ["Y. Yang", "Z. Chen", "Z. Liang", "G. Wang"], "venue": "International Conference on Rough Sets and Knowledge Technology. Springer, 2010, pp. 672\u2013678.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2010}, {"title": "Rough sets: Theoretical aspects of reasoning about data", "author": ["Z. Pawlak"], "venue": "Dordrecht: Kluwer Academic Publishers,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 1991}, {"title": "Rudiments of rough sets", "author": ["Z. Pawlak", "A. Skowron"], "venue": "Information Sciences, vol. 177, no. 1, pp. 3\u201327, 2007.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2007}, {"title": "Rough sets: Some extensions", "author": ["Z. Pawlak", "A. Skowron"], "venue": "Information Sciences, vol. 177, no. 1, pp. 28 \u2013 40, 2007, zdzis?aw Pawlak life and work (19262006). [Online]. Available: http:// www.sciencedirect.com/science/article/pii/S0020025506001496  TECHNICAL REPORT  14", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Discovery through rough set theory", "author": ["W. Ziarko"], "venue": "Communications of the ACM, vol. 42, no. 11, pp. 54\u201357, 1999.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 1999}, {"title": "A rough sets based characteristic relation approach for dynamic attribute generalization in data mining", "author": ["T.R. Li", "D. Ruan", "W. Geert", "J. Song", "Y. Xu"], "venue": "Knowledge-Based Systems, vol. 20, no. 5, pp. 485\u2013494, 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "Rough sets based matrix approaches with dynamic attribute variation in set-valued information systems", "author": ["J. Zhang", "T. Li", "D. Ruan", "D. Liu"], "venue": "International Journal of Approximate Reasoning, vol. 53, no. 4, pp. 620\u2013635, 2012.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "The incremental method for fast computing the rough fuzzy approximations", "author": ["Y. Cheng"], "venue": "Data & Knowledge Engineering, vol. 70, no. 1, pp. 84\u2013100, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Neighborhood rough set based heterogeneous feature subset selection", "author": ["Q.H. Hu", "D.R. Yu", "J.F. Liu", "C.X. Wu"], "venue": "Information Sciences, vol. 178, no. 18, pp. 3577\u20133594, Sep. 2008.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2008}, {"title": "A group incremental approach to feature selection applying rough set technique", "author": ["J. Liang", "F. Wang", "C. Dang", "Y. Qian"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 26, no. 2, pp. 294\u2013308, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Mapreduce: simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Communications of the ACM, vol. 51, no. 1, pp. 107\u2013113, 2008.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2008}, {"title": "Hadoop: The definitive guide. ", "author": ["T. White"], "venue": "O\u2019Reilly Media, Inc.\u201d,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Twister: a runtime for iterative mapreduce", "author": ["J. Ekanayake", "H. Li", "B. Zhang", "T. Gunarathne", "S.-H. Bae", "J. Qiu", "G. Fox"], "venue": "Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing. ACM, 2010, pp. 810\u2013818.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Phoenix++: modular mapreduce for shared-memory systems", "author": ["J. Talbot", "R.M. Yoo", "C. Kozyrakis"], "venue": "Proceedings of the second international workshop on MapReduce and its applications. ACM, 2011, pp. 9\u201316.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Mars: a mapreduce framework on graphics processors", "author": ["B. He", "W. Fang", "Q. Luo", "N.K. Govindaraju", "T. Wang"], "venue": "Proceedings of the 17th international conference on Parallel architectures and compilation techniques. ACM, 2008, pp. 260\u2013269.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2008}, {"title": "Mahout in action", "author": ["S. Owen", "R. Anil", "T. Dunning", "E. Friedman"], "venue": "2012.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2012}, {"title": "A parallel method for computing rough set approximations", "author": ["J.B. Zhang", "T.R. Li", "D. Ruan", "Z.Z. Gao", "C.B. Zhao"], "venue": "Information Sciences, vol. 194, pp. 209\u2013223, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel rough set based knowledge acquisition using mapreduce from big data", "author": ["J. Zhang", "T. Li", "Y. Pan"], "venue": "Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications. ACM, 2012, pp. 20\u201327.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2012}, {"title": "Parallel attribute reduction algorithms using mapreduce", "author": ["J. Qian", "D. Miao", "Z. Zhang", "X. Yue"], "venue": "Information Sciences, vol. 279, pp. 671 \u2013 690, 2014. [Online]. Available: http://www. sciencedirect.com/science/article/pii/S0020025514004666", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2014}, {"title": "The algorithm on knowledge reduction in incomplete information systems", "author": ["J.Y. Liang", "Z.B. Xu"], "venue": "International Journal of Uncertainty Fuzziness and Knowledge-based Systems, vol. 10, no. 1, pp. 95\u2013103, Feb. 2002.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2002}, {"title": "The feature selection problem: Traditional methods and a new algorithm", "author": ["K. Kira", "L.A. Rendell"], "venue": "Proceedings of the 10th National Conference on Artificial Intelligence, 1992, pp. 129\u2013134.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 1992}, {"title": "Feature selection using rough sets theory", "author": ["M. Modrzejewski"], "venue": "Machine Learning: ECML-93. Springer, 1993, pp. 213\u2013226.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1993}, {"title": "Consistency measure, inclusion degree and fuzzy measure in decision tables", "author": ["Y. Qian", "J. Liang", "C. Dang"], "venue": "Fuzzy sets and systems, vol. 159, no. 18, pp. 2353\u20132377, 2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "A new method for measuring uncertainty and fuzziness in rough set theory", "author": ["J.Y. Liang", "K.S. Chin", "C.Y. Dang", "R.C.M. Yam"], "venue": "International Journal of General Systems, vol. 31, no. 4, pp. 331\u2013342, 2002.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2002}, {"title": "Combination entropy and combination granulation in rough set theory", "author": ["Y.H. Qian", "J.Y. Liang"], "venue": "International Journal of Uncertainty Fuzziness and Knowledge-based Systems, vol. 16, no. 2, pp. 179\u2013193, Apr. 2008.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2008}, {"title": "Approximate entropy reducts", "author": ["D. Slezak"], "venue": "Fundamenta Informaticae, vol. 53, no. 3-4, pp. 365\u2013390, Dec. 2002.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2002}, {"title": "Resilient distributed  datasets: A fault-tolerant abstraction for in-memory cluster computing", "author": ["M. Zaharia", "M. Chowdhury", "T. Das", "A. Dave", "J. Ma", "M. McCauley", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation. USENIX Association, 2012, pp. 2\u20132.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2012}, {"title": "Rough sets and boolean reasoning", "author": ["Z. Pawlak", "A. Skowron"], "venue": "Information Sciences, vol. 177, no. 1, pp. 41\u201373, 2007.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2007}, {"title": "Plar: Parallel large-scale attribute reduction on cloud systems", "author": ["J. Zhang", "T. Li", "Y. Pan"], "venue": "2013 International Conference on Parallel and Distributed Computing, Applications and Technologies. IEEE, 2013, pp. 184\u2013191.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 1, "context": "Enormous amounts of data are generated every day with the amazing spread of computers and sensors in a widerange of domains, including social media, search engines, insurance companies, health care organizations, financial industry and many others [2].", "startOffset": 248, "endOffset": 251}, {"referenceID": 2, "context": "Now we are in the era of big data, which is characterized by 5Vs [3]: 1) Volume means the amount of data that needs to be managed is very huge; 2) Velocity means that the speed of data update is very high; 3) Variety means that the data is varied in nature and there are", "startOffset": 65, "endOffset": 68}, {"referenceID": 0, "context": "An early version (in Chinese) can be found in Chapter 3 of the book [1].", "startOffset": 68, "endOffset": 71}, {"referenceID": 3, "context": "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].", "startOffset": 142, "endOffset": 145}, {"referenceID": 4, "context": "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].", "startOffset": 147, "endOffset": 150}, {"referenceID": 5, "context": "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].", "startOffset": 152, "endOffset": 155}, {"referenceID": 6, "context": "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].", "startOffset": 157, "endOffset": 160}, {"referenceID": 7, "context": "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].", "startOffset": 162, "endOffset": 165}, {"referenceID": 8, "context": "It also helps people better understand the data by telling them which are key features, and has been attracted much attention in recent years [4], [5], [6], [7], [8], [9].", "startOffset": 167, "endOffset": 170}, {"referenceID": 9, "context": "Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13].", "startOffset": 138, "endOffset": 142}, {"referenceID": 10, "context": "Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13].", "startOffset": 144, "endOffset": 148}, {"referenceID": 11, "context": "Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13].", "startOffset": 150, "endOffset": 154}, {"referenceID": 12, "context": "Rough set theory, introduced by Pawlak in 1982, is a soft computing tool for dealing with inconsistent information in decision situations [10], [11], [12], and plays an important role in the fields of pattern recognition, feature selection and knowledge discovery [13].", "startOffset": 264, "endOffset": 268}, {"referenceID": 5, "context": "Attribute reduction in rough set theory provides a theoretic framework for consistencybased feature selection, which can retain the discernible ability of original features for the objects from the universe [6].", "startOffset": 207, "endOffset": 210}, {"referenceID": 13, "context": "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].", "startOffset": 123, "endOffset": 127}, {"referenceID": 14, "context": "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].", "startOffset": 129, "endOffset": 133}, {"referenceID": 15, "context": "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].", "startOffset": 135, "endOffset": 139}, {"referenceID": 7, "context": "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].", "startOffset": 141, "endOffset": 144}, {"referenceID": 5, "context": "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].", "startOffset": 146, "endOffset": 149}, {"referenceID": 16, "context": "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].", "startOffset": 151, "endOffset": 155}, {"referenceID": 17, "context": "To accelerate this process, incremental techniques combined with traditional rough set based methods are widely researched [14], [15], [16], [8], [6], [17], [18].", "startOffset": 157, "endOffset": 161}, {"referenceID": 5, "context": "introduced positive approximation, which is a theoretic framework for accelerating a heuristic process [6].", "startOffset": 103, "endOffset": 106}, {"referenceID": 17, "context": "developed a group of incremental feature selection algorithms based on rough sets [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 18, "context": "MapReduce, by Google, is a popular parallel programming model and a framework for processing big data on certain kinds of distributable problems using a large number of computers, collectively referred to as a cluster [19].", "startOffset": 218, "endOffset": 222}, {"referenceID": 19, "context": "implemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model.", "startOffset": 28, "endOffset": 32}, {"referenceID": 20, "context": "implemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model.", "startOffset": 42, "endOffset": 46}, {"referenceID": 21, "context": "implemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model.", "startOffset": 56, "endOffset": 60}, {"referenceID": 22, "context": "implemented, such as Hadoop [20], Twister [21], Phoenix [22] and Mars [23], which all can help developers to parallelize traditional algorithms by using MapReduce model.", "startOffset": 70, "endOffset": 74}, {"referenceID": 23, "context": "For example, Apache Mahout [24] is machine learning libraries, and produces implementations of parallel scalable machine learning algorithms on Hadoop platform by using MapReduce.", "startOffset": 27, "endOffset": 31}, {"referenceID": 24, "context": "developed a parallel algorithm for computing rough set approximations based on MapReduce [25].", "startOffset": 89, "endOffset": 93}, {"referenceID": 25, "context": "proposed a parallel rough set based knowledge acquisition method using MapReduce [26].", "startOffset": 81, "endOffset": 85}, {"referenceID": 26, "context": "presented a parallel attribute reduction algorithm based on MapReduce [27].", "startOffset": 70, "endOffset": 74}, {"referenceID": 19, "context": "However, all of these existing parallel methods make use of the classical MapReduce framework and are implemented on the Hadoop platform [20].", "startOffset": 137, "endOffset": 141}, {"referenceID": 5, "context": "In contrast, the most existing methods [6], [27] are designed 1) either for a single machine which means that the entire data must fit in the main memory and the parallelism is limited; 2) or for Hadoop which means that the data have to be loaded into the distributed memory frequently.", "startOffset": 39, "endOffset": 42}, {"referenceID": 26, "context": "In contrast, the most existing methods [6], [27] are designed 1) either for a single machine which means that the entire data must fit in the main memory and the parallelism is limited; 2) or for Hadoop which means that the data have to be loaded into the distributed memory frequently.", "startOffset": 44, "endOffset": 48}, {"referenceID": 27, "context": "support vector machine) to evaluate; b) filter which measures the attributes\u2019 significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].", "startOffset": 133, "endOffset": 137}, {"referenceID": 28, "context": "support vector machine) to evaluate; b) filter which measures the attributes\u2019 significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].", "startOffset": 148, "endOffset": 152}, {"referenceID": 29, "context": "support vector machine) to evaluate; b) filter which measures the attributes\u2019 significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].", "startOffset": 165, "endOffset": 169}, {"referenceID": 3, "context": "support vector machine) to evaluate; b) filter which measures the attributes\u2019 significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].", "startOffset": 186, "endOffset": 189}, {"referenceID": 30, "context": "support vector machine) to evaluate; b) filter which measures the attributes\u2019 significance with a metric, including information gain [29], distance [30], dependency [31] and consistency [4], [32].", "startOffset": 191, "endOffset": 195}, {"referenceID": 31, "context": "1 Heuristic attribute reduction algorithm To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy.", "startOffset": 191, "endOffset": 195}, {"referenceID": 27, "context": "1 Heuristic attribute reduction algorithm To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy.", "startOffset": 197, "endOffset": 201}, {"referenceID": 32, "context": "1 Heuristic attribute reduction algorithm To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy.", "startOffset": 203, "endOffset": 207}, {"referenceID": 33, "context": "1 Heuristic attribute reduction algorithm To select optimal feature subset efficiently and effectively, many heuristic attribute reduction algorithms were proposed during pasting two decades [33], [29], [34], [35], most of which make use of forward search strategy.", "startOffset": 209, "endOffset": 213}, {"referenceID": 5, "context": ", greater than a threshold), it takes the attribute with the maximal outer significance into the feature subset iteratively until the selected feature subset meets the stopping criterion, and finally we can get an attribute reduct [6].", "startOffset": 231, "endOffset": 234}, {"referenceID": 31, "context": "2 Representative significance measures of attributes For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35].", "startOffset": 171, "endOffset": 175}, {"referenceID": 27, "context": "2 Representative significance measures of attributes For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35].", "startOffset": 177, "endOffset": 181}, {"referenceID": 32, "context": "2 Representative significance measures of attributes For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35].", "startOffset": 183, "endOffset": 187}, {"referenceID": 33, "context": "2 Representative significance measures of attributes For efficient attribute reduction, many heuristic attribute reduction methods have been developed in rough set theory [33], [29], [34], [35].", "startOffset": 189, "endOffset": 193}, {"referenceID": 5, "context": "classified these attribute reduction methods into four categories: positive-region reduction, Shannon\u2019s conditional entropy reduction, Liang\u2019s conditional entropy reduction and combination conditional entropy reduction [6].", "startOffset": 219, "endOffset": 222}, {"referenceID": 34, "context": "The main abstraction SPARK provides is a Resilient Distributed Dataset (RDD) [36] that allows applications to keep data in the shared memory of multiple machines and can be operated on in parallel.", "startOffset": 77, "endOffset": 81}, {"referenceID": 18, "context": "These operations are similar to the map and reduce operations in the traditional MAPREDUCE [19] framework.", "startOffset": 91, "endOffset": 95}, {"referenceID": 35, "context": "As Y \u2286 Ei \u2286 U and Dj \u2208 U/D, according to the definition of equivalence partition [37], we have Y \u2286 Dj .", "startOffset": 81, "endOffset": 85}, {"referenceID": 5, "context": "\u2022 FSPA [6] is a general Feature Selection based on the Positive Approximation, which is a state-of-the-art algorithm in a single machine.", "startOffset": 7, "endOffset": 10}, {"referenceID": 36, "context": "\u2022 HadoopAR [38] is a parallel attribute reduction algorithm which is implemented in the Hadoop6 platform.", "startOffset": 11, "endOffset": 15}, {"referenceID": 5, "context": "Dataset Original features HAR-PR [6] FSPA-PR [6] PLAR-PR Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 24.", "startOffset": 33, "endOffset": 36}, {"referenceID": 5, "context": "Dataset Original features HAR-PR [6] FSPA-PR [6] PLAR-PR Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 24.", "startOffset": 45, "endOffset": 48}, {"referenceID": 5, "context": "Dataset Original features HAR-SCE [6] FSPA-SCE [6] PLAR-SCE Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 162.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "Dataset Original features HAR-SCE [6] FSPA-SCE [6] PLAR-SCE Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 162.", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "Dataset Original features HAR-LCE [6] FSPA-LCE [6] PLAR-LCE Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 300.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "Dataset Original features HAR-LCE [6] FSPA-LCE [6] PLAR-LCE Time (s) Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 300.", "startOffset": 47, "endOffset": 50}, {"referenceID": 5, "context": "Dataset Original features HAR-CCE [6] FSPA-CCE [6] PLAR-CCE Time Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 166.", "startOffset": 34, "endOffset": 37}, {"referenceID": 5, "context": "Dataset Original features HAR-CCE [6] FSPA-CCE [6] PLAR-CCE Time Selected features Time (s) Selected features Time (s) Selected features Mushroom 22 166.", "startOffset": 47, "endOffset": 50}], "year": 2016, "abstractText": "The rapid growth of emerging information technologies and application patterns in modern society, e.g., Internet, Internet of Things, Cloud Computing and Tri-network Convergence, has caused the advent of the era of big data. Big data contains huge values, however, mining knowledge from big data is a tremendously challenging task because of data uncertainty and inconsistency. Attribute reduction (also known as feature selection) can not only be used as an effective preprocessing step, but also exploits the data redundancy to reduce the uncertainty. However, existing solutions are designed 1) either for a single machine that means the entire data must fit in the main memory and the parallelism is limited; 2) or for the Hadoop platform which means that the data have to be loaded into the distributed memory frequently and therefore become inefficient. In this paper, we overcome these shortcomings for maximum efficiency possible, and propose a unified framework for Parallel Large-scale Attribute Reduction, termed PLAR, for big data analysis. PLAR consists of three components: 1) Granular Computing (GrC)-based initialization: it converts a decision table (i.e. original data representation) into a granularity representation which reduces the amount of space and hence can be easily cached in the distributed memory: 2) model-parallelism: it simultaneously evaluates all feature candidates and makes attribute reduction highly parallelizable; 3) dataparallelism: it computes the significance of an attribute in parallel using a MapReduce-style manner. We implement PLAR with four representative heuristic feature selection algorithms on SPARK, and evaluate them on various huge datasets, including UCI and astronomical datasets, finding our method\u2019s advantages beyond existing solutions.", "creator": "LaTeX with hyperref package"}}}