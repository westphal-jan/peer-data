{"id": "1610.01874", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Neural-based Noise Filtering from Word Embeddings", "abstract": "Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvement in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings.", "histories": [["v1", "Thu, 6 Oct 2016 13:52:52 GMT  (184kb,D)", "http://arxiv.org/abs/1610.01874v1", "9 pages, 4 figures, COLING 2016"]], "COMMENTS": "9 pages, 4 figures, COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["kim anh nguyen", "sabine schulte im walde", "ngoc thang vu"], "accepted": false, "id": "1610.01874"}, "pdf": {"name": "1610.01874.pdf", "metadata": {"source": "CRF", "title": "Neural-based Noise Filtering from Word Embeddings", "authors": ["Kim Anh Nguyen", "Sabine Schulte"], "emails": ["nguyenkh@ims.uni-stuttgart.de", "schulte@ims.uni-stuttgart.de", "thangvu@ims.uni-stuttgart.de"], "sections": [{"heading": "1 Introduction", "text": "The first model that encourages the use of neural-based techniques for learning word embeddings, as word embeddings are attractive because they can be learned in a thoughtless manner, from unlabeled blanks to unlabeled blanks. There are two essential approaches to creating word embeddings; the first approach makes use of neural-based techniques to learn word embeddings, such as the word embeddings model (Mikolov et al., 2013); and the second approach is based on matrix factorization (Pennington et al., 2014), building-word embeddings through factorizing word contexts."}, {"heading": "2 Learning Word Denoising Embeddings", "text": "In this section, we present the two contributions in this paper. Figure 1 illustrates our two models for learning word embedding: The first model on top, the full word that denotes the embedding \"CompEmb\" (Section 2.1), filters the noise of word embedding X to generate full word embedding X *, where the vector length of X * remains unchanged (called complete) compared to X after embedding; and the second model at the bottom of the figure, the complete word that denotes the embedding \"OverCompEmb\" (Section 2.2), filters the noise of word embedding X to produce overcomplete word embedding Z *, where the vector length of Z \u00b2 tends to be greater than the vector length of X (called overcomplete).For notations, leave X, RV \u00d7 L is an input set of word embedding where the V is the vocabulary size of X and the L is complete."}, {"heading": "2.1 Complete Word Denoising Embeddings", "text": "In this subsection, we aim to reduce noise in the given input word embeddings X by learning a denoising matrix Qc = underlying constants Qc. The full word denoising embeddings X * is then generated by projecting X into Qc. Specifically, in the face of an input problem X-RV \u00b7 L, we are trying to optimize the following objective function: argmin X, Qc, S V \u2211 i = 1 x xi \u2212 f (xi, Qc, S) 0 x, where f is a filter; S is a lateral inhibition matrix; and \u03b1 is a regulation hyperparameter. Inspired by studies of sparse modeling, the matrix S is chosen to be symmetrical and has zero on the diagonal plane. The goal of this matrix is to implement exemplary interaction between neurons and to increase the convergence speed of the neural network (2011)."}, {"heading": "2.2 Overcomplete Word Denoising Embeddings", "text": "To get over-complete word embeddings, we first use a sparse encoding method to convert the given input embeddings X to over-complete word embeddings Z. Second, we use over-complete word embeddings Z as intermediate embeddings to optimize the objective function: A series of input word embeddings X-RV-L is converted into over-complete word embeddings Z-RV-K by applying a sparse encoding method in Section 2.4 by minimizing the following equation 4: argmin X, Qo, S V-Z-i = 1-zi \u2212 f (xi, Qo, S)."}, {"heading": "2.3 Loss Function", "text": "For each term vector pair xi-X and yi-Y = f (X, Qc, S), we use cosmic similarity to measure the similarity between xi and yi as follows: sim (xi, yi) = xi \u00b7 yi-xi-xi-longyi (6) Let's minimize the difference between sim (xi, xi) and sim (xi, yi), equivalent \u2206 = 1 \u2212 sim (xi, yi). We then optimize the objective function in Equation 1 by minimizing \u2206; and the same loss function is also applied to optimize the objective function in Equation 4. Training is done by stochastic gradient descendence with the Adadelta updating rule (Zeiler, 2012)."}, {"heading": "2.4 Sparse Coding", "text": "The underlying assumption of the sparse encoding is that the input vectors can be precisely reconstructed by a sparse linear combination of some base vectors and a few number of non-zero coefficients. The aim is to implement a dense vector in RL by a sparse linear combination of a few columns of a matrix D, RL and K, in which K is a new vector length and the matrix D can be called a dictionary. Specifically, V input vectors of the L dimensions X = [x1, x2,..., xV], the dictionary and the sparse vectors are formulated as complete vectors in which K is a new vector length and the matrix D can be called a complete dictionary."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Experimental Settings", "text": "As input word embedings, we use two modern word embedding methods: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al., 2014). We use the word2vec tool3 and the web corpus ENCOW14A (Creator and Sculptor, 2012; Creator, 2015), which contains about 14.5 billion tokens to train Skip-gram models with 100 and 300 dimensions. For the GloVe method, we use pre-trained vectors with 100 and 300 dimension4, trained on 6 billion words from Wikipedia and the English Gigaword. Tanh function is used in both approaches as a non-linear activation function. Fixed filter depth T is set to 3; further hyperparameters are selected, as discussed in Section 3.2. To train the networks, we use the Theano Development Team, 2016, to apply our models with a miniature size of 2 or 0.5 layers."}, {"heading": "3.2 Hyperparameter Tuning", "text": "In both methods of denocializing word embeddings, the \"1 regularization penalty \u03b1 is set to 0.5 without coordination in Eqs 1 and 4. The method for learning overcomplete word embeddings is based on the mediation of word embeddings Z in order to minimize the objective function in Eqs 4. The rarity of Z depends on the\" 1 regularization \u03bb in Eqs 7; and the length vector K of Z is implied as K = \u03b3L. Therefore, we strive to tune and design \"Z\" in such a way that it represents the closest approximation to the original vector representation X. We perform a grid search based on what is to be discussed in Section 3.3. The hyperparameter settings are illustrated in Figures 3a and 3b for thrift and overcompleteness of word embeddings."}, {"heading": "3.3 Effects of Word Denoising Embeddings", "text": "In this section, we quantify the effects of word denocializing embeddings on three types of tasks: similarity and kinship tasks, synonymy recognition, and embedding tasks in brackets of nouns. Compared to the performance of word denocializing embeddings, we take modern word embeddings (Skip-gram and GloVe word embeddings) as baseline lines. In addition, we also use public source code5 to re-implement the two methods proposed by Faruqui et al. (2015), the vectors A (sparse overcomplete vectors) and B (sparse binary overcomplete vectors). The effects of word denoizing embeddings on the tasks are shown in Table 1. Results show that the vectors X and Z outperform the original vectors X, with the exception of NP task, where the vectors B are based on the 300-dimensional word vectors h\u00e9vectors are still the best."}, {"heading": "3.3.1 Relatedness and Similarity Tasks", "text": "For the kinship task, we use two types of data sets: MEN (Bruni et al., 2014) consists of 3000 word pairs consisting of 656 nouns, 57 adjectives and 38 verbs. The WordSim-353 kinship data set (Finkelstein et al., 2001) contains 252 word pairs. For the similarity task, we again evaluate the denosing vectors on two types of data sets: SimLex-999 (Hill et al., 2015) contains 999 word pairs, including 666 nouns, 222 verbs and 111 adjective pairs. The WordSim-353 similarity data set consists of 203 word pairs. In addition, we evaluate our denoting vectors on the WordSim-353 data set, which contains 353 pairs for both similarity and kinship relationships. We calculate cosmic similarities between the vectors of two words forming a test pair, and report on the saving vector based on both the WordSim-353 pair and the similarity in the Sord353 data set."}, {"heading": "3.3.2 Synonymy", "text": "We evaluate 80 TOEFL questions (Test of English as a Foreign Language, 1997) and 50 ESL questions (Test of English as a Foreign Language, 2001). The first set of data represents a subset of 80 multiple choice synonym questions from the TOEFL test: A word is paired with four options, one of which is a valid synonym. The second set contains 50 multiple choice synonym questions, and the goal is to choose a valid synonym from four options. For each question, we calculate the cosinal similarity between the target word and the four candidates. The proposed answer is the candidate with the highest cosinus score. We use accuracy to evaluate performance.5https: / / github.com / mfaruqui / sparse-coding"}, {"heading": "3.3.3 Phrase parsing as Classification", "text": "Lazaridou et al. (2013) introduced a dataset of noun phrases (NP) in which each NP consists of three elements: The first element is either an adjective or a noun, and the other elements are all nouns. For a given NP (such as blood pressure medicine), the task is to predict whether it is a left-bracket NP, e.g. (blood pressure medicine) or a right-bracket NP, e.g. blood (pressure medicine).The dataset contains 2227 noun sentences divided into 10 folds. For each NP, we use the average of the word vectors as characteristics to feed into the classifier by setting the hyperparameters (w1, w2 and w3) for each element (e1, e2 and e3) within the NP: ~ eNP = 13 (w1 ~ e1 + w2 ~ e2 ~ e3). Then we use the classification of the NPs by means of a support machine (Fe2) with the first Vector of the remaining NP (NP) and F3."}, {"heading": "3.4 Effects of Filter Depth", "text": "As mentioned above, the architecture of the filter f is a feed network with a fixed depth T. For each level T, the filter f attempts to reduce noise within the input vectors by evaluating these vectors based on vectors of a previous level T \u2212 1. To investigate the effects of each level T, we use pre-formed GloVe vectors with 100 dimensions to evaluate the denocializing power of the vectors in detecting the synonymy in the TOEFL dataset over several levels of T. The results are presented in Figure 4. The accuracy of synonymy detection greatly increases from 63.2% to 88.6% according to the number of levels T from 0 to 3. However, the denoizing power of the vectors falls with the number of levels T > 3. This evaluation shows that the filter f with a constant fixed depth T can be designed so that noise can be efficiently filtered for word embedding. In other words, the number of levels exceeds the T with a constant number of information T (T) losses in the case of our T."}, {"heading": "4 Conclusion", "text": "To the best of our knowledge, we are the first to work on filtering noise in Word embedding. In this work, we have presented two novel models for improving Word embedding by reducing noise in modern Word embedding models. The underlying idea in our models was to use a deep feed-forward neural network filter to reduce noise. The first model generated complete Word embedding, the second model yielded over-complete word embedding. We demonstrated that the word denoising embeddings exceeds the originally state-of-the-art Word embedding in several benchmark tasks."}, {"heading": "Acknowledgements", "text": "The research was funded by the Ministry of Education and Training of the Socialist Republic of Vietnam (Scholarship 977 / QD-BGDDT; Nguyen Kim Anh) and the DFG Heisenberg Scholarship SCHU2580 / 1 (Sabine Schulte im Walde) as well as in cooperation between Project D12 and Project A8 in the DFG Collaborative Research Centre SFB 732."}], "references": [{"title": "Using mined coreference chains as a resource for a semantic task", "author": ["Heike Adel", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1447\u20131452, Doha, Qatar.", "citeRegEx": "Adel and Sch\u00fctze.,? 2014", "shortCiteRegEx": "Adel and Sch\u00fctze.", "year": 2014}, {"title": "Multimodal distributional semantics", "author": ["Elia Bruni", "Nam-Khanh Tran", "Marco Baroni."], "venue": "Journal of Artifical Intelligence Research (JAIR), 49:1\u201347.", "citeRegEx": "Bruni et al\\.,? 2014", "shortCiteRegEx": "Bruni et al\\.", "year": 2014}, {"title": "Sparse overcomplete word vector representations", "author": ["Manaal Faruqui", "Yulia Tsvetkov", "Dani Yogatama", "Chris Dyer", "Noah A. Smith."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 1491\u20131500, Beijing, China.", "citeRegEx": "Faruqui et al\\.,? 2015", "shortCiteRegEx": "Faruqui et al\\.", "year": 2015}, {"title": "Placing search in context: The concept revisited", "author": ["Lev Finkelstein", "Evgeniy Gabrilovich", "Yossi Matias", "Ehud Rivlin", "Zach Solan", "Gadi Wolfman", "Eytan Ruppin."], "venue": "Proceedings of the 10th International Conference on the World Wide Web, pages 406\u2013414.", "citeRegEx": "Finkelstein et al\\.,? 2001", "shortCiteRegEx": "Finkelstein et al\\.", "year": 2001}, {"title": "Learning fast approximations of sparse coding", "author": ["Karol Gregor", "Yann LeCun."], "venue": "Proceedings of the 27th International Conference on Machine Learning (ICML), Haifa, Israel, pages 399\u2013406.", "citeRegEx": "Gregor and LeCun.,? 2010", "shortCiteRegEx": "Gregor and LeCun.", "year": 2010}, {"title": "Distributional structure", "author": ["Zellig S. Harris."], "venue": "Word, 10(23):146\u2013162.", "citeRegEx": "Harris.,? 1954", "shortCiteRegEx": "Harris.", "year": 1954}, {"title": "Simlex-999: Evaluating semantic models with (genuine) similarity estimation", "author": ["Felix Hill", "Roi Reichart", "Anna Korhonen."], "venue": "Computational Linguistics, 41(4):665\u2013695.", "citeRegEx": "Hill et al\\.,? 2015", "shortCiteRegEx": "Hill et al\\.", "year": 2015}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "A solution to Platos problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge", "author": ["Thomas K. Landauer", "Susan T. Dumais."], "venue": "Psychological Review, 104(2):211\u2013240.", "citeRegEx": "Landauer and Dumais.,? 1997", "shortCiteRegEx": "Landauer and Dumais.", "year": 1997}, {"title": "Fish transporters and miracle homes: How compositional distributional semantics can help NP parsing", "author": ["Angeliki Lazaridou", "Eva Maria Vecchi", "Marco Baroni."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1908\u20131913, Doha, Qatar.", "citeRegEx": "Lazaridou et al\\.,? 2013", "shortCiteRegEx": "Lazaridou et al\\.", "year": 2013}, {"title": "Linguistic regularities in continuous space word representations", "author": ["Tomas Mikolov", "Wen-tau Yih", "Geoffrey Zweig."], "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL), pages 746\u2013751, Atlanta, Georgia.", "citeRegEx": "Mikolov et al\\.,? 2013", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Integrating distributional lexical contrast into word embeddings for antonym-synonym distinction", "author": ["Kim Anh Nguyen", "Sabine Schulte im Walde", "Ngoc Thang Vu."], "venue": "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL), pages 454\u2013459, Berlin, Germany.", "citeRegEx": "Nguyen et al\\.,? 2016", "shortCiteRegEx": "Nguyen et al\\.", "year": 2016}, {"title": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images", "author": ["Bruno A. Olshausen", "David J. Field."], "venue": "Nature, 381(6583):607\u2013609.", "citeRegEx": "Olshausen and Field.,? 1996", "shortCiteRegEx": "Olshausen and Field.", "year": 1996}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543, Doha, Qatar.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "A multitask objective to inject lexical contrast into distributional semantics", "author": ["Nghia The Pham", "Angeliki Lazaridou", "Marco Baroni."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics (ACL), pages 21\u201326, Beijing, China.", "citeRegEx": "Pham et al\\.,? 2015", "shortCiteRegEx": "Pham et al\\.", "year": 2015}, {"title": "Building large corpora from the web using a new efficient tool chain", "author": ["Roland Sch\u00e4fer", "Felix Bildhauer."], "venue": "Proceedings of the 8th International Conference on Language Resources and Evaluation, pages 486\u2013493, Istanbul, Turkey.", "citeRegEx": "Sch\u00e4fer and Bildhauer.,? 2012", "shortCiteRegEx": "Sch\u00e4fer and Bildhauer.", "year": 2012}, {"title": "Processing and querying large web corpora with the COW14 architecture", "author": ["Roland Sch\u00e4fer."], "venue": "Proceedings of the 3rd Workshop on Challenges in the Management of Large Corpora, pages 28\u201334, Mannheim, Germany.", "citeRegEx": "Sch\u00e4fer.,? 2015", "shortCiteRegEx": "Sch\u00e4fer.", "year": 2015}, {"title": "Nonparametric Statistics for the Behavioral Sciences", "author": ["Sidney Siegel", "N. John Castellan."], "venue": "McGraw-Hill, Boston, MA.", "citeRegEx": "Siegel and Castellan.,? 1988", "shortCiteRegEx": "Siegel and Castellan.", "year": 1988}, {"title": "Structured sparse coding via lateral inhibition", "author": ["Arthur D. Szlam", "Karol Gregor", "Yann L. Cun."], "venue": "Advances in Neural Information Processing Systems (NIPS), 24:1116\u20131124.", "citeRegEx": "Szlam et al\\.,? 2011", "shortCiteRegEx": "Szlam et al\\.", "year": 2011}, {"title": "Theano: A Python framework for fast computation of mathematical expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints, abs/1605.02688.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Mining the web for synonyms: PMI-IR versus LSA on TOEFL", "author": ["Peter D. Turney."], "venue": "Proceedings of the 12th European Conference on Machine Learning (ECML), pages 491\u2013502.", "citeRegEx": "Turney.,? 2001", "shortCiteRegEx": "Turney.", "year": 2001}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["Matthew D. Zeiler."], "venue": "CoRR, abs/1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}], "referenceMentions": [{"referenceID": 10, "context": "The first approach makes use of neural-based techniques to learn word embeddings, such as the Skip-gram model (Mikolov et al., 2013).", "startOffset": 110, "endOffset": 132}, {"referenceID": 13, "context": "The second approach is based on matrix factorization (Pennington et al., 2014), building word embeddings by factorizing word-context co-occurrence matrices.", "startOffset": 53, "endOffset": 78}, {"referenceID": 5, "context": "(2013), relying on the distributional hypothesis (Harris, 1954) that words with similar distributions have related meanings.", "startOffset": 49, "endOffset": 63}, {"referenceID": 4, "context": ", Kim (2014)), word similarity (e.", "startOffset": 2, "endOffset": 13}, {"referenceID": 4, "context": ", Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.", "startOffset": 2, "endOffset": 63}, {"referenceID": 4, "context": ", Kim (2014)), word similarity (e.g., Pennington et al. (2014)), and parsing (e.g., Lazaridou et al. (2013)).", "startOffset": 2, "endOffset": 108}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification.", "startOffset": 13, "endOffset": 37}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet.", "startOffset": 13, "endOffset": 166}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings.", "startOffset": 13, "endOffset": 447}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings.", "startOffset": 13, "endOffset": 717}, {"referenceID": 0, "context": "For example, Adel and Sch\u00fctze (2014) applied coreference chains to Skip-gram models in order to create word embeddings for antonym identification. Pham et al. (2015) proposed an extension of a Skip-gram model by integrating synonyms and antonyms from WordNet. Their extended Skip-gram model outperformed a standard Skip-gram model on both general semantic tasks and distinguishing antonyms from synonyms. In a similar spirit, Nguyen et al. (2016) integrated distributional lexical contrast into every single context of a target word in a Skip-gram model for training word embeddings. The resulting word embeddings were used in similarity tasks, and to distinguish between antonyms and synonyms. Faruqui et al. (2015) improved word embeddings without relying on lexical resources, by applying ideas from sparse coding to transform dense word embeddings into sparse word embeddings. The dense vectors in their models can be transformed into sparse overcomplete vectors or sparse binary overcomplete vectors. They showed that the resulting vector representations were more similar to interpretable features in NLP and outperformed the original vector representations on several benchmark tasks. In this paper, we aim to improve word embeddings by reducing their noise. The hypothesis behind our approaches is that word embeddings contain unnecessary information, i.e. noise. We start out with the idea of learning word embeddings as suggested by Mikolov et al. (2013), relying on the distributional hypothesis (Harris, 1954) that words with similar distributions have related meanings.", "startOffset": 13, "endOffset": 1465}, {"referenceID": 18, "context": "The goal of this matrix is to implement excitatory interaction between neurons, and to increase the convergence speed of the neural network (Szlam et al., 2011).", "startOffset": 140, "endOffset": 160}, {"referenceID": 4, "context": ", solving low-rank approximation problem or keeping the number of terms at zero (Gregor and LeCun, 2010)).", "startOffset": 80, "endOffset": 104}, {"referenceID": 21, "context": "Training is done through Stochastic Gradient Descent with the Adadelta update rule (Zeiler, 2012).", "startOffset": 83, "endOffset": 97}, {"referenceID": 12, "context": "The underlying assumption of sparse coding is that the input vectors can be reconstructed accurately as a linear combination of some basis vectors and a few number of non-zero coefficients (Olshausen and Field, 1996).", "startOffset": 189, "endOffset": 216}, {"referenceID": 10, "context": "1 Experimental Settings As input word embeddings, we rely on two state-of-the-art word embeddings methods: word2vec (Mikolov et al., 2013) and GloVe (Pennington et al.", "startOffset": 116, "endOffset": 138}, {"referenceID": 13, "context": ", 2013) and GloVe (Pennington et al., 2014).", "startOffset": 18, "endOffset": 43}, {"referenceID": 15, "context": "We use the word2vec tool3 and the web corpus ENCOW14A (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015) which contains approximately 14.", "startOffset": 54, "endOffset": 98}, {"referenceID": 16, "context": "We use the word2vec tool3 and the web corpus ENCOW14A (Sch\u00e4fer and Bildhauer, 2012; Sch\u00e4fer, 2015) which contains approximately 14.", "startOffset": 54, "endOffset": 98}, {"referenceID": 1, "context": "1 Relatedness and Similarity Tasks For the relatedness task, we use two kinds of datasets: MEN (Bruni et al., 2014) consists of 3000 word pairs comprising 656 nouns, 57 adjectives and 38 verbs.", "startOffset": 95, "endOffset": 115}, {"referenceID": 3, "context": "The WordSim-353 relatedness dataset (Finkelstein et al., 2001) contains 252 word pairs.", "startOffset": 36, "endOffset": 62}, {"referenceID": 6, "context": "Concerning the similarity tasks, we evaluate the denoising vectors again on two kinds of datasets: SimLex-999 (Hill et al., 2015) contains 999 word pairs including 666 noun, 222 verb and 111 adjective pairs.", "startOffset": 110, "endOffset": 129}, {"referenceID": 17, "context": "We calculate cosine similarity between the vectors of two words forming a test pair, and report the Spearman rank-order correlation coefficient \u03c1 (Siegel and Castellan, 1988) against the respective gold standards of human ratings.", "startOffset": 146, "endOffset": 174}, {"referenceID": 8, "context": "2 Synonymy We evaluate on 80 TOEFL (Test of English as a Foreign Language) synonym questions (Landauer and Dumais, 1997) and 50 ESL (English as a Second Language) questions (Turney, 2001).", "startOffset": 93, "endOffset": 120}, {"referenceID": 20, "context": "2 Synonymy We evaluate on 80 TOEFL (Test of English as a Foreign Language) synonym questions (Landauer and Dumais, 1997) and 50 ESL (English as a Second Language) questions (Turney, 2001).", "startOffset": 173, "endOffset": 187}, {"referenceID": 1, "context": "Besides, we also use the public source code5 to re-implement the two methods suggested by Faruqui et al. (2015) which are vectors A (sparse overcomplete vectors) and B (sparse binary overcomplete vectors).", "startOffset": 90, "endOffset": 112}, {"referenceID": 2, "context": "Vectors X represent the baselines; vectors A and B were suggested by Faruqui et al. (2015); the vector length Z\u2217 is equal to 10 times of vector length X.", "startOffset": 69, "endOffset": 91}], "year": 2016, "abstractText": "Word embeddings have been demonstrated to benefit NLP tasks impressively. Yet, there is room for improvement in the vector representations, because current word embeddings typically contain unnecessary information, i.e., noise. We propose two novel models to improve word embeddings by unsupervised learning, in order to yield word denoising embeddings. The word denoising embeddings are obtained by strengthening salient information and weakening noise in the original word embeddings, based on a deep feed-forward neural network filter. Results from benchmark tasks show that the filtered word denoising embeddings outperform the original word embeddings.", "creator": "TeX"}}}