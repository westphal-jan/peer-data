{"id": "1610.01891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text", "abstract": "One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645.", "histories": [["v1", "Thu, 6 Oct 2016 14:38:09 GMT  (337kb,D)", "http://arxiv.org/abs/1610.01891v1", "Hindawi Publishing. Computational Intelligence and Neuroscience Volume 2016 (2016), Article ID 3483528, 24 pages Received 27 May 2016; Revised 8 August 2016; Accepted 18 September 2016. Special Issue on \"Smart Data: Where the Big Data Meets the Semantics\". Academic Editor: Trong H. Duong"]], "COMMENTS": "Hindawi Publishing. Computational Intelligence and Neuroscience Volume 2016 (2016), Article ID 3483528, 24 pages Received 27 May 2016; Revised 8 August 2016; Accepted 18 September 2016. Special Issue on \"Smart Data: Where the Big Data Meets the Semantics\". Academic Editor: Trong H. Duong", "reviews": [], "SUBJECTS": "cs.CL cs.AI cs.LG cs.NE", "authors": ["sadikin mujiono", "mohamad ivan fanany", "chan basaruddin"], "accepted": false, "id": "1610.01891"}, "pdf": {"name": "1610.01891.pdf", "metadata": {"source": "CRF", "title": "A New Data Representation Based on Training Data Characteristics to Extract Drug Named-Entity in Medical Text", "authors": ["Sadikin Mujiono", "Mohamad Ivan Fanany", "Chan Basaruddin"], "emails": ["mujiono.sadikin@mercubuana.ac.id"], "sections": [{"heading": null, "text": "This year it is more than ever before."}, {"heading": "1 Related Works", "text": "The idea behind it is that the idea is about a project, where it is about a project that deals with the question, to what extent it is about a project that deals with the question, to what extent it is about a project that deals with the question, to what extent it is about a project that deals with the question, to what extent it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, in which it is about a project, a project, in which it is about a project, a project, in which it is about a project, a project, a project is about a project, a project, a project, a project, a project, a project, a project, a project in which it is about a project, a project, a project, a project, a project, a project, a project, a project in which it is about a project, a project in which is about a project, a project, a project, a project, a project, a project, a project"}, {"heading": "2 Method & Material", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Framework", "text": "In this study, using the word2vec value properties, we conducted three experiments based on different data representation techniques: the first and second experiments examine conventional tuple data representation, while the third experiment examines sequence data representation. In the second phase, we describe the organization of these three experiments in this section. Generally, the proposed method for extracting drug names in this study consists of two main phases. The proposed method of the first experiment consists of 4 steps (see Figure 1). The first step is a data representation for the formulation of the feature representation. The output of the first step is the tuples of the formation and the test data set. The second step is the dataset labeling, which is applied both to the test and training data. The third step represents the identification of the model."}, {"heading": "2.2 Training Data Analysis", "text": "Each set in the dataset contains four data types, i.e. drug, group, brand and drug-n. If the set does not contain any of these four types, the type value is zero. In the study, we extracted drug and drug-n. Overall, the DrugBank and MedLine datasets represent the quantity of drug name target far less than the target outside the drug. Segura et al. [31] present the first basic statistics of the dataset. More detailed research into token distribution in the training dataset is described in this section. The MedLine dataset contains 25,783 individual tokens consisting of 4,003 unique tokens. These token distributions are not uniform but are dominated by a small portion of some unique tokens. If all unique tokens are ordered 6 / 25 and ordered based on the most common occurrences in the judgments, the quartile distribution is shown in Figure 4."}, {"heading": "2.3 Word Embedding Analysis", "text": "To represent the data set, we used the word embedding model proposed by Mikolov et al. [21]. We treated all sentences as a corpus after combining the training data set and the test dataset. The word2vec training model used was the CBOW (Continuous Bag Of Words) model with the context window length 5 and the vector dimension 100. The result of the word2vec training is the representation of the word in 100-dimensional line vectors. On the basis of the line vector, the similarities or differences between words can be estimated. The following description is a summary of the word2vec result, which is used as the base reference for the data presentation technique and the experiment scenarios. By taking some samples of drug targets and non-drug vector representations, it is shown that the drug word has more similarities (cosmic distance) to a different drug sample from a different sample from a different sample group of drugs and that some of the drug group are not reversed."}, {"heading": "2.4 Feature Representation, Data Formatting, & Data Labelling", "text": "In fact, we are able to move to another world, in which we are able, in which we are able, in which we are able, in which we are able to change the world."}, {"heading": "2.4.1 First Technique", "text": "The formatting of the first data set (one sequence for all records) is done as follows: In the first step, all records in the data set are formatted as a token sequence. Let the token sequence look like this: t1t2t3t4t4t5t6t7t8... tnwith n is the number of tokens in the sequences, then the data set format will be: t1t2t2t3t4t5; t2t3t4t5t6;...... tn \u2212 4tn \u2212 2tn \u2212 1tn; A sample of records and their drug name are taken from DrugBank training data Table 5 are the raw data of 3 samples with three relevant fields, i.e. records, character drug position and drug name. Table 6 illustrates a part of the data set and its designation as a result of the raw data in Table 5. Refer to the drug name field in the data set, i.e., a data set number 6 is identified as the first part of the media drawing."}, {"heading": "2.4.2 Second Technique", "text": "The second technique is used to handle a sequence that comes from each record in the record. In this treatment, we added special characters * to the last part of the token if its record length is less than 5. By using the second technique, the first record returned a record to the sample as shown in Table 8."}, {"heading": "2.4.3 Third Technique", "text": "Of course, the NLP sentence is a sequence in which the occurrence of the current word is conditioned by the previous one. Based on the word2vec value analysis, it is shown that we can intuitively separate the drug word from the non-drug word by their Euclidean distance 12 / 25. Therefore, we used the Euclidean distance between the current words and the previous one to represent the influence. Thus, each current input xi is represented by [xvixdi], which is the concatenation of the word2vec value xvi and its Euclidean distance from the previous one, xdi. Each x is the row vector with the dimension length is 200, the first 100 values are its word2vector, and the remainder of all 100 values is the euclidean distance from the previous one. For the first word, each value of xdi is 0. In the LSTM model, \"the task is to extract the drug name from the medical data text, the binary classification is the clidean interval between each word and the previous one."}, {"heading": "2.5 Wiki Sources", "text": "In this study, we also use Wikipedia as an additional text source in the word2vec training, as used by [30]. The wiki addition is used to evaluate the impact of the training data volume on improving the quality of the word vector."}, {"heading": "2.6 Candidates Selection", "text": "In the MedLine dataset, 171 out of 2,000 tokens (less than ten percent) are medicines, while in the DrugBank, the number of drug tokens is 180 out of 5,252 [31]. Thus, the majority of these tokens are non-medicinal and other noises such as a stop word and special or numeric characters. Based on this fact, we propose a step in candidate selection to eliminate these noises. We examine two mechanisms in candidate selection: the first is based on the distribution of tokens; the second is formed by selecting an x / y part of the cluster result of the data test. In the first scenario, we used only 2 / 3 of the token that appears in the lower 2 / 3 part of the total token, as shown in Table 1 and Table 2; while in the second mechanism, we selected x / y (x < y), which is a part of the total token after the tokens are grouped into clusters."}, {"heading": "2.7 Overview of NN Model", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.7.1 MLP", "text": "In the first experiment, we used multi-layer perceptron NN to train the model and evaluate performance [19]. By creating a series of m examples, the total cost function can be defined as follows: J (W, b) = [1 m \u00b2 i = 1 J (W, b; xi, yi) + 2 nl \u2212 1 l = 1 sl \u00b2 i = 1 (W (l) ji) 2 (1) J (W, b) = [1 m \u00b2 i = 1 (W (l) ji) 2 (2).2 (2).13 / 25In the definition of J (W, b), the first term is an average sum of -squares."}, {"heading": "2.7.2 DBN", "text": "DBN is a learning model consisting of two or more stacked RBMs [8,14]. An RBM is an undirected graph learning model associated with a Markov Random Fields (MRF). In DBN, RBM acts as a feature extractor, where the pre-training process provides initial weights that can be fine-tuned in the discriminatory process in the last shift. The last shift can be formed by logistic regression or any discriminatory standard classifiers [8]. RBM was originally developed for the observation of binary data [7, 35]. It is a popular type of unattended binary data model [13,34]. Some derivatives of RBM models are also proposed to tackle continuous / real values proposed in [5, 36]."}, {"heading": "2.7.3 SAE", "text": "A neural network of the autoencoder (AE) is one of the unattended learning algorithms. NN tries to learn a function h (w, x) \u2248 x. The architecture of the autoencoder NN also consists of input, hidden and output layers. The special characteristic of the autoencoder is that the target output is similar to the input. The interesting structure of the data is estimated by imposing a certain constraint on the network which limits the number of hidden units. However, if the number of hidden units must be greater, it can be imposed on the hidden units with sparsity restrictions [22]. Sparsity constraint is used to limit the average value of activation of hidden units to a certain value. As used in the DBN model after we trained the SAE, the trained weight of NN was used to initialize the classification."}, {"heading": "2.7.4 RNN-LSTM", "text": "RNN (Recurrent Neural Network) is an NN that takes the previous input into account when determining the output of the current input. RNN is powerful when applied to the dataset with a sequential pattern or when the current state input depends on the previous one, such as the time series data, records of the NLP. An LSTM network is a special type of RNN that also consists of 3 layers, i.e. an input layer, a single recursive hidden layer and an output layer [16]. The main innovation of LSTM is that its hidden layer consists of one or more memory blocks. Each block comprises one or more memory cells. By default, the inputs are connected to all cells and gates, while the cells are connected to the exits. The gates are connected to other gates and cells in the hidden layer."}, {"heading": "2.8 Dataset", "text": "To validate the proposed approach, we used the open dataset from DrugBank and MedLine, which was also used by previous researchers, and drug labelling documents from various drug manufacturers and regulators located in Indonesia: 1. http: / / www.kalbemed.com / 2. http: / / www.dechacare.com / 3. http: / / infoobatindonesia.com / obat /, and 4. http: / / www.pom.go.id / webreg / index.php / home / produk / 01. The drug labels are written in Bahasa Indonesia and their common content is drug name, drug components, indication, counter-indication, dosage and warning."}, {"heading": "2.9 Evaluation", "text": "To evaluate the performance of the proposed method, we use common parameters in data mining, i.e. precision, recall and F-score. The formula of these parameters is as follows: Let C = {C1, C2, C3... Cn} is a set of the extracted drug name of this method, and K = {K1, K2, K3,... Kl} is a set of the actual drug name in document D (| Cj |) (4) Recall (Ki, Cj) = (TruePositive) (TruePositive + False Positive) = (| | Positive + False Positive) = (| Positive + False Positive) = (| Positive | Cj |) (4) Recall K-Score) (Ki, Cj) = (TruePositive) (Ki, Cj) = (K-Score) and Cj-Score (all K-Score) (Ki-Score)."}, {"heading": "3 Results and Discussion", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 MLP Learning Performance", "text": "The following experiments are the part of the first experiment. These experiments are performed to evaluate the contribution of the three regularization settings, as described in Section 2.7.1. By arranging the set in the training data set as 5 gram words, the amount of the generated sample is represented in Table 10. We are doing training and testing of the MLP-NN learning model for all these test data compositions. The result of the model performance on both data sets, i.e., MedLine & DrugBank, is shown in the learning phase, Figure 6 and Figure 7. The NN learning parameters used for all experiments are: 500 input nodes, two hidden layers in which each layer has 100 nodes with sigmoid activation, and 6 output nodes with Softmax function; the learning rate = 1, dynamics = 0.5; and epochs = 100. We used mini-batch scenario in training with batch size 100."}, {"heading": "3.2 Open Dataset Performance", "text": "In Table 11, 12, 13 and 14, the numbering (1), (2) and (3) in the far left column shows the technique of candidate selection with: 1. (1): All data tests are selected.2. (2): 2 / 3 of the data test are selected and 3. (3): 2 / 3 part of 3 clusters for MedLine or 3 / 4 part of 4 clusters for DrugBank"}, {"heading": "3.2.1 MLP-NN Performance", "text": "In this first experiment, using two data representation techniques and three candidate selection scenarios, we have six goal-oriented experimental scenarios. The result of the experiment, which uses the first data representation technology and three candidate selection scenarios, is presented in Table 11. When calculating the F score, we select only the predicted target provided by the lowest error (the minimum one). For the MedLine record, the best performance is shown by the L2 regulation setting, where the error is 0.41818, in the third candidate scenario selection scenario with F score 0.439516, while the DrugBank scores scores together with L0 and L1 regulation setting, with an error test of 0.0802, in the second candidate selection scenario, the F score was 0.641745. Overall, one can conclude that DrugBank experiments provide the best F-score scores scores-Scores-Scores-Scores-Scores-Scores-Scores-Scores-Scores-Score-Scores-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score-Score"}, {"heading": "3.2.2 DBN & SAE Performance", "text": "In the second experiment, which includes DBN and SAE learning model, we use only the experiment scenario, which gives the best results in the first experiment. The best experiment scenario uses the second data representation technology with wiki text as an additional source in the word2vec training step.In the DBN experiment, we use two stacked RBMs with 500 nodes of visible unit, 100 nodes of hidden layer for the first and also the second RBMs. The learning parameters used are as follows: pulse = 0; and alpha = 1. We used mini-batch scenario in training, with the batch size of 100. As for RBM limitations, the range of input data value is limited to [0.. 1], like the original RBM, which is developed for binary data type - while the range is the vector of the word value [\u2212 1.. 1.. snodes snodes]."}, {"heading": "3.2.3 LSTM Performance", "text": "Each individual LSTM block consists of two stacked hidden layers, one input node with each input dimension is 200 as described in subsection 2.4.3. All hidden layers are fully connected to each other. We used sigmoid as the output activation function, which is best suited for binary classification. We implemented a peephole connection LSTM variant, in which their gate layers consider the Cell19 / 25state [9]. In addition to implementing the peephole connection, we also use a coupled one of forgot and input gatekeepers. The detailed individual LSTM architecture and its calculation of the single gate formula can be found in [23]. The LSTM experiments were implemented with several different parameter settings. Their results, presented in this section, are the best of all our experiments. Each input file consists of two components, their word vector value and their clicking distance to match the previous input data we present with the input data of both 001."}, {"heading": "3.3 Drug Label Dataset Performance", "text": "With regard to the Indonesian drug label, we could not find any specific external knowledge that could be used to support the extraction of the drug name contained in the drug label. In light of this obstruction, we found that our proposed method is better suited than any other previous approach. As the Drug20 / 25label texts are collected from different locations by drug distributors, manufacturers and state regulatory authorities, it does not clearly contain training and test data as in DrugBanks or Medline records. The other characteristics of these texts are the more structured sentences contained in the data. Although the texts come from different sources, they are all similar documents (the possibly machine-generated drug label).After the data purification step (removal of HTML tags, etc.) we comment on the records manually. The total amount of data sets after performing the data presentation step is, as described in subsection 2.4, 1,0200, for this test result we will most likely be able to verify all data from this experiment."}, {"heading": "3.4 Choosing The Best Scenario", "text": "In the first and second experiment, we examined various experiment scenarios, which include three parameters examined: additional wiki source, data visualization techniques, and the selection of drug candidates. In general, the wiki addition contributes to improving F-score performance. the additional source in the word2vec training improves the quality of the resulting word2vec. By adding common words from the wiki, the difference between common words and non-common words, i.e. the drug name, becomes greater (better distinctive). A problem in dealing with this problem is the unbalanced quantity between drug brand and other tokens [31]. In addition, the targeted drug units are only a small part of the total token. Therefore, most tokens are noise. In dealing with this problem, the second and third candidate selection scenarios show their contribution to reducing the noise [31]. Since the possibility to increase the noise value is then treated as the second noise and the third set."}, {"heading": "4 Conclusion & Future Works", "text": "This study proposes a new approach to data representation and classification in order to extract drug name units contained in the sets of medical text documents; the proposed approach solves the problem of multiple symbols for a single unit, which has remained unsolved in previous studies; and introduces some techniques to address the lack of specific external knowledge. Of course, the words contained in the sentence follow certain sequence patterns, i.e. the current word is conditioned by other previous words; based on the sequence concept of the drug texts applying the sequence model NN, better results are obtained; in this study, we presented three data presentation techniques; the first and second techniques treat the sentence as a non-sequence pattern evaluated with the non-sequential NN classifier (MLP, DBN, SAE), while the third technique treats the sentences as a sequence to provide data that are used as the input of the STN classifier, i.e."}, {"heading": "5 Acknowledgment", "text": "This work is supported by Contract No 1004 / UN2.R12 / HKP5.00 / 2016, financed by the Indonesian Ministry of Research and Higher Education."}, {"heading": "6 Conflict of Interests", "text": "The authors declare that there is no conflict of interest in the publication of this paper."}], "references": [{"title": "Text mining for pharmacovigilance : Using machine learning for drug name recognition and drug \u2013 drug interaction extraction and classification", "author": ["A. Ben", "F. Mahbub", "A. Karanasiou", "Y. Mrabet", "A. Lavelli", "P. Zweigenbaum"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "UTurku : Drug Named Entity Recognition and Drug-Drug Interaction Extraction Using SVM Classification and 22/25  Domain Knowledge", "author": ["J. Bj\u00f6rne", "S. Kaewphan", "T. Salakoski"], "venue": "In Second Joint Conferernce on Lexical and Computational Semantic,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2013}, {"title": "Using Natural Language Processing to Identify Pharmacokinetic Drug- Drug Interactions Described in Drug Package Inserts", "author": ["R. Boyce", "G. Gardner"], "venue": "In the 2012 Workshop on Biomedical Natural Language Processing (BioNLP", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2012}, {"title": "A novel feature-based approach to extract drug-drug interactions from biomedical", "author": ["Q.C. Bui", "P.M.A. Sloot", "E.M. Van Mulligen", "J.A. Kors"], "venue": "text. Bioinformatics,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2014}, {"title": "Continuous Restricted Boltzman Machine with an Implementation Training Algorithm", "author": ["H. Chen", "A. Murray"], "venue": "imag Signal Processing,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "A study of active learning methods for named entity recognition in clinical text", "author": ["Y. Chen", "T.A. Lasko", "Q. Mei", "J.C. Denny", "H. Xu"], "venue": "Journal of biomedical informatics,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "Training restricted boltzmann machines on word observations", "author": ["G. Dahl", "R. Adams", "H. Larochelle"], "venue": "arXiv preprint arXiv:1202.5695,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 17th Iberoamerican Congress, CIARP 2012, Buenos Aires, Argentina", "author": ["A. Fischer", "C. Igel"], "venue": "September 3-6,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Recurrent nets that time and count", "author": ["F.A. Gers", "J. Schmihuber"], "venue": "In Proceedings of the IEEE-INNS-ENNS International Joint Conference on,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2000}, {"title": "LASIGE : using Conditional Random Fields and ChEBI ontology", "author": ["T. Grego", "F.M. Couto"], "venue": "In 7th International Workshop on Semantic Evaluation (SemEval 2013).,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Named entity recognition with long short-term memory", "author": ["J. Hammerton"], "venue": "Proceedings of CoNLL,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2003}, {"title": "The DDI corpus : An annotated corpus with pharmacological substances and drug \u2013 drug interactions", "author": ["M. Herrero-zazo", "I. Segura-bedmar", "P. Mart\u0301\u0131nez", "T. Declerck"], "venue": "Journal of biomedical informatics,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2013}, {"title": "A Practical Guide to Training Restricted Boltzmann Machines A Practical Guide to Training Restricted Boltzmann Machines", "author": ["G. Hinton"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2010}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2006}, {"title": "Lstm Can Solve Hard. In Advances in neural information processing", "author": ["S. Hochreiter"], "venue": null, "citeRegEx": "15", "shortCiteRegEx": "15", "year": 1996}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 1997}, {"title": "Enhancing medical named entity recognition with an extended segment representation technique", "author": ["S. Keretna", "C. Peng", "D. Creighton", "K. Bashir"], "venue": "Computer Methods and Programs in Bimoedicine,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Artificial Intelligence in Medicine Boosting drug named entity recognition using an aggregate classifier", "author": ["I. Korkontzelos", "D. Piliouras", "A.W. Dowsey", "S. Ananiadou"], "venue": "Artificial Intelligence in Medicine,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Drug name recognition: Approaches and resources", "author": ["S. Liu", "B. Tang", "Q. Chen", "X. Wang"], "venue": "Information (Switzerland),", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Efficient Estimation of Word Representations in Vector Space", "author": ["T. Mikolov", "G. Corrado", "K. Chen", "J. Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2013}, {"title": "Jannlab neural network framework for java", "author": ["S. Otte", "D. Krechel", "M. Liwicki"], "venue": "In Poster Proceedings Conference MLDM", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2013}, {"title": "A Survey of Biological Entity Recognition Approaches", "author": ["G. Pal", "S. Gosal"], "venue": "International Journal on Recent and Innovation Trends in Computing and Communication,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Prediction as a candidate for learning deep hierarchical models of data", "author": ["R.B. Palm"], "venue": null, "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Translation and Classification Algorithm of FDA-Drugs to DOEN2011 Class Therapy to Estimate Drug - Drug Interaction", "author": ["M. Sadikin", "I. Wasito"], "venue": "In The 2nd International Conference on Information Systems for Business Competitiveness", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2013}, {"title": "Toward Object Interaction Mining By Starting With Object Extraction Based on Pattern Learning Method", "author": ["M. Sadikin", "I. Wasito"], "venue": "In 2014 Asia-Pacific Materials Science and Information Technology Conference (APMSIT", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "Mining Adverse Drug Reactions from online healthcare forums using Hidden Markov Model", "author": ["H. Sampathkumar", "X.-w. Chen", "B. Luo"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "Exploring Word Embedding for Drug Name Recognition", "author": ["I. Segura-bedmar", "P. Mart"], "venue": "In The Sixth International Workshop on Health Text Mining and Information Analysis, number September,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Extraction of drug-drug interactions from biomedical texts (ddiextraction 2013)", "author": ["I. Segura-Bedmar", "P. Martinez", "M. Herrero-Zazo. Semeval-2013 task 9"], "venue": "In Proceedings of the Seventh International Workshop on Semantic Evaluation", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}, {"title": "Drug name recognition and classification in biomedical texts A case study outlining approaches underpinning automated systems", "author": ["I. Segura-bedmar", "P. Mart\u0131"], "venue": "Drug Discovery Today,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2008}, {"title": "Data normalization in the learning of restricted Boltzmann machines", "author": ["Y. Tang", "I. Sutskever"], "venue": "Department of Computer science, Toronto university,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient", "author": ["T. Tieleman"], "venue": "Proceedings of the 25th International Conference on Machine Learning,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2008}, {"title": "Exponential Family Harmoniums with an Application to Information Retrieval", "author": ["M. Welling", "M. Rosen-zvi", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2005}, {"title": "Unsupervised biomedical named entity recognition : Experiments with clinical and biological texts", "author": ["S. Zhang", "N. Elhadad"], "venue": "Journal of biomedical informatics,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2013}], "referenceMentions": [{"referenceID": 23, "context": "In many recent cases, however, many drugs are withdrawn from the market when it was discovered that the interaction between the drugs is hazardous to health [27].", "startOffset": 157, "endOffset": 161}, {"referenceID": 17, "context": "Drug name recognition is a primary task of medical text data extraction since the drug finding is the essential element in solving other information extraction problems [18,37].", "startOffset": 169, "endOffset": 176}, {"referenceID": 32, "context": "Drug name recognition is a primary task of medical text data extraction since the drug finding is the essential element in solving other information extraction problems [18,37].", "startOffset": 169, "endOffset": 176}, {"referenceID": 11, "context": "Among derivative work of drug name extractions are drug-drug interaction [12], drug adverse reaction [29], or other applications (information retrieval, decision support system, drug development or drug discovery) [32].", "startOffset": 73, "endOffset": 77}, {"referenceID": 25, "context": "Among derivative work of drug name extractions are drug-drug interaction [12], drug adverse reaction [29], or other applications (information retrieval, decision support system, drug development or drug discovery) [32].", "startOffset": 101, "endOffset": 105}, {"referenceID": 28, "context": "Among derivative work of drug name extractions are drug-drug interaction [12], drug adverse reaction [29], or other applications (information retrieval, decision support system, drug development or drug discovery) [32].", "startOffset": 214, "endOffset": 218}, {"referenceID": 16, "context": "First, the drug name entities are usually unstructured texts [17] where the number of new entities is quickly growing over time.", "startOffset": 61, "endOffset": 65}, {"referenceID": 21, "context": "Thus, it is hard to create a dictionary which always includes the entire lexicon and is up to date [25].", "startOffset": 99, "endOffset": 103}, {"referenceID": 18, "context": "Third, many drug names contain a combination of non-word and word symbols [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 9, "context": "Fourth, the other problem in drug name extraction is that a single drug name might be represented by multiple tokens [10].", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "Due to the complexity in extracting multiple tokens for drugs, some researchers such as [2] even ignores that case in the MedLine & DrugBank training with the reason that the multiple tokens drug is only 18% of all drug names.", "startOffset": 88, "endOffset": 91}, {"referenceID": 19, "context": "In this study, the vector of words is provided by word embedding methods proposed by Mikolov [21].", "startOffset": 93, "endOffset": 97}, {"referenceID": 0, "context": "html, which is also used by [1, 2, 10].", "startOffset": 28, "endOffset": 38}, {"referenceID": 1, "context": "html, which is also used by [1, 2, 10].", "startOffset": 28, "endOffset": 38}, {"referenceID": 9, "context": "html, which is also used by [1, 2, 10].", "startOffset": 28, "endOffset": 38}, {"referenceID": 21, "context": "[25] summarizes their survey on various entity recognition approaches.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "The approaches can be categorized into three models: dictionary based, rule based, and learning based methods [17, 33].", "startOffset": 110, "endOffset": 118}, {"referenceID": 27, "context": "5% in F-score [31].", "startOffset": 14, "endOffset": 18}, {"referenceID": 9, "context": "CRF is used by one of the best [10] participants in SemEval challenges in the clinical text (https://www.", "startOffset": 31, "endOffset": 35}, {"referenceID": 9, "context": "As for the use of external knowledge aimed to increase the performance, the author [10] uses ChEBI (Chemical Entities of Biological Interest), i.", "startOffset": 83, "endOffset": 87}, {"referenceID": 26, "context": "A hybrid approach model, which combines statistical learning and dictionary based, is proposed by [30].", "startOffset": 98, "endOffset": 102}, {"referenceID": 5, "context": "The result of a CRF based active learning, which is applied to NER BIO (Beginning, Inside, Output) annotation token for extracting named entity in the clinical text, is presented in [6].", "startOffset": 182, "endOffset": 185}, {"referenceID": 0, "context": "[1].", "startOffset": 0, "endOffset": 3}, {"referenceID": 17, "context": "[18].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "Related to their approach, in our previous work, we propose a pattern learning that utilizes the regular expression surrounding drug names and their compounds [28].", "startOffset": 159, "endOffset": 163}, {"referenceID": 26, "context": "To overcome the multiple tokens problem, we propose a new technique which treats a target entity as a set of tokens (a tuple) at once rather than treating the target entity as a single token surrounded by other tokens such as used by [30] or [4].", "startOffset": 234, "endOffset": 238}, {"referenceID": 3, "context": "To overcome the multiple tokens problem, we propose a new technique which treats a target entity as a set of tokens (a tuple) at once rather than treating the target entity as a single token surrounded by other tokens such as used by [30] or [4].", "startOffset": 242, "endOffset": 245}, {"referenceID": 27, "context": "[31] present the first basic statistics of the dataset.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[21].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "In the first and second techniques, we try to overcome the multiple tokens drawback that left unsolved in [2] by formatting a single input data as an N - gram model with N=5 (one tuple data consist 5 tokens) to accommodate the maximum token which acts as a single drug entity target name.", "startOffset": 106, "endOffset": 109}, {"referenceID": 0, "context": "Result: Labelled dataset Input:array of tuple, array of drug ; output: array of label {Array of drug contains list of drug and drug-n only} ; label[]<=1 Initialization; for each t in tuple do for each d in drug do if length (d) = 1 then if t[1] = d[1] then //match 1 token drug; label <= 2, break, exit from for each d in drug; else", "startOffset": 241, "endOffset": 244}, {"referenceID": 0, "context": "Result: Labelled dataset Input:array of tuple, array of drug ; output: array of label {Array of drug contains list of drug and drug-n only} ; label[]<=1 Initialization; for each t in tuple do for each d in drug do if length (d) = 1 then if t[1] = d[1] then //match 1 token drug; label <= 2, break, exit from for each d in drug; else", "startOffset": 248, "endOffset": 251}, {"referenceID": 0, "context": "end else if length (d) = 2 then if t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "end else if length (d) = 2 then if t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "end else if length (d) = 2 then if t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "end else if length (d) = 2 then if t[1] = d[1] and t[2] = d[2] then //match 2 tokens drug; label <= 3, break, exit from for each d in drug; else", "startOffset": 59, "endOffset": 62}, {"referenceID": 0, "context": "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "end else if length (d) = 3 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] then label <= 4, break, exit from for each d in drug; else", "startOffset": 75, "endOffset": 78}, {"referenceID": 0, "context": "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "end else if length (d) = 4 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] then label <= 5, break, exit from for each d in drug; else", "startOffset": 91, "endOffset": 94}, {"referenceID": 0, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 36, "endOffset": 39}, {"referenceID": 0, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 52, "endOffset": 55}, {"referenceID": 1, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 59, "endOffset": 62}, {"referenceID": 2, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 68, "endOffset": 71}, {"referenceID": 2, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 75, "endOffset": 78}, {"referenceID": 3, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 84, "endOffset": 87}, {"referenceID": 3, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 91, "endOffset": 94}, {"referenceID": 4, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 100, "endOffset": 103}, {"referenceID": 4, "context": "end else if length (d) = 5 then if t[1] = d[1] and t[2] = d[2] and t[3] = d[3] and t[4] = d[4] and t[5] = d[5] then label <= 6, break, exit from for each d in drug; else", "startOffset": 107, "endOffset": 110}, {"referenceID": 26, "context": "In this study we also utilize Wikipedia as the additional text sources in word2vec training as used by [30].", "startOffset": 103, "endOffset": 107}, {"referenceID": 27, "context": "252 [31].", "startOffset": 4, "endOffset": 8}, {"referenceID": 22, "context": "The model training and testing are implemented by modifying the code from [26] which can be downloaded at https://github.", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "DBN is a learning model composed of two or more stacked RBM [8,14].", "startOffset": 60, "endOffset": 66}, {"referenceID": 13, "context": "DBN is a learning model composed of two or more stacked RBM [8,14].", "startOffset": 60, "endOffset": 66}, {"referenceID": 7, "context": "The last layer may be formed by logistic regression or any standard discriminative classifiers [8].", "startOffset": 95, "endOffset": 98}, {"referenceID": 6, "context": "RBM was originally developed for binary data observation [7, 35].", "startOffset": 57, "endOffset": 64}, {"referenceID": 30, "context": "RBM was originally developed for binary data observation [7, 35].", "startOffset": 57, "endOffset": 64}, {"referenceID": 12, "context": "It is a popular type of unsupervised model for binary data [13,34].", "startOffset": 59, "endOffset": 66}, {"referenceID": 29, "context": "It is a popular type of unsupervised model for binary data [13,34].", "startOffset": 59, "endOffset": 66}, {"referenceID": 4, "context": "Some derivative of RBM models are also proposed to tackle a continuous/real values suggested in [5, 36].", "startOffset": 96, "endOffset": 103}, {"referenceID": 31, "context": "Some derivative of RBM models are also proposed to tackle a continuous/real values suggested in [5, 36].", "startOffset": 96, "endOffset": 103}, {"referenceID": 15, "context": ", an input layer, a single recurrent hidden layer, and an output layer [16].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "The single standard LSTM is a hidden layer with input, memory cell, and output gates [11,23].", "startOffset": 85, "endOffset": 92}, {"referenceID": 24, "context": "Adopted from [28], the parameter computations formula are:", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "state [9].", "startOffset": 6, "endOffset": 9}, {"referenceID": 14, "context": "In treating both input data components, we adapt the Adding Problem Experiment as presented in [15].", "startOffset": 95, "endOffset": 99}, {"referenceID": 20, "context": "We use the Jannlab tools [24] with some modifications in the part of entry to conform with our data settings.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "The Best of SemEval 2013 [31] 0.", "startOffset": 25, "endOffset": 29}, {"referenceID": 9, "context": "7150 [10] 0.", "startOffset": 5, "endOffset": 9}, {"referenceID": 26, "context": "5700 With external knowledge, ChEBI [30]+Wiki 0.", "startOffset": 36, "endOffset": 40}, {"referenceID": 0, "context": "7200 With external knowledge, DINTO [1] 0.", "startOffset": 36, "endOffset": 39}, {"referenceID": 1, "context": "7200 Additional feature, BIO [2] 0.", "startOffset": 29, "endOffset": 32}, {"referenceID": 27, "context": "One problem in mining drug name entity from medical text is the imbalanced quantity between drug token and other tokens [31].", "startOffset": 120, "endOffset": 124}, {"referenceID": 2, "context": "The first step improvement can be the incorporation of additional handcrafted features - such as the words position, the use of capital case at the beginning of the word, the type of character - as also used in the previous studies [3, 30].", "startOffset": 232, "endOffset": 239}, {"referenceID": 26, "context": "The first step improvement can be the incorporation of additional handcrafted features - such as the words position, the use of capital case at the beginning of the word, the type of character - as also used in the previous studies [3, 30].", "startOffset": 232, "endOffset": 239}], "year": 2016, "abstractText": "One essential task in information extraction from the medical corpus is drug name recognition. Compared with text sources come from other domains, the medical text is special and has unique characteristics. In addition, the medical text mining poses more challenges, e.g., more unstructured text, the fast growing of new terms addition, a wide range of name variation for the same drug. The mining is even more challenging due to the lack of labeled dataset sources and external knowledge, as well as multiple token representations for a single drug name that is more common in the real application setting. Although many approaches have been proposed to overwhelm the task, some problems remained with poor F-score performance (less than 0.75). This paper presents a new treatment in data representation techniques to overcome some of those challenges. We propose three data representation techniques based on the characteristics of word distribution and word similarities as a result of word embedding training. The first technique is evaluated with the standard NN model, i.e., MLP (Multi-Layer Perceptrons). The second technique involves two deep network classifiers, i.e., DBN (Deep Belief Networks), and SAE (Stacked Denoising Encoders). The third technique represents the sentence as a sequence that is evaluated with a recurrent NN model, i.e., LSTM (Long Short Term Memory). In extracting the drug name entities, the third technique gives the best F-score performance compared to the state of the art, with its average F-score being 0.8645. Keywords\u2014 drug name entity, word embedding, MLP, DBN, SAE, LSTM", "creator": "LaTeX with hyperref package"}}}