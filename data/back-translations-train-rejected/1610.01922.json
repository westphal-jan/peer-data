{"id": "1610.01922", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Adaptive Online Sequential ELM for Concept Drift Tackling", "abstract": "A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. In this paper, we propose concept drift tackling method as an enhancement of Online Sequential Extreme Learning Machine (OS-ELM) and Constructive Enhancement OS-ELM (CEOS-ELM) by adding adaptive capability for classification and regression problem. The scheme is named as adaptive OS-ELM (AOS-ELM). It is a single classifier scheme that works well to handle real drift, virtual drift, and hybrid drift. The AOS-ELM also works well for sudden drift and recurrent context change type. The scheme is a simple unified method implemented in simple lines of code. We evaluated AOS-ELM on regression and classification problem by using concept drift public data set (SEA and STAGGER) and other public data sets such as MNIST, USPS, and IDS. Experiments show that our method gives higher kappa value compared to the multiclassifier ELM ensemble. Even though AOS-ELM in practice does not need hidden nodes increase, we address some issues related to the increasing of the hidden nodes such as error condition and rank values. We propose taking the rank of the pseudoinverse matrix as an indicator parameter to detect underfitting condition.", "histories": [["v1", "Thu, 6 Oct 2016 16:08:52 GMT  (766kb,D)", "http://arxiv.org/abs/1610.01922v1", "Hindawi Publishing. Computational Intelligence and Neuroscience Volume 2016 (2016), Article ID 8091267, 17 pages Received 29 January 2016, Accepted 17 May 2016. Special Issue on \"Advances in Neural Networks and Hybrid-Metaheuristics: Theory, Algorithms, and Novel Engineering Applications\". Academic Editor: Stefan Haufe"]], "COMMENTS": "Hindawi Publishing. Computational Intelligence and Neuroscience Volume 2016 (2016), Article ID 8091267, 17 pages Received 29 January 2016, Accepted 17 May 2016. Special Issue on \"Advances in Neural Networks and Hybrid-Metaheuristics: Theory, Algorithms, and Novel Engineering Applications\". Academic Editor: Stefan Haufe", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["arif budiman", "mohamad ivan fanany", "chan basaruddin"], "accepted": false, "id": "1610.01922"}, "pdf": {"name": "1610.01922.pdf", "metadata": {"source": "CRF", "title": "Adaptive Online Sequential ELM for Concept Drift Tackling", "authors": ["Arif Budiman", "Mohamad Ivan Fanany", "Chan Basaruddin"], "emails": ["arif.budiman21@ui.ac.id"], "sections": [{"heading": null, "text": "This year is the highest in the history of the country."}, {"heading": "1 Related Works", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.1 Notations", "text": "We specify the notations used in this article for easier understanding: \u2022 Matrix is written in uppercase letters (e.g. X). \u2022 Vector is written in lowercase letters (e.g. x). \u2022 The transposition of a matrix X is written as XT. \u2022 The amount of training data is N. Each input file x contains some d attributes. The target has m number of class levels. \u2022 An input matrix X can be called Xd \u00b7 N and the target matrix T as TN \u00b7 m.3 / 26 \u2022 The hidden layer matrix is H. The input weight matrix is \u03b2. The output weight matrix \u0445H is the additional block part of matrix H. The matrix K is the automatic corrix of H \u00b7 \u00b7 \u00b7 \u00b7 The input matrix can be represented as H \u00b7 \u00b7 \u00b7 The input matrix of H \u00b7 \u00b7 \u00b7."}, {"heading": "1.2 Concept Drift Strategies", "text": "In this context, we must also mention the fact that the people mentioned in the United States are not people who are able to play by the rules, but people who are able to play by the rules."}, {"heading": "1.3 ELM in Sequential Learning", "text": "In this section, we briefly explained ELM's previous related work in sequential learning and adaptive environments. [ELM] is gaining popularity thanks to its speed of learning, generalization, and simplicity. [Huang] explained the term \"extreme,\" which means going beyond conventional artificial neural learning, which requires iterative coordination. However, the ELM is moving toward the brain like learning, where hidden neurons do not need to be tuned. The initial function of an SLFN with a single hidden layer matrix H can function as: fL (x) = 1 \u03b2iH (ai, bi, x) (2), in which H \u2020 is the pseudo-inversion of H. H \u2020 by left pseudo-category of H = (H T H) \u2212 1H T (3) We can use regression or regulated minimum squares to be: \u03b2 (H T H H H) H \u2212 TH T."}, {"heading": "2 Proposed Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Theoritical Background of AOS-ELM", "text": "The question that arises is to what extent it is actually a matter of a way in which people are able to change the world. (...) The question is to what extent people are able to change the world. (...) The question is to what extent people are able to change the world. (...) The question is only to what extent people are able to change the world. (...) The question is to what extent people are able to change the world. (...) The question is to what extent people are able to change the world. (...) The question is only to what extent they are able to change the world. (...) The question is to what extent they are able to change the world. (...)"}, {"heading": "2.2 AOS-ELM Algorithms", "text": "In this section we have the AOS-ELM pseudo codes 1 in the kth sequential with X (k) training input and T (k) target to update (k).Basically we have three pseudo codes, namely OSELMSeq (algorithm 1) as OS-ELM and CEOS-ELM pseudo codes (the AOSELMDSeq (algorithm 1 as AOS-ELM pseudo1The Matlab source code, data set and demo file implementation are at https: / github.com / abudiman250172 / abudiman250172 / adaptive-OS-ELM.9 / 26codes handling virtual drift; and AOSELMRDSeq (algorithm 2 as AOS-ELM pseudo codes address real drift. We can combine each pseudo code together to form a hybrid drift algorithm."}, {"heading": "3 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "3.1 Experiments Design in Classification", "text": "In fact, we will be able to move to another world, where we will move to another world, where we will move to another world, where we will move to another world."}, {"heading": "3.2 SEA and STAGGER Concepts Result", "text": "We compared between AOS-ELM without \u03b4L increase (AOS-ELM1) and \u03b4L increase (AOS-ELM2). We used 5-fold cross-validation and compared between NORM and ROS parameters. For SEA, the parameters are: L0 = 3000 and \u03b4L = 500 increase per drift. For STAGGER, the parameters fail: L0 = 9 and \u03b4L = 5 hidden nodes per drift. 13 / 26 The AOS-ELM has a better accuracy with better recovery time (see Table 3a, 3b) than CEOS-ELM, while the non-adaptive OS-ELM fails (see Fig. 5). The AOS-ELM2 improved forgetability better than AOS-ELM1. Compared to Kolter, the et.al result using dynamically weighted majority (DWM) of the DWM result (DOS-ELM2) is the result for DWM-ELM1 in the proximity of the WM-A3M."}, {"heading": "3.3 Concept Drift Detection", "text": "Drift detection works on the basis of a loss estimate (see Fig. 2), which compares the current predictive accuracy with previous feedback. Using a similar method on [1, 31], we can evaluate 14 / 26 the intersection between the decrease and increase in accuracy in Fig. 6. If the successive loss power exceeds a certain threshold, the drift warning status is triggered. We have measured the output power from the new concept output and compared it with the previous output. If certain criteria are met, the new AOS-ELM will be obliged. Otherwise, the previous AOS-ELM will be withdrawn."}, {"heading": "3.4 MNIST and MNIST+USPS Result", "text": "The results are as follows: 1. Experiment 1 - Virtual Drift.The AOS-ELM of the [XgreyXHOG] version has Cohen's kappa of test accuracy 95.72 (0.21)%, which is similar to its non-adaptive ELM and the offline ELM of the [XgreyXHOG] version with the same hidden node number L = 2000. It has a better accuracy than single attributes [Xgrey] or [XHOG] (see Table 4b).It proves our explanation in the theoretical background of Section 2.1.Note: We set L0 = 200 for [XHOG] ELM based on the same ratio between the number of input nodes with hidden nodes of [Xgrey] ELM.2. Experiment 2 - Real DriftThe end result, as shown in Table 5b, is that the AOS-ELM-ELM ensemble has a better kappa performance than the ELM group and a marginally better ELM-ELM group-ELM-ELM-ELM-5b increase."}, {"heading": "3.5 The effect of hidden nodes increase", "text": "The initial size of the hidden nodes L0 selection is important to have good generalization performance. Some studies [17,21] suggest that the hidden nodes must be at least the same size as the value of the training data. However, in a data stream it is difficult to determine a fixed number of hidden nodes according to this proposal. However, the larger L0 requires more computing resources and processing time, and probably there is no significant result in the end. Therefore, we have the requirement to increase the value of the total training data (713) and the multiple conditions of the drift L17 / 26on the drift event: No increase, 500, 1000 and 2000 using ROS parameters. However, the larger L0 has better influence than the higher class of training data (666) and the ranking of the total training data (713) and multiple conditions of the drift event: No increase, 1000 and 2000 using ROS parameters."}, {"heading": "4 AOS-ELM in Regression", "text": "In this experiment, we used AOS-ELM with a single input node and a single output node per concept. We defined the following concept: \u2022 C1 is sine function with 50000 training / 5000 tests; \u2022 C2 is sine function with 50000 training / 5000 tests; \u2022 C3 is Gauss function with 50000 training / 5000 tests. The sequential experiments are the following drift equations: 1. Experiment 1: C1-ELM regression ability to maintain the previous knowledge of the regression concept. We select the constant value that gives the best regression result of each concept. Our goal is to show the AOS-ELM regression ability at the end of each training experiment."}, {"heading": "5 Simulation in Big Data stream : Intrusion", "text": "Detection System (IDS) KDD Cup 1999IDS is a network security technology that scans every network packet traffic to detect potential exploits that then send alarms or take active action to the intrusion prevention system. Some machine learning methods were applied in the hope of improving detection rates and adaptive capabilities [37].In this experiment, we used KDD Cup 1999 Competition dataset. The complete dataset had 4898431 network packets and grouped to be 23 classes (A Normal Class and 2221 / 26 attack names based on a signature-based detection) [11]. The dataset has a Control Information Header (CI) for providing the data in numerical and multicategory values as characteristics. We focused on service names (IP ports) attributes because they are specific differentiators for applications. The CI and the number of attack classes are not fixed."}, {"heading": "6 Challenges and Future Research", "text": "\u2022 We need to study the optimal transition space that minimizes the gap to the new concept learning model. In certain cases, the AOS-ELM may have \"insufficient adaptation\" and may require larger training data to achieve the new convergence. \u2022 We need to test the consistency of AOS-ELM for various pseudo-inverse methods (e.g. Greville's method [36]). We are considering some ideas for future research on AOS-ELM: \u2022 The need for transfer learning to solve big data problems when distribution data changes. \u2022 The AOS-ELM integration with other ELM methods, e.g. weighted OS-ELM for unbalanced learning [30], ELM autoencoders (ELM-AE) [39], stacked ELM [40], and so on. \u2022 A detailed systematic explanation based on rule extraction [2] for AOS-ELM in handling adaptive environments."}, {"heading": "7 Conclusion", "text": "The proposed method offers better adaptability than non-adaptive OS-ELM and CEOS-ELM in terms of maintaining detection performance when handling concept drifts. It uses a simple line of code and is particularly easy to use for successive drifts, compared to adaptive ensemble methods. While most adaptive classifiers work differently for each virtual, real drift and hybrid drift scenarios, AOS-ELM addresses these drifts by simple block matrix reconstruction and ranking. AOS-ELM met the requirements for accuracy, simplicity, speed and flexibility. However, in certain VD and HD cases, AOS-ELM accuracy cannot exceed the non-adaptive sequential ELM sequence, which includes future training data. In the FE cases, the AOS-ELM-ELM has better accuracy than the real data implementation, which does not exceed the adaptive ELM sequence, and we prefer to know the future ELM group of data exactly when the behavior is better."}], "references": [{"title": "Just-in-time classifiers for recurrent concepts", "author": ["C. Alippi", "G. Boracchi", "M. Roveri"], "venue": "Neural Networks and Learning Systems, IEEE Transactions on, 24(4):620\u2013634, April", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2013}, {"title": "Rule extraction from support vector machines: A review", "author": ["N.H. Barakat", "A.P. Bradley"], "venue": "Neurocomputing, 74(1-3):178\u2013190,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2010}, {"title": "Constructive, robust and adaptive os-elm in human action recognition", "author": ["A. Budiman", "M. Fanany", "C. Basaruddin"], "venue": "In Industrial Automation, Information and Communications Technology (IAICT), 2014 International Conference on, pages 39\u201345, Aug", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Classification of uncertain data streams based on extreme learning machine", "author": ["K. Cao", "G. Wang", "D. Han", "J. Ning", "X. Zhang"], "venue": "Cognitive Computation, pages 1\u201311,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "A survey on feature selection methods", "author": ["G. Chandrashekar", "F. Sahin"], "venue": "Computers & Electrical Engineering, 40(1):16 \u2013 28,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Statistical methods: The geometric approach (david j", "author": ["L.C.A. Corsten"], "venue": "saville and graham r. wood). SIAM Review, 34(3):506\u2013508,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1992}, {"title": "A Probabilistic Theory of Pattern Recognition", "author": ["L. Devroye", "L. Gy\u00f6rfi", "G. Lugosi"], "venue": "Springer,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 1996}, {"title": "Ensemble methods in machine learning", "author": ["T.G. Dietterich"], "venue": "In Proceedings of the First International Workshop on Multiple Classifier Systems, MCS \u201900, pages 1\u201315, London, UK, UK,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2000}, {"title": "A review on real time data stream classification and adapting to various concept drift scenarios", "author": ["P. Dongre", "L. Malik"], "venue": "In Advance Computing Conference (IACC), 2014 IEEE International, pages 533\u2013537, Feb", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive concept drift detection", "author": ["A. Dries", "U. R\u00fcckert"], "venue": "Statistical Analysis and Data Mining, 2(5-6):311\u2013327, Dec.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Comput. Surv.,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "A general framework for mining concept-drifting data streams with skewed distributions", "author": ["J. Gao", "W. Fan", "J. Han", "P.S. Yu"], "venue": "In In Proc. SDM\u201907,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2007}, {"title": "Loan", "author": ["G.H. Golub", "C.F. Va"], "venue": "Matrix Computations (3rd Ed.). Johns Hopkins University Press, Baltimore, MD, USA,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1996}, {"title": "Introduction to Probability", "author": ["C.M. Grinstead", "J.L. Snell"], "venue": "AMS,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "A robust online sequential extreme learning machine", "author": ["M.-T.T. Hoang", "H.T. Huynh", "N.H. Vo", "Y. Won"], "venue": "In Proceedings of the 4th International Symposium on Neural Networks: Advances in Neural Networks, pages 1077\u20131086, Berlin, Heidelberg,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2007}, {"title": "Trends in extreme learning machines: A review", "author": ["G. Huang", "G.-B. Huang", "S. Song", "K. You"], "venue": "Neural Networks, 61(0):32 \u2013 48,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2015}, {"title": "An insight into extreme learning machines: Random neurons, random features and kernels", "author": ["G.-B. Huang"], "venue": "Cognitive Computation, 6(3),", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "What are extreme learning machines? filling the gap between frank rosenblatt\u2019s dream and john von neumann\u2019s puzzle", "author": ["G.-B. Huang"], "venue": "Cognitive Computation, 7:263\u2013278, June", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Extreme learning machine for regression and multiclass classification", "author": ["G.-B. Huang", "H. Zhou", "X. Ding", "R. Zhang"], "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part B, 42(2):513\u2013529,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, 70(1-3):489\u2013501,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2006}, {"title": "Extreme learning machine based supervised subspace learning", "author": ["A. Iosifidis"], "venue": "Neurocomputing, 167:158 \u2013 164,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2015}, {"title": "Dynamic weighted majority: An ensemble method for drifting concepts", "author": ["J.Z. Kolter", "M.A. Maloof"], "venue": "J. Mach. Learn. Res., 8:2755\u20132790, Dec.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Classifier ensembles for changing environments", "author": ["L. Kuncheva"], "venue": "In Multiple Classifier Systems, volume 3077 of Lecture Notes in Computer Science, pages 1\u201315. Springer Berlin Heidelberg,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Classifier ensembles for detecting concept change in streaming data: Overview and perspectives", "author": ["L.I. Kuncheva"], "venue": "In 2nd Workshop SUEMA 2008 (ECAI 2008), pages 5\u201310,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "A constructive enhancement for online sequential extreme learning machine", "author": ["Y. Lan", "Y.C. Soh", "G.-B. Huang"], "venue": "In Neural Networks, 2009. IJCNN 2009. International Joint Conference on, pages 1708\u20131713, June", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2009}, {"title": "and C", "author": ["Y. LeCu"], "venue": "Cortes. MNIST handwritten digit database,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on, 17(6):1411\u20131423, Nov", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2006}, {"title": "Trainable classifier-fusion schemes: An application to pedestrian detection", "author": ["O. Ludwig", "D. Delgado", "V. Goncalves", "U. Nunes"], "venue": "In Intelligent Transportation Systems, 2009. ITSC \u201909. 12th International IEEE Conference on, pages 1\u20136, Oct", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2009}, {"title": "Weighted online sequential extreme learning machine for class imbalance learning", "author": ["B. Mirza", "Z. Lin", "K.-A. Toh"], "venue": "Neural Processing Letters, 38(3):465\u2013486,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2013}, {"title": "Adaptive classifiers-ensemble system for tracking concept drift", "author": ["K. Nishida", "K. Yamauchi"], "venue": "In Machine Learning and Cybernetics, 2007 International Conference on, volume 6, pages 3607\u20133612, Aug", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering, 22(10):1345\u20131359,", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2010}, {"title": "Geometric Approach to Support Vector Machines Learning for Large Datasets", "author": ["R. Strack"], "venue": "PhD thesis, Richmond, VA, USA,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "A streaming ensemble algorithm (sea) for large-scale classification", "author": ["W.N. Street", "Y. Kim"], "venue": "pages 377\u2013382. ACM,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2001}, {"title": "Online and adaptive pseudoinverse solutions for {ELM} weights", "author": ["A. van Schaik", "J. Tapson"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2015}, {"title": "Article: Survey on intrusion detection system using machine learning techniques", "author": ["S.K. Wagh", "V.K. Pachghare", "S.R. Kolhe"], "venue": "International Journal of Computer Applications, 78(16):30\u201337, September", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2013}, {"title": "Transfer learning beyond text classification", "author": ["Q. Yang"], "venue": "In Proceedings of the 1st Asian Conference on Machine Learning: Advances in Machine Learning, ACML \u201909, pages 10\u201322, Berlin, Heidelberg,", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2009}, {"title": "Denoising laplacian multi-layer extreme learning machine", "author": ["N. Zhang", "S. Ding", "Z. Shi"], "venue": "Neurocomputing, 171:1066 \u2013 1074,", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Stacked extreme learning machines", "author": ["H. Zhou", "G.-B. Huang", "Z. Lin", "H. Wang", "Y.C. Soh"], "venue": "Cybernetics, IEEE Transactions on, 45(9):2013\u20132025,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning under Concept Drift: an Overview", "author": ["I. Zliobaite"], "venue": "Computing Research Repository, abs/1010.4,", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "This challenging issue is known as concept drift [12], in which the statistical properties of the input attributes and target classes shifted over time.", "startOffset": 49, "endOffset": 53}, {"referenceID": 8, "context": "The combined decision of many single classifiers (mainly using ensemble members diversification) is more accurate than single classifier [9].", "startOffset": 137, "endOffset": 140}, {"referenceID": 19, "context": "[21] [20] [18], [17] [19].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "[21] [20] [18], [17] [19].", "startOffset": 5, "endOffset": 9}, {"referenceID": 16, "context": "[21] [20] [18], [17] [19].", "startOffset": 10, "endOffset": 14}, {"referenceID": 15, "context": "[21] [20] [18], [17] [19].", "startOffset": 16, "endOffset": 20}, {"referenceID": 17, "context": "[21] [20] [18], [17] [19].", "startOffset": 21, "endOffset": 25}, {"referenceID": 26, "context": "In this paper, we focused on the learning adaptation method as an enhancement to Online Sequential Extreme Learning Machine (OS-ELM) [28] and Constructive Enhancement OS-ELM (CEOS-ELM) [26].", "startOffset": 133, "endOffset": 137}, {"referenceID": 24, "context": "In this paper, we focused on the learning adaptation method as an enhancement to Online Sequential Extreme Learning Machine (OS-ELM) [28] and Constructive Enhancement OS-ELM (CEOS-ELM) [26].", "startOffset": 185, "endOffset": 189}, {"referenceID": 23, "context": "The AOS-ELM has capability to handle multiple concept drift problems either changes in the number of attributes (virtual drift/VD) or the number of target classes (real drift/RD) or both at the same time (hybrid drift/HD), also for recurrent context (all concepts occur alternately) or sudden drift (new concept substitutes previous concepts) [25].", "startOffset": 343, "endOffset": 347}, {"referenceID": 7, "context": "Our scope of attribute changes discussed in this paper is on the feature space concatenation that widely used in data fusion, kernel fusion, and ensemble learning [8] and not on the feature selection (irrelevant features removal) methods [5].", "startOffset": 163, "endOffset": 166}, {"referenceID": 4, "context": "Our scope of attribute changes discussed in this paper is on the feature space concatenation that widely used in data fusion, kernel fusion, and ensemble learning [8] and not on the feature selection (irrelevant features removal) methods [5].", "startOffset": 238, "endOffset": 241}, {"referenceID": 2, "context": "A preliminary version of RD and its early results appeared in conference proceedings [3].", "startOffset": 85, "endOffset": 88}, {"referenceID": 15, "context": "Unlike ensemble systems [17,41] that need to manage the complex combination of a vast number of classifiers, we pursue a single classifier for simple implementation while retaining comparable performance for handling multiple (consecutive) drifts.", "startOffset": 24, "endOffset": 31}, {"referenceID": 38, "context": "Unlike ensemble systems [17,41] that need to manage the complex combination of a vast number of classifiers, we pursue a single classifier for simple implementation while retaining comparable performance for handling multiple (consecutive) drifts.", "startOffset": 24, "endOffset": 31}, {"referenceID": 30, "context": "Transfer learning focuses on extracting the knowledge from one or more source task domains and applies the knowledge to a different target task domain [32].", "startOffset": 151, "endOffset": 155}, {"referenceID": 35, "context": "ing is not associated with time and requires the entire training and testing data set [38].", "startOffset": 86, "endOffset": 90}, {"referenceID": 25, "context": "In this paper, we discussed the transfer learning on numeric handwritten MNIST [27] to alpha-numeric handwritten USPS [33] recognition.", "startOffset": 79, "endOffset": 83}, {"referenceID": 32, "context": "The artificial data sets are streaming ensemble algorithm (SEA) [35] and STAGGER [23], which are commonly used as benchmark in sequential learning.", "startOffset": 64, "endOffset": 68}, {"referenceID": 21, "context": "The artificial data sets are streaming ensemble algorithm (SEA) [35] and STAGGER [23], which are commonly used as benchmark in sequential learning.", "startOffset": 81, "endOffset": 85}, {"referenceID": 25, "context": "The real data sets are handwritten recognition data: MNIST for numeric [27] and USPS for alpha-numeric classes [33].", "startOffset": 71, "endOffset": 75}, {"referenceID": 10, "context": "[12] explained many concept drift methods have been developed, but the terminologies are not well established.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "Other terms are concept shift or conditional change [13].", "startOffset": 52, "endOffset": 56}, {"referenceID": 11, "context": "Other terms are feature change [13], temporary drift, or sampling shift.", "startOffset": 31, "endOffset": 35}, {"referenceID": 38, "context": "Popular concept drift handling methods are indicated by ellipses [41].", "startOffset": 65, "endOffset": 69}, {"referenceID": 22, "context": "Kuncheva [24,25] explained the various configuration patterns of data sources over time as random noise, random trends (gradual changes), random substitutions (abrupt or sudden changes), and systematic trends (recurring context).", "startOffset": 9, "endOffset": 16}, {"referenceID": 23, "context": "Kuncheva [24,25] explained the various configuration patterns of data sources over time as random noise, random trends (gradual changes), random substitutions (abrupt or sudden changes), and systematic trends (recurring context).", "startOffset": 9, "endOffset": 16}, {"referenceID": 38, "context": "\u017dliobait\u0117 [41] proposed a taxonomy of concept drift tackling methods as shown in Fig.", "startOffset": 10, "endOffset": 14}, {"referenceID": 38, "context": "\u017dliobait\u0117 [41] explained that most attention on the concept drift tackling methods are drawn to multi-classifier model selection and fusion rules, but little attention on the model construction of base classifier.", "startOffset": 10, "endOffset": 14}, {"referenceID": 10, "context": "[12] proposed a complete online adaptive learning scheme that organized four modules: memory, change detection, learning, and loss estimation (See Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[12].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "Huang [18] explained the term \u2019Extreme\u2019 meant to move beyond conventional artificial neural network learning that required iterative tuning.", "startOffset": 6, "endOffset": 10}, {"referenceID": 2, "context": "Based on [3], Liang et.", "startOffset": 9, "endOffset": 12}, {"referenceID": 26, "context": "[28] proposed online learning for ELM named OS-ELM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 24, "context": "CEOS-ELM [26] has addressed this problem by adding hidden nodes in the sequential", "startOffset": 9, "endOffset": 13}, {"referenceID": 15, "context": "To the best of our knowledge, no previous single base ELM approach specifically addresses many concept drifts learning [17].", "startOffset": 119, "endOffset": 123}, {"referenceID": 3, "context": "However, some papers [4, 36] already discussed how the ELM implementation in adaptive environment.", "startOffset": 21, "endOffset": 28}, {"referenceID": 33, "context": "However, some papers [4, 36] already discussed how the ELM implementation in adaptive environment.", "startOffset": 21, "endOffset": 28}, {"referenceID": 33, "context": "[36] proposed Online Pseudo Inverse Update Method (OPIUM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "[4] proposed two-phase classification algorithm: First, weighted ensemble classifier based on ELM (WEC-ELM) algorithm, which can dynamically adjust classifier", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "Learning is the process of constructing function \u03b2\u0302 to map between observation and its nature called (class) [7].", "startOffset": 109, "endOffset": 112}, {"referenceID": 6, "context": "According to [7], the empirical squared error minimization is consistent under general conditions.", "startOffset": 13, "endOffset": 16}, {"referenceID": 13, "context": "Based on Law of Large Numbers (LLN) theorem [15] and Theorem 2.", "startOffset": 44, "endOffset": 48}, {"referenceID": 10, "context": "The Concept drift refers to an online supervised learning model when the relation between the input data and the target variable changes over time [12].", "startOffset": 147, "endOffset": 151}, {"referenceID": 5, "context": "Our transition space idea was inspired by geometric approach for solving many problems in the fields of pattern recognition and machine learning [6, 34].", "startOffset": 145, "endOffset": 152}, {"referenceID": 31, "context": "Our transition space idea was inspired by geometric approach for solving many problems in the fields of pattern recognition and machine learning [6, 34].", "startOffset": 145, "endOffset": 152}, {"referenceID": 15, "context": "[17] explained interpolation theory from ELM point of view as stated by the following description:", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "2 and Learning Principle I of ELM Theory [18], the input weight and bias as hidden nodes H parameters are independent of training samples and their learning environment through randomization.", "startOffset": 41, "endOffset": 45}, {"referenceID": 15, "context": "[17] explained universal approximation capability of ELM as described by the following theorem:", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "3 and inspired by the related works [3, 26], we devised the AOS-ELM real drift capability by modifying the output matrix with zero block matrix concatenation to change the size dimension of the matrix without changing the value.", "startOffset": 36, "endOffset": 43}, {"referenceID": 24, "context": "3 and inspired by the related works [3, 26], we devised the AOS-ELM real drift capability by modifying the output matrix with zero block matrix concatenation to change the size dimension of the matrix without changing the value.", "startOffset": 36, "endOffset": 43}, {"referenceID": 20, "context": "According to [22], the target values \u2208 {0, 1} is equivalent with \u2208 {\u22121, 1}.", "startOffset": 13, "endOffset": 17}, {"referenceID": 0, "context": "Algorithm 1 Algorithm OSELMSeq {OS-ELM Sequential } Require: X(k) \u2208 [\u22121, 1]Rd\u00d7N , T(k) \u2208 [0, 1]RN\u00d7m, A(k), bL, K(k\u22121), \u03b2(k\u22121) Ensure: \u03b2(k),K(k) 1: Compute H(k) = g(A(k) \u00b7X(k) + bL) 2: if IncreaseHiddenNodes == true then 3: \u2206Ad\u00d7\u03b4L = RandomNumbers([\u22121, 1] ,Rd\u00d7\u03b4L) 4: \u2206b\u03b4L = RandomNumbers([\u22121, 1] ,R) 5: A(k) = [ A(k)\u2206Ad\u00d7\u03b4L ]", "startOffset": 89, "endOffset": 95}, {"referenceID": 32, "context": "SEA [35] and STAGGER [23] (See Table 2a).", "startOffset": 4, "endOffset": 8}, {"referenceID": 21, "context": "SEA [35] and STAGGER [23] (See Table 2a).", "startOffset": 21, "endOffset": 25}, {"referenceID": 22, "context": "The expected result is the classifier has good performance for the newest concept [24].", "startOffset": 82, "endOffset": 86}, {"referenceID": 25, "context": "We tested our algorithm with real-world public data sets from MNIST numeric (0 to 9) [27] and the USPS alphanumeric (A to Z, 0 to 9) handwritten dataset [33].", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": "We used original grey-level image attributes [Xgrey] of MNIST data set and the combination of [Xgrey] with additional attributes from the 9x9 bins histogram of orientated gradients (XHOG) of grey-level image features [29].", "startOffset": 217, "endOffset": 221}, {"referenceID": 14, "context": "We designed the initial input weights and bias based on robust OS-ELM with regularization scalar c (ROS) [16] and then based on initial random from the normal distribution (NORM).", "startOffset": 105, "endOffset": 109}, {"referenceID": 9, "context": "al [10].", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "al [10].", "startOffset": 3, "endOffset": 7}, {"referenceID": 22, "context": "The performance expectation of sequential version classifier is to approximate the offline version of the classifier (desiderata for online classifiers [24]).", "startOffset": 152, "endOffset": 156}, {"referenceID": 21, "context": "SEA - X - OS-ELM, CEOS-ELM, Kolter [23]", "startOffset": 35, "endOffset": 39}, {"referenceID": 21, "context": "STAGGER - X - OS-ELM, CEOS-ELM, Kolter [23]", "startOffset": 39, "endOffset": 43}, {"referenceID": 21, "context": "Predictive Accuracy The accuracy measurement of the future sequential training data [23].", "startOffset": 84, "endOffset": 88}, {"referenceID": 21, "context": "Comparison with inducing decision trees (DWM-ITI) for STAGGER [23], AOS-ELM outperformed DWM.", "startOffset": 62, "endOffset": 66}, {"referenceID": 0, "context": "Using similar method on [1, 31], we can", "startOffset": 24, "endOffset": 31}, {"referenceID": 29, "context": "Using similar method on [1, 31], we can", "startOffset": 24, "endOffset": 31}, {"referenceID": 15, "context": "Some studies [17,21] suggested for hidden node size to be equal at least to the rank value of training data.", "startOffset": 13, "endOffset": 20}, {"referenceID": 19, "context": "Some studies [17,21] suggested for hidden node size to be equal at least to the rank value of training data.", "startOffset": 13, "endOffset": 20}, {"referenceID": 24, "context": "Thus, we have a requirement to increase \u03b4L in sequential stage [26].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "poor training data or poor learning parameter selection may cause the diagonal squared matrix P has less diagonalizable [14], thus not full rank anymore.", "startOffset": 120, "endOffset": 124}, {"referenceID": 34, "context": "Some machine learning methods have been applied with the hope of improving detection rates and adaptive capability [37].", "startOffset": 115, "endOffset": 119}, {"referenceID": 33, "context": ", Greville\u2019s method [36]).", "startOffset": 20, "endOffset": 24}, {"referenceID": 28, "context": ", Weighted OS-ELM for imbalanced learning [30], ELM Autoencoder (ELM-AE) [39], Stacked ELM [40], etc.", "startOffset": 42, "endOffset": 46}, {"referenceID": 36, "context": ", Weighted OS-ELM for imbalanced learning [30], ELM Autoencoder (ELM-AE) [39], Stacked ELM [40], etc.", "startOffset": 73, "endOffset": 77}, {"referenceID": 37, "context": ", Weighted OS-ELM for imbalanced learning [30], ELM Autoencoder (ELM-AE) [39], Stacked ELM [40], etc.", "startOffset": 91, "endOffset": 95}, {"referenceID": 1, "context": "\u2022 A detail systematic explanation based on rule extraction [2] for AOS-ELM in handling adaptive environment.", "startOffset": 59, "endOffset": 62}], "year": 2016, "abstractText": "A machine learning method needs to adapt to over time changes in the environment. Such changes are known as concept drift. One approach to concept drift handling is by feeding the whole training data set once again into a learning machine for retraining. Another approach is by rebuilding an ensemble classifiers to adapt to a new training data set. In either approach, retraining or rebuilding classifiers are expensive and not practical. In this paper, we propose an enhancement of Online-Sequential Extreme Learning Machine (OS-ELM) and its variant Constructive Enhancement OS-ELM (CEOS-ELM) by adding an adaptive capability for classification and regression problem. The scheme is named as Adaptive OS-ELM (AOS-ELM). It is a single classifier scheme that works well to handle real drift, virtual drift, and both drifts occurred at the same time (hybrid drift). The AOS-ELM also works well for sudden drift as well as recurrent context change type. The scheme is a simple unified method implemented in simple lines of code. We evaluated AOS-ELM on regression and classification problem by using various public dataset widely used for concept drift verification from SEA and STAGGER; and other public datasets such as MNIST and USPS. Experiments show that our method gives higher kappa value compared to the multi-classifier ELM ensemble. Even though AOS-ELM in practice does not need hidden nodes increase, we address some issues related to the increasing of the hidden nodes such as error condition and rank values. We propose to take the rank of the pseudo inverse matrix as an indicator parameter to detect \u2019under-fitting\u2019 condition. Keywords\u2014 adaptive, concept drift, extreme learning machine, online sequential.", "creator": "LaTeX with hyperref package"}}}