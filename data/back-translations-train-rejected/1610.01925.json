{"id": "1610.01925", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Metaheuristic Algorithms for Convolution Neural Network", "abstract": "A typical modern optimization technique is usually either heuristic or metaheuristic. This technique has managed to solve some optimization problems in the research area of science, engineering, and industry. However, implementation strategy of metaheuristic for accuracy improvement on convolution neural networks (CNN), a famous deep learning method, is still rarely investigated. Deep learning relates to a type of machine learning technique, where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human. In this paper, we propose the implementation strategy of three popular metaheuristic approaches, that is, simulated annealing, differential evolution, and harmony search, to optimize CNN. The performances of these metaheuristic methods in optimizing CNN on classifying MNIST and CIFAR dataset were evaluated and compared. Furthermore, the proposed methods are also compared with the original CNN. Although the proposed methods show an increase in the computation time, their accuracy has also been improved (up to 7.14 percent).", "histories": [["v1", "Thu, 6 Oct 2016 16:11:06 GMT  (499kb,D)", "http://arxiv.org/abs/1610.01925v1", "Article ID 1537325, 13 pages. Received 29 January 2016; Revised 15 April 2016; Accepted 10 May 2016. Academic Editor: Martin Hagan. in Hindawi Publishing. Computational Intelligence and Neuroscience Volume 2016 (2016)"]], "COMMENTS": "Article ID 1537325, 13 pages. Received 29 January 2016; Revised 15 April 2016; Accepted 10 May 2016. Academic Editor: Martin Hagan. in Hindawi Publishing. Computational Intelligence and Neuroscience Volume 2016 (2016)", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.NE", "authors": ["l m rasdi rere", "mohamad ivan fanany", "aniati murni arymurthy"], "accepted": false, "id": "1610.01925"}, "pdf": {"name": "1610.01925.pdf", "metadata": {"source": "CRF", "title": "Metaheuristic Algorithms for Convolution Neural Networks", "authors": ["L.M. Rasdi Rere", "Mohamad Ivan Fanany", "Aniati Murni Arymurthy"], "emails": ["laode.mohammad@ui.ac.id"], "sections": [{"heading": null, "text": "This technique has managed to solve some optimization problems in the fields of science, engineering and industry. However, the implementation of strategies to improve the functioning of machines and systems that can be applied by a human is not yet so advanced. In this paper, we propose the implementation of three popular approaches, most of which are capable of approaching the goal of artificial intelligence to create a machine that successfully performs all intellectual tasks that can be performed by a human."}, {"heading": "1 Metaheuristic algorithms", "text": "Metaheuristic methods work for three main purposes: to solve problems quickly, to solve large problems, to produce a more robust algorithm. These methods are also simple to design and flexible and easy to implement [2]. In general, metaheuristic algorithms use a combination of rules and randomization to duplicate the phenomena of nature. The biological system imitates metaheuristic2 / 16 algorithms, for example, are evolutionary strategy, GA, and DE. Phenomena of ethology for example are particle swarm optimization (PSO), bee colony optimization (BCO), bacterial foraging optimization algorithms (BFOA), and ant colony optimization (ACO).Metristic metrics of physics."}, {"heading": "1.1 Simulated Annealing algorithm", "text": "SA is a technique of random search for the problem of global optimization. It mimics the process of annealing in material processing [20]. This technique was first proposed in 1983 by Kirkpatrick, Gelatt and Vecchi [5]. SA's basic idea is to use the random search, which not only allows for changes that improve fitness function, but also maintains some changes that are not ideal. As an example, in a minimum optimization problem, all better changes that reduce the fitness value f (x) are accepted, but some changes that increase f (x) are also accepted with a transition probability (p) as follows: p = exp (\u2212 \u2206 E kT) (1), for which the energy level changes, k is the Boltzmann constant, and T is the temperature for controlling the annealing process. This equation is based on the Boltzmann distribution in physics [20]. The following is the standard method of gene temperature optimizer for optimizer problem 1. If the solution temperature is chosen 4."}, {"heading": "1.2 Differential Evolution algorithm", "text": "Differential evolution is first proposed by Price and Storn in 1995 to solve the Chebyshev polynomial problem [1]. This algorithm is based on the difference of the individual, taking advantage of the random search in the solution space, and finally applying the method of mutation, crossover, and selection to obtain the appropriate individual in the system [14]. There are several types in DE, including the classical form DE / rand / 1 / bin, which indicates that in the mutation process the target vector is randomly selected and only a single other vector is used. The acronym bin shows that the crossover process is organized by a rule of binomial decision. The method of the DE algorithm is demonstrated by the following steps: 1. Determination of the parameter setting: Population size is the number of individuals. Mutation factor (F) controls the increase in the two individual differences to avoid stagnation. Crossover rate (CR) determines the number of mutations of the individual, as many as the following."}, {"heading": "1.3 Harmony Search algorithm", "text": "In fact, the players play each key together in the realm of possibility. In the case of pitches, a real harmony is created. This experience is stored in the memory of each individual player, and the players have the opportunity to find a better key. [8] There are three possible alternatives when a key is improvised by a musician."}, {"heading": "2 Convolution Neural Network", "text": "In fact, it is so that most of them are able to survive themselves, and that they are able to survive themselves. (...) Most of them are not able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are able to survive themselves. (...) Most of them are not able to survive themselves. (...)"}, {"heading": "3 Design of proposed methods", "text": "The architecture of this proposed method refers to a simple CNN structure (LeNet-5), not to a complex structure such as AlexNet [6]. We use two variations of the construction structure. First, i-6c-2s-12c-2s, where the number of C1 is 6, and C2 is 12. Second, i-8c-2s-16c-2s, where the number of C1 is 8 and C2 is 18. The core size of all folding layers is 5x5, and the scale of partial scanning is 2. This architecture is designed for the detection of handwritten digits from MNIST datasets. In this proposed method, SA, DE and HS algorithms are used to train CNN (CNNSA, CNNDE, CNNHS) to find the state of best accuracy and also to minimize estimated errors and indicators of network complexity. This goal can be achieved by calculating the lost function of the vector solution or standard function error on this paper, if the following = 12 hours are lost in the actual training set = N."}, {"heading": "3.1 Design of CNNSA method", "text": "In principle, the algorithm on CNN calculates the values of weight and bias in which they are used on the last layer to calculate the lost function. These values of weight and bias in the last layer are used as a solution vector, which is referred to as x in the SA algorithm by adding \u2206 x randomly. Selecting the correct layer of this value will significantly increase the accuracy. For example, in the CNNSA on an epoch when it is x = 0.0008 \u00d7 edge, the accuracy is 88.12, where this value is 5.73 greater than the original CNN (82.39). However, if \u0394x = 0.0001 \u00d7 rand, its accuracy is 85.79 and its value is only 3.40 greater than the original CNN.In addition, this solution vector is updated on the basis of the SA algorithm, where this value is 5.73 greater than the original CNN (82.39) SA. However, if x = 0.001 \u00d7 rand is fulfilled, this value is greater than the original CNN.In addition, its accuracy is updated by both the original CNN and the original CNN."}, {"heading": "3.2 Design of CNNDE method", "text": "This method first calculates all the values of weight and bias. The values of weight and bias on the last layer (x) are used to calculate the lost function, and then these new values are used by random additions \u2206 x to initialize the individuals in the population. 7 / 16Result: Accuracy, time initialization and setup: i-6c-2s-12c-2s; Calculation process: Weights (w), bias (b), lost function f (x); individual xiM in the population: w and b on the last layer; while the termination criteria are not met for each individual xiM in the population PM, auxiliary parents choose x1M, x 2 M, x 3 M; generate offspring xchildM by mutation and recombination; PM + 1 = PM + 1% Best (xchildM, xiM); end M = M + 1; update x for all layers; end algorithm: greater than CNN = 6,00001 when the selection of this system is 0,001 = 0,001."}, {"heading": "3.3 Design of CNNHS method", "text": "For the first time, like CNNSA and CNNDE, this method calculates all values of weight and preload. In this method, the values of weight and preload on the last layer (x) are used to calculate the lost function, and then these new values are randomly added to initialize the harmonic memory. \u2206 x is also an important aspect in this method, while selecting the right layer (x) significantly increases the value of accuracy. For example, for an epoch in CNNHS (i-8c-2s-16c-2s), if \u2206 x = 0.0008 \u00d7 rand, then the accuracy is 87.23, where this value is 7.14 greater than the original CNN (80.09). However, if \u2206 x = 0.0001 \u00d7 rand, its accuracy is 80.23, the value is only 0.14 greater than CNN.In addition, this harmonic memory is updated based on the HS algorithm."}, {"heading": "4 Simulation and results", "text": "In this paper, the primary goal is to improve the accuracy of the original CNN by using SA, DE and HS algorithm. This can be accomplished by minimizing the classification task error that was tested on the MNIST dataset. Some of the examples image for MNIST dataset are shown in Figure.2.In CNNSA experiment, the size of the CNCNCNN dataset was specified = 10 and maximum of iteration (maxit) = 10. In CNNDE, the population size showed = 10 and maxit = 10. Since it is difficult to ensure control of the parameters, in all experiments the values of c = 0.5 for SA, F = 0.8 and cr = 0.3 for DE as well as HMCR = 0.8 and PAR = 0.3 for HS."}, {"heading": "5 Conclusion", "text": "This work shows that SA, DE and HS algorithms improve the accuracy of CNN. Although there is an increase in computing time, the error of the proposed method is smaller for all variations of the epoch than the original CNN. It is possible to validate the performance of this proposed method on other benchmark datasets such as ORL, INRIA, Hollywood II and ImageNet. This strategy can also be developed for other metaheuristic algorithms such as ACO, PSO and BCO to optimize CNN. For the future study, metaheuristic algorithms must be researched that are applied to the other DL methods, such as the relapsing neural network, deep faith network and AlexNet (a newer variant of CNN)."}], "references": [{"title": "A survey on optimization metaheuristics", "author": ["I. Boussaid", "J. Lepagnot", "P. Siarry"], "venue": "Information Science,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2013}, {"title": "Metaheuristics From Design to Implementation", "author": ["El-Ghazali Talbi"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2009}, {"title": "Comparison of training methods for deep neural networks", "author": ["P.O. Glauner"], "venue": "Master Thesis,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Optimization by simulated annealing", "author": ["S. Kirkpatrick", "C. Gelatt", "M. Vecchi"], "venue": "Science, New Series,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 1983}, {"title": "Imagenet classification with eep convolutional neural networks", "author": ["A. Krizhhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "in Proc. Advances in Neural Information Processing Systems 25, Lake Tahoe, Nevada,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2012}, {"title": "Convolution nework and applications in vision", "author": ["Y. LeCun", "K. Kavukcuoglu", "C. Farabet"], "venue": "in Proc. IEEE International Symposium on Circuit and Systems,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2010}, {"title": "A new meta-heuristic algorithm for continuous engineering optimization: harmony search theory and practice", "author": ["K.S. Lee", "Z.W. Geem"], "venue": "Comput. Methods Appl. Mech. Engrg,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2005}, {"title": "On optimization methods for deep learning", "author": ["Q. Lee", "J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "A. Ng"], "venue": "in Proc. The28th International Conference on Machine Learning,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Deep Learning: Methods and Application", "author": ["Li Deng", "Dong Yu"], "venue": "Foundation and Trends in Signal Processing,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2013}, {"title": "Deep learning via hessian-free optimization", "author": ["J. Martens"], "venue": "Proc. The27th International Conference on Machine Learning, Haifa, Israel,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2010}, {"title": "Deep learning applications and challenges in big data analytics", "author": ["M.M. Najafabadi", "et. al"], "venue": "Journal of Big Data,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "An adaptive differential evolution algorithm", "author": ["N. Noman", "D. Bollgala", "H. Iba"], "venue": "IEEE Evolutionary Computation,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Krylov subspace descent for deep learning", "author": ["O.Vinyal", "D. Poyey"], "venue": "Proc. The15th International Conference on Artificial Intelligent and Statistics (AISTATS), La Palma, Canada Island,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2012}, {"title": "Master thesis: Prediction as a candidate for learning deep hierarchical model of data", "author": ["R. Palm"], "venue": null, "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2012}, {"title": "Simulated annealing algorithm for deep learning", "author": ["L.M.R. Rere", "M.I. Fanany", "A.M. Arymurthy"], "venue": "Procedia Computer Science,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2015}, {"title": "Deep learning using genetic algorithms", "author": ["J.L. Sweeney"], "venue": "Master Thesis,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Engineering Optimization: an introduction with metaheuristic application", "author": ["Xin-She Yang"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2010}, {"title": "Learning Deep Architecture for AI, volume 2:No.1", "author": ["Yoshua Bengio"], "venue": "Foundation and Trends in Machine Learning,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2009}, {"title": "The genetic convolutional neural network model based on random sample. ijunesstt", "author": ["Y. Zhining", "P. Yunming"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}], "referenceMentions": [{"referenceID": 10, "context": "Deep learning (DL) is mainly motivated by the research of artificial intelligent, in which the general goal is to imitate the ability of human brain to observe, analyze, learn and make a decision, especially for complex problem [13].", "startOffset": 228, "endOffset": 232}, {"referenceID": 8, "context": "The current reputation of DL is implicitly due to drastically improve the abilities of chip processing, significantly decrease the cost of computing hardware and advance research in machine learning and signal processing [10].", "startOffset": 221, "endOffset": 225}, {"referenceID": 8, "context": "In general, the model of DL technique can be classified into discriminative models, generative models, and hybrid model [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 15, "context": "Although the sound character of DL to solve a variety of learning tasks, training is difficult [18] [3] [17].", "startOffset": 95, "endOffset": 99}, {"referenceID": 2, "context": "Although the sound character of DL to solve a variety of learning tasks, training is difficult [18] [3] [17].", "startOffset": 100, "endOffset": 103}, {"referenceID": 14, "context": "Although the sound character of DL to solve a variety of learning tasks, training is difficult [18] [3] [17].", "startOffset": 104, "endOffset": 108}, {"referenceID": 7, "context": "Nevertheless, CG is slow, so that it needs multicore CPUs and availability of a vast number of RAMs [9].", "startOffset": 100, "endOffset": 103}, {"referenceID": 9, "context": "Hessian-free optimization (HFO) has been applied to train deep auto-encoders [12], proficient in handling under fitting problem, and more efficient than pre-training + fine tuning proposed by Hinton and Salakhutdinov [4].", "startOffset": 77, "endOffset": 81}, {"referenceID": 12, "context": "However, KSD needs more memory than HFO [15].", "startOffset": 40, "endOffset": 44}, {"referenceID": 16, "context": "These optimization techniques have been applied to solve any optimization problems in the research area of science, engineering, and even industry [20].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "One of paper is the combining of genetic algorithm (GA) and CNN, proposed by You Zhining and Pu Yunming [22].", "startOffset": 104, "endOffset": 108}, {"referenceID": 1, "context": "These methods are also simple to design as well as flexible and easy to implement [2].", "startOffset": 82, "endOffset": 85}, {"referenceID": 0, "context": "Phenomena of physic are SA, microcanonical annealing and threshold accepting method [1].", "startOffset": 84, "endOffset": 87}, {"referenceID": 6, "context": "Another form of metaheuristic is inspired by music phenomena, such as HS algorithm [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 0, "context": "Some examples of these algorithms are PSO, BCO, ACO, and BFOA [1].", "startOffset": 62, "endOffset": 65}, {"referenceID": 16, "context": "It mimics the process of annealing in material processing [20].", "startOffset": 58, "endOffset": 62}, {"referenceID": 3, "context": "This technique was firstly proposed in 1983 by Kirkpatrick, Gelatt, and Vecchi [5].", "startOffset": 79, "endOffset": 82}, {"referenceID": 16, "context": "This equation is based on the Boltzmann distribution in physics [20].", "startOffset": 64, "endOffset": 68}, {"referenceID": 0, "context": "Differential Evolution is firstly proposed by Price and Storn in 1995, to solve the Chebyshev polynomial problem [1].", "startOffset": 113, "endOffset": 116}, {"referenceID": 11, "context": "This algorithm is created on individual\u2019s difference, exploiting random search in the space of solution, and finally operate the procedure of mutation, crossover, as well as selection to obtain the suitable individual in system [14].", "startOffset": 228, "endOffset": 232}, {"referenceID": 6, "context": "in 2001 [8].", "startOffset": 8, "endOffset": 11}, {"referenceID": 6, "context": "In the case of pitches create a real harmony; this experience is stored in the memory of each player and they have the opportunity to create better harmony next time [8].", "startOffset": 166, "endOffset": 169}, {"referenceID": 6, "context": "The procedure of HS algorithm can be summarized into five steps as follows [8]:", "startOffset": 75, "endOffset": 78}, {"referenceID": 17, "context": "A substantial advantage of this method, especially for pattern recognition compared with conventional approaches is due to its capability in reducing the dimension of data, extracting the feature sequentially, and classifying in one structure of network [21].", "startOffset": 254, "endOffset": 258}, {"referenceID": 5, "context": "found the state-of-the-art performance on a number of tasks for pattern recognition using error gradient method [7].", "startOffset": 112, "endOffset": 115}, {"referenceID": 5, "context": "The last layer is F6 that is the process of classification [7].", "startOffset": 59, "endOffset": 62}, {"referenceID": 4, "context": "The architecture of this proposed method refers to a simple CNN structure (LeNet-5), not a complex structure like AlexNet [6].", "startOffset": 122, "endOffset": 125}, {"referenceID": 13, "context": "The original program of this simulation is DeepLearn Toolbox from Palm [16].", "startOffset": 71, "endOffset": 75}], "year": 2016, "abstractText": "A typical modern optimization technique is usually either heuristic or metaheuristic. This technique has managed to solve some optimization problems in the research area of science, engineering, and industry. However, implementation strategy of metaheuristic for accuracy improvement on convolution neural networks (CNN), a famous deep learning method, is still rarely investigated. Deep learning relates to a type of machine learning technique, where its aim is to move closer to the goal of artificial intelligence of creating a machine that could successfully perform any intellectual tasks that can be carried out by a human. In this paper, we propose the implementation strategy of three popular metaheuristic approaches, i.e. simulated annealing, differential evolution and harmony search, to optimize CNN. The performance of these metaheuristic methods in optimizing CNN on classifying MNIST and CIFAR dataset were evaluated and compared. Furthermore, the proposed methods are also compared with the original CNN. Although the proposed methods show an increase in the computation time, their accuracy has also been improved (up to 5.73 percent). Keywords\u2014 metaheuristic, convolution neural network, deep learning, simulated annealing, differential evolution, harmony search", "creator": "LaTeX with hyperref package"}}}