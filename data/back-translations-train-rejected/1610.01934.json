{"id": "1610.01934", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks", "abstract": "Deep neural networks have proven to be quite effective in a wide variety of machine learning tasks, ranging from improved speech recognition systems to advancing the development of autonomous vehicles. However, despite their superior performance in many applications, these models have been recently shown to be susceptible to a particular type of attack possible through the generation of particular synthetic examples referred to as adversarial samples. These samples are constructed by manipulating real examples from the training data distribution in order to \"fool\" the original neural model, resulting in misclassification (with high confidence) of previously correctly classified samples. Addressing this weakness is of utmost importance if deep neural architectures are to be applied to critical applications, such as those in the domain of cybersecurity. In this paper, we present an analysis of this fundamental flaw lurking in all neural architectures to uncover limitations of previously proposed defense mechanisms. More importantly, we present a unifying framework for protecting deep neural models using a non-invertible data transformation--developing two adversary-resilient architectures utilizing both linear and nonlinear dimensionality reduction. Empirical results indicate that our framework provides better robustness compared to state-of-art solutions while having negligible degradation in accuracy.", "histories": [["v1", "Thu, 6 Oct 2016 16:20:45 GMT  (986kb,D)", "http://arxiv.org/abs/1610.01934v1", null], ["v2", "Fri, 7 Oct 2016 14:08:07 GMT  (986kb,D)", "http://arxiv.org/abs/1610.01934v2", null], ["v3", "Mon, 17 Oct 2016 15:45:44 GMT  (987kb,D)", "http://arxiv.org/abs/1610.01934v3", null], ["v4", "Tue, 18 Oct 2016 19:53:00 GMT  (987kb,D)", "http://arxiv.org/abs/1610.01934v4", null], ["v5", "Tue, 13 Dec 2016 20:13:33 GMT  (987kb,D)", "http://arxiv.org/abs/1610.01934v5", null]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["qinglong wang", "wenbo guo", "alexander g ororbia ii", "xinyu xing", "lin lin", "c lee giles", "xue liu", "peng liu", "gang xiong"], "accepted": false, "id": "1610.01934"}, "pdf": {"name": "1610.01934.pdf", "metadata": {"source": "CRF", "title": "Using Non-invertible Data Transformations to Build Adversary-Resistant Deep Neural Networks", "authors": ["Qinglong Wang", "Wenbo Guo", "Alexander G. Ororbia II", "Xinyu Xing", "Lin Lin", "C. Lee Giles", "Xue Liu", "Peng Liu", "Gang Xiong"], "emails": [], "sections": [{"heading": null, "text": "This year, it is more than ever before in the history of the country in which it is a country, in which it is a country, in which it is a country, in which it is a country."}, {"heading": "II. BACKGROUND", "text": "We briefly introduce the established DNN (Deep Neural Network) model and then describe how to generate adversarial samples from it to exploit its inherent flaws. Finally, we discuss our threat model."}, {"heading": "A. Deep Neural Networks", "text": "A typical DNN architecture consists of several successive layers of processing elements, or so-called \"neurons.\" Each processing layer can be considered a learning of a different, more abstract representation of the original multidimensional input distribution. As a whole, a DNN can be considered a highly complex function capable of mapping the original multidimensional data in a nonlinear manner. As shown in Figure 2, a typical DNN contains an input layer, several hidden layers, and an output layer. The input layer takes each data sample in the form of a multidimensional vector. Starting from input, the calculation of the activations of each subsequent layer requires only a matrix multiplication (where a weight / parameter vector is equal in length to the hidden units in the target layer)."}, {"heading": "B. The Adversarial Sample Problem", "text": "Although a well-trained model is able to detect out-of-sample patterns, a deep neural architecture can easily be deceived by introducing input variable interference, which is often indistinguishable from the human eye. [35] However, these so-called \"blind spots\" or opposing samples can exist because the DNN entry space is unlimited [10]. Based on this basic error, we can detect specific input data patterns that are able to bypass DNN models. Specifically, [10] has investigated how attackers can find the most powerful blind spots through effective optimization techniques. In multi-level classification tasks, such contradictory samples can cause a DNN model to classify a random class next to the correct one (sometimes not even a reasonable alternative). In addition, DNN models that share the same design goal, such as detecting a common highly complex, relatively large-scale non-linear function."}, {"heading": "C. Threat Model", "text": "The threat models presented in this part all follow the line of exploiting the sensitivity of the DNN models with respect to the input examples. We will first introduce an attack that uses the direct knowledge of the target DNN model. This attack is only valid on the assumption that all detailed DNN information relating to the assumptions mentioned is passed on to an adversary. As this assumption is too strong to be used in real-world scenarios, we will present another threat model that is also effective for deflecting a normal DNN, but is not limited to the assumptions mentioned."}, {"heading": "III. MOTIVATION", "text": "We begin this section with an overview of newer solutions developed to defend against enemy samples, which can be divided into two categories: 1) Extending the training set and 2) Improving model complexity. The former is mainly represented by opposing training, while the latter mainly combines different data transformations with a standard DNN."}, {"heading": "A. Data Augmentation", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "B. Enhancing Model Complexity", "text": "DNN models are already complex, both in terms of the nonlinear function they attempt to approximate and in terms of their layered composition of many parameters. The architecture is simple to facilitate the flow of information forwards and backwards, which greatly reduces the effort involved in generating opposing samples. [26] Therefore, several ideas [26], [11] have been proposed to increase the complexity of the DNN architecture, with the aim of improving the tolerance of complex DNN models for opposing samples generated from simple DNN models. [26] A defensive distillation mechanism is being developed that forms a DNN from data samples distilled by another DNN. By using the knowledge transmitted from the other DNN, the learned DNN classifiers become less sensitive to opposing samples. Although this method proves effective, it still relies on a DNN counter-model that extends from input to input."}, {"heading": "IV. DATA TRANSFORMATION ENHANCED DNN FRAMEWORK", "text": "In this section, we will fully specify the design goals of our framework and select a specific type of data transformation that meets these goals."}, {"heading": "A. Design Goals", "text": "Recall that many previously proposed solutions, especially hostile training methods [10], [24], can be classified as forms of data multiplication, but by their very nature these approaches cannot possibly hope to cover the entire data space unless they have had access to all the important representative points of the underlying diversity (which is highly unlikely in practice), implying that attackers can always find adversary samples that target a particular DNN model. More importantly, the development of adversary training methods requires the invention of efficient methods to generate adversary samples [10], [35] which, as a result, provides more useful tools for attackers, facilitating new attacks designed specifically for DNN models learned using contradictory training algorithms. We argue that a robust DNN architecture has the property that adversary samples cannot be generated from itself."}, {"heading": "B. Framework Overview", "text": "In fact, the fact is that most of them are not a mere formation, but a group of people who are able to move, who are able to move, and who are able to move."}, {"heading": "V. DATA TRANSFORMATION ENHANCED DNNS", "text": "We present two variants of an adversary's resilient DNN architecture that meet the two main objectives of our proposed defense framework (Section IV). Specifically, one variant uses a linear method while the other uses a nonlinear approach. In terms of the linear method, we show that there is theoretically a lower limit for the reconstruction error. More importantly, we design our linear method in such a way that it exhibits a reconstruction error that is substantially greater than that lower limit, which in turn fulfills the characteristics that we have defined for a non-invertible data transformation. In terms of the second approach, we use a nonlinear approach to dimension reduction that we demonstrate fulfills the second characteristic of a non-invertible data transformation. Critically, we show that to restore the original data from this low-dimensional representation, a reversal of the corresponding method of dimensional reduction is required, which can be converted into a square problem with a non-positive semi-defined constraint."}, {"heading": "A. Designed Linear Mapping (DLM) DNN", "text": "It is only a matter of time before such a process occurs, in which such a process occurs. (...) It is only a matter of time before such a process occurs. (...) It is only a matter of time before such a process occurs. (...) It is only a matter of time before such a process occurs. (...) It is only a matter of time before such a process occurs. (...) It is a matter of time before such a process occurs. (...) \"It is a matter of time before such a process occurs.\" (...) \"It is a matter of time.\" (...) \"It is a matter of time.\" (...) \"It is a matter of time.\" (...) \"It is a matter of time.\""}, {"heading": "B. Dimensionality-Reduction: (DrLIM) DNN", "text": "As introduced in Section II, adversary samples are generated by modifying legitimate samples with small interferences, but when they are processed by normal DNN models, the decisions made in a lower dimensional space are completely different from those made for legitimate samples, even if adverse samples are highly similar to legitimate ones. Note that this feature also occurs in cross-model adverse samples. Therefore, we intend to apply a dimensionality reduction method that maintains the similarity of high-dimensional samples in their lower dimensional mappings. In addition, our method must be able to extract critical information contained in the original data. As the formation of a DNN is already computationally intensive, our approach must be gradual in order to avoid retraining of DNN.N. Based on these considerations, we prefer the dimensionality reduction method DrLIM, proposed in [12]. DrLIM is specifically designed to maintain the similarity between high-dimensional samples."}, {"heading": "VI. EVALUATION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experiments Settings", "text": "MNIST includes a training split of 600,000 grayscale images of handwritten digits and a test set of 10,000 images. Each image has a dimensionality of 28x28 = 784 pixels. In the following experiments, we evaluate the proposed approaches using two types of cross-samples. First, to prove that our mechanisms actually receive the classification power of the DNN, we test them with the original test set. Then, we test our methods using counter-examples to show that we are achieving our secondary design goal."}, {"heading": "B. Limitations of Adversarial Training", "text": "Recently, it has been shown that the classification of errors during the test period is effective [10], [24]. Adversarial training samples are generated using the Fast Gradient Method (Section II). In the test phase, new \"adversarial\" samples are generated, which are based on the same objective as the original DNN models. In this case, the adversarial training results are delivered in improved robustness."}, {"heading": "C. Classification Performance", "text": "It is only a matter of time before it happens, until it happens."}, {"heading": "D. Reconstruction Performance", "text": "In fact, it is so that we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in which we are in a time, in a time, in which we are in which we are in which we are in a time, in a time, in which we are in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in a time, in which we are in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in which we are in a time, in"}, {"heading": "VII. RELATED WORK", "text": "In this paper, we build resilient DNN architectures using non-reversible methods of dimension reduction. Related research usually falls into the field of adversarial machine learning or dimensionality reduction. In this section, we present several state-of-the-art adversarial technologies of machine learning, which can be categorized either as data extension or DNN model complexity enhancement. Finally, we present several prevalent methods for dimension reduction and examine their suitability within our proposed framework."}, {"heading": "A. Adversarial Machine Learning", "text": "It is as if it has been able to play by the rules that applied in the past."}, {"heading": "B. Dimensionality Reduction Methods", "text": "From the previous analysis in the aforementioned way, we saw ourselves in a position to be in the position we are in and in which we are in a position to be in a position to be in, to be in a position to be in, to be in a position to be in, to be in a position to be in, to be in a position to be in, to be in a position to be in, to be in a position to be in."}, {"heading": "VIII. CONCLUSION", "text": "Our framework design is based on an analysis of both the \"blind spot\" of DNNs and the limitations of the currently proposed solutions. With our proposed framework, we developed two robust DNN architectures that utilize non-reversible data transformation mechanisms. By using the first proposed approach to the processing of data fed into DNN models, we demonstrated empirically that creating an adversary sample for that architecture entails significant distortions and thus leads to easily detectable adversarial samples. In contrast, in the second architecture, we demonstrated theoretically that it is impossible for an adversary to create an adversarial sample to attack it, implying that our proposed framework no longer suffers from attacks based on the generation of model-specific adversarial samples. Furthermore, we demonstrated that recently investigated adversarial training methods are insufficient defense mechanisms."}], "references": [{"title": "Deep learning with non-medical training used for chest pathology identification", "author": ["Y. Bar", "I. Diamant", "L. Wolf", "H. Greenspan"], "venue": "In SPIE Medical Imaging, pages 94140V\u201394140V. International Society for Optics and Photonics,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Deep learning. Book in preparation for", "author": ["I.G.Y. Bengio", "A. Courville"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2016}, {"title": "Defensive distillation is not robust to adversarial examples", "author": ["N. Carlini", "D. Wagner"], "venue": "arXiv preprint arXiv:1607.04311,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Large-scale malware classification using random projections and neural networks", "author": ["G.E. Dahl", "J.W. Stokes", "L. Deng", "D. Yu"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2013}, {"title": "Relaxations and randomized methods for nonconvex qcqps", "author": ["A. d?Aspremont", "S. Boyd"], "venue": null, "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2003}, {"title": "Compressed sensing", "author": ["D.L. Donoho"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2006}, {"title": "The approximation of one matrix by another of lower rank", "author": ["C. Eckart", "G. Young"], "venue": null, "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1936}, {"title": "Scene parsing with multiscale feature learning, purity trees, and optimal covers", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun"], "venue": "arXiv preprint arXiv:1202.2160,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Compressive sensing. In Handbook of mathematical methods in imaging, pages 187\u2013228", "author": ["M. Fornasier", "H. Rauhut"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Explaining and Harnessing Adversarial Examples", "author": ["I. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2014}, {"title": "Towards deep neural network architectures robust to adversarial examples", "author": ["S. Gu", "L. Rigazio"], "venue": "[cs],", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2014}, {"title": "Dimensionality reduction by learning an invariant mapping", "author": ["R. Hadsell", "S. Chopra", "Y. LeCun"], "venue": "IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR\u201906),", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2006}, {"title": "Learning long-range vision for autonomous off-road driving", "author": ["R. Hadsell", "P. Sermanet", "J. Ben", "A. Erkan", "M. Scoffier", "K. Kavukcuoglu", "U. Muller", "Y. LeCun"], "venue": "Journal of Field Robotics,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2009}, {"title": "Learning multiple layers of representation", "author": ["G.E. Hinton"], "venue": "Trends in cognitive sciences,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2007}, {"title": "Learning with a strong adversary", "author": ["R. Huang", "B. Xu", "D. Schuurmans", "C. Szepesv\u00e1ri"], "venue": "CoRR, abs/1511.03034,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Principal component analysis", "author": ["I. Jolliffe"], "venue": "Wiley Online Library,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2002}, {"title": "Nonmetric multidimensional scaling: a numerical method", "author": ["J.B. Kruskal"], "venue": "Psychometrika, 29(2):115\u2013129,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 1964}, {"title": "The mnist database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Visualizing data using t-sne", "author": ["L. v. d. Maaten", "G. Hinton"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2008}, {"title": "Distributional smoothing with virtual adversarial training", "author": ["T. Miyato", "S.-i. Maeda", "M. Koyama", "K. Nakae", "S. Ishii"], "venue": "stat, 1050:25,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "On optimization methods for deep learning", "author": ["J. Ngiam", "A. Coates", "A. Lahiri", "B. Prochnow", "Q.V. Le", "A.Y. Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2011}, {"title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "author": ["A. Nguyen", "J. Yosinski", "J. Clune"], "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Improving back-propagation by adding an adversarial gradient", "author": ["A. N\u00f8kland"], "venue": "[cs],", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Unifying adversarial training algorithms with flexible deep data gradient regularization", "author": ["A.G. Ororbia II", "C.L. Giles", "D. Kifer"], "venue": "[cs],", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "The limitations of deep learning in adversarial settings", "author": ["N. Papernot", "P. McDaniel", "S. Jha", "M. Fredrikson", "Z.B. Celik", "A. Swami"], "venue": "IEEE European Symposium on Security and Privacy (EuroS&P),", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Distillation as a defense to adversarial perturbations against deep neural networks", "author": ["N. Papernot", "P. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "venue": "arXiv preprint arXiv:1511.04508,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}, {"title": "Malware classification with recurrent networks", "author": ["R. Pascanu", "J.W. Stokes", "H. Sanossian", "M. Marinescu", "A. Thomas"], "venue": "In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Minimax rates of estimation for high-dimensional linear regression over-balls", "author": ["G. Raskutti", "M.J. Wainwright", "B. Yu"], "venue": "IEEE Transactions on Information Theory,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2011}, {"title": "Contractive auto-encoders: Explicit invariance during feature extraction", "author": ["S. Rifai", "P. Vincent", "X. Muller", "X. Glorot", "Y. Bengio"], "venue": "In Proceedings of the 28th international conference on machine learning", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2011}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["S.T. Roweis", "L.K. Saul"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2000}, {"title": "Learning representations by back-propagating errors", "author": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "venue": "Cognitive modeling,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 1988}, {"title": "Recognizing functions in binaries with neural networks", "author": ["E.C.R. Shin", "D. Song", "R. Moazzezi"], "venue": "In 24th USENIX Security Symposium (USENIX Security", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Mastering the game of go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "search. Nature,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2016}, {"title": "Dropout: a simple way to prevent neural networks from overfitting", "author": ["N. Srivastava", "G.E. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 1958}, {"title": "Intriguing properties of neural networks", "author": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2014}, {"title": "Nonlinear Optimization: Complexity Issues", "author": ["S.A. Vavasis"], "venue": null, "citeRegEx": "36", "shortCiteRegEx": "36", "year": 1991}, {"title": "Deep learning of feature representation with multiple instance learning for medical image analysis", "author": ["Y. Xu", "T. Mo", "Q. Feng", "P. Zhong", "M. Lai", "I. Eric", "C. Chang"], "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2014}, {"title": "Droid-sec: Deep learning in android malware detection", "author": ["Z. Yuan", "Y. Lu", "Z. Wang", "Y. Xue"], "venue": "In ACM SIGCOMM Computer Communication Review,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "In European Conference on Computer Vision,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2014}], "referenceMentions": [{"referenceID": 32, "context": "Aside from its highly publicized victories in Go [33], there have been numerous successful applications of deep neural network (DNN) learning in image and speech recognition.", "startOffset": 49, "endOffset": 53}, {"referenceID": 0, "context": "Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8].", "startOffset": 83, "endOffset": 86}, {"referenceID": 36, "context": "Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8].", "startOffset": 88, "endOffset": 92}, {"referenceID": 12, "context": "Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8].", "startOffset": 115, "endOffset": 119}, {"referenceID": 7, "context": "Recent interest has been to integrate it into critical fields like medical imaging [1], [37] and self-driving cars [13], [8].", "startOffset": 121, "endOffset": 124}, {"referenceID": 26, "context": "In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].", "startOffset": 134, "endOffset": 138}, {"referenceID": 3, "context": "In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].", "startOffset": 140, "endOffset": 143}, {"referenceID": 37, "context": "In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].", "startOffset": 145, "endOffset": 149}, {"referenceID": 31, "context": "In cybersecurity, security companies have demonstrated that deep learning could offer a far better way to detect all types of malware [27], [4], [38] and recognize such functions in binary code [32].", "startOffset": 194, "endOffset": 198}, {"referenceID": 34, "context": "However, recent work[35], [22] uncovered a potential flaw in DNN-powered systems that could be readily exploited by an attack.", "startOffset": 20, "endOffset": 24}, {"referenceID": 21, "context": "However, recent work[35], [22] uncovered a potential flaw in DNN-powered systems that could be readily exploited by an attack.", "startOffset": 26, "endOffset": 30}, {"referenceID": 10, "context": "To mitigate the aforementioned kind of attack, solutions [11], [15], [20] proposed generally follow the basic idea of adversarial training in which a DNN is trained with both samples from the original data distribution as well as artificially synthesized adversarial ones.", "startOffset": 57, "endOffset": 61}, {"referenceID": 14, "context": "To mitigate the aforementioned kind of attack, solutions [11], [15], [20] proposed generally follow the basic idea of adversarial training in which a DNN is trained with both samples from the original data distribution as well as artificially synthesized adversarial ones.", "startOffset": 63, "endOffset": 67}, {"referenceID": 19, "context": "To mitigate the aforementioned kind of attack, solutions [11], [15], [20] proposed generally follow the basic idea of adversarial training in which a DNN is trained with both samples from the original data distribution as well as artificially synthesized adversarial ones.", "startOffset": 69, "endOffset": 73}, {"referenceID": 23, "context": "A recent unification of previous approaches [24] showed that they were all special cases of a general, regularized objective function,DataGrad.", "startOffset": 44, "endOffset": 48}, {"referenceID": 23, "context": "Hence, a newly trained DNN would only be robust with respect to previously observed adversarial samples (or for those relatively near to training samples of the underlying manifold if one uses the general approximation to DataGrad developed in [24]).", "startOffset": 244, "endOffset": 248}, {"referenceID": 9, "context": "Demonstration of an adversarial example generated from a panda picture [10].", "startOffset": 71, "endOffset": 75}, {"referenceID": 25, "context": "Technically speaking, our framework is similar to a distillation process, which trains a DNN model using the knowledge transferred from a different model [26].", "startOffset": 154, "endOffset": 158}, {"referenceID": 33, "context": "Standard neural network [34] with two hidden layers, where neurons", "startOffset": 24, "endOffset": 28}, {"referenceID": 1, "context": "This architecture is often referred to as a feed-forward neural network [2]", "startOffset": 72, "endOffset": 75}, {"referenceID": 1, "context": "1There are many types of activations to choose from, including the hyperbolic tangent, the logistic sigmoid, or the linear rectified function, etc [2]", "startOffset": 147, "endOffset": 150}, {"referenceID": 13, "context": "This bottomup propagation of information is also referred to as feedforward inference [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 30, "context": "Parameter gradients are calculated using back-propagation of errors [31].", "startOffset": 68, "endOffset": 72}, {"referenceID": 20, "context": "Since the gradients of the weights represent their influence on the final cost, there have been multiple algorithms developed for finding optimal weights more accurately and efficiently [21].", "startOffset": 186, "endOffset": 190}, {"referenceID": 34, "context": "Even though a well trained model is capable of recognizing out-of-sample patterns, a deep neural architecture can be easily fooled by introducing perturbations to the input variables that are often indistinguishable to the human eye [35].", "startOffset": 233, "endOffset": 237}, {"referenceID": 9, "context": "These socalled \u201cblind spots\u201d, or adversarial samples, exist because the input space of DNN is unbounded [10].", "startOffset": 104, "endOffset": 108}, {"referenceID": 9, "context": "More specifically, it was studied in [10] that attackers can find the most powerful blind spots via effective optimization procedures.", "startOffset": 37, "endOffset": 41}, {"referenceID": 34, "context": "Furthermore, according to [35], DNN models that share the same design goal, for example recognizing the same image set, all approximate a common highly complex, nonlinear function.", "startOffset": 26, "endOffset": 30}, {"referenceID": 9, "context": "In [10], the efficient and straightforward fast gradient sign method was proposed for calculating adversarial perturbation as shown in in (1):", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "To resolve the blind spot issue, there have been many data augmentation methods proposed for deep learning tasks [10], [24].", "startOffset": 113, "endOffset": 117}, {"referenceID": 23, "context": "To resolve the blind spot issue, there have been many data augmentation methods proposed for deep learning tasks [10], [24].", "startOffset": 119, "endOffset": 123}, {"referenceID": 23, "context": "The same mechanism has also been employed for defending against adversarial samples, also known as adversarial training [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 9, "context": "Given the high dimensionality of data distributions that DNNs typically learn from, the input space is generally considered infinite [10].", "startOffset": 133, "endOffset": 137}, {"referenceID": 23, "context": "Adversarial training can be formally described as adding a regularization term known as DataGrad to a DNN\u2019s training cost function [24].", "startOffset": 131, "endOffset": 135}, {"referenceID": 23, "context": "This is due to the fact that, in most adversarial training approaches [24], [10], adversarial samples can be generated efficiently for a particular type of DNN.", "startOffset": 70, "endOffset": 74}, {"referenceID": 9, "context": "This is due to the fact that, in most adversarial training approaches [24], [10], adversarial samples can be generated efficiently for a particular type of DNN.", "startOffset": 76, "endOffset": 80}, {"referenceID": 9, "context": "The fast gradient sign method [10] can generate a large pool of adversarial samples quickly while DataGrad [24] focuses on dynamically generating them per every parameter update.", "startOffset": 30, "endOffset": 34}, {"referenceID": 23, "context": "The fast gradient sign method [10] can generate a large pool of adversarial samples quickly while DataGrad [24] focuses on dynamically generating them per every parameter update.", "startOffset": 107, "endOffset": 111}, {"referenceID": 23, "context": "DataGrad, as mentioned [24], could be viewed as taking advantage of adversarial perturbations to better explore the underlying data manifold\u2013however, while this leads to improved generalization, it does not offer any guarantees in covering all possible blind-spots.", "startOffset": 23, "endOffset": 27}, {"referenceID": 25, "context": "Therefore, several ideas [26], [11] have been proposed to enhance the complexity of DNN architecture, aiming to improve the tolerance of complex DNN models with respect to adversarial samples generated from simple DNN models.", "startOffset": 25, "endOffset": 29}, {"referenceID": 10, "context": "Therefore, several ideas [26], [11] have been proposed to enhance the complexity of DNN architecture, aiming to improve the tolerance of complex DNN models with respect to adversarial samples generated from simple DNN models.", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "[26] develops a defensive distillation mechanism, which trains a DNN from data samples that are distilled by another DNN.", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "[11] proposed stacking an auto-encoder together with a normal DNN, similar to [26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[11] proposed stacking an auto-encoder together with a normal DNN, similar to [26].", "startOffset": 78, "endOffset": 82}, {"referenceID": 9, "context": "Recall that many previously proposed solutions, especially adversarial training methods [10], [24], can be classified as forms of data augmentation.", "startOffset": 88, "endOffset": 92}, {"referenceID": 23, "context": "Recall that many previously proposed solutions, especially adversarial training methods [10], [24], can be classified as forms of data augmentation.", "startOffset": 94, "endOffset": 98}, {"referenceID": 9, "context": "More importantly, developing adversarial training methods has required the invention of efficient methods for generating adversarial samples [10], [35], consequently providing more useful tools for attackers.", "startOffset": 141, "endOffset": 145}, {"referenceID": 34, "context": "More importantly, developing adversarial training methods has required the invention of efficient methods for generating adversarial samples [10], [35], consequently providing more useful tools for attackers.", "startOffset": 147, "endOffset": 151}, {"referenceID": 4, "context": "This belongs to a class of NP-hard problems, according to [5].", "startOffset": 58, "endOffset": 61}, {"referenceID": 15, "context": "PCA [16] is one of the most widely adopted dimensional reduction methods, especially since it is computationally efficient and easy to implement.", "startOffset": 4, "endOffset": 8}, {"referenceID": 6, "context": "According to the Eckart-Young Theorem [7], the optimal solution is obtained when W consists of the q largest eigenvalues of XX .", "startOffset": 38, "endOffset": 41}, {"referenceID": 27, "context": "we estimate reconstruction X\u0302 for X using high-dimensional linear regression [28] (we omit calculation details due to space constraints).", "startOffset": 77, "endOffset": 81}, {"referenceID": 27, "context": "According to Theorem 1 in [28], we can obtain a lower bound of the reconstruction error, which is the L2 norm of the difference between X and X\u0302 as shown in (6): (", "startOffset": 26, "endOffset": 30}, {"referenceID": 27, "context": "In addition, according to Theorem 2 in [28], there also exists an upper bound of the reconstruction error as follows: (", "startOffset": 39, "endOffset": 43}, {"referenceID": 27, "context": "According to [28], the upper bound of the reconstruction error depends on both the data transformation matrix A and noise \u03c9.", "startOffset": 13, "endOffset": 17}, {"referenceID": 8, "context": "However, since we specifically design A to be highly correlated, the upper bound will be significantly larger than the lower bound [9], [6], and thus result in a larger range for the reconstruction error.", "startOffset": 131, "endOffset": 134}, {"referenceID": 5, "context": "However, since we specifically design A to be highly correlated, the upper bound will be significantly larger than the lower bound [9], [6], and thus result in a larger range for the reconstruction error.", "startOffset": 136, "endOffset": 139}, {"referenceID": 11, "context": "Because of these considerations, we employ the dimensionality reduction method DrLIM proposed in [12].", "startOffset": 97, "endOffset": 101}, {"referenceID": 11, "context": "But first, we briefly review DrLIM (for a detailed explication please see [12]).", "startOffset": 74, "endOffset": 78}, {"referenceID": 38, "context": "For the forward pass of a conventional neural network, it is not guaranteed that the weight matrices are invertible [39], implying that information lost during pooling cannot be recovered.", "startOffset": 116, "endOffset": 120}, {"referenceID": 35, "context": "From earlier work [36], the formulation (12) implies a quadratic problem with a non-positive semi-definite constraint, which is an NP-hard problem.", "startOffset": 18, "endOffset": 22}, {"referenceID": 17, "context": "We evaluate our framework by performing experiments on the the widely-used MNIST data set [18].", "startOffset": 90, "endOffset": 94}, {"referenceID": 9, "context": "Recently, adversarial training has been shown to be effective in decreasing the classification error rates at test time [10], [24].", "startOffset": 120, "endOffset": 124}, {"referenceID": 23, "context": "Recently, adversarial training has been shown to be effective in decreasing the classification error rates at test time [10], [24].", "startOffset": 126, "endOffset": 130}, {"referenceID": 34, "context": "Adversarial training approaches usually augment the training set by integrating adversarial samples [35].", "startOffset": 100, "endOffset": 104}, {"referenceID": 9, "context": "For example, recent work[10] introduced such an objective function, directly combining a regularization term with the original cost function.", "startOffset": 24, "endOffset": 28}, {"referenceID": 22, "context": "Instead of training a DNN with a mixture of adversarial and legitimate samples, other approaches [23] use only adversarial samples.", "startOffset": 97, "endOffset": 101}, {"referenceID": 23, "context": "Despite the difference between these strategies, a unifying framework,DataGrad [24], was proposed to generalize adversarial training of deep architectures and help explain prior approaches [11], [29].", "startOffset": 79, "endOffset": 83}, {"referenceID": 10, "context": "Despite the difference between these strategies, a unifying framework,DataGrad [24], was proposed to generalize adversarial training of deep architectures and help explain prior approaches [11], [29].", "startOffset": 189, "endOffset": 193}, {"referenceID": 28, "context": "Despite the difference between these strategies, a unifying framework,DataGrad [24], was proposed to generalize adversarial training of deep architectures and help explain prior approaches [11], [29].", "startOffset": 195, "endOffset": 199}, {"referenceID": 25, "context": "Recent work [26], denoted as distillation enhances a normal DNN by enabling it to utilize other informative inputs.", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "If an attacker [3] generates adversarial samples by artificially reducing the magnitude of the input to the final softmax function and modifies the objective function used for generating adversarial samples [25], the defensively distilled networks are open to assault.", "startOffset": 15, "endOffset": 18}, {"referenceID": 24, "context": "If an attacker [3] generates adversarial samples by artificially reducing the magnitude of the input to the final softmax function and modifies the objective function used for generating adversarial samples [25], the defensively distilled networks are open to assault.", "startOffset": 207, "endOffset": 211}, {"referenceID": 10, "context": "A denoising autoencoder [11] approach was also proposed to filter out adversarial perturbations.", "startOffset": 24, "endOffset": 28}, {"referenceID": 25, "context": "However, it too suffers from the same problem as [26].", "startOffset": 49, "endOffset": 53}, {"referenceID": 10, "context": "Indeed, adversarial samples of this stacked DNN [11] show even smaller distortion compared with adversarial samples generated from the original DNN.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "One popular linear dimension reduction method with aforementioned characteristic is multi-dimensional scaling (MDS) [17].", "startOffset": 116, "endOffset": 120}, {"referenceID": 29, "context": "We also investigated two nonlinear dimensional reduction methods: locally linear embedding (LLE) [30] and t-SNE [19].", "startOffset": 97, "endOffset": 101}, {"referenceID": 18, "context": "We also investigated two nonlinear dimensional reduction methods: locally linear embedding (LLE) [30] and t-SNE [19].", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "In [12], the authors demonstrate that when this is not the case, LLE will be unable to learn a proper global embedding.", "startOffset": 3, "endOffset": 7}], "year": 2016, "abstractText": "Deep neural networks have proven to be quite effective in a wide variety of machine learning tasks, ranging from improved speech recognition systems to advancing the development of autonomous vehicles. However, despite their superior performance in many applications, these models have been recently shown to be susceptible to a particular type of attack possible through the generation of particular synthetic examples referred to as adversarial samples. These samples are constructed by manipulating real examples from the training data distribution in order to \u201cfool\u201d the original neural model, resulting in misclassification (with high confidence) of previously correctly classified samples. Addressing this weakness is of utmost importance if deep neural architectures are to be applied to critical applications, such as those in the domain of cybersecurity. In this paper, we present an analysis of this fundamental flaw lurking in all neural architectures to uncover limitations of previously proposed defense mechanisms. More importantly, we present a unifying framework for protecting deep neural models using a non-invertible data transformation\u2013developing two adversary-resilient architectures utilizing both linear and nonlinear dimensionality reduction. Empirical results indicate that our framework provides better robustness compared to stateof-art solutions while having negligible degradation in accuracy.", "creator": "LaTeX with hyperref package"}}}