{"id": "1610.01935", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Sequence-based Sleep Stage Classification using Conditional Neural Fields", "abstract": "Sleep signals from a polysomnographic database are sequences in nature. Commonly employed analysis and classification methods, however, ignored this fact and treated the sleep signals as non-sequence data. Treating the sleep signals as sequences, this paper compared two powerful unsupervised feature extractors and three sequence-based classifiers regarding accuracy and computational (training and testing) time after 10-folds cross-validation. The compared feature extractors are Deep Belief Networks (DBN) and Fuzzy C-Means (FCM) clustering. Whereas the compared sequence-based classifiers are Hidden Markov Models (HMM), Conditional Random Fields (CRF) and its variants, i.e., Hidden-state CRF (HCRF) and Latent-Dynamic CRF (LDCRF); and Conditional Neural Fields (CNF) and its variant (LDCNF). In this study, we use two datasets. The first dataset is an open (public) polysomnographic dataset downloadable from the Internet, while the second dataset is our polysomnographic dataset (also available for download). For the first dataset, the combination of FCM and CNF gives the highest accuracy (96.75\\%) with relatively short training time (0.33 hours). For the second dataset, the combination of DBN and CRF gives the accuracy of 99.96\\% but with 1.02 hours training time, whereas the combination of DBN and CNF gives slightly less accuracy (99.69\\%) but also less computation time (0.89 hours).", "histories": [["v1", "Thu, 6 Oct 2016 16:26:58 GMT  (152kb,D)", "http://arxiv.org/abs/1610.01935v1", "14 pages. Submitted to Computational and Mathematical Methods in Medicine (Hindawi Publishin). Article ID 7163687"]], "COMMENTS": "14 pages. Submitted to Computational and Mathematical Methods in Medicine (Hindawi Publishin). Article ID 7163687", "reviews": [], "SUBJECTS": "cs.NE cs.LG", "authors": ["intan nurma yulita", "mohamad ivan fanany", "aniati murni arymurthy"], "accepted": false, "id": "1610.01935"}, "pdf": {"name": "1610.01935.pdf", "metadata": {"source": "CRF", "title": "Sequence-based Sleep Stage Classification using Conditional Neural Fields", "authors": ["Intan Nurma Yulita", "Mohamad Ivan Fanany", "Aniati Murni Arymurthy"], "emails": ["intanurma@gmail.com"], "sections": [{"heading": null, "text": "This year, it will be able to solve the problems mentioned, but it is not yet able to solve the problems mentioned."}, {"heading": "1 2. Materials and Methods", "text": "This chapter describes our data set, the methods used for feature extraction and classification, and our experiments. This research compares two unattended feature extraction methods: DBN and FCM; and three classification methods: HMM, CRF variants, and CNF variants. Each of these methods is described in the following section."}, {"heading": "1.1 Dataset", "text": "s University Hospital and University College Dublin, which can be downloaded at http / / physionet.org / pn3 / ucddb /. This dataset is also used by La \u0178ngkvist et al. 2012, but this paper only tests the first 10 of the available 25 datasets.The dataset consists of a series of recordings of electroencephalography (EEG), electrooculography (EOG) and electromyography (EMG).For the classification, five sleep stages are recorded: awake, stage 1 (S1), stage 2 (S2), slow sleep (SWS) and rapid eye movement (REM).2 / 142 The second dataset was created by a collaboration between Mitra Keluarga Kemayoran Hospital, Kemarta. The dataset was developed through a collaboration between the Ayaya Mitayan Hospital and the Faculty of Sleep Sciences, University Hospital (Kemal), Complete Sleep (5 hours)."}, {"heading": "1.2 Conditional Random Fields", "text": "Detection can be performed without processing dynamic interactions between labels in a sequence. However, taking dynamics into account, modeling certainly has an advantage in exploiting the correlation between labels. Some techniques are specifically aimed at extracting such a correlation. X = x1, x2, x3, x4,.... x (n) (1) Y = y1, y2, y3, y4,.... y (n) (2), where n is the length of the data segment, while x (n) is a feature of vector data from sleep that has a sequence label yn. Members of yn are the set of possible labels. In CRF, F is a global feature and consists of local features defined as follows: F (F, Y, Y, Y, Y), Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, y, y, y, y, y, y, y, y, y, y."}, {"heading": "1.2.1 Hidden States", "text": "To mitigate this constraint, HCRF is designed to insert an intermediate structure of 3 / 14some hidden states into the CRF [5], [23]. Due to this intermediate structure, the conditional models of the HCRF result in the following: p (Y | X, \u03bb) = \u2211 h (Y | h, X.\u03bb) p (h | X, \u03bb) (7), where h is the vector of the intermediate structure. However, the HCRF structure, which is designed to capture the correlation between the characteristics of the data, ignores the dynamics of the intercorrelated label that previously belonged to the CRF structure. Therefore, LDCRF was developed to combine the CRF structure with HCRF [19]. To address the HCRF constraints, p (Y | h, X, \u03bb) of the LDCRF as an exposed model of the LDCRF was formulated as follows: p (CRF) | Y (Y) (Y) (1) (1, F (p), p, p (p), p, p (1, p), p, p (1), p, p (1), p, p, p (1)."}, {"heading": "1.2.2 Gate Function", "text": "Conditional Neural Fields (CNF) combine the advantages of CRF and artificial neural networks [17]. CRF has the advantage of representing the structural relationship between their labels, while artificial neural networks capture the correlation between input and output. In order to represent the function of artificial neural networks in the CRF, a new intermediate structure is added: The new intermediate structure consists of several K-gate functions. These K-gate functions extract the input characteristics to K dimensions. By using the gate functions, the state characteristic is then formulated as follows: F (Y, X) = \u2211 i K \u2211 g = 1 \u03d1 (gf (Y, X, n) (12) As a CRF, the CNF does not capture the intermediate structure between the correlations. Therefore, LDCNF was developed to overcome these deficiencies and also investigated the complex nonlinear relationship of input and output [15]. The resulting model shows two layers: hidden states and gages."}, {"heading": "1.3 Deep Belief Networks", "text": "Deep Belief Networks (DBN) is a directional acyclic graph composed of some hidden variables arranged in layers [11]. DBN operates in two schemes; the first scheme learns unattended as a feature extractor for input data to find a subset of features; the second scheme learns supervised as a classifier; the result of the first scheme becomes the entrance to the second scheme. DBN consists of several Boltzmann Restricted Machines (RBMs). The RBM plays a role in obtaining the weights of the adjacent two layers and constructs a new feature subset. In order to create a model, RBM uses some visible and hidden units. The units in the visible and hidden layers are interconnected, but no connections between units within visible and units within hidden layer. The reconstruction process is carried out by a forward and backward mechanism from the top to the RM and vice versa."}, {"heading": "1.4 FCM Clustering", "text": "FCM clustering is a technique for grouping or grouping cluster objects into all cluster memberships [8]. Although the object is in the membership of all clusters, the degree of membership is different for each cluster. It means that the higher degree of membership in a cluster, then the properties of the object are closer to the center of that cluster. In this study, FCM is used as a function extraction, so it has the same role as DBN. \u2212 The output of function extraction to be achieved in this study is a subset of characteristics that have these characteristics toward the centre.The FCM clustering algorithm is as follows: 1. Initialize X: X \u2212 X is the one to process in the FCM n-dimensional matrix of data and m-attributes. 2. Determine parameter settings: Some clusters c will not be changed within the processing cluster, so the number needs to be defined."}, {"heading": "1.5 Design of The Proposed Method", "text": "The classification of the sleep stages in this study is described in Figure 2: In an earlier step, all signals are pre-processed using notch filtering and downward sampling methods; the pre-processed result is extracted to obtain essential characteristics; the results of the feature extraction are treated in three scenarios before the CNF is modeled, namely: 6 / 14Some new features are extracted by DBN as the number of classes used. In contrast to FCM clustering, which extracts the number of clusters as new features, the three scenarios exhibit some different characteristics. To form a CNF model, the window size used is 0 and the maximum window size is 1000. Model convergence training is performed using the Broyden-Fletcher-Gold-Shanno algorithm. To test the resulting model, the experimental process is performed through 10 cross-validations."}, {"heading": "2 Results and Discussion", "text": "This section explains the evaluation of conditional models obtained from both datasets. Accuracy (%) and calculation time (hours) are evaluated."}, {"heading": "2.1 St. Vincent\u2019s Dataset", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "2.2 Mitra Keluarga Kemayoran\u2019s Dataset", "text": "Mitra Keluarga Kemayoran's dataset also has five classes. To perform the classification of sleep stages in the dataset of Mitra Keluarga Kemayoran, the DBN is used for the extraction of characteristics. The extracted characteristic subset consists of five names of this class. According to Table 7, the use of two to four gates gives an accuracy within a range of 96.99% to 98.9% with a computation time of 0.82 to 0.89 hours for each fold. The accuracy of FCM clustering decreased when using FCM characteristics. On the other hand, the computation time obtained is only 82.7% with the addition of three gates. This implementation is described in detail in Table 8. The table shows that the accuracy of FCM clustering did not increase with the addition of one or two additional gates, on the other hand, the computation time was increased with the addition of three gates. This indicates that the use of FCM clusters in CNF clusters is less accurate than the CNF classification of DN 11 with CRF characteristics."}, {"heading": "3 Conclusion", "text": "This work suggested the sequence-based treatment of sleep signals to classify the sleep stage using conditional neural fields (CNF). In this study, we compared DBN and FCM clustering as feature extractors. The result showed that CNF is largely superior in accuracy and computational time compared to CRF, HCRF, LDCRF and LDCNF. On the other hand, in terms of feature extraction, we found that the DBN is better than FCM clustering. In the first dataset, the accuracy of DBN11 / 14 is lower than the FCM clustering, but with a very thin margin. While in the second dataset, the accuracy of the DBN feature extractor is much higher than FCM clustering. Since FCM clustering is shown to be effective methods for classifying the sleep stage in St. Vincent's data, it may be interesting for our future study to determine the use of a different type of fuzzy clustering (FM) based on the composition of the data."}, {"heading": "4 Acknowledgment", "text": "This work is supported by the Higher Education Center of Excellence Research Grant under contract number 1068 / UN2.R12 / HKP5.00 / 2016, which is funded by the Indonesian Ministry of Research and Higher Education."}, {"heading": "5 Conflict of Interests", "text": "The authors declare that there is no conflict of interest in the publication of this paper."}], "references": [{"title": "The analysis of hospital infection data using hidden markov models", "author": ["B. Cooper", "M. Lipsitch"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1995}, {"title": "A hidden markov model-based isolated and meaningful hand gesture recognition", "author": ["M. Elmezain", "A. Al-hamadi"], "venue": "In Proceeding of World Academy of Science,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2008}, {"title": "Hidden conditional random fields for phone classification", "author": ["A.G. et. al"], "venue": "Interspeech, pages 1117\u20131120,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2005}, {"title": "Hidden conditional random fields", "author": ["A.Q. et. al"], "venue": "IEEE Trans. Pattern Anal. Mach. Intellr,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2007}, {"title": "Language recognition using latent dynamic conditional random field model with phonological features", "author": ["S.B. et. al"], "venue": "Mathematical Problems in Engineering,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Sleep stages classification using wavelettransform and neural network", "author": ["V.P.J. et. al"], "venue": "In Proceedings of the IEEE-EMBS International Conference on Biomedical and Health Informatics,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2012}, {"title": "Implementation of the fuzzy c-means clustering algorithm in meteorological data", "author": ["Y.L. et. al"], "venue": "International Journal of Database Theory and Application,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}, {"title": "An integrated sleep stage classification device based on electrocardiograph", "author": ["I.H. et.al"], "venue": "signal. 2012 International Conference on Advanced Computer Science and Information Systems (ICACSIS),", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "Sleep stages classification using shallow classifier", "author": ["E.P. Giri", "M. Fanany", "A.M. Arymurthy"], "venue": "International Conference on Advanced Computer Science and Information Systems ICACSIS,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G.E. Hinton", "S. Osindero", "Y.W. Teh"], "venue": "Neural Computation,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2006}, {"title": "Conditional random fields: Probabilistic models for segmenting and labeling sequence data", "author": ["J.D. Lafferty", "A. McCallum", "F.C.N. Pereira"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2001}, {"title": "Sleep stage classification using unsupervised feature learning", "author": ["M. Langkvist", "L. Karlsson", "A. Loutfi"], "venue": "Advances in Artificial Neural Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2012}, {"title": "Fundamentals Of Speech Recognition", "author": ["Lawrence Rabiner", "Biing-Hwang Juang"], "venue": null, "citeRegEx": "14", "shortCiteRegEx": "14", "year": 1993}, {"title": "Sequential emotion recognition using latent dynamic conditional neural fields", "author": ["J.C. . Levesque", "L.P. Morency", "C. Gagn\u00e9"], "venue": "In Proc. 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Conditional random fields for image labeling", "author": ["T. Liu", "X. Huang", "J. Ma"], "venue": "Mathematical Problems in Engineering,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Conditional neural fields", "author": ["J. Peng", "L. Bo", "J. Xu"], "venue": "In Proceedings of Neural Information Processing Systems (NIPS),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2009}, {"title": "Conditional random fields for object recognition", "author": ["A. Quattoni", "M. Collins", "T. Darrell"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2004}, {"title": "Cuav sensor fusion with latent-dynamic conditional random fields in coronal plane estimation", "author": ["A.M. Rahimi", "R. Ruschel", "B.S. Manjunath"], "venue": null, "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Fuzzy-based latent-dynamic conditional random fields for continuous gesture recognition", "author": ["S. Z"], "venue": "Comput. Methods Appl. Mech. EngrgOptical Engineering194,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Hidden markov models for detecting anomalous fish trajectories in underwater footage", "author": ["C. Spampinato", "S. Palazzo"], "venue": "IEEE International Workshop on Machine Learning for Signal Processing,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2012}, {"title": "An introduction to conditional random fields", "author": ["C. Sutton", "A. McCallum"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2011}, {"title": "Hidden state conditional random field for abnormal activity recognition in smart homes", "author": ["Y. Tong", "R. Chen", "J. Gao"], "venue": "Entropy,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "What\u2019s the point? frame-wise pointing gesture recognition with latent-dynamic conditional random fields", "author": ["C. Wittner", "B. Schauerte", "R. Stiefelhagen"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2015}, {"title": "Hidden markov models and their applications in biological sequence analysis", "author": ["B.-J. Yoon"], "venue": "Current Genomics,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2009}, {"title": "Fuzzy hidden markov models for indonesian speech classification", "author": ["I.N. Yulita", "T.H. Liong", "Adiwijaya"], "venue": "Journal of Advanced Computational Intelligence and Intelligent Informatics (JACIII),", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Automatic sleep stage classification based on sparse deep belief net and combination of multiple classifiers", "author": ["J. Zhang", "Y. Wu", "J. Bai", "F. Chen"], "venue": "Sage Journals,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}], "referenceMentions": [{"referenceID": 5, "context": "They used wavelet and artificial neural networks [7], shallow classifiers [10], and Deep Belief Networks [27].", "startOffset": 49, "endOffset": 52}, {"referenceID": 8, "context": "They used wavelet and artificial neural networks [7], shallow classifiers [10], and Deep Belief Networks [27].", "startOffset": 74, "endOffset": 78}, {"referenceID": 25, "context": "They used wavelet and artificial neural networks [7], shallow classifiers [10], and Deep Belief Networks [27].", "startOffset": 105, "endOffset": 109}, {"referenceID": 12, "context": "On the other hand, HMM is a widely known method for labeling sequence data [14].", "startOffset": 75, "endOffset": 79}, {"referenceID": 24, "context": "This approach has been used in many areas such as speech [26], gesture [3], marine [21], health [2],and Biology [25], and also sleep stage [13] .", "startOffset": 57, "endOffset": 61}, {"referenceID": 1, "context": "This approach has been used in many areas such as speech [26], gesture [3], marine [21], health [2],and Biology [25], and also sleep stage [13] .", "startOffset": 71, "endOffset": 74}, {"referenceID": 19, "context": "This approach has been used in many areas such as speech [26], gesture [3], marine [21], health [2],and Biology [25], and also sleep stage [13] .", "startOffset": 83, "endOffset": 87}, {"referenceID": 0, "context": "This approach has been used in many areas such as speech [26], gesture [3], marine [21], health [2],and Biology [25], and also sleep stage [13] .", "startOffset": 96, "endOffset": 99}, {"referenceID": 23, "context": "This approach has been used in many areas such as speech [26], gesture [3], marine [21], health [2],and Biology [25], and also sleep stage [13] .", "startOffset": 112, "endOffset": 116}, {"referenceID": 11, "context": "This approach has been used in many areas such as speech [26], gesture [3], marine [21], health [2],and Biology [25], and also sleep stage [13] .", "startOffset": 139, "endOffset": 143}, {"referenceID": 10, "context": "proposes the use of Conditional Random Fields (CRF) to overcome the weakness of HMM [12].", "startOffset": 84, "endOffset": 88}, {"referenceID": 16, "context": "Unlike HMM, CRF also accommodates bias in HMM [18].", "startOffset": 46, "endOffset": 50}, {"referenceID": 2, "context": "Due to its success in the sequence labeling, CRF variants have been developed with some additional functions such as hidden-state (in HCRF) to capture the interacting features in the data [4].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "However, the HCRF eliminated the role of interacting labels, so the LDCRF was proposed to combine these factors [6], [24], [20].", "startOffset": 112, "endOffset": 115}, {"referenceID": 22, "context": "However, the HCRF eliminated the role of interacting labels, so the LDCRF was proposed to combine these factors [6], [24], [20].", "startOffset": 117, "endOffset": 121}, {"referenceID": 18, "context": "However, the HCRF eliminated the role of interacting labels, so the LDCRF was proposed to combine these factors [6], [24], [20].", "startOffset": 123, "endOffset": 127}, {"referenceID": 15, "context": "The new layer is called the gate, which is built into the internal processes [17].", "startOffset": 77, "endOffset": 81}, {"referenceID": 11, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[13] also found that hand-crafted features works better than machine-generated features automatically obtained from raw data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "It was about 8 hours full night sleep of polysomnogram records from five males and females [9].", "startOffset": 91, "endOffset": 94}, {"referenceID": 14, "context": "Some techniques are specifically modeled sequential data aimed at extracting such correlation, one of them is CRF [16].", "startOffset": 114, "endOffset": 118}, {"referenceID": 20, "context": "In CRF, F is a global feature and consists of local features that are defined as follows [22]:", "startOffset": 89, "endOffset": 93}, {"referenceID": 3, "context": "some hidden states into the CRF [5], [23].", "startOffset": 32, "endOffset": 35}, {"referenceID": 21, "context": "some hidden states into the CRF [5], [23].", "startOffset": 37, "endOffset": 41}, {"referenceID": 17, "context": "Therefore, LDCRF was developed to combine the CRF structure and HCRF [19].", "startOffset": 69, "endOffset": 73}, {"referenceID": 15, "context": "2 Gate Function Conditional Neural Fields (CNF) combines the advantages of CRF and Artificial Neural Networks [17].", "startOffset": 110, "endOffset": 114}, {"referenceID": 13, "context": "Thus, LDCNF was developed to overcome these deficiencies and also studied the complex non-linear relationship of input to output [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 9, "context": "3 Deep Belief Networks Deep Belief Networks (DBN) a directed acyclic graph that is composed of some hidden variables that are arranged in layers [11].", "startOffset": 145, "endOffset": 149}, {"referenceID": 6, "context": "4 FCM Clustering FCM clustering is a technique to group or to cluster objects into all cluster memberships [8].", "startOffset": 107, "endOffset": 110}], "year": 2016, "abstractText": "Sleep signals from a polysomnographic database are sequences in nature. Commonly employed analysis and classification methods, however, ignored this fact and treated the sleep signals as non-sequence data. Treating the sleep signals as sequences, this paper compared two powerful unsupervised feature extractors and three sequence-based classifiers regarding accuracy and computational (training and testing) time after 10-folds cross-validation. The compared feature extractors are Deep Belief Networks (DBN) and Fuzzy C-Means (FCM) clustering. Whereas the compared sequence-based classifiers are Hidden Markov Models (HMM), Conditional Random Fields (CRF) and its variants, i.e., Hidden-state CRF (HCRF) and Latent-Dynamic CRF (LDCRF); and Conditional Neural Fields (CNF) and its variant (LDCNF). In this study, we use two datasets. The first dataset is an open (public) polysomnographic dataset downloadable from the Internet, while the second dataset is our polysomnographic dataset (also available for download). For the first dataset, the combination of FCM and CNF gives the highest accuracy (96.75%) with relatively short training time (0.33 hours). For the second dataset, the combination of DBN and CRF gives the accuracy of 99.96% but with 1.02 hours training time, whereas the combination of DBN and CNF gives slightly less accuracy (99.69%) but also less computation time (0.89 hours). Keywords\u2014 Sleep stage, Conditional Neural Fields, Deep Belief Networks, Fuzzy C-Means Clustering, Classification", "creator": "LaTeX with hyperref package"}}}