{"id": "1610.01986", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Active exploration in parameterized reinforcement learning", "abstract": "Online model-free reinforcement learning (RL) methods with continuous actions are playing a prominent role when dealing with real-world applications such as Robotics. However, when confronted to non-stationary environments, these methods crucially rely on an exploration-exploitation trade-off which is rarely dynamically and automatically adjusted to changes in the environment. Here we propose an active exploration algorithm for RL in structured (parameterized) continuous action space. This framework deals with a set of discrete actions, each of which is parameterized with continuous variables. Discrete exploration is controlled through a Boltzmann softmax function with an inverse temperature $\\beta$ parameter. In parallel, a Gaussian exploration is applied to the continuous action parameters. We apply a meta-learning algorithm based on the comparison between variations of short-term and long-term reward running averages to simultaneously tune $\\beta$ and the width of the Gaussian distribution from which continuous action parameters are drawn. When applied to a simple virtual human-robot interaction task, we show that this algorithm outperforms continuous parameterized RL both without active exploration and with active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm.", "histories": [["v1", "Thu, 6 Oct 2016 18:34:04 GMT  (217kb,D)", "http://arxiv.org/abs/1610.01986v1", "Submitted to EWRL2016"]], "COMMENTS": "Submitted to EWRL2016", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["mehdi khamassi", "costas tzafestas"], "accepted": false, "id": "1610.01986"}, "pdf": {"name": "1610.01986.pdf", "metadata": {"source": "CRF", "title": "Active exploration in parameterized reinforcement learning", "authors": ["Mehdi Khamassi", "Costas Tzafestas"], "emails": ["mehdi.khamassi@upmc.fr", "ktzaf@cs.ntua.gr"], "sections": [{"heading": null, "text": "Keywords: Reinforcement Learning, Exploration / Exploitation, Multi-Armed Bandits, Meta-Learning, Active Exploration, Parameterized / Structured Reinforcement Learning."}, {"heading": "1. Introduction", "text": "In fact, it is the case that it is a matter of a way in which people are able to determine themselves what they want and what they do not want. (...) In fact, it is the case that people are able to determine themselves. (...) It is the case that people are able to determine themselves. (...) It is the case that people are able to determine themselves what they want. (...) It is the case that people are able to determine themselves what they want. (...) It is the case that people are able to determine themselves. (...) It is the case that they are able to determine themselves. (...) It is so. \"(...) It is so.\" (...) It is so. \"(...) It is so.\" (...) It is so. \"(...) It is so. (...) It is so. (...)"}, {"heading": "2. Active exploration algorithm", "text": "The algorithm is summarized in algorithm 1. It first uses Q-Learning (Watkins and Dayan, 1992) to learn the value of discrete action (Qt + 1, a). \u2212 Qt (st, at). \u2212 Qt (st, at). The probability of discrete action (st, at). \u2212 Qt (st, at). \u2212 Qt (st, at). \u2212 Qt (st, at)."}, {"heading": "3. Experiments", "text": "We test the algorithm described in section 2 in a simple simulated human-robot interaction with a single state (6 discrete actions and continuous action parameters between - 100 and 100. The task resembles a non-stationary stochastic, multi-armed bandit task, except that there is a fixed probability of reward for each discrete action, an action is only rewarded if its continuous parameters are within a Gaussian distribution around the current optimal action parameter \u00b5 with variance? (unknown to the robot) Any time measurement, change so that the task is not stationary and requires constant re-exploration and learning by the robot. The reward is given by a dynamic system based on the virtual engagement e (t) of the human in the task. This engagement is intended to represent the attention that the human pays to the robot. It starts at 5, increases to a maximum emaxix = 10, if the robot builds, decreases and decreases the corresponding actions with the corresponding parameters."}, {"heading": "4. Conclusion", "text": "In this paper, we demonstrated that a meta-learning algorithm based on online variations of reward averages can be used to adjust two exploration parameters simultaneously to choose between discrete actions and continuous action parameters in a parameterized action space. While we had previously successfully used the Kalman Q-Learning of Geist and Pietquin (2010) to coordinate model-based and model-free reinforcement learning in a stationary task (Viejo et al., 2015), it was not suitable for the current non-stationary task. The proposed active exploration scheme could be a promising solution for parameterized reinforcement learning robotics applications."}, {"heading": "Acknowledgments", "text": "We thank Kenji Doya, Beno Et Girard, Olivier Pietquin, Bilal Piot, Inaki Rano, Olivier Sigaud and Guillaume Viejo for their useful discussions. This research was supported in part by the EU-funded BabyRobot project (H2020-ICT-24-2015, grant agreement no. 687831) (MK, CT), by the Agence Nationale de la Recherche (ANR-11-IDEX0004-02 Sorbonne-Universite, its SU-15-R-PERSU-14 Robot Parallearning Project) (MK) and by Labex SMART (ANR-11-LABX-65 Online Budgeted Learning Project) (MK)."}], "references": [{"title": "Robot skill learning: From reinforcement learning to evolution", "author": ["F. Stulp", "O. Sigaud"], "venue": null, "citeRegEx": "Stulp and Sigaud.,? \\Q2003\\E", "shortCiteRegEx": "Stulp and Sigaud.", "year": 2003}, {"title": "Modeling choice and reaction time during", "author": ["G. Viejo", "M. Khamassi", "A. Brovelli", "B. Girard"], "venue": null, "citeRegEx": "Viejo et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Viejo et al\\.", "year": 2007}], "referenceMentions": [], "year": 2016, "abstractText": "Online model-free reinforcement learning (RL) methods with continuous actions are playing a prominent role when dealing with real-world applications such as Robotics. However, when confronted to non-stationary environments, these methods crucially rely on an exploration-exploitation trade-off which is rarely dynamically and automatically adjusted to changes in the environment. Here we propose an active exploration algorithm for RL in structured (parameterized) continuous action space. This framework deals with a set of discrete actions, each of which is parameterized with continuous variables. Discrete exploration is controlled through a Boltzmann softmax function with an inverse temperature \u03b2 parameter. In parallel, a Gaussian exploration is applied to the continuous action parameters. We apply a meta-learning algorithm based on the comparison between variations of short-term and long-term reward running averages to simultaneously tune \u03b2 and the width of the Gaussian distribution from which continuous action parameters are drawn. When applied to a simple virtual human-robot interaction task, we show that this algorithm outperforms continuous parameterized RL both without active exploration and with active exploration based on uncertainty variations measured by a Kalman-Q-learning algorithm.", "creator": "LaTeX with hyperref package"}}}