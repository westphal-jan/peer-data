{"id": "1610.02003", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Scalable Machine Translation in Memory Constrained Environments", "abstract": "Machine translation is the discipline concerned with developing automated tools for translating from one human language to another. Statistical machine translation (SMT) is the dominant paradigm in this field. In SMT, translations are generated by means of statistical models whose parameters are learned from bilingual data. Scalability is a key concern in SMT, as one would like to make use of as much data as possible to train better translation systems.", "histories": [["v1", "Thu, 6 Oct 2016 19:22:49 GMT  (1168kb,D)", "http://arxiv.org/abs/1610.02003v1", "Master Thesis"]], "COMMENTS": "Master Thesis", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["paul baltescu"], "accepted": false, "id": "1610.02003"}, "pdf": {"name": "1610.02003.pdf", "metadata": {"source": "CRF", "title": "Scalable Machine Translation in Memory Constrained Environments", "authors": ["Paul-Dan Baltescu"], "emails": [], "sections": [{"heading": null, "text": "In this area, machine translation is the dominant paradigm. In this area, translations are generated using statistical models whose parameters are learned from bilingual data. Scalability is a central concern in SMT, as one wants to use as much data as possible to build better translation systems. In recent years, mobile devices with sufficient computing power have become widely used. Although they are very successful, mobile applications based on NLP systems continue to follow a client-server architecture that is of limited use because access to the Internet is often limited and expensive. The goal of this dissertation is to construct a scalable machine translation system that can work with the resources available on a mobile device."}, {"heading": "1 Introduction 1", "text": "1.1 Goals of theses.......................................... 2 1.2 contributions.................................. 3 1.3 Theses Structure............................................"}, {"heading": "2 Statistical Machine Translation 7", "text": "The overarching role that countries play is that they feel they are able to advance their interests, the overarching role that they play, the overarching role that they play, the over-arching role that they play, the over-arching role that they play, the over-arching role that they play, the over-arching role that they play, the over-arching role that they play, the over-arching role that they play, the over-arching role that they play, the over-arching role that they play, the over-arching role that they play, the over-arching role that they play."}, {"heading": "3 Online Grammar Extractors 28", "text": "3.1 Introduction........................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "4 Neural Language Models 39", "text": ".)......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "5 Conclusions 60", "text": "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -"}, {"heading": "1.1 Thesis Goals", "text": "The primary objective of this thesis is to present a scalable approach to the construction of high-quality machine translation systems that can run in memory-constrained environments such as mobile devices or utility machines. We will address the two main challenges that prevent standard translation systems from operating in such environments: the representation of the translation model in memory and in the structure of the language model. First, we will examine compact representations of translation models based on suffix arrays to efficiently localize phrases on the source side of a parallel corpus and extract translation rules on the fly. Second, we will examine neural language models as an efficient alternative to traditional n-gram language models and analyze the impact of the most popular scaling techniques on end-to-end-to-end translation quality. We will show that the introduction of our proposed alternatives into a standard translation system will preserve their rapid translation equivalency to replace their components."}, {"heading": "1.2 Contributions", "text": "This year it has come to the point where it will be able to retaliate in order to retaliate."}, {"heading": "1.3 Thesis Structure", "text": "In this section, we discuss the structure of the thesis and summarize the contents of each chapter: we refer to the translation methods presented in this dissertation for publications of which we are the lead author; we show which parts are based on previously published material as we review the topics covered by each chapter. Chapter 2: Statistical Machine Translation 1The code is released at: https: / / github.com / oxlm.4This chapter presents a short introduction to statistical machine translation. Our goal is to explain how a translation system works, how we prepare the reader for the topics covered in the next chapters; we discuss the standard approaches applied by each component of a translation system and show where difficulties arise when the amount of memory is limited."}, {"heading": "2.1 Introduction", "text": "In this chapter, we discuss the core components of a standardized machine translation system. We take a pragmatic approach and go step by step through the stages involved in building a translation system from the ground up. Starting from large amounts of parallel and monolingual texts that we both use, it is possible to distinguish the different types of translation. Our presentation closely follows the steps of building a base system with cdec (Dyer et al., 2010) and Moses (Koehn et al., 2007), two popular open source translation tools that deal with these tools, is also useful if the work in the final chapters of this thesis is tightly integrated with these frameworks. In fact, the reader can use the key components discussed here, and the extensions of Chapter 3 and Chapter 4, and construct an efficient, high-quality system that can work on a storage medium. We show this in Chapter 5. We provide minimal information that we will not provide comprehensive statistical verification of the trends and intuitive techniques used instead."}, {"heading": "2.2 Alignment Models", "text": "In fact, it is such that most of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to move, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move"}, {"heading": "2.3 Translation Models", "text": "Most translation models are based on one of the following two formalisms: finite state transducers (FSTs) or synchronous context-free grammars (SCFGs). These formalisms are similar in nature to their better-known monolingual counterparts, finite state machines and context-free grammars, but have the ability to model a target language in addition to the source language, making them suitable for machine translation. In this section, we present their formal definitions and briefly describe a model of each type. We also introduce phrase tables, the standard approach to storing translation models in memory, and explain how they are constructed from a word aligned parallel to the corpus."}, {"heading": "2.3.1 Finite State Transducers", "text": "In fact, the translation of words used in the translation is a set of source and target words (words, phrases, etc.).StatesQ are a concise presentation of translation hypotheses, and the transitions D specify the basic rules defining the space of valid translations. For example, transitions can model the process of translating individual word units as follows: a transition q1 s / t - Q2 - Q2 - Q2 - Q2 - Q3 - Q3 - Q4 - Q4 - Q4 - Q4 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q5 - Q"}, {"heading": "2.3.2 Synchronous Context Free Grammars", "text": "In the United States, where the United States and the EU are able to bridge the differences between the two countries, it is not possible to bridge the differences between the two countries. \"We,\" according to the author, \"are both able to make clear the differences between the two countries.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"We.\" - \"-\" We. \"-\" - \"We.\" - \"-\" We. \"-\" - \"We.\" -. \"-.\" - \"We.\" -. -. -. -. -. - \"-\" We. \"-\" - \"We.\" - \"-\" We. \"-\" - \"We.\" - \"-\" - \"We.\" - \"-\" - \"-\" We. \"-\" - \"-\" - \"-\" We. \"-\" - \"-\" - \"-\" - \"We.\" - \"-\" - \"-\" - \"-\". \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-.\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-.\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-.\" - \"-\" - \"-\" - \"-\" -. \"-\" - \"-\" - \"-\" - \"-.\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-\" - \"-. -\" - \"-\" - \"-\" -. - \"-\" - \"-\" - \"-\" - \"-. -\" - \"-\" -. -. -.. -. -. -. -. \"-.\" -. \"-.\" -. \"-.\" -. -. -. -. -. -. -. \"-.\" -. \"-.\" -. \"-\" - \"-"}, {"heading": "2.3.3 Phrase Tables", "text": "In subsection 2.3.1 and subsection 2.3.2, we present the two most common formalities that define the structure of the rules used by translation systems. In this subsection, we focus on how these rules are stored in memory and explain how they are extracted from a verbatim parallel phrase pattern (a task known as phrase or grammar extraction).The traditional approach to storing translation rules in memory is achieved via phrase tables. A phrase table maps each source phrase to all the target phrases that align with it in the parallel text. For each target phrase, the phrase table also stores an additional set of scores whose role is discussed in Section 2.5. Phrase tables are often circumscribed so that only the most common phrase pairs are retained. Pruning can be based on the weights associated with each target phrase, or by holding a limited number of target phrases for each."}, {"heading": "2.4 Language Models", "text": "In the eeisn eeisrcnh-eaeaJnlhsrcnlhsrteeaeaaJnlrh nvo rf\u00fc ide eeisn-eaeaJnlrmnlhsrteeaeaJnlrh-eaJnlrgnea-eaJnlrh-eaJnlrsrteeaeaeSrlrh-eaJngr-eaeSrrrteeaeaeHnlrmnlrrrteeaeaeaeaeaeaeaeeeeaeeeeaee.nwdr eiD \"nI\" s tis rf\u00fc, \"n so asds rf\u00fc ide eeisrrrrf\u00fc ide eeisrlrlrlrlrlrrteeaeaeaeeeaeeeeeeeeeVrlrlrlrlrlrlrrrrrrrrteeu, nlrrf\u00fc, nlrrf\u00fc, nlrf\u00fc,\" so tasos ros ros for ros, rfos for rrfos, rrrrrrrrfos."}, {"heading": "2.5 Scoring Model", "text": "The translation formalities introduced in Section 2.3 consist of a set of rules that define the entire set of valid translations. However, not all of these translations are equally good. The scoring model provides a framework for comparing these translations by assigning a probability to all output sentences. However, the aim of the decoder (Section 2.6) is to follow the best translation in terms of the scoring model of the translation model.The aim of a translation model is to find arg maxt P (t) where s is the source sentence and t is a possible translation. It is useful to extend this definition to the translation rules used by the system to produce t. Let d show a set of rules (derivatives) and S (t) that produce derivatives d. We can rewrite our goal as arg maxt d (t) S (t). Unfortunately, this optimization problem is unworkable for both FSTS and FSCs."}, {"heading": "2.6 Decoding", "text": "In a machine translation system, the decoder is the component responsible for translating a source sentence into a target sentence or creating a list of the n-best translations. At a high level, the decoder generates a series of partial translation hypotheses by repeatedly applying rules licensed by the translation model (Section 2.3) and evaluating them with the scoring model (Section 2.5). In this section, we examine how decoding is performed for both FST and SCFG-based translation models. Both FST and SCFG models exhibit a high degree of ambiguity and define a massively exponential space of valid translations for each source sentence. Aggressive, fine-tuned optimization techniques must be applied to keep the decoder workable and maintain a high bar for translation quality. In this section, some of these techniques are reviewed."}, {"heading": "2.6.1 Decoding with FST Models", "text": "This year it is more than ever before."}, {"heading": "2.6.2 Decoding with SCFG Models", "text": "rf\u00fc ide rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf"}, {"heading": "2.7 Evaluation", "text": "Assessing the quality of a machine translation system is a tough problem, because sentences can often be translated in many ways. It is possible that equivalent translations do not share words, while sentences with many words in common terms have completely different meanings. Human translators are able to judge when a system produces good translations, but a scalable solution was necessary to support the massive investment in machine translation in recent decades. As a result, the research community, which has defined and adopted several automatic metrics to evaluate translation quality, is considering several obvious advantages over human assessments in its controversy: (i) they facilitate rapid iteration by seeing what new features bring improvements, (ii) they are a cost-effective way to compare systems developed by different research groups (provided that the same training and testing data are used) and (iii) they eliminate the subjective bias and the inherent human errors."}, {"heading": "3.1 Introduction", "text": "As explained in Section 2.3.3, phrase tables load all pairs of phrases that can be extracted from a parallel corpus in memory and organize them as a dictionary by mapping source phrases to lists of target phrases. However, phrase tables are very efficient in queries because they support constant time access to translation rules, but their main weakness is their enormous memory space, which makes them unsuitable for storage environments. This problem is exacerbated in SCFG-based systems, where the number of extractable rules is exponentially within the maximum range of a phrase. In fact, scaling phrase tables for hierarchical phrase systems is problematic, even without additional memory constraints. A naive solution, often used by the research community to facilitate decoding with limited resources (e.g. on raw material machines) is to filter the phrase table and to remove any translation rules that are not applicable to a particular test."}, {"heading": "3.2 Grammar Extraction for Contiguous Phrases", "text": "A suffix array (Manber and Myers, 1990) is a memory-efficient data structure that can efficiently locate all the beginnings of a suffix array. (D) A suffix array is the list of suffix arrays in lexicographic order. (D) A suffix array is the list of suffix arrays in lexicographic order. (D) A suffix array is the starting position of the i-th smallest suffix in w, i.e."}, {"heading": "3.3 Grammar Extraction for Phrases with Gaps", "text": "In Chapter 2, we have shown that hierarchical translation systems are based on synchronous context-free grammar formalisms that allow them to use translation rules that contain gaps. In this section, we present an algorithm for extracting synchronous context-free rules from a parallel corpus that requires us to improve the extraction algorithms from Section 3.2 to deal with contiguous phrases. We first published this algorithm in Baltescu and Blunsom (2014) and it was later adopted as a central piece in Er et al. (2015) \"s work on massive scaling of discounted phrases using GPU.\" Let's take some notes to facilitate the exposure of phrase extraction."}, {"heading": "3.4 Implementation Details", "text": "Our Diagram Extractor is designed as a stand-alone tool that takes as input a verbatim parallel corpus and a test set and produces as output the set of translation rules applicable to each set in the test set. The Extractor produces the output in the format expected by 34the cdec decoder, but the implementation is complete in itself and easily expandable to other hierarchical diagrams based on diagrams translation systems. Our tool performs the diagram extraction in two steps. The pre-processing step takes as input the parallel corpus and file containing the word alignments, and writes on disk binary representations of the data structures needed in the extraction step: a dictionary that includes diagrams tokens on numeric id diagrams, the source data field, the word alignment, the precalculated index of frequently discontinuous diagrams overlaps, and the data book descriptions of the diagrams are needed in the Extractor:"}, {"heading": "3.5 Experiments", "text": "In this section, we present a series of experiments demonstrating the benefits of our new extraction algorithm. We compare our implementation with the cdec cython extractor, which implements the algorithm proposed by Lopez (2007). To make the comparison fair and prove that the speed we get is actually a result of our new algorithm, we also report results for a C + + implementation of the algorithm in Lopez (2007).In our experiments, we used French-English data from the Europarl corpus, a set of 2M set pairs containing a total of 105M tokens. Training data was tokenized, lowercase and sentence pairs with unusual length ratios were filtered out of the corpus preparation scripts available in cdec.2. The corpus was aligned with fast alignments (Dyer et al., 2013) and the alignments were symmetrized."}, {"heading": "3.6 Summary", "text": "In this chapter, we explored compact alternatives to phrase tables. We started with a brief presentation of existing techniques and demonstrated why online grammar extractors are a natural choice for constructing an autonomous translation system that can run on a utility machine or mobile device. We first explored how suffix array grammar extractors work in phrase-based translation systems, and then introduced a novel hierarchical phrase extraction algorithm that is four times faster than Lopez (2007). We provided details of our open source implementation and showed how to maximize parallelism without having a negative impact on memory footprint. Finally, we presented several experiments demonstrating the benefits of the approach proposed in this chapter.38Chapter 4Neural Language Models"}, {"heading": "4.1 Introduction", "text": "This year, the time has come for us to be able to find a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, and that is able to find a solution that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution, that is capable of finding a solution."}, {"heading": "4.2 Experimental Setup", "text": "In our experiments, we use data from the 2014 edition of the workshop in Machine Translation.1 We train standard phrase-based translation systems for French \u2192 English \u2192 Czech and English \u2192 German with the Moses toolkit (Koehn et al., 2007).We used the europarl and the news commentary corpora as parallel data for training the translation systems. The parallel corpora were tokenized, reduced in size and sentences longer than 80 words were removed with standard word processing.2 Table 4.1 contains statistics on the training corpora after the pre-processing step. We aligned the translation systems with the newer data of 2013 by applying minimal error rate training (Och, 2003) and we use the newest 2014 corpora to report unextracted BLEU results of over 3 rounds. The language models were trained on the europarl, news commentary, and the 2007 editions of the news corpora."}, {"heading": "4.3 Model Description", "text": "As a basis for our study, we implement a probabilistic neural language model as defined in Bengio et al. (2003). (3) For each word w in the target vocabulary Vt, we learn two distributed representations qw and rw in RD. The vector qw captures the syntactic and semantic role of the word w when w is part of a conditioning context, while rw its3We use our implementation as scalable open source neural language modeling tools at: https: / github.com / pauldb89 / oxlm.43role as a prediction. To simplify our notations, we let hi denote the conditional context wi \u2212 1i \u2212 n for a word wi in a given corpus. To find the conditional probability P (wi | hi), our model first calculates a context projection vector Vi = f (n \u00b2 Cj \u2212 1 = jj \u2212 qwi), with a specific transformative \u2212 qwi."}, {"heading": "4.4 Normalization", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.4.1 Introduction", "text": "Optimizing the calculation of the softmax function in the output layer of the network is the most important step in creating neural language models in practice. This operation is performed for each data point presented as input to the network both during training and during decoding. For a given input, the temporal complexity of the softmax step is queried several hundred thousand times for a medium-length sentence. Previous publications on neural language modeling in machine translation have addressed this problem in two different ways. Vaswani et al. (2013) and Devlin et al. (2014) simply ignore normalization in decoding, although Devlin et al. (2014) change their training goal to learn self-normalized models, i.e. models where the sum of values in the output layer is explicit."}, {"heading": "4.4.2 Class Factored Models", "text": "The class-based factorization trick (Goodman, 2001) is a way to reduce the excessive calculation effort in the Softmax level. We divide the vocabulary into toK classes {C1,..., CK} so that V = K i = 1 Ci and Ci-Cj = \u2205 1 \u2264 i < j \u2264 K. We define the conditional probabilities as: P (wi | hi) = P (wi | ci, hi) P (wi | ci, hi), where ci is the class to which the word wi belongs, d. We adjust the model definition to also take into account the class probabilities P (ci | hi). We associate for each class c a distributed representation sc and a distortion term tc. The class-conditioned probabilities are calculated using the projection vector p with a new scoring function (c, hi) = sTcp + tc. The probabilities are standardized separately: sc and a class representation tc (wi | ci, hi, hi). The class-conditioned probabilities are calculated p with a new scoring function (c, hi), hi) = sTcp + tc. (ci | hi) are calculated separately: sc and a class representation sc and a distortion concept tc. The class-conditioned probabilities are calculated sc (wi | ci, hi, hi, hi, hi, hi) using the projection vector p with a new scoring function (c, hi, hi p (c, hi)."}, {"heading": "4.4.3 Tree Factored Models", "text": "One can take the idea set out in the previous subsection a step further and construct a tree above the vocabulary Vt. The words in the vocabulary are used to label the leaves of the tree. Let n1,.., nk the nodes on the way from the root (n1) to the leaf labeled with wi (nk). The probability of the word wi to follow the context is defined as: P (wi | hi) = k-j = 2 P (nj | n1,.., nj \u2212 1, hi).We associate a distributed representation sn and the bias term tn to each node in the tree. The conditional probabilities are obtained by using the scoring function (nj, hi): P (nj | n1,., hi)., nj \u2212 1, hi = exp (nj, hi)."}, {"heading": "4.4.4 Experiments", "text": "In this section, we evaluate three types of models: unnormalized, class-factored, and tree-factored models. These models use diagonal context matrices (Section 4.6) and are trained using noise-contrast estimation (Section 4.5), except for tree-factored models where noise-contrast estimation does not improve training time compared to standard gradients. In Table 4.3, we note that class-factor models perform best in terms of translation quality, with 0.5 BLEU points for fr \u2192 en and 0.3 BLEU points for en \u2192 de over unnormalized models, which are the next best performing models in terms of translation quality. These results also suggest that tree-factored models perform poorly compared to the other candidates: -1.2 BLEU points for fr \u2192 en, -1.3 BLEU points for en \u2192 cs, and -1 BLEU points for en \u2192 de. We believe this is a consequence of the introduction of an artificial vocabulary over hierarchy."}, {"heading": "4.5 Noise Contrastive Training", "text": "The question of the manner and manner in which the individual countries behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner and manner in which they behave in the manner in which they behave in the manner and manner in which they behave in the manner in which they behave in the manner in which they behave in the manner in which they behave in the manner in which they behave in the manner in which they behave in the manner and manner in which they behave in the manner in which they behave in the manner in which they behave in the manner and manner in which they behave in the manner in which they behave in the manner in which they behave in the manner in which they behave in the manner and manner in which they behave in the manner in which they behave in the manner in the manner in which they behave in the manner in which they behave in the manner and manner in which they behave in the manner in which they behave in the manner in the manner in which they behave in the manner in which they behave in the manner in the manner in which they behave in the manner and manner in which they behave in the manner in which they behave in the manner in the manner in which they behave in the manner in which they behave in the way in which they behave in which they behave in the way in the way in the manner in which they behave in the way in which they behave in the way in the way in which they behave in which they behave in the way in the way in which they behave in which they behave in the way in the way in the way in which they behave in which they behave"}, {"heading": "4.6 Diagonal Context Matrices", "text": "In this section we will examine diagonal context matrices as a source for the reduction of computational costs for the calculation of the projection vector P. In the standard definition of a neural language model, these costs are dominated by the Softmax step, but once tricks such as contrastive estimation of noise or hierarchical factorizations are applied, this process becomes the main obstacle for training and querying the model. In a diagonal matrix, all elements outside the main definition are equal to zero. Consequently, the matrix vector products Cjqwi \u2212 j51Acceptance Ratio Memory BLEUs for calculating the projection vector p to the definition's scalar products and the time complexity is reduced from O (D2) to O (D). A similar time shortening is achieved in the backpropagation algorithm, since only O (D) context parameters for calculating the context parameter of the definition must be updated by the definition's calculation of the Context parameter Definition Definition Definition, which also has a need for validity in 2014."}, {"heading": "4.7 Quality vs. Memory Trade-Off", "text": "In this section, we compare neural language models and back-off n-gram models across a wide range of memory and track how the final quality of a translation system changes as these models grow in size. We are conducting this analysis with two objectives in mind. Firstly, we are trying to verify the hypothesis that neural language models are actually better suited to storage environments. Secondly, we are seeking to understand how neural language models function relative to back-off n-gram models if no memory restrictions are enforced, and if the enlargement of word embedding beyond what is normally reported in the research literature is sufficient to correct any BLEU score discrepancies. In this analysis, we have used a compact e-based implementation for the construction of n-gram models (Heafield, 2011). A 5-gram model containing monolingual data in English requires 12 GB of memory."}, {"heading": "4.8 Direct N-gram Features", "text": "We are not only able to understand the different language models, but also to compare the different language models with each other when the appearance of several words together in the context of conditioning provides more precise signals. From the perspective of the underlying neural network, pre-54dictive weights, we are edges that connect the input and output layers of the network and connect the output layers of network direct lyric with each other. (2003). Mikolov et al. (2011a) we extend these features to n-grams and show that they are useful for reducing the perplexity and improving the word error rate in language classes. Direct n-gram characteristics are reminiscent of maximum language classes. (2011a) we extend these characteristics to n-grams and show that they are useful for reducing language errors."}, {"heading": "4.9 Source Sentence Conditioning", "text": "In fact, we are in a position to go in search of a solution that enables us, enables us to go in search of a solution."}, {"heading": "4.10 Summary", "text": "In this chapter, we introduced neural language models as a compact alternative to traditional back-off models for N grammar. Neural language models are notoriously difficult to scale. 58 We addressed the major structural inefficiencies in these models and analyzed the impact of the proposed optimizations on end-to-end translation quality. First, we focused on accelerating the softmax step in the output layer of the neural network. We examined the factoring layer using word classes and multilayer trees, and simply ignored normalization as a whole. We found that class-factored models perform best in terms of quality, but unstandardized models are significantly faster. Tree-factored models do not perform nearly as well as the other candidates and do not provide significant acceleration. We also found that the clustering algorithm used to define word classes has a significant effect on Brown's choice (and that Brown may have a good clustering model)."}, {"heading": "5.1 Building a Compact Translation System", "text": "This year is the highest in the history of the country."}, {"heading": "5.2 Related Work", "text": "Our work focuses on replacing the memory-intensive components of standard translation systems in order to make them usable on mobile devices. In recent years, an alternative approach to building translation systems has emerged, namely the so-called neural translation systems (Kalchburner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015). These systems use a single neural network to model the entire translation process as opposed to standard translation systems, which are a collection of purpose-specific models (Chapter 2). Neural translation models are an example of the more generic framework for sequence mapping (Sutsket al., 2014), in which the models construct a distributed representation of the source sentence, which is then used to generate the target sentence. These models may correspond to state-of-the-art standard translation systems, but require newer and deeper architectures than these."}, {"heading": "5.3 Conclusions", "text": "This year, the time has come for us to be able to unite."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Machine translation is the discipline concerned with developing automated tools for translating from one human language to another. Statistical machine translation (SMT) is the dominant paradigm in this field. In SMT, translations are generated by means of statistical models whose parameters are learned from bilingual data. Scalability is a key concern in SMT, as one would like to make use of as much data as possible to train better translation systems. In recent years, mobile devices with adequate computing power have become widely available. Despite being very successful, mobile applications relying on NLP systems continue to follow a client-server architecture, which is of limited use because access to internet is often limited and expensive. The goal of this dissertation is to show how to construct a scalable machine translation system that can operate with the limited resources available on a mobile device. The main challenge for porting translation systems on mobile devices is memory usage. The amount of memory available on a mobile device is far less than what is typically available on the server side of a client-server application. In this thesis, we investigate alternatives for the two components which prevent standard translation systems from working on mobile devices due to high memory usage. We show that once these standard components are replaced with our proposed alternatives, we obtain a scalable translation system that can work on a device with limited memory. The first two chapters of this thesis are introductory. Chapter 1 discusses the task we undertake in greater detail and highlights our contributions. Chapter 2 provides a brief introduction to statistical machine translation. In Chapter 3, we explore online grammar extractors as a memory efficient alternative to phrase tables. We propose a faster and simpler extraction algorithm for translation rules containing gaps, thereby improving the extraction time for hierarchical phase-based translation systems. In Chapter 4, we conduct a thorough investigation on how neural language models should be integrated in translation systems. We settle on a novel combination of noise contrastive estimation and factoring the output layer using Brown clusters. We obtain a high quality translation system that is fast both when training and decoding and we use it to show that neural language models outperform traditional n-gram models in memory constrained environments. Chapter 5 concludes our work showing that online grammar extractors and neural language models allow us to build scalable, high quality systems that can translate text with the limited resources available on a mobile device.", "creator": "LaTeX with hyperref package"}}}