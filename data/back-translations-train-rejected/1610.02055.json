{"id": "1610.02055", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Places: An Image Database for Deep Scene Understanding", "abstract": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach near-human semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.", "histories": [["v1", "Thu, 6 Oct 2016 20:14:13 GMT  (3512kb,D)", "http://arxiv.org/abs/1610.02055v1", null]], "reviews": [], "SUBJECTS": "cs.CV cs.AI", "authors": ["bolei zhou", "aditya khosla", "agata lapedriza", "antonio torralba", "aude oliva"], "accepted": false, "id": "1610.02055"}, "pdf": {"name": "1610.02055.pdf", "metadata": {"source": "CRF", "title": "Places: An Image Database for Deep Scene Understanding", "authors": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Antonio Torralba", "Aude Oliva"], "emails": [], "sections": [{"heading": null, "text": "Index terms - scene understanding, scene classification, visual recognition, deep learning, deep feature, image dataset.F"}, {"heading": "1 INTRODUCTION", "text": "What does it take to achieve human-level performance with a machine learning algorithm? In the case of supervised learning, the problem is twofold: firstly, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object recognition [3], or the need for temporal connections between different storage units for natural language processing [4], [5]. Secondly, it must have access to a training data set with adequate coverage (quasi-exhaustive representation of classes and diversity of examples) and density (enough samples to cover the diversity of each class). The optimal space for these data sets is often skill-dependent, but the rise of multi-million item sets has enabled unprecedented performance in many areas of artificial intelligence.The successes of Deep Blue in chess, Watson in \"Jeopardy!\" and AlphaGo, unlike their human adversaries, are thus achieved not just 400,000 data sets, but very much in 2.5 million data sets."}, {"heading": "2 PLACES DATABASE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Coverage of the categorical space", "text": "The primary advantage of having a high-quality dataset is comprehensive coverage of the categorical space we want to learn. Places \"strategy is to provide an exhaustive list of the categories of environments that occur in the world and are limited by spaces that a human body would fit into (e.g. closet, shower).The SUN dataset (Scene UNderstanding) [13] provided this first list of semantic categories. The SUN dataset was built around a quasi-exhaustive list of scene categories with different functionalities, namely categories with unique identities in the discourse. By using WordNet [14], the SUN database team selected 70,000 words and concrete terms that describe scenes, places and environments that can be used to complete the phrase\" I am in one place \"or\" let's go to one place. \"Most of the words referring to basic names and entry categories within the same scene are referred to by 900 [15] scenes, after they have been passed through one or more of the same categories."}, {"heading": "2.2 Construction of the database", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.2.1 Step 1: Downloading images using scene category and adjectives", "text": "Candidate images were downloaded from the online image search engines (Google Images, Bing Images and Flickr) with a keyword from the list of scene classes in the SUN database [13]. To increase the variety of visual features in the Places dataset (see Fig. 2), each scene class query was combined with 696 common English adjectives (e.g. chaotic, messy, economical, sunny, desolate, etc.). Approximately 60 million images (color images of at least 200 x 200 pixels in size) were identified with unique URLs. Importantly, the places and SUN datasets complement each other: PCAbased duplicate removal was performed within each scene category in both databases, so that they do not contain the same images."}, {"heading": "2.2.2 Step 2: Labeling images with ground truth category", "text": "Picture bottom truth label verification was performed by crowdsourcing the task to Amazon Mechanical Turk (AMT). Fig.3 illustrates the experimental paradigm used: AMT workers were each given instructions relating to a specific image category at a time (e.g. cliff), with a definition and samples of true and false images. Workers then performed a go / no-go categorical task (Fig.3). The experimental interface showed a central image, flanked by smaller version of the images that the worker had just responded to, on the left, and will respond to the next, on the right. Information from the construction of the SUN Dataset suggests that the first iteration of the caption will show that more than 50% of the downloaded images are not true examples of the category. As illustrated in Fig.3, the default answer is set to No (see pictures with bold red contours), so the worker can press the space bar more easily to move the majority of the images forward."}, {"heading": "2.2.3 Step 3: Scaling up the dataset using a classifier", "text": "As a result of the previous round of image description, 53 million downloaded images were not assigned to any of the 476 scene categories (e.g., a bedroom image could have been downloaded when retrieving images for the living room category but was marked as negative by the AMT employee), so a third annotation task was designed to reclassify and comment on these images using a semi-automatic bootstrapping approach. AlexNet [1], a scene classifier based on deep learning, was trained to classify the remaining 53 million images: we initially randomly selected 1,000 images per scene category as a training set and 50 images as a validation set (for the 413 categories with more than 1,000 samples). AlexNet achieved 32% scene classification accuracy in the validation determined after the training and was then used to classify the 53 million images. We used the predicted class rating by AlexNet to classify the scene category to the best within a few scenarios with 5.000."}, {"heading": "2.2.4 Step 4: Improving the separation of similar classes", "text": "Despite the initial effort to bundle synonyms from WordNet, the scene list from the SUN database still contained categories with very narrow synonyms (e.g. \"ski hut\" and \"ski resort\" or \"landfill\" and \"landfill\"). We identified 46 synonymous pairs like these and merged their images into a single category.4 In addition, some scene categories are easily confused with blurred categorical boundaries, as in Fig. 5. This means that answering the question \"Does image belong to class A?\" could be difficult. It is easier to answer the question \"Does image belong to class A or B?\" In this case, the decision boundary for a human observer becomes clearer and it also moves closer to the final task of training a computer system to solve it. In the previous three steps of AMT annotation, \"it became clear that workers were confused with some high-level scenario categories,\" for example, by \"dividing images from Canybuti\" and \"4ki\" into common categories. \""}, {"heading": "2.3 Scene-Centric Datasets", "text": "Scene-centered datasets correspond to images labeled with a scene or place name, as opposed to an object name. Fig. 6 illustrates the differences between the number of images found in Places, ImageNet, and SUN for a number of scene categories common to all three datasets. Places Database is the largest scene-centered dataset to date."}, {"heading": "2.3.1 Defining the Benchmarks of the Places", "text": "Here we describe four subsets of locations as benchmarks. Places205 and Places88 are from [2]. Two new benchmarks have been added: Of the 434 categories, we have selected 365 categories with more than 4000 images each to the Places365 standard and Places365 challenge.Places365 standard has 1,803,460 training images with the number of images per class varying from 3,068 to 5,000. The validation set has 50 images per class and the test set has 900 images per class.Note that the experiments in this paper on Places365 standard.Places365 challenge contains the same categories as the Places365 standard, but the training set is significantly larger with a total of 8 million training images and the test set contains the same images as the Places365 standard.Places365 challenge contains the images files from Places365-Challenge."}, {"heading": "2.3.2 Dataset Diversity", "text": "In view of the fact that these two are a very complex matter, which is a very complex, complex and complex matter, which is a complex matter, which is a complex matter, which is a complex matter, which is a complex matter, and which is about the single matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-matrix-"}, {"heading": "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION", "text": "Considering the impressive performance of deep revolutionary neural networks (CNNs), especially on the ImageNet benchmark [1], [12], we chose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 revolutionary layer CNN [20], and then trained them on places 205 and 365 standard, respectively, to create basic CNN models. The trained CNNs are called PlacesSubset-CNN, i.e. Places205-AlexNet or Places365VGG. All of the Places CNNs presented here were trained with the Caffe package [21] on the Nvidia Tesla K40 and Titan X2 GPUs. Given the recent ground-breaking performance of the Residual Network (ResNet) on the ImageNet classification [22], we further refined the ResNet152 on places 365 standard (referred to as Places365-ResNet) and trained it on other networks compared to the ResNet."}, {"heading": "3.1 Results on Places205 and Places365", "text": "The ranking results for top-1 accuracy and top-5 accuracy are listed in Table 1. As a baseline comparison, we show the results of a linear SVM database based on ImageNet CNN features of 5000 images per category in Places205 and 50 images per category in SUN205. Places CNNs are much better than the results of a linear CNM database, while, as expected, Places205GoogLeNet and Places205-VGG-Places205 images per scene with a large lead due to their deeper structures. To date (October 2, 2016) the best placed results on the test set of Places205 Leader3 is 64.10% on top-1 accuracy and 90.65% on top-5 accuracy."}, {"heading": "3.2 Web-demo for Scene Recognition", "text": "Based on the Places-CNN we trained, we created a web demo for scene detection5, accessible via a computer browser or mobile phone. People can upload photos to the web demo to predict the type of environment, with the 5 most likely semantic categories and relevant scene attributes. Two screenshots of the prediction result on the mobile phone are in Fig.10. Note that people can submit feedback about the result. the top 5 recognition accuracy of our web demos in the wild is about 72% (of the 9,925 anonymous feedback dated October 19, 2014 to May 5, 2016), which is impressive given that people have uploaded all kinds of photos from real life. http: / / places.csail.edu / demo.htmlGT: Construction site Top-1: Martial Arts Gym (0,157) Top-2: stable (0,156) Top-3: box ring: 90 (0,091): (0,091) (0,091) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005)) (0,005) (0,005) (0,005) (0,005) (0,005)) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005)) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005) (0,005"}, {"heading": "3.3 Generic Visual Features from ImageNet-CNNs and Places-CNNs", "text": "Indeed, in the first half of the twentieth century, the number of unemployed in the US tripled, while the number of unemployed increased in the first half of the year in the first half of the year in the first half of the year."}, {"heading": "3.4 Visualization of the Internal Units and the CNNs", "text": "By visualizing the reactions of the units to different levels of the network layers, we can have a better understanding of what has been learned within CNNs and what the differences are between the object-centric CNN units trained on ImageNet and the scene-centric CNN, trained in places since they share the same architecture (here we use AlexNet). Following the methodology in [33], we estimate the receptive fields of the units in the Places-CNN and ImageNet-CNN, which share the receptive fields visual fields visual fields visual fields units in the Places-CNN and ImageNet-CNN ranges. Then we segment the images with high activation of the estimated receptive fields receptive Units 1 5 10 20 50 01020202020203040506070Number of training samples per category-C lass ifica tion accu racyCombined kernel [37.5] HoG2x2 [2xse23,5] S2xsiFT [2xsift] 222.6 [2x7] Density."}, {"heading": "4 CONCLUSION", "text": "From the Tiny Image Dataset [35] to ImageNet [11] and Places [2], to the rise of multi-million item initiatives and other densely labeled datasets [36] - [39], data-hungry machine learning algorithms have enabled a closer semantic classification of visual patterns such as objects and scenes. Places, with its wide reach and wide variety of examples, provides an ecosystem of visual context to guide progress on current persistent visual recognition problems, which could include determining what happens in a given environment, detecting inconsistent objects or human behavior in a given location, and predicting future events or the cause of events in a given scene."}, {"heading": "ACKNOWLEDGMENTS", "text": "The authors thank Santani Teng, Zoya Bylinskii, Mathew Monfort, and Caitlin Mullin for their comments on the paper. Over the years, the Places project has been supported by the National Science Foundation under grants # 1016862 to A.O and # 1524817 to A.T, ONR N000141613116 to A.O, as well as the MIT Big Data Initiative at CSAIL, Toyota, Google, Xerox, and Amazon Awards, and a hardware donation from NVIDIA Corporation to A.O and A.T. B.Z is supported by a Facebook Fellowship."}], "references": [{"title": "Imagenet classification with deep convolutional neural networks.", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "Advances in Neural Information Processing Systems,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2012}, {"title": "Learning deep features for scene recognition using places database", "author": ["B. Zhou", "A. Lapedriza", "J. Xiao", "A. Torralba", "A. Oliva"], "venue": "In Advances in Neural Information Processing Systems, 2014.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Rich feature hierarchies for accurate object detection and semantic segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural computation, 1997.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep blue", "author": ["M. Campbell", "A.J. Hoane", "F.-h. Hsu"], "venue": "Artificial intelligence, 2002.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Watson: beyond jeopardy!", "author": ["D. Ferrucci", "A. Levas", "S. Bagchi", "D. Gondek", "E.T. Mueller"], "venue": "Artificial Intelligence,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2013}, {"title": "Mastering the game of go with deep neural networks and tree search", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre", "G. Van Den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot"], "venue": "Nature, 2016.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE, 1998.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 1998}, {"title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet: A large-scale hierarchical image database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "Proc. CVPR, 2009.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Imagenet large scale visual recognition challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "venue": "International Journal of Computer Vision, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Sun database: Large-scale scene recognition from abbey to zoo", "author": ["J. Xiao", "J. Hays", "K.A. Ehinger", "A. Oliva", "A. Torralba"], "venue": "Proc. CVPR, 2010.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2010}, {"title": "Wordnet: a lexical database for english", "author": ["G.A. Miller"], "venue": "Communications of the ACM, vol. 38, no. 11, pp. 39\u201341, 1995.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 1995}, {"title": "Pictures and names: Making the connection", "author": ["P. Jolicoeur", "M.A. Gluck", "S.M. Kosslyn"], "venue": "Cognitive psychology, 1984.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1984}, {"title": "Unbiased look at dataset bias", "author": ["A. Torralba", "A.A. Efros"], "venue": "Proc. CVPR, 2011.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Indices of diversity and evenness", "author": ["C. Heip", "P. Herman", "K. Soetaert"], "venue": "Oceanis, 1998.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 1998}, {"title": "Measurement of diversity.", "author": ["E.H. Simpson"], "venue": "Nature,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1949}, {"title": "Going deeper with convolutions", "author": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "venue": "Proc. CVPR, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "arXiv preprint arXiv:1409.1556, 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Caffe: An open source convolutional architecture for fast feature embedding", "author": ["Y. Jia"], "venue": "http://caffe.berkeleyvision.org/, 2013.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "Proc. CVPR, 2016.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2016}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["J. Donahue", "Y. Jia", "O. Vinyals", "J. Hoffman", "N. Zhang", "E. Tzeng", "T. Darrell"], "venue": "2014.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Cnn features off-the-shelf: an astounding baseline for recognition", "author": ["A.S. Razavian", "H. Azizpour", "J. Sullivan", "S. Carlsson"], "venue": "arXiv preprint arXiv:1403.6382, 2014.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Recognizing indoor scenes", "author": ["A. Quattoni", "A. Torralba"], "venue": "Proc. CVPR, 2009.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2009}, {"title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "author": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "venue": "Proc. CVPR, 2006.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Sun attribute database: Discovering, annotating, and recognizing scene attributes", "author": ["G. Patterson", "J. Hays"], "venue": "Proc. CVPR, 2012.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2012}, {"title": "Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories", "author": ["L. Fei-Fei", "R. Fergus", "P. Perona"], "venue": "Computer Vision and Image Understanding, 2007.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2007}, {"title": "Caltech-256 object category dataset", "author": ["G. Griffin", "A. Holub", "P. Perona"], "venue": "2007.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2007}, {"title": "Human action recognition by learning bases of action attributes and parts", "author": ["B. Yao", "X. Jiang", "A. Khosla", "A.L. Lin", "L. Guibas", "L. Fei-Fei"], "venue": "Proc. ICCV, 2011.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2011}, {"title": "What, where and who? classifying events by scene and object recognition", "author": ["L.-J. Li", "L. Fei-Fei"], "venue": "Proc. ICCV, 2007.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2007}, {"title": "LIBLINEAR: A library for large linear classification", "author": ["R.-E. Fan", "K.-W. Chang", "C.-J. Hsieh", "X.-R. Wang", "C.-J. Lin"], "venue": "2008.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2008}, {"title": "Object detectors emerge in deep scene cnns", "author": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "venue": "International Conference on Learning Representations, 2015.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "author": ["A. Nguyen", "A. Dosovitskiy", "T. Yosinski", "Jason band Brox", "J. Clune"], "venue": "arXiv preprint arXiv:1605.09304, 2016.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "80 million tiny images: A large data set for nonparametric object and scene recognition", "author": ["A. Torralba", "R. Fergus", "W.T. Freeman"], "venue": "IEEE Trans. on Pattern Analysis and Machine Intelligence, 2008.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2008}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "European Conference on Computer Vision. Springer, 2014, pp. 740\u2013755.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}, {"title": "Semantic understanding of scenes through the ade20k dataset", "author": ["B. Zhou", "H. Zhao", "X. Puig", "S. Fidler", "A. Barriuso", "A. Torralba"], "venue": "arXiv preprint arXiv:1608.05442, 2016.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2016}, {"title": "The pascal visual object classes (voc) challenge", "author": ["M. Everingham", "L. Van Gool", "C.K. Williams", "J. Winn", "A. Zisserman"], "venue": "Int\u2019l Journal of Computer Vision, 2010.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2010}, {"title": "The cityscapes dataset for semantic urban scene understanding", "author": ["M. Cordts", "M. Omran", "S. Ramos", "T. Rehfeld", "M. Enzweiler", "R. Benenson", "U. Franke", "S. Roth", "B. Schiele"], "venue": "arXiv preprint arXiv:1604.01685, 2016.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5].", "startOffset": 122, "endOffset": 125}, {"referenceID": 2, "context": "First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5].", "startOffset": 169, "endOffset": 172}, {"referenceID": 3, "context": "First, the algorithm must be suitable for the task, such as pattern classification in the case of object recognition [1], [2], pattern localization for object detection [3] or the necessity of temporal connections between different memory units for natural language processing [4], [5].", "startOffset": 277, "endOffset": 280}, {"referenceID": 4, "context": "6 million, and 30 million items, respectively [6]\u2013 [8].", "startOffset": 46, "endOffset": 49}, {"referenceID": 6, "context": "6 million, and 30 million items, respectively [6]\u2013 [8].", "startOffset": 51, "endOffset": 54}, {"referenceID": 0, "context": "Convolutional Neural Networks [1], [9] have likewise achieved near human-level visual recognition, trained on 1.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "Convolutional Neural Networks [1], [9] have likewise achieved near human-level visual recognition, trained on 1.", "startOffset": 35, "endOffset": 38}, {"referenceID": 8, "context": "2 million object [10]\u2013[12] and 2.", "startOffset": 17, "endOffset": 21}, {"referenceID": 10, "context": "2 million object [10]\u2013[12] and 2.", "startOffset": 22, "endOffset": 26}, {"referenceID": 1, "context": "5 million scene images [2].", "startOffset": 23, "endOffset": 26}, {"referenceID": 11, "context": "The SUN (Scene UNderstanding) dataset [13] provided that initial list of semantic categories.", "startOffset": 38, "endOffset": 42}, {"referenceID": 12, "context": "Through the use of WordNet [14], the SUN database team selected 70,000 words and concrete terms that described scenes, places and environments that can be used to complete the phrase \u201cI am in a place\u201d, or \u201clet\u2019s go to the/a place\u201d.", "startOffset": 27, "endOffset": 31}, {"referenceID": 13, "context": "Most of the words referred to basic and entry-level names ( [15]), resulting in a corpus of 900 different scene categories after bundling together synonyms, and separating classes described by the same word but referring to different environments (e.", "startOffset": 60, "endOffset": 64}, {"referenceID": 11, "context": "Details about the building of that initial corpus can be found in [13].", "startOffset": 66, "endOffset": 70}, {"referenceID": 11, "context": "1 Step 1: Downloading images using scene category and adjectives From online image search engines (Google Images, Bing Images, and Flickr), candidate images were downloaded using a query word from the list of scene classes provided by the SUN database [13].", "startOffset": 252, "endOffset": 256}, {"referenceID": 0, "context": "A deep learning-based scene classifier, AlexNet [1], was trained to classify the remaining 53 million images: We first randomly selected 1,000 images per scene category as training set and 50 images as validation set (for the 413 categories which had more than 1000 samples).", "startOffset": 48, "endOffset": 51}, {"referenceID": 1, "context": "Places205 and Places88 are from [2].", "startOffset": 32, "endOffset": 35}, {"referenceID": 1, "context": "Places205, described in [2], has 2.", "startOffset": 24, "endOffset": 27}, {"referenceID": 10, "context": "Places88 contains the 88 common scene categories among the ImageNet [12], SUN [13] and Places205 databases.", "startOffset": 68, "endOffset": 72}, {"referenceID": 11, "context": "Places88 contains the 88 common scene categories among the ImageNet [12], SUN [13] and Places205 databases.", "startOffset": 78, "endOffset": 82}, {"referenceID": 1, "context": "Note that Places88 contains only the images obtained in round 2 of annotations, from the first version of Places used in [2].", "startOffset": 121, "endOffset": 124}, {"referenceID": 14, "context": "2 Dataset Diversity Given the types of images found on the internet, some categories will be more biased than others in terms of viewpoints, types of objects, or even image style [16].", "startOffset": 179, "endOffset": 183}, {"referenceID": 14, "context": "Even datasets covering the same visual classes have notable differences providing different generalization performances when used to train a classifier [16].", "startOffset": 152, "endOffset": 156}, {"referenceID": 15, "context": "Several measures of diversity have been proposed, particularly in biology for characterizing the richness of an ecosystem (see [17] for a review).", "startOffset": 127, "endOffset": 131}, {"referenceID": 16, "context": "Here, we propose to use a measure inspired by the Simpson index of diversity [18].", "startOffset": 77, "endOffset": 81}, {"referenceID": 0, "context": "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.", "startOffset": 179, "endOffset": 182}, {"referenceID": 10, "context": "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.", "startOffset": 184, "endOffset": 188}, {"referenceID": 0, "context": "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.", "startOffset": 241, "endOffset": 244}, {"referenceID": 17, "context": "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.", "startOffset": 256, "endOffset": 260}, {"referenceID": 18, "context": "3 CONVOLUTIONAL NEURAL NETWORKS FOR SCENE CLASSIFICATION Given the impressive performance of the deep Convolutional Neural Networks (CNNs), particularly on the ImageNet benchmark [1], [12], we choose three popular CNN architectures, AlexNet [1], GoogLeNet [19], and VGG 16 convolutional-layer CNN [20], then train them on Places205 and Places365-Standard respectively to create baseline CNN models.", "startOffset": 297, "endOffset": 301}, {"referenceID": 19, "context": "All the Places-CNNs presented here were trained using the Caffe package [21] on Nvidia GPUs Tesla K40 and Titan X2.", "startOffset": 72, "endOffset": 76}, {"referenceID": 20, "context": "Additionally, given the recent breakthrough performances of the Residual Network (ResNet) on ImageNet classification [22], we further fine-tuned ResNet152 on the Places365-Standard (termed as Places365-ResNet) and compared it with the other trained-from-scratch PlacesCNNs for scene classification.", "startOffset": 117, "endOffset": 121}, {"referenceID": 1, "context": "1 Results on Places205 and Places365 After training the various Places-CNNs, we used the final output layer of each network to classify the test set images of Places205 and SUN205 (see [2]).", "startOffset": 185, "endOffset": 188}, {"referenceID": 1, "context": "Places365 has 160 more categories than Places205, the Top5 accuracy of the Places205-CNNs (trained on the previous version of Places [2]) on the test set only drops by 2.", "startOffset": 133, "endOffset": 136}, {"referenceID": 21, "context": "Activations from the higher-level layers of a CNN, also termed deep features, have proven to be effective generic features with state-ofthe-art performance on various image datasets [23], [24].", "startOffset": 182, "endOffset": 186}, {"referenceID": 22, "context": "Activations from the higher-level layers of a CNN, also termed deep features, have proven to be effective generic features with state-ofthe-art performance on various image datasets [23], [24].", "startOffset": 188, "endOffset": 192}, {"referenceID": 11, "context": "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].", "startOffset": 125, "endOffset": 129}, {"referenceID": 23, "context": "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].", "startOffset": 144, "endOffset": 148}, {"referenceID": 24, "context": "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].", "startOffset": 158, "endOffset": 162}, {"referenceID": 25, "context": "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].", "startOffset": 178, "endOffset": 182}, {"referenceID": 26, "context": "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].", "startOffset": 195, "endOffset": 199}, {"referenceID": 27, "context": "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].", "startOffset": 212, "endOffset": 216}, {"referenceID": 28, "context": "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].", "startOffset": 236, "endOffset": 240}, {"referenceID": 29, "context": "The deep features from several Places-CNNs and ImageNet-CNNs on the following scene and object benchmarks are tested: SUN397 [13], MIT Indoor67 [25], Scene15 [26], SUN Attribute [27], Caltech101 [28], Caltech256 [29], Stanford Action40 [30], and UIUC Event8 [31].", "startOffset": 258, "endOffset": 262}, {"referenceID": 11, "context": "In the SUN397 experiment [13], the training set size is 50 images per category.", "startOffset": 25, "endOffset": 29}, {"referenceID": 23, "context": "In the MIT Indoor67 experiment [25], the training set size is 100 images per category.", "startOffset": 31, "endOffset": 35}, {"referenceID": 24, "context": "In the Scene15 experiment [26], the training set size is 50 images per category.", "startOffset": 26, "endOffset": 30}, {"referenceID": 25, "context": "In the SUN Attribute experiment [27], the training set size is 150 images per attribute.", "startOffset": 32, "endOffset": 36}, {"referenceID": 26, "context": "In Caltech101 and Caltech256 experiment [28], [29], the training set size is 30 images per category.", "startOffset": 40, "endOffset": 44}, {"referenceID": 27, "context": "In Caltech101 and Caltech256 experiment [28], [29], the training set size is 30 images per category.", "startOffset": 46, "endOffset": 50}, {"referenceID": 28, "context": "In the Stanford Action40 experiment [30], the training set size is 100 images per category.", "startOffset": 36, "endOffset": 40}, {"referenceID": 29, "context": "In the UIUC Event8 experiment [31], the training set size is 70 images per category and the test set size is 60 images per category.", "startOffset": 30, "endOffset": 34}, {"referenceID": 30, "context": "The classifier is a linear SVM with the same default parameters for the two deep feature layers (C=1) [32].", "startOffset": 102, "endOffset": 106}, {"referenceID": 31, "context": "Following the methodology in [33] we estimated the receptive fields of the units in the Places-CNN and ImageNet-CNN.", "startOffset": 29, "endOffset": 33}, {"referenceID": 1, "context": "We compare the deep features of Places365VGG, Places205-AlexNet (result reported in [2]), and ImageNet-AlexNet, to those hand-designed features.", "startOffset": 84, "endOffset": 87}, {"referenceID": 11, "context": "Results of other hand-designed features/kernels are fetched from [13].", "startOffset": 65, "endOffset": 69}, {"referenceID": 32, "context": "We further synthesized preferred input images for the Places-CNN by using the image synthesis technique proposed in [34].", "startOffset": 116, "endOffset": 120}, {"referenceID": 31, "context": "See the detailed visualization methodology in [33].", "startOffset": 46, "endOffset": 50}], "year": 2016, "abstractText": "The rise of multi-million-item dataset initiatives has enabled data-hungry machine learning algorithms to reach nearhuman semantic classification at tasks such as object and scene recognition. Here we describe the Places Database, a repository of 10 million scene photographs, labeled with scene semantic categories and attributes, comprising a quasi-exhaustive list of the types of environments encountered in the world. Using state of the art Convolutional Neural Networks, we provide impressive baseline performances at scene classification. With its high-coverage and high-diversity of exemplars, the Places Database offers an ecosystem to guide future progress on currently intractable visual recognition problems.", "creator": "LaTeX with hyperref package"}}}