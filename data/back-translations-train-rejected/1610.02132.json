{"id": "1610.02132", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "QSGD: Communication-Optimal Stochastic Gradient Descent, with Applications to Training Neural Networks", "abstract": "Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks. A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates between nodes can be very large. Consequently, lossy compresion heuristics have been proposed, by which nodes only communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge, and it is not clear whether they are optimal.", "histories": [["v1", "Fri, 7 Oct 2016 03:44:34 GMT  (701kb,D)", "http://arxiv.org/abs/1610.02132v1", null], ["v2", "Mon, 10 Apr 2017 22:25:06 GMT  (1597kb,D)", "http://arxiv.org/abs/1610.02132v2", null], ["v3", "Thu, 25 May 2017 08:05:19 GMT  (1819kb,D)", "http://arxiv.org/abs/1610.02132v3", null]], "reviews": [], "SUBJECTS": "cs.LG cs.DS", "authors": ["dan alistarh", "demjan grubic", "jerry li", "ryota tomioka", "milan vojnovic"], "accepted": false, "id": "1610.02132"}, "pdf": {"name": "1610.02132.pdf", "metadata": {"source": "CRF", "title": "QSGD: Randomized Quantization for Communication-Optimal Stochastic Gradient Descent", "authors": ["Dan Alistarh", "Jerry Z. Li", "Ryota Tomioka", "Milan Vojnovic"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we propose Quantized SGD (QSGD), a family of compression schemes that allow the compression of gradient updates on each node while guaranteeing convergence under standard assumptions. QSGD allows the user to balance compression and convergence time: it can communicate a sublinear number of bits per iteration in the model dimension and achieve asymptotically optimal communication costs. We supplement our theoretical results with empirical data showing that QSGD can significantly reduce communication costs while competing with standard uncompressed techniques in a variety of real-world tasks."}, {"heading": "1 Introduction", "text": "The wave of massive data has led to significant interest in distributed algorithms for scaling calculations in the context of machine learning and optimization, based on the distribution of calculations across multiple CPU threads, GPUs, or machines in large computing clusters. However, much attention has been paid to scaling large-scale stochastic descent (SGD) algorithms that emerge in many applications, including computer vision, language processing, and various classification and regression tasks, such as product recommendations or click-rate predictions. In short, SGD can be defined as a sequence. Let f: Rn \u2192 R be a function we want to minimize."}, {"heading": "2 Preliminaries", "text": "We consider stochastic gradient descent (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD (SGD) (SGD) (SGD) (SGD) (SGD) (SGD (SGD) (SGD) (SGD) (SGD) (SGD (SGD) (SGD) (SGD) (SGD (SGD) (SGD) (SGD) (SGD) (SGD (SGD) (SGD) (SGD) (SGD) (SGD) (SGD (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD) (SGD"}, {"heading": "2.1 Parallel Stochastic Gradient Descent", "text": "We consider synchronous data-parallel SGD, motivated by modeling the real world multi-GPU systems = we only recover in the communication costs of SGD in this setting. We have a series of K-processors p1, p2,.., pK that proceed in synchronous steps and communicate with point-to-point messages. Each processor maintains a local copy of a vector x of dimension n, which represents the current value of the minimizer. Each processor also has access to many private, independent stochastic gradients for f. The algorithm proceeds in synchronous iterations, described in Algorithm 1. Specifically, each processor x aggregates the value of x, then receives random gradient updates for each component of x, then communicates these updates to all peers, and finally aggregates the received updates and applies them to its local model. It is important that we recover encodings from them and decode them, respectively, after 7 and 3 steps respectively, and that we only capture them in each case."}, {"heading": "3 Random Quantization SGD Schemes", "text": "In this section, we present our main results on random quantization schemes. Each scheme is a lossy compression encoding defined by a random quantization function applied to each stochastic gradient vector, followed by a lossless encoding scheme used to communicate a quantified stochastic gradient vector. We then present a strict generalization of this scheme with a tuning parameter that allows a smooth control of the number of information bits used to encode a stochastic gradient vector of dimension n."}, {"heading": "3.1 A Simple Random Quantization SGD Scheme", "text": "We define a random quantization function which is defined for each vector v Rn so that v Q = 0 Q = 0 Q = 0 Q = 0 Q s as random vector Q (v) s (v) s (v) s (v) s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s i s s s s s s i s s s s s s s i s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s s"}, {"heading": "3.2 A Generalized Random Quantization SGD Scheme", "text": "In this section, we will consider a more general, parameterizable compression scheme for Q vectors. As already mentioned, this random quantization function uses a strict generalization of what was previously represented, and this allows to trade the number of information bits used to encode a stochastic gradient vector at the convergence rate. This random quantization function is called Q (v, s), where s-1 is the tuning parameter, and is defined for each v-Rn so that v 6 = 0 asQi (v, s) = V-2 \u00b7 sgn (vi)."}, {"heading": "4 Experiments", "text": "We have predicted a simple extension of Theorem 3.3, that the second moment will then become an experiment aimed at data parallel and model parallel settings. We have only a limited number of 32 million people who are able to move, to be able to move in the real world, both in the real world and in the real world in which they are moving in the real world, as well as in the real world in which they are investigating the relationship between power and granularity, in which quantization is applied to the gradient vector. In particular, we apply quantization to buckets of successive vector components, using the basic random quantization (3). Setting d = 1 does not correspond to quantization (vanilla SGD), and d = n corresponds to the complete quantization that O (s n log n) sends, bits per biteration, we predict a simple extension to a 3.3."}, {"heading": "5 Conclusions and Future Work", "text": "We have introduced QSGD, a family of lossy compression techniques that provide a smooth trade-off between the amount of communication per iteration and runtime. QSGD can communicate sublinear bits per iteration and is optimal for convex optimization. Experimental results suggest that QSGD can be practical as it significantly reduces communication costs and competes with uncompressed standard techniques. In future work, we plan to explore optimized implementations for QSGD and its scale potential in large-scale applications. On the theoretical side, it is interesting to consider settings in which both gradients and the model are transferred in quantified form, as well as applications of randomized quantification beyond SGD."}, {"heading": "A A Compression Scheme for Qs Matching Theorem 3.5", "text": "In this section we describe a scheme for the coding of Qs and provide an upper limit for the expected number of information bits it will use, which is the limit in Theorem 3.5.We observe that for each vector v the output of Q (v, s) is, of course, by a tuple (v, s) as a function of R\\ {0} to Bs, whereBs = {A, \u03c3, z).R \u00b7 Rn \u00b7 Rn \u00b7 Rn \u00b7 Rn \u00b7 Rn \u00b7 Rn \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B \u00b7 B, B \u00b7 B \u00b7 B \u00b7 B, B \u00b7 B \u00b7 B \u00b7 B \u00b7 B, B \u00b7 B \u00b7 B, B \u00b7 B \u00b7 B, B \u00b7 B, B \u00b7 B \u00b7 B \u00b7 B, B \u00b7 B, B \u00b7 B, B \u00b7 B, B \u00b7 B \u00b7 B, B \u00b7 B, B \u00b7 B, B, B \u00b7 B, B \u00b7 B \u00b7 B, B \u00b7 B, B \u00b7 B, B, B \u00b7 B, B, B \u00b7 B \u00b7 B, B, B \u00b7 B, B \u00b7 B, B, B, B \u00b7 B, B, B \u00b7 B, B"}, {"heading": "B A Compression Scheme for Qs Matching Theorem 3.6", "text": "In the case of the quantified SGD scheme, which requires any coordination function, it is easy to see that we can improve the constant Q in bit length in theorem A.2 by using a different encoding of Q (v, s). This corresponds to the regime in which s = \u221a n, i.e., where the quantified update is not expected to be economical. In this case, there is no advantage in transferring the position of the next non-number, as this will simply be the next coordinate of the vector. Therefore, we can also easily transfer the value of each coordinate in sequence. Motivated by the above remark, we define the following alternative compression function. Define Elias \"(k) = Elias (k + 1) to be a compression function on all non-negative natural numbers. It is easy to see that this is uniquely decodable."}, {"heading": "C Quantization for Non-convex SGD", "text": "As already mentioned, our techniques are portable and can easily be applied to a variety of settings in which SGD is applied. As a demonstration of this, we show here how we can use quantification based on current results showing that SGD is convex to smooth, non-convex functions. However, in the course of this work, our theory only looks at the case of f being a convex function. However, in many interesting applications such as neural network formation, the target is not convex, where much less is known. However, there has been an interesting line of recent work showing that SGD at least always demonstrably converts to a local minima when f is converted. For example, by applying Theorem 2.1 in [9], we immediately obtain the following convergence result for quantized SGD. Let Qs be the quantization function defined in Section 3.2."}, {"heading": "D Quantized Gradient Descent: Description and Analysis", "text": "In this section, we will look at the effects of lossy compression on standard (non-stochastic) Q components. As this approach is not parallel to data, we must first modify the design of the iterative method as described in Algorithm 2. In particular, we assume that we will limit the communication costs of gradient updates. We will now provide a quantifying function for the gradient descendants, which demonstrate the convergence of gradient descendants with quantification, and then finally limit the length of the encoding. We will consider the following deterministic quantization function, inspired by [18]."}, {"heading": "E Quantized SVRG", "text": "D (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). (D). ("}, {"heading": "F Details of the experiment", "text": "CIFAR-10) for this task: x \u2192 Conv2D * (5, 64) \u2192 MaxPool \u2192 Dropout \u2192 Conv2D * (7, 128) \u2192 MaxPool \u2192 Dropout \u2192 Conv2D * (9, 128) \u2192 MaxPool \u2192 Dropout \u2192 Linear (4096) \u2192 ReLU \u2192 Dropout \u2192 Linear (4096) \u2192 ReLU \u2192 Dropout \u2192 Linear (10). Here, each Conv2D * (s, c) is realized as a composition of (s \u2212 1) / 2 3 x 3 2D folding layers with C filters followed by batch normalization and reflected linear activation, which can be expressed as x \u2192 Conv2D (3, c) \u2192 BN \u2192 RL \u2192 \u00b7 \u00b7 \u00b7 \u2192 Conv2D (3, c) \u2192 BN \u2192 RL (s \u2212 1) / 2 times"}], "references": [], "referenceMentions": [], "year": 2017, "abstractText": "<lb>Parallel implementations of stochastic gradient descent (SGD) have received significant research attention, thanks<lb>to excellent scalability properties of this algorithm, and to its efficiency in the context of training deep neural networks.<lb>A fundamental barrier for parallelizing large-scale SGD is the fact that the cost of communicating the gradient updates<lb>between nodes can be very large. Consequently, lossy compresion heuristics have been proposed, by which nodes only<lb>communicate quantized gradients. Although effective in practice, these heuristics do not always provably converge,<lb>and it is not clear whether they are optimal.<lb>In this paper, we propose Quantized SGD (QSGD), a family of compression schemes which allow the compression<lb>of gradient updates at each node, while guaranteeing convergence under standard assumptions. QSGD allows the user<lb>to trade off compression and convergence time: it can communicate a sublinear number of bits per iteration in the<lb>model dimension, and can achieve asymptotically optimal communication cost. We complement our theoretical results<lb>with empirical data, showing that QSGD can significantly reduce communication cost, while being competitive with<lb>standard uncompressed techniques on a variety of real tasks.", "creator": "LaTeX with hyperref package"}}}