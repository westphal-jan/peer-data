{"id": "1610.02164", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Deep Reinforcement Learning From Raw Pixels in Doom", "abstract": "Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTM-A3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.", "histories": [["v1", "Fri, 7 Oct 2016 07:07:47 GMT  (1352kb,D)", "http://arxiv.org/abs/1610.02164v1", "Bachelor's thesis"]], "COMMENTS": "Bachelor's thesis", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["danijar hafner"], "accepted": false, "id": "1610.02164"}, "pdf": {"name": "1610.02164.pdf", "metadata": {"source": "CRF", "title": "Deep Reinforcement Learning From Raw Pixels in Doom", "authors": ["Danijar Hafner", "Tobias Friedrich"], "emails": [], "sections": [{"heading": null, "text": "In this paper, we examine the challenges that arise in such complex environments and summarize current methods to approach them. We select a task within the Doom game that has not yet been tackled. The agent's goal is to fight enemies in a 3D world that consists of five rooms. To this end, we train the DQN and LSTMA3C algorithms. Results show that both algorithms learn sensible strategies, but do not achieve high scores given the amount of training. We provide insights into the learned behavior that can serve as a valuable starting point for further research in the Doom field."}, {"heading": "1 Introduction 1", "text": "1.1.................................................................................................................."}, {"heading": "2 Reinforcement Learning Background 4", "text": "2.1 Agent and Environment......................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "3 Challenges in Complex Environments 13", "text": "3.1 Large State Area......................................................................................................................................"}, {"heading": "4 Algorithms for Learning from Pixels 20", "text": "4.1 Deep Q-Network............................... 20 4.2 Asynchronous Advantages Actor Critics............ 21"}, {"heading": "5 Experiments in the Doom Domain 22", "text": "5.1 The Doom Domain....................................................................................................................."}, {"heading": "6 Conclusions 28", "text": "Bibliography 28iChapter 1IntroductionUnderstanding human thought and behavior is one of our greatest challenges. Scientists approach this problem from various disciplines, including psychology, philosophy, neuroscience, cognitive science, and computer science. The computer science community tends to model behavior farther away from the biological example. However, models are often evaluated on the basis of complex tasks and their effectiveness is demonstrated. Specifically, it is about enhanced learning (RL) and intelligent control, two communities within machine learning and, more generally, artificial intelligence, which focus on finding strategies to behave in an unknown environment. This general environment allows the application of methods to financial trading, advertising, robotics, power plant optimization, aircraft design, and much more [1, 30]. This thesis provides an overview of the current state of the art in RL and applies a selection of it to the doom video game, a current and challenging test bed we perform in Chapter 5. We explain the structure of this RL and the algorithm in Chapter 5."}, {"heading": "1.1 The Reinforcement Learning Setting", "text": "The RL setting defines an environment and an agent who interacts with it. The environment can be any problem we want to solve. For example, it could be a race track where a car is parked, an advertising network or the stock exchange. Each environment reveals information to the agent, such as a camera image from the perspective of the car, the profile of a user to whom we want to show ads, or the current stock prices. The agent uses this information to interact with the environment. In 1 of our examples, the agent could control the steering wheel and accelerator, select ads to display, or buy and sell shares. In addition, the agent receives a reward signal depending on the outcome of his actions. The problem with the RL is learning and selecting the best action sequences in an initially unknown environment. In the area of control theory, we also speak of a control problem, because the agent tries to control the environment with the actions available."}, {"heading": "1.2 Human-Like Artificial Intelligence", "text": "RL has been used to model the behavior of humans and artificial agents, assuming that humans are trying to optimize a reward signal, a signal that can be arbitrarily complex and could be learned over the course of their lives and evolution. For example, it is known that the neurotransmitter dopamine plays a critical role in motivation and is related to such a reward system in the human brain. However, modelling human behavior as an RL problem with a complex reward function is not entirely consensual. Although any behavior can be modeled as a result of a reward function, simpler underlying principles may exist than these. These principles may be more valuable to model human behavior and build intelligent agents. Furthermore, current RL algorithms can hardly be compared to human behavior."}, {"heading": "1.3 Relation to Supervised and Unsupervised", "text": "LearningSupervised Learning (SL) is the dominant framework in machine learning, driven by successes in areas such as computer vision and natural language processing, and the recent breakthrough of deep neural networks. In SL, we learn from labeled examples that we assume to be independent, with the goal of either classifying invisible examples or predicting a scalar property of them. Compared to SL, RL is more general in that it defines sequential problems. Although not always useful, we could model each SL problem as a single-level RL problem. Another link between the two frameworks is that many RL algorithms use SL internally for functional approximation (Sections 2.6.12 and 3.1). Unsupervised Learning (UL) is an orthogonal framework for RL in which examples are not deflected."}, {"heading": "1.4 Reinforcement Learning in Games", "text": "Traditional work often focuses on simple tasks, such as balancing a rod on a cart in 2D. However, we want to build agents that can cope with the additional difficulties that occur in complex environments (Chapter 3). Video games offer a convenient way to evaluate algorithms in complex environments, since their interface is clearly defined and many games define a score that we pass on as a reward signal to the agent. Board games are also frequently addressed with RL approaches, but are not considered in this work because their rules are known in advance. Most noteworthy is that the Atari environment provided by ALE [3] consists of 57 low-resolution 2D games. The agent can learn to play agent-agent-agent-agent by watching either screen pixels or the main memory used by the game agent-agent-agent-agent. 3D environments in which the agent-perspective pixel images are observed include the driving simulator Torcs [35], which we call Agent-agent-agent-agent-pixel or the player-player-player-block game we call Agent-agent-agent-player, and the Agent-player-player-player-player-player-player."}, {"heading": "2.1 Agent and Environment", "text": "Formally, we define the environment as a partially observable Markov decision-making process (POMDP) = a process of decision (MDP) consisting of a state space S, an action space A and an observation space X. Furthermore, we define a transition function T: S \u00b7 A \u2192 Dist (S), which we also refer to as dynamics, an observation function O: S \u2192 Dist (X) and a reward function R: S \u00b7 A \u2192 Dist (R), where Dist (D) refers to the space of the random variables overD. We refer to T ass \u2032 = Pr (s, a) = s \u2032). The initial state is s0 \u0445S and we model the terminal states implicitly by using a recursive transition probability of 1 and a reward of 0.We use O (s) to model that the actor may not be able to observe the entire state of the environment. If S = X and vice versa, we assume the observation ability."}, {"heading": "2.2 Value-Based Methods", "text": "RL methods can be roughly divided into value-oriented and policy-based (Section 2.7). RL theory often assumes fully observable environments, so we first assume that the agent has found a way to reconstruct st from the observations x0,... xt. Of course, this is not entirely possible depending on the O. We will discuss the challenge of partial observability later in Section 3.2.An essential concept of value-oriented methods is the value function V \u00b2 (st) = IE\u03c0 [Rt]. We use V \u00b2 to designate the value function under an optimal policy. The value function has an interesting property known as the Bellman equation: V \u03c0 (s) = IE\u03c0 [R (s, a) +."}, {"heading": "2.3 Policy Iteration", "text": "In each iteration k, we perform two steps: During the evaluation, we estimate Q\u03c0k based on observed interactions, e.g. using a Monte Carlo estimate. We call this estimate Q \u03c0k. During the evaluation, we update the policy to act greedily in relation to this estimate, and construct a new policy \u03c0k + 1 with \u03c0k + 1 (a, s) = {1, if a = arg maxa \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, 0, otherwise. We can break the ties in the Arg max arbitrarily. If Q \u00b2 p = Q\u03c0k, it is easy to see that this step is a monotonous improvement, because B \u00b2 K + 1 is a valid policy and that Q \u00b2 S, a \u00b2 A: max a \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, A \u00b2, A is a valid policy."}, {"heading": "2.4 Exploration versus Exploitation", "text": "The problem of visiting new states is mentioned in RL Exploration, as opposed to exploitation, which means being greedy with regard to our current estimate of the Q value. There is a fundamental trade-off between exploration and exploitation in RL. At any time, we could choose to either follow the policy we currently consider the best, or take measures that we believe are worse when we have the potential to discover better courses of action. In complex environments, exploration is one of the biggest challenges, and we discuss advanced approaches in Section 3.5. The prevailing approach to exploration is the simple EpsilonGreedy strategy, where the agent selects a random action with a probability consistent with normal policy."}, {"heading": "2.5 Temporal Difference Learning", "text": "While policy iteration, in combination with the Epsilongiedy Exploration Strategy, finds the optimal policy, the Monte Carlo estimates have a comparatively high deviation, so we often have to observe each Q state to get closer to the optimal policy. We can improve data efficiency by using the idea of bootstrapping, where we estimate the Q values from a single transition: Q-\u03c0 (st, at) = {rt + 1 if st is terminal, rt + 1 + \u03b3Q (st + 1, at + 1) otherwise. (2.3) The approximation is much smaller, but leads to a distortion because our initial approximate Q values may be arbitrarily wrong. In practice, bootstrapping is very common, as Monte Carlo estimates are not comprehensible. Equation 2.3 allows us to update the Q-\u03c0 estimate after each time step, rather than after each episode, which leads to the online SAR25 algorithm. [We use a small learning rate]"}, {"heading": "2.6 Eligibility Traces", "text": "One problem with time difference methods such as SARSA and Q-Learning is that updates to the approximate Q function only directly affect the Q values of the previous states. If there are long intervals between good measures and the corresponding rewards, many updates to the Q function may be required for the rewards to spread backwards to the good measures. The TD algorithm responds by assigning Eligibility Permissions to each Q status. When the agent receives Eligibility, we need to find out which state and which measures led to the reward so that we can make it more likely. The TD algorithm provides a response by assigning Eligibility Permissions to each Q status. Once we encounter Eligibility Permission, we apply the Time Difference Calculation for each Q state."}, {"heading": "2.6.1 Function Approximation", "text": "Gradient-based function approximation is widely used in the literature. By means of a derivative function approximator such as linear regression or neural networks-9works based on randomly initialized parameters \u03b80, we can perform the gradient-based update rule: \u03b8t + 1 = \u03b8t + \u03b1\u03b4t-\u03b8Q (st, at). (2.7) This is the scalar offset of the new estimate compared to the previous estimate, which is given by the time difference error of an algorithm such as SARSA or QLearning, and \u03b1 is a low learning rate. Background to the functional approximation using neural networks is outside the scope of this work."}, {"heading": "2.7 Policy-Based Methods", "text": "In contrast to value-based methods, policy-based methods parametrize policy directly. However, depending on the problem, it may be easier to find a good policy than to first approach the Q function. Using a parametrized functional imperator (Section 2.6.1), we aim to find a good set of parameters designed so that measures that are taken out of policy are quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quantitative quanti"}, {"heading": "2.8 Actor-Critic Methods", "text": "In the previous section, we have trained politics along the gradient of the expected reward, which is equivalent to IE\u03c0\u03b8 [Rt \u0445 \u03b8 ln\u03c0\u03b8 (a | s)]. If we scan transitions from the environment to estimate this expectation, the rewards can have a high variance. Thus, if amplification requires many transitions to obtain a sufficient estimate, we improve the data efficiency of this algorithm by subtracting a baseline B (s) from the reward, which reduces the variance of the expectation. If B (s) is an approximate function, we designate its critic and the political operative's approximator as actors. In order not to introduce a distortion of the gradient of the reward, the baseline gradient of the policy 0 [33] must be: 11IED (a | s) is an approximate function, which we call the unpredictable task. B (s) is its approximator of the political actor and critic."}, {"heading": "3.1 Large State Space", "text": "Doom is a 3D environment in which agents view perspective 2D projections as pixel matrices from their position in the world. Having such a large state space makes tabular versions of RL algorithms insoluble. We can adapt these algorithms to the use of function approximators and make them tractable. However, each time, the agent receives tens of thousands of pixels, which is a computational challenge even in the case of function approximation. Downsampling input images offer only a partial solution to this, as we need to preserve information necessary for effective control. [13] We want our agents to find small representations of their observations that are helpful for selecting actions. Therefore, abstraction of individual pixels is necessary. Convolutionary neural Networks (CNNs) provide a computationally effective way to learn such abstractions [13]. Compared to normal fully connected neural networks, each of the CNNs will not consist of multiple filters that can be shifted to one another, each of which is preceded by several evolutionary layers."}, {"heading": "3.2 Partial Observability", "text": "In complex environments, observations do not fully reveal the state of the environment. The perspective view that the agent observes in the doom environment contains reduced information in several ways: \u2022 The agent can only look in the forward direction and his field of vision covers only a fraction of the entire environment. \u2022 Obstacles such as walls hide the parts of the scene behind them. \u2022 The perspective projection loses information about the 3D geometry of the scene. The reconstruction of the 3D geometry and thus positions of objects is not trivial and might not even have a unique solution. \u2022 Many 3D environments include basic simulations of physics. While the agent can see the objects in his view, the pixels do not contain direct information about velocities. It might be possible to infer them. \u2022 Several other time factors are not represented in the current image, such as whether an object or enemy exists in another space. To learn good policy, the agent must recognize spatial and temporal correlations in his input."}, {"heading": "3.3 Stable Function Approximation", "text": "Various forms of neural networks have been successfully applied to monitored and unattended learning problems. In these applications, the data set is often known before training and can be decorative, and many machine learning algorithms expect independent and identically distributed data. This is problematic in the RL setting, where we want to improve policy while collecting observations sequentially. Observations can be highly correlated due to the sequential nature underlying this memory. To make data too decorative, the agent can use a replay memory, as introduced by Mnih et al. [13] to store previously occurring transitions. At any time, the agent then tries a random batch of transitions from that memory and uses it for training. To initialize the memory, one can execute a random policy prior to the training phase. Note that the transitions are still triggered by the start distribution of the MDP. The work mentioned first succeeded in playing several Atari-2i games."}, {"heading": "3.4 Sparse and Delayed Rewards", "text": "Sometimes we can help and guide the agent by rewarding all actions with positive or negative rewards. But, in many real-world tasks, the agent receives zero rewards most of the time and sees only binary feedback at the end of each episode. Rare feedback is common when creating a more computerized reward signal is not easy or when we do not want to prejudge the solutions the agent might find. In these environments, when the agent finally receives a reward, he must assign credits to all his previous actions. We can use Eligibility Traceability (Section 2.6) to bring about a functional approximation by keeping an eye on all transitions since the beginning of the episode. When the agent receives a reward, it includes updates for all stored states relative to the expired reward. Another way to address sparse and delayed rewards is to use methods of temporal abstraction, as explained in Section 3.5.416."}, {"heading": "3.5 Efficient Exploration", "text": "Algorithms such as Q-Learning (Section 2.5) are optimal in the tabular case, assuming that every state is visited at some point [29]. In complex environments, it is impossible to visit any of the many states. As we use functional approximation, the agent can already generalize between similar states. There are several paradigms for the problem of effectively finding interesting and unknown experiences that contribute to improving policy."}, {"heading": "3.5.1 Random Exploration", "text": "We can apply a simple EpsilonGreedy strategy for exploration, in which we select a completely random action at each step with a probability \u03b5 (0, 1] and otherwise act according to our policy. If we start at \u03b5 = 1 and decrease exponentially over time, this strategy guarantees that we visit each state over and over again. In simple environments, EpsilonGreedy may actually visit each state often enough to derive the optimal policy, but in complex 3D environments, we do not even visit each state once in a reasonable time. Visits to new states would be important to discover better strategies.One reason that random exploration still works reasonably well in complex environments [13, 14] can in part be attributed to functional approximation. If the functional approximation value is generalized over similar states, visiting a state also improves the assessment of similar states. However, more sophisticated methods exist and can lead to better results."}, {"heading": "3.5.2 Optimism in the Face of Uncertainty", "text": "A simple paradigm for promoting research into value-based algorithms (Section 2.2) is the optimistic initialization of estimated Q values. We can do this either by pre-training the functional approximation model or by adding a positive distortion of its results. Whenever the agent visits an unknown state, he reverses his Q value downwards. Less visited states have still assigned high values, so that the agent tends to visit them when faced with the choice. The Bayesian approach is to count the visits of each state to calculate the uncertainty of its appreciation [24]. Combined with Q-Learning, this corresponds to the true Q function, since there are enough random samples [11]. Unfortunately, it is difficult to obtain truly random samples for a general MDP because it is sequential. Another problem with the counting is that the state space can be large or continuous and we want to generalize between similar laws."}, {"heading": "3.5.3 Curiosity and Intrinsic Motivation", "text": "The first approximator, called the model, attempts to predict the next observation, thus approaching the transition function of the MDP. The second approximator, called the controller, executes control. Its goal is to maximize both the expected yields and to cause the greatest reduction in the model's predictive errors [19]. It therefore attempts to provide new observations on the model that are new but learnable, inspired by the way people are bored by known knowledge and knowledge that they do not understand [20]. The model controller architecture has been extended in several ways. Ngo et al. [15] combined it with planning to more effectively escape known areas of government space. Schmidhuber [21] recently proposed to divide neurons between the model and controller networks in a way that allows the controller to use the model arbitrarily for control."}, {"heading": "3.5.4 Temporal Abstraction", "text": "Most of the exploration methods mentioned (Section 3.5) determine the next exploration measure in each time step. Temporarily extended measures could be beneficial for both exploration and exploitation. [18] The most common framework for the temporal abstraction in the RL literature is that of Sutton et al. [27] The idea is to learn several low-level strategies that interact with the world. A high-level policy considers the same inputs but has options to choose as measures. If high-level policy opts for an option, the corresponding low-level policy is executed for a fixed or random number of time steps.While there are several ways of obtaining options, two newer approaches have been shown to work in complex environments."}, {"heading": "4.1 Deep Q-Network", "text": "The most common algorithm currently used for learning in high-dimensional state spaces is the Deep Q Network (DQN) algorithm proposed by Mnih et al. [13]. It is based on a traditional Q-Learning algorithm with function approximation and EpsilonGreedy exploration. In its original form, DQN does not use permission tracking. The algorithm uses a two-layer CNN followed by a linear, fully interconnected layer to approximate the Q function. Instead of taking both the state and the measures as input, it issues approximate Q values for all actions simultaneously, using only one state as input.To decorate transitions that the agent collects during the game, it uses a large replay memory. After each time step, we randomly select a series of transitions from the replay memory. We use the time difference Q-Learning rule to update the neural network."}, {"heading": "4.2 Asynchronous Advantage Actor Critic", "text": "A3C by Mnih et al. [14] is a method of the actor and critic that is much more memory efficient than DQN because it does not require replay memory. Instead, transitions are updated in parallel and asynchronously by training in multiple versions of the same environment. Entropy regulation (Section 3.5.2) is applied to promote exploration; each of the originally up to 16 threads manages a copy of the model and interacts with an instance of the environment. Each thread collects a few transitions before regaining authorization (Section 2.6) and calculating gradients according to the AAC algorithm (Section 2.8) based on its current copy of the model. It then applies this gradient to the common model and updates its copy to the current version of the common model. A version of A3C uses the network architecture as DQN, except for the use of a softmax activation function in the last model."}, {"heading": "5.1 The Doom Domain", "text": "The Doom domain [9] provides RL tasks simulated by the game mechanics of the first person shooter Doom. This 3D game features different types of enemies, items and weapons. A level editor can be used to create custom-made tasks. We use the DeathMatch task defined in the Gym Collection [5]. We first describe the Doom domain in general. In Doom, the agent observes picture frames that are perspective 2D projections of the world from the position of the agent. A frame also contains elements of the user interface at the bottom, including the amount of ammunition of the agent's selected weapon, the remaining health points of the agent and additional game-specific information. We do not extract this information explicitly. Each frame is presented as a tenth of the dimensions Screen width, screen height and color channel. We can select the width and height of the agent's attack, the remaining health points, additional game play information and specific information."}, {"heading": "5.2 Applied Preprocessing", "text": "To reduce the calculation requirements, we select the smallest available screen resolution of 160 x 120 pixels. We continue to reduce the observations to 80 x 60 pixels and calculate the average across the color channels to produce a grayscale image. We experiment with delta images, where we pass the difference between the current and the last observation to the agent. Both variants are shown in Figure 5.2.We continue to use history images as they were originally used by DQN in the Atari domain, and perform a random action during the first batch of an episode. [9] Namely, we collect 23multiple frames, stack them, and show them to the agent as an observation. We then repeat the agent's action choice in the next time steps while collecting a new batch of images. We perform a random action during the first batch of an episode. History frames have several advantages: They contain time information, allow for more efficient data processing, and cause the actions to be performed in multiple time steps."}, {"heading": "5.3 Methods to Stabilize Learning", "text": "Both DQN and LSTM-A3C are sensitive to the choice of hyperparameters [14, 23]. Since the training times are in the order of hours or days, it is not possible for most researchers to perform an excessive hyperparameter search. We can normalize observations and rewards, as described in the previous section, to make it more likely that hyperparameters can be transferred between tasks and domains. It was found that it is essential to set the gradients of the networks in both 24DQN and LSTM-A3C. Without this step, the approximators differ in the early stages of learning. Specifically, we set each element x of the gradient to x = max {\u2212 10, min {x, + 10}}. The exact threshold at which the training is clicked did not seem to have a major impact on training stability or outcomes. We lower the learning rate linear during the training to ensure convergence, as of M9, but not of Mnial [14]."}, {"heading": "5.4 Evaluation Methodology", "text": "DQN uses a repeat memory of the last 104 transitions and size 64 samples from them, following the decisions of Kempka et al. [9]. We annex \u03b5 from 1.0 to 0.1 over the course of 2 \u0445 106 time steps, which corresponds to one third of the training duration. We start both training and annealing \u03b5 after the first 104 observations, corresponding to the initialization of the repeat memory from transitions collected by a random policy. For LSTM-A3C, we use 16 learning threads that apply their accumulated gradients all 5 observations using common optimization statistics, an entropy regulating factor of \u03b2 = 0.01. In addition, we scale the loss of the critic by 0.5 [14]. Both algorithms work on images of history 6 at one time and use RMSProp with a decay parameter of 0.99 for optimizations [14]. The learning rate starts at \u03b1 = 2 \u00d7 10 \u2212 5 Kempoom, similar to the one for each of 104-5 observations [5]."}, {"heading": "5.5 Characteristics of Learned Policies", "text": "The results of DQN and LSTM-A3C are similar in many cases, suggesting that both algorithms detect common structures of the DeathMatch task. The agents did not show any particular changes in the reception of delta frames as input (Section 5.2). This may be because neural networks, given history frames, can easily learn to calculate differences between these frames when they are 25 useful to solve the task. Consequently, we conclude that delta frames in combination with history frames may not be useful."}, {"heading": "5.5.1 Fighting Against Enemies", "text": "As expected, agents develop a tendency to target enemies. However, since an agent must look directly at enemies in order to shoot them, he always faces an enemy in a step of time before he receives a positive reward. However, the targeting is inaccurate and agents tend to alternately look left and right past their enemies. It would be interesting to conduct experiments without stacking historical frames to understand whether the learned strategies are repeated by the action. In several cases, agents lose enemies from their field of vision by usually turning to other enemies. No trained agent was found to memorize such enemies after not seeing them anymore. As a result, the agents were often attacked from behind, causing the episode to end. Surprisingly, LSTM-A3C agents tend to attack only when they face an enemy, but sometimes miss the chance to do so. In contrast, QDN-A3C agents were found to shoot more frequently and more frequently than L3C-agents would be required to do."}, {"heading": "5.5.2 Navigation in the World", "text": "Most agents learn to avoid walls while facing them, which requires at least a limited amount of spatial awareness. In comparison, a random agent repeatedly runs into a wall and stays in that position until the end of the episodes. In some cases, the agent walks backwards against a wall at an angle, using this strategy to enter one of the rooms with the health-enhancing elements. This behavior could be attributed to the lack of a negative reward when the agent picks up the more powerful weapons in two of the adjacent rooms."}], "references": [{"title": "An application of reinforcement learning to aerobatic helicopter flight", "author": ["P. Abbeel", "A. Coates", "M. Quigley", "A.Y. Ng"], "venue": "Advances in neural information processing systems, 19:1", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2007}, {"title": "Deep reinforcement learning in a 3-d blockworld environment", "author": ["T. Barron", "M. Whitehead", "A. Yeung"], "venue": "Deep Reinforcement Learning: Frontiers and Challenges, IJCAI 2016", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "The arcade learning environment: An evaluation platform for general agents", "author": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Unifying count-based exploration and intrinsic motivation", "author": ["M.G. Bellemare", "S. Srinivasan", "G. Ostrovski", "T. Schaul", "D. Saxton", "R. Munos"], "venue": "arXiv preprint arXiv:1606.01868", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "and W", "author": ["G. Brockman", "V. Cheung", "L. Pettersson", "J. Schneider", "J. Schulman", "J. Tang"], "venue": "Zaremba. Openai gym", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "et al", "author": ["M.P. Deisenroth", "G. Neumann", "J. Peters"], "venue": "A survey on policy search for robotics. Foundations and Trends in Robotics, 2", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Neural turing machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka"], "venue": "arXiv preprint arXiv:1410.5401", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep recurrent q-learning for partially observable mdps", "author": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Vizdoom: A doom-based ai research platform for visual reinforcement learning", "author": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Ja\u015bkowski"], "venue": "CoRR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Hierarchical reinforcement learning using spatio-temporal abstractions and deep neural networks", "author": ["R. Krishnamurthy", "A.S. Lakshminarayanan", "P. Kumar", "B. Ravindran"], "venue": "arXiv preprint arXiv:1605.05359", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2016}, {"title": "A unifying framework for computational reinforcement learning theory", "author": ["L. Li"], "venue": "PhD thesis, Rutgers, The State University of New Jersey", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "A critical review of recurrent 29  neural networks for sequence learning", "author": ["Z.C. Lipton", "J. Berkowitz", "C. Elkan"], "venue": "arXiv preprint arXiv:1506.00019", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Playing atari with deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "5602", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2013}, {"title": "Asynchronous methods for deep reinforcement learning", "author": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T.P. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "Proceedings of the 33rd International Conference on Machine Learning", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2016}, {"title": "Confidence-based progress-driven self-generated goals for skill acquisition in developmental robots", "author": ["H. Ngo", "M. Luciw", "A. F\u00f6rster", "J. Schmidhuber"], "venue": "Frontiers in Psychology, 4:833", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Action-conditional video prediction using deep networks in atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "Advances in Neural Information Processing Systems, pages 2863\u20132871", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "Control of memory", "author": ["J. Oh", "V. Chockalingam", "S. Singh", "H. Lee"], "venue": "active perception, and action in minecraft. CoRR, abs/1605.09128", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A neural model of hierarchical reinforcement learning", "author": ["D. Rasmussen", "C. Eliasmith"], "venue": "Proceedings of the 36th Annual Conference of the Cognitive Science Society, pages 1252\u20131257. Cognitive Science Society Austin, TX", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "author": ["J. Schmidhuber"], "venue": "From animals to animats: proceedings of the first international conference on simulation of adaptive behavior. Citeseer", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1991}, {"title": "Formal theory of creativity", "author": ["J. Schmidhuber"], "venue": "fun, and intrinsic motivation (1990\u20132010). IEEE Transactions on Autonomous Mental Development, 2", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2010}, {"title": "On learning to think: Algorithmic information theory for novel combinations of reinforcement learning controllers and recurrent neural world models", "author": ["J. Schmidhuber"], "venue": "arXiv preprint arXiv:1511.09249", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Reinforcement learning and human behavior", "author": ["H. Shteingart", "Y. Loewenstein"], "venue": "Current opinion in neurobiology, 25:93\u201398", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2014}, {"title": "Parameter selection for the deep q-learning algorithm", "author": ["N. Sprague"], "venue": "Proceedings of the Multidisciplinary Conference on Reinforcement Learning and Decision Making ", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2015}, {"title": "A bayesian framework for reinforcement learning", "author": ["M. Strens"], "venue": "ICML, pages 943\u2013950", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2000}, {"title": "Reinforcement learning: An introduction", "author": ["R.S. Sutton", "A.G. Barto"], "venue": "volume 1. MIT press Cambridge", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1998}, {"title": "et al", "author": ["R.S. Sutton", "D.A. McAllester", "S.P. Singh", "Y. Mansour"], "venue": "Policy gradient methods for reinforcement learning with function approximation. In NIPS, volume 99, pages 1057\u20131063", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Between mdps and semi-mdps: A framework for temporal abstraction in reinforcement learning", "author": ["R.S. Sutton", "D. Precup", "S. Singh"], "venue": "Artificial intelligence, 112", "citeRegEx": "27", "shortCiteRegEx": null, "year": 1999}, {"title": "A deep hierarchical approach to lifelong learning in minecraft", "author": ["C. Tessler", "S. Givony", "T. Zahavy", "D.J. Mankowitz", "S. Mannor"], "venue": "arXiv preprint arXiv:1604.07255", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Asynchronous stochastic approximation and q-learning", "author": ["J.N. Tsitsiklis"], "venue": "Machine Learning, 16", "citeRegEx": "29", "shortCiteRegEx": null, "year": 1994}, {"title": "Online actor\u2013critic algorithm to solve the continuous-time infinite horizon optimal control problem", "author": ["K.G. Vamvoudakis", "F.L. Lewis"], "venue": "Automatica, 46", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2010}, {"title": "Q-learning", "author": ["C.J. Watkins", "P. Dayan"], "venue": "Machine learning, 8", "citeRegEx": "31", "shortCiteRegEx": null, "year": 1992}, {"title": "Memory networks", "author": ["J. Weston", "S. Chopra", "A. Bordes"], "venue": "arXiv preprint arXiv:1410.3916", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2014}, {"title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning", "author": ["R.J. Williams"], "venue": "Machine learning, 8", "citeRegEx": "33", "shortCiteRegEx": null, "year": 1992}, {"title": "Function optimization using connectionist reinforcement learning algorithms", "author": ["R.J. Williams", "J. Peng"], "venue": "Connection Science, 3", "citeRegEx": "34", "shortCiteRegEx": null, "year": 1991}, {"title": "TORCS", "author": ["B. Wymann", "C. Dimitrakakis", "A. Sumner", "E. Espi\u00e9", "C. Guionneau", "R. Coulom"], "venue": "the open racing car simulator, v1.3.5. http://www.torcs.org", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "European Conference on Computer Vision, pages 818\u2013833. Springer", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "This general setting allows methods to be applied to financial trading, advertising, robotics, power plant optimization, aircraft design, and more [1, 30].", "startOffset": 147, "endOffset": 154}, {"referenceID": 29, "context": "This general setting allows methods to be applied to financial trading, advertising, robotics, power plant optimization, aircraft design, and more [1, 30].", "startOffset": 147, "endOffset": 154}, {"referenceID": 21, "context": "For further details, please refer to Shteingart and Loewenstein [22].", "startOffset": 64, "endOffset": 68}, {"referenceID": 2, "context": "Most notably, the Atari environment provided by ALE [3] consists of 57 low-resolution 2D games.", "startOffset": 52, "endOffset": 55}, {"referenceID": 34, "context": "3D environments where the agent observes perspective pixel images include the driving simulator Torcs [35], several similar block-world games that we refer to as Minecraft domain, and the first-person shooter game Doom [9] (Section 5.", "startOffset": 102, "endOffset": 106}, {"referenceID": 8, "context": "3D environments where the agent observes perspective pixel images include the driving simulator Torcs [35], several similar block-world games that we refer to as Minecraft domain, and the first-person shooter game Doom [9] (Section 5.", "startOffset": 219, "endOffset": 222}, {"referenceID": 24, "context": "In the case of an estimation error, the update may not be a monotonic improvement, but the algorithm is known to converge to Q\u2217 as the number of visits of each Q-state approaches infinity [25].", "startOffset": 188, "endOffset": 192}, {"referenceID": 0, "context": "The dominant approach to exploration is the straightforward EpsilonGreedy strategy, where the agent picks a random action with probability \u03b5 \u2208 [0, 1], and the action according to its normal policy otherwise.", "startOffset": 143, "endOffset": 149}, {"referenceID": 24, "context": "3 allows us to update the estimate Q\u0302 after each time step rather than after each episode, resulting in the online-algorithm SARSA [25].", "startOffset": 131, "endOffset": 135}, {"referenceID": 30, "context": "A common modification to this is known as Q-Learning, as proposed by Watkins and Dayan [31].", "startOffset": 87, "endOffset": 91}, {"referenceID": 24, "context": "The Q-Learning algorithm might be one of the more important breakthroughs in RL [25].", "startOffset": 80, "endOffset": 84}, {"referenceID": 28, "context": "Q-Learning converges to Q\u2217 given continued exploration [29].", "startOffset": 55, "endOffset": 59}, {"referenceID": 24, "context": "1: Accumulating, Dutch, and replacing eligibility traces (Sutton and Barto [25]).", "startOffset": 75, "endOffset": 79}, {"referenceID": 0, "context": "The state in which we receive the reward has an eligibility of 1 and the eligibility of previous states decays exponentially over time by a factor \u03bb \u2208 [0, 1]: et(st\u2212k) = (\u03b3\u03bb) .", "startOffset": 151, "endOffset": 157}, {"referenceID": 5, "context": "Several methods for searching the space of possible policy parameters have been explored, including random search, evolutionary search , and gradientbased search [6].", "startOffset": 162, "endOffset": 165}, {"referenceID": 25, "context": "gradient estimator [26]:", "startOffset": 19, "endOffset": 23}, {"referenceID": 32, "context": "This yields the Reinforce algorithm proposed by Williams [33].", "startOffset": 57, "endOffset": 61}, {"referenceID": 32, "context": "To not introduce bias to the gradient of the reward, the gradient of the baseline with respect to the policy must be 0 [33]:", "startOffset": 119, "endOffset": 123}, {"referenceID": 12, "context": "Convolutional neural networks (CNNs) provide a computationally effective way to learn such abstractions [13].", "startOffset": 104, "endOffset": 108}, {"referenceID": 15, "context": "[16])", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Zeiler and Fergus [36] visualize the layers of CNNs and show that they actually learn more abstract features in each layer.", "startOffset": 18, "endOffset": 22}, {"referenceID": 12, "context": "The agent then decides for an action we repeat while collecting the next stack of inputs [13].", "startOffset": 89, "endOffset": 93}, {"referenceID": 7, "context": "It is also common to use recurrent neural networks (RNNs) [8, 14] to address the problem of partial observability.", "startOffset": 58, "endOffset": 65}, {"referenceID": 13, "context": "It is also common to use recurrent neural networks (RNNs) [8, 14] to address the problem of partial observability.", "startOffset": 58, "endOffset": 65}, {"referenceID": 11, "context": "In particular, a variant called Long Short-Term Memory (LSTM) and its variations like Gated Recurrent Unit (GRU) have proven to be effective in a wide range of sequential problems [12].", "startOffset": 180, "endOffset": 184}, {"referenceID": 15, "context": "[16] were able to learn useful representations from videos, allowing them to predict up to 100 observations in the Atari domain (Figure 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 31, "context": "A recent advancement was applying memory network architectures [32, 7] to RL problems in the Minecraft environment [17].", "startOffset": 63, "endOffset": 70}, {"referenceID": 6, "context": "A recent advancement was applying memory network architectures [32, 7] to RL problems in the Minecraft environment [17].", "startOffset": 63, "endOffset": 70}, {"referenceID": 16, "context": "A recent advancement was applying memory network architectures [32, 7] to RL problems in the Minecraft environment [17].", "startOffset": 115, "endOffset": 119}, {"referenceID": 12, "context": "[13] to store previously encountered transitions.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The mentioned work first managed to learn to play several 2D Atari games [3] without the use of hand-crafted features.", "startOffset": 73, "endOffset": 76}, {"referenceID": 1, "context": "It has also been applied to simple tasks in the Minecraft [2] and Doom [9] domains.", "startOffset": 58, "endOffset": 61}, {"referenceID": 8, "context": "It has also been applied to simple tasks in the Minecraft [2] and Doom [9] domains.", "startOffset": 71, "endOffset": 74}, {"referenceID": 12, "context": "[13] used the idea of a target network.", "startOffset": 0, "endOffset": 4}, {"referenceID": 13, "context": "[14] proposed an alternative to using replay memories that involves multiple versions of the agent simultaneously interacting with copies of the environment.", "startOffset": 0, "endOffset": 4}, {"referenceID": 28, "context": "5) are optimal in the tabular case under the assumption that each state will be visited over and over again, eventually [29].", "startOffset": 120, "endOffset": 124}, {"referenceID": 12, "context": "One reason that random exploration still works reasonably well in complex environments [13, 14] can partly be attributed to function approximation.", "startOffset": 87, "endOffset": 95}, {"referenceID": 13, "context": "One reason that random exploration still works reasonably well in complex environments [13, 14] can partly be attributed to function approximation.", "startOffset": 87, "endOffset": 95}, {"referenceID": 23, "context": "The Bayesian approach is to count the visits of each state to compute the uncertainty of its value estimate [24].", "startOffset": 108, "endOffset": 112}, {"referenceID": 10, "context": "Combined with Q-learning, this converges to the true Q-function, given enough random samples [11].", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "[4] recently suggested a sequential density estimation model to derive pseudo-counts for each state in a non-tabular setting.", "startOffset": 0, "endOffset": 3}, {"referenceID": 33, "context": "While we usually do not estimate the Q-function here, we can add the entropy of the policy as a regularization term to the its gradient [34] with a small factor \u03b2 \u2208 R specifying the amount of regularization:", "startOffset": 136, "endOffset": 140}, {"referenceID": 13, "context": "[14].", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "Its objective is both to maximize expected returns and to cause the highest reduction in prediction error of the model [19].", "startOffset": 119, "endOffset": 123}, {"referenceID": 19, "context": "It therefore tries to provide new observations to the model that are novel but learnable, inspired by the way humans are bored by both known knowledge and knowledge they cannot understand [20].", "startOffset": 188, "endOffset": 192}, {"referenceID": 14, "context": "[15] combined it with planning to escape known areas of the state space more effectively.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Schmidhuber [21] recently proposed shared neurons between the model and controller networks in a way that allows the controller to arbitrarily exploit the model for control.", "startOffset": 12, "endOffset": 16}, {"referenceID": 17, "context": "Temporally extended actions could be beneficial to both exploration and exploitation [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 26, "context": "[27].", "startOffset": 0, "endOffset": 4}, {"referenceID": 9, "context": "[10] used spectral clustering to group states with cluster centers representing options.", "startOffset": 0, "endOffset": 4}, {"referenceID": 27, "context": "[28] trained multiple CNNs on simple tasks in the Minecraft domain.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "Rasmussen and Eliasmith [18] propose one such architecture and show that it is able to learn simple visual tasks.", "startOffset": 24, "endOffset": 28}, {"referenceID": 12, "context": "[13].", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "[9] who introduced this domain.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2] applied DQN to two tasks in the Minecraft domain: Collecting as many blocks of a certain color as possible, and navigating forward on a pathway without falling.", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "[14] is an actor-critic method that is considerable more memory-efficient than DQN, because it does not require the use of a replay memory.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The Doom domain [9] provides RL tasks simulated by the game mechanics of the first-person shooter Doom.", "startOffset": 16, "endOffset": 19}, {"referenceID": 4, "context": "We use the DeathMatch task defined in the Gym collection [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Finally, we apply normalization: We scale the observed pixels into the range [0, 1] and normalize rewards to (\u22121, 0,+1) using rt \u2190 sgn(rt) [13].", "startOffset": 77, "endOffset": 83}, {"referenceID": 12, "context": "Finally, we apply normalization: We scale the observed pixels into the range [0, 1] and normalize rewards to (\u22121, 0,+1) using rt \u2190 sgn(rt) [13].", "startOffset": 139, "endOffset": 143}, {"referenceID": 13, "context": "Both DQN and LSTM-A3C are sensitive to the choice of hyper parameters [14, 23].", "startOffset": 70, "endOffset": 78}, {"referenceID": 22, "context": "Both DQN and LSTM-A3C are sensitive to the choice of hyper parameters [14, 23].", "startOffset": 70, "endOffset": 78}, {"referenceID": 12, "context": "[13, 14], but not by Kempka et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 13, "context": "[13, 14], but not by Kempka et al.", "startOffset": 0, "endOffset": 8}, {"referenceID": 8, "context": "[9], Barron et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 1, "context": "[2].", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9].", "startOffset": 0, "endOffset": 3}, {"referenceID": 13, "context": "5 [14].", "startOffset": 2, "endOffset": 6}, {"referenceID": 13, "context": "99 for optimization [14].", "startOffset": 20, "endOffset": 24}, {"referenceID": 8, "context": "[9] who use a fixed learning rate of 10\u22125 in the Doom domain.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "Using current reinforcement learning methods, it has recently become possible to learn to play unknown 3D games from raw pixels. In this work, we study the challenges that arise in such complex environments, and summarize current methods to approach these. We choose a task within the Doom game, that has not been approached yet. The goal for the agent is to fight enemies in a 3D world consisting of five rooms. We train the DQN and LSTMA3C algorithms on this task. Results show that both algorithms learn sensible policies, but fail to achieve high scores given the amount of training. We provide insights into the learned behavior, which can serve as a valuable starting point for further research in the Doom domain.", "creator": "LaTeX with hyperref package"}}}