{"id": "1610.02273", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "6-Oct-2016", "title": "Near-Data Processing for Differentiable Machine Learning Models", "abstract": "In computer architecture, near-data processing (NDP) refers to augmenting the memory or the storage with processing power so that it can process the data stored therein, passing only the processed data upwards in the memory hierarchy. By offloading the computational burden of CPU and saving the need for transferring raw data, NDP has a great potential in terms of accelerating computation and reducing power consumption. Despite its potential, NDP had only limited success until recently, mainly due to the performance mismatch in logic and memory process technologies. Recently, there have been two major changes in the game, making NDP more appealing than ever. The first is the success of deep learning, which often requires frequent transfers of big data for training. The second is the advent of NAND flash-based solid-state drives (SSDs) containing multicore CPUs that can be used for data processing. In this paper, we evaluate the potential of NDP for machine learning using a new SSD platform that allows us to simulate in-storage processing (ISP) of machine learning workloads. Although our platform named ISPML can execute various algorithms, this paper focuses on the stochastic gradient decent (SGD) algorithm, which is the de facto standard method for training deep neural networks. We implement and compare three variants of SGD (synchronous, downpour, and elastic averaging) using the ISP-ML platform, in which we exploit the multiple NAND channels for implementing parallel SGD. In addition, we compare the performance of ISP optimization and that of conventional in-host processing optimization. To the best of our knowledge, this is one of the first attempts to apply NDP to the optimization for machine learning.", "histories": [["v1", "Thu, 6 Oct 2016 05:28:33 GMT  (8050kb,D)", "https://arxiv.org/abs/1610.02273v1", "9 pages, 7 figures"], ["v2", "Wed, 9 Nov 2016 01:58:47 GMT  (3696kb,D)", "http://arxiv.org/abs/1610.02273v2", "12 pages, 7 figures"], ["v3", "Fri, 28 Apr 2017 03:57:56 GMT  (3603kb,D)", "http://arxiv.org/abs/1610.02273v3", "12 pages, 7 figures"]], "COMMENTS": "9 pages, 7 figures", "reviews": [], "SUBJECTS": "cs.AR cs.DC cs.LG", "authors": ["hyeokjun choe", "seil lee", "hyunha nam", "seongsik park", "seijoon kim", "eui-young chung", "sungroh yoon"], "accepted": false, "id": "1610.02273"}, "pdf": {"name": "1610.02273.pdf", "metadata": {"source": "CRF", "title": "Near-Data Processing for Differentiable Machine Learning Models", "authors": ["Hyeokjun Choe", "Seil Lee", "Hyunha Nam", "Seongsik Park", "Seijoon Kim", "Eui-Young Chung", "Sungroh Yoon"], "emails": ["sryoon@snu.ac.kr"], "sections": [{"heading": null, "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "II. BACKGROUND", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Machine Learning as an Optimization Problem", "text": "Machine learning is a branch of artificial intelligence that aims to give computers the ability to learn without being explicitly programmed. (23) Depending on the type of feedback available for training, we can roughly classify machine learning tasks into three categories [24]: supervised learning, unsupervised learning and reinforcement of learning. (27) To facilitate further explanations, we can briefly review the basic formulation of supervised learning, focusing only on the materials that are directly relevant to current work. (27) For different types of machine learning algorithms, the core concept can often be explained by the following equations: F (D, \u03b8) = L (D, \u03b8) + r (1). (D)"}, {"heading": "B. Parallel and Distributed SGD", "text": "SGD is widely used in machine learning because of its simplicity and effectiveness [27]. However, the execution time of SGD increases rapidly with increasing data size. Consequently, various approaches have been proposed to accelerate SGD-based training by parallelization and / or distributed computation. [19] Zinkevich et al. [28] proposed an algorithm that implements parallel SGD in a distributed computing architecture. However, this algorithm often suffers from excessive latency caused by the need to synchronize all slave nodes. To overcome this weakness, Recht et al. [28] proposed the lock-free Hogwild! algorithm, which can update parameters asynchronously. Hogwild! is normally implemented on a single machine with a multicore processor. Dean et al. [20] suggested the Downpour SGD for distributed computing systems by extending the Hogwild algorithm!"}, {"heading": "C. Fundamentals of Solid-State Drives (SSDs)", "text": "SSDs have evolved into a type of next-generation storage medium that uses NAND flash memory [17]. SSDs have several advantages over hard drives (such as power consumption [29], IO performance [30], and mechanical properties).As shown in the right image in Figure 1 (a), a typical SSD consists of an SSD controller, a DRAM buffer, and a NAND flash array. The SSD controller typically consists of an embedded processor, a cache controller, channel controllers, and a host interface logic. The embedded processor used within an SSD controller typically consists of 2-4 cores with a clock frequency of 300-550 MHz. The DRAM component controlled by the cache controller plays the role of a cache buffer when the NAND flash array unit is read or written."}, {"heading": "D. Previous Work on Near-Data Processing", "text": "In many large data analytics systems, it is critical to minimize data flows, not only to avoid general performance deterioration, but also to improve the efficiency and reliability of data. The paradigm of data processing in various areas is moving quickly from a computer-centric to a data-centric system. Inspired by these trends, the concept of NDP [3] has recently attracted considerable interest. In NDP, the calculation is performed at the most appropriate location (other than the CPU / GPU), and the calculation could be performed in the memory where the entered data is stored. We can divide existing NDP approaches into two main categories, namely PIM and ISP.PIM aims to perform computing within the main memory."}, {"heading": "E. Our Approach", "text": "Of the two NDP categories (PIM and ISP), the approach described here belongs to the ISP category. Compared to the existing ISP approaches, our method is unique in that it supports gradient-based training for differentiated models. To the best of the authors \"knowledge, no ISP-based optimization of machine learning algorithms has ever been implemented and evaluated using a gradient descend algorithm such as SGD. In addition, our methodology supports parallel SGD, which is critical for enabling large-scale training on massive data.In addition to its widespread application, SGD has some appealing features that facilitate hardware implementations.We can implement parallel SGD via the master-slave architecture of the cache controller and the channel controller. In the next section, we can also use effective techniques that were originally developed for distributed and parallel computation.It is important that any SGD Iterware cannot be excessively complex and overcomplementary."}, {"heading": "III. PROPOSED METHODOLOGY", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "B. Parallel SGD Implementation on ISP-ML", "text": "In this section, we focus on describing the details of our ISP-based implementation and customizing these SGD algorithms and hiding the purely algorithmic details of each method; we refer to the corresponding references. In this section, we focus on describing the details of our ISP-based implementation and customizing these SGD algorithms and hiding the purely algorithmic details of each method; we refer interested readers to the corresponding references. Our ISP-ML platform performs the parallel SGD-based implementation in a master slave manner in which the cache controller and the slaves are the channel controllers. Algorithms 1-3 (shown in Figure 2) describe the operations performed by each channel controller (the boxed operations processed in each algorithm)."}, {"heading": "C. Methodology for IHP-ISP Performance Comparison", "text": "In order to evaluate the effectiveness of ISP, it is crucial to compare the performance of ISP and conventional IHP. However, performing this type of comparison is not trivial (see sections IV-C and V-B for additional discussion on this topic).In addition, we propose a practical methodology to accurately compare IHP-ML performance, which is illustrated in Figure 3. Note: This comparison methodology does not only apply to parallel SGD implementations. SGD1: Read page-sized data from NAND array."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Setup and Implementation", "text": "In fact, most of them will be able to play by the rules that they have established in recent years, and they will be able to play by the rules that they have set for their goals."}, {"heading": "B. Performance Comparison: ISP-Based Optimization", "text": "To determine which SGD algorithm would be most suitable for use in the ISP, we implemented and analyzed three types of SGD algorithms: synchronous SGD, downpour SGD, and EASGD. For downpour SGD and EASGD, we set the communication period (\u03c4) to 1. The moving rate (\u03b1) for EASGD was 0.001. To make a fair comparison, we chose different learning rates for different algorithms that yielded the best performance for each algorithm. Figure 4 shows the test accuracy of three algorithms with different number of channels (4, 8, and 16) in terms of wall clock time. As shown in Figure 4, EASGD achieved the best convergence speed in all the cases tested. On average, EASGD performed better than synchronous and downpour SGD by factors of 2.96 and 1.41, respectively. In the case of 4 and 8 channels, synchronous downpour GD results can be achieved as a low convergence speed for ISP."}, {"heading": "C. Performance comparison: IHP versus ISP", "text": "In this context, we conducted additional experiments to compare the performance of IHP-based and ISP-based EASGD. We tested the effectiveness of ISP in a storage space situation with 5 different configurations of IHP memory: 2GB, 4GB, 8GB, 16GB and 32GB. We assumed that the host had already loaded all the data into the main memory for IHP. This assumption is realistic, since modern machine learning techniques often employ a prefetch strategy to disguise the initial data transfer latency. As shown in Figure 5, ISP-based EASGD has achieved the best performance in our experiments with 16 channels. Convergence speed of IHP-based optimization slowed down in accordance with the reduced memory size."}, {"heading": "D. Channel Parallelism", "text": "To further investigate the effects of data-level parallelism on performance, we compared the accuracy of the three SGD algorithms in variing the number of channels (4, 8, and 16) as shown in Figure 6. Convergence speed of all three algorithms improved by using more channels; synchronous SGD improved 1.93 times as the number of channels increased from 8 to 16. As shown in Figure 6 (d), the improvement in convergence speed tends to be proportional to the number of channels. These results suggest that the communication effort required by the ISP is negligible and that the ISP does not suffer from the communication bottleneck that normally occurs in distributed computer systems."}, {"heading": "E. Effects of Communication Period in Asynchronous SGD", "text": "Finally, we investigated how changes in the communication period (i.e. how often data exchange occurs during distributed optimization) affect SGD performance in the ISP environment. Figure 7 shows the test accuracy of the downpour SGD and EASGD algorithms compared to the wall clock time when we varied their communication periods. As described in [21], downpour SGD normally achieved high performance for a low communication period [\u03c4 = 1, 4] and became unstable for a high communication period in the ISP. Interestingly, unlike conventional settings for distributed computer systems, the performance of EASGD decreased with increasing communication periods in the ISP setting. This result occurs because the communication overhead in the ISP chip is significantly lower than in the distributed computer system. Consequently, there is no need to reduce the communication periods in the ISP environment."}, {"heading": "V. DISCUSSION", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Parallelism in ISP", "text": "Given advances in underlying hardware and semiconductor technology, ISP can offer several advantages for the type of data processing in the field of machine learning. For example, our ISP-ML platform could minimize the portability of data within an SSD (e.g. energy efficiency, security and reliability).By leveraging the benefits of fast communication within an SSD, we will be able to develop a new way of data processing."}, {"heading": "C. Opportunities for Future Research", "text": "In this paper, we focused on implementing and testing ISP-based SGD as proof of concept. The simplicity and popularity of (parallel) SGD is at the heart of our choice. By design, it is possible to execute other algorithms in our ISP-ML framework immediately. To unleash the full power of ISP, we need additional ISP-specific optimization efforts, which are typically executed in the form of hardware designs."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors would like to thank Byunghan Lee and other members of the Data Science Laboratory of Seoul National University and the Design Technology Laboratory of Yonsei University for constructive discussions supported by the MSIP / IITP ICT-F Program (No.R711716-0235), the National Research Foundation of Korea (NRF), funded by the Ministry of Science, ICT & Future Planning (2014M3C9A3063541, 2016M3A7B4911115) and the Brain Korea 21 Plus Project in 2017."}], "references": [{"title": "Deep learning", "author": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "venue": "Nature, vol. 521, no. 7553, pp. 436\u2013444, 2015.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep learning in bioinformatics", "author": ["S. Min", "B. Lee", "S. Yoon"], "venue": "Briefings in Bioinformatics, p. bbw068, 2016.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Near-data processing: Insights from a MICRO-46 workshop", "author": ["R. Balasubramonian", "J. Chang", "T. Manning", "J.H. Moreno", "R. Murphy", "R. Nair", "S. Swanson"], "venue": "Micro, IEEE, vol. 34, no. 4, pp. 36\u201342, 2014.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Processing in memory: The Terasys massively parallel PIM array", "author": ["M. Gokhale", "B. Holmes", "K. Iobst"], "venue": "Computer, vol. 28, no. 4, pp. 23\u201331, 1995.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1995}, {"title": "Exploring specialized near-memory processing for data intensive operations", "author": ["S.F. Yitbarek", "T. Yang", "R. Das", "T. Austin"], "venue": "Design, Automation & Test in Europe Conference & Exhibition (DATE), 2016. IEEE, 2016, pp. 1449\u20131452.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Design and Evaluation of a Processing-in-Memory Architecture for the Smart Memory Cube", "author": ["E. Azarkhish", "D. Rossi", "I. Loi", "L. Benini"], "venue": "International Conference on Architecture of Computing Systems. Springer, 2016, pp. 19\u201331.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2016}, {"title": "Scaling deep learning on multiple in-memory processors", "author": ["L. Xu", "D.P. Zhang", "N. Jayasena"], "venue": "WoNDP: 3rd Workshop on Near- Data Processing In conjunction with MICRO-48, 2015.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Neurostream: Scalable and energy efficient deep learning with smart memory cubes", "author": ["E. Azarkhish", "D. Rossi", "I. Loi", "L. Benini"], "venue": "arXiv preprint arXiv:1701.06420, 2017.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2017}, {"title": "PRIME: A novel processing-in-memory architecture for neural network computation in reram-based main memory", "author": ["P. Chi", "S. Li", "C. Xu", "T. Zhang", "J. Zhao", "Y. Liu", "Y. Wang", "Y. Xie"], "venue": "Proceedings of the 43rd International Symposium on Computer Architecture. IEEE Press, 2016, pp. 27\u201339.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2016}, {"title": "Active disks: Programming model, algorithms and evaluation", "author": ["A. Acharya", "M. Uysal", "J. Saltz"], "venue": "ACM SIGOPS Operating Systems Review, vol. 32, no. 5. ACM, 1998, pp. 81\u201391.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 1998}, {"title": "In-storage processing of database scans and joins", "author": ["S. Kim", "H. Oh", "C. Park", "S. Cho", "S.-W. Lee", "B. Moon"], "venue": "Information Sciences, vol. 327, pp. 183\u2013200, 2016.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2016}, {"title": "ActiveSort: Efficient external sorting using active SSDs in the MapReduce framework", "author": ["Y.-S. Lee", "L.C. Quero", "S.-H. Kim", "J.-S. Kim", "S. Maeng"], "venue": "Future Generation Computer Systems, 2016.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2016}, {"title": "Energy efficient scale-in clusters with instorage processing for big-data analytics", "author": ["I.S. Choi", "Y.-S. Kee"], "venue": "Proceedings of the 2015 International Symposium on Memory Systems. ACM, 2015, pp. 265\u2013 273.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "ImageNet Large Scale Visual Recognition Challenge", "author": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "venue": "International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211\u2013252, 2015.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2015}, {"title": "A space-efficient flash translation layer for CompactFlash systems", "author": ["J. Kim", "J.M. Kim", "S.H. Noh", "S.L. Min", "Y. Cho"], "venue": "IEEE Transactions on Consumer Electronics, vol. 48, no. 2, pp. 366\u2013375, 2002.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2002}, {"title": "DFTL: a flash translation layer employing demand-based selective caching of page-level address mappings", "author": ["A. Gupta", "Y. Kim", "B. Urgaonkar"], "venue": "vol. 44,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2009}, {"title": "Architecture exploration of high-performance PCs with a solid-state disk", "author": ["D. Kim", "K. Bang", "S.-H. Ha", "S. Yoon", "E.-Y. Chung"], "venue": "IEEE Transactions on Computers, vol. 59, no. 7, pp. 878\u2013890, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "An effective pre-store/pre-load method exploiting intra-request idle time of NAND flash-based storage devices", "author": ["J.-Y. Kim", "T.-H. You", "S.-H. Park", "H. Seo", "S. Yoon", "E.-Y. Chung"], "venue": "under review, 2016.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2016}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "L. Li", "A.J. Smola"], "venue": "Advances in neural information processing systems, 2010, pp. 2595\u20132603.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2010}, {"title": "Large scale distributed deep networks", "author": ["J. Dean", "G. Corrado", "R. Monga", "K. Chen", "M. Devin", "M. Mao", "A. Senior", "P. Tucker", "K. Yang", "Q.V. Le"], "venue": "Advances in Neural Information Processing Systems, 2012, pp. 1223\u20131231.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep learning with elastic averaging SGD", "author": ["S. Zhang", "A.E. Choromanska", "Y. LeCun"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 685\u2013693.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "The MNIST database of handwritten digits", "author": ["Y. LeCun", "C. Cortes", "C.J. Burges"], "venue": "1998.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 1998}, {"title": "Some studies in machine learning using the game of checkers", "author": ["A.L. Samuel"], "venue": "IBM Journal of research and development, vol. 3, no. 3, pp. 210\u2013229, 1959.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1959}, {"title": "A modern approach", "author": ["S. Russell", "P. Norvig", "A. Intelligence"], "venue": "Artificial Intelligence. Prentice-Hall, Egnlewood Cliffs, vol. 25, p. 27, 1995.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1995}, {"title": "Pattern recognition and machine learning", "author": ["C.M. Bishop"], "venue": "2006.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2006}, {"title": "Machine learning: a probabilistic perspective", "author": ["K.P. Murphy"], "venue": "MIT press,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2012}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["B. Recht", "C. Re", "S. Wright", "F. Niu"], "venue": "Advances in Neural Information Processing Systems, 2011, pp. 693\u2013701.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2011}, {"title": "SSD Characterization: From Energy Consumption\u2019s Perspective", "author": ["B. Yoo", "Y. Won", "S. Cho", "S. Kang", "J. Choi", "S. Yoon"], "venue": "HotStorage, 2011.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2011}, {"title": "Io workload characterization revisited: A data-mining approach", "author": ["B. Seo", "S. Kang", "J. Choi", "J. Cha", "Y. Won", "S. Yoon"], "venue": "IEEE Transactions on Computers, vol. 63, no. 12, pp. 3026\u20133038, 2014.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Deduplication in SSD for reducing write amplification factor", "author": ["J. Kim", "I. Son", "J. Choi", "S. Yoon", "S. Kang", "Y. Won", "J. Cha"], "venue": "Proceedings of the 9th USENIX Conference on File and Storage Technologies (FAST\u201911), 2011.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey of flash translation layer", "author": ["T.-S. Chung", "D.-J. Park", "S. Park", "D.-H. Lee", "S.-W. Lee", "H.-J. Song"], "venue": "Journal of Systems Architecture, vol. 55, no. 5, pp. 332\u2013343, 2009.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2009}, {"title": "Hybrid memory cube (HMC)", "author": ["J.T. Pawlowski"], "venue": "Hot Chips 23 Symposium (HCS), 2011 IEEE. IEEE, 2011, pp. 1\u201324.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2011}, {"title": "Hybrid memory cube specification 1.0", "author": ["H. Consortium"], "venue": "2013.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2013}, {"title": "Bluedbm: an appliance for big data analytics", "author": ["S.-W. Jun", "M. Liu", "S. Lee", "J. Hicks", "J. Ankcorn", "M. King", "S. Xu"], "venue": "Computer Architecture (ISCA), 2015 ACM/IEEE 42nd Annual International Symposium on. IEEE, 2015, pp. 1\u201313.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2015}, {"title": "Biscuit: A framework for near-data processing of big data workloads", "author": ["B. Gu", "A.S. Yoon", "D.-H. Bae", "I. Jo", "J. Lee", "J. Yoon", "J.-U. Kang", "M. Kwon", "C. Yoon", "S. Cho"], "venue": "Computer Architecture (ISCA), 2016 ACM/IEEE 43rd Annual International Symposium on. IEEE, 2016, pp. 153\u2013165.", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2016}, {"title": "Efficient hardware implementation of the hyperbolic tangent sigmoid function", "author": ["A.H. Namin", "K. Leboeuf", "R. Muscedere", "H. Wu", "M. Ahmadi"], "venue": "Circuits and Systems, 2009. ISCAS 2009. IEEE International Symposium on. IEEE, 2009, pp. 2117\u20132120.", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2009}, {"title": "Best practices for convolutional neural networks applied to visual document analysis.", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "in ICDAR,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2003}, {"title": "Revisiting distributed synchronous SGD", "author": ["J. Chen", "R. Monga", "S. Bengio", "R. Jozefowicz"], "venue": "arXiv preprint arXiv:1604.00981, 2016.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2016}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J. Duchi", "E. Hazan", "Y. Singer"], "venue": "The Journal of Machine Learning Research, vol. 12, pp. 2121\u20132159, 2011.", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2011}, {"title": "ADADELTA: an adaptive learning rate method", "author": ["M.D. Zeiler"], "venue": "arXiv preprint arXiv:1212.5701, 2012.", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2012}, {"title": "NAND flash memory with multiple page sizes for high-performance storage devices", "author": ["J.-Y. Kim", "S.-H. Park", "H. Seo", "K.-W. Song", "S. Yoon", "E.-Y. Chung"], "venue": "IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 24, no. 2, pp. 764\u2013768, 2016.", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 0, "context": "However, because increasingly large datasets are being used to train largescale models such as deep networks [1], [2], the overhead incurred by the need to move data in the hierarchy becomes more salient and critically affects the overall computational", "startOffset": 109, "endOffset": 112}, {"referenceID": 1, "context": "However, because increasingly large datasets are being used to train largescale models such as deep networks [1], [2], the overhead incurred by the need to move data in the hierarchy becomes more salient and critically affects the overall computational", "startOffset": 114, "endOffset": 117}, {"referenceID": 2, "context": "The idea behind near-data processing (NDP) [3] is to equip the memory or storage with intelligence (i.", "startOffset": 43, "endOffset": 46}, {"referenceID": 3, "context": "The various approaches to realizing NDP have included processing in memory (PIM) [4]\u2013[9] and in-storage processing (ISP) [10]\u2013 [13].", "startOffset": 81, "endOffset": 84}, {"referenceID": 8, "context": "The various approaches to realizing NDP have included processing in memory (PIM) [4]\u2013[9] and in-storage processing (ISP) [10]\u2013 [13].", "startOffset": 85, "endOffset": 88}, {"referenceID": 9, "context": "The various approaches to realizing NDP have included processing in memory (PIM) [4]\u2013[9] and in-storage processing (ISP) [10]\u2013 [13].", "startOffset": 121, "endOffset": 125}, {"referenceID": 12, "context": "The various approaches to realizing NDP have included processing in memory (PIM) [4]\u2013[9] and in-storage processing (ISP) [10]\u2013 [13].", "startOffset": 127, "endOffset": 131}, {"referenceID": 14, "context": ", for address translation and garbage collection [15], [16]).", "startOffset": 49, "endOffset": 53}, {"referenceID": 15, "context": ", for address translation and garbage collection [15], [16]).", "startOffset": 55, "endOffset": 59}, {"referenceID": 16, "context": "Usually, these SSDbased processors experience considerable idle time that can be exploited for purposes other than SSD housekeeping [17], [18].", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "Usually, these SSDbased processors experience considerable idle time that can be exploited for purposes other than SSD housekeeping [17], [18].", "startOffset": 138, "endOffset": 142}, {"referenceID": 13, "context": "1For instance, the popular ImageNet dataset [14] contains over 1.", "startOffset": 44, "endOffset": 48}, {"referenceID": 18, "context": "Specifically, we implement three types of parallel SGD: synchronous SGD [19], Downpour SGD [20], and elastic averaging SGD (EASGD) [21].", "startOffset": 72, "endOffset": 76}, {"referenceID": 19, "context": "Specifically, we implement three types of parallel SGD: synchronous SGD [19], Downpour SGD [20], and elastic averaging SGD (EASGD) [21].", "startOffset": 91, "endOffset": 95}, {"referenceID": 20, "context": "Specifically, we implement three types of parallel SGD: synchronous SGD [19], Downpour SGD [20], and elastic averaging SGD (EASGD) [21].", "startOffset": 131, "endOffset": 135}, {"referenceID": 21, "context": "We compare the performance of these parallel SGD implementations using a 10-times amplified version of MNIST [22].", "startOffset": 109, "endOffset": 113}, {"referenceID": 22, "context": "Machine learning is a branch of artificial intelligence that aims to provide computers with the ability to learn without being programmed explicitly [23].", "startOffset": 149, "endOffset": 153}, {"referenceID": 23, "context": "Depending on the type of feedback available for training, we can broadly classify machine learning tasks into three categories [24]: supervised learning, unsupervised learning, and reinforcement learning.", "startOffset": 127, "endOffset": 131}, {"referenceID": 24, "context": "More in-depth reviews of machine learning can be found in [25]\u2013[27].", "startOffset": 58, "endOffset": 62}, {"referenceID": 24, "context": "For various types of machine learning algorithms, the core concept can often be explained using the following equations [25]:", "startOffset": 120, "endOffset": 124}, {"referenceID": 25, "context": "The method of (batch) gradient descent [26] is a first-order iterative optimization algorithm to find the minimum value of F (D,\u03b8) by updating \u03b8 in every iteration t in the direction of the negative gradient of F (D,\u03b8), where \u03b7 is the learning rate.", "startOffset": 39, "endOffset": 43}, {"referenceID": 18, "context": "[19] proposed an algorithm that implements parallel SGD in a distributed computing architecture.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "[28] proposed the lock-free Hogwild! algorithm that can update parameters asynchronously.", "startOffset": 0, "endOffset": 4}, {"referenceID": 19, "context": "[20] proposed the Downpour SGD for distributed computing systems by extending the Hogwild! algorithm.", "startOffset": 0, "endOffset": 4}, {"referenceID": 20, "context": "Recently, a new parallel SGD algorithm called EASGD was proposed [21], and many EASGD-based approaches have reported its effectiveness in distributed environments.", "startOffset": 65, "endOffset": 69}, {"referenceID": 16, "context": "SSDs have emerged as a type of next-generation storage device that uses NAND flash memory [17].", "startOffset": 90, "endOffset": 94}, {"referenceID": 27, "context": "SSDs have several advantages over HDDs (such as energy consumption [29], IO performance [30], and mechanical characteristics).", "startOffset": 67, "endOffset": 71}, {"referenceID": 28, "context": "SSDs have several advantages over HDDs (such as energy consumption [29], IO performance [30], and mechanical characteristics).", "startOffset": 88, "endOffset": 92}, {"referenceID": 29, "context": "In addition, the reliability and durability of NAND flash cells are limited [31].", "startOffset": 76, "endOffset": 80}, {"referenceID": 30, "context": "To improve the performance and durability of the NAND flash array, SSDs are thus managed by software called the flash translation layer (FTL) [32], which performs wear-leveling and garbage collection [15], [16].", "startOffset": 142, "endOffset": 146}, {"referenceID": 14, "context": "To improve the performance and durability of the NAND flash array, SSDs are thus managed by software called the flash translation layer (FTL) [32], which performs wear-leveling and garbage collection [15], [16].", "startOffset": 200, "endOffset": 204}, {"referenceID": 15, "context": "To improve the performance and durability of the NAND flash array, SSDs are thus managed by software called the flash translation layer (FTL) [32], which performs wear-leveling and garbage collection [15], [16].", "startOffset": 206, "endOffset": 210}, {"referenceID": 2, "context": "Inspired by these trends, the concept of NDP [3] has recently attracted considerable interest.", "startOffset": 45, "endOffset": 48}, {"referenceID": 8, "context": "For example, computation might be performed in memory or in the storage device where the input data reside [9].", "startOffset": 107, "endOffset": 110}, {"referenceID": 3, "context": "[4], various PIM approaches have been proposed.", "startOffset": 0, "endOffset": 3}, {"referenceID": 4, "context": "[5] reported accelerator logic for data-intensive operations (e.", "startOffset": 0, "endOffset": 3}, {"referenceID": 31, "context": "table lookups) in a type of three-dimensional memory called a hybrid memory cube (HMC) [33], [34].", "startOffset": 87, "endOffset": 91}, {"referenceID": 32, "context": "table lookups) in a type of three-dimensional memory called a hybrid memory cube (HMC) [33], [34].", "startOffset": 93, "endOffset": 97}, {"referenceID": 5, "context": "[6] proposed another HMC architecture called smart memory cube (SMC) and verified its performance using a simulator.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The popularity of deep learning [1] and its heavy computation demands has sparked the development of new PIM ap-", "startOffset": 32, "endOffset": 35}, {"referenceID": 6, "context": "[7] implemented a convolutional neural network (CNN) [27] on an HMC-based PIM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] proposed a flexible SMC-based PIM called NeuroCluster and implemented a CNN on it.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] proposed a resistive random access memory (ReRAM)-based PIM architecture that employed a crossbar array to perform matrix multiplications efficiently.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "[10].", "startOffset": 0, "endOffset": 4}, {"referenceID": 10, "context": "Many existing ISP methods have focused on popular (but inherently simple) algorithms to perform scan, join, and query operations [11].", "startOffset": 129, "endOffset": 133}, {"referenceID": 11, "context": "[12] proposed running a merge operation (frequently used by external sort operations in Hadoop) inside an SSD to reduce IO transfers and read/write operations, which also functions to extend the lifetime of the NAND flash inside the SSD.", "startOffset": 0, "endOffset": 4}, {"referenceID": 12, "context": "[13] implemented algorithms for linear regression, k-means, and string matching in the flash memory controller (FMC) via reconfigurable stream processors.", "startOffset": 0, "endOffset": 4}, {"referenceID": 33, "context": "BlueDBM [35] is an ISP system architecture for distributed computing systems with a flash memory-based embedded field programmable gate array (FPGA).", "startOffset": 8, "endOffset": 12}, {"referenceID": 34, "context": "Biscuit [36] is an SSD framework equipped with FMCs with pattern matching logic that enables MySQL queries.", "startOffset": 8, "endOffset": 12}, {"referenceID": 18, "context": "Using our ISP-ML platform, we implemented the three types of parallel SGD algorithms (synchronous SGD [19], Downpour SGD [20], and EASGD [21]).", "startOffset": 102, "endOffset": 106}, {"referenceID": 19, "context": "Using our ISP-ML platform, we implemented the three types of parallel SGD algorithms (synchronous SGD [19], Downpour SGD [20], and EASGD [21]).", "startOffset": 121, "endOffset": 125}, {"referenceID": 20, "context": "Using our ISP-ML platform, we implemented the three types of parallel SGD algorithms (synchronous SGD [19], Downpour SGD [20], and EASGD [21]).", "startOffset": 137, "endOffset": 141}, {"referenceID": 18, "context": "Pseudo-code of the three SGD algorithms implemented in ISP-ML: synchronous SGD [19], Downpour SGD [20], and EASGD [21].", "startOffset": 79, "endOffset": 83}, {"referenceID": 19, "context": "Pseudo-code of the three SGD algorithms implemented in ISP-ML: synchronous SGD [19], Downpour SGD [20], and EASGD [21].", "startOffset": 98, "endOffset": 102}, {"referenceID": 20, "context": "Pseudo-code of the three SGD algorithms implemented in ISP-ML: synchronous SGD [19], Downpour SGD [20], and EASGD [21].", "startOffset": 114, "endOffset": 118}, {"referenceID": 15, "context": "We used an ARM 926EJ-S (400MHz) as the embedded processor inside ISP-ML and DFTL [16] as the FTL for ISP-ML.", "startOffset": 81, "endOffset": 85}, {"referenceID": 35, "context": "Because the FPU we used did not support an exponential operation, we designed custom logic to implement the exp(\u00b7) function based on previous work [37].", "startOffset": 147, "endOffset": 151}, {"referenceID": 21, "context": "As test data, we utilized the samples from the MNIST database [22].", "startOffset": 62, "endOffset": 66}, {"referenceID": 36, "context": "distortion [38] to produce 10 times more data than the original MNIST (approximately 600,000 training and 10,000 test", "startOffset": 11, "endOffset": 15}, {"referenceID": 37, "context": "A reasonable explanation for this result would be that Downpour SGD calculates gradients based on more outdated parameters as the number of channels increases [39].", "startOffset": 159, "endOffset": 163}, {"referenceID": 20, "context": "As described in [21], Downpour SGD normally achieved a high performance for a low communication period [\u03c4 = 1, 4] and became unstable for a high communication period [\u03c4 = 16, 64] in ISP.", "startOffset": 16, "endOffset": 20}, {"referenceID": 38, "context": "Our future work also includes the following: First, we will be able to implement adaptive optimization algorithms such as Adagrad [40] and Adadelta [41].", "startOffset": 130, "endOffset": 134}, {"referenceID": 39, "context": "Our future work also includes the following: First, we will be able to implement adaptive optimization algorithms such as Adagrad [40] and Adadelta [41].", "startOffset": 148, "endOffset": 152}, {"referenceID": 40, "context": "The effectiveness of using multiple page sizes has already been reported for conventional SSDs [42].", "startOffset": 95, "endOffset": 99}], "year": 2017, "abstractText": "Near-data processing (NDP) refers to augmenting memory or storage with processing power. Despite its potential for acceleration computing and reducing power requirements, only limited progress has been made in popularizing NDP for various reasons. Recently, two major changes have occurred that have ignited renewed interest and caused a resurgence of NDP. The first is the success of machine learning (ML), which often demands a great deal of computation for training, requiring frequent transfers of big data. The second is the popularity of NAND flash-based solid-state drives (SSDs) containing multicore processors that can accommodate extra computation for data processing. In this paper, we evaluate the potential of NDP for ML using a new SSD platform that allows us to simulate instorage processing (ISP) of ML workloads. Our platform (named ISP-ML) is a full-fledged simulator of a realistic multi-channel SSD that can execute various ML algorithms using data stored in the SSD. To conduct a thorough performance analysis and an in-depth comparison with alternative techniques, we focus on a specific algorithm: stochastic gradient descent (SGD), which is the de facto standard for training differentiable models such as logistic regression and neural networks. We implement and compare three SGD variants (synchronous, Downpour, and elastic averaging) using ISP-ML, exploiting the multiple NAND channels to parallelize SGD. In addition, we compare the performance of ISP and that of conventional in-host processing, revealing the advantages of ISP. Based on the advantages and limitations identified through our experiments, we further discuss directions for future research on ISP for accelerating ML.", "creator": "LaTeX with hyperref package"}}}