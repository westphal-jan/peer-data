{"id": "1610.02348", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Adaptive Convolutional ELM For Concept Drift Handling in Online Stream Data", "abstract": "In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional hiererchical features representation learner combined with Elastic ELM (E$^2$LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability in ensemble level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels.", "histories": [["v1", "Fri, 7 Oct 2016 16:53:09 GMT  (1060kb,D)", "http://arxiv.org/abs/1610.02348v1", "Submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems. Special Issue on Efficient and Rapid Machine Learning Algorithms for Big Data and Dynamic Varying Systems"]], "COMMENTS": "Submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems. Special Issue on Efficient and Rapid Machine Learning Algorithms for Big Data and Dynamic Varying Systems", "reviews": [], "SUBJECTS": "cs.AI cs.LG cs.NE", "authors": ["arif budiman", "mohamad ivan fanany", "chan basaruddin"], "accepted": false, "id": "1610.02348"}, "pdf": {"name": "1610.02348.pdf", "metadata": {"source": "CRF", "title": "Adaptive Convolutional ELM For Concept Drift Handling in Online Stream Data", "authors": ["Arif Budiman", "Mohamad Ivan Fanany", "Chan Basaruddin"], "emails": ["arif.budiman21@ui.ac.id"], "sections": [{"heading": null, "text": "In this paper, we propose the Adaptive Convolutional ELM Method (ACNNELM) as an extension of the Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method aims at the concept of drift handling. We have improved CNN as a revolutionary feature learner combined with Elastic ELM (E2LM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability at the classification level (ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability at the ensemble level (ACNNELM-2). Our proposed Adaptive CNNELM drift adaptability is flexible. Our concept works better at the classification level and ensemble level, while most methods of the current NISM method are not extended at the ensemble level."}, {"heading": "1 Introduction", "text": "In fact, most people are able to determine for themselves what they want and what they want. (...) It is not that people are able to decide what they want and what they want. (...) It is not that they want it. (...) It is that they do not want it. (...) It is that they do not want it. (...) It is that they do not want it. (...) It is that they do not want it. (...) It is that they do not want it. (...) \"\"... \"\" \"...\" \"\"... \"\" \"...........................................................................\" \"...................................................\" \"...................................................................\" \"...........................................................\" \".........................................................................................................................................\" \"\"....................... \"...........\"................... \"...............................\"....... \".........\"....... \".......\"....... \""}, {"heading": "1.1 Notations", "text": "We used the notations in this work to make it easier for the readers: \u2022 Matrix is written in bold capital letters (i.e., X). \u2022 Vector is written in lowercase letters (i.e., x). \u2022 The transposition of a matrix X is written as XT. \u2022 The amount of training data is N. Each input data x contains some d attributes. The target has m number of classes. An input matrix X can be called Xd \u00b7 N and the target matrix T as TN \u00b7 m. \u2022 We call the subscription font with parentheses to show the time sequence number. X (0) is the subset of input data at the time k = 0 as the initialization stage. X (1), X (2),... (concept), X (the input data is set at the time T)."}, {"heading": "2 Literature Reviews", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Extreme Learning Machine (ELM)", "text": "However, the ELM learning objective is to maintain the output weight (\u03b2) (\u03b2). (\u03b2) The ELM learning objective is to reduce the output weight (\u03b2) (\u03b2). (\u03b2) The ELM learning objective is to maintain the output weight (\u03b2). (\u03b2) The ELM learning objective is to reduce the output weight (\u03b2). (\u03b2) The ELM learning objective is to maintain the output weight (\u03b2). (\u03b2) The ELM learning objective is to maintain the output weight (\u03b2). (\u03b2) The ELM learning objective is to reduce the output weight (\u03b2). (\u03b2) The ELM learning objective is where H \u2020 T (1) the H \u2020 is pseudoinverse. (Moore Penrose generalized inverse) of H. ELMlearning is simply equivalent to find the smallest solution for \u03b2 of the linear system H\u03b2 = T if the output weight \u03b2 = H \u2020 T.ridden layer matx"}, {"heading": "2.2 Convolutional Neural Networks (CNN)", "text": "Unlike SLFN in ELM, CNN consists of some folding and sub-sampling layers in the feed-forward architecture. CNN is the first successful depth architecture that maintains the characteristics of traditional NN. CNN has excellent performance in spatial visual classification [36]. The key advantage of CNN over other deep learning methods is the use of fewer parameters [43]. At the end of the CNN layer, CNN is followed by a fully connected multi-layer neural network (see fig. 1) [43]. Many variants of CNN architectures in literature, but the basic common building blocks are the revolutionary layer, pooling layer and fully connected layer [8]. CNN's input layer is designed to exploit the 2D structure with d \u00d7 d \u00d7 r of the image, where d is the height and width of the image, and r has the number of channels (i.e. grayscale image = 1 and r = 3)."}, {"heading": "2.3 CNN ELM Integration", "text": "Huang et. al. [15] explained the ELM theories are not only valid for fully connected ELM architecture, but are also actually valid for local connections, called local receptive fields (LRF) or similar to kernels in CNN term. Huang et. al. suggested ELM-LRF, which has connections between input layer and hidden nodes, are randomly generated according to any continuous different types of probability distributions. According to Huang et. al. the random hidden nodes CNN is a kind of local receptive fields. Unlike CNN, ELM with hidden nodes considers the essence of ELM for non-iterative initial weights spreadsheet. Pang et. al. [33] implements deep Constitutional hidden ELM (DC-ELM). It uses multiple conversion layers and pooling layers for high levels DC-ISM abstraction of input images asasasasa.Then the abstract features are classified by a petal ELM."}, {"heading": "2.4 Concept Drift", "text": "The short-term explanation of the concept drift (CD) was given by Gama, et. al. [7] and Minku [29] based on the Bayean decision theory for class production and incoming data X.6 / 28The concept is the entire distribution (common distribution P (X, c) in a given time step. The CD represents all changes in the common distribution when P (c | X) has changed; i.e., the CD type has a categorization as follows [6,7,12,37] 1. Real Drift (RD) refers to changes in P (0) and P (1) are each the common distribution in the time k (0) and in the group (1). The CD type has a categorization as follows [6,7,12,37] 1. Real Drift (RD) refers to changes in P (c | X). The change in P (c | X) can be caused by a change in the class boundary (the number of classes) or the class probabilities."}, {"heading": "2.5 CNN in Concept Drift", "text": "Grachten et. al. [28] proposed some adaptation strategies for individual CNN classification systems based on two common adaptive approaches: 1) REUSE is to reuse the model after the task change and replace the old task training data with the new task training data. 2) RESET is to ignore the presentations learned so far and start the new task learning with a randomly initialized model. In addition, Grachten et. al. has replaced the task training data categorized as follows: 1. RESET: Initialize parameters with random values; 2. RESET PRF: Combine the previous regulation of revolution filters (PRF) with the RESET option to \"hang\" maps on \"-hang\" basis sometimes a bit of the RESET baseline, and the gains are usually moderate; 3. REUSE ALL: Initialize all parameters from the previous model (except the Selection Layer CEUF; RSE: REUF)."}, {"heading": "3 Proposed Method", "text": "We used common CNN-ELM integration [\u03b2 9, 15,25,33] architecture when the last folding layer output is fed as a hidden node weight H by ELM. However, for the final H we used nonlinear optimal tanh (1,7159 x tanh (23 x H) activation function [23] to have better generalization accuracy. We also used CNN global expansion structure [44] to improve performance accuracy. We used the E2LM as a parallel monitored classifier to replace fully connected NN. Compared to the regular ELM method, we do not need input weight as a hidden nodes10 / 28 parameter (see Fig. 9). We used the integration architecture becomes two models: 1) ACNNELM-1: This ACNNELM model works on the basis of AOS-ELM for minimal adjustment of the drift ELM-level ACNM comparability."}, {"heading": "4 Experiment and Performance Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data set", "text": "The MNIST dataset is a balanced dataset that contains numerical handwriting (10 target classes) with a size of 28 x 28 pixels in a grayscale image. Therefore, we developed advanced MNIST data with larger training examples by adding 3 types of image noise (see Figure 16). Finally, our extended MNIST dataset contains 240,000 examples of training data and 40,000 examples of test data. We also improved the 28 x 28 image size as attributes with additional attributes based on NIST images."}, {"heading": "4.2 Experiment Methods", "text": "We defined the scope of the work as follows: 1. We improved DeepLearn Toolbox [32] with Matlab Parallel Computing Toolbox.15 / 282. We used single precision as double precision for all calculations in this work. Double precision has an improvement in accuracy in Matrix Inverse Computation as single precision. The ELM in Matlab with double precision is standard and it has a better accuracy (i.e. 4-5% higher) than single precision. However, the use of single precision has more CPU and memory savings especially for parallelization in big-stream data. Unfortunately, most of the work did not mention how their precision method is used. We want to achieve an improvement in accuracy, not due to precision computation factors. 3. We focused on simple CNN architecture consisting of convolution layers (c), following reLU activation layer, the ISMISM layer (s) in this paper."}, {"heading": "4.3 Performance Results", "text": "Unlike CNN, CNN's performance for large sequential training data is better when using a larger number of eras. CNN has better scalability than ELM for large sequential learning data, but it takes longer for iteration to improve performance (see Fig. 20). At Epoch = 50, the test accuracy is 90.32%. From a learning time perspective, the time for 50 iterations is equivalent to 50 models of individual CNN ELMs sequentially. 18 / 28We compared the benchmark result with our CNN-ELM method: 1. The performance of CNN-ELM can be improved by using more layers in extended structure. In Table 3, the model 6c-2s-12c-2s has better accuracy than the model 6c-2s.. with additional zero block matrix to expand the columns to 10 layers."}, {"heading": "5 Conclusion", "text": "The proposed method provides better adaptability for classification level (ACNNELM-1) and ensemble level (ACNNELM-2). ACNNELM-2 has better computational scalability and performance accuracy than ACNNELM-1 due to the benefit of the aggregation ensemble. 23 / 28 However, some CNN-related parameters need to be investigated further, such as iterations, random weight for the assignment of cores, error feedback optimization, decay parameters and larger layers for larger feature dimensions. In addition, we need to investigate and implement CNN ELM for non-spatial recognition, i.e. for the recognition of human actions [2]. We are considering some ideas for future research: \u2022 We will develop the methods on a different CNN framework that fully supported the CUDA GPU calculation. The purpose is to increase the scaling capability and to improve training time in large image data sets to accelerate learning parameters \u2022 We need to further study CNELM."}, {"heading": "6 Acknowledgment", "text": "This work is supported by the Higher Education Center of Excellence Research Grant under contract number 1068 / UN2.R12 / HKP5.00 / 2016, which is funded by the Indonesian Ministry of Research and Higher Education."}, {"heading": "7 Conflict of Interests", "text": "The authors declare that there is no conflict of interest in the publication of this paper."}], "references": [{"title": "Learning deep architectures for ai", "author": ["Y. Bengio"], "venue": "Found. Trends Mach. Learn., 2(1):1\u2013127, Jan.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2009}, {"title": "Constructive, robust and adaptive os-elm in human action recognition", "author": ["A. Budiman", "M.I. Fanany", "C. Basaruddin"], "venue": "In Industrial Automation, Information and Communications Technology (IAICT), 2014 International Conference on, pages 39\u201345, Aug", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive online sequential elm for concept drift tackling", "author": ["A. Budiman", "M.I. Fanany", "C. Basaruddin"], "venue": "Hindawi,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2016}, {"title": "A review on real time data stream classification and adapting to various concept drift scenarios", "author": ["P. Dongre", "L. Malik"], "venue": "In Advance Computing Conference (IACC), 2014 IEEE International, pages 533\u2013537, Feb", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2014}, {"title": "Incremental learning of concept drift in nonstationary environments", "author": ["R. Elwell", "R. Polikar"], "venue": "Neural Networks, IEEE Transactions on, 22(10):1517\u20131531, Oct", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2011}, {"title": "A survey on concept drift adaptation", "author": ["J. a. Gama", "I. \u017dliobait\u0117", "A. Bifet", "M. Pechenizkiy", "A. Bouchachia"], "venue": "ACM Comput. Surv.,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Recent advances in convolutional neural networks", "author": ["J. Gu", "Z. Wang", "J. Kuen", "L. Ma", "A. Shahroudy", "B. Shuai", "T. Liu", "X. Wang", "G. Wang"], "venue": "CoRR, abs/1512.07108,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid deep learning cnn-elm model and its application in handwritten numeral recognition", "author": ["L. Guo", "S. Ding"], "venue": "page 2673\u20132680, 7", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel extreme learning machine for regression based on mapreduce", "author": ["Q. He", "T. Shang", "F. Zhuang", "Z. Shi"], "venue": "Neurocomput., 102:52\u201358, Feb.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2013}, {"title": "A fast learning algorithm for deep belief nets", "author": ["G. Hinton", "S. Osindero", "Y.-W. Teh"], "venue": "Neural Computation, 18(7):1527\u20131554,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2006}, {"title": "Learning from streaming data with concept drift and imbalance: an overview", "author": ["T. Hoens", "R. Polikar", "N. Chawla"], "venue": "Progress in Artificial Intelligence, 1(1):89\u2013101,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "Trends in extreme learning machines: A review", "author": ["G. Huang", "G.-B. Huang", "S. Song", "K. You"], "venue": "Neural Networks, 61(0):32 \u2013 48,", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "An insight into extreme learning machines: Random neurons, random features and kernels", "author": ["G.-B. Huang"], "venue": "Cognitive Computation, 6(3),", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Local receptive fields based extreme learning machine", "author": ["G.-B. Huang", "Z. Bai", "L.L.C. Kasun", "C.M. Vong"], "venue": "IEEE Computational Intelligence Magazine (accepted), 10,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Extreme learning machines: a survey", "author": ["G.-B. Huang", "D. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics, 2(2):107\u2013122,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2011}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, 70(1-3):489\u2013501,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "An enhanced online sequential extreme learning machine algorithm", "author": ["Y. Jun", "M.-J. Er"], "venue": "In Control and Decision Conference, 2008. CCDC 2008. Chinese, pages 2902\u20132907, July", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2008}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In Advances in Neural Information Processing Systems, pages 1097\u20131105,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2012}, {"title": "Classifier ensembles for changing environments", "author": ["L. Kuncheva"], "venue": "In Multiple Classifier Systems, volume 3077 of Lecture Notes in Computer Science, pages 1\u201315. Springer Berlin Heidelberg,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2004}, {"title": "Classifier ensembles for detecting concept change in streaming data: Overview and perspectives", "author": ["L.I. Kuncheva"], "venue": "In 2nd Workshop SUEMA 2008 (ECAI 2008), pages 5\u201310,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2008}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In S. Haykin and B. Kosko, editors, Intelligent Signal Processing, pages 306\u2013351. IEEE Press,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2001}, {"title": "Effiicient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 9\u201350, London, UK, UK,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 1998}, {"title": "and C", "author": ["Y. LeCu"], "venue": "Cortes. MNIST handwritten digit database,", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2010}, {"title": "Image classification using fast learning convolutional neural networks", "author": ["K. Lee", "D.-C. Park"], "venue": null, "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on, 17(6):1411\u20131423, Nov", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2006}, {"title": "Ensemble based extreme learning machine", "author": ["N. Liu", "H. Wang"], "venue": "IEEE Signal Processing Letters, 17(8):754\u2013757, Aug", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2010}, {"title": "Grachten", "author": ["C.E.C.C. Maarte"], "venue": "Strategies for conceptual change in convolutional neural networks.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "The impact of diversity on online ensemble learning in the presence of concept drift", "author": ["L. Minku", "A. White", "X. Yao"], "venue": "Knowledge and Data Engineering, IEEE Transactions on, 22(5):730\u2013742, May", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}, {"title": "Meta-cognitive online sequential extreme learning machine for imbalanced and concept-drifting data classification", "author": ["B. Mirza", "Z. Lin"], "venue": "Neural Networks, 80:79\u201394,", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep learning applications and challenges in big data analytics", "author": ["M.M. Najafabadi", "F. Villanustre", "T.M. Khoshgoftaar", "N. Seliya", "R. Wald", "E. Muharemagic"], "venue": "Journal of Big Data, 2(1):1\u201321,", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "Deep convolutional extreme learning machine and its application in handwritten digit classification", "author": ["S. Pang", "X. Yang"], "venue": "Hindawi,", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep Boltzmann machines", "author": ["R. Salakhutdinov", "G. Hinton"], "venue": "In Proceedings of the International Conference on Artificial Intelligence and Statistics, volume 5, pages 448\u2013455,", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2009}, {"title": "Accelerating Large-Scale Convolutional Neural Networks with Parallel Graphics Multiprocessors, pages 82\u201391", "author": ["D. Scherer", "H. Schulz", "S. Behnke"], "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg,", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2010}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In Proceedings of the Seventh International Conference on Document Analysis and Recognition Volume 2, ICDAR \u201903, pages 958\u2013, Washington, DC, USA,", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2003}, {"title": "Handling local concept drift with dynamic integration of classifiers: Domain of antibiotic resistance in nosocomial infections", "author": ["A. Tsymbal", "M. Pechenizkiy", "P. Cunningham", "S. Puuronen"], "venue": "In Computer-Based Medical Systems, 2006. CBMS 2006. 19th IEEE International Symposium on, pages 679\u2013684,", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2006}, {"title": "Dynamic integration of classifiers for handling concept drift", "author": ["A. Tsymbal", "M. Pechenizkiy", "P. Cunningham", "S. Puuronen"], "venue": "Inf. Fusion, 9(1):56\u201368, Jan.", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2008}, {"title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion", "author": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "P.-A. Manzagol"], "venue": "J. Mach. Learn. Res., 11:3371\u20133408, Dec.", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2010}, {"title": "Elastic extreme learning machine for big data classification", "author": ["J. Xin", "Z. Wang", "L. Qu", "G. Wang"], "venue": "Neurocomputing, 149, Part A:464 \u2013 471,", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2015}, {"title": "Hybrid adaptive classifier ensemble", "author": ["Z. Yu", "L. Li", "J. Liu", "G. Han"], "venue": "IEEE Transactions on Cybernetics, 45(2):177\u2013190, Feb", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2015}, {"title": "Comparative study between incremental and ensemble learning on data streams: Case study", "author": ["W. Zang", "P. Zhang", "C. Zhou", "L. Guo"], "venue": "Journal Of Big Data, 1(1):1\u201316,", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2014}, {"title": "Visualizing and Understanding Convolutional Networks, pages 818\u2013833", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Springer International Publishing, Cham,", "citeRegEx": "43", "shortCiteRegEx": null, "year": 2014}, {"title": "Adaptive convolutional neural network and its application in face recognition", "author": ["Y. Zhang", "D. Zhao", "J. Sun", "G. Zou", "W. Li"], "venue": "Neural Processing Letters, 43(2):389\u2013399,", "citeRegEx": "44", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning under Concept Drift: an Overview", "author": ["I. Zliobaite"], "venue": "Computing Research Repository, abs/1010.4,", "citeRegEx": "45", "shortCiteRegEx": null, "year": 2010}, {"title": "Weighted extreme learning machine for imbalance learning", "author": ["W. Zong", "G.-B. Huang", "Y. Chen"], "venue": "Neurocomput., 101:229\u2013242, Feb.", "citeRegEx": "46", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 5, "context": "problem [7,29]).", "startOffset": 8, "endOffset": 14}, {"referenceID": 27, "context": "problem [7,29]).", "startOffset": 8, "endOffset": 14}, {"referenceID": 5, "context": "The aim of CD handling [7] is to boost the generalization accuracy when the drift occurs.", "startOffset": 23, "endOffset": 26}, {"referenceID": 3, "context": "Common handling methods are based on classifier ensemble [5].", "startOffset": 57, "endOffset": 60}, {"referenceID": 2, "context": "However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].", "startOffset": 110, "endOffset": 123}, {"referenceID": 18, "context": "However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].", "startOffset": 110, "endOffset": 123}, {"referenceID": 19, "context": "However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].", "startOffset": 110, "endOffset": 123}, {"referenceID": 42, "context": "However, ensemble methods are difficult to manage complexities when handling many types of consecutive drifts [3, 20,21,45].", "startOffset": 110, "endOffset": 123}, {"referenceID": 0, "context": "One of the recent online big stream data approaches are based on Deep Learning (DL) techniques [1].", "startOffset": 95, "endOffset": 98}, {"referenceID": 9, "context": "The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.", "startOffset": 82, "endOffset": 86}, {"referenceID": 31, "context": "The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.", "startOffset": 146, "endOffset": 150}, {"referenceID": 36, "context": "The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.", "startOffset": 200, "endOffset": 204}, {"referenceID": 20, "context": "The Deep Learning has many variants such as a Deep Belief Network (DBN) by Hinton [11], Deep Boltzmann Machine (DBM) by Salakhutdinov and Hinton, [34], Stacked Denoising Autoencoders (SDA) by Vincent [39], and Convolutional Neural Network (CNN) by LeCun [22], and many others.", "startOffset": 254, "endOffset": 258}, {"referenceID": 29, "context": "Extreme Learning Machine (ELM), Support Vector Machine (SVM), Multi-Layer Perceptron Neural Network (MLP NN), Hidden Markov Model (HMM) may not be able to handle such big stream data directly [31] although they worked successfully in classification problem in many areas.", "startOffset": 192, "endOffset": 196}, {"referenceID": 6, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 76, "endOffset": 85}, {"referenceID": 20, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 76, "endOffset": 85}, {"referenceID": 40, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 76, "endOffset": 85}, {"referenceID": 11, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 94, "endOffset": 104}, {"referenceID": 14, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 94, "endOffset": 104}, {"referenceID": 15, "context": "In this paper, we proposed a new adaptive scheme of integration between CNN [8,22,43] and ELM [13,16,17] to handle concept drift in online big stream data.", "startOffset": 94, "endOffset": 104}, {"referenceID": 2, "context": "We studied ACNNELM scheme for concept drift either changes in the number of feature inputs named virtual drift (VD) or the number of classes named real drift (RD) or consecutive drift when VD and RD occurred at the same time named hybrid drift (HD) [3] in recurrent context (all concepts occur alternately).", "startOffset": 249, "endOffset": 252}, {"referenceID": 1, "context": "We developed ACNNELM based on our previous work on adaptive ELM scheme named Adaptive OS-ELM (AOS-ELM) that works as single ELM classifier for CD handling [2, 3].", "startOffset": 155, "endOffset": 161}, {"referenceID": 2, "context": "We developed ACNNELM based on our previous work on adaptive ELM scheme named Adaptive OS-ELM (AOS-ELM) that works as single ELM classifier for CD handling [2, 3].", "startOffset": 155, "endOffset": 161}, {"referenceID": 11, "context": "Extreme Learning Machine (ELM) works based on generalized pseudoinverse for non iterative learning in single hidden layer feedforward neural network (SLFN) architecture [13,16,17].", "startOffset": 169, "endOffset": 179}, {"referenceID": 14, "context": "Extreme Learning Machine (ELM) works based on generalized pseudoinverse for non iterative learning in single hidden layer feedforward neural network (SLFN) architecture [13,16,17].", "startOffset": 169, "endOffset": 179}, {"referenceID": 15, "context": "Extreme Learning Machine (ELM) works based on generalized pseudoinverse for non iterative learning in single hidden layer feedforward neural network (SLFN) architecture [13,16,17].", "startOffset": 169, "endOffset": 179}, {"referenceID": 24, "context": "A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [26] has sequential learning phase.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "However, increasing the hidden nodes number may improve the performance named Constructive Enhancement OS-ELM [18].", "startOffset": 110, "endOffset": 114}, {"referenceID": 37, "context": "Another approach is Elastic Extreme Learning Machine (ELM) [40] or Parallel ELM [10] based on MapReduce framework to solve large sequential training data in a parallel way.", "startOffset": 59, "endOffset": 63}, {"referenceID": 8, "context": "Another approach is Elastic Extreme Learning Machine (ELM) [40] or Parallel ELM [10] based on MapReduce framework to solve large sequential training data in a parallel way.", "startOffset": 80, "endOffset": 84}, {"referenceID": 37, "context": "Therefore, ELM learning is efficient for rapidly massive training data set [40].", "startOffset": 75, "endOffset": 79}, {"referenceID": 37, "context": "ELM is more computing efficient, better performance and support parallel computation than OS-ELM [40], but ELM did not address the possibility for hidden nodes increased during the training.", "startOffset": 97, "endOffset": 101}, {"referenceID": 2, "context": "In this paper, we developed parallelization framework to integrate with CNN to solve concept drift issues in sequential learning as enhancement from our previous work Adaptive OS-ELM (AOS-ELM) [3].", "startOffset": 193, "endOffset": 196}, {"referenceID": 33, "context": "CNN has excellent performance for spatial visual classification [36].", "startOffset": 64, "endOffset": 68}, {"referenceID": 40, "context": "The key benefit of CNN comparing with another deep learning methods are using fewer parameters [43].", "startOffset": 95, "endOffset": 99}, {"referenceID": 40, "context": "1) [43].", "startOffset": 3, "endOffset": 7}, {"referenceID": 6, "context": "Many variants of CNN architectures in the literature, but the basic common building blocks are convolutional layer, pooling layer and fully connected layer [8].", "startOffset": 156, "endOffset": 159}, {"referenceID": 40, "context": "At the end of the CNN layer, there may be any densely connected NN layers for supervised learning [43].", "startOffset": 98, "endOffset": 102}, {"referenceID": 32, "context": "The convolution operations are heavy computation but inherently parallel, which getting beneficial from a hardware parallel implementation [35].", "startOffset": 139, "endOffset": 143}, {"referenceID": 17, "context": "However, the CNN network size is still limited mainly by the amount of memory available on current GPUs [19].", "startOffset": 104, "endOffset": 108}, {"referenceID": 40, "context": "CNN Architecture from AlexNet [43] .", "startOffset": 30, "endOffset": 34}, {"referenceID": 13, "context": "[15] explained the ELM theories are not only valid for fully connected ELM architecture but are also actually valid for local connections, named local receptive fields (LRF) or similar with Kernel in CNN term.", "startOffset": 0, "endOffset": 4}, {"referenceID": 30, "context": "[33] implemented deep convolutional ELM (DC-ELM).", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "[9] introduced an integration model of CNN-ELM, and applied to handwritten digit recognition.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "Structure of the deep hybrid CNN-ELM model [9] .", "startOffset": 43, "endOffset": 46}, {"referenceID": 23, "context": "[25] proposed an integration of CNN with OS-ELM.", "startOffset": 0, "endOffset": 4}, {"referenceID": 5, "context": "[7] and Minku [29] based on Bayesian decision theory for class output c and incoming data X.", "startOffset": 0, "endOffset": 3}, {"referenceID": 27, "context": "[7] and Minku [29] based on Bayesian decision theory for class output c and incoming data X.", "startOffset": 14, "endOffset": 18}, {"referenceID": 4, "context": "The CD type has categorization as follows [6,7,12,37].", "startOffset": 42, "endOffset": 53}, {"referenceID": 5, "context": "The CD type has categorization as follows [6,7,12,37].", "startOffset": 42, "endOffset": 53}, {"referenceID": 10, "context": "The CD type has categorization as follows [6,7,12,37].", "startOffset": 42, "endOffset": 53}, {"referenceID": 34, "context": "The CD type has categorization as follows [6,7,12,37].", "startOffset": 42, "endOffset": 53}, {"referenceID": 2, "context": "[3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 35, "context": "Ensemble learning is the common approaches to tackle concept drift, in which are combined using a form of voting [38,45].", "startOffset": 113, "endOffset": 120}, {"referenceID": 42, "context": "Ensemble learning is the common approaches to tackle concept drift, in which are combined using a form of voting [38,45].", "startOffset": 113, "endOffset": 120}, {"referenceID": 39, "context": "The ensemble approach can integrate the results of individual classifiers into a unified predicted result to improve the accuracy and robustness than single classifiers [42].", "startOffset": 169, "endOffset": 173}, {"referenceID": 38, "context": "[41] proposed a general hybrid adaptive ensemble learning framework (See Fig.", "startOffset": 0, "endOffset": 4}, {"referenceID": 25, "context": "[27] proposed an ensemble based ELM (EN-ELM) which uses the cross-validation scheme to build ELM classifiers ensemble.", "startOffset": 0, "endOffset": 4}, {"referenceID": 38, "context": "[41].", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "The adaptive ensemble approach may be not practical and flexible if each member itself is not designed to be adaptive [3] and may need to recall the previous training data (not single pass learning [20]).", "startOffset": 118, "endOffset": 121}, {"referenceID": 18, "context": "The adaptive ensemble approach may be not practical and flexible if each member itself is not designed to be adaptive [3] and may need to recall the previous training data (not single pass learning [20]).", "startOffset": 198, "endOffset": 202}, {"referenceID": 2, "context": "Moreover, another simple approach is using single classifier [3, 28,30].", "startOffset": 61, "endOffset": 71}, {"referenceID": 26, "context": "Moreover, another simple approach is using single classifier [3, 28,30].", "startOffset": 61, "endOffset": 71}, {"referenceID": 28, "context": "Moreover, another simple approach is using single classifier [3, 28,30].", "startOffset": 61, "endOffset": 71}, {"referenceID": 28, "context": "[30] proposed OS-ELM for imbalanced and concept drift tackling named meta-cognitive OS-ELM (MOS-ELM) that was developed based on Weighted OS-ELM (WOS-ELM) [46].", "startOffset": 0, "endOffset": 4}, {"referenceID": 43, "context": "[30] proposed OS-ELM for imbalanced and concept drift tackling named meta-cognitive OS-ELM (MOS-ELM) that was developed based on Weighted OS-ELM (WOS-ELM) [46].", "startOffset": 155, "endOffset": 159}, {"referenceID": 2, "context": "Also, it can be applied for concept replacement and recurring [3] by using simple matrix adjustment and multiplication.", "startOffset": 62, "endOffset": 65}, {"referenceID": 12, "context": "According to interpolation theory from ELM point of view and Learning Principle I of ELM Theory [14], the input weight and bias as hidden nodes H parameters are independent of training samples and their learning environment through randomization.", "startOffset": 96, "endOffset": 100}, {"referenceID": 16, "context": "According to universal approximation theory and inspired by the related works [18], the AOS-ELM has real drift capability by modifying the output matrix with zero block matrix concatenation to change the size matrix dimension without changing the norm value.", "startOffset": 78, "endOffset": 82}, {"referenceID": 26, "context": "[28] proposed some adapting strategies for single CNN classifier system based on two common adaptive approaches: 1) REUSE is to reuse the model upon the change of task and replace the old task training data with the new task training data.", "startOffset": 0, "endOffset": 4}, {"referenceID": 26, "context": "\u2019s experiment for a single data set; training methods in grey rounded boxes, represent models in circles, data instances in document shapes, and the evaluation method in white rounded box [28] .", "startOffset": 188, "endOffset": 192}, {"referenceID": 41, "context": "[44] proposed an adaptive CNN (ACNN), whose structure automatic expansion based on the average system error and training recognition rate performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 48}, {"referenceID": 13, "context": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 48}, {"referenceID": 23, "context": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 48}, {"referenceID": 30, "context": "We used common CNN-ELM integration [9, 15,25,33] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 48}, {"referenceID": 21, "context": "7159\u00d7 tanh( 23 \u00d7H) activation function [23] to have better generalization accuracy.", "startOffset": 39, "endOffset": 43}, {"referenceID": 41, "context": "We used also CNN global expansion structure [44] to improve the performance accuracy.", "startOffset": 44, "endOffset": 48}, {"referenceID": 41, "context": "The Adaptive CNN architecture with Global expansion (2), Local expansion (3) and Incremental Learning (4) [44] .", "startOffset": 106, "endOffset": 110}, {"referenceID": 26, "context": "Using this simple model, we have flexibilities to use reuse or reset strategy [28] for recurrent or sudden concept drift handling.", "startOffset": 78, "endOffset": 82}, {"referenceID": 22, "context": "The dataset has been divided for 60,000 examples for training data and separated 10,000 examples for testing data [24].", "startOffset": 114, "endOffset": 118}, {"referenceID": 2, "context": "The HOG additional attributes have been used in our research [3] to simulate VD scenario.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "posed based human action recognition [2].", "startOffset": 37, "endOffset": 40}], "year": 2016, "abstractText": "In big data era, the data continuously generated and its distribution may keep changes overtime. These challenges in online stream of data are known as concept drift. In this paper, we proposed the Adaptive Convolutional ELM method (ACNNELM) as enhancement of Convolutional Neural Network (CNN) with a hybrid Extreme Learning Machine (ELM) model plus adaptive capability. This method is aimed for concept drift handling. We enhanced the CNN as convolutional hiererchical features representation learner combined with Elastic ELM (ELM) as a parallel supervised classifier. We propose an Adaptive OS-ELM (AOS-ELM) for concept drift adaptability in classifier level (named ACNNELM-1) and matrices concatenation ensembles for concept drift adaptability in ensemble level (named ACNNELM-2). Our proposed Adaptive CNNELM is flexible that works well in classifier level and ensemble level while most current methods only proposed to work on either one of the levels. We verified our method in extended MNIST data set and not MNIST data set. We set the experiment to simulate virtual drift, real drift, and hybrid drift event and we demonstrated how our CNNELM adaptability works. Our proposed method works well and gives better accuracy, computation scalability, and concept drifts adaptability compared to the regular ELM and CNN. Further researches are still required to study the optimum parameters and to use more varied image data set. Keywords\u2014 deep learning, extreme learning machine, convolutional, neural network, big data, online,concept drift", "creator": "LaTeX with hyperref package"}}}