{"id": "1610.02373", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Distributed Averaging CNN-ELM for Big Data", "abstract": "Increasing the scalability of machine learning to handle big volume of data is a challenging task. The scale up approach has some limitations. In this paper, we proposed a scale out approach for CNN-ELM based on MapReduce on classifier level. Map process is the CNN-ELM training for certain partition of data. It involves many CNN-ELM models that can be trained asynchronously. Reduce process is the averaging of all CNN-ELM weights as final training result. This approach can save a lot of training time than single CNN-ELM models trained alone. This approach also increased the scalability of machine learning by combining scale out and scale up approaches. We verified our method in extended MNIST data set and not-MNIST data set experiment. However, it has some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. Further researches to use more complex image data set are required.", "histories": [["v1", "Fri, 7 Oct 2016 18:59:23 GMT  (308kb,D)", "http://arxiv.org/abs/1610.02373v1", "Submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems"]], "COMMENTS": "Submitted to IEEE Transactions on Systems, Man and Cybernetics: Systems", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.DC", "authors": ["arif budiman", "mohamad ivan fanany", "chan basaruddin"], "accepted": false, "id": "1610.02373"}, "pdf": {"name": "1610.02373.pdf", "metadata": {"source": "CRF", "title": "Distributed Averaging CNN-ELM for Big Data", "authors": ["Arif Budiman", "Mohamad Ivan Fanany", "Chan Basaruddin"], "emails": ["intanurma@gmail.com"], "sections": [{"heading": null, "text": "Improving the scalability of machine learning in dealing with large amounts of data is a challenging task. The scaling approach has some limitations. In this paper, we propose a scaling approach for CNN-ELM based on MapReduce at the classification level. The mapping process is the CNN-ELM training for specific data partitions. It includes many CNN-ELM models that can be trained asynchronously. Reduction process is averaging all CNN-ELM weights as the final training result. This approach can save a lot of training time than individual CNN-ELM models that are trained alone. This approach also increases the scalability of machine learning by combining scaling and scaling approaches. We verified our method in extended MNIST data sets and non-MNIST data set experiments. However, it has some drawbacks due to additional iteration parameters that need to be captured carefully and the distribution of training data that need to be carefully selected."}, {"heading": "1 Introduction", "text": "It's about a massive growth in the amount of data in a single system, but the benefits of big data become meaningless if none of the parties involved are interested in processing data and can adapt to the data quickly enough. It's about the fact that the data has to be processed in a single data centre, so it's about much more than just processing it in a single data centre. How many new hardware and software technologies we need to get most hardware and software up and running effectively can be seen in the way that it's simple, in the way that the data is processed."}, {"heading": "2 Literature Reviews", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Parallel SGD and weight averaging", "text": "SGD is a very popular training algorithm for different models of machine learning, i.e., regression, SVM and NN. (\u03b2) Zinkevich et.al [12] proposed a parallel model of SGD based on MapReduce, which is very suitable for parallel and large-scale machine learning. In parallel SGD, however, the training data is retrieved locally from each model and only communicated when it is finished. (12] 1: Define P = bm / kc 2: Random distribution of training, giving P examples for each machine. (3: for all i-learning locations {1,..., k} parallel 4: Random breakdown of data on machine i 5: Initialize wi, 0 = 0 6: for all p-learning points {1,..., P} do 7: Get the pth training on the ith learning machine ci, p 8: wi, p-1 categorized learning on the machine iii."}, {"heading": "2.2 MapReduce in ELM", "text": "The ELM learning goal is the initial weight (\u03b2), which can be calculated by: \u03b2 = H \u2020 T. H \u2020 is a pseudoinverse (Moore-Penrose generalized inverse) function of H. The ELM learning goal is to find the smallest solution of the linear system H\u03b2 \u2212 Y, which can be achieved by generalizing \u03b2-Penrose. The ELM learning goal is the smallest solution of the linear system H\u03b2 \u2212 Y, which can be achieved when \u03b2-H \u2020 is a pseudoinverse (Moore-Penrose generalized inverse) function of H. The ELM learning goal is to find the smallest solution of the linear system H\u03b2 \u2212 Y, which can be achieved when \u03b2-H \u2020 T is a generalized (Moore-Penrose generalized) function of H. The ELM learning goal is to find the smallest solution of the linear system H\u03b2 \u2212 Y."}, {"heading": "2.3 MapReduce in CNN", "text": "A simple CNN system consists of several convolution layers and subsequent layers in the feed-forward architecture. CNN has excellent performance for spatial visual classification. The input layer represents the 2D structure with d \u00b7 d \u00b7 r of the image, and r is the number of input layers and subsequent layers. The convolution layer has c filters (or kernels) of size k \u00b7 k \u00b7 q, where k < d and q are either the same or smaller than the number of input channels r. The filters have locally connected structure, each associated with the image, to c feature maps of size d \u2212 k + 1. If we have the rth feature card as hr at a given layer, whose filters are determined by the weights, each is associated with the image to size d \u2212 k + 1."}, {"heading": "3 Proposed Method", "text": "We used the common CNN-ELM architecture [5, 8, 19] when the last folding layer is output as the hidden node weight H of the ELM (see Fig. 1). For better generalization accuracy, we used the nonlinear optimal Tanh activation function (1.7159 \u00d7 Tanh (23 \u00b7 H). We used the E2LM as a parallel monitored classifier to replace fully connected NN. Compared to regular ELM, we do not need an input weight as a hidden node parameter (see Fig. 2). The idea of reverse motion is similar to closely connected NN backpropagation error methods with cost function: J (\u03b2; z, t) = 12 \u0445 H (z) \u03b2 \u2212 T \u2022 2 (16) Then it spread back with SGD to optimize the weight cores of the folding layers (see Fig. 3).. Detail algorithm is explained on algorithm 2."}, {"heading": "4 Experiment and Performance Results", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data set", "text": "The MNIST dataset is a balanced dataset containing numerical (0-9) (10 target class) sizes of 28 x 28 pixels in a grayscale image; the dataset has been split for 60,000 examples of training data and separate 10,000 examples of test data [16]; we have expanded the MNIST dataset by 3 times by adding 3 types of image noise (see Figs. 4) to be 240,000 examples of training data and 40,000 examples of test data; for additional experiments, we have used the non-MNIST dataset for large amounts of data [2], which contains a lot of foolish images (see Figs. 5 and 6); non-MNIST has a gray scale of 28 x 28 image size as7 / 141: Define P = 1141: Define P = bm / kc 2: 1 / kc 2: Randomly partitioning the training and giving P examples for each machine."}, {"heading": "4.2 Experiment Methods", "text": "We defined the scope of the work as follows: 1. We expanded DeepLearn Toolbox [18] with Matlab Parallel Computing Toolbox.8 / 142. We used a single precision for all calculations. 3. We focused on a simple CNN architecture consisting of folding layers (c), followed by the activation layer (s) of the reLU and the pooling layer (s) with down-sampling in this paper. 4. We compared the performance in verifying accuracy with an undistributed sequential CNN-ELM classifier of similar structure size. To test our method, we formulated the following research questions: \u2022 How does the performance of a number of iterations follow? \u2022 What is the effectiveness of the weight average of the CNN-ELM model for different number of workouts? \u2022 What is the performance consistency of the weight on the average of the CNN-ELM model by number of iterations?"}, {"heading": "4.3 Performance Results", "text": "In this section, we explained the research questions as follows: \u2022 The performance of CNN-ELM can be improved by using back propagation algorithms. However, we need to select the appropriate parameter for the learning rate, the number of batches and the number of iterations that could affect the final performance (see Figure 7)."}, {"heading": "5 Conclusion", "text": "The proposed CNN-ELM method provides better scalability for the parallel processing of large data sets. We can distribute large data sets. We can assign CNN-ELM classifiers for each partition, then we simply aggregate the result by calculating the weights of all CNN-ELM parameters. Thus, it can save a lot of training time between 10 and 14 hours as sequential training. However, more CNN-ELM classifiers (smaller partition) have worse performance for the average of CNN-ELM as well as more iterations and data distribution effects. We think some ideas for future research: 11 / 14"}, {"heading": "6 Acknowledgment", "text": "This work is supported by the Higher Education Center of Excellence Research Grant under contract number 1068 / UN2.R12 / HKP5.00 / 2016, which is funded by the Indonesian Ministry of Research and Higher Education."}, {"heading": "7 Conflict of Interests", "text": "The authors declare that there is no conflict of interest regarding the publication of this paper.References1. B. Barney.2. Y. Bulatov. notmnist dataset, September 2011.3. J. Dean and S. Ghemawat. Mapreduce: Simplified data processing on large clusters. Commun. ACM, 51 (1): 107-113, 2018.4. J. Gu, Z. Wang, J. Kuen, L. Ma, A. Shahroudy, B. Shuai, T. Liu, X. Wang, and G. Wang. Recent advances in convolutional neural networks. CoRR, abs / 1512.07108. L. Guo and S. Ding. A hybrid deep learning cnn-elm model and its application in handwritten numeral recognition. pp. 2673-2680, 7 2015.6. Q. Er, T. Shang, F. Zhuang."}], "references": [{"title": "Mapreduce: Simplified data processing on large clusters", "author": ["J. Dean", "S. Ghemawat"], "venue": "Commun. ACM, 51(1):107\u2013113, Jan.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2008}, {"title": "Recent advances in convolutional neural networks", "author": ["J. Gu", "Z. Wang", "J. Kuen", "L. Ma", "A. Shahroudy", "B. Shuai", "T. Liu", "X. Wang", "G. Wang"], "venue": "CoRR, abs/1512.07108,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2015}, {"title": "A hybrid deep learning cnn-elm model and its application in handwritten numeral recognition", "author": ["L. Guo", "S. Ding"], "venue": "page 2673\u20132680, 7", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2015}, {"title": "Parallel extreme learning machine for regression based on mapreduce", "author": ["Q. He", "T. Shang", "F. Zhuang", "Z. Shi"], "venue": "Neurocomput., 102:52\u201358, Feb.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "Trends in extreme learning machines: A review", "author": ["G. Huang", "G.-B. Huang", "S. Song", "K. You"], "venue": "Neural Networks, 61(0):32 \u2013 48,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2015}, {"title": "Local receptive fields based extreme learning machine", "author": ["G.-B. Huang", "Z. Bai", "L.L.C. Kasun", "C.M. Vong"], "venue": "IEEE Computational Intelligence Magazine (accepted), 10,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2015}, {"title": "Extreme learning machines: a survey", "author": ["G.-B. Huang", "D. Wang", "Y. Lan"], "venue": "International Journal of Machine Learning and Cybernetics, 2(2):107\u2013122,", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2011}, {"title": "Extreme learning machine: theory and applications", "author": ["G.-B. Huang", "Q.Y. Zhu", "C.K. Siew"], "venue": "Neurocomputing, 70(1-3):489\u2013501,", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2006}, {"title": "An efficient learning scheme for extreme learning machine and its application", "author": ["Miso Jang", "S.-Y. Min"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, editors, Advances in Neural Information Processing Systems 25, pages 1097\u20131105. Curran Associates, Inc.,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2012}, {"title": "3D data management: Controlling data volume, velocity, and variety", "author": ["D. Laney"], "venue": "Technical report, META Group, February", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2001}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "In S. Haykin and B. Kosko, editors, Intelligent Signal Processing, pages 306\u2013351. IEEE Press,", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2001}, {"title": "Effiicient backprop", "author": ["Y. LeCun", "L. Bottou", "G.B. Orr", "K.-R. M\u00fcller"], "venue": "In Neural Networks: Tricks of the Trade, This Book is an Outgrowth of a 1996 NIPS Workshop, pages 9\u201350, London, UK, UK,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 1998}, {"title": "and C", "author": ["Y. LeCu"], "venue": "Cortes. Mnist handwritten digit database,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "A fast and accurate online sequential learning algorithm for feedforward networks", "author": ["N.-Y. Liang", "G.-B. Huang", "P. Saratchandran", "N. Sundararajan"], "venue": "Neural Networks, IEEE Transactions on, 17(6):1411\u20131423, Nov", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2006}, {"title": "Deep convolutional extreme learning machine and its application in handwritten digit classification", "author": ["S. Pang", "X. Yang"], "venue": "Hindawi,", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "Acceleration of stochastic approximation by averaging", "author": ["B.T. Polyak", "A.B. Juditsky"], "venue": "SIAM Journal on Control and Optimization, 30(4):838\u2013855,", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1992}, {"title": "Accelerating Large-Scale Convolutional Neural Networks with Parallel Graphics Multiprocessors, pages 82\u201391", "author": ["D. Scherer", "H. Schulz", "S. Behnke"], "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg,", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2010}, {"title": "Best practices for convolutional neural networks applied to visual document analysis", "author": ["P.Y. Simard", "D. Steinkraus", "J.C. Platt"], "venue": "In Proceedings of the Seventh International Conference on Document Analysis and Recognition Volume 2, ICDAR \u201903, pages 958\u2013, Washington, DC, USA,", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2003}, {"title": "A survey on platforms for big data analytics", "author": ["D. Singh", "C.K. Reddy"], "venue": "Journal of Big Data, 2(1):1\u201320,", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Map-reduce for machine learning on multicore", "author": ["C. tao Chu", "S.K. Kim", "Y. an Lin", "Y. Yu", "G. Bradski", "K. Olukotun", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2007}, {"title": "Map-reduce as a programming model for custom computing machines", "author": ["C. Tsang", "K. Tsoi", "J.H. Yeung", "B.S. Kwan", "A.P. Chan", "C.C. Cheung", "P.H. Leong"], "venue": "Field-Programmable Custom Computing Machines, Annual IEEE Symposium on, 00(undefined):149\u2013159,", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2008}, {"title": "Parallelizing convolutional neural networks for action event recognition in surveillance videos", "author": ["Q. Wang", "J. Zhao", "D. Gong", "Y. Shen", "M. Li", "Y. Lei"], "venue": "International Journal of Parallel Programming, pages 1\u201326,", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2016}, {"title": "Elastic extreme learning machine for big data classification", "author": ["J. Xin", "Z. Wang", "L. Qu", "G. Wang"], "venue": "Neurocomputing, 149, Part A:464 \u2013 471,", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing and Understanding Convolutional Networks, pages 818\u2013833", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "Springer International Publishing, Cham,", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2014}, {"title": "Parallelized stochastic gradient descent", "author": ["M. Zinkevich", "M. Weimer", "L. Li", "A.J. Smola"], "venue": "In J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta, editors, Advances in Neural Information Processing Systems 23, pages 2595\u20132603. Curran Associates, Inc.,", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 10, "context": "Volume and velocity issues are critical in overcoming big data challenges [13].", "startOffset": 74, "endOffset": 78}, {"referenceID": 19, "context": "It can be categorized into the following two types of scalability [23]:", "startOffset": 66, "endOffset": 70}, {"referenceID": 0, "context": "To overcome overhead complexities in parallel programming, Google introduced a programming model named MapReduce [3].", "startOffset": 113, "endOffset": 116}, {"referenceID": 21, "context": "MapReduce provides two essential functions: 1) Map function, it processes each sub problems to another nodes within the cluster; 2) Reduce function, it organizes the results from each node to be a cohesive solution [25].", "startOffset": 215, "endOffset": 219}, {"referenceID": 21, "context": "Developing MapReduce is simple by firstly exposing structure and process similarity and then aggregation process [25].", "startOffset": 113, "endOffset": 117}, {"referenceID": 20, "context": ", locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].", "startOffset": 221, "endOffset": 225}, {"referenceID": 25, "context": ", locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].", "startOffset": 261, "endOffset": 265}, {"referenceID": 22, "context": ", locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].", "startOffset": 302, "endOffset": 306}, {"referenceID": 23, "context": ", locally weighted linear regression (LWLR), k-means, logistic regression (LR), naive Bayes (NB), support vector machine (SVM), gaussian discriminant analysis (GDA), expectation\u2013maximization (EM) and backpropagation (NN) [24], stochastic gradient descent (SGD) [29], convolutional neural network (CNN) [26], extreme learning machine (ELM) [27].", "startOffset": 339, "endOffset": 343}, {"referenceID": 11, "context": "CNN [14] is a popular machine learning that getting benefits from parallel computation.", "startOffset": 4, "endOffset": 8}, {"referenceID": 9, "context": "However, the scale up approach still has limitation mainly caused by the amount of memory available on GPUs [12,21].", "startOffset": 108, "endOffset": 115}, {"referenceID": 17, "context": "However, the scale up approach still has limitation mainly caused by the amount of memory available on GPUs [12,21].", "startOffset": 108, "endOffset": 115}, {"referenceID": 1, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 35, "endOffset": 45}, {"referenceID": 11, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 35, "endOffset": 45}, {"referenceID": 24, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 35, "endOffset": 45}, {"referenceID": 4, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 55, "endOffset": 67}, {"referenceID": 6, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 55, "endOffset": 67}, {"referenceID": 7, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 55, "endOffset": 67}, {"referenceID": 23, "context": "We integrated the CNN architecture [4, 14,28] with ELM [7,9, 10,27].", "startOffset": 55, "endOffset": 67}, {"referenceID": 25, "context": "We employed parallel stochastic gradient descent (SGD) algorithm [29] to fine tune the weights of CNN-ELM and to average the final weights of CNN-ELM.", "startOffset": 65, "endOffset": 69}, {"referenceID": 9, "context": "al [12] proposed a parallel model of SGD based on MapReduce that highly suitable for parallel and large-scale machine learning.", "startOffset": 3, "endOffset": 7}, {"referenceID": 9, "context": "[12] 1: Define P = bm/kc 2: Randomly partition the training, giving P examples to each machine.", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "al [20].", "startOffset": 3, "endOffset": 7}, {"referenceID": 4, "context": "2 MapReduce in ELM Extreme Learning Machine (ELM) is one of the famous machine learning that firstly proposed by Huang [7, 9, 10].", "startOffset": 119, "endOffset": 129}, {"referenceID": 6, "context": "2 MapReduce in ELM Extreme Learning Machine (ELM) is one of the famous machine learning that firstly proposed by Huang [7, 9, 10].", "startOffset": 119, "endOffset": 129}, {"referenceID": 7, "context": "2 MapReduce in ELM Extreme Learning Machine (ELM) is one of the famous machine learning that firstly proposed by Huang [7, 9, 10].", "startOffset": 119, "endOffset": 129}, {"referenceID": 14, "context": "2 can be solved by sequential series using block matrices inverse (A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [17]) or by MapReduce approach (Elastic Extreme Learning Machine (ELM) [27] or Parallel ELM [6]).", "startOffset": 163, "endOffset": 167}, {"referenceID": 23, "context": "2 can be solved by sequential series using block matrices inverse (A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [17]) or by MapReduce approach (Elastic Extreme Learning Machine (ELM) [27] or Parallel ELM [6]).", "startOffset": 234, "endOffset": 238}, {"referenceID": 3, "context": "2 can be solved by sequential series using block matrices inverse (A Fast and Accurate Online Sequential named online sequential extreme learning machine (OS-ELM) [17]) or by MapReduce approach (Elastic Extreme Learning Machine (ELM) [27] or Parallel ELM [6]).", "startOffset": 255, "endOffset": 258}, {"referenceID": 23, "context": "Therefore, MapReduce based ELM is more efficient for massive training data set, can be solved easily by parallel computation and has better performance [27].", "startOffset": 152, "endOffset": 156}, {"referenceID": 8, "context": "al [11] explained on BP Trained Weight-based ELM that the optimized input weights with BP training is more feasible than randomly assigned weights.", "startOffset": 3, "endOffset": 7}, {"referenceID": 11, "context": "3 MapReduce in CNN CNN is biologically-inspired [14] from visual cortex that has a convolution arrangement of cells (a receptive field that sensitive to small sub visual field and local filter) and following by simple cells that only respond maximally to specific trigger within receptive fields.", "startOffset": 48, "endOffset": 52}, {"referenceID": 18, "context": "CNN has excellent performance for spatial visual classification [22].", "startOffset": 64, "endOffset": 68}, {"referenceID": 24, "context": "1) [28].", "startOffset": 3, "endOffset": 7}, {"referenceID": 1, "context": "Many variants of CNN architectures in the literatures, but the basic common building blocks are convolutional layer, pooling layer and fully connected layer [4].", "startOffset": 157, "endOffset": 160}, {"referenceID": 17, "context": ", GPU [21].", "startOffset": 6, "endOffset": 10}, {"referenceID": 9, "context": "However, the GPU has memory size limitation that limit the CNN network size to achieve better accuracy [12].", "startOffset": 103, "endOffset": 107}, {"referenceID": 17, "context": "Most CNN implementations are using GPU [21] to speed up convolution operation that required hundred numbers of core processors.", "startOffset": 39, "endOffset": 43}, {"referenceID": 22, "context": "al [26] used MapReduce on Hadoop platform to take advantage of the computing power of multi core CPU to solve matrix parallel computation.", "startOffset": 3, "endOffset": 7}, {"referenceID": 17, "context": "al [21] explained because shared memory is very limited, so it reuses loaded data as often as possible.", "startOffset": 3, "endOffset": 7}, {"referenceID": 2, "context": "We used common CNN-ELM integration [5, 8, 19] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 45}, {"referenceID": 5, "context": "We used common CNN-ELM integration [5, 8, 19] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 45}, {"referenceID": 15, "context": "We used common CNN-ELM integration [5, 8, 19] architecture when the last convolution layer output is fed as hidden nodes weight H of ELM (See Fig.", "startOffset": 35, "endOffset": 45}, {"referenceID": 12, "context": "7159\u00d7 tanh( 23 \u00d7H) activation function [15].", "startOffset": 39, "endOffset": 43}, {"referenceID": 13, "context": "The dataset has been divided for 60,000 examples for training data and separated 10,000 examples for testing data [16].", "startOffset": 114, "endOffset": 118}], "year": 2016, "abstractText": "Increasing the scalability of machine learning to handle big volume of data is a challenging task. The scale up approach has some limitations. In this paper, we proposed a scale out approach for CNN-ELM based on MapReduce on classifier level. Map process is the CNN-ELM training for certain partition of data. It involves many CNN-ELM models that can be trained asynchronously. Reduce process is the averaging of all CNN-ELM weights as final training result. This approach can save a lot of training time than single CNN-ELM models trained alone. This approach also increased the scalability of machine learning by combining scale out and scale up approaches. We verified our method in extended MNIST data set and not-MNIST data set experiment. However, it has some drawbacks by additional iteration learning parameters that need to be carefully taken and training data distribution that need to be carefully selected. Further researches to use more complex image data set are required. Keywords\u2014 deep learning, extreme learning machine, convolutional, neural network, big data, map reduce", "creator": "LaTeX with hyperref package"}}}