{"id": "1610.02391", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization", "abstract": "We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are \"important\" for predictions from these models - or visual explanations.", "histories": [["v1", "Fri, 7 Oct 2016 19:54:24 GMT  (8245kb,D)", "http://arxiv.org/abs/1610.02391v1", "17 pages, 16 figures"], ["v2", "Fri, 30 Dec 2016 07:19:35 GMT  (8596kb,D)", "http://arxiv.org/abs/1610.02391v2", "22 pages, 19 figures"], ["v3", "Tue, 21 Mar 2017 23:48:00 GMT  (9133kb,D)", "http://arxiv.org/abs/1610.02391v3", "24 pages, 22 figures. Adds bias experiments, and robustness to adversarial noise"]], "COMMENTS": "17 pages, 16 figures", "reviews": [], "SUBJECTS": "cs.CV cs.AI cs.LG", "authors": ["ramprasaath r selvaraju", "michael cogswell", "abhishek das", "ramakrishna vedantam", "devi parikh", "dhruv batra"], "accepted": false, "id": "1610.02391"}, "pdf": {"name": "1610.02391.pdf", "metadata": {"source": "CRF", "title": "Grad-CAM: Why did you say that? Visual Explanations from Deep Networks via Gradient-based Localization", "authors": ["Ramprasaath R. Selvaraju", "Abhishek Das", "Ramakrishna Vedantam", "Michael Cogswell", "Devi Parikh", "Dhruv Batra", "Virginia Tech"], "emails": ["dbatra}@vt.edu"], "sections": [{"heading": null, "text": "Our approach, called Gradient-weighted Class Activation Mapping (Grade-CAM), uses the class-specific gradient information that flows into the last revolutionary layer of a CNN to generate a rough localization map of the important regions in the image. Grad-CAM is a strict generalization of Class Activation Mapping (CAM) [43]. While CAM is limited to a narrow class of CNN models, Grad-CAM is generally applicable to all CNN-based architectures. We also show how Grad-CAM can be combined with existing pixel-space visualizations (such as Guided Backpropagation [38]) to produce a high-resolution class discriminatory visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to answer image classification, image captioning, and visual questions (VQA) that exceed visualization models (QA)."}, {"heading": "1. Introduction", "text": "In fact, most people who are able to move are able to move, to move, to move and to move, to move, to move, to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move, to move and to move, to move and to move, to move and to move, to move and to move."}, {"heading": "2. Related Work", "text": "A number of previous work [36, 38, 41] have visualized predictions by CNN, highlighting \"important\" pixels (i.e., the change in the intensities of these pixels have the greatest impact on the results of the prediction). In particular, Simonyan et al., \"Visualizing partial derivatives of predicted class outcomes.\" Pixel intensities, while Guided Backpropagation [38] and Deconvolution [41] make changes to \"raw\" gradients that lead to qualitative improvements. Despite producing fine-grained visualizations, these methods are not class discriminatory. Visualizations relating to different classes are nearly identical (see Figures 1b and 1g). Other visualization methods synthesize images to achieve maximum prediction."}, {"heading": "3. Approach", "text": "It is. (It is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it is.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it.) It is. (it. (it.) It is. (it.) It. (it. (it.) It. (it.) It. (it. (it.) is. (it.) It. (it. (it.) It. (it. (it.) is. (it. (it.) It. (it.) It. (it. (it.) It. (it. (it.) is. (it. (it.) It. (it.) It. (it. (it.) It. (it.) is. (it. (it.) It. (it. It. (it. (it. It. It. (it.) It. (it.) is. It. (it. (it.) It. (it. (it.) It.) is. (it. (it.) It. (it. (it. It. (it.) is. (it.) It. (it. (it. It. It. (it.) It. (it.) It. (it. (it. (it. It.) is. It. It. It. (it. It.) is. (it. It. (it. (it.) is. It. (it. It. It. It. It. It. It. It. It. It. It. It. (it. It"}, {"heading": "4. Weakly-supervised Localization", "text": "In this section, we evaluate the localization capability of grade CAM in the context of image classification. The ImageNet localization challenge [9] requires competing approaches to provide delimitation fields in addition to classification labeling. Similar to classification, the evaluation is done for both the top 1 and the top 5 predicted categories. Similar to Zhou et al. [43], for an image, we first get class predictions from our network. Next, we create grade CAM localization maps for each of the predicted classes and binarize with a threshold of 15% of the maximum intensity. This results in contiguous pixel segments and we tighten our delimitation box by the largest single segment. We evaluate the pre-built standard model VGG-16 [37] from the Caffe [17] model Zoo. Following the ILSVRC evaluation, we report both top 1 and top 5 localization errors from the table."}, {"heading": "5. Evaluating Visualizations", "text": "Our first human study explores the main premise of our approach: Are grade CAM visualizations more class discriminatory than previous techniques? In addition, we want to understand whether our class discriminatory interpretations can induce an end user to trust the visualized models. In these experiments, we use CNNs from VGG and AlexNet tuned to PASCAL VOC 2007, and the validation set is used to generate visualizations."}, {"heading": "5.1. Evaluating Class Discrimination", "text": "We select images from the PASCAL VOC 2007 set that contain exactly two annotated categories, and create visualizations for one of the classes. For both VGG-16 and AlexNet CNNs, we get visualizations using four techniques: deconvolution, guided back propagation, and degree CAM versions of each of these methods (deconvolution grade CAM and guided grade CAM). We show the workers on Amazon Mechanical Turk (AMT) visualizations and ask them, \"Which of the two object categories is represented in the image?,\" as shown in Fig. 4a. The two PASCAL categories shown in the original image are presented as options, which task measures whether people can recognize which class is visualized from the visualization. Similarly, the category that is visualized in 61.23% of cases (compared with 44.44% for guided back propagation; therefore, grade CAM improves human perfection by 16.79%), we find that backward CAM is stronger in our class, although it is the best in backward dance."}, {"heading": "5.2. Evaluating Trust", "text": "In fact, most of them are able to survive on their own if they do not follow the rules they have imposed on themselves. In fact, it is that they are able to survive on their own, and that they are able to survive on their own."}, {"heading": "6. Analyzing Failure Modes for Image Classification", "text": "To see what errors a network makes, we first get a list of examples that the network (VGG-16) does not correctly classify. For the incorrectly classified examples, we use Guided Grad-CAM to visualize both the correct class and the predicted class. A great advantage of Guided Grad-CAM over other methods is its ability to more usefully examine and explain classification errors, since our visualizations are more high-resolution and class discriminatory. As shown in Fig. 5, some errors are due to ambiguities inherent in the ImageNet classification. We can also see that seemingly unreasonable predictions have reasonable explanations, which is a similar observation to HOGgles [40]."}, {"heading": "7. Image Captioning and VQA", "text": "In this section, we apply our grade CAM technique to the image captions [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks. We find that grade CAM leads to interpretable visual explanations for these tasks, compared to visualizations that do not change noticeably across different predictions."}, {"heading": "7.1. Image Captioning", "text": "In this section, we visualize the spatial support for a simple caption model (without attention) using degree CAM visualizations. Specifically, we build on the publicly available implementation of \"neuraltalk2\" 3, which uses a finetuned VGG-16 CNN for images and an LSTM-based language model. Based on a caption, we calculate the gradient of its log probability w.r.t. units in the last revolutionary layer of CNN (Conv5 _ 3 for VGG-16) and generate degree CAM visualizations as in Section 3. The results are shown in Fig. 6a. For example, the first time, the degree CAM maps for the generated caption localizes each occurrence of both the dragons and humans despite their relatively small size. In the upper right example, we see how degree CAM models correctly highlight the spatial overlay of density and the man, but the woman in the vicinity is not mentioned."}, {"heading": "7.2. Visual Question Answering", "text": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions. The image and questions are fused together to predict the answer (typically a distribution of the top 1000 common answers in the VQA train set). As it is a classification problem, visualization is similar to the case of image classification: in the face of an image and a question, choose an answer and use Grad-CAM to show image proofs that support the answer. Qualitative examples. We explain the answers of a VQA model [27] that combines a CNN and an LSTM representation by a pointed multiplication. Despite the complexity of the task involving both visual and linguistic components, the evidence found by Grad-CAM is surprisingly intuitive and informative. Consider, for example, the image in Figure 7, the type of the question paired with the color of the fire hydrant?"}, {"heading": "8. Conclusion", "text": "In this paper, we proposed a novel class-discriminatory localization technology - Gradient-weighted Class Activation Mapping (Grad-CAM) - to make CNN-based models more transparent through visual explanations. Our technique achieved better localization accuracy than Simonyan et al at the ILSVRC val. [36] In addition, we combined our Grad-CAM localizations with existing high-resolution visualizations that exhibit poor localization capability to obtain high-resolution class discriminatory visualizations, Guided GradCAM. Extensive human studies with visualizations show that our localization-enhanced visualizations can more accurately distinguish between classes and better reveal the reliability of a classifier. Finally, we provide some quantitative and qualitative results to interpret predictions from image classification, visual questions, and image labeling. We believe that a true KI system should not only be intelligent, but should be able to trust in its location and ability."}, {"heading": "B. Experimental Results", "text": "In this section we provide more qualitative results for grade CAM and guided grade CAM. We only have the task of image classification, caption and VQA.B.1. Image ClassificationFigure A1: Visualizations for randomly sampled images from the COCO validation datasets. Predicted classes are mentioned at the top of each column. We use grade CAM and guided grade CAM to visualize the regions of the image that provide support for a specific prediction. The results below correspond to the VGG-16 [37] network that is trained on ImageNet. Fig. A1 shows randomly sampled examples from COCO [24] validation set. COCO images usually have multiple objects per image and grade CAM visualizations show precise localization to support prediction of the model. Guided grade CAM can locate even tiny objects."}, {"heading": "C. Ablation studies", "text": "In this context, it is also worth mentioning the fact that most of them are people who are unable to position themselves in public."}], "references": [{"title": "Analyzing the Behavior of Visual Question Answering Models", "author": ["A. Agrawal", "D. Batra", "D. Parikh"], "venue": "EMNLP", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2016}, {"title": "CloudCV: Large Scale Distributed Computer Vision as a Cloud Service", "author": ["H. Agrawal", "C.S. Mathialagan", "Y. Goyal", "N. Chavali", "P. Banik", "A. Mohapatra", "A. Osman", "D. Batra"], "venue": "Mobile Cloud Visual Media Computing, pages 265\u2013290. Springer", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "VQA: Visual Question Answering", "author": ["S. Antol", "A. Agrawal", "J. Lu", "M. Mitchell", "D. Batra", "C. Lawrence Zitnick", "D. Parikh"], "venue": "ICCV", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Self-taught object localization with deep networks", "author": ["L. Bazzani", "A. Bergamo", "D. Anguelov", "L. Torresani"], "venue": "WACV", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2016}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2013}, {"title": "Microsoft COCO captions: Data Collection and Evaluation Server", "author": ["X. Chen", "H. Fang", "T.-Y. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "arXiv preprint arXiv:1504.00325", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2015}, {"title": "Weakly supervised object localization with multi-fold multiple instance learning", "author": ["R.G. Cinbis", "J. Verbeek", "C. Schmid"], "venue": "IEEE transactions on pattern analysis and machine intelligence", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2016}, {"title": "and D", "author": ["A. Das", "H. Agrawal", "C.L. Zitnick", "D. Parikh"], "venue": "Batra. Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions? In EMNLP", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2016}, {"title": "ImageNet: A Large-Scale Hierarchical Image Database", "author": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2009}, {"title": "Inverting Convolutional Networks with Convolutional Networks", "author": ["A. Dosovitskiy", "T. Brox"], "venue": "CVPR", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing Higherlayer Features of a Deep Network", "author": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "venue": "University of Montreal, 1341", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "et al", "author": ["H. Fang", "S. Gupta", "F. Iandola", "R.K. Srivastava", "L. Deng", "P. Doll\u00e1r", "J. Gao", "X. He", "M. Mitchell", "J.C. Platt"], "venue": "From Captions to Visual Concepts and Back. In CVPR", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question Answering", "author": ["H. Gao", "J. Mao", "J. Zhou", "Z. Huang", "L. Wang", "W. Xu"], "venue": "NIPS", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation", "author": ["R. Girshick", "J. Donahue", "T. Darrell", "J. Malik"], "venue": "CVPR", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2014}, {"title": "Diagnosing Error in Object Detectors", "author": ["D. Hoiem", "Y. Chodpathumwan", "Q. Dai"], "venue": "ECCV", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2012}, {"title": "Introduction to Expert Systems", "author": ["P. Jackson"], "venue": "Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 3rd edition", "citeRegEx": "16", "shortCiteRegEx": null, "year": 1998}, {"title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "author": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "venue": "ACM MM", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Becoming the Expert - Interactive Multi-Class Machine Teaching", "author": ["E. Johns", "O. Mac Aodha", "G.J. Brostow"], "venue": "CVPR", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Densecap: Fully convolutional localization networks for dense captioning", "author": ["J. Johnson", "A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2016}, {"title": "What I learned from competing against a ConvNet on ImageNet", "author": ["A. Karpathy"], "venue": "http://karpathy.github.io/2014/09/02/what-i-learned-fromcompeting-against-a-convnet-on-imagenet/", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "CVPR", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "NIPS", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Network in network", "author": ["M. Lin", "Q. Chen", "S. Yan"], "venue": "ICLR", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2014}, {"title": "Microsoft coco: Common objects in context", "author": ["T.-Y. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": "In ECCV", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2014}, {"title": "The Mythos of Model Interpretability", "author": ["Z.C. Lipton"], "venue": "ArXiv e-prints, June 2016", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "Fully convolutional networks for semantic segmentation", "author": ["J. Long", "E. Shelhamer", "T. Darrell"], "venue": "CVPR", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2015}, {"title": "Deeper LSTM and normalized CNN Visual Question Answering model", "author": ["J. Lu", "X. Lin", "D. Batra", "D. Parikh"], "venue": "https://github. com/VT-vision-lab/VQA_LSTM_CNN", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Visualizing deep convolutional neural networks using natural pre-images", "author": ["A. Mahendran", "A. Vedaldi"], "venue": "International Journal of Computer Vision, pages 1\u201323", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2016}, {"title": "Ask your neurons: A neural-based approach to answering questions about images", "author": ["M. Malinowski", "M. Rohrbach", "M. Fritz"], "venue": "ICCV", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Learning and transferring mid-level image representations using convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "CVPR", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Is object localization for free? \u2013 weakly-supervised learning with convolutional neural networks", "author": ["M. Oquab", "L. Bottou", "I. Laptev", "J. Sivic"], "venue": "CVPR", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2015}, {"title": "From image-level to pixel-level labeling with convolutional networks", "author": ["P.O. Pinheiro", "R. Collobert"], "venue": "CVPR", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2015}, {"title": "Exploring models and data for image question answering", "author": ["M. Ren", "R. Kiros", "R. Zemel"], "venue": "NIPS", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2015}, {"title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "author": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "venue": "SIGKDD", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2016}, {"title": "G", "author": ["D. Silver", "A. Huang", "C.J. Maddison", "A. Guez", "L. Sifre"], "venue": "Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, et al. Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484\u2013489", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2016}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "author": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "venue": "CoRR, abs/1312.6034", "citeRegEx": "36", "shortCiteRegEx": null, "year": 2013}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "ICLR", "citeRegEx": "37", "shortCiteRegEx": null, "year": 2015}, {"title": "Striving for Simplicity: The All Convolutional Net", "author": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M.A. Riedmiller"], "venue": "CoRR, abs/1412.6806", "citeRegEx": "38", "shortCiteRegEx": null, "year": 2014}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CVPR", "citeRegEx": "39", "shortCiteRegEx": null, "year": 2015}, {"title": "HOGgles: Visualizing Object Detection Features", "author": ["C. Vondrick", "A. Khosla", "T. Malisiewicz", "A. Torralba"], "venue": "ICCV", "citeRegEx": "40", "shortCiteRegEx": null, "year": 2013}, {"title": "Visualizing and understanding convolutional networks", "author": ["M.D. Zeiler", "R. Fergus"], "venue": "ECCV", "citeRegEx": "41", "shortCiteRegEx": null, "year": 2014}, {"title": "Top-down Neural Attention by Excitation Backprop", "author": ["J. Zhang", "Z. Lin", "J. Brandt", "X. Shen", "S. Sclaroff"], "venue": "ECCV", "citeRegEx": "42", "shortCiteRegEx": null, "year": 2016}], "referenceMentions": [{"referenceID": 37, "context": "(b) Guided Backpropagation [38]:", "startOffset": 27, "endOffset": 31}, {"referenceID": 37, "context": "We also show how Grad-CAM may be combined with existing pixel-space visualizations (such as Guided Backpropagation [38]) to create a high-resolution class-discriminative visualization (Guided Grad-CAM).", "startOffset": 115, "endOffset": 119}, {"referenceID": 37, "context": "In the context of image classification models, our visualizations (a) lend insight into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), and (b) outperform pixel-space gradient visualizations (Guided Backpropagation [38] and Deconvolution [41]) on the ILSVRC-15 weakly supervised localization task.", "startOffset": 277, "endOffset": 281}, {"referenceID": 40, "context": "In the context of image classification models, our visualizations (a) lend insight into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), and (b) outperform pixel-space gradient visualizations (Guided Backpropagation [38] and Deconvolution [41]) on the ILSVRC-15 weakly supervised localization task.", "startOffset": 300, "endOffset": 304}, {"referenceID": 1, "context": "com/ramprs/ grad-cam/ and a demo is available on CloudCV [2]1.", "startOffset": 57, "endOffset": 60}, {"referenceID": 21, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 167, "endOffset": 171}, {"referenceID": 13, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 192, "endOffset": 196}, {"referenceID": 25, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 220, "endOffset": 224}, {"referenceID": 5, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 243, "endOffset": 254}, {"referenceID": 11, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 243, "endOffset": 254}, {"referenceID": 18, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 243, "endOffset": 254}, {"referenceID": 2, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 301, "endOffset": 316}, {"referenceID": 12, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 301, "endOffset": 316}, {"referenceID": 28, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 301, "endOffset": 316}, {"referenceID": 32, "context": "Convolutional Neural Networks (CNNs) and other deep networks have enabled unprecedented breakthroughs in a variety of computer vision tasks, from image classification [22] to object detection [14], semantic segmentation [26], image captioning [6, 12, 19], and more recently, visual question answering [3, 13, 29, 33].", "startOffset": 301, "endOffset": 316}, {"referenceID": 24, "context": "While these deep neural networks enable superior performance, their lack of decomposability into intuitive and understandable components makes them hard to interpret [25].", "startOffset": 166, "endOffset": 170}, {"referenceID": 2, "context": "visual question answering [3]), the goal of transparency & explanations is to identify the failure modes [1, 15], thereby helping researchers focus their efforts on the most fruitful research directions.", "startOffset": 26, "endOffset": 29}, {"referenceID": 0, "context": "visual question answering [3]), the goal of transparency & explanations is to identify the failure modes [1, 15], thereby helping researchers focus their efforts on the most fruitful research directions.", "startOffset": 105, "endOffset": 112}, {"referenceID": 14, "context": "visual question answering [3]), the goal of transparency & explanations is to identify the failure modes [1, 15], thereby helping researchers focus their efforts on the most fruitful research directions.", "startOffset": 105, "endOffset": 112}, {"referenceID": 19, "context": ", image classification [20] on a set of categories with enough training data), the goal is to establish trust with users.", "startOffset": 23, "endOffset": 27}, {"referenceID": 34, "context": "chess or Go playing bots [35]), the goal of transparency & explanations is machine teaching [18] \u2013 i.", "startOffset": 25, "endOffset": 29}, {"referenceID": 17, "context": "chess or Go playing bots [35]), the goal of transparency & explanations is machine teaching [18] \u2013 i.", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "Classical rule-based or expert systems [16] were highly interpretable but not very accurate (or robust).", "startOffset": 39, "endOffset": 43}, {"referenceID": 8, "context": "What makes a good visual explanation? Consider image classification [9] \u2013 a \u2018good\u2019 visual explanation from the model justifying a predicted class should be (a) classdiscriminative (i.", "startOffset": 68, "endOffset": 71}, {"referenceID": 37, "context": "Pixel-space gradient visualizations such as Guided Backpropagation [38] and Deconvolution [41] are high-resolution and highlight fine-grained details in the image, but are not class-discriminative (for example, the visualization for both \u2018cat\u2019 and \u2018dog\u2019 in Figures 1b and 1g are very similar).", "startOffset": 67, "endOffset": 71}, {"referenceID": 40, "context": "Pixel-space gradient visualizations such as Guided Backpropagation [38] and Deconvolution [41] are high-resolution and highlight fine-grained details in the image, but are not class-discriminative (for example, the visualization for both \u2018cat\u2019 and \u2018dog\u2019 in Figures 1b and 1g are very similar).", "startOffset": 90, "endOffset": 94}, {"referenceID": 36, "context": "14 \u00d7 14 in VGGNet [37]) and hence does not show fine-grained details.", "startOffset": 18, "endOffset": 22}, {"referenceID": 35, "context": "A number of previous works [36, 38, 41] have visualized CNN predictions by highlighting \u2018important\u2019 pixels (i.", "startOffset": 27, "endOffset": 39}, {"referenceID": 37, "context": "A number of previous works [36, 38, 41] have visualized CNN predictions by highlighting \u2018important\u2019 pixels (i.", "startOffset": 27, "endOffset": 39}, {"referenceID": 40, "context": "A number of previous works [36, 38, 41] have visualized CNN predictions by highlighting \u2018important\u2019 pixels (i.", "startOffset": 27, "endOffset": 39}, {"referenceID": 35, "context": "[36] visualize partial derivatives of predicted class scores w.", "startOffset": 0, "endOffset": 4}, {"referenceID": 37, "context": "pixel intensities, while Guided Backpropagation [38] and Deconvolution [41] make modifications to \u2018raw\u2019 gradients that result in qualitative improvements.", "startOffset": 48, "endOffset": 52}, {"referenceID": 40, "context": "pixel intensities, while Guided Backpropagation [38] and Deconvolution [41] make modifications to \u2018raw\u2019 gradients that result in qualitative improvements.", "startOffset": 71, "endOffset": 75}, {"referenceID": 35, "context": "Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10].", "startOffset": 83, "endOffset": 91}, {"referenceID": 10, "context": "Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10].", "startOffset": 83, "endOffset": 91}, {"referenceID": 27, "context": "Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10].", "startOffset": 126, "endOffset": 134}, {"referenceID": 9, "context": "Other visualization methods synthesize images to maximally activate a network unit [36, 11] or invert a latent representation [28, 10].", "startOffset": 126, "endOffset": 134}, {"referenceID": 24, "context": "Motivated by notions of interpretability [25] and assessing trust in models [34], we evaluate Grad-CAM visualizations in a manner similar to Ribeiro et al.", "startOffset": 41, "endOffset": 45}, {"referenceID": 33, "context": "Motivated by notions of interpretability [25] and assessing trust in models [34], we evaluate Grad-CAM visualizations in a manner similar to Ribeiro et al.", "startOffset": 76, "endOffset": 80}, {"referenceID": 33, "context": "[34] via human studies to show that they can be important tools for users to evaluate and place trust in automated systems.", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Another relevant line of work is weakly supervised localization in the context of CNNs, where the task is to localize objects in images using only whole image class labels [7, 30, 31, 43].", "startOffset": 172, "endOffset": 187}, {"referenceID": 29, "context": "Another relevant line of work is weakly supervised localization in the context of CNNs, where the task is to localize objects in images using only whole image class labels [7, 30, 31, 43].", "startOffset": 172, "endOffset": 187}, {"referenceID": 30, "context": "Another relevant line of work is weakly supervised localization in the context of CNNs, where the task is to localize objects in images using only whole image class labels [7, 30, 31, 43].", "startOffset": 172, "endOffset": 187}, {"referenceID": 22, "context": "This approach modifies image classification CNN architectures replacing fully-connected layers with convolutional layers and global average pooling [23], thus achieving class-specific feature maps.", "startOffset": 148, "endOffset": 152}, {"referenceID": 30, "context": "Others have investigated similar methods using global max pooling [31] and log-sum-exp pooling [32].", "startOffset": 66, "endOffset": 70}, {"referenceID": 31, "context": "Others have investigated similar methods using global max pooling [31] and log-sum-exp pooling [32].", "startOffset": 95, "endOffset": 99}, {"referenceID": 40, "context": "Zeiler and Fergus [41] perturb inputs by occluding small patches and classifying the occluded image, typically resulting in lower classification scores for relevant objects when those objects are occluded.", "startOffset": 18, "endOffset": 22}, {"referenceID": 3, "context": "This principle is applied for localization in [4].", "startOffset": 46, "endOffset": 49}, {"referenceID": 29, "context": "[30] classify many patches containing a pixel then average these patch class-wise scores to provide the pixel\u2019s class-wise score.", "startOffset": 0, "endOffset": 4}, {"referenceID": 41, "context": "[42] introduce a probabilistic Winner-Take-All formulation for modelling the top-down attention for neural classification models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 36, "context": "Notice that this results in a coarse heat-map of the same size as the convolutional feature maps (14\u00d7 14 in the case of last convolutional layers of VGG [37] and AlexNet [22] networks).", "startOffset": 153, "endOffset": 157}, {"referenceID": 21, "context": "Notice that this results in a coarse heat-map of the same size as the convolutional feature maps (14\u00d7 14 in the case of last convolutional layers of VGG [37] and AlexNet [22] networks).", "startOffset": 170, "endOffset": 174}, {"referenceID": 36, "context": "This figure analyzes how localizations change qualitatively as we perform Grad-CAM with respect to different feature maps in a CNN (VGG16 [37]).", "startOffset": 138, "endOffset": 142}, {"referenceID": 4, "context": "A number of works have asserted that as the depth of a CNN increases, higher-level visual constructs are captured [5, 28].", "startOffset": 114, "endOffset": 121}, {"referenceID": 27, "context": "A number of works have asserted that as the depth of a CNN increases, higher-level visual constructs are captured [5, 28].", "startOffset": 114, "endOffset": 121}, {"referenceID": 8, "context": "The ImageNet localization challenge [9] requires competing approaches to provide bounding boxes in addition to classification labels.", "startOffset": 36, "endOffset": 39}, {"referenceID": 36, "context": "We evaluate the pretrained off-the-shelf VGG-16 [37] model from the Caffe [17] Model Zoo.", "startOffset": 48, "endOffset": 52}, {"referenceID": 16, "context": "We evaluate the pretrained off-the-shelf VGG-16 [37] model from the Caffe [17] Model Zoo.", "startOffset": 74, "endOffset": 78}, {"referenceID": 35, "context": "[36] for the VGG-16 model, which uses grabcut to postprocess image space gradients into heat maps.", "startOffset": 0, "endOffset": 4}, {"referenceID": 35, "context": "Backprop on VGG-16 [36] 61.", "startOffset": 19, "endOffset": 23}, {"referenceID": 35, "context": "Grad-CAM outperforms [36] which uses backpropagation to pixel-space for localization.", "startOffset": 21, "endOffset": 25}, {"referenceID": 33, "context": "That is, in the vicinity of the input data point, our explanation should be faithful to the model [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 40, "context": "One obvious choice for such a visualization is image occlusion [41], where we mask out different regions of the input image, and perform multiple CNN evaluations to observe how the resulting score for the class of interest changes.", "startOffset": 63, "endOffset": 67}, {"referenceID": 8, "context": "We use Guided Grad-CAM to analyze failure modes of the VGG-16 CNN on ImageNet classification [9].", "startOffset": 93, "endOffset": 96}, {"referenceID": 20, "context": "6a Visual explanations from image captioning model [21] highlighting image regions considered to be important for producing the captions.", "startOffset": 51, "endOffset": 55}, {"referenceID": 18, "context": "6b Grad-CAM localizations of a global or holistic captioning model for captions generated by a dense captioning model [19] for the three bounding box proposals marked on the left.", "startOffset": 118, "endOffset": 122}, {"referenceID": 39, "context": "We can also see that seemingly unreasonable predictions have reasonable explanations, which is a similar observation to HOGgles [40].", "startOffset": 128, "endOffset": 132}, {"referenceID": 5, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 75, "endOffset": 86}, {"referenceID": 18, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 75, "endOffset": 86}, {"referenceID": 38, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 75, "endOffset": 86}, {"referenceID": 2, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 123, "endOffset": 138}, {"referenceID": 12, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 123, "endOffset": 138}, {"referenceID": 28, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 123, "endOffset": 138}, {"referenceID": 32, "context": "In this subsection we apply our Grad-CAM technique to the image captioning [6, 19, 39] and Visual Question Answering (VQA) [3, 13, 29, 33] tasks.", "startOffset": 123, "endOffset": 138}, {"referenceID": 20, "context": "More specifically, we build on top of the publicly available \u2018neuraltalk2\u20193 implementation [21] that makes use of a finetuned VGG-16 CNN for images and an LSTMbased language model.", "startOffset": 91, "endOffset": 95}, {"referenceID": 18, "context": "[19] recently introduced the Dense Captioning (DenseCap) task that requires a system to jointly localize and caption salient regions in a given image.", "startOffset": 0, "endOffset": 4}, {"referenceID": 18, "context": "The model proposed in [19] consists of a Fully Convolutional Localization Network (FCLN) and an LSTM-based language model that produces both bounding boxes for regions of interest and associated captions in a single forward pass.", "startOffset": 22, "endOffset": 26}, {"referenceID": 2, "context": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions.", "startOffset": 22, "endOffset": 37}, {"referenceID": 12, "context": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions.", "startOffset": 22, "endOffset": 37}, {"referenceID": 28, "context": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions.", "startOffset": 22, "endOffset": 37}, {"referenceID": 32, "context": "Typical VQA pipelines [3, 13, 29, 33] consist of a CNN to model images and an RNN language model for questions.", "startOffset": 22, "endOffset": 37}, {"referenceID": 26, "context": "We explain the answers of a VQA model [27] that combines a CNN and an LSTM representation through a pointwise multiplication.", "startOffset": 38, "endOffset": 42}, {"referenceID": 7, "context": "[8] collected human attention maps for a subset of the VQA dataset [3].", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[8] collected human attention maps for a subset of the VQA dataset [3].", "startOffset": 67, "endOffset": 70}, {"referenceID": 26, "context": "Human attention maps are compared to Grad-CAM visualizations of the simple VQA model introduced above [27] on 1374 question-image (QI) pairs from the validation set of the VQA dataset [3].", "startOffset": 102, "endOffset": 106}, {"referenceID": 2, "context": "Human attention maps are compared to Grad-CAM visualizations of the simple VQA model introduced above [27] on 1374 question-image (QI) pairs from the validation set of the VQA dataset [3].", "startOffset": 184, "endOffset": 187}, {"referenceID": 7, "context": "We use the rank correlation evaluation protocal developed in [8].", "startOffset": 61, "endOffset": 64}, {"referenceID": 35, "context": "[36].", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "com/ramprs/grad-cam/ and an online demo is available on CloudCV [2]4.", "startOffset": 64, "endOffset": 67}], "year": 2016, "abstractText": "We propose a technique for making Convolutional Neural Network (CNN)-based models more transparent by visualizing the regions of input that are \u2018important\u2019 for predictions from these models \u2013 or visual explanations. Our approach, called Gradient-weighted Class Activation Mapping (Grad-CAM), uses the class-specific gradient information flowing into the final convolutional layer of a CNN to produce a coarse localization map of the important regions in the image. Grad-CAM is a strict generalization of the Class Activation Mapping (CAM) [43]. While CAM is limited to a narrow class of CNN models, Grad-CAM is broadly applicable to any CNN-based architectures. We also show how Grad-CAM may be combined with existing pixel-space visualizations (such as Guided Backpropagation [38]) to create a high-resolution class-discriminative visualization (Guided Grad-CAM). We generate Grad-CAM and Guided Grad-CAM visual explanations to better understand image classification, image captioning, and visual question answering (VQA) models. In the context of image classification models, our visualizations (a) lend insight into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), and (b) outperform pixel-space gradient visualizations (Guided Backpropagation [38] and Deconvolution [41]) on the ILSVRC-15 weakly supervised localization task. For image captioning and VQA, our visualizations expose the somewhat surprising insight that common CNN + Long Short Term Memory (LSTM) models can often be good at localizing discriminative input image regions despite not being trained on grounded image-text pairs. Finally, we design and conduct human studies to measure if 1 ar X iv :1 61 0. 02 39 1v 1 [ cs .C V ] 7 O ct 2 01 6 Guided Grad-CAM explanations help users establish trust in the predictions made by deep networks. Interestingly, we show that Guided Grad-CAM helps untrained users successfully discern a \u2018stronger\u2019 deep network from a \u2018weaker\u2019 one even when both networks make identical predictions, simply on the basis of their different explanations. Our code is available at https://github.com/ramprs/ grad-cam/ and a demo is available on CloudCV [2]1.", "creator": "LaTeX with hyperref package"}}}