{"id": "1610.02424", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "7-Oct-2016", "title": "Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models", "abstract": "Neural sequence models are widely used to model time-series data in many fields. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-$B$ candidates -- resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose \\emph{Diverse Beam Search} (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space -- implying that DBS is a \\emph{better search algorithm}. Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.", "histories": [["v1", "Fri, 7 Oct 2016 20:56:47 GMT  (7887kb,D)", "http://arxiv.org/abs/1610.02424v1", "14 pages"]], "COMMENTS": "14 pages", "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.CV", "authors": ["ashwin k vijayakumar", "michael cogswell", "ramprasath r selvaraju", "qing sun", "stefan lee", "david crandall", "dhruv batra"], "accepted": false, "id": "1610.02424"}, "pdf": {"name": "1610.02424.pdf", "metadata": {"source": "CRF", "title": "DIVERSE BEAM SEARCH: DECODING DIVERSE SOLUTIONS FROM NEURAL SEQUENCE MODELS", "authors": ["Ashwin K Vijayakumar", "Michael Cogswell", "Ramprasath R. Selvaraju", "Qing Sun", "Stefan Lee", "David Crandall", "Dhruv Batra"], "emails": ["ashwinkv@vt.edu", "cogswell@vt.edu", "ram21@vt.edu", "sunqing@vt.edu", "steflee@vt.edu", "djcran@indiana.edu,", "dbatra@vt.edu"], "sections": [{"heading": "1 INTRODUCTION", "text": "This year, the time has come for an agreement to be reached, and it will only take a few days."}, {"heading": "2 PRELIMINARIES: DECODING RNNS WITH BEAM SEARCH", "text": "We start with a refresher on BS before describing our generalization, Miscellaneous ray search = sum of probabilities for the notation, let [n] denote the set of natural numbers from 1 to n and let [n] index the first n elements of a vector v-Rm. The decoding problem is trained to estimate the probability of the sequences of this conditional probability distribution from a finite dictionary V which returns an input x. The RNN updates its internal state and estimates the conditional probability distribution over the next output and all previous output tokens. We denote the logarithms of this conditional probability distribution over all tokens at a time t as sequence (yt) = Log Pr (yt-1) and estimates the conditional probability distribution over the next output and all previous output tokens. To simplify the notation, we index the valithms of this conditional probability distribution over all tokens."}, {"heading": "3 DIVERSE BEAM SEARCH: FORMULATION AND ALGORITHM", "text": "To overcome this shortcoming, we consider increasing the target in Eq. 1 with a dissimilarity term (Y [t]), which measures the diversity between candidate sequences. To avoid this common optimization problem, we divide the beam budget B into G groups and optimize each group using the beam search, while fixing the number of possible groups. This doubly greedy approach extends over time and across groups (Y [t]) into a function only of the possible expansions of the current group. Let Y [t] split the amount of all B rays at a time."}, {"heading": "4 RELATED WORK", "text": "Dre rf\u00fc nde nllrfhUeaeaee\u00fccnlrrfhsrc\u00fce\u00fceegnlrf\u00fc rf\u00fc ide nlrfhue\u00fce\u00fceeegnln rf\u00fc ide nlrrfhue\u00fceeeegnln rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rrf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rf\u00fc rfu rfu rfu rfu rfu rfu rfu"}, {"heading": "5 EXPERIMENTS", "text": "First, we will explain the baselines and evaluation metrics used in this paper. Next, we will move on to analyzing the effects of DBS parameters, and report on the results of captions, machine translation, and visual question generation. Although these tasks are reported, it should be noted that DBS is a task-agnostic algorithm that can replace BS to decode various solutions. We compare beam search and the following existing methods: - Li & Jurafsky (2016): This work modifies BS by introducing an intra-Siberian rank. For each partial solution, the sets of | V | continuations are sorted and assigned to intra-Siberian rankings k [L] in the order of decreasing protocol probabilities, - (yt): The protocol probability of an extenstion is then reduced in proportion to its rank, and continuations are re-sorted under these modified protocol probabilities."}, {"heading": "5.1 SENSITIVITY ANALYSIS AND EFFECT OF DIVERSITY FUNCTIONS", "text": "In this section we examine the effects of the number of groups, the strength of the diversity penalty, and various forms of diversity functions for language models. Further discussions and experimental details are included in the supplementary materials. Number of groups (G). SettingG = B allows us to explore the maximum diversity of space, while setting G = 1 reduces our method to BS, which leads to increased exploitation of the search space by the 1-best decoding. Thus, the number of groups can be increased to explore different modes of the model. Empirically, we find that the maximum diversity of exploration is correlated with improved care and hence is used to report results unless it is otherwise.Diversity rigor (\u03bb) specifies the diversity between the common logical probability and the diversity of terms. As expected, we find that a higher value of diversity produces a more diverse list; however, excessive values and the probability of excessive results in the model."}, {"heading": "5.2 IMAGE CAPTIONING", "text": "We evaluate on two sets of data - COCO (Lin et al., 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpaththy & Fei-Fei (2015) for COCO. Results: PASCAL-50S is used only for testing, storing 200 validation images that are used to tune hyperparameters. We observe that gains on PASCAL-50S are more pronounced (7.24% and 9.60% Oracle @ 20 improvements over BS and Li & Jurafsky (2016)."}, {"heading": "5.3 MACHINE TRANSLATION", "text": "Data set and models. We use the English-French parallel data from the europarl corpus as a training set. We report on the results of news-test-2013 and news-test-2014 and use newstest2012 to tune DBS parameters. We train an encoder decoder architecture, as proposed in Bahdanau et al. (2014), using the dl4mt-tutorial2 code repository. The encoder consists of a bidirectional recurrent unit (Gated Recurrent Unit) with attention. We use set level BLEU scores (Papineni et al., 2002) to calculate oracle metrics and report unique n-grams that resemble image captioning. Table 2 shows that DBS consistently exceeds standard baselles in both measurements."}, {"heading": "5.4 VISUAL QUESTION GENERATION", "text": "We also report on the results of another new task - Visual Question Generation (Mostafazadeh et al., 2016). We use the VQA dataset (Antol et al., 2015) to train a model that resembles the caption. Instead of captions, the training set now consists of 3 questions per image. VQA requires the model to think about several issues that are central to vision - such as the position and color of an object, relationships between objects, and natural language. Likewise, learning to ask the \"right\" questions relevant to the image requires the model to think about these finer aspects as well, making question generation an interesting task. While beam search for the sample yields results in similarly formulated questions (see Figure 3), DBS brings to light the details covered by the model in other modes. This raises a variety of questions of different kinds along the lines of Antol et al. (2015) (see Figure 3)."}, {"heading": "6 CONCLUSION", "text": "Search for bars is the most commonly used approximate inference algorithm for decoding sequences from RNNs, but suffers from a lack of diversity. Producing multiple highly similar and generic outputs is not only wasteful in terms of computation, but also detrimental to tasks with inherent ambiguity such as captions. In this paper, we presented Various Beam Search, which describes 2https: / / github.com / nyu-dl / dl4mt-tutorialbeam search as an optimization problem and adds a definition of diversity to the target. The result is a \"doubly greedy\" approximate algorithm that produces different decodes, using roughly the same time and resources as bar search. Our method consistently outperforms bar search and other baselines in all of our experiments, without additional calculation or task-specific overhead. DBS is task-agnostic and can be applied in any case, what makes it applicable in multiple areas - BS."}], "references": [{"title": "VQA: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Antol et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Antol et al\\.", "year": 2015}, {"title": "Neural machine translation by jointly learning to align and translate", "author": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Proceedings of the International Conference on Learning Representations (ICLR),", "citeRegEx": "Bahdanau et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2014}, {"title": "Diverse M-Best Solutions in Markov Random Fields", "author": ["Dhruv Batra", "Payman Yadollahpour", "Abner Guzman-Rivera", "Gregory Shakhnarovich"], "venue": "In Proceedings of European Conference on Computer Vision (ECCV),", "citeRegEx": "Batra et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Batra et al\\.", "year": 2012}, {"title": "Computer, respond to this email", "author": ["Greg Corrado"], "venue": "Google Research Blog,", "citeRegEx": "Corrado.,? \\Q2015\\E", "shortCiteRegEx": "Corrado.", "year": 2015}, {"title": "Visual storytelling", "author": ["Francis Ferraro", "Ishan Mostafazadeh", "Nasrinand Misra", "Aishwarya Agrawal", "Jacob Devlin", "Ross Girshick", "Xiadong He", "Pushmeet Kohli", "Dhruv Batra", "C Lawrence Zitnick"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies (NAACL HLT),", "citeRegEx": "Ferraro et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ferraro et al\\.", "year": 2016}, {"title": "Solving the problem of cascading errors: Approximate bayesian inference for linguistic annotation pipelines", "author": ["Jenny Rose Finkel", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Finkel et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Finkel et al\\.", "year": 2006}, {"title": "A systematic exploration of diversity in machine translation", "author": ["K. Gimpel", "D. Batra", "C. Dyer", "G. Shakhnarovich"], "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Gimpel et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gimpel et al\\.", "year": 2013}, {"title": "Speech recognition with deep recurrent neural networks", "author": ["Alex Graves", "Abdel-rahman Mohamed", "Geoffrey E. Hinton"], "venue": null, "citeRegEx": "Graves et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "How hard can it be? Estimating the difficulty of visual search in an image", "author": ["Radu Tudor Ionescu", "Bogdan Alexe", "Marius Leordeanu", "Marius Popescu", "Dim Papadopoulos", "Vittorio Ferrari"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Ionescu et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Ionescu et al\\.", "year": 2016}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["Andrej Karpathy", "Li Fei-Fei"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Karpathy and Fei.Fei.,? \\Q2015\\E", "shortCiteRegEx": "Karpathy and Fei.Fei.", "year": 2015}, {"title": "Inferring m-best diverse labelings in a single one", "author": ["Alexander Kirillov", "Bogdan Savchynskyy", "Dmitrij Schlesinger", "Dmitry Vetrov", "Carsten Rother"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Kirillov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kirillov et al\\.", "year": 2015}, {"title": "Mutual information and diverse decoding improve neural machine translation", "author": ["Jiwei Li", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1601.00372,", "citeRegEx": "Li and Jurafsky.,? \\Q2016\\E", "shortCiteRegEx": "Li and Jurafsky.", "year": 2016}, {"title": "A diversity-promoting objective function for neural conversation models", "author": ["Jiwei Li", "Michel Galley", "Chris Brockett", "Jianfeng Gao", "Bill Dolan"], "venue": "Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics \u2013 Human Language Technologies (NAACL HLT),", "citeRegEx": "Li et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Li et al\\.", "year": 2015}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S. Corrado", "Jeff Dean"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Generating natural questions about an image", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende"], "venue": "Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL),", "citeRegEx": "Mostafazadeh et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Mostafazadeh et al\\.", "year": 2016}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the Annual Meeting on Association for Computational Linguistics (ACL),", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "N-best maximal decoders for part models", "author": ["Dennis Park", "Deva Ramanan"], "venue": "In Proceedings of IEEE International Conference on Computer Vision (ICCV),", "citeRegEx": "Park and Ramanan.,? \\Q2011\\E", "shortCiteRegEx": "Park and Ramanan.", "year": 2011}, {"title": "Submodular meets structured: Finding diverse subsets in exponentially-large structured item sets", "author": ["Adarsh Prasad", "Stefanie Jegelka", "Dhruv Batra"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "citeRegEx": "Prasad et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Prasad et al\\.", "year": 2014}, {"title": "Cider: Consensus-based image description evaluation", "author": ["Ramakrishna Vedantam", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vedantam et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vedantam et al\\.", "year": 2015}, {"title": "Sequence to sequence-video to text", "author": ["Subhashini Venugopalan", "Marcus Rohrbach", "Jeffrey Donahue", "Raymond Mooney", "Trevor Darrell", "Kate Saenko"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Venugopalan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Venugopalan et al\\.", "year": 2015}, {"title": "A neural conversational model", "author": ["Oriol Vinyals", "Quoc Le"], "venue": "arXiv preprint arXiv:1506.05869,", "citeRegEx": "Vinyals and Le.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals and Le.", "year": 2015}, {"title": "Show and tell: A neural image caption generator", "author": ["Oriol Vinyals", "Alexander Toshev", "Samy Bengio", "Dumitru Erhan"], "venue": "In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "citeRegEx": "Vinyals et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 7, "context": "In the last few years, Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs) or more generally, neural sequence models have become the standard choice for modeling time-series data for a wide range of applications such as speech recognition (Graves et al., 2013), machine translation (Bahdanau et al.", "startOffset": 261, "endOffset": 282}, {"referenceID": 1, "context": ", 2013), machine translation (Bahdanau et al., 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al.", "startOffset": 29, "endOffset": 52}, {"referenceID": 21, "context": ", 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al.", "startOffset": 80, "endOffset": 128}, {"referenceID": 19, "context": ", 2014), conversation modeling (Vinyals & Le, 2015), image and video captioning (Vinyals et al., 2015; Venugopalan et al., 2015), and visual question answering (Antol et al.", "startOffset": 80, "endOffset": 128}, {"referenceID": 0, "context": ", 2015), and visual question answering (Antol et al., 2015).", "startOffset": 39, "endOffset": 59}, {"referenceID": 21, "context": "improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics, it is common practice (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016) to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths.", "startOffset": 134, "endOffset": 204}, {"referenceID": 4, "context": "improvements in posterior-probabilities not necessarily corresponding to improvements in task-specific metrics, it is common practice (Vinyals et al., 2015; Karpathy & Fei-Fei, 2015; Ferraro et al., 2016) to deliberately throttle BS to become a poorer optimization algorithm by using reduced beam widths.", "startOffset": 134, "endOffset": 204}, {"referenceID": 3, "context": "always saying \u201cI don\u2019t know\u201d in conversation models (Corrado, 2015).", "startOffset": 52, "endOffset": 67}, {"referenceID": 2, "context": "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms \u2013 the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.", "startOffset": 117, "endOffset": 181}, {"referenceID": 17, "context": "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms \u2013 the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.", "startOffset": 117, "endOffset": 181}, {"referenceID": 10, "context": "Drawing from recent work in the probabilistic graphical models literature on Diverse M-Best (DivMBest) MAP inference (Batra et al., 2012; Prasad et al., 2014; Kirillov et al., 2015), we optimize an objective that consists of two terms \u2013 the sequence likelihood under the model and a dissimilarity term that encourages beams across groups to differ.", "startOffset": 117, "endOffset": 181}, {"referenceID": 2, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).", "startOffset": 105, "endOffset": 191}, {"referenceID": 10, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).", "startOffset": 105, "endOffset": 191}, {"referenceID": 17, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014).", "startOffset": 105, "endOffset": 191}, {"referenceID": 2, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity.", "startOffset": 128, "endOffset": 213}, {"referenceID": 2, "context": "The task of generating diverse structured outputs from probabilistic models has been studied extensively (Park & Ramanan, 2011; Batra et al., 2012; Kirillov et al., 2015; Prasad et al., 2014). Batra et al. (2012) formalized this task for Markov Random Fields as the DivMBest problem and presented a greedy approach which solves for outputs iteratively, conditioning on previous solutions to induce diversity. Kirillov et al. (2015) show how these solutions can be found jointly for certain kinds of energy functions.", "startOffset": 128, "endOffset": 432}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm.", "startOffset": 49, "endOffset": 70}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference.", "startOffset": 49, "endOffset": 1046}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference. Diverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation. In this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al.", "startOffset": 49, "endOffset": 1442}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference. Diverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation. In this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al. (2013). This discourages sequences from sharing common roots, implicitly resulting in diverse lists.", "startOffset": 49, "endOffset": 1539}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference. Diverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation. In this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al. (2013). This discourages sequences from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective as in DBS rather than a heuristic provides easier generalization to incorporate different notions of diversity and control for the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms this method. Through a novel decoding objective that maximizes mutual information between inputs and predicted outputs, Li et al. (2015) penalize decoding generic, input independent sequences.", "startOffset": 49, "endOffset": 2057}, {"referenceID": 6, "context": "Most related to our proposed approach is that of Gimpel et al. (2013) who apply the DivMBest approach to machine translation using beam search as a black-box inference algorithm. To obtain diverse solutions, beam searches of arbitrary size are sequentially performed while retaining the top-scoring candidate and using it to update the diversity term. This approach is extremely wasteful because in each iteration only one solution returned by beam search is kept. Consequently, the iterative method is time consuming and is poorly suited for batch processing or producing a large number of solutions. Our algorithm avoids these shortcomings by integrating diversity within BS such that no beams are discarded. By running multiple beam searches in parallel and at staggered time offsets, we obtain large time savings making our method comparable to classical BS. One potential advantage over our method is that more complex diversity measures at the sentence-level can be incorporated. However, as observed empirically by us and Li et al. (2015), initial words tend to significantly impact the diversity of the resultant sequences \u2013 suggesting that later words may not contribute significantly to diverse inference. Diverse Decoding for RNNs. Some efforts have been made to produce diverse decodings from recurrent models for conversation modeling and machine translation. In this context, our work is closely related to Li & Jurafsky (2016), who propose a BS diversification heuristic to overcome the shortcomings of Gimpel et al. (2013). This discourages sequences from sharing common roots, implicitly resulting in diverse lists. Introducing diversity through a modified objective as in DBS rather than a heuristic provides easier generalization to incorporate different notions of diversity and control for the exploration-exploitation trade-off as detailed in Section 5.1. Furthermore, we find that DBS outperforms this method. Through a novel decoding objective that maximizes mutual information between inputs and predicted outputs, Li et al. (2015) penalize decoding generic, input independent sequences. This is achieved by training an additional target language model. Although this work and DBS share the same goals (producing diverse decodings), the techniques developed are disjoint and complementary \u2013 Li et al. (2015) develops a new model (RNN translation model with an RNN target language model), while DBS is a modified inference algorithm that can be applied to any model where BS is applicable.", "startOffset": 49, "endOffset": 2333}, {"referenceID": 12, "context": "- Li et al. (2015): These models are decoded using a modified objective, P (y|x)\u2212 \u03bbU(y), where U(y) is an unconditioned target sequence model.", "startOffset": 2, "endOffset": 19}, {"referenceID": 12, "context": "Similar to Li et al. (2015), we divide these counts by the total number of words generated to bias against long sentences.", "startOffset": 11, "endOffset": 28}, {"referenceID": 13, "context": "While all the previous diversity functions discussed above perform exact matches, neural embeddings such as word2vec (Mikolov et al., 2013) can penalize semantically similar words like synonyms.", "startOffset": 117, "endOffset": 139}, {"referenceID": 6, "context": "The current group is penalized for producing the same n-grams as previous groups, regardless of alignment in time \u2013 similar to Gimpel et al. (2013). This is proportional to the number of times each n-gram in a candidate occurred in previous groups.", "startOffset": 127, "endOffset": 148}, {"referenceID": 18, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015).", "startOffset": 23, "endOffset": 46}, {"referenceID": 21, "context": "We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository.", "startOffset": 28, "endOffset": 50}, {"referenceID": 17, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO.", "startOffset": 24, "endOffset": 105}, {"referenceID": 17, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing save 200 validation images used to tune hyperparameters. We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. As it can be observed from Table 1, DBS outperforms both BS and Li & Jurafsky (2016) on both datasets.", "startOffset": 24, "endOffset": 393}, {"referenceID": 17, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing save 200 validation images used to tune hyperparameters. We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. As it can be observed from Table 1, DBS outperforms both BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.24% and 9.60% Oracle@20 improvements against BS and Li & Jurafsky (2016)) than COCO.", "startOffset": 24, "endOffset": 543}, {"referenceID": 17, "context": ", 2014) and PASCAL-50S (Vedantam et al., 2015). We use the public splits as in Karpathy & Fei-Fei (2015) for COCO. PASCAL-50S is used only for testing save 200 validation images used to tune hyperparameters. We train a captioning model (Vinyals et al., 2015) using the neuraltalk21 code repository. Results. As it can be observed from Table 1, DBS outperforms both BS and Li & Jurafsky (2016) on both datasets. We observe that gains on PASCAL-50S are more pronounced (7.24% and 9.60% Oracle@20 improvements against BS and Li & Jurafsky (2016)) than COCO. This suggests diverse predictions are especially advantageous when there is a mismatch between training and testing sets making DBS a better inference strategy in real-world applications. Table 1 also shows the number of distinct n-grams produced by different techniques. Our method produces significantly more distinct n-grams (almost 300% increase in the number of 4-grams produced) as compared to BS. We also note that our method tends to produce slightly longer captions compared to beam search on average. Moreover, on the PASCAL-50S test split we observe that DBS finds more likely top-1 solutions on average \u2013 DBS obtains a maximum log-probability of -6.53 as against -6.91 got by BS of same beam width. While the performance of DBS is guaranteed to be better than a BS of size B/G, this experimental evidence suggests that using DBS as a replacement to BS leads to better or at least comparable performance. Table 1: Oracle accuracy and distinct n-grams on COCO and PASCAL-50S datasets for image captioning at B = 20. Although we report CIDEr, we observe similar trends in other standard metrics. Dataset Method Oracle Accuracy (CIDEr) Diversity Statistics @1 @5 @10 @20 distinct-1 distinct-2 distinct-3 distinct-4 Beam Search 0.5379 0.8394 0.9670 1.0763 0.40 1.51 3.25 5.67 Li & Jurafsky (2016) 0.", "startOffset": 24, "endOffset": 1859}, {"referenceID": 12, "context": "44 Li et al. (2015) 0.", "startOffset": 3, "endOffset": 20}, {"referenceID": 12, "context": "44 Li et al. (2015) 0.4980 0.8135 0.9687 1.0737 0.42 1.37 3.46 6.10 Beam Search 0.8727 1.2174 1.3346 1.4098 0.12 0.57 1.35 2.50 Li & Jurafsky (2016) 0.", "startOffset": 3, "endOffset": 149}, {"referenceID": 12, "context": "44 Li et al. (2015) 0.4980 0.8135 0.9687 1.0737 0.42 1.37 3.46 6.10 Beam Search 0.8727 1.2174 1.3346 1.4098 0.12 0.57 1.35 2.50 Li & Jurafsky (2016) 0.9142 1.1133 1.1694 1.1914 0.15 0.97 2.43 5.31 COCO DBS 0.8688 1.2338 1.3568 1.4288 0.18 1.26 3.67 7.33 Li et al. (2015) 0.", "startOffset": 3, "endOffset": 271}, {"referenceID": 8, "context": "This notion is studied by Ionescu et al. (2016), which defines a \u201cdifficulty score\u201d: the human response time for solving a visual search task.", "startOffset": 26, "endOffset": 48}, {"referenceID": 15, "context": "We use sentence level BLEU scores (Papineni et al., 2002) to compute oracle metrics and report distinct n-grams similar to image-captioning.", "startOffset": 34, "endOffset": 57}, {"referenceID": 1, "context": "We train a encoder-decoder architecture as proposed in Bahdanau et al. (2014) using the dl4mt-tutorial2 code repository.", "startOffset": 55, "endOffset": 78}, {"referenceID": 1, "context": "We train a encoder-decoder architecture as proposed in Bahdanau et al. (2014) using the dl4mt-tutorial2 code repository. The encoder consists of a bi-directional recurrent network (Gated Recurrent Unit) with attention. We use sentence level BLEU scores (Papineni et al., 2002) to compute oracle metrics and report distinct n-grams similar to image-captioning. From Table 2, we see that DBS consistently outperforms standard baselines with respect to both metrics. Table 2: Quantitative results on En-Fr machine translation on the newstest-2013 dataset (atB = 20). Although we report BLEU-4 values, we find similar trends hold for lower BLEU metrics as well. Method Oracle Accuracy (BLEU-4) Diversity Statistics @1 @5 @10 @20 distinct-1 distinct-2 distinct-3 distinct-4 Beam Search 13.52 16.67 17.63 18.44 0.04 0.75 2.10 3.23 Li & Jurafsky (2016) 13.", "startOffset": 55, "endOffset": 846}, {"referenceID": 1, "context": "We train a encoder-decoder architecture as proposed in Bahdanau et al. (2014) using the dl4mt-tutorial2 code repository. The encoder consists of a bi-directional recurrent network (Gated Recurrent Unit) with attention. We use sentence level BLEU scores (Papineni et al., 2002) to compute oracle metrics and report distinct n-grams similar to image-captioning. From Table 2, we see that DBS consistently outperforms standard baselines with respect to both metrics. Table 2: Quantitative results on En-Fr machine translation on the newstest-2013 dataset (atB = 20). Although we report BLEU-4 values, we find similar trends hold for lower BLEU metrics as well. Method Oracle Accuracy (BLEU-4) Diversity Statistics @1 @5 @10 @20 distinct-1 distinct-2 distinct-3 distinct-4 Beam Search 13.52 16.67 17.63 18.44 0.04 0.75 2.10 3.23 Li & Jurafsky (2016) 13.63 17.11 17.50 18.34 0.04 0.81 2.92 4.61 DBS 13.69 17.51 17.80 18.77 0.06 0.95 3.67 5.54 Li et al. (2015) 13.", "startOffset": 55, "endOffset": 955}, {"referenceID": 14, "context": "We also report results on another novel task \u2013 Visual Question Generation (Mostafazadeh et al., 2016).", "startOffset": 74, "endOffset": 101}, {"referenceID": 0, "context": "We use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning.", "startOffset": 23, "endOffset": 43}, {"referenceID": 0, "context": "We use the VQA dataset (Antol et al., 2015) to train a model similar to image captioning. Instead of captions, the training set now consists of 3 questions per image. VQA requires the model to reason about multiple problems that are central to vision \u2013 like the position and color of an object, relationships between objects and natural language. Similarly, learning to ask the \u201cright\u201d questions pertinent to the image also requires the model to reason about these finer aspects making question generation an interesting task. While using beam search to sample outputs results in similarly worded questions (see Fig. 3), DBS brings out the details captured by the model in other modes. This promotes diverse questions of different types as defined by Antol et al. (2015) (see Fig.", "startOffset": 24, "endOffset": 771}], "year": 2016, "abstractText": "Neural sequence models are widely used to model time-series data. Equally ubiquitous is the usage of beam search (BS) as an approximate inference algorithm to decode output sequences from these models. BS explores the search space in a greedy left-right fashion retaining only the top-B candidates \u2013 resulting in sequences that differ only slightly from each other. Producing lists of nearly identical sequences is not only computationally wasteful but also typically fails to capture the inherent ambiguity of complex AI tasks. To overcome this problem, we propose Diverse Beam Search (DBS), an alternative to BS that decodes a list of diverse outputs by optimizing for a diversity-augmented objective. We observe that our method finds better top-1 solutions by controlling for the exploration and exploitation of the search space \u2013 implying that DBS is a better search algorithm. Moreover, these gains are achieved with minimal computational or memory overhead as compared to beam search. To demonstrate the broad applicability of our method, we present results on image captioning, machine translation and visual question generation using both standard quantitative metrics and qualitative human studies. Our method consistently outperforms BS and previously proposed techniques for diverse decoding from neural sequence models.", "creator": "LaTeX with hyperref package"}}}