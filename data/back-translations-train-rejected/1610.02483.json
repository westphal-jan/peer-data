{"id": "1610.02483", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2016", "title": "Boost K-Means", "abstract": "Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. Since then, continuous efforts have been taken to enhance its performance. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is explicitly driven by an objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. K-means therefore becomes simpler, faster and better. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios.", "histories": [["v1", "Sat, 8 Oct 2016 04:36:42 GMT  (381kb,D)", "https://arxiv.org/abs/1610.02483v1", "11 pages, 10 figures"], ["v2", "Sun, 4 Dec 2016 07:32:37 GMT  (244kb,D)", "http://arxiv.org/abs/1610.02483v2", "11 pages, 6 figures"]], "COMMENTS": "11 pages, 10 figures", "reviews": [], "SUBJECTS": "cs.LG cs.CV cs.DB", "authors": ["wan-lei zhao", "cheng-hao deng", "chong-wah ngo"], "accepted": false, "id": "1610.02483"}, "pdf": {"name": "1610.02483.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Wan-Lei Zhao", "Cheng-Hao Deng"], "emails": ["wlzhao@xmu.edu.cn", "cscwngo@gapps.cityu.edu.hk"], "sections": [{"heading": null, "text": "Index terms - clusters, k-means, incremental optimizationF"}, {"heading": "1 INTRODUCTION", "text": "In the last three decades, the number of cluster algorithms used in various areas of application has increased significantly."}, {"heading": "2 RELATED WORKS", "text": "In fact, most of them will be able to go to another world, where they can go to another world, where they can go to another world, where they can go to another world."}, {"heading": "3 CLUSTERING OBJECTIVE FUNCTIONS", "text": "In this section, the cluster objective functions on which our k-mean method is based are presented. Essentially, two objective functions are derived that aim to optimize cluster outcomes from various aspects. Furthermore, we show that these two objective functions can be reduced to a single form.3"}, {"heading": "3.1 Preliminaries", "text": "To facilitate the subsequent discussions, several variables are defined. Throughout the paper, the size of the input data is specified as n, while the number of clusters is specified as k. The partition formed by a cluster method is represented as {S1, \u00b7 \u00b7, Sr \u00b7, Sk}. Accordingly, the cluster size is defined as n1, \u00b7 \u00b7, nr, \u00b7 \u00b7, nk. The composite vector of a cluster is defined as Dr = xi, Sr xi. The cluster centrifugal Cr1 is defined by its members, Cr = 1 xi nr i = 1 xi nr = Dr nr (2) The inner product of Cr is specified as C \u2032 rCr = (1 x)."}, {"heading": "3.2 Objective Functions", "text": "In this section, two objective functions (also known as criterion functions [1]) are developed. Furthermore, with the support of the results obtained in Section 3.1, these objective functions are reduced to simple shapes, referring to one column vector above the paper, while another focuses on optimizing different clusters. In this paper, we focus on creating a cluster solution defined by the elements within each cluster. Therefore, it does not take into account the relationship between the elements associated with different clusters. The first objective function we are looking at is minimizing the distance of each element to its cluster headquarters, which is nothing more than the objective function of k-means.I1 = xi q (xi) = r."}, {"heading": "4 K-MEANS DRIVEN BY OBJECTIVE FUNCTION", "text": "In this section, two iterative cluster procedures are presented using the objective function developed in Section 3: One generates k clusters directly (referred to as direct k-way k-means), while the other generates k clusters by sequentially cutting input data k-1 times (referred to as bisecting k-means) Both cluster strategies are based on incremental clustering [1], [23] and are driven by objective function I \u043c 1 (Eqn. 9)."}, {"heading": "4.1 Clustering Algorithm", "text": "The basic idea of clustering is that you move from the Su clusters to the Sv-1 processes as soon as this movement leads to a higher result. (Dv + xi) \"It's not the way we move, but the way we move.\" (D) \"It's the way we move.\" (D) \"It's the way we move.\" (D) \"It's the way we move.\" (D) \"It's the way we move.\" (D) \"It's the way we move.\" (D) \"It's the way we move.\" (D) \"It's the way we move.\" (D \")\" It's the way we move. \"(D\") \"It's the way we move.\""}, {"heading": "4.2 Scalability Analysis", "text": "In this context, the question must be asked as to the causes of the growth of climate change in the United States and in Europe. (...) It is the question of the causes of climate change, the causes of climate change, the causes of climate change, the causes of climate change, the causes of climate change, the causes of climate change, the causes of climate change, the causes of climate change, the causes of climate change and the causes of climate change, the causes of climate change and the causes of climate change, the causes of climate change and the causes of climate change, the causes of climate change, the causes of climate change and the causes of climate change, the causes of climate change and the causes of climate change, the causes of climate change and the causes of climate change, the causes of climate change and the causes of climate change, the causes of the causes of climate change and the causes of the causes, the causes of the causes of the causes and the causes of the causes, the causes of the causes of the causes and the causes of the causes of the climate change."}, {"heading": "5 EXPERIMENTS", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "5.1 Evaluation of Clustering Distortion", "text": "This year it is more than ever before."}, {"heading": "5.2 Nearest Neighbor Search by Product Quantizer (PQ)", "text": "In this section, BKM is used for visual vocabulary training by means of product quantization [5]. Following the practice of [5], 100K SIFT characteristics are used for product quantization training, while SIFT1M set [5] is encoded with the trained product quantizers as the reference set for the nearest search (NNS).The callback @ top-k obtained is averaged for each method over 1,000 queries on average. In the experiment, two different settings for the product quantizer are tested: The 128-dimensional SIFT vector is encoded with 8 or 16 product quantizers, respectively. For clarity reasons, the evaluations are presented separately for direct k-ways and double-edged kmeans.Recall @ top-100 for direct k-ways in Figure 4 (a) - (d) under two different settings (m = 8 and m = 16).As can be seen from the numbers, the performances of k-means + + and BM (all are not very close)."}, {"heading": "5.3 Document Clustering", "text": "In this section, the performance of the proposed method is evaluated in the context of document clustering. Following [1], 15 document records are used for evaluation, the documents are represented using the TF / IDF model and normalized to unit length. Similar to [1], entropy is evaluated as the following entropy = k \u00b2 r = 1 nr 1 log c \u00b2 c \u00b2 i = 1 nir \u00b2 nr \u00b2 nr \u00b2 nr, (18) where c is the number of classes. Eqn. 18 evaluates the degree to which these elements from the same class are placed in a cluster. The lower the value, the better the result. In the experiment, each method performs clustering for 10 runs, and the run with the lowest entropy is represented in Table 3. The presented entropy is averaged to what degree these elements from the same class are placed in a cluster. The lower the value, the better the result."}, {"heading": "5.4 Scalability Test on Image Clustering", "text": "This year it is more than ever before in the history of the city."}, {"heading": "6 CONCLUSION", "text": "We have presented a novel k-mean variant: First, a cluster objective function is developed that is feasible for the entire l2-10 space. Supported by the objective function, the traditional k-mean clustering has been brought into simpler form. Interestingly, in this novel k-mean variant, we find that neither the costly initial mapping nor the search for the nearest centrid are necessary for each sample in the iteration, resulting in higher speed and significantly lower cluster distortion. Furthermore, the proposed cluster method, when performed in the manner of top-down bisecting, achieves the highest scalability and best quality among all hierarchical k-mean variants. Extensive experiments have been conducted in different contexts and on different data sets."}, {"heading": "ACKNOWLEDGEMENT", "text": "This work is supported by the National Natural Science Foundation of China with a grant of 61572408. The authors would like to thank Prof. George Karypis of the University of Minnesota, USA, for his detailed explanations of the implementation of repeated double-edged means."}, {"heading": "APPENDIX A: OPTIMALITY OF INCREMENTAL KMEANS", "text": "As shown in Eqn. 13 and Eqn. 9, two optimal goals are quite similar. In this section we show that an optimal solution in terms of objective function (Eqn. 9) can be achieved with an incremental update scheme presented in Alg. 1. 6.0.0.01. Proof: Let Ao = {S1, S2, \u00b7 \u00b7, Sk} be an optimal solution and assume that there is an element d and clusters Si and Sj so that d."}, {"heading": "APPENDIX B: CONVERGENCE OF INCREMENTAL KMEANS", "text": "Si and Sj are two clusters. d is initially part of Si, and Di is the composition of Si exclude d, Ci is the middle of Si exclude d, Dj, Cj is the composition and middle of Cluster Sj, the moving state from d from Si to Sj should be fulfilled (Di + d) \"(Di + d) ni + 1 + D \u2032 jDj nj < D \u2032 iDi ni + (Dj + d) \u2032 (Dj + d) nj + 1 (20) This equation can be rewritten as: (Di + d)\" ni + 1 \u2212 D \u2032 iDi ni < (Dj + d) \u2032 (Dj + d) nj + 1 \u2212 D \u2032 jDj + 1 \u2212 jj ni jj \u2032 ni jj nid (1d \u2032 Di + 1d)."}], "references": [{"title": "Empirical and theoretical comparisons of selected criterion functions for document clustering", "author": ["Y. Zhao", "G. Karypis"], "venue": "Machine Learning, vol. 55, pp. 311\u2013331, 2004.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2004}, {"title": "Web-scale image clustering revisited", "author": ["Y. Avrithis", "Y. Kalantidis", "E. Anagnostopoulos", "I.Z. Emiris"], "venue": "ICCV, pp. 1502\u20131510, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "Normalized cuts and image segmentation", "author": ["J. Shi", "J. Malik"], "venue": "Trans. PAMI, vol. 22, pp. 888\u2013905, Aug. 2000.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2000}, {"title": "Video Google: A text retrieval approach to object matching in videos", "author": ["J. Sivic", "A. Zisserman"], "venue": "ICCV, pp. 1470\u20131477, Oct. 2003.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2003}, {"title": "Product quantization for nearest neighbor search", "author": ["H. J\u00e9gou", "M. Douze", "C. Schmid"], "venue": "Trans. PAMI, vol. 33, pp. 117\u2013128, Jan. 2011.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2011}, {"title": "Scalable nearest neighbor algorithms for high dimensional data", "author": ["M. Muja", "D.G. Lowe"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on, vol. 36, pp. 2227\u20132240, 2014.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2014}, {"title": "Additive quantization for extreme vector compression", "author": ["A. Babenko", "V. Lempitsky"], "venue": "CVPR, pp. 931\u2013938, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Least squares quantization in PCM", "author": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory, vol. 28, pp. 129\u2013137, 1982.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 1982}, {"title": "Top 10 algorithms in data mining", "author": ["X. Wu", "V. Kumar", "J.R. Quinlan", "J. Ghosh", "Q. Yang", "H. Motoda", "G.J. McLachlan", "A. Ng", "B. Liu", "P.S. Yu", "Z.-H. Zhou", "M. Steinbach", "D.J. Hand", "D. Steinberg"], "venue": "Knowledge and Information System, vol. 14, pp. 1\u201337, Dec. 2007.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2007}, {"title": "Streaming k-means approximation", "author": ["N. Ailon", "R. Jaiswal", "C. Monteleoni"], "venue": "NIPS, pp. 10\u201318, 2009.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2009}, {"title": "k-means requires exponentially many iterations even in the plane", "author": ["A. Vattani"], "venue": "Discrete and Computational Geometry, vol. 45, no. 4, pp. 596\u2013616, 2011.  11", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2011}, {"title": "K-means++: The advantages of careful seeding", "author": ["D. Arthur", "S. Vassilvitskii"], "venue": "Proceedings of the Eighteenth Annual ACM- SIAM Symposium on Discrete Algorithms, pp. 1027\u20131035, 2007.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2007}, {"title": "Fast and accurate k-means for large datasets", "author": ["M. Shindler", "A. Wong", "A.W. Meyerson"], "venue": "NIPS, pp. 2375\u20132383, 2011.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2011}, {"title": "An efficient k-means clustering algorithm: Analysis and implementation", "author": ["T. Kanungo", "D.M. Mount", "N.S. Neanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Trans. PAMI, vol. 24, pp. 881\u2013892, Jul. 2002.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2002}, {"title": "Using the triangle inequality to accelerate", "author": ["C. Elkan"], "venue": "ICML, 2003.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2003}, {"title": "Web-scale k-means clustering", "author": ["D. Sculley"], "venue": "In Proceedings of the 19th international conference on World wide web, pp. 1177\u20131178, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Scalable k-means++", "author": ["B. Bahmani", "B. Moseley", "A. Vattani", "R. Kumar", "S. Vassilvitskii"], "venue": "Proceedings of the VLDB Endowment, vol. 5, no. 7, pp. 622\u2013633, 2012.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Scalable k-means by ranked retrieval", "author": ["A. Broder", "L. Garcia-Pueyo", "V. Josifovski", "S. Vassilvitskii", "S. Venkatesan"], "venue": "Proceedings of the 7th ACM international conference on Web search and data mining, pp. 233\u2013242, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Web scale photo hash clustering on a single machine", "author": ["Y. Gong", "M. Pawlowski", "F. Yang", "L. Brandy", "L. Boundev", "R. Fergus"], "venue": "CVPR, pp. 19\u201327, 2015.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2015}, {"title": "Convergence properties of the kmeans algorithm", "author": ["L. Bottou", "Y. Bengio"], "venue": "Advances in Neural Information Processing Systems, pp. 585\u2013592, 1995.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1995}, {"title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "author": ["M. Ester", "H. peter Kriegel", "J. Sander", "X. Xu"], "venue": "In Proceedings of Knowledge Discovery and Data Mining, pp. 226\u2013231, 1996.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 1996}, {"title": "Mean shift: A robust approach toward feature space analysis", "author": ["D. Comaniciu", "P. Mee"], "venue": "Trans. PAMI, vol. 24, pp. 603\u2013619, May 2002.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2002}, {"title": "Data clustering: A review", "author": ["A.K. Jain", "M.N. Murty", "P.J. Flynn"], "venue": "ACM Computer Survey, vol. 31, pp. 264\u2013323, Sep. 1999.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 1999}, {"title": "Multidimensional binary search trees used for associative searching", "author": ["J.L. Bentley"], "venue": "Communications of ACM, vol. 18, pp. 509\u2013 517, Sep. 1975.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 1975}, {"title": "Accelerating exact k-means algorithms with geometric reasoning", "author": ["D. Pelleg", "A. Moore"], "venue": "Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 277\u2013281, ACM, 1999.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1999}, {"title": "Fast and exact out-of-core k-means clustering", "author": ["A. Goswami", "R. Jin", "G. Agrawal"], "venue": "Proceedings of the Fourth IEEE International Conference on Data Mining, pp. 83\u201390, 2004.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2004}, {"title": "Algorithms for Clustering Data", "author": ["A.K. Jain", "R.C. Dubes"], "venue": null, "citeRegEx": "28", "shortCiteRegEx": "28", "year": 1988}, {"title": "Hierarchical clustering algorithms for document datasets", "author": ["Y. Zhao", "G. Karypis"], "venue": "Data Mining and Knowledge Discovery, vol. 10, no. 2, pp. 141\u2013168, 2005.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2005}, {"title": "eds., Self-Organizing Maps", "author": ["T. Kohonen", "M.R. Schroeder", "T.S. Huang"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2001}, {"title": "Distinctive image features from scale-invariant keypoints", "author": ["D. Lowe"], "venue": "IJCV, vol. 60, pp. 91\u2013110, Nov. 2004.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2004}, {"title": "YFCC100M: The new data in multimedia research", "author": ["B. Thomee", "D.A. Shamma", "G. Friedland", "B. Elizalde", "K. Ni", "D. Poland", "D. Borth", "L.-J. Li"], "venue": "Communications of the ACM, vol. 59, no. 2, pp. 64\u201373, 2016.", "citeRegEx": "32", "shortCiteRegEx": null, "year": 2016}, {"title": "Scale and affine invariant interest point detectors", "author": ["K. Mikolajczyk", "C. Schmid"], "venue": "IJCV, vol. 60, pp. 63\u201386, Oct. 2004.", "citeRegEx": "33", "shortCiteRegEx": null, "year": 2004}, {"title": "Three things everyone should know to improve object retrieval", "author": ["R. Arandjelovic", "A. Zisserman"], "venue": "CVPR, pp. 2911\u20132918, Jun. 2012.", "citeRegEx": "34", "shortCiteRegEx": null, "year": 2012}, {"title": "Aggregating local descriptors into compact codes", "author": ["H. J\u00e9gou", "F. Perronnin", "M. Douze", "J. S\u00e1nchez", "P. P\u00e9rez", "C. Schmid"], "venue": "Trans. PAMI, vol. 34, pp. 1704\u20131716, Sep. 2012.", "citeRegEx": "35", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 95, "endOffset": 98}, {"referenceID": 1, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 135, "endOffset": 138}, {"referenceID": 2, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 205, "endOffset": 208}, {"referenceID": 4, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 237, "endOffset": 240}, {"referenceID": 5, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 242, "endOffset": 245}, {"referenceID": 6, "context": "Clustering problems arise from variety of applications, such as documents/web pages clustering [1], pattern recognition, image linking [2], image segmentation [3], data compression via vector quantization [4] and nearest neighbor search [5], [6], [7].", "startOffset": 247, "endOffset": 250}, {"referenceID": 7, "context": "Among these algorithms, kmeans [8] remains a popular choice for its simplicity, efficiency and moderate but stable performance across different problems.", "startOffset": 31, "endOffset": 34}, {"referenceID": 8, "context": "It was known as one of top ten most popular algorithms in data mining [9].", "startOffset": 70, "endOffset": 73}, {"referenceID": 9, "context": "In some specific scenarios, the running time of k-means could be even exponential in the worst case [10], [11].", "startOffset": 100, "endOffset": 104}, {"referenceID": 10, "context": "In some specific scenarios, the running time of k-means could be even exponential in the worst case [10], [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 11, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 94, "endOffset": 98}, {"referenceID": 12, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 100, "endOffset": 104}, {"referenceID": 13, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 14, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 125, "endOffset": 129}, {"referenceID": 15, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 137, "endOffset": 141}, {"referenceID": 16, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 143, "endOffset": 147}, {"referenceID": 17, "context": "As a consequence, recent research has been working on either improving its clustering quality [12], [13] or efficiency [14], [15], [16], [13], [17], [18].", "startOffset": 149, "endOffset": 153}, {"referenceID": 1, "context": "K-means has been also tailored to perform web-scale image clustering [2], [19].", "startOffset": 69, "endOffset": 72}, {"referenceID": 18, "context": "K-means has been also tailored to perform web-scale image clustering [2], [19].", "startOffset": 74, "endOffset": 78}, {"referenceID": 7, "context": "This is known as Lloyd iteration procedure [8].", "startOffset": 43, "endOffset": 46}, {"referenceID": 19, "context": "In general kmeans only converges to local minimum [20].", "startOffset": 50, "endOffset": 54}, {"referenceID": 20, "context": "Compared with other well-known clustering algorithms such as DBSCAN [21] and Mean shift [22], this complexity is considerably low.", "startOffset": 68, "endOffset": 72}, {"referenceID": 21, "context": "Compared with other well-known clustering algorithms such as DBSCAN [21] and Mean shift [22], this complexity is considerably low.", "startOffset": 88, "endOffset": 92}, {"referenceID": 15, "context": "The existing efforts [16], [18] in enhancing the scalability of k-means for web-scale tasks often come with price of lower clustering quality.", "startOffset": 21, "endOffset": 25}, {"referenceID": 17, "context": "The existing efforts [16], [18] in enhancing the scalability of k-means for web-scale tasks often come with price of lower clustering quality.", "startOffset": 27, "endOffset": 31}, {"referenceID": 11, "context": "On the other hand, k-means++ proposed in [12], [17] focuses on enhancing the clustering quality by a careful design of the initialization procedure.", "startOffset": 41, "endOffset": 45}, {"referenceID": 16, "context": "On the other hand, k-means++ proposed in [12], [17] focuses on enhancing the clustering quality by a careful design of the initialization procedure.", "startOffset": 47, "endOffset": 51}, {"referenceID": 0, "context": "Inspired by the work in [1], a novel objective function is derived from Eqn.", "startOffset": 24, "endOffset": 27}, {"referenceID": 0, "context": "In addition, driven by the objective function, sample is moved from one cluster to another cluster when we find this movement leads to higher objective function score, which is known as incremental clustering [1], [23].", "startOffset": 209, "endOffset": 212}, {"referenceID": 0, "context": "Extensive experiments are conducted to contrast the performance of proposed method with k-means and its variants including tasks document clustering [1], nearest neighbor search (NNS) with product quantization [4] and image clustering.", "startOffset": 149, "endOffset": 152}, {"referenceID": 3, "context": "Extensive experiments are conducted to contrast the performance of proposed method with k-means and its variants including tasks document clustering [1], nearest neighbor search (NNS) with product quantization [4] and image clustering.", "startOffset": 210, "endOffset": 213}, {"referenceID": 22, "context": "Due to its versatility in different contexts, it has been studied in the last three decades [24].", "startOffset": 92, "endOffset": 96}, {"referenceID": 15, "context": "People are searching for clustering methods that are scalable [16], [17], [18] to web-scale data.", "startOffset": 62, "endOffset": 66}, {"referenceID": 16, "context": "People are searching for clustering methods that are scalable [16], [17], [18] to web-scale data.", "startOffset": 68, "endOffset": 72}, {"referenceID": 17, "context": "People are searching for clustering methods that are scalable [16], [17], [18] to web-scale data.", "startOffset": 74, "endOffset": 78}, {"referenceID": 11, "context": "[12], [17].", "startOffset": 0, "endOffset": 4}, {"referenceID": 16, "context": "[12], [17].", "startOffset": 6, "endOffset": 10}, {"referenceID": 11, "context": "According to [12], k-means iteration also converges faster due to the careful selection on the initial cluster centroids.", "startOffset": 13, "endOffset": 17}, {"referenceID": 16, "context": "Although the number of scanning rounds has been reduced to a few in [17], the extra computational cost is still inevitable.", "startOffset": 68, "endOffset": 72}, {"referenceID": 13, "context": "[14] proposed to index dataset in a KD Tree [25] to speed-up the sample-to-centroid nearest neighbor search.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[14] proposed to index dataset in a KD Tree [25] to speed-up the sample-to-centroid nearest neighbor search.", "startOffset": 44, "endOffset": 48}, {"referenceID": 24, "context": "[26].", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "A recent work [18] takes similar way to speed-up the nearest neighbor search by indexing dataset with inverted file structure.", "startOffset": 14, "endOffset": 18}, {"referenceID": 15, "context": "Namely, methods in [16], [27] only pick a small portion of the whole dataset to update the cluster centroids each time.", "startOffset": 19, "endOffset": 23}, {"referenceID": 25, "context": "Namely, methods in [16], [27] only pick a small portion of the whole dataset to update the cluster centroids each time.", "startOffset": 25, "endOffset": 29}, {"referenceID": 0, "context": "Apart from above methods, there is another easy way to reduce the number of comparisons between samples and centroids, namely performing clustering in a top-down hierarchical manner [1], [28], [29].", "startOffset": 182, "endOffset": 185}, {"referenceID": 26, "context": "Apart from above methods, there is another easy way to reduce the number of comparisons between samples and centroids, namely performing clustering in a top-down hierarchical manner [1], [28], [29].", "startOffset": 187, "endOffset": 191}, {"referenceID": 27, "context": "Apart from above methods, there is another easy way to reduce the number of comparisons between samples and centroids, namely performing clustering in a top-down hierarchical manner [1], [28], [29].", "startOffset": 193, "endOffset": 197}, {"referenceID": 0, "context": "In addition to that, another interesting idea from [1], [29] is that cluster centroids are updated incrementally [1], [23].", "startOffset": 51, "endOffset": 54}, {"referenceID": 27, "context": "In addition to that, another interesting idea from [1], [29] is that cluster centroids are updated incrementally [1], [23].", "startOffset": 56, "endOffset": 60}, {"referenceID": 0, "context": "In addition to that, another interesting idea from [1], [29] is that cluster centroids are updated incrementally [1], [23].", "startOffset": 113, "endOffset": 116}, {"referenceID": 0, "context": "Moreover, the update process is explicitly driven by an objective function (called as criterion function in [1], [29]).", "startOffset": 108, "endOffset": 111}, {"referenceID": 27, "context": "Moreover, the update process is explicitly driven by an objective function (called as criterion function in [1], [29]).", "startOffset": 113, "endOffset": 117}, {"referenceID": 0, "context": "Unfortunately, objective functions proposed in [1], [28], [29] are based on the assumption that input data are in unit length.", "startOffset": 47, "endOffset": 50}, {"referenceID": 26, "context": "Unfortunately, objective functions proposed in [1], [28], [29] are based on the assumption that input data are in unit length.", "startOffset": 52, "endOffset": 56}, {"referenceID": 27, "context": "Unfortunately, objective functions proposed in [1], [28], [29] are based on the assumption that input data are in unit length.", "startOffset": 58, "endOffset": 62}, {"referenceID": 0, "context": "In other word, objective function proposed in [1] is the special case of our proposed form.", "startOffset": 46, "endOffset": 49}, {"referenceID": 0, "context": "In this section, two objective functions (also known as criterion functions [1]) are developed.", "startOffset": 76, "endOffset": 79}, {"referenceID": 0, "context": "According to [1], objective functions are categorized into two groups.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "9 is in the same form as the first objective function in [1], they are derived from different initial objectives.", "startOffset": 57, "endOffset": 60}, {"referenceID": 0, "context": "Noticed that similar optimization objectives have been discussed under Cosine similarity measure in [1].", "startOffset": 100, "endOffset": 103}, {"referenceID": 0, "context": "Both clustering strategies are built upon incremental clustering [1], [23] and driven by objective function I\u2217 1 (Eqn.", "startOffset": 65, "endOffset": 68}, {"referenceID": 11, "context": "It is possible to do the initial assignment following the way of k-means or k-means++ [12].", "startOffset": 86, "endOffset": 90}, {"referenceID": 28, "context": "Note that this incremental updating scheme is essentially different from online learning vector quantization (LVQ) [30], in which the cluster centroids are updated incrementally.", "startOffset": 115, "endOffset": 119}, {"referenceID": 27, "context": "As discussed in [29], there are basically two ways to organize the priority queue.", "startOffset": 16, "endOffset": 20}, {"referenceID": 27, "context": "Similar as [29], we find splitting the biggest cluster usually demonstrates more stable performance.", "startOffset": 11, "endOffset": 15}, {"referenceID": 4, "context": "In the first experiment, dataset SIFT1M [5] is adopted to evaluate the clustering quality.", "startOffset": 40, "endOffset": 43}, {"referenceID": 4, "context": "In the second experiment, BKM is tested on the nearest neighbor search task based on product quantizer (PQ) [5] in which this method is adopted for quantizer training.", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Following the practice of [1], [29], 15 document datasets2 have been adopted.", "startOffset": 26, "endOffset": 29}, {"referenceID": 27, "context": "Following the practice of [1], [29], 15 document datasets2 have been adopted.", "startOffset": 31, "endOffset": 35}, {"referenceID": 15, "context": "In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison.", "startOffset": 65, "endOffset": 69}, {"referenceID": 27, "context": "In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison.", "startOffset": 104, "endOffset": 108}, {"referenceID": 28, "context": "In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison.", "startOffset": 152, "endOffset": 156}, {"referenceID": 11, "context": "In addition, representative k-means variants, such as Mini-Batch [16], Repeated Bisecting k-means (RBK) [29], online Learning Vector Quantization (LVQ) [30] and k-means++ [12] are considered in the comparison.", "startOffset": 171, "endOffset": 175}, {"referenceID": 29, "context": "The experiment in this section is conducted using 1 million SIFT features [31].", "startOffset": 74, "endOffset": 78}, {"referenceID": 1, "context": "17) is adopted for evaluation [2], which takes average over Eqn.", "startOffset": 30, "endOffset": 33}, {"referenceID": 7, "context": "k-means boost k-means Initial assigment k-way bisecting k-way bisecting Random k-means [8] BsKM BKM(rnd) BsBKM(rnd) Probability based [12] k-means++ [12] BsBKM++ BKM(kpp) BsBKM(kpp) None - BKM(non) BsBKM(non)", "startOffset": 87, "endOffset": 90}, {"referenceID": 11, "context": "k-means boost k-means Initial assigment k-way bisecting k-way bisecting Random k-means [8] BsKM BKM(rnd) BsBKM(rnd) Probability based [12] k-means++ [12] BsBKM++ BKM(kpp) BsBKM(kpp) None - BKM(non) BsBKM(non)", "startOffset": 134, "endOffset": 138}, {"referenceID": 11, "context": "k-means boost k-means Initial assigment k-way bisecting k-way bisecting Random k-means [8] BsKM BKM(rnd) BsBKM(rnd) Probability based [12] k-means++ [12] BsBKM++ BKM(kpp) BsBKM(kpp) None - BKM(non) BsBKM(non)", "startOffset": 149, "endOffset": 153}, {"referenceID": 4, "context": "This chart is plotted with 128 clustering runs (k = 1, 024) on SIFT100K [5] for each method.", "startOffset": 72, "endOffset": 75}, {"referenceID": 4, "context": "In this section, BKM is applied for visual vocabulary training using product quantization [5].", "startOffset": 90, "endOffset": 93}, {"referenceID": 4, "context": "Following the practice of [5], 100K SIFT features are used for product quantizer training, while SIFT1M set [5] is encoded with the trained product quantizers as the reference set for nearest neighbor search (NNS).", "startOffset": 26, "endOffset": 29}, {"referenceID": 4, "context": "Following the practice of [5], 100K SIFT features are used for product quantizer training, while SIFT1M set [5] is encoded with the trained product quantizers as the reference set for nearest neighbor search (NNS).", "startOffset": 108, "endOffset": 111}, {"referenceID": 0, "context": "Following in [1], 15 document datasets are used for evaluation.", "startOffset": 13, "endOffset": 16}, {"referenceID": 0, "context": "Similar to [1], entropy is adopted for the evaluation as following", "startOffset": 11, "endOffset": 14}, {"referenceID": 4, "context": "The assymetric distance calculation (ADC) [5] is adopted for nearest neighbor search.", "startOffset": 42, "endOffset": 45}, {"referenceID": 27, "context": "Furthermore, methods with bisecting strategy demonstrate slightly better performance than that of direct k-way in the document clustering task, which shares similar observation as [29].", "startOffset": 180, "endOffset": 184}, {"referenceID": 30, "context": "The experiment is conducted on 10 million Flickr images (Flickr10M), which are a subset of YFCC100M [32].", "startOffset": 100, "endOffset": 104}, {"referenceID": 31, "context": "Hessian-Affine [33] keypoints are extracted from each image and are described by RootSIFT feature [34].", "startOffset": 15, "endOffset": 19}, {"referenceID": 32, "context": "Hessian-Affine [33] keypoints are extracted from each image and are described by RootSIFT feature [34].", "startOffset": 98, "endOffset": 102}, {"referenceID": 33, "context": "Finally, the RootSIFT features from each image are pooled by VLAD [35] with a small visual vocabulary of size 64.", "startOffset": 66, "endOffset": 70}, {"referenceID": 33, "context": "Following [35], the final VLAD vector is normalized to unit length.", "startOffset": 10, "endOffset": 14}], "year": 2016, "abstractText": "Due to its simplicity and versatility, k-means remains popular since it was proposed three decades ago. The performance of k-means has been enhanced from different perspectives over the years. Unfortunately, a good trade-off between quality and efficiency is hardly reached. In this paper, a novel k-means variant is presented. Different from most of k-means variants, the clustering procedure is driven by an explicit objective function, which is feasible for the whole l2-space. The classic egg-chicken loop in k-means has been simplified to a pure stochastic optimization procedure. The procedure of k-means becomes simpler and converges to a considerably better local optima. The effectiveness of this new variant has been studied extensively in different contexts, such as document clustering, nearest neighbor search and image clustering. Superior performance is observed across different scenarios.", "creator": "LaTeX with hyperref package"}}}