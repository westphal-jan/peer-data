{"id": "1610.02527", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2016", "title": "Federated Optimization: Distributed Machine Learning for On-Device Intelligence", "abstract": "We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal.", "histories": [["v1", "Sat, 8 Oct 2016 13:25:15 GMT  (243kb,D)", "http://arxiv.org/abs/1610.02527v1", "38 pages"]], "COMMENTS": "38 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jakub kone\\v{c}n\\'y", "h brendan mcmahan", "daniel ramage", "peter richt\\'arik"], "accepted": false, "id": "1610.02527"}, "pdf": {"name": "1610.02527.pdf", "metadata": {"source": "CRF", "title": "Federated Optimization: Distributed Machine Learning for On-Device Intelligence", "authors": ["Jakub Kone\u010dn\u00fd", "H. Brendan McMahan"], "emails": ["kubo.konecny@gmail.com", "mcmahan@google.com", "dramage@google.com", "peter.richtarik@ed.ac.uk"], "sections": [{"heading": null, "text": "A motivating example is when we store training data locally on users \"mobile devices, rather than logging them into a data center for training purposes. In federated optimization, the devices are used as computing nodes, which perform the calculation of their local data to update a global model. We assume that we have an extremely large number of devices on the network - as many as the number of users of a particular service, each of which has only a tiny fraction of the available aggregate data. In particular, we expect the number of locally available data points to be much smaller than the number of devices. Moreover, as different users generate data with different patterns, no device is expected to have a representative sample of the total distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm that shows encouraging experimental results for sparse convex problems."}, {"heading": "1 Introduction", "text": "In many cases, these devices are rarely separated from their owners, and the combination of rich user interactions and powerful sensors means that they have access to an unprecedented amount of data, much of which is private. We advocate an alternative - federated learning - that includes the training data distributed on mobile devices, and learn a common model by aggregating locally computed updates over the years. 161 0.02 527v 1 [cs.L] central coordination server. This is a direct application of the principle of concentrated collection or data minimization proposed by privacy."}, {"heading": "1.1 Problem Formulation", "text": "In fact, it is such that it is a matter of a way, in which it concerns a way, in which people live in the real world, in the real world, in which they live, in the real world, in which they live, in the real world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in the real world, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live."}, {"heading": "1.2 The Setting of Federated Optimization", "text": "The main purpose of this paper is to draw the attention of machine learning and optimization communities to a new and increasingly practical distributed optimization environment in which none of the typical assumptions are met, and communication efficiency is of paramount importance. In particular, federated optimization algorithms potentially need to deal with the following data: \u2022 The data on each node can be distributed across a large number of nodes. In particular, the number of available data points is far from being a representative sample of the general distribution. \u2022 Unbalanced nodes can be stored by orders of magnitude in the number of training examples. \u2022 Non-IID: Data on each node can be drawn from a different distribution; that is, the available data points are far from being a representative sample of the total distribution."}, {"heading": "2 Related Work", "text": "In this section we will give a detailed overview of the relevant literature. We will focus in particular on algorithms that can be used to solve problem (1) in different contexts. First, in sections 2.1 and 2.2 we will look at algorithms that should be run on a single computer. In section 2.3 we will follow with a discussion of the distributed environment in which no single node has direct access to all the data described in f. We will describe a paradigm for measuring the efficiency of distributed methods, followed by an overview of existing methods and comments on whether or not they have been developed with a view to communication efficiency."}, {"heading": "2.1 Baseline Algorithms", "text": "This year it has come to the point that it has never come as far as this year."}, {"heading": "2.2 A Novel Breed of Randomized Algorithms", "text": "In fact, the fact is that most of them are able to outdo themselves and that they are able to outdo themselves. \"(S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S. S."}, {"heading": "2.3 Distributed Setting", "text": "In this section, we look at the literature on algorithms to solve (1) a problem in the distributed environment. When we talk about distributed settings, we refer to the case where the data describing the fi functions are not stored on a single storage device; this may also include settings where one's own data simply does not fit into a single RAM / computer / node, but two are sufficient; this also covers the case where the data is distributed over multiple data centers around the world and over many nodes in those data centers. The point is that there is no single processing unit in the system that would have direct access to all the data. Therefore, distributed settings do not include parallels between individual processors. Compared to local calculations on each node, the cost of communication between nodes is much higher in terms of both speed and energy consumption [6, 89] in terms of introducing new computer-based challenges, not only for optimization procedures."}, {"heading": "2.3.1 A Paradigm for Measuring Distributed Optimization Efficiency", "text": "In fact, most of us are able to save ourselves. (...) It is as if most of us are not able to save ourselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...) It is as if they were able to save themselves. (...)"}, {"heading": "2.3.2 Distributed Algorithms", "text": "In fact, it is not the case that they are able to hide themselves as they have done in recent years. (...) It is not the case that they are able to trump themselves. (...) It is not the case that they are able to trump themselves. (...) It is not the case that they are able to trump themselves. (...) It is not the case that they are able to trump themselves. (...) It is the case that they are not able to trump themselves. (...) It is not the case that they are able to trump themselves. (...) It is not the case that they are able to trump themselves. (...) () () () () () () (() () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () () () () () () () () () () () () () () () () () () () () () () ()) () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () () (() () () () ((() () () () (() (() () () (() () (() () (() (() (() () () (() (() (() ((() () ((() (() (() ((() (() (() ((() (() (() ((() (() ((() (() ((() () (((() (() ((() () (((() (() ((() (() ((((()) (((((((())) (((((((())"}, {"heading": "2.3.3 Communication-Efficient Algorithms", "text": "This year, it is as far as ever in the history of the city, where it is as far as never before in the history of the city."}, {"heading": "3 Algorithms for Federated Optimization", "text": "In this section, we will present the first algorithm designed with the unique challenges of federated optimization in mind. Before proceeding with the explanation, we will first return to two important and, at first glance, unrelated algorithms: The connection between these algorithms helped motivate our research, namely the stochastic variance-reduced gradient (SVRG) [43, 47], a stochastic method with explicit variance reduction, and the distributed approximated newton (DANE) [91] for distributed optimization. Following the descriptions, their connection leads to a new distributed optimization algorithm, which at first sight is almost identical to the SVRG algorithm we call Federated SVRG (FSVRG). Although this algorithm seems to work well in practice under simple circumstances, its performance in the general environment we specify in Section 3.3 is still unsatisfactory."}, {"heading": "3.1 Desirable Algorithmic Properties", "text": "It is a useful thought experiment to consider the properties that one hopes to find in an algorithm for the non-IID, unbalanced, and massively distributed setting that we are looking at. Specifically: (A) If the algorithm is initialized to the optimal solution, it stays there. (B) If all the data is on a single node, the algorithm should converge into O (1) rounds of communication. (D) If each feature occurs on a single node, the problems are completely separable (each machine essentially learns a fragmented block of parameters), then the algorithm should converge into O (1) rounds of communication. (D) If each node contains an identical data set, then the algorithm should converge into O (1) rounds of communication. In the case of convex problems, \"convergence\" has the usual technical meaning of finding a solution sufficiently close to the global minimum, but these properties are also useful for non-converged problems."}, {"heading": "3.2 SVRG", "text": "The SVRG algorithm runs in two nested loops."}, {"heading": "3.3 Distributed Problem Formulation", "text": "In this section, we will introduce the notation and specify the structure of the distributed version of the problem we are looking at (1), focusing on the case in which the fi are convex. We will assume that the data {xi, yi} ni = 1 describing the fi functions are stored over a large number of nodes. Let K be the number of nodes. Let Pk denote a partition of the data point indexes {1,.., n} for k {1,..., K}, so that Pk is the amount stored on the k node, and define nk = | Pk |. That is, let us assume that Pk, Pl =, every time k 6 = l, and K = 1 nk = nk = n. We will then define the local empirical loss asFk (w) def = 1nk = empirical k (w)."}, {"heading": "3.4 DANE", "text": "In this section, we will introduce a general reasoning that provides greater intuitive support for the DANE algorithms, which we will describe in more detail below. We will pursue this reasoning in Appendix A. We will establish a link between two existing methods that are not known in the literature. If we want to design a distributed algorithm to solve the problem described above (8), we will not prioritize the data that describe how it works. The first, and as we will see, is a rather naive idea to minimize each of its local functions, and the average results (a variant of this idea appeared in [107]): wt + 1k = arg min. Rd Fk (w), w t + 1 = 1 nk + 1k. Clearly, it makes no sense to run these algorithms for more than one iteration."}, {"heading": "3.5 SVRG meets DANE", "text": "Another point that is considered a disadvantage of DANE is the need to find the exact minimum of (10) where an arbitrary optimization algorithm can be used to obtain relative accuracy on a locally defined subproblem. We replace the exact optimization with an approximate solution by using all optimization algorithms that could be used to solve the SVRG algorithms."}, {"heading": "3.6 Federated SVRG", "text": "Empirically, algorithm 3 fits into the model of distributed optimization efficiency described in Section 2.3.1, as we can weigh how many stochastic iterations should be performed locally against communication costs. However, several modifications are necessary to achieve good performance in the full federated optimization setting (Section 3.3). A very important aspect to address is that the number of data points available to a particular node can vary greatly from the average number of data points available to each individual node. Moreover, this setting always comes from the fact that the locally available data is bundled around a particular pattern and therefore does not represent a representative sample of the overall distribution we are trying to learn. In the Experiments section, we focus on the case of regulated logistic regression of L2, but the ideas are transferred to other generalized linear prediction problems."}, {"heading": "3.6.1 Notation", "text": "Note that in the case of large-scale generalized linear prediction problems, the data that results is almost always sparse, for example due to the appearance of the \"bag-of-words\" wording style. This means that only a small subset of d elements of the vector xi have unequal values. In this class of problems, the gradient \"fi (w) is a multiple of the data vector x. this creates additional complications, but also the potential for using the problem structure and thus faster algorithms. Before proceeding, we summarize and label a number of quantities needed to describe the algorithm. \u2022 n - number of data points / training examples / functions. \u2022 Pk - set of indices corresponding to data points stored on the device. \u2022 nk = number of data points stored on the device k. \u2022 nj = number of data points stored on the device k. \u2022 nj = number of data points stored on the device k. \u2022 nj = number of data points stored on the device k."}, {"heading": "3.6.2 Intuition Behind FSVRG Updates", "text": "(we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we). (we)....................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "3.7 Further Notes", "text": "If we look at the first sentence, we see the equivalence of two algorithms, take the second and try to modify it to make it suitable for determining a federated optimization. Of course, the question arises: Is it possible to achieve the same by modifying the first algorithm that is suitable for federated optimization - by changing only the local optimization goal? We have actually tried to experiment with the idea, but we do not report the details for two reasons. First, the requirement for an exact solution of the local partial problem is often impractical."}, {"heading": "4 Experiments", "text": "In this section, we present the first experimental results in the context of federated optimization. In particular, we provide results based on a data set based on public Google + -Posts8 and clustered by the user - simulating each user as an independent node. This preliminary experiment shows why none of the existing algorithms is suitable for federated optimization and how robust our proposed method is against challenges that occur there."}, {"heading": "4.1 Predicting Comments on Public Google+ Posts", "text": "The data sets presented here are based on public Google + posts. We randomly selected 10, 000 authors who have at least 100 public posts in English, and try to predict whether a post will receive at least one comment (i.e., a binary classification task). We only divide the data chronologically on a per-author basis, taking the previous 75% for training and the following 25% for testing. The total number of training examples is n = 2, 166, 693. We have created a simple term model based on the 20, 000 most common words in dictionaries, which are based on all Google + data. These results in a problem with the dimension d = 20, 002, the additional two functions represent a bias term and variable for unknown words. We then use a logistic regression model to make a prediction based on these traits. We form the distributed optimization problem as follows."}, {"heading": "5 Conclusions and Future Challenges", "text": "We have introduced a new setting for distributed optimization, which we call federal optimization. This setting is motivated by the vision outlined, in which users do not share the data they generate with companies, but make part of their computing power available to solve optimization problems. We explain why the existing methods are not applicable in this area. Also, the distributed algorithms, which can be applied very slowly in the presence of nodes, must be addressed by the optimization problems. We explain why the existing methods are not applicable."}, {"heading": "A Distributed Optimization via Quadratic Perturbations", "text": "This is the consequence of a discussion that leads DANE to a general algorithmic error (9). (9) We use this method to propose a similar but new method, which, unlike DANE, is identical for all types of data. (9) We emphasize its relationship to the dual CoCoCoA algorithms for distributed optimizations. (9) We now assume that this method is identical for all k. (2) All arguments can be easily expanded, but would change the notation for current Purpose.A.1 New method We are now introducing a new method (5), which also belongs to the family of square perturbation (9)."}], "references": [{"title": "Deep learning with differential privacy", "author": ["Mart\u0301\u0131n Abadi", "Andy Chu", "Ian Goodfellow", "Brendan H McMahan", "Ilya Mironov", "Kunal Talwar", "Li Zhang"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2016}, {"title": "Distributed delayed stochastic optimization", "author": ["Alekh Agarwal", "John C Duchi"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2011}, {"title": "Katyusha: The first direct acceleration of stochastic gradient methods", "author": ["Zeyuan Allen-Zhu"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2016}, {"title": "Exploiting the structure: Stochastic gradient methods using raw clusters", "author": ["Zeyuan Allen-Zhu", "Yang Yuan", "Karthik Sridharan"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}, {"title": "Communication complexity of distributed convex learning and optimization", "author": ["Yossi Arjevani", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Scaling up machine learning: Parallel and distributed approaches", "author": ["Ron Bekkerman", "Mikhail Bilenko", "John Langford"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2011}, {"title": "Distributed asynchronous computation of fixed points", "author": ["Dimitri P Bertsekas"], "venue": "Mathematical Programming,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 1983}, {"title": "Parallel and distributed computation: numerical methods", "author": ["Dimitri P Bertsekas", "John N Tsitsiklis"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 1989}, {"title": "Sgd-qn: Careful quasi-Newton stochastic gradient descent", "author": ["Antoine Bordes", "L\u00e9on Bottou", "Patrick Gallinari"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2009}, {"title": "Curiously fast convergence of some stochastic gradient descent algorithms", "author": ["L\u00e9on Bottou"], "venue": "In Proceedings of the symposium on learning and data science,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2009}, {"title": "Stochastic gradient descent tricks", "author": ["L\u00e9on Bottou"], "venue": null, "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2012}, {"title": "Optimization methods for large-scale machine learning", "author": ["L\u00e9on Bottou", "Frank E Curtis", "Jorge Nocedal"], "venue": null, "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "The tradeoffs of large scale learning", "author": ["Olivier Bousquet", "L\u00e9on Bottou"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2008}, {"title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "author": ["Stephen Boyd", "Neal Parikh", "Eric Chu", "Borja Peleato", "Jonathan Eckstein"], "venue": "Foundations and Trends R  \u00a9 in Machine Learning,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2011}, {"title": "Parallel coordinate descent for l1regularized loss minimization", "author": ["Joseph Bradley", "Aapo Kyrola", "Daniel Bickson", "Carlos Guestrin"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2011}, {"title": "A stochastic quasi-Newton method for large-scale optimization", "author": ["Richard H Byrd", "Samantha L Hansen", "Jorge Nocedal", "Yoram Singer"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Differentially private empirical risk minimization", "author": ["Kamalika Chaudhuri", "Claire Monteleoni", "Anand D Sarwate"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2011}, {"title": "Project adam: Building an efficient and scalable deep learning training system", "author": ["Trishul Chilimbi", "Yutaka Suzue", "Johnson Apacible", "Karthik Kalyanaraman"], "venue": "In 11th USENIX Symposium on Operating Systems Design and Implementation", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2014}, {"title": "Primal method for ERM with flexible mini-batching schemes and nonconvex losses", "author": ["Dominik Csiba", "Peter Richt\u00e1rik"], "venue": null, "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Coordinate descent face-off: primal or dual", "author": ["Dominik Csiba", "Peter Richt\u00e1rik"], "venue": null, "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2016}, {"title": "Taming the wild: A unified analysis of hogwild-style algorithms", "author": ["Christopher M De Sa", "Ce Zhang", "Kunle Olukotun", "Christopher R\u00e9"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Large scale distributed deep networks", "author": ["Jeffrey Dean", "Greg Corrado", "Rajat Monga", "Kai Chen", "Matthieu Devin", "Mark Mao", "Andrew Senior", "Paul Tucker", "Ke Yang", "Quoc V Le"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "MapReduce: Simplified data processing on large clusters", "author": ["Jeffrey Dean", "Sanjay Ghemawat"], "venue": "Communications of the ACM,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2008}, {"title": "A simple practical accelerated method for finite sums", "author": ["Aaron Defazio"], "venue": "arXiv preprint arXiv:1602.02442,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "SAGA: A fast incremental gradient method with support for non-strongly convex composite objectives", "author": ["Aaron Defazio", "Francis Bach", "Simon Lacoste-Julien"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2014}, {"title": "Optimal distributed online prediction using mini-batches", "author": ["Ofer Dekel", "Ran Gilad-Bachrach", "Ohad Shamir", "Lin Xiao"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2012}, {"title": "On the global and linear convergence of the generalized alternating direction method of multipliers", "author": ["Wei Deng", "Wotao Yin"], "venue": "Journal of Scientific Computing,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Dual averaging for distributed optimization: convergence analysis and network scaling", "author": ["John C Duchi", "Alekh Agarwal", "Martin J Wainwright"], "venue": "Automatic control, IEEE Transactions on,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2012}, {"title": "Asynchronous stochastic convex optimization", "author": ["John C Duchi", "Sorathan Chaturapruek", "Christopher R\u00e9"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Estimation, optimization, and parallelism when data is sparse", "author": ["John C Duchi", "Michael I Jordan", "Brendan H McMahan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2013}, {"title": "Privacy aware learning", "author": ["John C Duchi", "Michael I Jordan", "Martin J Wainwright"], "venue": "Journal of the Association for Computing Machinery,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2014}, {"title": "The Algorithmic Foundations of Differential Privacy", "author": ["Cynthia Dwork", "Aaron Roth"], "venue": "Foundations and Trends in Theoretical Computer Science. Now Publishers,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Fast distributed coordinate descent for nonstrongly convex losses", "author": ["Olivier Fercoq", "Zheng Qu", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "In Machine Learning for Signal Processing (MLSP),", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2014}, {"title": "Accelerated, parallel, and proximal coordinate descent", "author": ["Olivier Fercoq", "Peter Richt\u00e1rik"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}, {"title": "Un-regularizing: approximate proximal point and faster stochastic algorithms for empirical risk minimization", "author": ["Roy Frostig", "Rong Ge", "Sham M Kakade", "Aaron Sidford"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning (ICML),", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2015}, {"title": "Stochastic block BFGS: squeezing more curvature out of data", "author": ["Robert Mansel Gower", "Donald Goldfarb", "Peter Richt\u00e1rik"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2016}, {"title": "Randomized quasi-Newton updates are linearly convergent matrix inversion algorithms", "author": ["Robert Mansel Gower", "Peter Richt\u00e1rik"], "venue": null, "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2016}, {"title": "Why random reshuffling beats stochastic gradient descent", "author": ["Mert G\u00fcrb\u00fczbalaban", "Asu Ozdaglar", "Pablo Parrilo"], "venue": null, "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Stop wasting my gradients: Practical SVRG", "author": ["Reza Harikandeh", "Mohamed Osama Ahmed", "Alim Virani", "Mark Schmidt", "Jakub Kone\u010dn\u00fd", "Scott Sallinen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Communication-efficient distributed dual coordinate ascent", "author": ["Martin Jaggi", "Virginia Smith", "Martin Tak\u00e1\u010d", "Jonathan Terhorst", "Sanjay Krishnan", "Thomas Hofmann", "Michael I Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "42", "shortCiteRegEx": "42", "year": 2014}, {"title": "Accelerating stochastic gradient descent using predictive variance reduction", "author": ["Rie Johnson", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2013}, {"title": "Mini-batch semi-stochastic gradient descent in the proximal setting", "author": ["Jakub Kone\u010dn\u00fd", "Jie Liu", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "IEEE Journal of Selected Topics in Signal Processing,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 2016}, {"title": "Federated optimization: Distributed optimization beyond the datacenter", "author": ["Jakub Kone\u010dn\u00fd", "Brendan H McMahan", "Daniel Ramage"], "venue": null, "citeRegEx": "45", "shortCiteRegEx": "45", "year": 2015}, {"title": "Semi-stochastic gradient descent methods", "author": ["Jakub Kone\u010dn\u00fd", "Peter Richt\u00e1rik"], "venue": null, "citeRegEx": "47", "shortCiteRegEx": "47", "year": 2013}, {"title": "ASAGA: Asynchronous parallel saga", "author": ["R\u00e9mi Leblond", "Fabian Pedregosa", "Simon Lacoste-Julien"], "venue": null, "citeRegEx": "48", "shortCiteRegEx": "48", "year": 2016}, {"title": "Efficient backprop", "author": ["Yann A LeCun", "L\u00e9on Bottou", "Genevieve B Orr", "Klaus-Robert M\u00fcller"], "venue": "In Neural networks: Tricks of the trade,", "citeRegEx": "49", "shortCiteRegEx": "49", "year": 2012}, {"title": "Distributed stochastic variance reduced gradient methods", "author": ["Jason Lee", "Tengyu Ma", "Qihang Lin"], "venue": null, "citeRegEx": "50", "shortCiteRegEx": "50", "year": 2015}, {"title": "Efficient accelerated coordinate descent methods and faster algorithms for solving linear systems", "author": ["Yin Tat Lee", "Aaron Sidford"], "venue": "In Foundations of Computer Science (FOCS),", "citeRegEx": "51", "shortCiteRegEx": "51", "year": 2013}, {"title": "Large-scale logistic regression and linear support vector machines using spark", "author": ["Chieh-Yen Lin", "Cheng-Hao Tsai", "Ching-Pei Lee", "Chih-Jen Lin"], "venue": "In Big Data (Big Data),", "citeRegEx": "52", "shortCiteRegEx": "52", "year": 2014}, {"title": "A universal catalyst for first-order optimization", "author": ["Hongzhou Lin", "Julien Mairal", "Zaid Harchaoui"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "53", "shortCiteRegEx": "53", "year": 2015}, {"title": "On the limited memory BFGS method for large scale optimization", "author": ["Dong C Liu", "Jorge Nocedal"], "venue": "Mathematical programming,", "citeRegEx": "54", "shortCiteRegEx": "54", "year": 1989}, {"title": "Asynchronous stochastic coordinate descent: Parallelism and convergence properties", "author": ["Ji Liu", "Stephen J Wright"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "55", "shortCiteRegEx": "55", "year": 2015}, {"title": "An asynchronous parallel stochastic coordinate descent algorithm", "author": ["Ji Liu", "Stephen J Wright", "Christopher R\u00e9", "Victor Bittorf", "Srikrishna Sridhar"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "56", "shortCiteRegEx": "56", "year": 2015}, {"title": "Distributed optimization with arbitrary local solvers", "author": ["Chenxin Ma", "Jakub Kone\u010dn\u00fd", "Martin Jaggi", "Virginia Smith", "Michael I Jordan", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": null, "citeRegEx": "57", "shortCiteRegEx": "57", "year": 2015}, {"title": "Adding vs. averaging in distributed primal-dual optimization", "author": ["Chenxin Ma", "Virginia Smith", "Martin Jaggi", "Michael Jordan", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "In Proceedings of The 32nd International Conference on Machine Learning,", "citeRegEx": "58", "shortCiteRegEx": "58", "year": 2015}, {"title": "An efficient distributed learning algorithm based on effective local functional approximations", "author": ["Dhruv Mahajan", "Nikunj Agrawal", "S Sathiya Keerthi", "S Sundararajan", "Leon Bottou"], "venue": "arXiv preprint arXiv:1310.8418,", "citeRegEx": "59", "shortCiteRegEx": "59", "year": 2013}, {"title": "Perturbed iterate analysis for asynchronous stochastic optimization", "author": ["Horia Mania", "Xinghao Pan", "Dimitris Papailiopoulos", "Benjamin Recht", "Kannan Ramchandran", "Michael I Jordan"], "venue": null, "citeRegEx": "60", "shortCiteRegEx": "60", "year": 2015}, {"title": "Distributed block coordinate descent for minimizing partially separable functions", "author": ["Jakub Mare\u010dek", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "In Numerical Analysis and Optimization,", "citeRegEx": "61", "shortCiteRegEx": "61", "year": 2015}, {"title": "Federated learning of deep networks using model averaging", "author": ["Brendan H McMahan", "Eider Moore", "Daniel Ramage", "Blaise Aguera y Arcas"], "venue": null, "citeRegEx": "62", "shortCiteRegEx": "62", "year": 2016}, {"title": "A linearly-convergent stochastic l-bfgs algorithm", "author": ["Philipp Moritz", "Robert Nishihara", "Michael Jordan"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "63", "shortCiteRegEx": "63", "year": 2016}, {"title": "Non-asymptotic analysis of stochastic approximation algorithms for machine learning", "author": ["Eric Moulines", "Francis R Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "64", "shortCiteRegEx": "64", "year": 2011}, {"title": "Stochastic gradient descent, weighted sampling, and the randomized kaczmarz algorithm", "author": ["Deanna Needell", "Rachel Ward", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "65", "shortCiteRegEx": "65", "year": 2014}, {"title": "Robust stochastic approximation approach to stochastic programming", "author": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "66", "shortCiteRegEx": "66", "year": 2009}, {"title": "Efficiency of coordinate descent methods on huge-scale optimization problems", "author": ["Yu Nesterov"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "67", "shortCiteRegEx": "67", "year": 2012}, {"title": "A method of solving a convex programming problem with convergence rate o(1/k)", "author": ["Yurii Nesterov"], "venue": "Soviet Mathematics Doklady,", "citeRegEx": "68", "shortCiteRegEx": "68", "year": 1983}, {"title": "Introductory Lectures on Convex Optimization", "author": ["Yurii Nesterov"], "venue": "A Basic Course. Kluwer,", "citeRegEx": "69", "shortCiteRegEx": "69", "year": 2004}, {"title": "On optimization methods for deep learning", "author": ["Jiquan Ngiam", "Adam Coates", "Ahbik Lahiri", "Bobby Prochnow", "Quoc V Le", "Andrew Y Ng"], "venue": "In Proceedings of the 28th International Conference on Machine Learning,", "citeRegEx": "70", "shortCiteRegEx": "70", "year": 2011}, {"title": "Hogwild: A lock-free approach to parallelizing stochastic gradient descent", "author": ["Feng Niu", "Benjamin Recht", "Christopher Re", "Stephen Wright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "71", "shortCiteRegEx": "71", "year": 2011}, {"title": "Arock: an algorithmic framework for asynchronous parallel coordinate updates", "author": ["Zhimin Peng", "Yangyang Xu", "Ming Yan", "Wotao Yin"], "venue": null, "citeRegEx": "72", "shortCiteRegEx": "72", "year": 2015}, {"title": "Some methods of speeding up the convergence of iteration methods", "author": ["B.T. Polyak"], "venue": "USSR Computational Mathematics and Mathematical Physics,", "citeRegEx": "73", "shortCiteRegEx": "73", "year": 1964}, {"title": "SDNA: Stochastic dual newton ascent for empirical risk minimization", "author": ["Zheng Qu", "Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d", "Olivier Fercoq"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "citeRegEx": "74", "shortCiteRegEx": "74", "year": 2016}, {"title": "Quartz: Randomized dual coordinate ascent with arbitrary sampling", "author": ["Zheng Qu", "Peter Richt\u00e1rik", "Tong Zhang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "75", "shortCiteRegEx": "75", "year": 2015}, {"title": "On variance reduction in stochastic gradient descent and its asynchronous variants", "author": ["Sashank J Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnab\u00e1s P\u00f3cz\u00f3s", "Alex Smola"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "76", "shortCiteRegEx": "76", "year": 2015}, {"title": "Stochastic variance reduction for nonconvex optimization", "author": ["Sashank J Reddi", "Ahmed Hefny", "Suvrit Sra", "Barnab\u00e1s P\u00f3cz\u00f3s", "Alex Smola"], "venue": null, "citeRegEx": "77", "shortCiteRegEx": "77", "year": 2016}, {"title": "AIDE: Fast and communication efficient distributed optimization", "author": ["Sashank J Reddi", "Jakub Kone\u010dn\u00fd", "Peter Richt\u00e1rik", "Barnab\u00e1s P\u00f3cz\u00f3s", "Alex Smola"], "venue": null, "citeRegEx": "78", "shortCiteRegEx": "78", "year": 2016}, {"title": "Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "79", "shortCiteRegEx": "79", "year": 2014}, {"title": "Parallel coordinate descent methods for big data optimization", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Mathematical Programming,", "citeRegEx": "80", "shortCiteRegEx": "80", "year": 2016}, {"title": "Distributed coordinate descent method for learning with big data", "author": ["Peter Richt\u00e1rik", "Martin Tak\u00e1\u010d"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "81", "shortCiteRegEx": "81", "year": 2016}, {"title": "A stochastic approximation method", "author": ["Herbert Robbins", "Sutton Monro"], "venue": "The Annals of Mathematical Statistics,", "citeRegEx": "82", "shortCiteRegEx": "82", "year": 1951}, {"title": "A stochastic gradient method with an exponential convergence rate for finite training sets", "author": ["Nicolas Le Roux", "Mark Schmidt", "Francis Bach"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "83", "shortCiteRegEx": "83", "year": 2012}, {"title": "Nonuniform stochastic average gradient method for training conditional random fields", "author": ["Mark Schmidt", "Reza Babanezhad", "Mohamed Ahmed", "Aaron Defazio", "Ann Clifton", "Anoop Sarkar"], "venue": "In Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "84", "shortCiteRegEx": "84", "year": 2015}, {"title": "Minimizing finite sums with the stochastic average gradient", "author": ["Mark Schmidt", "Nicolas Le Roux", "Francis Bach"], "venue": null, "citeRegEx": "85", "shortCiteRegEx": "85", "year": 2013}, {"title": "SDCA without duality, regularization, and individual convexity", "author": ["Shai Shalev-Shwartz"], "venue": null, "citeRegEx": "86", "shortCiteRegEx": "86", "year": 2016}, {"title": "Pegasos: Primal estimated subgradient solver for svm", "author": ["Shai Shalev-Shwartz", "Yoram Singer", "Nathan Srebro", "Andrew Cotter"], "venue": "Mathematical programming,", "citeRegEx": "87", "shortCiteRegEx": "87", "year": 2011}, {"title": "Stochastic dual coordinate ascent methods for regularized loss", "author": ["Shai Shalev-Shwartz", "Tong Zhang"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "88", "shortCiteRegEx": "88", "year": 2013}, {"title": "Exascale computing technology challenges", "author": ["John Shalf", "Sudip Dosanjh", "John Morrison"], "venue": "In High Performance Computing for Computational Science\u2013VECPAR", "citeRegEx": "89", "shortCiteRegEx": "89", "year": 2010}, {"title": "Distributed stochastic optimization and learning", "author": ["Ohad Shamir", "Nathan Srebro"], "venue": "In Communication, Control and Computing (Allerton),", "citeRegEx": "90", "shortCiteRegEx": "90", "year": 2014}, {"title": "Communication-efficient distributed optimization using an approximate newton-type method", "author": ["Ohad Shamir", "Nati Srebro", "Tong Zhang"], "venue": "In Proceedings of the 31st International Conference on Machine Learning,", "citeRegEx": "91", "shortCiteRegEx": "91", "year": 2014}, {"title": "L1-regularized distributed optimization: A communication-efficient primal-dual framework", "author": ["Virginia Smith", "Simone Forte", "Michael I Jordan", "Martin Jaggi"], "venue": null, "citeRegEx": "92", "shortCiteRegEx": "92", "year": 2015}, {"title": "Mini-batch primal and dual methods for SVMs", "author": ["Martin Tak\u00e1\u010d", "Avleen Bijral", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": "In Proceedings of the 30th International Conference on Machine Learning,", "citeRegEx": "93", "shortCiteRegEx": "93", "year": 2013}, {"title": "Distributed mini-batch SDCA", "author": ["Martin Tak\u00e1\u010d", "Peter Richt\u00e1rik", "Nathan Srebro"], "venue": null, "citeRegEx": "94", "shortCiteRegEx": "94", "year": 2015}, {"title": "Problems in decentralized decision making and computation", "author": ["John Nikolas Tsitsiklis"], "venue": "Technical report, DTIC Document,", "citeRegEx": "95", "shortCiteRegEx": "95", "year": 1984}, {"title": "An overview of statistical learning theory", "author": ["Vladimir N Vapnik"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "96", "shortCiteRegEx": "96", "year": 1999}, {"title": "Randomized block coordinate descent for online and stochastic optimization", "author": ["Huahua Wang", "Arindam Banerjee"], "venue": null, "citeRegEx": "97", "shortCiteRegEx": "97", "year": 2014}, {"title": "Consumer data privacy in a networked world: A framework for protecting privacy and promoting innovation in the global digital economy", "author": ["White House Report"], "venue": "Journal of Privacy and Confidentiality,", "citeRegEx": "98", "shortCiteRegEx": "98", "year": 2013}, {"title": "Tight complexity bounds for optimizing composite objectives", "author": ["Blake Woodworth", "Nathan Srebro"], "venue": null, "citeRegEx": "99", "shortCiteRegEx": "99", "year": 2016}, {"title": "A proximal stochastic gradient method with progressive variance reduction", "author": ["Lin Xiao", "Tong Zhang"], "venue": "SIAM Journal on Optimization,", "citeRegEx": "100", "shortCiteRegEx": "100", "year": 2014}, {"title": "Trading computation for communication: Distributed stochastic dual coordinate ascent", "author": ["Tianbao Yang"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "101", "shortCiteRegEx": "101", "year": 2013}, {"title": "Spark: cluster computing with working sets", "author": ["Matei Zaharia", "Mosharaf Chowdhury", "Michael J Franklin", "Scott Shenker", "Ion Stoica"], "venue": "In Proceedings of the 2nd USENIX conference on Hot topics in cloud computing,", "citeRegEx": "102", "shortCiteRegEx": "102", "year": 2010}, {"title": "Information-theoretic lower bounds for distributed statistical estimation with communication constraints", "author": ["Yuchen Zhang", "John Duchi", "Michael I Jordan", "Martin J Wainwright"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "103", "shortCiteRegEx": "103", "year": 2013}, {"title": "Communication-efficient algorithms for statistical optimization", "author": ["Yuchen Zhang", "John C Duchi", "Martin J Wainwright"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "104", "shortCiteRegEx": "104", "year": 2013}, {"title": "DiSCO: Distributed optimization for self-concordant empirical loss", "author": ["Yuchen Zhang", "Xiao Lin"], "venue": "In Proceedings of The 32th International Conference on Machine Learning,", "citeRegEx": "105", "shortCiteRegEx": "105", "year": 2015}, {"title": "Distributed newton methods for regularized logistic regression", "author": ["Yong Zhuang", "Wei-Sheng Chin", "Yu-Chin Juan", "Chih-Jen Lin"], "venue": "In Advances in Knowledge Discovery and Data Mining,", "citeRegEx": "106", "shortCiteRegEx": "106", "year": 2015}], "referenceMentions": [{"referenceID": 94, "context": "This is a direct application of the principle of focused collection or data minimization proposed by the 2012 White House report on the privacy of consumer data [98].", "startOffset": 161, "endOffset": 165}, {"referenceID": 16, "context": "The centralized algorithm could be modified to produce a differentially private model [17, 33, 1], which allows the model to be released while protecting the privacy of the individuals contributing updates to the training process.", "startOffset": 86, "endOffset": 97}, {"referenceID": 31, "context": "The centralized algorithm could be modified to produce a differentially private model [17, 33, 1], which allows the model to be released while protecting the privacy of the individuals contributing updates to the training process.", "startOffset": 86, "endOffset": 97}, {"referenceID": 0, "context": "The centralized algorithm could be modified to produce a differentially private model [17, 33, 1], which allows the model to be released while protecting the privacy of the individuals contributing updates to the training process.", "startOffset": 86, "endOffset": 97}, {"referenceID": 30, "context": "If protection from even a malicious (or compromised) coordinating server is needed, techniques from local differential privacy can be applied to privatize the individual updates [32].", "startOffset": 178, "endOffset": 182}, {"referenceID": 58, "context": "A more complete discussion of applications of federated learning as well as privacy ramifications can be found in [62].", "startOffset": 114, "endOffset": 118}, {"referenceID": 85, "context": "The practical issue is that the time it takes to communicate between a processor and memory on the same node is normally many orders of magnitude smaller than the time needed for two nodes to communicate; similar conclusions hold for the energy required [89].", "startOffset": 254, "endOffset": 258}, {"referenceID": 53, "context": "This setting, in which a single vector \u03b4 \u2208 Rd is communicated in each round, covers most existing first-order methods, including dual methods such as CoCoA+ [57].", "startOffset": 157, "endOffset": 161}, {"referenceID": 65, "context": "A trivial benchmark for solving problems of structure (1) is Gradient Descent (GD) in the case when functions fi are smooth (or Subgradient Descent for non-smooth functions) [69].", "startOffset": 174, "endOffset": 178}, {"referenceID": 69, "context": "Acceleration ideas for gradient methods in convex optimization can be traced back to the work of Polyak [73] and Nesterov [68, 69].", "startOffset": 104, "endOffset": 108}, {"referenceID": 64, "context": "Acceleration ideas for gradient methods in convex optimization can be traced back to the work of Polyak [73] and Nesterov [68, 69].", "startOffset": 122, "endOffset": 130}, {"referenceID": 65, "context": "Acceleration ideas for gradient methods in convex optimization can be traced back to the work of Polyak [73] and Nesterov [68, 69].", "startOffset": 122, "endOffset": 130}, {"referenceID": 78, "context": "At present a basic, albeit in practice extremely popular, alternative to GD is Stochastic Gradient Descent (SGD), dating back to the seminal work of Robbins and Monro [82].", "startOffset": 167, "endOffset": 171}, {"referenceID": 62, "context": "For a theoretical analysis for convex functions we refer the reader to [66, 64, 65] and [87, 93] for SVM problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 60, "context": "For a theoretical analysis for convex functions we refer the reader to [66, 64, 65] and [87, 93] for SVM problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 61, "context": "For a theoretical analysis for convex functions we refer the reader to [66, 64, 65] and [87, 93] for SVM problems.", "startOffset": 71, "endOffset": 83}, {"referenceID": 83, "context": "For a theoretical analysis for convex functions we refer the reader to [66, 64, 65] and [87, 93] for SVM problems.", "startOffset": 88, "endOffset": 96}, {"referenceID": 89, "context": "For a theoretical analysis for convex functions we refer the reader to [66, 64, 65] and [87, 93] for SVM problems.", "startOffset": 88, "endOffset": 96}, {"referenceID": 11, "context": "In a recent review [12], the authors outline further research directions.", "startOffset": 19, "endOffset": 23}, {"referenceID": 10, "context": "For a more practically-focused discussion, see [11].", "startOffset": 47, "endOffset": 51}, {"referenceID": 45, "context": "In the context of neural networks, computation of stochastic gradients is referred to as backpropagation [49].", "startOffset": 105, "endOffset": 109}, {"referenceID": 66, "context": "Performance of several competitive algorithms for training deep neural networks has been compared in [70].", "startOffset": 101, "endOffset": 105}, {"referenceID": 9, "context": "This ordering is replaced by another random order after each such cycle [10].", "startOffset": 72, "endOffset": 76}, {"referenceID": 37, "context": "Theoretical understanding of this phenomenon had been a long standing open problem, understood recently in [40].", "startOffset": 107, "endOffset": 111}, {"referenceID": 63, "context": "Although the idea of coordinate descent has been around for several decades in various contexts (and for quadratic functions dates back even much further, to works on the Gauss-Seidel methods), it came to prominence in machine learning and optimization with the work of Nesterov [67] which equipped the method with a randomization strategy.", "startOffset": 279, "endOffset": 283}, {"referenceID": 75, "context": "Numerous follow-up works extended the concept to proximal setting [79], single processor parallelism [15, 80] and develop efficiently implementable acceleration [51].", "startOffset": 66, "endOffset": 70}, {"referenceID": 14, "context": "Numerous follow-up works extended the concept to proximal setting [79], single processor parallelism [15, 80] and develop efficiently implementable acceleration [51].", "startOffset": 101, "endOffset": 109}, {"referenceID": 76, "context": "Numerous follow-up works extended the concept to proximal setting [79], single processor parallelism [15, 80] and develop efficiently implementable acceleration [51].", "startOffset": 101, "endOffset": 109}, {"referenceID": 47, "context": "Numerous follow-up works extended the concept to proximal setting [79], single processor parallelism [15, 80] and develop efficiently implementable acceleration [51].", "startOffset": 161, "endOffset": 165}, {"referenceID": 33, "context": "All of these three properties were connected in a single algorithm in [35], to which we refer the reader for a review of the early developments in the area of RCD, particularly to overview in Table 1 therein.", "startOffset": 70, "endOffset": 74}, {"referenceID": 84, "context": "Applying RCD leads to an algorithm for solving (1) known under the name Stochastic Dual Coordinate Ascent [88].", "startOffset": 106, "endOffset": 110}, {"referenceID": 84, "context": "The work [88] was first to show that by", "startOffset": 9, "endOffset": 13}, {"referenceID": 75, "context": "applying RCD [79] to the dual problem, one also solves the primal problem (1).", "startOffset": 13, "endOffset": 17}, {"referenceID": 19, "context": "For a theoretical and computational comparison of applying RCD to the primal versus the dual problems, see [21].", "startOffset": 107, "endOffset": 111}, {"referenceID": 71, "context": "A directly primal-dual randomized coordinate descent method called Quartz, was developed in [75].", "startOffset": 92, "endOffset": 96}, {"referenceID": 70, "context": "It has been recently shown in SDNA [74] that incorporating curvature information contained in random low dimensional subspaces spanned by a few coordinates can sometimes lead to dramatic speedups.", "startOffset": 35, "endOffset": 39}, {"referenceID": 82, "context": "Recent works [86, 20] interpret the SDCA method in primal-only setting, shedding light onto why this method works as a SGD method with a version of variance reduction property.", "startOffset": 13, "endOffset": 21}, {"referenceID": 18, "context": "Recent works [86, 20] interpret the SDCA method in primal-only setting, shedding light onto why this method works as a SGD method with a version of variance reduction property.", "startOffset": 13, "endOffset": 21}, {"referenceID": 79, "context": "The first notable algorithm from this class is the Stochastic Average Gradient (SAG) [83, 85].", "startOffset": 85, "endOffset": 93}, {"referenceID": 81, "context": "The first notable algorithm from this class is the Stochastic Average Gradient (SAG) [83, 85].", "startOffset": 85, "endOffset": 93}, {"referenceID": 80, "context": "This methods has been recently extended for use in Conditional Random Fields [84].", "startOffset": 77, "endOffset": 81}, {"referenceID": 24, "context": "A followup algorithm SAGA [26] and its simplification [25], modifies the SAG algorithm to achieve unbiased estimate of the gradients.", "startOffset": 26, "endOffset": 30}, {"referenceID": 23, "context": "A followup algorithm SAGA [26] and its simplification [25], modifies the SAG algorithm to achieve unbiased estimate of the gradients.", "startOffset": 54, "endOffset": 58}, {"referenceID": 40, "context": "Another algorithm from the SGD class of methods is Stochastic Variance Reduced Gradient1 (SVRG) [43] and [47, 100, 44].", "startOffset": 96, "endOffset": 100}, {"referenceID": 43, "context": "Another algorithm from the SGD class of methods is Stochastic Variance Reduced Gradient1 (SVRG) [43] and [47, 100, 44].", "startOffset": 105, "endOffset": 118}, {"referenceID": 96, "context": "Another algorithm from the SGD class of methods is Stochastic Variance Reduced Gradient1 (SVRG) [43] and [47, 100, 44].", "startOffset": 105, "endOffset": 118}, {"referenceID": 41, "context": "Another algorithm from the SGD class of methods is Stochastic Variance Reduced Gradient1 (SVRG) [43] and [47, 100, 44].", "startOffset": 105, "endOffset": 118}, {"referenceID": 38, "context": "This and several other practical issues have been recently addressed in [41], making the algorithm competitive with SGD early on, and superior in later iterations.", "startOffset": 72, "endOffset": 76}, {"referenceID": 43, "context": "The same algorithm was simultaneously introduced as Semi-Stochastic Gradient Descent (S2GD) [47].", "startOffset": 92, "endOffset": 96}, {"referenceID": 40, "context": "Vanilla experiments in [43, 77] suggest that SVRG matches basic SGD, and even outperforms in the sense that variance of the iterates seems to be significantly smaller for SVRG.", "startOffset": 23, "endOffset": 31}, {"referenceID": 73, "context": "Vanilla experiments in [43, 77] suggest that SVRG matches basic SGD, and even outperforms in the sense that variance of the iterates seems to be significantly smaller for SVRG.", "startOffset": 23, "endOffset": 31}, {"referenceID": 93, "context": "There already exist attempts at combining SVRG type algorithms with randomized coordinate descent [46, 97].", "startOffset": 98, "endOffset": 106}, {"referenceID": 24, "context": "The first attempt to unify algorithms such as SVRG and SAG/SAGA already appeared in the SAGA paper [26], where the authors interpret SAGA as a midpoint between SAG and SVRG.", "startOffset": 99, "endOffset": 103}, {"referenceID": 72, "context": "Recent work [76] presents a general algorithm, which recovers SVRG, SAGA, SAG and GD as special cases, and obtains an asynchronous variant of these algorithms as a byproduct of the formulation.", "startOffset": 12, "endOffset": 16}, {"referenceID": 2, "context": "SVRG can be equipped with momentum (and negative momentum), leading to a new accelerated SVRG method known as Katyusha [3].", "startOffset": 119, "endOffset": 122}, {"referenceID": 3, "context": "SVRG can be further accelerated via a raw clustering mechanism [4].", "startOffset": 63, "endOffset": 66}, {"referenceID": 15, "context": "A third class of new algorithms are the Stochastic quasiNewton methods [16, 9].", "startOffset": 71, "endOffset": 78}, {"referenceID": 8, "context": "A third class of new algorithms are the Stochastic quasiNewton methods [16, 9].", "startOffset": 71, "endOffset": 78}, {"referenceID": 50, "context": "These algorithms in general try to mimic the limited memory BFGS method (L-BFGS) [54], but model the local curvature information using inexact gradients \u2014 coming from the SGD procedure.", "startOffset": 81, "endOffset": 85}, {"referenceID": 59, "context": "A recent attempt at combining these methods with SVRG can be found in [63].", "startOffset": 70, "endOffset": 74}, {"referenceID": 35, "context": "In [38], the authors utilize recent progress in the area of stochastic matrix inversion [39] revealing new connections with quasi-Newton methods, and devise a new stochastic limited memory BFGS method working in tandem with SVRG.", "startOffset": 3, "endOffset": 7}, {"referenceID": 36, "context": "In [38], the authors utilize recent progress in the area of stochastic matrix inversion [39] revealing new connections with quasi-Newton methods, and devise a new stochastic limited memory BFGS method working in tandem with SVRG.", "startOffset": 88, "endOffset": 92}, {"referenceID": 92, "context": "When one can find exact minimum of the empirical risk, everything reduces to balancing approximation\u2013 estimation tradeoff that is the object of abundant literature \u2014 see for instance [96].", "startOffset": 183, "endOffset": 187}, {"referenceID": 12, "context": "An assessment of asymptotic performance of some optimization algorithms as learning algorithms in large-scale learning problems2 has been introduced in [13].", "startOffset": 152, "endOffset": 156}, {"referenceID": 38, "context": "Recent extension in [41] has shown that the variance reduced algorithms (SAG, SVRG, .", "startOffset": 20, "endOffset": 24}, {"referenceID": 49, "context": "A general method, referred to as Universal Catalyst [53, 37], effectively enables conversion of a number of the algorithms mentioned in the previous sections to their \u2018accelerated\u2019 variants.", "startOffset": 52, "endOffset": 60}, {"referenceID": 34, "context": "A general method, referred to as Universal Catalyst [53, 37], effectively enables conversion of a number of the algorithms mentioned in the previous sections to their \u2018accelerated\u2019 variants.", "startOffset": 52, "endOffset": 60}, {"referenceID": 95, "context": "Recently, lower and upper bounds for complexity of stochastic methods on problems of the form (1) were recently obtained in [99].", "startOffset": 124, "endOffset": 128}, {"referenceID": 5, "context": "Compared with local computation on any single node, the cost of communication between nodes is much higher both in terms of speed and energy consumption [6, 89], introducing new computational challenges, not only for optimization procedures.", "startOffset": 153, "endOffset": 160}, {"referenceID": 85, "context": "Compared with local computation on any single node, the cost of communication between nodes is much higher both in terms of speed and energy consumption [6, 89], introducing new computational challenges, not only for optimization procedures.", "startOffset": 153, "endOffset": 160}, {"referenceID": 53, "context": "The question is: \u201cHow do we decide which algorithm is the best for our purpose?\u201d Initial version of this reasoning already appeared in [57], and applies also to [78].", "startOffset": 135, "endOffset": 139}, {"referenceID": 74, "context": "The question is: \u201cHow do we decide which algorithm is the best for our purpose?\u201d Initial version of this reasoning already appeared in [57], and applies also to [78].", "startOffset": 161, "endOffset": 165}, {"referenceID": 53, "context": "This issue was first time explicitly addressed in CoCoA [57], which is rather framework than a algorithm, which works as follows (more detailed description follows in Section 2.", "startOffset": 56, "endOffset": 60}, {"referenceID": 39, "context": "The general upper bound on number of iterations of the CoCoA framework is I( ,\u0398) = O(log(1/ )) 1\u2212\u0398 [42, 58, 57] for strongly convex objectives.", "startOffset": 99, "endOffset": 111}, {"referenceID": 54, "context": "The general upper bound on number of iterations of the CoCoA framework is I( ,\u0398) = O(log(1/ )) 1\u2212\u0398 [42, 58, 57] for strongly convex objectives.", "startOffset": 99, "endOffset": 111}, {"referenceID": 53, "context": "The general upper bound on number of iterations of the CoCoA framework is I( ,\u0398) = O(log(1/ )) 1\u2212\u0398 [42, 58, 57] for strongly convex objectives.", "startOffset": 99, "endOffset": 111}, {"referenceID": 22, "context": "By communication round we typically understand a single MapReduce operation [24], implemented efficiently for iterative procedures [36], such as optimization algorithms.", "startOffset": 76, "endOffset": 80}, {"referenceID": 98, "context": "Spark [102] has been established as a popular open source framework for implementing distributed iterative algorithms, and includes several of the algorithms mentioned in this section.", "startOffset": 6, "endOffset": 11}, {"referenceID": 7, "context": "Optimization in distributed setting has been studied for decades, tracing back to at least works of Bertsekas and Tsitsiklis [8, 7, 95].", "startOffset": 125, "endOffset": 135}, {"referenceID": 6, "context": "Optimization in distributed setting has been studied for decades, tracing back to at least works of Bertsekas and Tsitsiklis [8, 7, 95].", "startOffset": 125, "endOffset": 135}, {"referenceID": 91, "context": "Optimization in distributed setting has been studied for decades, tracing back to at least works of Bertsekas and Tsitsiklis [8, 7, 95].", "startOffset": 125, "endOffset": 135}, {"referenceID": 25, "context": "As an example, [27, 29] provide theoretically linear speedup with number of nodes, but are difficult to implement efficiently, as the nodes need to synchronize frequently in order to compute reasonable gradient averages.", "startOffset": 15, "endOffset": 23}, {"referenceID": 27, "context": "As an example, [27, 29] provide theoretically linear speedup with number of nodes, but are difficult to implement efficiently, as the nodes need to synchronize frequently in order to compute reasonable gradient averages.", "startOffset": 15, "endOffset": 23}, {"referenceID": 67, "context": "As an alternative, no synchronization between workers is assumed in [71, 2, 31].", "startOffset": 68, "endOffset": 79}, {"referenceID": 1, "context": "As an alternative, no synchronization between workers is assumed in [71, 2, 31].", "startOffset": 68, "endOffset": 79}, {"referenceID": 29, "context": "As an alternative, no synchronization between workers is assumed in [71, 2, 31].", "startOffset": 68, "endOffset": 79}, {"referenceID": 56, "context": "In fact, two recent works [60, 48] highlight subtle but important issue with labelling of iterates in the presence of asynchrony, rendering most of the existing analyses of asynchronous optimization algorithms incorrect.", "startOffset": 26, "endOffset": 34}, {"referenceID": 44, "context": "In fact, two recent works [60, 48] highlight subtle but important issue with labelling of iterates in the presence of asynchrony, rendering most of the existing analyses of asynchronous optimization algorithms incorrect.", "startOffset": 26, "endOffset": 34}, {"referenceID": 28, "context": "Asymptotically optimal convergent rates were proven in [30] with considerably milder assumptions.", "startOffset": 55, "endOffset": 59}, {"referenceID": 20, "context": "Improved analysis of asynchronous SGD was also presented in [22], simultaneously with a version that uses lower-precision arithmetic was introduced without sacrificing performance, which is a trend that might find use in other parts of machine learning in the following years.", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "The practical usefulness has been demonstrated for instance by Google\u2019s Downpour SGD [23] and Microsoft\u2019s Project Adam [18].", "startOffset": 85, "endOffset": 89}, {"referenceID": 17, "context": "The practical usefulness has been demonstrated for instance by Google\u2019s Downpour SGD [23] and Microsoft\u2019s Project Adam [18].", "startOffset": 119, "endOffset": 123}, {"referenceID": 77, "context": "The first distributed versions of Coordinate Descent algorithms were the Hydra and its accelerated variant, Hydra2, [81, 34], which has been demonstrated to be very efficient on large sparse problems implemented on a computing cluster.", "startOffset": 116, "endOffset": 124}, {"referenceID": 32, "context": "The first distributed versions of Coordinate Descent algorithms were the Hydra and its accelerated variant, Hydra2, [81, 34], which has been demonstrated to be very efficient on large sparse problems implemented on a computing cluster.", "startOffset": 116, "endOffset": 124}, {"referenceID": 57, "context": "An extended version with description of implementation details is presented in [61].", "startOffset": 79, "endOffset": 83}, {"referenceID": 52, "context": "Effect of asynchrony has been explored and partially theoretically understood in the works of [56, 55].", "startOffset": 94, "endOffset": 102}, {"referenceID": 51, "context": "Effect of asynchrony has been explored and partially theoretically understood in the works of [56, 55].", "startOffset": 94, "endOffset": 102}, {"referenceID": 68, "context": "Another asynchronous, rather framework than an algorithm, for coordinate updates, applicable to wider class of objectives is presented in [72].", "startOffset": 138, "endOffset": 142}, {"referenceID": 90, "context": "This does not need to be an issue, if a dual version of coordinate descent is used \u2014 in which the distribution is done by data points [94] followed by works on Communication Efficient Dual Cooridante Ascent, described in next section.", "startOffset": 134, "endOffset": 138}, {"referenceID": 46, "context": "A scheme for replicating data to simulate iid sampling in distributed environment was proposed in [50].", "startOffset": 98, "endOffset": 102}, {"referenceID": 74, "context": "A relatively similar method to Algorithm 3 presented here has been proposed in [78], which was analysed, and in [59], a largely experimental work that can be also cast as communication efficient \u2014 described in detail in Section 2.", "startOffset": 79, "endOffset": 83}, {"referenceID": 55, "context": "A relatively similar method to Algorithm 3 presented here has been proposed in [78], which was analysed, and in [59], a largely experimental work that can be also cast as communication efficient \u2014 described in detail in Section 2.", "startOffset": 112, "endOffset": 116}, {"referenceID": 13, "context": "Another class of algorithms relevant for this work is Alternating Direction Method of Multipliers (ADMM) [14, 28].", "startOffset": 105, "endOffset": 113}, {"referenceID": 26, "context": "Another class of algorithms relevant for this work is Alternating Direction Method of Multipliers (ADMM) [14, 28].", "startOffset": 105, "endOffset": 113}, {"referenceID": 86, "context": "Fundamental limitations of stochastic versions of the problem (1) in terms of runtime, communication costs and number of samples used are studied in [90].", "startOffset": 149, "endOffset": 153}, {"referenceID": 100, "context": "Efficient algorithms and lower bounds for distributed statistical estimation are established in [104, 103].", "startOffset": 96, "endOffset": 106}, {"referenceID": 99, "context": "Efficient algorithms and lower bounds for distributed statistical estimation are established in [104, 103].", "startOffset": 96, "endOffset": 106}, {"referenceID": 100, "context": "In the case of [104, 103] also K n/K, that the number of nodes K is much smaller than the number of data point on each node is also assumed.", "startOffset": 15, "endOffset": 25}, {"referenceID": 99, "context": "In the case of [104, 103] also K n/K, that the number of nodes K is much smaller than the number of data point on each node is also assumed.", "startOffset": 15, "endOffset": 25}, {"referenceID": 4, "context": "Lower bounds on communication complexity of distributed convex optimization of (1) are presented in [5], concluding that for IID data distributions, existing algorithms already achieve optimal complexity in specific settings.", "startOffset": 100, "endOffset": 103}, {"referenceID": 87, "context": "proposed the DANE algorithm, Distributed Approximate Newton [91], to exactly solve a general subproblem available locally, before averaging their solutions.", "startOffset": 60, "endOffset": 64}, {"referenceID": 55, "context": "A quite similar approach was proposed in [59], with richer class class of subproblems that can be formulated locally, and solved approximately.", "startOffset": 41, "endOffset": 45}, {"referenceID": 74, "context": "An analysis of inexact version of DANE and its accelerated variant, AIDE, appeared recently in [78].", "startOffset": 95, "endOffset": 99}, {"referenceID": 101, "context": "The DiSCO algorithm [105] of Zhang and Xiao is based on inexact damped Newton method.", "startOffset": 20, "endOffset": 25}, {"referenceID": 4, "context": "The theoretical upper bound on number of rounds of communication improves upon DANE and other methods, and in certain settings matches the lower bound presented in [5].", "startOffset": 164, "endOffset": 167}, {"referenceID": 48, "context": "The DiSCO algorithm is related to [52, 106], a distributed truncated Newton method.", "startOffset": 34, "endOffset": 43}, {"referenceID": 102, "context": "The DiSCO algorithm is related to [52, 106], a distributed truncated Newton method.", "startOffset": 34, "endOffset": 43}, {"referenceID": 97, "context": "The first version of the algorithm was proposed as DisDCA in [101], without convergence guarantees.", "startOffset": 61, "endOffset": 66}, {"referenceID": 39, "context": "First analysis was introduced in [42], with further improvements in [58], and a more general version in [57].", "startOffset": 33, "endOffset": 37}, {"referenceID": 54, "context": "First analysis was introduced in [42], with further improvements in [58], and a more general version in [57].", "startOffset": 68, "endOffset": 72}, {"referenceID": 53, "context": "First analysis was introduced in [42], with further improvements in [58], and a more general version in [57].", "startOffset": 104, "endOffset": 108}, {"referenceID": 88, "context": "Recently, its variant for L1-regularized objectives was introduced in [92].", "startOffset": 70, "endOffset": 74}, {"referenceID": 40, "context": "Namely, the algorithms are the Stochastic Variance Reduced Gradient (SVRG) [43, 47], a stochastic method with explicit variance reduction, and the Distributed Approximate Newton (DANE) [91] for distributed optimization.", "startOffset": 75, "endOffset": 83}, {"referenceID": 43, "context": "Namely, the algorithms are the Stochastic Variance Reduced Gradient (SVRG) [43, 47], a stochastic method with explicit variance reduction, and the Distributed Approximate Newton (DANE) [91] for distributed optimization.", "startOffset": 75, "endOffset": 83}, {"referenceID": 87, "context": "Namely, the algorithms are the Stochastic Variance Reduced Gradient (SVRG) [43, 47], a stochastic method with explicit variance reduction, and the Distributed Approximate Newton (DANE) [91] for distributed optimization.", "startOffset": 185, "endOffset": 189}, {"referenceID": 40, "context": "The SVRG algorithm [43, 47] is a stochastic method designed to solve problem (1) on a single node.", "startOffset": 19, "endOffset": 27}, {"referenceID": 43, "context": "The SVRG algorithm [43, 47] is a stochastic method designed to solve problem (1) on a single node.", "startOffset": 19, "endOffset": 27}, {"referenceID": 87, "context": "In this section, we introduce a general reasoning providing stronger intuitive support for the DANE algorithm [91], which we describe in detail below.", "startOffset": 110, "endOffset": 114}, {"referenceID": 87, "context": "We present the Distributed Approximate Newton algorithm (DANE) [91], as Algorithm 2.", "startOffset": 63, "endOffset": 67}, {"referenceID": 53, "context": "We adapt the idea from the CoCoA algorithm [57], in which an arbitrary optimization algorithm is used to obtain relative \u0398 accuracy on a locally defined subproblem.", "startOffset": 43, "endOffset": 47}, {"referenceID": 74, "context": "Since the first version of this paper, this connection has been mentioned in [78], which analyses an inexact version of the DANE algorithm.", "startOffset": 77, "endOffset": 81}, {"referenceID": 53, "context": "Without any extra information, or in the case of fully dense data, averaging the local updates is the only way that actually makes sense \u2014 because each node outputs approximate solution of a proxy to the overall objective, and there is no induced separability structure in the outputs such as in CoCoA [57].", "startOffset": 302, "endOffset": 306}, {"referenceID": 53, "context": "\u2022 The purple triangles (COCOA) are for the CoCoA+ algorithm [57].", "startOffset": 60, "endOffset": 64}, {"referenceID": 75, "context": "Althought it is expected that the algorithm could be modified to depend on average of these quantities (which could be orders of magnitude smaller), akin to coordinate descent algorithms [79], it has not been done yet.", "startOffset": 187, "endOffset": 191}, {"referenceID": 42, "context": "Since the first version of this paper [45], additional experimental results were presented in [62].", "startOffset": 38, "endOffset": 42}, {"referenceID": 58, "context": "Since the first version of this paper [45], additional experimental results were presented in [62].", "startOffset": 94, "endOffset": 98}], "year": 2016, "abstractText": "We introduce a new and increasingly relevant setting for distributed optimization in machine learning, where the data defining the optimization are unevenly distributed over an extremely large number of nodes. The goal is to train a high-quality centralized model. We refer to this setting as Federated Optimization. In this setting, communication efficiency is of the utmost importance and minimizing the number of rounds of communication is the principal goal. A motivating example arises when we keep the training data locally on users\u2019 mobile devices instead of logging it to a data center for training. In federated optimization, the devices are used as compute nodes performing computation on their local data in order to update a global model. We suppose that we have extremely large number of devices in the network \u2014 as many as the number of users of a given service, each of which has only a tiny fraction of the total data available. In particular, we expect the number of data points available locally to be much smaller than the number of devices. Additionally, since different users generate data with different patterns, it is reasonable to assume that no device has a representative sample of the overall distribution. We show that existing algorithms are not suitable for this setting, and propose a new algorithm which shows encouraging experimental results for sparse convex problems. This work also sets a path for future research needed in the context of federated optimization.", "creator": "LaTeX with hyperref package"}}}