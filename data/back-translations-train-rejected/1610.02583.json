{"id": "1610.02583", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "8-Oct-2016", "title": "A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation", "abstract": "We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit.", "histories": [["v1", "Sat, 8 Oct 2016 21:10:40 GMT  (840kb,D)", "http://arxiv.org/abs/1610.02583v1", "9 pages"], ["v2", "Tue, 17 Oct 2017 04:48:14 GMT  (842kb,D)", "http://arxiv.org/abs/1610.02583v2", "9 pages"]], "COMMENTS": "9 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["gang chen"], "accepted": false, "id": "1610.02583"}, "pdf": {"name": "1610.02583.pdf", "metadata": {"source": "CRF", "title": "A Gentle Tutorial of Recurrent Neural Network with Error Backpropagation", "authors": ["Gang Chen"], "emails": ["gangchen@buffalo.edu,"], "sections": [{"heading": "A Gentle Tutorial of Recurrent Neural Network with Error", "text": "BackpropagationGang Chen, Department of Computer Science and Engineering, SUNY in Buffalo"}, {"heading": "1 abstract", "text": "We describe recurrent neural networks (RNNs) that have drawn a lot of attention to sequential tasks such as handwriting recognition, speech recognition, and image to text. However, compared to general feedback neural networks, RNNs have feedback loops, which makes it a little difficult to understand the step of backpropagation. Therefore, we focus on the basics, in particular the backpropagation of errors to calculate gradients in terms of model parameters. Furthermore, we detail how the algorithm for backpropagation of errors to long-term memory (LSTM) is applied by deploying the memory unit."}, {"heading": "2 Sequential data", "text": "Sequential data is common in a variety of areas, including natural language processing, speech recognition, and computer biology. Generally, it is divided into time series and ordered data structures; in terms of time series data, it changes over time and remains consistent in adjacent clips. For example, the successful prediction of protein interactions requires knowledge of the secondary structures of proteins, and semantic analysis could involve commenting tokens with parts of language tags. The goal of this work is sequence identification, i.e. classification of all elements in a sequence [4]. For example, in handwritten word identification, we require semantic analysis of the secondary structures of proteins."}, {"heading": "3 Recurrent neural networks", "text": "An RNN is a type of neural network that can send feedback signals (to form a directed cycle), such as Hopfield net (1) and long-term memory (LSTM). RNN is a dynamic system in which the hidden state depends not only on the current observation, but can also be derived from the previous hidden definition. Specifically, we can use the hidden variables as memory to capture long-term information from a sequence. Thus, it contains information about the entire sequence that can be derived from the recursive definition in Eq. 1. In other words, RNN can use the hidden variables as memory to capture information from a sequence."}, {"heading": "3.1 Long short-term memory (LSTM)", "text": "The architecture of the storage cell has the same inputs as a normal urrent, but controlling the control of the cell is more than ever before. (The architecture of the storage cell has the same inputs as the cell.) (The cell has the same inputs as the cell. (The cell is the cell we observed up to this step.) The cell has the same inputs as the cell. (The cell is the cell we observed up to this step.) The cell has the same inputs as the cell.) The cell has the same inputs as the cell. (The cell is a storage cell) ct. (The cell we encrypt the information of the cell up to this step.) The cell has the same inputs (ht) and the cell. (The cell is the cell we observed up to this step.)"}, {"heading": "3.2 Error backpropagation", "text": "In this section we will give detailed information on how to derive gradients, in particular equation 28. As we have previously talked about RNNs, we can apply the same strategy to unfold the storage unit shown in Figure 3. Suppose we have the least square objective functionL (x, \u03b8) = min. In order to understand easily in the following, we use L (t) = 12 (yt \u2212 zt) 2.For time stage T, we assume derived w.r.t. Derivative-T-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z-Z"}, {"heading": "3.3 Parameters learning", "text": "Forward: We can use Equation 16-21 to update states as a neural network from step 1 to T. Answer: We can reverse propagate the error from T to 1 via Equation 25-34. After we have obtained gradients by means of reverse propagation, the model \u03b8 can be learned using gradient-based methods such as stochastic gradient descent and L-BFGS. If we use stochastic gradient descent (SGD) to bring the system up to date = {Whz, Wxo, Wxi, Wxf, Wxc, Who, Whi, Whf, Whc}, then we have a system with terrain where the learning rate is the same. Note that further tricks can be applied here."}, {"heading": "4 Applications", "text": "RNNs can be used to process sequential data, such as speech recognition. RNNs can also be expanded into multiple layers in two directions. In addition, RNNs can be combined with other neural networks [3], such as a Convolutionary Neural Network to solve video-text problems, and two LSTMs for machine translation."}], "references": [{"title": "Neural Networks and Physical Systems with Emergent Collective Computational Abilities", "author": ["J.J. Hopfield"], "venue": "In Neurocomputing: Foundations of Research,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1988}, {"title": "Long Short-Term Memory", "author": ["Hochreiter", "Sepp", "Schmidhuber", "J\u00fcrgen"], "venue": "In Neural Comput.,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1997}, {"title": "Generalized K-fan Multimodal Deep Model with Shared Representations", "author": ["G. Chen", "S.N. Srihari"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Machine Learning: Basics, Models and Trends, 2016", "author": ["Gang Chen"], "venue": null, "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2016}], "referenceMentions": [{"referenceID": 3, "context": "classify all items in a sequence [4].", "startOffset": 33, "endOffset": 36}, {"referenceID": 0, "context": "A RNN is a kind of neural networks, which can send feedback signals (to form a directed cycle), such as Hopfield net [1] and long-short term memory (LSTM) [2].", "startOffset": 117, "endOffset": 120}, {"referenceID": 1, "context": "A RNN is a kind of neural networks, which can send feedback signals (to form a directed cycle), such as Hopfield net [1] and long-short term memory (LSTM) [2].", "startOffset": 155, "endOffset": 158}, {"referenceID": 1, "context": "Considering the weakness of RNNs, long short term memory (LSTM) was proposed to handle gradient vanishing problem [2].", "startOffset": 114, "endOffset": 117}, {"referenceID": 1, "context": "LSTM [2] extends the RNNs model with two advantages: (1) introduce the memory information (or cell) (2) handle long sequential better, considering the gradient vanishing problem, refer to Fig.", "startOffset": 5, "endOffset": 8}, {"referenceID": 2, "context": "Moreover, RNNs can be combines with other neural networks [3], such as convolutional neural network to handle video to text problem, as well as combining two LSTMs for machine translation.", "startOffset": 58, "endOffset": 61}], "year": 2016, "abstractText": "We describe recurrent neural networks (RNNs), which have attracted great attention on sequential tasks, such as handwriting recognition, speech recognition and image to text. However, compared to general feedforward neural networks, RNNs have feedback loops, which makes it a little hard to understand the backpropagation step. Thus, we focus on basics, especially the error backpropagation to compute gradients with respect to model parameters. Further, we go into detail on how error backpropagation algorithm is applied on long short-term memory (LSTM) by unfolding the memory unit.", "creator": "LaTeX with hyperref package"}}}