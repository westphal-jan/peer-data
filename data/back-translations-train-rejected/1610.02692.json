{"id": "1610.02692", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2016", "title": "Open-Ended Visual Question-Answering", "abstract": "This thesis report studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework. As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based). We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer. This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset. The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations.", "histories": [["v1", "Sun, 9 Oct 2016 16:38:31 GMT  (6052kb,D)", "http://arxiv.org/abs/1610.02692v1", "Bachelor thesis report graded with A with honours at ETSETB Telecom BCN school, Universitat Polit\\`ecnica de Catalunya (UPC). June 2016. Source code and models are publicly available atthis http URL"]], "COMMENTS": "Bachelor thesis report graded with A with honours at ETSETB Telecom BCN school, Universitat Polit\\`ecnica de Catalunya (UPC). June 2016. Source code and models are publicly available atthis http URL", "reviews": [], "SUBJECTS": "cs.CL cs.CV cs.MM", "authors": ["issey masuda", "santiago pascual de la puente", "xavier giro-i-nieto"], "accepted": false, "id": "1610.02692"}, "pdf": {"name": "1610.02692.pdf", "metadata": {"source": "CRF", "title": "Open-Ended Visual Question-Answering", "authors": [], "emails": [], "sections": [{"heading": null, "text": "Visual Question Answers with Open Ends"}, {"heading": "A Degree Thesis", "text": "Submitted to the Faculty of the Escola Te cnica Superior d'Enginyeria de Telecomunicacio \u0301 de Barcelona"}, {"heading": "In partial fulfilment", "text": "The prerequisites for the conclusion of SCIENCE AND TELECOMMUNICATION 16 are questions.Questions.Questions.Questions.Questions.Questions.Questions.Questions.Questions.2016.Questions.Questions.Questions.2016.Questions.Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016 Questions.2016"}, {"heading": "Acknowledgements", "text": "I would also like to thank Santiago Pascual de la Puente for the innumerable times he has helped me during the project with his wise advice and knowledge in deep learning. My partners in the X-theses group also deserve a mention here, as they talk to them week after week about the project and listen to what they have researched to enrich this project. Together with them, I would like to thank Albert Gil for his help and support in using the GPI cluster. I would also like to thank Marc Bolan, Petia Radeva and the rest of the Computer Vision group at the Universitat de Barcelona for their advice and for providing very useful data for our experiments."}, {"heading": "1 Introduction 11", "text": "1.1 Purpose.............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................."}, {"heading": "2 State of the art 17", "text": "2.1 Image processing................................................ 172.2 Text processing......................................... 182.3 Answering visual questions.........................................................................................."}, {"heading": "3 Methodology 21", "text": "The question of how to proceed has not yet been decided."}, {"heading": "4 Results 31", "text": "......................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................"}, {"heading": "5 Budget 40", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6 Conclusions 41", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 Appendices 42", "text": "The reserves for the definition of the task and the necessary capacities are very useful in this example to solve the problem. (349.). The reserves for the definition of the task and the determination of needs (340.). (340. The reserves for the reserves. (340.). The reserves for the reserves. (340.). The reserves for the reserves. (340.). The reserves. (340.). The reserves. (340.). The reserves. (340.). The reserves. (340.). The reserves. The reserves. (340.). The reserves. The reserves. The reserves. (340.). The reserves. (340. The reserves. The reserves. (340. The reserves.)."}, {"heading": "1.1 Statement of purpose", "text": "In recent years, the number of publications and job offers related to deep learning has multiplied, and academia and industry are pushing to accelerate development and research in this area, because deep learning has a large number of problems that have been addressed in recent years."}, {"heading": "1.2 Requirements and specifications", "text": "One of the main building blocks of this project is the software that was developed to take part in the challenge and to be able to create and test different models.1http: / / www.visualqa.org / challenge.html 2http: / / www.visualqa.org / workshop.html12Regarding this, the requirements for this project are as follows: \u2022 Develop software that can be used in the future to continue research in this area, starting with a skeletal / base project \u2022 Create a deep neural network model that uses NLP and CV techniques to process the question or image \u2022 Try different model configurations to increase the accuracy of the original model \u2022 Submit the results to the CVPR16 VQA ChallengeThe specifications are as follows: \u2022 Use Python as the programming language \u2022 Build the project using a deep learning framework. Keras3 has been selected as a framework and can be executed on TheanoT4 or Floens4."}, {"heading": "1.3 Methods and procedures", "text": "This thesis represents the first attempt to solve the problem of visual response to the Visual Question by the GPI and TALP research groups at the Universitat Polite de Catalunya. We started to develop the project from scratch (with a view to the project itself), but using a deep learning framework called Keras. Keras is a neural network library for Python, which is built to be easy to use and allows rapid prototyping, and this was achieved by building a shell around another deep learning python library, which is responsible for managing tensors and performing low level calculations. This second library, which functions as a backend, can be either Theano or TensorFlow. We have done our experiments with Keras on Theano.Apart from these libraries, the only resources developed by other authors are the visual features of our last model. The Computer Vision Group at the University of Barcelona has provided us with the pre-calculated visual features of the VA data set."}, {"heading": "1.4 Work Plan", "text": "The project was developed in collaboration between the GPI and TALP research groups of the Universitat Polite de Catalunya. Discussions and decisions on the project took place in a regular weekly meeting 3http: / / keras.io / 4http: / / deeplearning.net / software / theano / 5https: / / www.tensorflow.org / 13. It was complemented by a second two-hour research seminar where other students completed their bachelor, master or doctoral thesis at the GPI. Below is the work plan of this project and its deviations from the original plan. These deviations are explained in detail in section 1.5 Incidents and Modifications."}, {"heading": "1.4.1 Work Packages", "text": "\u2022 WP 1: Project proposal and work plan \u2022 WP 2: Introduction to Deep Learning and Python \u2022 WP 3: Introduction to (visual) questions answering tasks and the Keras framework \u2022 WP 4: First VQA model \u2022 WP 5: Critical review of the project \u2022 WP 6: Participation in the VQA Challenge CVPR16 \u2022 WP 7: Final report of the project \u2022 WP 8: Presentation and oral defence14"}, {"heading": "1.4.2 Gantt Diagram", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1.5 Incidents and Modifications", "text": "During the project, we had to change some work packages definition and tasks as we wanted to focus more on the visual question and answer challenge. First, the goal of the project was to develop a system that could generate both questions and answers from an image, which would have medical application in patients with mild cognitive impairment (early stages of Alzheimer's) who may receive automated reminiscence therapy based on the images captured by egocentric cameras. However, solving the VQA challenge in terms of annotated data sets, metrics and potential effects was more practicable, so it was decided to tackle this task first. The medical applications described will be explored by other students in the fall of 2016. We also completed the new task of writing an extended abstraction for the VQA workshop. We decided to write and submit the extended abstraction as this gave me some expertise in paper composition and this way we could share with the community some of our ideals."}, {"heading": "2.1 Image processing", "text": "Deep Convolutional Neural Networks (CNN) have proven to be state-of-the-art, leading to typical computer vision tasks such as image acquisition, object recognition and object recognition. A common approach to images is to use a commercial model (VGG [21], AlexNet [10], GoogLeNet [22] etc.) prepared for such tasks with a few large image datasets such as ImageNet1 [4] and fully connected layers. Convolutional layers used in image processing perform 2D coils of the previous layers 1http: / / www.image-net.org / 17output (which can be an image) and fully connected layers. The layers used in image processing perform 2D coils of the previous layers 1http: / / www.image-net.org / 17output, where the weights specify the folding filter."}, {"heading": "2.2 Text processing", "text": "In fact, the fact is that most people are able to move to another world, in which they are able, in which they are able to move, in which they are able, and in which they are able, in which they are able to move, in which they are able, in which they are able, in which they are able, in which they are able to move, in which they are able to move, in which they are able, in which they are able to move."}, {"heading": "2.3 Visual Question Answering", "text": "In fact, most people who are in a position to put themselves in the world, to put themselves in another world, to put themselves in another world, to put themselves in another world, to put themselves in another world, in which they are able to live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they, in which they live, in which they, in which they, in which they live, in which they, in which they live, in which they, in which they, in which they, in which they live."}, {"heading": "3.1 A programmer\u2019s word", "text": "In the early stages of this work, when we were looking for any base code to perform VQA tasks that we could start with, we found that the open source projects for VQA were not reusable at all. In fact, it seems that the vast majority of the research code there was not developed with best practice programming or reusability in mind. Therefore, we decided to develop our code with some important things in mind: modularity, abstraction, reusability. We intended to apply as many good practices as possible, as we had a time constraint because we wanted to present our results in the VQA Challenge. As always with software projects, the time variable was crucial to how much more modular, abstract or reusable our code was at the end. Nevertheless, we think that the final work will be very useful as a starting point for future projects related to VQA and also as a good end of the process."}, {"heading": "3.1.1 Choosing the best language", "text": "C + +, Lua and Python were the finalists in this search. C + + was discarded because it is sometimes a bit cumbersome to prototype things quickly, because of the syntax itself and the fact that it is a compiled language. Lua and Python have a very similar syntax, because they are both high level and both programming language with a fast learning curve and also fast for prototyping. In the beginning, all the open source projects we found had something to do with VQA written in Lua using a deep learning framework called Torch1. This seemed to be a good reason for choosing Lua over Python, but when we looked into the future of the community, we found that frameworks like Theano or TensorFlow were very successful."}, {"heading": "3.1.2 A Pythonic project", "text": "After choosing Python as the programming language, one of the things we wanted to do was improve the readability and reusability of this project by following a code style guide. In the programming world, there are many languages, and for each of them there are tons and tons of styles that developers tend to program with, and we programmers are choosy. Therefore, the most popular languages usually have a code style guide that defines what the code should look like and what is good practice in that language. Using these code style guidelines increases the readability of the code and helps you develop a better code that is easier to expand or use. For Python, this code style guide is called PEP823. The code presented with this project follows the PEP8 policy."}, {"heading": "3.1.3 An eight-leg cat called octocat", "text": "In order to make the project as professional as possible and to keep track of the changes we've made, we've used Git4 as a version control system (VCS). Git gives us the ability to work in parallel and prototype things when needed, without the fear of not being able to restore our previous work or having to do all the tedious manual backups. With Git, we've created a history of our project development process. To store our Git repository, we've used GitHub, as it allows us to use the project as open source and allow community contributions after completion. After the deadline for the VQA challenge, we've released our GitHub repository 5 as public, so anyone can use the code."}, {"heading": "3.2 Dataset", "text": "To train a model (in supervised learning) we need a very large amount of data. This data is an example of input-output pairs. In our case, the input is both the image and the question and the output is the answer.To train our models, we have used the real image VQA dataset 6, which is one of the largest visual question-answering datasets. This dataset is provided by the organizers of the VQA Challenge and is divided into the typical three subgroups: train, validation and test. The train subset consists of 82,783 images, 248,349 questions and 2,483,490 answers; the validation by 40,504 images, 121,512 questions and 1,215,120 answers; and finally the test consists of 81,434 pictures and 244,302 questions. The full explanation of how the organizers created this dataset can be found in their paper."}, {"heading": "3.3 Text-based QA toy example", "text": "As we explained in our Work Plan 1.4, we started to familiarize ourselves with VQA tasks and the functioning of the Keras library using a text-based QA model. The type of text-based QA problem we were dealing with was a toy example where a short story and a question related to that story were given to the model so that it could predict an answer to a single word."}, {"heading": "3.3.1 Tokenization", "text": "The first step is to convert the words (from the story and the question) into numbers that can be fed into the model. We did this pre-processing with a tokenizer from Keras, which is responsible for tokenizing the text sequences. By tokenization here, we mean splitting the entire string into words, removing the unnecessary ones (punctuation for example), and converting each word into a number. This number is the index of that word in a dictionary that we have previously created. Our tokenizer's dictionary or vocabulary may or may not be a predefined one. We did not use a predefined dictionary, but created our own by using the training data. In order to create such a dictionary, its size is important, the number of unique words it can contain. In addition, a special word is added that represents \"unknown\" words, i.e. words that are not in the previous list. From this point on, a string is no longer a \"string\" but a \"dictionary that represents a dictionary."}, {"heading": "3.3.2 Model architecture", "text": "This year, it is more than ever before in the history of the city."}, {"heading": "3.3.3 Model up and running", "text": "After we had this model built, we trained it with 10,000 samples for the QA1 task defined by Weston et. al [23] and did some small tests. At this time, we did not conduct extensive experiments as this was not our goal and because the parameters of the model and the dataset we used were designed as a toy example rather than a real solution. In this phase, we were able to train the model as quickly as possible and verify that the whole process worked."}, {"heading": "3.4 Extending the text-based QA model for visual QA", "text": "Based on the previous text-based QA model, we modified it so that it could be used for visual QA. Note that the architecture shown in Figure 3.1 is based on the idea that we have a story that gives the model some information and then ask a question about that story. The model uses the information gained from the story to answer the question. In visual QA, our story is the picture that gives us the information we need to answer the question."}, {"heading": "3.4.1 Image processing", "text": "With the idea that our image is the \"story\" from which we must extract information, we changed the plot branch to an image branch. In such a branch, we use the Convolutionary Neural Network VGG-16 proposed by Simonyan et. al. [21], a commercially available model to extract the visual features, as you can see in Figure 3.2. We did not use the output of the entire model, but truncated it to the last revolutionary layer before the fully connected fc-4096. Using the output of the conv layers instead of the fully connected ones is a common practice to extract visual feature maps.In order to be able to combine this visual information with the information obtained from the question, we must turn this 2D map into a vector. We used a flat layer for this and then we give this vector to the repeating block. Notice that we are now repeating the image (our visual history) instead of the question."}, {"heading": "3.4.2 Modifications on the question branch", "text": "As shown in Figure 3.2, the question branch now has only one word embedded. This means that at each time step a question word is inserted into the model and, as the visual characteristics repeat, each is merged with the information of the entire image. The dimension of the word embedding and the visual characteristics are different, so our merging process is now not a summation but a concatenation of both vectors."}, {"heading": "3.4.3 Model parameters", "text": "The most important parameters of this model are: vocabulary size, hidden LSTM units, embed size, question maximum length, learning rate and lot size. It is also important which optimizer should be used. We set the lot size so that it is the maximum we could store in GPU RAM, with a value of only 32 samples. We must also take into account that we have to fit the compiled model (its weights) into the GPU RAM, and that is very expensive, since some of our layers and thus its weights are huge, as we will see today. The learning rate for the network parameter was determined by the Adam Optimizer [9], which modifies the learning rate through training. We only have to specify the initial learning rate, and we chose the standard size proposed by Kingma et., and hence its weights, are huge, as we see today.In the original paper, the 0.0001.For the maximum length of the question, we have the maximum amount of the workout of the question of the maximum length of the maximum length of the workout of the question."}, {"heading": "3.4.4 Implementation and training", "text": "These weights were the result of training the entire VGG-16 on ImageNet8, one of the largest image sets available today. When we frozen the VGG-16 weights, we did not fine-tune it, but only trained our own levels. To train this model, we started using the computing service of the Image Processing Group (GPI) at the Universitat Politecnica de Catalunya. Since we could fit only 32 samples per batch, the training process was faced with a speed of 17-24 hours per epoch, as the size of the vocabulary required the need to create huge vectors that represent the answers, and the size of the compiled model reflected this. Since we could fit only 32 samples per batch, the training process was at a speed of 17-24 hours per epoch, in which Ventidia Titan X GPUs were equipped with 12 GB of RAM."}, {"heading": "3.5 Model improvement: towards the final model", "text": "The prohibitive duration of the training process led us to choose to pre-calculate the visual characteristics of the image. This approach made sense because we did not modify the values of the VGG-16 convolution network responsible for extracting these characteristics. Instead of pre-calculating the visual characteristics using an isolated VGG-16, our partners from the Computer Vision Group of the University of Barcelona (UB) provided us with these characteristics, which were extracted using a new CNN called Kernelized CNN (Liu et. al. [11]). For a brief description, see Methods and Procedures 1.3. The dimension of the output vector of the KCNN module is 1024. The rest of the parameters and functionality remain the same as the architecture described in Section 3.4.8http: / / www.image-net.org / 27."}, {"heading": "3.5.1 Implementation and training", "text": "In the previous case, we used an abstract model implementation of keras called Sequential9, which essentially consists of a stack of layers. This model also allows the possibility to merge two sequential models into one, which is what we used to create the two input branches. For this modified model, we switched to the more flexible Functional API10, which is supposed to form more powerful models in a graph approximation. This new interface allows us to work with the tensors themselves, making it easier to modify the model and make it more complex. By using the predetermined visual features and this new implementation, we reduced the training time of an epoch to less than one hour (about 40 minutes)."}, {"heading": "3.5.2 Batch normalization and reducing the learning rate", "text": "One of the first modifications we tried was the addition of a batch normalization layer (Figure 3.4) after the merging process, as this supports the training process and usually increases accuracy. Ioffe and Szegedy propose to introduce the normalization of the input distribution of the layers within the model architecture [7], introducing this normalization through their novel layer (batch normalization), which reduces internal covariate shift. We also sequentially reduced the initial learning rate from 0.001 to 0.0003 and 0.0001, and found that the latter provided the best accuracy, as we will explain later in chapter 4 of results."}, {"heading": "3.6 The final model: sentence embedding", "text": "Our last model was the one that predicted the answers with greater accuracy and posed the VQA challenge. Several changes have been made to the preliminary prototypes so that we can have a look at the different blocks represented in Figure 3.5.9http: / / keras.io / getting-started / sequential-model-guide / 10http: / / keras.io / getting-started / functional-api-guide / 28"}, {"heading": "3.6.1 Question embedding", "text": "The Question Branch has been modified by adding an LSTM at the end of the word embedding, creating a sentence embedding, in our case the embedding of the question.The resulting vector of the sentence embedding module is a dense and semantic representation of the whole question, as it was in our text-based QA Model 3.1. The difference here is that we did not choose the same value for the word embedding dimension and the number of hidden units of the LSTM. We have specified 100 as the word embedding dimension and 256 as the number of hidden units of the LSTM, which is a common value. We have increased the number of hidden units as this can help to increase the accuracy in the compressed representation of the questions, but we have not changed the embedding dimension as this could reduce the word embedding density."}, {"heading": "3.6.2 Image semantic projection", "text": "In order to be able to project the visual features into a space of the same dimension as the question of embedding, we have opted for a fully connected plane according to the KCNN module. The fully connected plane can be considered a matrix operation that projects the 1024 vector of the features into a 256 vector in semantic space. As an activation function for this plane, we have chosen ReLU."}, {"heading": "3.6.3 Merging and predicting", "text": "Since both textual and visual characteristics have been projected into a 256-dimensional space, we can merge them and merge those characteristics. Now, since both the question and the image are represented by a single vector and not by a sequence of vectors, there is no need to add an LSTM after the merge, and we can feed the resulting merged vector to Softmax so that it can predict the answer."}, {"heading": "3.6.4 Other modifications", "text": "The learning rate of this model was initialized to 0.0001 compared to 0.001 of the first KCNN model. We also tried to reduce the learning rate to 0.00001 and add a batch normalization phase after the merging process, but as we will see in the next chapter, neither model increased the accuracy of the original final model. Before undergoing the VQA challenge via the test set, we also tried to train the model with the entire subset of training and the 70% subset of validation, but that did not help.30Chapter 4Results In this chapter, the results of the various models exposed in Chapter 3 of the methodology are presented."}, {"heading": "4.1 Evaluation metric", "text": "The new formula for accuracy per answer is as follows: Acc (ans) = min (# human, answer 3, 1) (4.1) Accuracy over the entire data set is an average of accuracy per answer for all samples. The interpretation of Equation 4.1 is as follows: An answer is given as correct (accuracy equals 1) if the same exact answer was given by at least three human commentators. Zero equals zero accuracy, and from there each match yields 0.33 points for accuracy with a maximum of 1."}, {"heading": "4.2 Dataset", "text": "At this point, it is worth summarizing the features of the data set mentioned in 3.2: \u2022 Training data set: 82,783 pictures, 248,349 questions, 2,483,490 answers \u2022 Validation data set: 40,504 pictures, 121,512 questions, 1,215,120 answers \u2022 Test data set: 81,434 pictures, 244,302 questionsNote: For each picture, there are three questions and for each question, there are ten answers. These ten answers were provided by human commentators and the most common ones were selected. Most answers are the same but reformulated. The organizers also provide a Python script to evaluate the results in the same way as they do when submitting the test results."}, {"heading": "4.3 Models architectures and setups", "text": "In the following section, we will refer to the models using a number to be clearer and more concise. These identifiers are defined here with a description of the model / configuration: The results for Model 0 are not presented because we have only completed the construction phase, but have not completed the training process for the problems already described in Chapter 3. We include them here only to determine that this was our basic VQA model."}, {"heading": "4.4 Training and validation losses", "text": "One of the earlier results that helped us to improve our models was training, and most importantly, the validation loss. In the following figures you can see the evolution of training and validation losses per epoch. Figure 4.1: Training losses (blue) and validation losses (green) for model 1Figure 4.2: Training losses (blue) and validation losses (green) for model 2For models 1 and 2 we have found that the validation loss increases from epoch 3 to the end. We can also acknowledge that after the second epoch the model does not learn anymore, the training loss is fixed by a value with some \"noise\" and also that in the first epoch the model experiences a huge reduction in training loss. Both factors are an indicator that the models are slowly diverging from each other and therefore the learning rate is too high. In model 3 we reduce the learning rate to 1 / 10 of the original model by having a value of 0.01, as we can easily see in the 4.3 figure."}, {"heading": "4.5 Quantitative results in the VQA Challenge 2016", "text": "The model we presented to the CVPR16 VQA Challenge was model number 4. We get an accuracy of 53.62% over the test data set. In Table 4.2 we present a comparison between our accuracy and the accuracy of the base model and the top model. As we have not presented all the results from the different models, we do not have test accuracy for some of them (models 2 and 3)."}, {"heading": "4.5.1 A general overview", "text": "The first interpretation of these results is that the gap between the accuracy of the base model and the best (UC Berkeley & Sony) is quite small, only 12.41%. What this means is that it is very difficult to build models that are good at solving visual quality criteria, since the model must have a deep understanding of the scene and the question, and also have reasonably good reasoning skills. Another fact to note is that there is a performance difference between humans and models that perform such tasks, and that means that there is still room for further research in this area. In this context, it is worth noting that the human accuracy that this metric uses is relatively low compared to what one would expect (close to 1)."}, {"heading": "4.5.2 Our results", "text": "Well, when we evaluate our results, we can see in Table 4.2 that our model performs slightly worse than the baseline provided by the VQA Challenge organizers, for a reason below. The first reason is that our model predicts only single word responses, which means that we will not have 100% accuracy for multi-word responses, as we will never have a complete match. The second and most important reason is that the base model and many other models are presented in the challenge ([12], [13], [8], [1] that the average length of our VQA responses is 1.1 word with a 0.4 deviation."}, {"heading": "4.5.3 Per answer type results", "text": "The annotations (answers) of the VQA dataset are divided into three different types: Yes / No, Number or Other Questions. Each question has been assigned to one of these three answer types, which allows us to better understand how our model behaves in the face of different types of questions and how well it answers them. In analyzing the results per answer type shown in Table 4.2, we see a huge difference in accuracy between the Yes / No answers and the number or other answer types. The latter usually require a better understanding of the picture and the question in order to be able to answer them based on the type of questions (why...?, what is...?), as opposed to the more common question type...? for the Yes / No answer type. This difference can be better understood with the qualitative results in the following section."}, {"heading": "4.6 Qualitative results", "text": "In this section, we will present some examples of the results obtained for our best model, which come from the validation subgroup as we do not have answers for the test subset; the following examples are grouped by accuracy, with three examples for each accuracy, one per question type (yes / no, number and others); the VQA evaluation script punctures with 5 different accuracies (0, 30, 60, 90, 100) according to Eq.3 4.1.These examples were randomly selected from the results to obtain a sample representative of the entire dataset; 3Even if the challenge provides the above formula for calculating accuracy, it appears that the script is the result of (# humansthatsaidans) / 3 to the nearest lower integer363738 From these examples, we can see that the images in the VQA dataset (which are MS COCO) are rich in information and very different in the entire dataset."}, {"heading": "4.7 Some words at the VQA Challenge", "text": "This year, we will be able to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top, \"he said.\" We have to put ourselves at the top. \""}], "references": [{"title": "Vqa: Visual question answering", "author": ["Stanislaw Antol", "Aishwarya Agrawal", "Jiasen Lu", "Margaret Mitchell", "Dhruv Batra", "C Lawrence Zitnick", "Devi Parikh"], "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "On the properties of neural machine translation: Encoder-decoder approaches", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Dzmitry Bahdanau", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1409.1259,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning phrase representations using rnn encoder-decoder for statistical machine translation", "author": ["Kyunghyun Cho", "Bart Van Merri\u00ebnboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio"], "venue": "arXiv preprint arXiv:1406.1078,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Imagenet: A largescale hierarchical image database", "author": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "venue": "In Computer Vision and Pattern Recognition,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2009}, {"title": "Multimodal compact bilinear pooling for visual question answering and visual grounding", "author": ["Akira Fukui", "Dong Huk Park", "Daylen Yang", "Anna Rohrbach", "Trevor Darrell", "Marcus Rohrbach"], "venue": "arXiv preprint arXiv:1606.01847,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1997}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["Sergey Ioffe", "Christian Szegedy"], "venue": "arXiv preprint arXiv:1502.03167,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Multimodal residual learning for visual qa", "author": ["Jin-Hwa Kim", "Sang-Woo Lee", "Dong-Hyun Kwak", "Min-Oh Heo", "Jeonghee Kim", "Jung-Woo Ha", "Byoung-Tak Zhang"], "venue": "arXiv preprint arXiv:1606.01455,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2016}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba"], "venue": "arXiv preprint arXiv:1412.6980,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2014}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2012}, {"title": "Kernelized deep convolutional neural network for describing complex images", "author": ["Zhen Liu"], "venue": "arXiv preprint arXiv:1509.04581,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Hierarchical question-image coattention for visual question answering", "author": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "venue": "arXiv preprint arXiv:1606.00061,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2016}, {"title": "Learning to answer questions from image using convolutional neural network", "author": ["Lin Ma", "Zhengdong Lu", "Hang Li"], "venue": "arXiv preprint arXiv:1506.00333,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Learning models for actions and person-object interactions with transfer to question answering", "author": ["Arun Mallya", "Svetlana Lazebnik"], "venue": "arXiv preprint arXiv:1604.04808,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2016}, {"title": "Efficient estimation of word representations in vector space", "author": ["Tomas Mikolov", "Kai Chen", "Greg Corrado", "Jeffrey Dean"], "venue": "arXiv preprint arXiv:1301.3781,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Generating natural questions about an image", "author": ["Nasrin Mostafazadeh", "Ishan Misra", "Jacob Devlin", "Margaret Mitchell", "Xiaodong He", "Lucy Vanderwende"], "venue": "arXiv preprint arXiv:1603.06059,", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2016}, {"title": "Image question answering using convolutional neural network with dynamic parameter prediction", "author": ["Hyeonwoo Noh", "Paul Hongsuck Seo", "Bohyung Han"], "venue": "arXiv preprint arXiv:1511.05756,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2015}, {"title": "Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval", "author": ["Hamid Palangi", "Li Deng", "Yelong Shen", "Jianfeng Gao", "Xiaodong He", "Jianshu Chen", "Xinying Song", "Rabab Ward"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2016}, {"title": "Exploring models and data for image question answering", "author": ["Mengye Ren", "Ryan Kiros", "Richard Zemel"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["K. Simonyan", "A. Zisserman"], "venue": "In International Conference on Learning Representations,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Going deeper with convolutions", "author": ["Christian Szegedy", "Wei Liu", "Yangqing Jia", "Pierre Sermanet", "Scott Reed", "Dragomir Anguelov", "Dumitru Erhan", "Vincent Vanhoucke", "Andrew Rabinovich"], "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Towards ai-complete question answering: A set of prerequisite toy tasks", "author": ["Jason Weston", "Antoine Bordes", "Sumit Chopra", "Tomas Mikolov"], "venue": "arXiv preprint arXiv:1502.05698,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2015}, {"title": "Dynamic memory networks for visual and textual question answering", "author": ["Caiming Xiong", "Stephen Merity", "Richard Socher"], "venue": "arXiv preprint arXiv:1603.01417,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2016}, {"title": "Simple baseline for visual question answering", "author": ["Bolei Zhou", "Yuandong Tian", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus"], "venue": "arXiv preprint arXiv:1512.02167,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2015}, {"title": "Visual7w: Grounded question answering in images", "author": ["Yuke Zhu", "Oliver Groth", "Michael Bernstein", "Li Fei-Fei"], "venue": "arXiv preprint arXiv:1511.03416,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2015}], "referenceMentions": [{"referenceID": 22, "context": "Facebook AI Research presented a set of tasks, called bAbI [23], to evaluate AI models\u2019 text understanding and reasoning).", "startOffset": 59, "endOffset": 63}, {"referenceID": 10, "context": "They extracted these features using a special kind of CNNs called Kernelized CNN (KCNN) as proposed by Liu [11].", "startOffset": 107, "endOffset": 111}, {"referenceID": 20, "context": "A common approach when dealing with images is to use an off-the-shelf model (VGG [21], AlexNet [10], GoogLeNet [22], etc.", "startOffset": 81, "endOffset": 85}, {"referenceID": 9, "context": "A common approach when dealing with images is to use an off-the-shelf model (VGG [21], AlexNet [10], GoogLeNet [22], etc.", "startOffset": 95, "endOffset": 99}, {"referenceID": 21, "context": "A common approach when dealing with images is to use an off-the-shelf model (VGG [21], AlexNet [10], GoogLeNet [22], etc.", "startOffset": 111, "endOffset": 115}, {"referenceID": 3, "context": ") pre-trained to do such tasks with some large image dataset such as ImageNet1 [4] and use some of the inner-layer\u2019s outputs as a representation of the visual features of the image.", "startOffset": 79, "endOffset": 82}, {"referenceID": 14, "context": "[15][16].", "startOffset": 0, "endOffset": 4}, {"referenceID": 15, "context": "[15][16].", "startOffset": 4, "endOffset": 8}, {"referenceID": 5, "context": "For further details address [6].", "startOffset": 28, "endOffset": 31}, {"referenceID": 18, "context": "[19] takes the one-hot representation for each of the words in the text sequences, obtains its word embedding and then feeds the LSTM with them, one at each timestep, keeping the same order as presented in the sequence.", "startOffset": 0, "endOffset": 4}, {"referenceID": 2, "context": "[3] for statistical machine translation with GRU cells, which are a similar approach to that of LSTM.", "startOffset": 0, "endOffset": 3}, {"referenceID": 23, "context": "The most common approach is to extract visual features of the image using a pretrained off-the-shelf network and process the question using word embeddings or sentence embedding [24][20][12][8][25][1][26].", "startOffset": 178, "endOffset": 182}, {"referenceID": 19, "context": "The most common approach is to extract visual features of the image using a pretrained off-the-shelf network and process the question using word embeddings or sentence embedding [24][20][12][8][25][1][26].", "startOffset": 182, "endOffset": 186}, {"referenceID": 11, "context": "The most common approach is to extract visual features of the image using a pretrained off-the-shelf network and process the question using word embeddings or sentence embedding [24][20][12][8][25][1][26].", "startOffset": 186, "endOffset": 190}, {"referenceID": 7, "context": "The most common approach is to extract visual features of the image using a pretrained off-the-shelf network and process the question using word embeddings or sentence embedding [24][20][12][8][25][1][26].", "startOffset": 190, "endOffset": 193}, {"referenceID": 24, "context": "The most common approach is to extract visual features of the image using a pretrained off-the-shelf network and process the question using word embeddings or sentence embedding [24][20][12][8][25][1][26].", "startOffset": 193, "endOffset": 197}, {"referenceID": 0, "context": "The most common approach is to extract visual features of the image using a pretrained off-the-shelf network and process the question using word embeddings or sentence embedding [24][20][12][8][25][1][26].", "startOffset": 197, "endOffset": 200}, {"referenceID": 25, "context": "The most common approach is to extract visual features of the image using a pretrained off-the-shelf network and process the question using word embeddings or sentence embedding [24][20][12][8][25][1][26].", "startOffset": 200, "endOffset": 204}, {"referenceID": 0, "context": "[1], the organizers of the VQA challenge and the creators of the VQA dataset, propose as their baseline a model that uses VGG-16 [21] to extract the visual features of the image.", "startOffset": 0, "endOffset": 3}, {"referenceID": 20, "context": "[1], the organizers of the VQA challenge and the creators of the VQA dataset, propose as their baseline a model that uses VGG-16 [21] to extract the visual features of the image.", "startOffset": 129, "endOffset": 133}, {"referenceID": 21, "context": "A simple bag-of-words and word embedding model that uses GoogLeNet [22] for the image processing and a concatenation of both visual and textual features is what Zhou et.", "startOffset": 67, "endOffset": 71}, {"referenceID": 24, "context": "[25] present in their paper as a basic approximation to VQA.", "startOffset": 0, "endOffset": 4}, {"referenceID": 17, "context": "[18] propose, called Dynamic Parameter Prediction Network (DPPnet).", "startOffset": 0, "endOffset": 4}, {"referenceID": 1, "context": "These weights are predicted by a network (parameter prediction network) composed by a Gated Recurrent Unit (GRU) [2] connected to a fully-connected layer.", "startOffset": 113, "endOffset": 116}, {"referenceID": 25, "context": "Other authors propose attention models to improve the performance of the whole model, stating that most of the questions refer to specific image locations [26][24][12].", "startOffset": 155, "endOffset": 159}, {"referenceID": 23, "context": "Other authors propose attention models to improve the performance of the whole model, stating that most of the questions refer to specific image locations [26][24][12].", "startOffset": 159, "endOffset": 163}, {"referenceID": 11, "context": "Other authors propose attention models to improve the performance of the whole model, stating that most of the questions refer to specific image locations [26][24][12].", "startOffset": 163, "endOffset": 167}, {"referenceID": 25, "context": "In [26] the visual features (the output of fc7 of VGG-16) are treated as if they were the first word in the question,", "startOffset": 3, "endOffset": 7}, {"referenceID": 19, "context": "[20] present a similar but simplified method that also treats the image as the first word of the question but that does not have an attention model.", "startOffset": 0, "endOffset": 4}, {"referenceID": 23, "context": "[24] present a model based on Dynamic Memory Networks (DNM), that is a modular architecture with attention models.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "[12], attention in the question can also be applied to increase the model performance.", "startOffset": 0, "endOffset": 4}, {"referenceID": 7, "context": "More sophisticated approaches have also been presented, such as Multimodal Residual Learning applied to VQA [8] that uses Deep Residual Learning to build complex and very deep networks.", "startOffset": 108, "endOffset": 111}, {"referenceID": 13, "context": "Other works propose learning methods for specific sub-problems of VQA such as human action prediction and then apply those trained models for VQA tasks [14].", "startOffset": 152, "endOffset": 156}, {"referenceID": 4, "context": "[5] which uses Multimodal Compact Bilinear pooling (MCB) to merge the visual features and the information from the question.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "The whole explanation on how the organizers created this dataset can be found in their paper [1].", "startOffset": 93, "endOffset": 96}, {"referenceID": 2, "context": "[3] for a complete encoder-decoder architecture for automatic machine translation).", "startOffset": 0, "endOffset": 3}, {"referenceID": 15, "context": "The story branch has only a word embedding block [16].", "startOffset": 49, "endOffset": 53}, {"referenceID": 5, "context": "The output of that block is then given to a LSTM [6], which is a Recurrent Neural Network (RNN).", "startOffset": 49, "endOffset": 52}, {"referenceID": 18, "context": "This combination of the word embedding and a LSTM that sees all the question words and then outputs its memory state is known as a sentence embedding [19].", "startOffset": 150, "endOffset": 154}, {"referenceID": 22, "context": "al [23] and we did some small tests.", "startOffset": 3, "endOffset": 7}, {"referenceID": 20, "context": "[21], an off-the-shelf model, to extract the visual features, as you can see in Figure 3.", "startOffset": 0, "endOffset": 4}, {"referenceID": 8, "context": "The learning rate for the network\u2019s parameter was governed by the Adam optimizer [9] which modifies the learning rate through the training.", "startOffset": 81, "endOffset": 84}, {"referenceID": 10, "context": "[11]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 6, "context": "Ioffe and Szegedy propose to introduce the normalization of the layers\u2019 input distribution inside the model architecture [7].", "startOffset": 121, "endOffset": 124}, {"referenceID": 25, "context": "[26] we can see that it is 96%, much more logical a priori.", "startOffset": 0, "endOffset": 4}, {"referenceID": 11, "context": "The second and most important reason is that the baseline model and many of other models presented in the challenge ([12], [18], [13], [5], [8], [25], [1]), including the top ones, are a classifier built upon the N (in their case 1000) most frequent answers.", "startOffset": 117, "endOffset": 121}, {"referenceID": 17, "context": "The second and most important reason is that the baseline model and many of other models presented in the challenge ([12], [18], [13], [5], [8], [25], [1]), including the top ones, are a classifier built upon the N (in their case 1000) most frequent answers.", "startOffset": 123, "endOffset": 127}, {"referenceID": 12, "context": "The second and most important reason is that the baseline model and many of other models presented in the challenge ([12], [18], [13], [5], [8], [25], [1]), including the top ones, are a classifier built upon the N (in their case 1000) most frequent answers.", "startOffset": 129, "endOffset": 133}, {"referenceID": 4, "context": "The second and most important reason is that the baseline model and many of other models presented in the challenge ([12], [18], [13], [5], [8], [25], [1]), including the top ones, are a classifier built upon the N (in their case 1000) most frequent answers.", "startOffset": 135, "endOffset": 138}, {"referenceID": 7, "context": "The second and most important reason is that the baseline model and many of other models presented in the challenge ([12], [18], [13], [5], [8], [25], [1]), including the top ones, are a classifier built upon the N (in their case 1000) most frequent answers.", "startOffset": 140, "endOffset": 143}, {"referenceID": 24, "context": "The second and most important reason is that the baseline model and many of other models presented in the challenge ([12], [18], [13], [5], [8], [25], [1]), including the top ones, are a classifier built upon the N (in their case 1000) most frequent answers.", "startOffset": 145, "endOffset": 149}, {"referenceID": 0, "context": "The second and most important reason is that the baseline model and many of other models presented in the challenge ([12], [18], [13], [5], [8], [25], [1]), including the top ones, are a classifier built upon the N (in their case 1000) most frequent answers.", "startOffset": 151, "endOffset": 154}, {"referenceID": 16, "context": "(including Mitchell) have recently published a paper where they propose a model to generate natural questions [17] (we presented our extended abstract before this paper was published).", "startOffset": 110, "endOffset": 114}, {"referenceID": 0, "context": "Related Work Different network architectures have been presented to solve Visual Question-Answering tasks [1, 2, 5, 6].", "startOffset": 106, "endOffset": 118}, {"referenceID": 1, "context": "Related Work Different network architectures have been presented to solve Visual Question-Answering tasks [1, 2, 5, 6].", "startOffset": 106, "endOffset": 118}, {"referenceID": 4, "context": "Related Work Different network architectures have been presented to solve Visual Question-Answering tasks [1, 2, 5, 6].", "startOffset": 106, "endOffset": 118}, {"referenceID": 5, "context": "Related Work Different network architectures have been presented to solve Visual Question-Answering tasks [1, 2, 5, 6].", "startOffset": 106, "endOffset": 118}, {"referenceID": 4, "context": "The method to vectorize the question ranges from a simple Bag-Of-Words (BOW) representation [5] to the use of RNNs [2] to obtain an embedding of the question.", "startOffset": 92, "endOffset": 95}, {"referenceID": 1, "context": "The method to vectorize the question ranges from a simple Bag-Of-Words (BOW) representation [5] to the use of RNNs [2] to obtain an embedding of the question.", "startOffset": 115, "endOffset": 118}, {"referenceID": 2, "context": "[3] propose an encoder-decoder architecture using GRU RNNs to generate a question from a fact, which is a tuple of subject, relationship, object.", "startOffset": 0, "endOffset": 3}, {"referenceID": 3, "context": "We consider using the model VGG-16 net [4] to extract the visual features from the input image.", "startOffset": 39, "endOffset": 42}], "year": 2016, "abstractText": "This thesis studies methods to solve Visual Question-Answering (VQA) tasks with a Deep Learning framework. As a preliminary step, we explore Long Short-Term Memory (LSTM) networks used in Natural Language Processing (NLP) to tackle Question-Answering (text based). We then modify the previous model to accept an image as an input in addition to the question. For this purpose, we explore the VGG-16 and K-CNN convolutional neural networks to extract visual features from the image. These are merged with the word embedding or with a sentence embedding of the question to predict the answer. This work was successfully submitted to the Visual Question Answering Challenge 2016, where it achieved a 53,62% of accuracy in the test dataset. The developed software has followed the best programming practices and Python code style, providing a consistent baseline in Keras for different configurations. The source code and models are publicly available at https://github.com/imatge-upc/vqa-2016-cvprw.", "creator": "LaTeX with hyperref package"}}}