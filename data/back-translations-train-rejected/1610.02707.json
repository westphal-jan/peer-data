{"id": "1610.02707", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "9-Oct-2016", "title": "Multi-Objective Deep Reinforcement Learning", "abstract": "We propose Deep Optimistic Linear Support Learning (DOL) to solve high-dimensional multi-objective decision problems where the relative importances of the objectives are not known a priori. Using features from the high-dimensional inputs, DOL computes the convex coverage set containing all potential optimal solutions of the convex combinations of the objectives. To our knowledge, this is the first time that deep reinforcement learning has succeeded in learning multi-objective policies. In addition, we provide a testbed with two experiments to be used as a benchmark for deep multi-objective reinforcement learning.", "histories": [["v1", "Sun, 9 Oct 2016 19:08:36 GMT  (177kb,D)", "http://arxiv.org/abs/1610.02707v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["hossam mossalam", "yannis m assael", "diederik m roijers", "shimon whiteson"], "accepted": false, "id": "1610.02707"}, "pdf": {"name": "1610.02707.pdf", "metadata": {"source": "CRF", "title": "Multi-Objective Deep Reinforcement Learning", "authors": ["Hossam Mossalam", "Yannis M. Assael", "Diederik M. Roijers", "Shimon Whiteson"], "emails": ["shimon.whiteson}@cs.ox.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "This year, it has come to the point where there is only one person who is able to move around the world."}, {"heading": "2 Background", "text": "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _"}, {"heading": "3 Methodology", "text": "In this section, we propose our algorithms for MORL that use Deep Q-Learning. First, we propose our basic Deep OLS Learning (DOL-PR) algorithm; we build the OLS framework for multi-objective learning and integrate DQN. Then, we improve this algorithm by introducing Deep OLS Learning with Partial (DOL-PR) and Full Reuse (DOL-FR). DOL, DOL-PR and DOL-FR use a single objective subroutine defined together with DOL in Section 3.1."}, {"heading": "3.1 Deep OLS Learning (DOL)", "text": "There are two prerequisites for using the OLS framework: first, we need a scaled, i.e., an OLS-compliant, one-objective learning algorithm. OLS conformity means that instead of learning a single value per Q (s, a), we need a vector-based Q (s, a) value. To meet these requirements, we adapt our neural network architectures to output a matrix of | A | \u00d7 n (where n is the number of targets) rather than just of | A |, and we train for an extended number of episodes. To meet these requirements, we adjust our neural network architectures to output a matrix of | A | \u00d7 n (where n is the number of targets), and we define scaled deep Q learning that uses this network architecture, and optimize the parameters to maximize the inner product of w and Q values for a given number of episodes."}, {"heading": "3.2 Deep OLS Learning with Full (DOL-FR) and Partial Reuse (DOL-PR)", "text": "While DOL (without reuse) can tackle very large MOMDPs (with full reuse), it is very difficult for the entire QMDP network (with full QMDP network) to confine itself to the use of QMDP. (It is not the type of use of QMDP used in the sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequencer-sequ"}, {"heading": "4 Experimental Evaluation", "text": "In this section, we evaluate the performance of DOL and DOL-PR / FR. We use two multi-objective reinforcement learning problems called Mountain Car (MC) and Deep Sea Treasure (DST). First, we show how DOL and DOL-PR / FR are able to learn the right CSS by having direct access to the state of the problems. Then, we examine the scalability of our proposed methods and evaluate the performance of weight reuse. We create an image version of the DST problem using a bitmap as an input for scaled deep Q learning."}, {"heading": "4.1 Setup", "text": "To stabilize learning, we use a -greedy exploration policy with glow of = 1 to 0.05 for the first 2000 or 3000 episodes, and learning continues for an equal number of episodes, the discount factor is \u03b3 = 0.97, and the target network is reset every 100 episodes. To stabilize learning, we run parallel episodes in lots of 32. Parameters are optimized with Adam and a learning rate of 10 \u2212 3. In each experiment, we use an average of over 5 rounds. For the raw state model, we used an MLP architecture with a hidden layer of 100 neurons and reflected linear activations. To process the 3 x 11 x 10 image inputs from Deep Sea, we used two revolutionary layers of 16 x 3 x 3 and 32 x 3 x 3 and a fully connected layer above it. Finally, we publish the source code to facilitate future research to replicate our experiments 1."}, {"heading": "4.2 Multi-Objective Mountain Car", "text": "In fact, most of us are able to survive on our own, \"he told the German Press Agency in an interview with\" Welt am Sonntag \":\" I don't think we will be able to do what we are doing to put the world in order. \""}, {"heading": "5 Related Work", "text": "However, most algorithms in the literature [26-28] are based on an internal loop approach, which is not clear how to apply it to DQN, i.e. how to convert it to a multiple algorithm. Other work uses an external loop approach, but does not rely on a deep RL [29-31]. We argue that activating deep RL is essential for scaling major problems. Another popular class of MORL algorithms are heuristic political search methods that find a number of alternatives."}, {"heading": "6 Discussion", "text": "In this paper, we proposed three new algorithms that enable the use of deep Q-learning for multi-objective reinforcement learning; our algorithms build on the most recent optimistic linear support framework and solve the problem by learning a policy and corresponding value vectors per iteration; in addition, we expand the main Deep OLS Learning (DOL) to take advantage of the nature of neural networks; and introduce full (DOL-FR) and partial (DOL-PR) reuse of parameters between iterations to pave the way towards faster learning.We demonstrated empirically that our CCS algorithms can learn with high accuracy when faced with problems with large inputs, where DOL-PR outperforms DOL and DOL-FR, indicating that a) reuse is useful; and b) partial reuse instead of full reuse effectively prevents the model from getting stuck in a policy that was optimal for an earlier one."}, {"heading": "Acknowledgements", "text": "This work is partially supported by the TERESA project (EC-FP7 grant # 611153)."}], "references": [{"title": "End-to-end training of deep visuomotor policies", "author": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "arXiv preprint arXiv:1504.00702,", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2015}, {"title": "Data-efficient learning of feedback policies from image pixels using deep dynamical models", "author": ["Y.M. Assael", "N. Wahlstr\u00f6m", "T.B. Sch\u00f6n", "M.P. Deisenroth"], "venue": "NIPS Deep Reinforcement Learning Workshop,", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Embed to control: A locally linear latent dynamics model for control from raw images", "author": ["M. Watter", "J.T. Springenberg", "J. Boedecker", "M.A. Riedmiller"], "venue": "In NIPS,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2015}, {"title": "Multiple object recognition with visual attention", "author": ["J. Ba", "V. Mnih", "K. Kavukcuoglu"], "venue": "In ICLR,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Learning to communicate with deep multi-agent reinforcement learning", "author": ["J.N. Foerster", "Y.M. Assael", "N. de Freitas", "S. Whiteson"], "venue": "arXiv preprint arXiv:1605.06676,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2016}, {"title": "Deep learning for real-time Atari game play using offline Monte-Carlo tree search planning", "author": ["X. Guo", "S. Singh", "H. Lee", "R.L. Lewis", "X. Wang"], "venue": "In NIPS,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2014}, {"title": "Human-level control through deep reinforcement learning", "author": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G. Ostrovski", "S. Petersen", "C. Beattie", "A. Sadik", "I. Antonoglou", "H. King", "D. Kumaran", "D. Wierstra", "S. Legg", "D. Hassabis"], "venue": "Nature, 518(7540):529\u2013533,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Incentivizing exploration in reinforcement learning with deep predictive models", "author": ["B.C. Stadie", "S. Levine", "P. Abbeel"], "venue": "arXiv preprint arXiv:1507.00814,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Dueling network architectures for deep reinforcement learning", "author": ["Z. Wang", "N. de Freitas", "M. Lanctot"], "venue": "arXiv preprint 1511.06581,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Prioritized experience replay", "author": ["T. Schaul", "J. Quan", "I. Antonoglou", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "Deep reinforcement learning with double Q-learning", "author": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In AAAI,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2016}, {"title": "Action-conditional video prediction using deep networks in Atari games", "author": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In NIPS,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2015}, {"title": "Increasing the action gap: New operators for reinforcement learning", "author": ["M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos"], "venue": "In AAAI,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "Massively parallel methods for deep reinforcement learning", "author": ["A. Nair", "P. Srinivasan", "S. Blackwell", "C. Alcicek", "R. Fearon", "A.D. Maria", "V. Panneershelvam", "M. Suleyman", "C. Beattie", "S. Petersen", "S. Legg", "V. Mnih", "K. Kavukcuoglu", "D. Silver"], "venue": "In Deep Learning Workshop,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2015}, {"title": "Move Evaluation in Go Using Deep Convolutional Neural Networks", "author": ["C.J. Maddison", "A. Huang", "I. Sutskever", "D. Silver"], "venue": "In ICLR,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2015}, {"title": "Mastering the game of Go with deep neural networks and tree", "author": ["D. Silver", "A. Huang", "C. Maddison", "A. Guez", "L. Sifre", "G. van den Driessche", "J. Schrittwieser", "I. Antonoglou", "V. Panneershelvam", "M. Lanctot", "S. Dieleman", "D. Grewe", "J. Nham", "N. Kalchbrenner", "I. Sutskever", "T. Lillicrap", "M. Leach", "K. Kavukcuoglu", "T. Graepel", "D. Hassabis"], "venue": "search. Nature,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2016}, {"title": "Managing power consumption and performance of computing systems using reinforcement learning", "author": ["G. Tesauro", "R. Das", "H. Chan", "J.O. Kephart", "C. Lefurgy", "D.W. Levine", "F. Rawson"], "venue": "In NIPS 2007: Advances in Neural Information Processing Systems", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2007}, {"title": "A survey of multi-objective sequential decisionmaking", "author": ["D.M. Roijers", "P. Vamplew", "S. Whiteson", "R. Dazeley"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2013}, {"title": "Computing convex coverage sets for faster multi-objective coordination", "author": ["D.M. Roijers", "S. Whiteson", "F.A. Oliehoek"], "venue": "Journal of Artificial Intelligence Research,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2015}, {"title": "Multi-Objective Decision-Theoretic Planning", "author": ["D.M. Roijers"], "venue": "PhD thesis, University of Amsterdam,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2016}, {"title": "Point-based planning for multi-objective POMDPs", "author": ["D.M. Roijers", "S. Whiteson", "F.A. Oliehoek"], "venue": "In IJCAI 2015: Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2015}, {"title": "Introduction to reinforcement learning", "author": ["R.S. Sutton", "A.G. Barto"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 1998}, {"title": "Reinforcement Learning for Robots Using Neural Networks", "author": ["L. Lin"], "venue": "PhD thesis,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1993}, {"title": "Algorithms for partially observable Markov decision processes", "author": ["H.-T. Cheng"], "venue": "PhD thesis, University of British Columbia,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1988}, {"title": "Empirical evaluation methods for multiobjective reinforcement learning algorithms", "author": ["P. Vamplew", "R. Dazeley", "A. Berry", "E. Dekker", "R. Issabekov"], "venue": "Machine Learning,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2011}, {"title": "Learning all optimal policies with multiple criteria", "author": ["L. Barrett", "S. Narayanan"], "venue": "In ICML,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2008}, {"title": "Multi-objective reinforcement learning using sets of Pareto dominating policies", "author": ["K.V. Moffaert", "A. Now\u00e9"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2014}, {"title": "Model-based multi-objective reinforcement learning", "author": ["M.A. Wiering", "M. Withagen", "M.M. Drugan"], "venue": "ADPRL", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2014}, {"title": "The scalarized multi-objective multi-armed bandit problem: an empirical study of its exploration vs. exploitation tradeoff", "author": ["S.Q. Yahyaa", "M.M. Drugan", "B. Manderick"], "venue": "In IJCNN 2014: Proceedings of the 2014 International Joint Conference on Neural Networks,", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2014}, {"title": "A novel adaptive weight selection algorithm for multi-objective multi-agent reinforcement learning", "author": ["K. Van Moffaert", "T. Brys", "A. Chandra", "L. Esterle", "P.R. Lewis", "A. Now\u00e9"], "venue": "In IJCNN 2014: Proceedings of the 2013 International Joint Conference on Neural Networks,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Dynamic preferences in multi-criteria reinforcement learning", "author": ["S. Natarajan", "P. Tadepalli"], "venue": "In ICML,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2005}, {"title": "Evolutionary algorithms for solving multi-objective problems", "author": ["C.C. Coello", "G.B. Lamont", "D.A. Van Veldhuizen"], "venue": "Springer Science & Business Media,", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2007}, {"title": "Solving multi-objective reinforcement learning problems by EDA-RL - acquisition of various strategies", "author": ["H. Handa"], "venue": "ISDA", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2009}, {"title": "Pareto local policy search for MOMDP planning", "author": ["C. Kooijman", "M. de Waard", "M. Inja", "D.M. Roijers", "S. Whiteson"], "venue": "ESANN", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2015}, {"title": "Variational multi-objective coordination", "author": ["D.M. Roijers", "S. Whiteson", "A.T. Ihler", "F.A. Oliehoek"], "venue": "In MALIC 2015: NIPS Workshop on Learning, Inference and Control of Multi-Agent Systems,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2015}], "referenceMentions": [{"referenceID": 0, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 180, "endOffset": 185}, {"referenceID": 1, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 180, "endOffset": 185}, {"referenceID": 2, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 180, "endOffset": 185}, {"referenceID": 3, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 204, "endOffset": 207}, {"referenceID": 4, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 225, "endOffset": 228}, {"referenceID": 5, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 6, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 7, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 8, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 9, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 10, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 11, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 12, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 13, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 267, "endOffset": 273}, {"referenceID": 14, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 281, "endOffset": 289}, {"referenceID": 15, "context": "In recent years, advances in deep learning have been instrumental in solving a number of challenging reinforcement learning (RL) problems, including high-dimensional robot control [1\u20133], visual attention [4], solving riddles [5], the Atari learning environment (ALE) [6\u201314] and Go [15, 16].", "startOffset": 281, "endOffset": 289}, {"referenceID": 16, "context": "For example, an agent that may want to maximise the performance of a web application server, while minimising its power consumption [17].", "startOffset": 132, "endOffset": 136}, {"referenceID": 17, "context": "Such problems can be modelled as multi-objective Markov decision processes (MOMDPs), and solved with multi-objective reinforcement learning (MORL) [18].", "startOffset": 147, "endOffset": 151}, {"referenceID": 18, "context": "In this paper, we circumvent this issue by taking an outer loop approach [19] to multi-objective reinforcement learning, i.", "startOffset": 73, "endOffset": 77}, {"referenceID": 6, "context": "In order to enable the use of deep Q-Networks [7] for learning in MOMDPs, we build off the state-of-the-art optimistic linear support (OLS) framework [19, 20].", "startOffset": 46, "endOffset": 49}, {"referenceID": 18, "context": "In order to enable the use of deep Q-Networks [7] for learning in MOMDPs, we build off the state-of-the-art optimistic linear support (OLS) framework [19, 20].", "startOffset": 150, "endOffset": 158}, {"referenceID": 19, "context": "In order to enable the use of deep Q-Networks [7] for learning in MOMDPs, we build off the state-of-the-art optimistic linear support (OLS) framework [19, 20].", "startOffset": 150, "endOffset": 158}, {"referenceID": 20, "context": "the OLS framework solves a series of single-objective problems that become increasingly similar as the series progresses [21], which results in increasingly similar optimal value vectors.", "startOffset": 121, "endOffset": 125}, {"referenceID": 21, "context": "In a single-objective RL setting [22], an agent observes the current state st \u2208 S at each discrete time step t, chooses an action at \u2208 A according to a potentially stochastic policy \u03c0, observes a reward signal R(st, at) = rt \u2208 R, and transitions to a new state st+1.", "startOffset": 33, "endOffset": 37}, {"referenceID": 0, "context": "Its objective is to maximise an expectation over the discounted return, Rt = rt + \u03b3rt+1 + \u03b3rt+2 + \u00b7 \u00b7 \u00b7 , where rt is the reward received at time t and \u03b3 \u2208 [0, 1] is a discount factor.", "startOffset": 156, "endOffset": 162}, {"referenceID": 6, "context": "DeepQ-learning [7] uses neural networks parameterised by \u03b8 to represent Q(s, a; \u03b8).", "startOffset": 15, "endOffset": 18}, {"referenceID": 22, "context": "DQN uses experience replay [23]: during learning, the agent builds a dataset of episodic experiences and is then trained by sampling mini-batches of experiences.", "startOffset": 27, "endOffset": 31}, {"referenceID": 6, "context": "Experience replay is used in [7] to reduce variance by breaking correlation among the samples, whilst, it enables re-use of past experiences for learning.", "startOffset": 29, "endOffset": 32}, {"referenceID": 17, "context": "An MOMDP, is an MDP in which the reward function R(st, at) = rt \u2208 R describes a vector of n rewards, one for each objective [18].", "startOffset": 124, "endOffset": 128}, {"referenceID": 17, "context": "The corresponding coverage set is called the convex coverage set (CCS) [18].", "startOffset": 71, "endOffset": 75}, {"referenceID": 23, "context": "The corner weights are the weights at the corners of the convex upper surface [24], marked with crosses in the figure.", "startOffset": 78, "endOffset": 82}, {"referenceID": 20, "context": "Fortunately, we can exploit the following observation: the optimal value vectors (and thus optimal policies) for a scalarised MOMDP with a w and a w\u2032 that are close together, are typically close as well [21].", "startOffset": 203, "endOffset": 207}, {"referenceID": 19, "context": "These new corner weights and their estimated improvement are calculated using the newCornerWeights and estimateImprovement methods of OLS [20].", "startOffset": 138, "endOffset": 142}, {"referenceID": 6, "context": "For both the raw and image problems we follow the DQN setup of [7], employing experience replay and a target network to stabilise learning.", "startOffset": 63, "endOffset": 66}, {"referenceID": 21, "context": "MC is a variant of the famous mountain car problem introduced in [22].", "startOffset": 65, "endOffset": 69}, {"referenceID": 24, "context": "To test the performance of our algorithms on a problem with a larger CCS, we adapt the well-known deep sea treasure (DST) [25] benchmark for MORL.", "startOffset": 122, "endOffset": 126}, {"referenceID": 24, "context": "To be able to learn a CCS instead of a Pareto front, as it was in the original work [25], we have adapted the values of the treasures such that the value of the most efficient policy for reaching each treasure is in the CCS.", "startOffset": 84, "endOffset": 88}, {"referenceID": 17, "context": "Multi-objective reinforcement learning [18, 25] has recently seen a renewed interest.", "startOffset": 39, "endOffset": 47}, {"referenceID": 24, "context": "Multi-objective reinforcement learning [18, 25] has recently seen a renewed interest.", "startOffset": 39, "endOffset": 47}, {"referenceID": 25, "context": "Most algorithms in the literature [26\u201328] are however based on an inner loop approach, i.", "startOffset": 34, "endOffset": 41}, {"referenceID": 26, "context": "Most algorithms in the literature [26\u201328] are however based on an inner loop approach, i.", "startOffset": 34, "endOffset": 41}, {"referenceID": 27, "context": "Most algorithms in the literature [26\u201328] are however based on an inner loop approach, i.", "startOffset": 34, "endOffset": 41}, {"referenceID": 28, "context": "Other work does apply an outer loop approach but does not employ Deep RL [29\u201331].", "startOffset": 73, "endOffset": 80}, {"referenceID": 29, "context": "Other work does apply an outer loop approach but does not employ Deep RL [29\u201331].", "startOffset": 73, "endOffset": 80}, {"referenceID": 30, "context": "Other work does apply an outer loop approach but does not employ Deep RL [29\u201331].", "startOffset": 73, "endOffset": 80}, {"referenceID": 31, "context": "These are for example based on multi-objective evolutionary algorithms (MOEAs) [32, 33] or Pareto local search (PLS) [34].", "startOffset": 79, "endOffset": 87}, {"referenceID": 32, "context": "These are for example based on multi-objective evolutionary algorithms (MOEAs) [32, 33] or Pareto local search (PLS) [34].", "startOffset": 79, "endOffset": 87}, {"referenceID": 33, "context": "These are for example based on multi-objective evolutionary algorithms (MOEAs) [32, 33] or Pareto local search (PLS) [34].", "startOffset": 117, "endOffset": 121}, {"referenceID": 20, "context": "Notably, the OLSAR algorithm [21] does planning in multi-objective partially observable MDPs (POMDPs), and applies reuse to the alpha matrices that it makes use of to represent the multi-objective value function.", "startOffset": 29, "endOffset": 33}, {"referenceID": 34, "context": "Furthermore, the variational OLS (VOLS) algorithm [35], applies OLS to multiobjective coordination graphs and reuses reparameterisations of these graphs that are returned by the single-objective variational inference methods that VOLS uses as a subroutine.", "startOffset": 50, "endOffset": 54}], "year": 2016, "abstractText": "We propose Deep Optimistic Linear Support Learning (DOL) to solve highdimensional multi-objective decision problems where the relative importances of the objectives are not known a priori. Using features from the high-dimensional inputs, DOL computes the convex coverage set containing all potential optimal solutions of the convex combinations of the objectives. To our knowledge, this is the first time that deep reinforcement learning has succeeded in learning multiobjective policies. In addition, we provide a testbed with two experiments to be used as a benchmark for deep multi-objective reinforcement learning.", "creator": "LaTeX with hyperref package"}}}