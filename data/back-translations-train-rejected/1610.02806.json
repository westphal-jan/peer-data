{"id": "1610.02806", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder", "abstract": "We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks.", "histories": [["v1", "Mon, 10 Oct 2016 08:52:36 GMT  (375kb,D)", "http://arxiv.org/abs/1610.02806v1", "10 pages, 3 figures, COLING2016"]], "COMMENTS": "10 pages, 3 figures, COLING2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["yao zhou", "cong liu", "yan pan"], "accepted": false, "id": "1610.02806"}, "pdf": {"name": "1610.02806.pdf", "metadata": {"source": "CRF", "title": "Modelling Sentence Pairs with Tree-structured Attentive Encoder", "authors": ["Yao Zhou", "Cong Liu", "Yan Pan"], "emails": ["yoosan.zhou@gmail.com", "panyan5}@mail.sysu.edu.cn"], "sections": [{"heading": "1 Introduction", "text": "In fact, it is such that the majority of them will be able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to"}, {"heading": "2 Models", "text": "Let's start with a high-level discussion about our tree-structured attentive encoder. As shown in Figure 1, our goal is to evaluate this sentence pair (Sa, Sb). Our tree-structured attentive model consists of two components. In the first component, a sentence pair is fed to a Seq RNs that encodes each sentence and leads to a sentence pair. In the second component, the attentive TreeRNNs again encode a sentence that aims at the representation of the other sentence generated by the first component. Compared to the existing approaches to modeling sentence pairs, our attentive encoder takes into account not only the sentence itself, but also the other sentence in the pair. Finally, the two sentence vectors generated by the second component are fed into the multi-layer perceptron network to produce a distribution of possible values. These components are detailed in the following sections."}, {"heading": "2.1 Seq-RNNs", "text": "Considering an input sequence of arbitrary length, an RNN composer iteratively calculates a hidden state ht using the xt input vector and its previous hidden state ht \u2212 1. In this essay, the xt input vector is a word vector of the t-th word in a sentence. The hidden state ht can be used as a distributed representation of the token sequence ht \u2212 1 observed up to that time. Usually, the RNN transition function is ht = tanh (Wxt + Uht \u2212 1 + b) (1). We refer to the model that applies the RNN composer recursively to a sequence, such as the Seq RNNNs. Unfortunately, the default Seq RNNNs suffer from the problem that the gradients of the hidden states of earlier parts of the sequence replace the STNN illustrations."}, {"heading": "2.2 Standard Tree-RNNs", "text": "Compared to the standard RNN 2 (b), which calculates its hidden state from input to the current time step and the hidden state of the previous time step, the tree RNN composer is able to capture its hidden state from an input and the hidden states of the children. We now describe the Child-Sum TreeLSTM and Child-Sum Tree-GRU architectures, which are formed by applying the child-sum algorithm to LSTM and GRU. Child-Sum Tree-LSTM-LSTM is the implementation of Child-Sum Tree-LSTM the same as (Tai et al., 2015) We consider that a child-Sum Tree-LSTM component contains two parts: the external part and internal part. The external part consists of the inputs and outputs, and the internal part is the controller and memory of the composer."}, {"heading": "2.3 Attentive Tree-RNNs", "text": "The idea that we integrate attention into the standard Tree-RNN stems from: (1) there will be a semantic relevance between two sentences in the sentence-pair modeling tasks; (2) the effect of semantic relevance could be implemented in the process of constructing sentence representation by Tree-RNN, where each child should be assigned a different weighting; and (3) the attention mechanism is well suited for learning weights on a contextual collection in which a guided vector is present. (Soft Attention Layer In this work, the attention mechanism is implemented by a soft attention layer. In view of a collection of hidden states h1, h2,.. hn and an external vector s, which contains a weight layer for each hidden state as well as a weighted vector g via equations 4: mk = tanh (W) hk + U (m)."}, {"heading": "2.4 MLP", "text": "The Multilayer Perceptron Network (MLP) receives a pair of vectors generated by the sentence encoder to calculate a multinomial distribution over possible values. In two sentence representations hL and hR, we calculate their component-based product hL hR and their absolute difference. These characteristics are also used by Tai et al. (2015). Subsequently, we compress these characteristics into a low-dimensional vector hs from which the probability distribution p \u0432 can be calculated. Equations are: h \u00d7 = hL hR, h + = | hL \u2212 hR |, hs = \u03c3 (W (\u00d7) h \u00d7 + W (+) h + + b (h)), p \u0432\u043e\u043c = softmax (W (p) hs + b (p)), (5)"}, {"heading": "3 Experiments and Results", "text": "To make a meaningful comparison between the sequential models, tree-structured models, and attentive models, we present four baselines: (i) Seq-LSTMs, which learn two sentence representations through the sequential LSTMs; (ii) Seq-GRUs, like Seq-LSTMs, but using the GRU composer; (iii) Tree-LSTMs, which learn two sentence representations through the Dependency Tree-LSTMs; and (iv) Tree-GRUs, like Tree-LSTMs, but using the Child-Sum Tree-GRU composer. The two sentence representations are fed to the MLP to generate a probability distribution."}, {"heading": "3.1 Task 1: Semantic Similarity", "text": "The SICK dataset consists of 9927 sentence pairs with the division of 4500 training pairs, 500 development pairs and 4927 test pairs. Each sentence pair is assigned a similarity score of 1 to 5. A high score indicates that the sentence pair is highly related. All sentences are derived from existing image and video annotation datasets. The rating metrics are Pearson's r, Spearman's \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s.\" The rating metrics are \"s r, Spearman's\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s \"s\" s. \"Recall that the output of MLP\" (Section 2.4) is a probability distribution p. \"Our goal in this task is to predict a similarity score of two sentences."}, {"heading": "3.2 Task 2: Paraphrase Identification", "text": "N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N / N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N N"}, {"heading": "3.3 Task 3: True-False Question Selection", "text": "In this task, we use the AI2-8grade dataset compiled by (Baudis et al., 2016), taken from the AI2 Elementary School Science Questions published by the Allen Institute. Each set consists of a hypotheses set, which is processed by replacing the word in the question with an answer, and the evidence set extracted from a collection of CK12 textbooks. The number of sample pairs in training, development, and test set is 12689, 2483, and 11359, respectively. This set contains 626 words not found in glove vectors, most of which are entities and scientific jargon. The loss function is the same as the paraphrase identification, as this is also a binary classification task. We report on the accuracy of the development set and test set presented in Table 3 (right) dataset. As this set is incomplete, we can only compare the 2016 data set with our models (which are incomplete)."}, {"heading": "4 Quantitative Analysis", "text": "The reason for this is that the sentences of the SICK data set are image and video descriptions with a relatively simple sentence structure and fewer unusual words and named entities in the vocabulary. The second group gives us three examples of the MSRP test set. Considering that our model can make two assertions of fact but does not recognize the numbers (in group 2, line 3), we present the examples of the AI2-8 records in the last group. We can observe that our model is efficient in selecting the wrong questions, while our model is difficult to select the true answers unless the evidence of the question is very strong."}, {"heading": "5 Conclusion", "text": "In this paper, we have presented a way to incorporate attention into the Child-Sum Tree-LSTM and TreeGRU, which can be applied to the dependency tree. We evaluate the proposed models based on three sentence pair modeling tasks and achieve state-of-the-art results for two of them. Experimental results show that our attentive models are effective for modeling sentence pairs and can outperform all non-attentive counterparts. In the future, we will evaluate our models for the other sentence pair modeling tasks (such as RTE) and extend them to the seq2seq learning framework."}, {"heading": "Acknowledgements", "text": "This work was partially funded by the National Key Research and Development Program of China (2016YFB0201900), the National Science Foundation of China (grants 61472459, 61370021, U1401256, 61472453), Natural Science Foundation of Guangdong Province under grants S2013010011905."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["Kyunghyun Cho", "Yoshua Bengio"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Bahdanau et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Joint learning of sentence embeddings for relevance and entailment", "author": ["Baudis et al.2016] Petr Baudis", "Silvestr Stanko", "Jan Sedivy"], "venue": "arXiv preprint arXiv:1605.04655", "citeRegEx": "Baudis et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Baudis et al\\.", "year": 2016}, {"title": "A large annotated corpus for learning natural language inference", "author": ["Gabor Angeli", "Christopher Potts", "Christopher D Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Bowman et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Bowman et al\\.", "year": 2015}, {"title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "author": ["Chung et al.2014] Junyoung Chung", "\u00c7alar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio"], "venue": "Technical Report Arxiv report 1412.3555,", "citeRegEx": "Chung et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Chung et al\\.", "year": 2014}, {"title": "Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources", "author": ["Dolan et al.2004] Bill Dolan", "Chris Quirk", "Chris Brockett"], "venue": "In Proceedings of the 20th international conference on Computational Linguistics,", "citeRegEx": "Dolan et al\\.,? \\Q2004\\E", "shortCiteRegEx": "Dolan et al\\.", "year": 2004}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["Duchi et al.2011] John Duchi", "Elad Hazan", "Yoram Singer"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "Duchi et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Duchi et al\\.", "year": 2011}, {"title": "Multi-perspective sentence similarity modeling with convolutional neural networks", "author": ["He et al.2015] Hua He", "Kevin Gimpel", "Jimmy Lin"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Teaching machines to read and comprehend", "author": ["Tomas Kocisky", "Edward Grefenstette", "Lasse Espeholt", "Will Kay", "Mustafa Suleyman", "Phil Blunsom"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermann et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hermann et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "The vanishing gradient problem during learning recurrent neural nets and problem solutions", "author": ["Sepp Hochreiter"], "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems,", "citeRegEx": "Hochreiter.,? \\Q1998\\E", "shortCiteRegEx": "Hochreiter.", "year": 1998}, {"title": "Discriminative improvements to distributional sentence similarity", "author": ["Ji", "Eisenstein2013] Yangfeng Ji", "Jacob Eisenstein"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Ji et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ji et al\\.", "year": 2013}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),", "citeRegEx": "Kim.,? \\Q2014\\E", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Effective approaches to attention-based neural machine translation", "author": ["Hieu Pham", "Christopher D Manning"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Luong et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2015}, {"title": "Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. SemEval-2014", "author": ["Luisa Bentivogli", "Marco Baroni", "Raffaella Bernardi", "Stefano Menini", "Roberto Zamparelli"], "venue": null, "citeRegEx": "Marelli et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Marelli et al\\.", "year": 2014}, {"title": "Corpus-based and knowledge-based measures of text semantic similarity", "author": ["Courtney Corley", "Carlo Strapparava"], "venue": "In AAAI,", "citeRegEx": "Mihalcea et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Mihalcea et al\\.", "year": 2006}, {"title": "Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April", "author": ["Tom\u00e1\u0161 Mikolov"], "venue": null, "citeRegEx": "Mikolov.,? \\Q2012\\E", "shortCiteRegEx": "Mikolov.", "year": 2012}, {"title": "Glove: Global vectors for word representation", "author": ["Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Pennington et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Reasoning about entailment with neural attention", "author": ["Edward Grefenstette", "Karl Moritz Hermann", "Tom\u00e1\u0161 Ko\u010disk\u1ef3", "Phil Blunsom"], "venue": "In International Conference on Learning Representations", "citeRegEx": "Rockt\u00e4schel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Rockt\u00e4schel et al\\.", "year": 2016}, {"title": "A neural attention model for abstractive sentence summarization", "author": ["Sumit Chopra", "Jason Weston"], "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,", "citeRegEx": "Rush et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Rush et al\\.", "year": 2015}, {"title": "Neural responding machine for short-text conversation", "author": ["Shang et al.2015] Lifeng Shang", "Zhengdong Lu", "Hang Li"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Shang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Shang et al\\.", "year": 2015}, {"title": "An attentive neural architecture for fine-grained entity type classification", "author": ["Pontus Stenetorp", "Kentaro Inui", "Sebastian Riedel"], "venue": "In Proceedings of the 5th Workshop on Automated Knowledge Base Construction,", "citeRegEx": "Shimaoka et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Shimaoka et al\\.", "year": 2016}, {"title": "Dynamic pooling and unfolding recursive autoencoders for paraphrase detection", "author": ["Eric H Huang", "Jeffrey Pennin", "Christopher D Manning", "Andrew Y Ng"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Socher et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics, 2:207\u2013218", "author": ["Andrej Karpathy", "Quoc V Le", "Christopher D Manning", "Andrew Y Ng"], "venue": null, "citeRegEx": "Socher et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Socher et al\\.", "year": 2014}, {"title": "End-to-end memory networks", "author": ["arthur szlam", "Jason Weston", "Rob Fergus"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "Improved semantic representations from tree-structured long short-term memory networks", "author": ["Tai et al.2015] Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing,", "citeRegEx": "Tai et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tai et al\\.", "year": 2015}, {"title": "Abcnn: Attention-based convolutional neural network for modeling sentence pairs. arXiv preprint arXiv:1512.05193", "author": ["Yin et al.2015] Wenpeng Yin", "Hinrich Sch\u00fctze", "Bing Xiang", "Bowen Zhou"], "venue": null, "citeRegEx": "Yin et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yin et al\\.", "year": 2015}, {"title": "Ecnu: One stone two birds: Ensemble of heterogenous measures for semantic relatedness and textual entailment", "author": ["Zhao et al.2014] Jiang Zhao", "Tian Tian Zhu", "Man Lan"], "venue": "Proceedings of the SemEval,", "citeRegEx": "Zhao et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zhao et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 13, "context": "The applications include measuring the semantic relatedness of two sentences (Marelli et al., 2014), recognizing the textual entailment (Bowman et al.", "startOffset": 77, "endOffset": 99}, {"referenceID": 2, "context": ", 2014), recognizing the textual entailment (Bowman et al., 2015) between the premise and hypothesis sentences, paraphrase identification (He et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 6, "context": ", 2015) between the premise and hypothesis sentences, paraphrase identification (He et al., 2015), answer selection and query ranking (Yin et al.", "startOffset": 80, "endOffset": 97}, {"referenceID": 25, "context": ", 2015), answer selection and query ranking (Yin et al., 2015) etc.", "startOffset": 44, "endOffset": 62}, {"referenceID": 15, "context": "This compositional function takes a range of different forms, including (but not limited to) sequential recurrent neural networks (Seq-RNNs) (Mikolov, 2012), tree-structured recursive neural networks (TreeRNNs) (Socher et al.", "startOffset": 141, "endOffset": 156}, {"referenceID": 22, "context": "This compositional function takes a range of different forms, including (but not limited to) sequential recurrent neural networks (Seq-RNNs) (Mikolov, 2012), tree-structured recursive neural networks (TreeRNNs) (Socher et al., 2014; Tai et al., 2015) and convolutional neural networks (CNNs) (Kim, 2014).", "startOffset": 211, "endOffset": 250}, {"referenceID": 24, "context": "This compositional function takes a range of different forms, including (but not limited to) sequential recurrent neural networks (Seq-RNNs) (Mikolov, 2012), tree-structured recursive neural networks (TreeRNNs) (Socher et al., 2014; Tai et al., 2015) and convolutional neural networks (CNNs) (Kim, 2014).", "startOffset": 211, "endOffset": 250}, {"referenceID": 11, "context": ", 2015) and convolutional neural networks (CNNs) (Kim, 2014).", "startOffset": 49, "endOffset": 60}, {"referenceID": 0, "context": "We introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al.", "startOffset": 260, "endOffset": 303}, {"referenceID": 12, "context": "We introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al.", "startOffset": 260, "endOffset": 303}, {"referenceID": 18, "context": ", 2015), text summarization (Rush et al., 2015) to natural language conversation (Shang et al.", "startOffset": 28, "endOffset": 47}, {"referenceID": 19, "context": ", 2015) to natural language conversation (Shang et al., 2015) and other NLP tasks such as question answering (Sukhbaatar et al.", "startOffset": 41, "endOffset": 61}, {"referenceID": 23, "context": ", 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockt\u00e4schel et al.", "startOffset": 55, "endOffset": 102}, {"referenceID": 7, "context": ", 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockt\u00e4schel et al.", "startOffset": 55, "endOffset": 102}, {"referenceID": 17, "context": ", 2015), classification (Rockt\u00e4schel et al., 2016; Shimaoka et al., 2016).", "startOffset": 24, "endOffset": 73}, {"referenceID": 20, "context": ", 2015), classification (Rockt\u00e4schel et al., 2016; Shimaoka et al., 2016).", "startOffset": 24, "endOffset": 73}, {"referenceID": 0, "context": "We introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015) to natural language conversation (Shang et al., 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockt\u00e4schel et al., 2016; Shimaoka et al., 2016). In the machine translation, the attention mechanism is used to learn the alignments between source words and target words in the decoding phase. More generally, we consider that the motivation of attention mechanism is to allow the model to attend over a set of elements with the intention of attaching different emphases to each element. We argue that the attention mechanism used in a tree-structured model is different from a sequential model. Our idea is inspired by Rockt\u00e4schel et al. (2016) and Hermann et al.", "startOffset": 261, "endOffset": 1057}, {"referenceID": 0, "context": "We introduce an approach that combines recursive neural networks and recurrent neural networks with the attention mechanism, which has been widely used in the sequence to sequence learning (seq2seq) framework whose applications ranges from machine translation (Bahdanau et al., 2015; Luong et al., 2015), text summarization (Rush et al., 2015) to natural language conversation (Shang et al., 2015) and other NLP tasks such as question answering (Sukhbaatar et al., 2015; Hermann et al., 2015), classification (Rockt\u00e4schel et al., 2016; Shimaoka et al., 2016). In the machine translation, the attention mechanism is used to learn the alignments between source words and target words in the decoding phase. More generally, we consider that the motivation of attention mechanism is to allow the model to attend over a set of elements with the intention of attaching different emphases to each element. We argue that the attention mechanism used in a tree-structured model is different from a sequential model. Our idea is inspired by Rockt\u00e4schel et al. (2016) and Hermann et al. (2015). In this paper, we utilise the attention mechanism to select semantically more relevant child by the representation of one sentence learned by a Seq-RNNs, when constructing the head representation of the other sentence in the pair on a dependency tree.", "startOffset": 261, "endOffset": 1083}, {"referenceID": 9, "context": "Unfortunately, standard Seq-RNNs suffers from the problem that the gradients of the hidden states of earlier part of the sequence vanishes in long sequences (Hochreiter, 1998).", "startOffset": 157, "endOffset": 175}, {"referenceID": 3, "context": "Long Short-term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Chung et al., 2014) are two powerful and popular architectures that address this problem by introducing gates and memory.", "startOffset": 96, "endOffset": 116}, {"referenceID": 12, "context": "The implementations of standard LSTM and GRU in this paper are same as (Luong et al., 2015) and (Chung et", "startOffset": 71, "endOffset": 91}, {"referenceID": 24, "context": "In this paper, the implementation of Child-Sum Tree-LSTM is same as (Tai et al., 2015).", "startOffset": 68, "endOffset": 86}, {"referenceID": 24, "context": "These features are also used by Tai et al.(2015). We then compress these features into a low dimensional vector hs, which is used to compute the probability distribution p\u0302\u03b8.", "startOffset": 32, "endOffset": 49}, {"referenceID": 16, "context": "Config Value Config Value Word vectors Glove (Pennington et al., 2014) Dims of word vectors 300 OOV word vectors uniform(-0.", "startOffset": 45, "endOffset": 70}, {"referenceID": 5, "context": "5 Optim method Adagrad (Duchi et al., 2011) Num of epoch 10", "startOffset": 23, "endOffset": 43}, {"referenceID": 13, "context": "1 Task 1: Semantic Similarity First we conduct our semantic similarity experiment on the Sentences Involving Compositional Knowledge(SICK) dataset (Marelli et al., 2014)12.", "startOffset": 147, "endOffset": 169}, {"referenceID": 24, "context": "We take the same setup as (Tai et al., 2015) that computes a target distribution p as a function of prediction score y given by:", "startOffset": 26, "endOffset": 44}, {"referenceID": 26, "context": "ECNU (Zhao et al., 2014), the best result of SemEval 2014 submissions, achieves a 0.", "startOffset": 5, "endOffset": 24}, {"referenceID": 26, "context": "ECNU (Zhao et al., 2014), the best result of SemEval 2014 submissions, achieves a 0.8414 r score by a heavily feature-engineered approach. Kiros et al. (2015) presents an unsupervised approach to learn Dependency trees are parsed by the Stanford Parser package, http://nlp.", "startOffset": 6, "endOffset": 159}, {"referenceID": 26, "context": "Method r \u03c1 MSE ECNU (Zhao et al., 2014) 0.", "startOffset": 20, "endOffset": 39}, {"referenceID": 24, "context": "8414 - Dependency Tree-LSTMs (Tai et al., 2015) 0.", "startOffset": 29, "endOffset": 47}, {"referenceID": 6, "context": "2561 ConvNet (He et al., 2015) 0.", "startOffset": 13, "endOffset": 30}, {"referenceID": 24, "context": "We find a phenomenon also appeared in (Tai et al., 2015) that tree-structured models can outperform sequential counterparts.", "startOffset": 38, "endOffset": 56}, {"referenceID": 4, "context": "2 Task 2: Paraphrase Identification The next task we evaluate is paraphrase identification on the Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2004).", "startOffset": 142, "endOffset": 162}, {"referenceID": 5, "context": "He et al. (2015) show the effectiveness of convolutional nets with the similarity measurement layer for modelling sentence similarity.", "startOffset": 0, "endOffset": 17}, {"referenceID": 14, "context": "Method Acc(%) F1(%) Baseline (Mihalcea et al., 2006) 65.", "startOffset": 29, "endOffset": 52}, {"referenceID": 21, "context": "3 RAE (Socher et al., 2011) 76.", "startOffset": 6, "endOffset": 27}, {"referenceID": 25, "context": "0 ABCNN-3 (Yin et al., 2015) 78.", "startOffset": 10, "endOffset": 28}, {"referenceID": 1, "context": "7 Method Dev Acc(%) Test Acc(%) RNN (Baudis et al., 2016) 38.", "startOffset": 36, "endOffset": 57}, {"referenceID": 1, "context": "1 CNN (Baudis et al., 2016) 44.", "startOffset": 6, "endOffset": 27}, {"referenceID": 1, "context": "4 RNN-CNN (Baudis et al., 2016) 43.", "startOffset": 10, "endOffset": 31}, {"referenceID": 1, "context": "6 attn1511 (Baudis et al., 2016) 38.", "startOffset": 11, "endOffset": 32}, {"referenceID": 1, "context": "RNN (Baudis et al., 2016) 49.", "startOffset": 4, "endOffset": 25}, {"referenceID": 1, "context": "In this task, we use the AI2-8grade dataset built by (Baudis et al., 2016).", "startOffset": 53, "endOffset": 74}, {"referenceID": 1, "context": "Comparison to (Baudis et al., 2016), all of our models gain a significant improvement.", "startOffset": 14, "endOffset": 35}, {"referenceID": 1, "context": "Specially, our best result achieved by the Attentive Tree-LSTMs is higher than the best of (Baudis et al., 2016) by +28 percents.", "startOffset": 91, "endOffset": 112}, {"referenceID": 1, "context": "In this task, we use the AI2-8grade dataset built by (Baudis et al., 2016). This dataset is derived from the AI2 Elementary School Science Questions released by Allen Institute. Each sentence pair consists of a hypothesis sentence processed by substituting the wh-word in the question by answer and its evidence sentence extracted from a collection of CK12 textbooks. The number of sample pairs in the training, development, and test set are 12689, 2483 and 11359 respectively. This dataset contains 626 words not appearing in Glove vectors, most of which are named entities and scientific jargons. The loss function is the same as the paraphrase identification since this task is also a binary classification task. We reports the accuracy on development set and test set shown in Table 3 (right). Since this dataset is a fresh and uncompleted dataset, we only compare our models with Baudis et al. (2016) who have evaluated several models on it.", "startOffset": 54, "endOffset": 906}], "year": 2016, "abstractText": "We describe an attentive encoder that combines tree-structured recursive neural networks and sequential recurrent neural networks for modelling sentence pairs. Since existing attentive models exert attention on the sequential structure, we propose a way to incorporate attention into the tree topology. Specially, given a pair of sentences, our attentive encoder uses the representation of one sentence, which generated via an RNN, to guide the structural encoding of the other sentence on the dependency parse tree. We evaluate the proposed attentive encoder on three tasks: semantic similarity, paraphrase identification and true-false question selection. Experimental results show that our encoder outperforms all baselines and achieves state-of-the-art results on two tasks.", "creator": "LaTeX with hyperref package"}}}