{"id": "1610.02828", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Ranking academic institutions on potential paper acceptance in upcoming conferences", "abstract": "The crux of the problem in KDD Cup 2016 involves developing data mining techniques to rank research institutions based on publications. Rank importance of research institutions are derived from predictions on the number of full research papers that would potentially get accepted in upcoming top-tier conferences, utilizing public information on the web. This paper describes our solution to KDD Cup 2016. We used a two step approach in which we first identify full research papers corresponding to each conference of interest and then train two variants of exponential smoothing models to make predictions. Our solution achieves an overall score of 0.7508, while the winning submission scored 0.7656 in the overall results.", "histories": [["v1", "Mon, 10 Oct 2016 09:55:14 GMT  (35kb)", "http://arxiv.org/abs/1610.02828v1", "KDD 2016, KDD Cup 2016, Appeared in the KDD Cup Workshop 2016,this https URL"]], "COMMENTS": "KDD 2016, KDD Cup 2016, Appeared in the KDD Cup Workshop 2016,this https URL", "reviews": [], "SUBJECTS": "cs.AI cs.DL cs.LG", "authors": ["jobin wilson", "ram mohan", "muhammad arif", "santanu chaudhury", "brejesh lall"], "accepted": false, "id": "1610.02828"}, "pdf": {"name": "1610.02828.pdf", "metadata": {"source": "CRF", "title": "Ranking academic institutions on potential paper acceptance in upcoming conferences", "authors": ["Jobin Wilson", "Ram Mohan", "Muhammad Arif", "Santanu Chaudhury", "Brejesh Lall"], "emails": ["jobin.wilson@flytxt.com", "ram.mohan@flytxt.com", "muhammad.arif@flytxt.com", "santanuc@ee.iitd.ac.in", "brejesh.lall@ee.iitd.ac.in", "permissions@acm.org."], "sections": [{"heading": null, "text": "ar Xiv: 161 0.02 828v 1 [cs.A I] 1 0O ctCCS concepts \u2022 information systems \u2192 data mining; learning to rank; \u2022 computing methodologies \u2192 artificial intelligence; \u2022 mathematics of informatics \u2192 time series analysis; keywords web crawler; classification; exponential smoothing; ARIMA; cross-validation"}, {"heading": "1. INTRODUCTION", "text": "The goal of the KDD Cup 2016 was to develop data mining techniques that evaluate research institutions based on potential paper acceptance at upcoming top conferences, with public data sources available on the Internet. Specifically, the task was to evaluate institutions by predicting the number of their complete research papers that would receive permission to make digital or partial copies of this work for personal or class-wide use, provided that copies are not made for profit or commercial gain, and that copies bear this notice and full quotation on the first page. Copyrights for components of this work that belong to others than ACM must be honored. Abstraction by credit is permitted, or publication on servers or for distribution to lists requires prior specific permission and / or a fee."}, {"heading": "2. CONTEST EVALUATION", "text": "The competition was divided into three phases, with each phase spanning a calendar month. In accordance with each phase, predictions were made against several conferences associated with that phase. At the end of each phase, the organizers selected a specific conference from the group and evaluated the predictions by comparing them with the actual list of papers accepted by the selected conference. Overall, the results were calculated as a weighted sum of the results from each phase. Phase 1 had a weight of 20%, with Phase 2 and 3 each weighing 40%. The evaluation metric used to assess the quality of the predictions that corresponded to each phase is Normalized Discounted Cumulative Gain (NDCG), a commonly used metric for retrieving information [5]. NDCG values vary from 0.0 to 1.0, with the ideal ranking of the entities represented by a value of 1.0. In our problem, NDCG @ is calculated as a result of CG as soon as the actual GG = 1 is available."}, {"heading": "3. RELATED WORK", "text": "There are several studies that attempt to evaluate research institutions on the basis of a variety of performance indicators, such as articles in select high-level journals, major international awards, and high-profile researchers. [8] Although publishing an annual ranking of research institutions or universities has become a tradition for many academic institutions, newspapers, and journals, quantifying long-term scientific impacts is still considered a difficult problem. [15] The diversity of data considered, the methodology used, and subjective aspects involved in the ranking process in various ranking approaches pose further challenges to this problem. A popular ranking methodology called the Academic Ranking of World Universities (ARWU), proposed by Liu, Nian Cai, and Ying Cheng, uses a variety of indicators such as Nobel laureate alumni / staff and field medal winners, highly cited researchers in key areas, articles published in top journals such as Nature and Science, and Expanded, and the Index / Cita, as well as large-scale indicators by Science."}, {"heading": "4. DATA PREPARATION", "text": "We primarily used the MAG data available for download as part of the KDD Cup 2016 from [9]. We also used the ACM Digital Library (http: / / dl.acm.org /), which provides information on all papers published in each of the ACM conference proceedings, along with the paper title, providing information on the number of pages in a paper as well as the section to which it belongs in the proceedings. Microsoft Academic Knowledge API was also used in the initial phase for the validation purpose of the aggregated data. To extract and process data, we upload the MAG data set to a local Apache Spark [18] cluster consisting of 4 nodes, with each node having a configuration of 8 cores, 32 GB of RAM and 1 TB of hard disk. Spark Cluster runs on top of Hadoop with Yarn [14]. Spark SQL framework was used for data processing."}, {"heading": "5. METHODOLOGY", "text": "Specifically, the relevance value of belonging to a conference of interest for a given year is derived from the list of research papers accepted at the conference this year using the procedure described in Section 1. Thus, for each conference of interest, an ordered sequence of rel results is extracted that corresponds to a research institution, forming a univariate time series. We represent historical rel results of an institution in relation to a conference as a time series in which the first observation corresponds to the very first instance of the conference and the last observation corresponds to the most recent instance of the conference. In the event that an institution does not have accepted papers for a particular conference in a given year, rel results corresponding to that year would be set as zero in the corresponding time series. We use Box-Jenkins models [1] and two variants of exponential smoothing models to predict the rel result of each institution of interest in relation to specific problems."}, {"heading": "5.1 Time Series Models", "text": "5.1.1 ARIMA Model Autoregressive Integrated Average Forecast (ARIMA) In the case of ARIMA Modeling (ARIMA) predictive pattern is a generalization of an autoregressive moving average model (ARMA) and is applied when the original data does not appear stationary [1]. The initial differentiation step is applied to reduce non-stationarity. ARMA model is formed by a combination of two models, namely autoregressive (AR) and a constant. Equation 3 represents an AR (p) model in which the observed value and the random error observed in time is observed. Parameters of the model are represented by the values for i = 1, 2... p and the constant. Order of the model is denoted by p.yt = p."}, {"heading": "5.2 Causal Model", "text": "Causal modeling using Bayesian networks is an approach in which we attempt to determine how various factors, such as the number of authors in an institution within a particular Hindex range, the number of authors in an institution with a particular publication frequency range for a conference of interest, the weather at an institution, academic or industrial orientation, etc., affect the publication of institutions in an upcoming conference of interest.The causal model is presented as a directed acyclic graph with a common factorization of probability distribution according to the graph. Graphical structure and conditional dependencies are used to grasp the structure of our ranking problem. Learning from historical data is done in two steps. Causal structural learning is done with the ParallelPC package [3] and parameter learning is done with the bnlearn package [11], both of which are available as part of the RStudio."}, {"heading": "5.3 Network Model", "text": "We experimented with a simple network model by building a partner network based on co-authorship, with edge weights indicating the number of co-authorships between the institutions associated with that edge in relation to a particular conference of interest. Our assumption was that the prominence of a node within that network could potentially indicate its ranking, and the page rank algorithm was used to generate affiliation rankings from that chart. We observed that the cross-validation values for that model were low."}, {"heading": "5.4 Classification Models", "text": "To identify the complete research papers that were accepted in a particular conference in a given year, we started a manual process that proved to be very time-consuming. We downloaded from the conference website the list of accepted complete research papers that correspond to each year, and extracted the paper names using a Python script. We improved this process by automatically downloading the section names that are of interest to each instance of the conference (http: / / dl.acm.org /). To determine whether a paper was a complete research paper, we used manual rules based on the section names to which it belonged."}, {"heading": "6. OUR APPROACH", "text": "In this section, we present the specific procedures followed to generate our submissions for each phase using one or more of the previously described models, cross-validation was used to determine the quality of the predictions using the NDCG metric described in Section 2, and we implemented a cross-validation framework that used a prediction file, conference number, forecast year, and parameter N as input and generated the corresponding NDCG @ N value by comparing the predicted rankings with the actual rankings. To predict the rel value of an institution for the tenth instance of a conference, our models use the rel values of the institution from the first instance to the t-1st instance of that conference, and at each stage, an expected value of the NDCG @ 20 value was estimated that matches our predictions for an upcoming conference of interest by averaging the NCG values of the previous three instances of that conference."}, {"heading": "6.1 Phase 1", "text": "Our Phase 1 approach relies solely on MAG data to extract rel scores that correspond to research institutions for years to come. Since MAG does not distinguish complete research papers from others, we assume that rel scores extracted by taking into account all accepted work of a research institution in a conference would be a good indicator of their ranking in the upcoming instance of this conference. Phase 1 results refute our assumption that our actual score is significantly below our average cross-validation NDCG score. Our ranking process for this phase involves learning an individual ARIMA model that uses its annual rel score time series for a conference of interest, followed by predicting its rel score in the upcoming instance of this conference. We use the tsa.ARIMA model within the state model package [12]. The order of the model is specified as tuples (d, d, q, parameters, and the number of parameters are different in each case, p, q, and AR respectively."}, {"heading": "6.2 Phase 2", "text": "In Phase 2, complete research is identified by using the data from the ACM DL conference materials as an auxiliary source. We extract the name of the sections in the papers that include an essay, as well as the length of the essay from the conference papers using a parser. ICML papers that correspond to specific years were not available in ACM DL. We collect the essays that correspond to those years manually. As we analyze the essays, complete research is identified using simple rules based on section names. For example, sections with names such as \"Keynote,\" \"Panel,\" \"Industry Track,\" \"Poster,\" etc. are less likely to contain complete research. We construct filter rules by manually inserting such keywords into a dictionary, and then use substring matching to decide whether a section is likely to contain complete research. After extracting the list of complete research that corresponds to a cross-year of interest, we use the corresponding model to validate the corresponding phase 4, as described in the corresponding section."}, {"heading": "6.3 Phase 3", "text": "We note that phase 3 is much more complicated than the previous phases, as the best performing institutions are not consistent over time for the conferences of interest. For example, the number of unique compounds that have each reached the top 20 positions in the last 5 years (2011-2015) was 67. To determine complete research, we follow the procedure described in Section 5.4. Although manual rules are based solely on paper length, we could potentially provide an approximate list of complete research work, we use classification models and manually review their predictions to minimize false positives. Poor performance of ARIMA models is attributed to large variations in the rel-score data corresponding to this phase. During cross-validation, we observe that naive exponential smoothing and exponential smoothing models perform comparatively better. We perform a grating search to match the smoothing parameter for the exponential smoothing model to the ACM, which corresponds to the optimal results of the 734 pre-ferences for the conferences, the 70.M, the ACM, the ACM, and the ACM."}, {"heading": "7. DISCUSSION", "text": "We note that the KDD Cup 2016 was unique and challenging in several respects: the openness of the issue, the lack of basic truth-based data in advance, multiple possible solutions based on different facets of academic rankings such as quotation networks, co-authorship networks, similarities between conferences, conference venues, time information, etc., all contributed to the difficulty of the challenge. Getting the right data was one of the most important challenges we faced. Specifically, identifying full research from the set of all the accepted papers in a conference was not trivial. Our semi-automated approach to this task, considering ACM DL as a resource for auxiliary data, was actually a natural progression from a fully manual process to a semi-automatic mechanism involving a web crawler and multiple classification models, emphasizing the importance of the human-in-the-loop machine learning pipelines."}, {"heading": "8. CONCLUSION", "text": "In this article, we presented our solution for the KDD Cup 2016. We transformed the original ranking problem of the research institution into a time series prediction problem. Our solution involved a two-step process, in which we first identified complete research that corresponded to a conference of interest, and then, using a semi-automatic procedure, applied time series models such as ARIMA and exponential smoothing to generate predictions. We implemented a cross-validation framework to select the most appropriate model and its parameters according to each conference of interest. This framework proved to be an essential tool that helped us assess how well our models generalized."}, {"heading": "9. REFERENCES", "text": "[1] G. E. Box, G. M. Jenkins, G. C. Reinsel, and G. M.Ljung. Time series analysis: forecasting and control. John Wiley & Sons, 2015. [2] L. Breiman. Random forests. Machine learning, 45 (1): 5-32, 2001. [3] A. Hauser and P. Bu \ufffd hlmann. Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs. IEEE Intelligent Systems and their Applications, 13 (Aug): 2409-2464, 2012. [4] M. A. Hearst, S. T. Dumais, E. Osman, J. Platt, and B. Scholkopf. Support vector machines. IEEE Intelligent Systems and their Applications, 13 (4): 18-28, 1998. [5] K. Ja \ufffd rvelin and J. Keka \ufffd la \ufffd inen."}], "references": [{"title": "Time series analysis: forecasting and control", "author": ["G.E. Box", "G.M. Jenkins", "G.C. Reinsel", "G.M. Ljung"], "venue": "John Wiley & Sons,", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "Random forests", "author": ["L. Breiman"], "venue": "Machine learning, 45(1):5\u201332,", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2001}, {"title": "Characterization and greedy learning of interventional markov equivalence classes of directed acyclic graphs", "author": ["A. Hauser", "P. B\u00fchlmann"], "venue": "Journal of Machine Learning Research, 13(Aug):2409\u20132464,", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}, {"title": "Support vector machines", "author": ["M.A. Hearst", "S.T. Dumais", "E. Osman", "J. Platt", "B. Scholkopf"], "venue": "IEEE Intelligent Systems and their Applications, 13(4):18\u201328,", "citeRegEx": "4", "shortCiteRegEx": null, "year": 1998}, {"title": "Cumulated gain-based evaluation of ir techniques", "author": ["K. J\u00e4rvelin", "J. Kek\u00e4l\u00e4inen"], "venue": "ACM Transactions on Information Systems (TOIS), 20(4):422\u2013446,", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2002}, {"title": "Text categorization with support vector machines: Learning with many relevant features", "author": ["T. Joachims"], "venue": "European conference on machine learning, pages 137\u2013142. Springer,", "citeRegEx": "6", "shortCiteRegEx": null, "year": 1998}, {"title": "Time series forecasting using holt-winters exponential smoothing", "author": ["P.S. Kalekar"], "venue": "Kanwal Rekhi School of Information Technology, 4329008:1\u201313,", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2004}, {"title": "The academic ranking of world universities", "author": ["N.C. Liu", "Y. Cheng"], "venue": "Higher education in Europe, 30(2):127\u2013136,", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2005}, {"title": "Scikit-learn: Machine learning in python", "author": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2011}, {"title": "Learning bayesian networks with the bnlearn r package", "author": ["M. Scutari"], "venue": "arXiv preprint arXiv:0908.3817,", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2009}, {"title": "Statsmodels: Econometric and statistical modeling with python", "author": ["S. Seabold", "J. Perktold"], "venue": "Proceedings of the 9th Python in Science Conference, pages 57\u201361,", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2010}, {"title": "An overview of microsoft academic service (mas) and applications", "author": ["A. Sinha", "Z. Shen", "Y. Song", "H. Ma", "D. Eide", "B.-j. P. Hsu", "K. Wang"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2015}, {"title": "Apache hadoop yarn: Yet another resource negotiator", "author": ["V.K. Vavilapalli", "A.C. Murthy", "C. Douglas", "S. Agarwal", "M. Konar", "R. Evans", "T. Graves", "J. Lowe", "H. Shah", "S. Seth"], "venue": "In Proceedings of the 4th annual Symposium on Cloud Computing,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2013}, {"title": "Quantifying long-term scientific impact", "author": ["D. Wang", "C. Song", "A.-L. Barab\u00e1si"], "venue": "Science, 342(6154):127\u2013132,", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}, {"title": "Interactive machine learning: letting users build classifiers", "author": ["M. Ware", "E. Frank", "G. Holmes", "M. Hall", "I.H. Witten"], "venue": "International Journal of Human-Computer Studies, 55(3):281\u2013292,", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2001}, {"title": "Ranking universities based on performance evaluation by a hybrid mcdm model", "author": ["H.-Y. Wu", "J.-K. Chen", "I.-S. Chen", "H.-H. Zhuo"], "venue": "Measurement, 45(5):856\u2013880,", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2012}, {"title": "Spark: cluster computing with working sets", "author": ["M. Zaharia", "M. Chowdhury", "M.J. Franklin", "S. Shenker", "I. Stoica"], "venue": "HotCloud, 10:10\u201310,", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2010}], "referenceMentions": [{"referenceID": 11, "context": "A major public data source that we made use of was the Microsoft Academic Graph (MAG) [13], which includes information about publications, citations, authors, affiliations and venues.", "startOffset": 86, "endOffset": 90}, {"referenceID": 4, "context": "The evaluation metric employed to asses the quality of predictions corresponding to each phase is Normalized discounted cumulative gain (NDCG), a commonly used metric in information retrieval [5].", "startOffset": 192, "endOffset": 195}, {"referenceID": 7, "context": "There have been several studies which attempt to rank research institutions based on a variety of performance indicators such as published articles in selected top-tier journals, major international awards and highly cited researchers in prominent fields [8][17].", "startOffset": 255, "endOffset": 258}, {"referenceID": 15, "context": "There have been several studies which attempt to rank research institutions based on a variety of performance indicators such as published articles in selected top-tier journals, major international awards and highly cited researchers in prominent fields [8][17].", "startOffset": 258, "endOffset": 262}, {"referenceID": 13, "context": "ered a difficult problem [15].", "startOffset": 25, "endOffset": 29}, {"referenceID": 7, "context": "Cai, and Ying Cheng utilizes multifarious indicators such as alumni/staff as Nobel laureates and Fields Medalists, highly cited researchers in important fields, articles published in top journals such Nature and Science, and/or indexed by major citation indexes such as Science Citation Index-expanded, and per capita academic performance [8].", "startOffset": 339, "endOffset": 342}, {"referenceID": 15, "context": "(2012) proposed a hybrid multiplecriteria decision making (MCDM) model for weighing various performance evaluation indices as well as to rank research institutions [17].", "startOffset": 164, "endOffset": 168}, {"referenceID": 13, "context": "This metric provides a journal independent assessment of a paper\u2019s long term impact, and it also has a meaningful interpretation [15].", "startOffset": 129, "endOffset": 133}, {"referenceID": 16, "context": "For data extraction and processing, we downloaded the MAG dataset to a local Apache Spark[18] cluster consisting of 4 Nodes, with each node having a configuration of 8 cores, 32 GB RAM and 1TB hard disk.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "The Spark cluster was running on top of Hadoop with Yarn [14].", "startOffset": 57, "endOffset": 61}, {"referenceID": 0, "context": "We make use of Box-Jenkins models [1] as well as two variants of exponential smoothing models to forecast the rel score of each institution of interest with respect to specific conferences.", "startOffset": 34, "endOffset": 37}, {"referenceID": 0, "context": "Autoregressive Integrated Moving average (ARIMA)model is a generalization of an autoregressive moving average model (ARMA) and is applied when the original data appears nonstationary [1].", "startOffset": 183, "endOffset": 186}, {"referenceID": 0, "context": "Equation 3 represents an AR(p) model where yt is the observed value and \u01ebt is the random error observed at time t [1].", "startOffset": 114, "endOffset": 117}, {"referenceID": 0, "context": "ARMA(p,q) models [1] combine AR(p) and MA(q) models where p and q are the model orders corresponding to p autoregressive terms and q moving average terms, as represented in the Equation 5 yt = p \u2211", "startOffset": 17, "endOffset": 20}, {"referenceID": 6, "context": "The model assumes that the observed data fluctuates around a reasonably stable mean, and doesn\u2019t have a consistent pattern of growth or trend [7].", "startOffset": 142, "endOffset": 145}, {"referenceID": 2, "context": "Causal structure learning is performed using ParallelPC [3] package and parameter learning is performed using the bnlearn[11] package; both available as part of the RStudio.", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "Causal structure learning is performed using ParallelPC [3] package and parameter learning is performed using the bnlearn[11] package; both available as part of the RStudio.", "startOffset": 121, "endOffset": 125}, {"referenceID": 5, "context": "SVMs are based on Structural Risk Minimization principle by Vapnik(1995) from computational learning theory [6][4].", "startOffset": 108, "endOffset": 111}, {"referenceID": 3, "context": "SVMs are based on Structural Risk Minimization principle by Vapnik(1995) from computational learning theory [6][4].", "startOffset": 111, "endOffset": 114}, {"referenceID": 5, "context": "There have been several studies which argue that SVMs are well suited for text categorization tasks and show substantial performance gains compared with many popular models [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 8, "context": "classifier in scikit-learn package[10].", "startOffset": 34, "endOffset": 38}, {"referenceID": 1, "context": "Random Forests is a tree based ensemble learning method proposed by Leo Breiman [2].", "startOffset": 80, "endOffset": 83}, {"referenceID": 10, "context": "ARIMA model within the statsmodels package[12].", "startOffset": 42, "endOffset": 46}, {"referenceID": 14, "context": "considering ACM DL as an auxiliary data source was in fact a natural progression from a completely manual procedure to a semi-automated mechanism involving a web-crawler and multiple classification models, emphasizing the importance of human-in-the-loop machine learning pipelines [16].", "startOffset": 281, "endOffset": 285}], "year": 2016, "abstractText": "The crux of the problem in KDD Cup 2016 involves developing data mining techniques to rank research institutions based on publications. Rank importance of research institutions are derived from predictions on the number of full research papers that would potentially get accepted in upcoming top-tier conferences, utilizing public information on the web. This paper describes our solution to KDD Cup 2016. We used a two step approach in which we first identify full research papers corresponding to each conference of interest and then train two variants of exponential smoothing models to make predictions. Our solution achieves an overall score of 0.7508, while the winning submission scored 0.7656 in the overall results.", "creator": "LaTeX with hyperref package"}}}