{"id": "1610.02847", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Situational Awareness by Risk-Conscious Skills", "abstract": "Hierarchical Reinforcement Learning has been previously shown to speed up the convergence rate of RL planning algorithms as well as mitigate feature-based model misspecification (Mankowitz et. al. 2016a,b, Bacon 2015). To do so, it utilizes hierarchical abstractions, also known as skills -- a type of temporally extended action (Sutton et. al. 1999) to plan at a higher level, abstracting away from the lower-level details. We incorporate risk sensitivity, also referred to as Situational Awareness (SA), into hierarchical RL for the first time by defining and learning risk aware skills in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes with a theoretical convergence guarantee. We show in a RoboCup soccer domain that the learned risk aware skills exhibit complex human behaviors such as `time-wasting' in a soccer game. In addition, the learned risk aware skills are able to mitigate reward-based model misspecification.", "histories": [["v1", "Mon, 10 Oct 2016 11:01:32 GMT  (4088kb,D)", "http://arxiv.org/abs/1610.02847v1", null]], "reviews": [], "SUBJECTS": "cs.AI", "authors": ["daniel j mankowitz", "aviv tamar", "shie mannor"], "accepted": false, "id": "1610.02847"}, "pdf": {"name": "1610.02847.pdf", "metadata": {"source": "CRF", "title": "Situational Awareness by Risk-Conscious Skills", "authors": ["Daniel J. Mankowitz"], "emails": ["danielm@tx.technion.ac.il", "avivt@berkeley.edu", "shie@ee.technion.ac.il"], "sections": [{"heading": "1 Introduction", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "2 Background", "text": "(1999) A Semi-Markov Decision Process (SMDP) Sutton et al. (1999) A Semi-Markov Decision Process (1999) A Semi-Markov Decision Process (SMDP) Sutton et al. (1999) A Semi-Markov Decision Process (1999) A Semi-Markov Decision Process (1999) A Semi-Markov Decision Process (1999) A Semi-Markov Decision Process Process Process Process Process Process Process Process (1999) A Semi-Markov Decision Process Process Process Process Process (1999)."}, {"heading": "3 Probabilistic Goal SMDP (PG-SMDP)", "text": "In this work, we focus on solving problems where the agent must maximize his probability of success = \u03b2 = \u03b2-ability to solve a specific task in a limited time. A natural model for such problems is the PG-MDP model described above. However, we are interested in complex problems that require a certain hierarchical ability, and therefore propose to expand the PG-MDPs to include skills that lead to a PG-MDP model that leads to a PG-MDP model. We assume that we will be given a set of skills that we can use as skills."}, {"heading": "4 Risk-Aware Skill", "text": "We modify the typical definition of a skill to include a parameter, the so-called Risk Awareness Parameter (RAP), which is the parameter that controls the risk setting of Risk Awareness Skills (RAS). Definition 1. Risk Awareness Skill (RAS) is a time-extended action consisting of the 4-fold tolerance = < I, p (z), yw >, where I am the set of states from which the RAS can be initialized; \u043d\u043a is the parameterized intra-skill policy; p (z) is the probability of ending in the z-Z state; and yw-R is the Risk Awareness Parameter (RAP) regulated by the Risk Awareness Distribution (RAD); jw-Pw (\u00b7) is the parameterized intra-skill policy with the parameters w-Rm.In practice, the RAP can serve as the intra-skill policy (or the SAS parameter)."}, {"heading": "5 SARiCoS Algorithm", "text": "The algorithm \"Situational Awareness by Risk-Conscious Skills\" (SARiCoS) learns the parameters of a two-step policy for selecting skills defined as follows: \"Mic\u03b1,\" \"Developed Skills\" (SARiCoS) = \"MicroSkills\" (SARiCoS), \"MicroSkills\" (SARiCoS), \"MicroSkills\" (SARiCoS), \"MicroSkills\" (SARiCoS), \"MicroSkills\" (SAIS), \"MicroSkills\" (MicroSkills), \"MicroSkills\" (SAIS), \"MicroSkills\" (SAIS), \"MicroSkills\" (SAIS), \"MicroSkills\" (SAIS), \"MicroSkills\" (SIS), \"MicroSkills\" (SIS), \"(SIS),\" MicroSkills \"(SIS),\" (SIS), \"MicroSkills\" (SIS), \"(SIS),\" (SIS), \"(SIS,\" (SIS), \"(SIS),\" AIS, \"(SIS,\" (SIS), \"(SIS),\" (SAIS, \"(SAIS),\" (SAIS), \"(SAIS,\" (SAIS), \"(SAIS),\" (SAIS, \"(SAIS),\" (SAIS), \"(SAIS,\" (SAIS), \"(SAIS),\" (SAIS, \"(SAIS),\""}, {"heading": "5.1 Inter-skill policy and RAP Update Rules", "text": "We define the expected reward for adhering to a policy. (8) Let's merge the interqualification policy parameters and the continuous RAD parameters into a single vector. (8) Let's merge the interqualification policy parameters and the continuous RAD parameters into a single vector. (8) Let's take the derivatives of that goal and use the well-known probability trick Peters & Schaal (2008). (8) Let's take the results into a single vector. (8) Let's take the derivatives of that goal and use the well-known probability trick Peters & Schaal (2008)."}, {"heading": "6 Experiments", "text": "The experiments were conducted in the RoboCup 2D football simulation domain Akiyama & Nakashima (2014); a well-known benchmark for many AI challenges. In the experiments, we show the agent's ability to learn risk-taking skills (such as \"time-wasting\" skills in a football game), and therefore we show SA by maximizing the PG-SMDP target. In the RoboCup domain, we also show the agent's ability to leave local optimization strategies due to reward formulas and thus overcome reward model misspecifications. RoboCup Offense (RO): This domain 1 consists of two teams on a football field where the striker (the yellow agent) must score against a goalkeeper, as shown in Figure 3a. The striker has T = 150 time limits (length of the episode) to attempt and score."}, {"heading": "6.1 Situational Awareness by Risk-Conscious Skills", "text": "In this section we show that the learning of the internationality and the attack on the SARiCoS must be reduced by half in order to minimize the risks arising from the current situation. (1) The agent loses the game 0 \u2212 1) The agent wins the game 1 \u2212 0."}, {"heading": "6.2 Mitigating Reward-based Model Misspecified", "text": "We compared SARiCoS with the regular formulation Expected Return (ER), i.e. an implementation of the Actor-Critic Policy Gradient, which uses regular rewards at each step of the game to learn a game-defining policy.As shown in Figure 3d, the ER striker (light blue circle) does not learn to score goals because the algorithm quickly moves toward collecting positive dribbles from faraway rewards rD, farther and toward the ball rewards. Therefore, the ER agent is stuck in a local optima, which causes the agent to execute D until it3 http: / / www.collinsdictionary.com / dictionary / american / run-out-the-clock 4 https: / / youtu.be / xA-8rWJ4a7Itigings, which causes the agent to execute D until it3 is http: / / www.collinsdictiony.com / run-the-clock / american-run-the-4-out."}, {"heading": "7 Discussion", "text": "We find it interesting that an agent can learn complex human behavior simply by maximizing a risk-sensitive target. To this end, we have introduced the Risk-Aware Skills (SARiCoS) algorithm - a kind of parameterized option Sutton et al. (1999) with an additional risk-aware parameter (RAP). We have shown that this algorithm converges to a locally optimal solution to problems. We also show that SARiCoS can generate situational awareness (e.g. \"time-wasting\" policies in risk-aware skills in a time-dependent RoboCup football scenario."}, {"heading": "Acknowledgements", "text": "The research leading to these results was funded by the European Research Council under the Seventh Framework Programme of the European Union (FP / 2007-2013) / ERC Funding Agreement No 306638."}, {"heading": "A SARiCoS Supplementary Material", "text": "We define the expected reward for following a policy (K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K = K ="}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Hierarchical Reinforcement Learning has been previously shown to speed up the convergence rate of RL planning algorithms as well as mitigate feature-based model misspecification Mankowitz et al. (2016a,b); Bacon & Precup (2015). To do so, it utilizes hierarchical abstractions, also known as skills \u2013 a type of temporally extended action Sutton et al. (1999) to plan at a higher level, abstracting away from the lower-level details. We incorporate risk sensitivity, also referred to as Situational Awareness (SA) , into hierarchical RL for the first time by defining and learning risk aware skills in a Probabilistic Goal Semi-Markov Decision Process (PG-SMDP). This is achieved using our novel Situational Awareness by Risk-Conscious Skills (SARiCoS) algorithm which comes with a theoretical convergence guarantee. We show in a RoboCup soccer domain that the learned risk aware skills exhibit complex human behaviors such as \u2018time-wasting\u2019 in a soccer game. In addition, the learned risk aware skills are able to mitigate reward-based model misspecification.", "creator": "LaTeX with hyperref package"}}}