{"id": "1610.02891", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Personalizing a Dialogue System with Transfer Reinforcement Learning", "abstract": "It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users' data as a source domain and an individual user's data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose \"PETAL\"(PErsonalized Task diALogue), a transfer-learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.", "histories": [["v1", "Mon, 10 Oct 2016 12:51:05 GMT  (139kb)", "http://arxiv.org/abs/1610.02891v1", null], ["v2", "Mon, 17 Oct 2016 14:08:42 GMT  (140kb)", "http://arxiv.org/abs/1610.02891v2", null], ["v3", "Fri, 26 May 2017 14:05:07 GMT  (91kb,D)", "http://arxiv.org/abs/1610.02891v3", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.LG", "authors": ["kaixiang mo", "shuangyin li", "yu zhang", "jiajun li", "qiang yang"], "accepted": false, "id": "1610.02891"}, "pdf": {"name": "1610.02891.pdf", "metadata": {"source": "CRF", "title": "Personalizing a Dialogue System with Transfer Learning", "authors": ["Kaixiang Mo", "Shuangyin Li", "Yu Zhang", "Jiajun Li", "Qiang Yang"], "emails": ["qyang}@cse.ust.hk", "jiajun.li@alumni.ust.hk"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.02 891v 1 [cs.A I] 1 0O ct2 01"}, {"heading": "Introduction", "text": "In fact, most people are able to move to another world in which they are in the position in which they find themselves."}, {"heading": "Related Works", "text": "Personalized dialog systems could be categorized into rule-based dialog systems and learning-based dialog systems. Thompson et al. (Thompson et al. 2004) proposed an interactive system, the Adaptive Place Advisor System, in which users could select a location through an interactive conversation process, but the system could learn user preferences and use them to improve future conversations. Kim et al. (Kim et al. 2014; Bang et al. 2015) proposed a personalization framework for dialogue systems, in which user-related facts (triples) could be extracted and used to generate responses by adding user-related facts to the templates. Unlike rule-based systems, learning-based personalized dialog systems can learn states and actions from training data without requiring explicit rules. Casanueva et al. (Casanueva et al. 2015) suggested creating a personalized dialog system for a target speaker with similar data."}, {"heading": "Notations", "text": "In this essay, matrices are denoted in bold uppercase letters, row vectors in bold lowercase letters, scalars in lowercase letters. the text in the dialogues denoted in curlicue is represented by the adoption of word bags. Each of the involved word bag representations is a vector, each element of which has a binary value. v is the size of the dialog vocabulary. d is the dimension of word bags. M is a state projection matrix that maps word bag representations to faith vectors. O is an expression of the user in the dialog vocabulary representation, and o = OM is the faith state vector of one of the user's expressions. A is the response of an agent in the dictionary representation, and a is the faith state vector of one of the faith vectors."}, {"heading": "Problem Settings", "text": "Our goal is to collect a series of decisions on the final order of a target user. The j-th choice we want to collect is referred to as Cj, and the exact choice in Cj is indicated by cj. For example, latte is a choice c1 in a coffee type C1. To get the information needed to complete the order, we must ask the user a question in each round to guide the user to complete the order. We assume that all possible decisions can be detected using keyword matching methods. Inputs for this problem include 1. Dialog data {{{Ousi, A us i} T i = 0} from source customers etc. 2. Dialog history of the target user on or before the current time step i, {Oi, {Ok, Ak} i \u2212 1 k = 0}.The expected output is 1. An answer Auti for the target user at the time i.In order to solve the problem, we could choose a political action based on the current course of each step, starting from i to the current one."}, {"heading": "PETAL: A Framework for Personalized Dialogue Management", "text": "In this section, we present our transfer learning framework for personalized dialogue management. In this article, we use PETAL to describe both the proposed framework and the proposed algorithm."}, {"heading": "The Framework", "text": "The Q function is defined as the expected cumulative reward according to the policy. In order to build a personalized dialog system for the target user, we need to find a personalized Q function Q\u03c0ut for that user. However, our training data {{Outi, A ut i} T i} for the target user is very limited, and we can hardly estimate the personalized Q function Q\u03c0ut with the limited data. To model Q\u03c0ut, we need to transfer shared dialog knowledge from the source domain that has the dialog data of many other users."}, {"heading": "Personalized Dialogue Management", "text": "In this section we present the parametric form for bi = f (Hi | M = 1), Q = 1 (Hi, Ai = 1) and Qp (Hi, Ai = 1). All expressions and answers are projected into state vectors with a state projection matrix M, where M is initialized with the word2vec and updated in the learning process. (Hi, M) The memory factor is the memory factor by which the historical status vectors are discounted in each time step. The state projection matrix M is the parameter we want to learn. (Hi, M) The dialogue history, Hi = {Ok, i = 0} i, to a faith state vector. The corresponding belief set vector bi isbi = [ohi \u2212 1, oi \u2212 2, ai \u2212 1] in which the variables are defined as ohi = 0."}, {"heading": "Reward", "text": "The reward consists of the general reward and the personal reward, and the total reward is the sum of the general reward and the personal reward. The general and personal rewards can be defined as follows: 1. Personal rewards of positive values are obtained if the user confirms the agent's suggestion. This is related to the user's personal information. 4. General rewards of negative values are obtained by the agent when the dialogue has more phrases or the user refuses to pay if the system does not generate logical answers. Parameter learning We use the value attribution method (Bellman 1957) to learn both the general and the personal Q function. There are a total of four sets of parameters that need to be learned. The first parameter is the state projection matrix M, which is responsible for mapping the dialogue."}, {"heading": "Loss Function", "text": "When dealing with data from the real world, the training set consists of (Hi, Ai, ri), which records optimal human actions, and therefore the loss function is defined as follows: L (\u044b) = E [(ri + \u03bbQ (Hi + 1, Ai + 1 | \u044b) \u2212 Q (Hi, Ai | \u043a) 2]. In practical training with the user simulator, the loss function is defined as follows: L (\u044b) = E [(ri + maxA \u2032 i + 1\u03bbQ (Hi + 1, A \u2032 i + 1 | \u044b) \u2212 Q (Hi, Ai | \u043a) 2], where ri is the reward achieved in the time stage i, Oi + 1 is the reaction of the user at the time i + 1, and Hi + 1 is the update dialog at the time step i + 1. Note that the source domain users are separated from the target domain users so that the source data cannot be applied in the \"source domain.\""}, {"heading": "Complexity", "text": "The detailed algorithm is represented in algorithm 1. the number of parameters in our model is about d2 + dv, where v is the total vocabulary size and d is the dimension of the state vector. In our experiment, where v = 1, 500 and d = 50 is the number of general parameters 85k, the number of personal parameters is below 100 for each user, so that personalized parameters can be learned with a few data in the target domain. On the real world dataset with 2,185 complete dialogs, each epoch needs 1.5 hours on a server with 20 CPUs and the proposed model converges in about 3 epochs. Algorithm 1 PETAL: Transfer Learning with personalized POMDP 1: Input: Ds, Dt 2: Output: {M, w, wp {pu}}} {s: D = 1, w, w: 1, w i i i i i i i, wp, wp, wp, wp, wp, wp, wp, wp, wp, wp, wp, wp, wp, wp, wp, wp}} s: D, D: i, Di, Di: Di, Di: Di: i, D: i, D: i, D i: i, D i, D i: i, D i: i, D i, D i: i, wi i i i i i i i i i i i i i i i i, wi, D i i i i, wi i, wi, wi, wi, wi, D: i, D: i, D: i, D i, D i i i i i, D i i i, i i i i, i i i, i i, i i i i i, wi, wi, D i, i, i, i i, i, i, i i, i, i i i, i, i, i i i i, i i, i, i, i i i i i i, wp, wp, D: i, D: i: i, D: i, D: i, D: i, D: i: i, D: i, D: i"}, {"heading": "Experiments", "text": "In this section, we will experimentally test the effectiveness of the personalized POMDP model by performing experiments on a real dataset and a simulation dataset."}, {"heading": "Baselines", "text": "We compare our personalized model, referred to as \"PETAL,\" with six basic algorithms listed as follows: 1. NoneTL: Dialog system is trained only with data from target users. 2. Sim (Casanueva et al. 2015): Dialog system is trained with data from both the target user and the most similar user in the source domain.3. Bandit (Genevay and Laroche 2016): For each target user, we will use a bandit algorithm to find the most useful source user.4. PriorSim (Gas ic et al. 2013): For each target user, we will use the policies of the most similar user in the source domain.5. PriorAll (Gas ic et al. 2013): For each target user, we will use the dialogue policy that is prioritized for all users in the source domain.6. All: Guidelines are trained on all source user data."}, {"heading": "Experiments on Real-World Data", "text": "In this section, we evaluate our model based on a real data set. This real data set contains 2,185 coffee dialogs from 72 users collected by an O2O coffee ordering service between July 2015 and April 2016. We select 52 users with more than 23 dialogs as the source domain. Each of the remaining 20 users is used separately as the target domain. There are 1,859 coffee dialogs in the source domain and 329 coffee dialogs in the target domain. 221 dialogs in the target domain are used as training sets and the remaining 108 dialogs are used as test sets. Statistics of this data set are shown in Table 2.Evaluation metrics for each round of the test call, Hi, Ai, the model ranks the basic truth answer Ai among 10 randomly selected agent answers. The label assigned to Ai is 1 and those for randomly selected agent responses are 0."}, {"heading": "Experiments on Simulation Data", "text": "In this section we compare our model with the models of the simulation. Settings We have 11 simulated users in the source domain, where 10 users have their own special preferences, while the rest have no preferences. The target domain has 5 users who have different preferences from users in the source domain. A simulator is designed based on the real data sets used in the previous section. The simulator will give a specific order with the probability that the simulator will order one coffee at a time."}], "references": [{"title": "In Proceedings of International Conference on Big Data and Smart Computing", "author": ["Jeesoo Bang", "Hyungjong Noh", "Yonghee Kim", "Gary Geunbae Lee. Example-based chat-oriented dialogue system with personalized long-term memory"], "venue": "pages 238\u2013243,", "citeRegEx": "Bang et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Technical report", "author": ["Richard Bellman. A Markovian decision process"], "venue": "DTIC Document,", "citeRegEx": "Bellman 1957", "shortCiteRegEx": null, "year": 1957}, {"title": "In Proceedings of COMPSTAT\u20192010", "author": ["L\u00e9on Bottou. Large-scale machine learning with stochastic gradient descent"], "venue": "pages 177\u2013186. Springer,", "citeRegEx": "Bottou 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Knowledge transfer between speakers for personalised dialogue management", "author": ["Inigo Casanueva", "Thomas Hain", "Heidi Christensen", "Ricard Marxer", "Phil Green"], "venue": "Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Casanueva et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "deltaBLEU: A discriminative metric for generation tasks with intrinsically diverse targets", "author": ["Michel Galley", "Chris Brockett", "Alessandro Sordoni", "Yangfeng Ji", "Michael Auli", "Chris Quirk", "Margaret Mitchell", "Jianfeng Gao", "Bill Dolan"], "venue": "arXiv preprint arXiv:1506.06863,", "citeRegEx": "Galley et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "POMDP-based dialogue manager adaptation to extended domains", "author": ["Ga\u0161ic"], "venue": "In Proceedings of the 14th Annual Meeting of the Special Interest Group on Discourse and Dialogue,", "citeRegEx": "Ga\u0161ic,? \\Q2013\\E", "shortCiteRegEx": "Ga\u0161ic", "year": 2013}, {"title": "Incremental on-line adaptation of POMDP-based dialogue managers to extended domains", "author": ["Gasic"], "venue": "In Proceedings of the 15th Annual Conference of the International Speech", "citeRegEx": "Gasic,? \\Q2014\\E", "shortCiteRegEx": "Gasic", "year": 2014}, {"title": "In Proceedings of the 2016 International Conference on Autonomous Agents & Multiagent Systems", "author": ["Aude Genevay", "Romain Laroche. Transfer learning for user adaptation in spoken dialogue systems"], "venue": "pages 975\u2013983,", "citeRegEx": "Genevay and Laroche 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Acquisition and use of long-term memory for personalized dialog systems", "author": ["Kim"], "venue": "In Proceedings of International Workshop on Multimodal Analyses Enabling Artificial Agents in Human-Machine Interaction,", "citeRegEx": "Kim,? \\Q2014\\E", "shortCiteRegEx": "Kim", "year": 2014}, {"title": "In Proceedings of IEEE Workshop on Automatic Speech Recognition and Understanding", "author": ["Esther Levin", "Roberto Pieraccini", "Wieland Eckert. Learning dialogue strategies within the Markov decision process framework"], "venue": "pages 72\u201379,", "citeRegEx": "Levin et al. 1997", "shortCiteRegEx": null, "year": 1997}, {"title": "Deep reinforcement learning for dialogue generation", "author": ["Jiwei Li", "Will Monroe", "Alan Ritter", "Dan Jurafsky"], "venue": "arXiv preprint arXiv:1606.01541,", "citeRegEx": "Li et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation", "author": ["Lili Mou", "Yiping Song", "Rui Yan", "Ge Li", "Lu Zhang", "Zhi Jin"], "venue": "arXiv preprint arXiv:1607.00970,", "citeRegEx": "Mou et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "IEEE Transactions on Knowledge and Data Engineering", "author": ["Sinno Jialin Pan", "Qiang Yang. A survey on transfer learning"], "venue": "22(10):1345\u20131359,", "citeRegEx": "Pan and Yang 2010", "shortCiteRegEx": null, "year": 2010}, {"title": "In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing", "author": ["Alan Ritter", "Colin Cherry", "William B Dolan. Data-driven response generation in social media"], "venue": "pages 583\u2013593,", "citeRegEx": "Ritter et al. 2011", "shortCiteRegEx": null, "year": 2011}, {"title": "Hierarchical neural network generative models for movie dialogues", "author": ["Iulian V Serban", "Alessandro Sordoni", "Yoshua Bengio", "Aaron Courville", "Joelle Pineau"], "venue": "arXiv preprint arXiv:1507.04808,", "citeRegEx": "Serban et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Multi-transfer: Transfer learning with multiple views and multiple sources", "author": ["Ben Tan", "Erheng Zhong", "Evan Wei Xiang", "Qiang Yang"], "venue": "Statistical Analysis and Data Mining, 7(4):282\u2013293,", "citeRegEx": "Tan et al. 2014", "shortCiteRegEx": null, "year": 2014}, {"title": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "author": ["Ben Tan", "Yangqiu Song", "Erheng Zhong", "Qiang Yang. Transitive transfer learning"], "venue": "pages 1155\u20131164,", "citeRegEx": "Tan et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research, 10:1633\u20131685,", "citeRegEx": "Taylor and Stone 2009", "shortCiteRegEx": null, "year": 2009}, {"title": "Journal of Artificial Intelligence Research", "author": ["Cynthia A Thompson", "Mehmet H Goker", "Pat Langley. A personalized system for conversational recommendations"], "venue": "21:393\u2013 428,", "citeRegEx": "Thompson et al. 2004", "shortCiteRegEx": null, "year": 2004}, {"title": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "author": ["Ying Wei", "Yu Zheng", "Qiang Yang. Transfer knowledge between cities"], "venue": "pages 1905\u20131914,", "citeRegEx": "Wei et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Semantically conditioned lstm-based natural language generation for spoken dialogue systems", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Pei-Hao Su", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1508.01745,", "citeRegEx": "Wen et al. 2015", "shortCiteRegEx": null, "year": 2015}, {"title": "A network-based end-to-end trainable taskoriented dialogue system", "author": ["Tsung-Hsien Wen", "Milica Gasic", "Nikola Mrksic", "Lina M Rojas-Barahona", "Pei-Hao Su", "Stefan Ultes", "David Vandyke", "Steve Young"], "venue": "arXiv preprint arXiv:1604.04562,", "citeRegEx": "Wen et al. 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "End-to-end LSTM-based dialog control optimized with supervised and reinforcement learning", "author": ["Jason D Williams", "Geoffrey Zweig"], "venue": "arXiv preprint arXiv:1606.01269,", "citeRegEx": "Williams and Zweig 2016", "shortCiteRegEx": null, "year": 2016}, {"title": "POMDP-based statistical spoken dialog systems: A review", "author": ["Steve Young", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Jason D Williams"], "venue": "Proceedings of the IEEE, 101(5):1160\u20131179,", "citeRegEx": "Young et al. 2013", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [], "year": 2016, "abstractText": "It is difficult to train a personalized task-oriented dialogue system because the data collected from each individual is often insufficient. Personalized dialogue systems trained on a small dataset can overfit and make it difficult to adapt to different user needs. One way to solve this problem is to consider a collection of multiple users\u2019 data as a source domain and an individual user\u2019s data as a target domain, and to perform a transfer learning from the source to the target domain. By following this idea, we propose the \u201cPETAL\u201d (PErsonalized Task-oriented diALogue), a transfer learning framework based on POMDP to learn a personalized dialogue system. The system first learns common dialogue knowledge from the source domain and then adapts this knowledge to the target user. This framework can avoid the negative transfer problem by considering differences between source and target users. The policy in the personalized POMDP can learn to choose different actions appropriately for different users. Experimental results on a real-world coffee-shopping data and simulation data show that our personalized dialogue system can choose different optimal actions for different users, and thus effectively improve the dialogue quality under the personalized setting.", "creator": "LaTeX with hyperref package"}}}