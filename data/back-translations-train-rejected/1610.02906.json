{"id": "1610.02906", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "A General Framework for Content-enhanced Network Representation Learning", "abstract": "This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world net- works with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning.", "histories": [["v1", "Mon, 10 Oct 2016 13:27:01 GMT  (714kb,D)", "http://arxiv.org/abs/1610.02906v1", null], ["v2", "Tue, 11 Oct 2016 02:06:55 GMT  (513kb,D)", "http://arxiv.org/abs/1610.02906v2", null], ["v3", "Mon, 17 Oct 2016 13:55:02 GMT  (513kb,D)", "http://arxiv.org/abs/1610.02906v3", null]], "reviews": [], "SUBJECTS": "cs.SI cs.CL cs.LG", "authors": ["xiaofei sun", "jiang guo", "xiao ding", "ting liu"], "accepted": false, "id": "1610.02906"}, "pdf": {"name": "1610.02906.pdf", "metadata": {"source": "CRF", "title": "A General Framework for Content-enhanced Network Representation Learning", "authors": ["Xiaofei Sun", "Jiang Guo", "Xiao Ding", "Ting Liu"], "emails": ["tliu}@ir.hit.edu.cn"], "sections": [{"heading": "Introduction", "text": "In fact, it is in such a way that we see ourselves in a position to be in a position to be in, and that we will be able to put ourselves in a position to be in a different world, in which we are in a position to be in, in which we are in a position to be in, in which we are in a world, in which we are in a world, in which we are in, in which we are in, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live, we live in which we live in which we live in which we live, in which we live, in which we live in which we live in which we live, in which we live in which we live in which we live in which we live,"}, {"heading": "Related Work", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Text Embedding", "text": "In order to obtain text embedding (e.g. sentence, paragraph), a simple and intuitive approach would be averaging the embedding of each word in the text (Mitchell and Lapata 2010; Ferrone and Zanzotto 2013; Iyyer et al. 2015). More sophisticated models have been developed to use the internal structure of sentences or documents to support composition. Thus, Socher et al. (2013) and Socher et al. (2014) use recursive neural networks via parse trees to obtain sentence representations. To reduce dependence on syntatic parsing, Convolutionary Neural Networks (CNN) (Blunsom, Grefenstette and Kalchbrenner 2014; Johnson and Zhang 2015) are used, which use simple hierarchical structures from bottom up for composition. Another alternative model is the LSTM-based recurrent neural network (Kiros et al. 2015), a variant of RNN that uses long-term cells for the capture of long-term cells."}, {"heading": "Network Embedding", "text": "Some previous work focuses on characteristic vectors and the leading eigenvectors are considered network representations, e.g., MDS (Borg and Groenen 2005), IsoMap (Tenenbaum, De Silva and Langford 2000), LLE (Roweis and Saul 2000), and Laplacian eigenmaps (Belkin and Niyogi 2001). Recent advances include, however, DeepWalk (Perozzi, AlRfou and Skiena 2014), Vertex embedding with the Skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, Walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multi-scale representation learning, node2vec (Grover and Leskovec 2016) Qepang network various networking strategies (Walklet, Walklet and Network)."}, {"heading": "Problem Definition", "text": "Definition 1. (Network) G = (V, E, C) is intended to denote a network in which V represents the set of nodes representing the nodes of the network; E (V \u00b7 V) the set of edges representing the relationships between the nodes; and C the content of the nodes. C = {(v, doc) | v \u00b2 V; doc = {Si}, in which Si denotes the i-th set of doc and consists of word strings < w1, w2,..., wn >. Without loss of universality, we assume that the structure of the network is a directed graphic.1 Definition 2. (Network Embedding) In view of a network designated as G = (V, E, C), the goal of network embedding is to create a low-dimensional real vector representation ev \u00b2 V \u00b2, where d | V |. Let it penetrate = (e1, e.2), which is the network type that can be viewed as representative of a low-dimensional representation ev \u00b2."}, {"heading": "Method", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "General Framework", "text": "In order to maintain the structural information of a network, we describe a general framework that minimizes the following objective: Lg = \u2211 (u, v) * SPlog p (u, v; \u03b8) * SNlog (1 \u2212 p (u, v; \u03b8) * (1) where SP is the set of positive vertex pairs and SN is the set of negative pairs. (Deepwalk, walklet, node2vec), SP is the set of adjacent vertex pairs in the routes generated by random walking, and SN is the union of all negative sampling sets by replacing each undirected edge with two opposing edges. (Deepwalk, walklet, node2vec), SP is the set of adjacent vertex pairs in the routes generated by random walking, and SN is the union of all negative sampling setts. p (u, v; \u03b8) is the common probability between the world, the texu, and the texv - that the link is inseparable."}, {"heading": "Node-Node Link", "text": "Inspired by the idea of negative sampling (Mikolov et al. 2013b), we select for each edge (u, v, v) a series SNunn = {v, v, v, v, v,. Then SN = u, Vnn SNunn.Lnn = \u2211 (u, v) \u0192Enn [log p (v, u; \u03b8) \u2212 \u2211 v, v. \"SNunnlog p (v, u; \u03b8)] (2) Here p (v, u) (we omit for simplicity reasons) is calculated a logistic function: p (v, u) = \u03c3 (eu \u00b7 ev) = 11 + exp (\u2212 eu \u00b7 ev) (3) Equation 3 is a symmetrical process, however, which means p (v, u) = p (u, v), and this is not suitable for guided networks."}, {"heading": "Node-Content Link", "text": "The loss of the node contents is comparable to Eq.2. If we leave SNunc = {hi \u2032 | (u, c \u2032) | Enc} the negative sampling unit for the edge (u, c), then the loss can be written as follows: Lnc = \u2211 (u, c), Enc [log p (c, u; \u03b8) \u2212 \u2211 c \"SNunclog p (c, u; \u03b8)] (5), where (c, u) = \u03c3 (eu, fe (c)))) = \u03c3 (e out u-einu, fe (c)) (6) Instead of arbitrary embedding for each document c, we use a composition function fe (\u00b7) here to calculate the representation of the contents in order to fully grasp the semantics of the texts. In this essay, we further decompose each document into sentences, and model node concatenation separately (Figure 2). We examine three typical compositional models for learning the sentence representations."}, {"heading": "Joint Learning", "text": "Finally, we optimize the following common target function, which is a weighted combination of node loss (Eq.2) and node content loss (Eq.5): L = \u03b1 \u0445 Lnn + (1 \u2212 \u03b1) \u043d Lnc (10), where \u03b1 [0, 1] is a parameter for balancing the importance of the two goals. As the \u03b1 structure increases, more structural information (node linkage) is taken into account. In our implementation, we approach the effect of \u03b1 by instance sampling (node and node content) in each training epoch. Further details are shown in Algorithm 1.Algorithm 1 < Get Training 1.Input: Vn, Vc \u00b7 Loop \u00b2, Loop \u00b2 2; Loop \u00b2, Endpoint 2) and node content loss (Eq.5)."}, {"heading": "15 end", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Experiments Setup", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Dataset", "text": "We conduct experiments with two real-world datasets: DBLP (Tang et al. 2008) and Zhihu. An overview of these networks is given in Table 1.DBLP. We use the DBLP2 dataset to construct the citation network. Two popular conferences: SIGIR and KDD are selected as the two categories for classifying nodes.3 Hiere2cn.aminer.org / citation (V1 is used) 3Note that SIGIR mainly focuses on retrieving information, while KDD is more related to data mining, each paper is considered a node, and each directed edge between two nodes indicates a citation. We use the abstraction of each paper as content. Note that only 16.7% of the nodes have DBLP content, and we retain all nodes of DBLP for experiments.Zhihu Zhihu4 is an area of the social network where users are selected to create a user-based answer based on the three categories of knowledge."}, {"heading": "Baseline", "text": "For the experimental comparison we consider the following methods of network embedding:"}, {"heading": "Structure-Based Method", "text": "\u2022 DeepWalk (DW) (Perozzi, Al-Rfou, and Skiena 2014). DeepWalk learns how to embed vertexes using the skipgram model via vertext sequences generated by random walking in the network. \u2022 LINE (Tang et al. 2015). LINE takes into account both 1- and 2-order proximity and the concatenation of these two representations is used as final embedding. \u2022 Word2vec (W2V). We include an additional baseline that uses Word2vec (Mikolov et al. 2013a) to learn vertext embedding directly from node-node nodes."}, {"heading": "Content-Based Method", "text": "\u2022 Doc2vec (D2V) (Le and Mikolov 2014). Doc2vec is an extension of word2vec that learns how to display documents by predicting the surrounding words in contexts sam-4www.zhihu.com 5bitbucket.org / yoavgo / word2vecfpled from the document. Here, we use the Gensim implementation6. \u2022 Word Average (WAvg). Similar to the WAvg setting in our model (CENE), we are also interested in how well the word average performs in separate training."}, {"heading": "Combined Method", "text": "\u2022 Naive combination (NC): We link the two most powerful network embeddings learned using structure-based or content-based methods. \u2022 TADW (Yang et al. 2015): TADW integrates content information into network embeddings by factorizing a text-associated matrix."}, {"heading": "Evaluation", "text": "Using the metric used in previous studies (Perozzi, Al-Rfou and Skiena 2014; Tang et al. 2015), we randomly assign a portion (TR, from 10% to 90%) of the marked nodes as training data, with the remaining nodes to be tested. We use scikit-learn (Pedregosa et al. 2011) to train logistic regression classifiers. For each node, the experiments are performed 40 times independently and we report on the averaged micro-F1 measures."}, {"heading": "Training Protocols", "text": "The initial learning rate is \u03b70 = 0.025 for CENEWAvg and \u03b70 = 0.01 for CENERNN and CENEBiRNN. The dimension of embedding for both nodes and content is set to 200. Word embedding is preschooled, using the entire set of content connected to the network with a dimension of 200. In addition, the negative sampling size is SNunn for all methods 15 and SNunc 25 for CENE; the total number of samples T is 10 billion for LINE (1.) and LINE (2.), as shown in Tang et al. (2015); window size win = 5, running length t = 40 and number of steps per vertex \u03b3 = 50 for DeepWalk."}, {"heading": "Results and Analysis", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Classification tasks", "text": "The results are in Table 2 (DBLP), Table 3 (Zhihu Gender), Table 4 (Zhihu Location) and Table6radimrehurek.com / gensim / doc2vec. htmlA (Zhihu Profession).The proposed methods are both structure-based and content-based."}, {"heading": "Parameter Sensitivity", "text": "CENE has two hyperparameters: iteration number k and equilibrium balance \u03b1. We set the training percentage at 50% and test the classification value F1 with different k and \u03b1.Figure 5 shows F1 values with TR in the range of 10%, 50% to 90% for four different tasks. In all tasks, all three curves converge stable when k is close to 100. Figure 6 shows the effect of \u03b1. Note that at \u03b1 = 0, only content information is used, and when \u03b1 = 1, our model degenerates to a structure-based (W2V). As \u03b1 increases, the performance of CENE initially decreases, but decreases when \u03b1 is large enough. There is an abrupt decrease when \u03b1 grows from 0.9 to 1.0, indicating the importance of content information. Another notable phenomenon is that the performance of CENE in terms of location attributes on Zhihu continues to decrease with increasing growth, so this network structure itself makes little sense for a regional observation, as the social benefit is a critical one."}, {"heading": "Conclusion", "text": "In this paper, we introduce CENE, a novel network embedding method that uses both structural and textual content information in a network by treating content as a special type of node. Experiments to perform node classification using two real-world datasets demonstrate the effectiveness of our model. Three methods for embedding content are investigated, and we show that deeper models (RNN and BiRNN) are more competent for text modeling. In the future, we will expand our methods to networks with more diverse content such as images."}, {"heading": "Acknowledgments", "text": "This work was supported by the National Basic Research Programme (973 Programme) of China through Grant 2014CB340503, the National Natural Science Foundation of China (NSFC) through Grant 61472107 and 61133012."}], "references": [{"title": "Laplacian eigenmaps and spectral techniques for embedding and clustering", "author": ["Belkin", "M. Niyogi 2001] Belkin", "P. Niyogi"], "venue": "In NIPS,", "citeRegEx": "Belkin et al\\.,? \\Q2001\\E", "shortCiteRegEx": "Belkin et al\\.", "year": 2001}, {"title": "Modern multidimensional scaling: Theory and applications", "author": ["Borg", "I. Groenen 2005] Borg", "P.J. Groenen"], "venue": null, "citeRegEx": "Borg et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Borg et al\\.", "year": 2005}, {"title": "Grarep: Learning graph representations with global structural information", "author": ["Lu Cao", "S. Xu 2015] Cao", "W. Lu", "Q. Xu"], "venue": "In Proc. of CIKM,", "citeRegEx": "Cao et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Cao et al\\.", "year": 2015}, {"title": "Learning phrase representations using rnn encoderdecoder for statistical machine translation", "author": ["Cho"], "venue": "arXiv preprint arXiv:1406.1078", "citeRegEx": "Cho,? \\Q2014\\E", "shortCiteRegEx": "Cho", "year": 2014}, {"title": "Linear compositional distributional semantics and structural kernels", "author": ["Ferrone", "L. Zanzotto 2013] Ferrone", "F.M. Zanzotto"], "venue": "In Joint Symposium on Semantic Processing.,", "citeRegEx": "Ferrone et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Ferrone et al\\.", "year": 2013}, {"title": "Node2vec: Scalable feature learning for networks", "author": ["Grover", "A. Leskovec 2016] Grover", "J. Leskovec"], "venue": "In ACM SIGKDD,", "citeRegEx": "Grover et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Grover et al\\.", "year": 2016}, {"title": "Long short-term memory. Neural computation 9(8):1735\u20131780", "author": ["Hochreiter", "S. Schmidhuber 1997] Hochreiter", "J. Schmidhuber"], "venue": null, "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Latent space approaches to social network analysis", "author": ["Raftery Hoff", "P.D. Handcock 2002] Hoff", "A.E. Raftery", "M.S. Handcock"], "venue": "Journal of the american Statistical association", "citeRegEx": "Hoff et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Hoff et al\\.", "year": 2002}, {"title": "Deep unordered composition rivals syntactic methods for text classification", "author": ["Iyyer"], "venue": "In Proc. of ACL", "citeRegEx": "Iyyer,? \\Q2015\\E", "shortCiteRegEx": "Iyyer", "year": 2015}, {"title": "Effective use of word order for text categorization with convolutional neural networks", "author": ["Johnson", "R. Zhang 2015] Johnson", "T. Zhang"], "venue": "In Proc. of NAACL,", "citeRegEx": "Johnson et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Johnson et al\\.", "year": 2015}, {"title": "2016. Bag of tricks for efficient text classification", "author": ["Joulin"], "venue": "arXiv preprint arXiv:1607.01759", "citeRegEx": "Joulin,? \\Q2016\\E", "shortCiteRegEx": "Joulin", "year": 2016}, {"title": "Skip-thought vectors", "author": ["Kiros"], "venue": null, "citeRegEx": "Kiros,? \\Q2015\\E", "shortCiteRegEx": "Kiros", "year": 2015}, {"title": "Distributed representations of sentences and documents", "author": ["Le", "Q. Mikolov 2014] Le", "T. Mikolov"], "venue": "In Proc. of ICML,", "citeRegEx": "Le et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Le et al\\.", "year": 2014}, {"title": "The link-prediction problem for social networks. JASIST 1019\u20131031", "author": ["Liben-Nowell", "D. Kleinberg 2007] Liben-Nowell", "J. Kleinberg"], "venue": null, "citeRegEx": "Liben.Nowell et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Liben.Nowell et al\\.", "year": 2007}, {"title": "Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Distributed representations of words and phrases and their compositionality", "author": ["Mikolov"], "venue": null, "citeRegEx": "Mikolov,? \\Q2013\\E", "shortCiteRegEx": "Mikolov", "year": 2013}, {"title": "Composition in distributional models of semantics. Cognitive science 34:1388\u20131429", "author": ["Mitchell", "J. Lapata 2010] Mitchell", "M. Lapata"], "venue": null, "citeRegEx": "Mitchell et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Mitchell et al\\.", "year": 2010}, {"title": "Asymmetric transitivity preserving graph embedding", "author": ["Ou"], "venue": "In Proc. of ACM SIGKDD,", "citeRegEx": "Ou,? \\Q2016\\E", "shortCiteRegEx": "Ou", "year": 2016}, {"title": "Tri-party deep network representation", "author": ["Pan"], "venue": "In IJCAI", "citeRegEx": "Pan,? \\Q2016\\E", "shortCiteRegEx": "Pan", "year": 2016}, {"title": "Scikit-learn: Machine learning in Python", "author": ["Pedregosa"], "venue": null, "citeRegEx": "Pedregosa,? \\Q2011\\E", "shortCiteRegEx": "Pedregosa", "year": 2011}, {"title": "Deepwalk: Online learning of social representations", "author": ["Al-Rfou Perozzi", "B. Skiena 2014] Perozzi", "R. Al-Rfou", "S. Skiena"], "venue": "In Proc. of ACM SIGKDD,", "citeRegEx": "Perozzi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Perozzi et al\\.", "year": 2014}, {"title": "Walklets: Multiscale graph embeddings for interpretable network classification", "author": ["Kulkarni Perozzi", "B. Skiena 2016] Perozzi", "V. Kulkarni", "S. Skiena"], "venue": "arXiv preprint arXiv:1605.02115", "citeRegEx": "Perozzi et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Perozzi et al\\.", "year": 2016}, {"title": "Nonlinear dimensionality reduction by locally linear embedding", "author": ["Roweis", "S.T. Saul 2000] Roweis", "L.K. Saul"], "venue": "Science", "citeRegEx": "Roweis et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Roweis et al\\.", "year": 2000}, {"title": "Bidirectional recurrent neural networks", "author": ["Schuster", "M. Paliwal 1997] Schuster", "K.K. Paliwal"], "venue": "IEEE Transactions on Signal Processing", "citeRegEx": "Schuster et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Schuster et al\\.", "year": 1997}, {"title": "Collective classification in network data. AI magazine 29(3):93", "author": ["Sen"], "venue": null, "citeRegEx": "Sen,? \\Q2008\\E", "shortCiteRegEx": "Sen", "year": 2008}, {"title": "Recursive deep models for semantic compositionality over a sentiment treebank", "author": ["Socher"], "venue": "In Proc. of EMNLP,", "citeRegEx": "Socher,? \\Q2013\\E", "shortCiteRegEx": "Socher", "year": 2013}, {"title": "Grounded compositional semantics for finding and describing images with sentences", "author": ["Socher"], "venue": "TACL", "citeRegEx": "Socher,? \\Q2014\\E", "shortCiteRegEx": "Socher", "year": 2014}, {"title": "Arnetminer: Extraction and mining of academic social networks", "author": ["Tang"], "venue": "In KDD\u201908,", "citeRegEx": "Tang,? \\Q2008\\E", "shortCiteRegEx": "Tang", "year": 2008}, {"title": "Line: Large-scale information network embedding", "author": ["Tang"], "venue": "In Proc. of WWW,", "citeRegEx": "Tang,? \\Q2015\\E", "shortCiteRegEx": "Tang", "year": 2015}, {"title": "Pte: Predictive text embedding through large-scale heterogeneous text networks", "author": ["Qu Tang", "J. Mei 2015] Tang", "M. Qu", "Q. Mei"], "venue": "In Proc. of ACM SIGKDD,", "citeRegEx": "Tang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Tang et al\\.", "year": 2015}, {"title": "A global geometric framework for nonlinear dimensionality reduction", "author": ["De Silva Tenenbaum", "J.B. Langford 2000] Tenenbaum", "V. De Silva", "J.C. Langford"], "venue": null, "citeRegEx": "Tenenbaum et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Tenenbaum et al\\.", "year": 2000}, {"title": "Multi-label classification: An overview", "author": ["Tsoumakas", "G. Katakis 2006] Tsoumakas", "I. Katakis"], "venue": "Dept. of Informatics,", "citeRegEx": "Tsoumakas et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Tsoumakas et al\\.", "year": 2006}, {"title": "Inferring correspondences from multiple sources for microblog user tags", "author": ["Liu Tu", "C. Sun 2014] Tu", "Z. Liu", "M. Sun"], "venue": "In Chinese National Conference on Social Media Processing,", "citeRegEx": "Tu et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Tu et al\\.", "year": 2014}, {"title": "Structural deep network embedding", "author": ["Cui Wang", "D. Zhu 2016] Wang", "P. Cui", "W. Zhu"], "venue": "In Proc. of ACM SIGKDD,", "citeRegEx": "Wang et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2016}, {"title": "Comprehend deepwalk as matrix factorization", "author": ["Yang", "C. Liu 2015] Yang", "Z. Liu"], "venue": "arXiv preprint arXiv:1501.00358", "citeRegEx": "Yang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Yang et al\\.", "year": 2015}, {"title": "Network representation learning with rich text information", "author": ["Yang"], "venue": "In Proc. of IJCAI,", "citeRegEx": "Yang,? \\Q2015\\E", "shortCiteRegEx": "Yang", "year": 2015}, {"title": "Personalized entity recommendation: A heterogeneous information network approach", "author": ["Yu"], "venue": "In Proc. of the WSDM,", "citeRegEx": "Yu,? \\Q2014\\E", "shortCiteRegEx": "Yu", "year": 2014}], "referenceMentions": [{"referenceID": 29, "context": "Various approaches have been proposed toward this goal, typically including Deepwalk (Perozzi, Al-Rfou, and Skiena 2014), LINE (Tang et al. 2015), GraRep (Cao, Lu, and Xu 2015), and node2vec (Grover and Leskovec 2016).", "startOffset": 127, "endOffset": 145}, {"referenceID": 34, "context": "To cope with this challenge, Yang et al. (2015) presented text-associated DeepWalk (TADW), which incorporates textual features into network embeddings through matrix factorization.", "startOffset": 29, "endOffset": 48}, {"referenceID": 8, "context": ", sentence, paragraph), a simple and intuitive approach would be averaging the embeddings of each word in the text (Mitchell and Lapata 2010; Ferrone and Zanzotto 2013; Iyyer et al. 2015). More sophisticated models have been designed to utilize the internal structure of sentences or documents to assist the composition. For example, Socher et al. (2013) and Socher et al.", "startOffset": 169, "endOffset": 355}, {"referenceID": 8, "context": ", sentence, paragraph), a simple and intuitive approach would be averaging the embeddings of each word in the text (Mitchell and Lapata 2010; Ferrone and Zanzotto 2013; Iyyer et al. 2015). More sophisticated models have been designed to utilize the internal structure of sentences or documents to assist the composition. For example, Socher et al. (2013) and Socher et al. (2014) use recursive neural networks over parse trees to obtain sentence representations.", "startOffset": 169, "endOffset": 380}, {"referenceID": 29, "context": "LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process.", "startOffset": 5, "endOffset": 23}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network.", "startOffset": 131, "endOffset": 428}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process.", "startOffset": 131, "endOffset": 663}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process. These methods could also be applied to prediction tasks in heterogeneous text networks (Tang, Qu, and Mei 2015). Another attempt is based on the factorization of relationship matrix (Yang and Liu 2015). Most recently, Wang, Cui, and Zhu (2016) adopt a deep model to capture the nonlinear network structure.", "startOffset": 131, "endOffset": 1040}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process. These methods could also be applied to prediction tasks in heterogeneous text networks (Tang, Qu, and Mei 2015). Another attempt is based on the factorization of relationship matrix (Yang and Liu 2015). Most recently, Wang, Cui, and Zhu (2016) adopt a deep model to capture the nonlinear network structure. Yang et al. (2015) present the first work that combines structure and content information for learning network embeddings.", "startOffset": 131, "endOffset": 1122}, {"referenceID": 14, "context": "Recent advancements include DeepWalk (Perozzi, AlRfou, and Skiena 2014), which learns vertex embeddings using the skip-gram model (Mikolov et al. 2013b) on vertex sequences generated by random walking on the network. Inspired by Deepwalk, walklet (Perozzi, Kulkarni, and Skiena 2016) focuses on multiscale representation learning, node2vec (Grover and Leskovec 2016) explores different random walk strategy and Ou et al. (2016) emphasises the asymmetric transitivity of a network. Some others focus on depicting the distance between vertices. LINE (Tang et al. 2015) exploits both first-order and second-order proximity in an network while Cao, Lu, and Xu (2015) expand the proximity into k-order (or k-step) and integrates global structural information of the network into the learning process. These methods could also be applied to prediction tasks in heterogeneous text networks (Tang, Qu, and Mei 2015). Another attempt is based on the factorization of relationship matrix (Yang and Liu 2015). Most recently, Wang, Cui, and Zhu (2016) adopt a deep model to capture the nonlinear network structure. Yang et al. (2015) present the first work that combines structure and content information for learning network embeddings. They show that DeepWalk is equivalent to matrix factorization (MF) and text features of vertices can be incorporated via factorizing a text-associated matrix. This method, however, suffers from the high computation cost of MF and has difficulties scaling to large-scale networks. Pan et al. (2016) instead combines DeepWalk with Doc2Vec (Le and Mikolov 2014), along with partial labels of nodes that constitutes a semi-supervised model.", "startOffset": 131, "endOffset": 1524}, {"referenceID": 3, "context": "Here we use the gated recurrent unit (GRU) proposed by Cho et al. (2014). GRU is a simplified version of the LSTM unit proposed earlier (Hochreiter and Schmidhuber 1997), with fewer parameters while still preserving the ability of capturing long-term dependencies.", "startOffset": 55, "endOffset": 73}, {"referenceID": 29, "context": "\u2022 LINE (Tang et al. 2015).", "startOffset": 7, "endOffset": 25}, {"referenceID": 34, "context": "\u2022 TADW (Yang et al. 2015).", "startOffset": 7, "endOffset": 25}, {"referenceID": 29, "context": "Following the metric used in previous studies (Perozzi, Al-Rfou, and Skiena 2014; Tang et al. 2015), we randomly sample a portion (TR, from 10% to 90%) of the labeled vertices as training data, with the rest of the vertices for testing.", "startOffset": 46, "endOffset": 99}, {"referenceID": 27, "context": "In addition, the negative sampling size SN nn is 15 for all methods, and SN nc is 25 for CENE; the total number of samples T is 10 billion for LINE (1st) and LINE (2nd) as shown in Tang et al. (2015); window size win = 5, walk length t = 40 and number of walks per vertex \u03b3 = 50 for DeepWalk.", "startOffset": 181, "endOffset": 200}], "year": 2016, "abstractText": "This paper investigates the problem of network embedding, which aims at learning low-dimensional vector representation of nodes in networks. Most existing network embedding methods rely solely on the network structure, i.e., the linkage relationships between nodes, but ignore the rich content information associated with it, which is common in real world networks and beneficial to describing the characteristics of a node. In this paper, we propose content-enhanced network embedding (CENE), which is capable of jointly leveraging the network structure and the content information. Our approach integrates text modeling and structure modeling in a general framework by treating the content information as a special kind of node. Experiments on several real world networks with application to node classification show that our models outperform all existing network embedding methods, demonstrating the merits of content information and joint learning. Introduction Network embedding, which aims at learning lowdimensional vector representations of a network, has attracted increasing interest in recent years. It has been shown highly effective in many important tasks in network analysis involving predictions over nodes and edges, such as node classification (Tsoumakas and Katakis 2006; Sen et al. 2008), recommendation (Tu, Liu, and Sun 2014; Yu et al. 2014) and link prediction (Liben-Nowell and Kleinberg 2007). Various approaches have been proposed toward this goal, typically including Deepwalk (Perozzi, Al-Rfou, and Skiena 2014), LINE (Tang et al. 2015), GraRep (Cao, Lu, and Xu 2015), and node2vec (Grover and Leskovec 2016). These models have been proven effective in several real world networks. Most of the previous approaches utilize information only from the network structure, i.e., the linkage relationships between nodes, while paying scant attention to the content of each node, which is common in real-world networks. In a typical social network with users as vertices, the user-generated contents (e.g., texts, images) will serve as rich extra information which should be important for node representation and beneficial to downstream applications. Figure 1 shows an example network from Quora, a community question answering website. Users in Quora can follow each other, creating directed connections in the network. How does the shape", "creator": "LaTeX with hyperref package"}}}