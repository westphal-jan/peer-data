{"id": "1610.02995", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Extrapolation and learning equations", "abstract": "In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified.", "histories": [["v1", "Mon, 10 Oct 2016 16:47:36 GMT  (1884kb,D)", "http://arxiv.org/abs/1610.02995v1", "13 pages, 8 figures, 4 tables"]], "COMMENTS": "13 pages, 8 figures, 4 tables", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["georg martius", "christoph h lampert"], "accepted": false, "id": "1610.02995"}, "pdf": {"name": "1610.02995.pdf", "metadata": {"source": "CRF", "title": "Extrapolation and learning equations", "authors": ["Georg Martius", "Christoph H. Lampert"], "emails": ["gmartius@ist.ac.at", "chl@ist.ac.at"], "sections": [{"heading": null, "text": "In the natural sciences, however, the primary goal is to find an interpretable function for a phenomenon, since it enables results to be understood and generalized. This paper proposes a novel type of functional learning network, the so-called Equation Learner (EQL), which can learn analytical expressions and extrapolate to invisible areas. It is implemented as a continuous differentiable feed network and enables efficient gradient-based training. By regulating scarcity, concise interpretable expressions can be obtained, often identifying the true underlying source expression."}, {"heading": "Introduction", "text": "The quality of a model is typically measured by its ability to generalize from a training set to hitherto invisible data from the same distribution. In regression tasks, generalization essentially amounts to interpolation if the training data is sufficiently dense. As long as models are selected correctly, i.e. in such a way that the data do not match, the regression problem is well understood and can - at least conceptually - be considered solved. In such a case, for the sake of robustness and safety, it is desirable to have a regression model that continues to make good predictions, or at least does not fail disastrously. This setting, which we call extrapolation, is the subject of the current paper. We are particularly interested in regression systems that can be evaluated by real predictions or realization strategies."}, {"heading": "Regression and extrapolation", "text": "We consider a multivariate regression problem with a training set {x1, y1),., (xN, yN) with x-Rn, y-Rm. Since our main interest is based on extrapolation in the context of the dynamics of physical systems, we assume that the data comes from an unknown analytical system (or a system of functions). The general task is to learn a function that incorporates the true functional relationship as well as possible into the quadratic loss. For example, the function can reflect a system of ordinary differential equations that govern the movements of a robotic arm or similar. The general task is to integrate the true functional relationship as well as possible into the quadratic loss, i. e."}, {"heading": "Discussion of the architecture", "text": "The proposed network architecture differs in two main aspects of typical feed-forward networks: the existence of multiplication factors that govern a physical system and can extrapolate to new parts of the input space. Sigmoid nonlinearities are the canonical choice of activation function for artificial neural networks (ANN) and have proven successful. In fact, we include sigmoids in our architecture, making them a superclass of ANNs. However, they have typically been disabled by the training process corresponding to their absence in the physical equations under consideration. Other, predominantly local nonlinearities, especially radial basic functions [Broomhead and Lowe, 1988] which we do not include, as they are not expected to extrapolate. Other nonlinears, such as (square) roots and logarithms, may be useful in principle for learning physical equations."}, {"heading": "Network training", "text": "The EQL is completely differentiable in its free parameters \u03b8 = {W (1),.., W (L), b (1),.., b (L), which allows us to learn it in an end-to-end manner with back propagation. We adopt a lasso-like goal [Tibshirani, 1996], L (D) = 1N | D = 1], and apply a stochastic gradient descendent algorithm with mini batches and Adam [Kingma and Ba, 2015] for the calculation of updates: the updates + 1 = Adam (D (t), the variation, (8) where D (t) is the current mini batch and size."}, {"heading": "Model selection for extrapolation", "text": "EQL networks have a number of hyperparameters, such as the number of layers, the number of units and the regularization constant. Unfortunately, standard techniques for model selection, such as holding-out set evaluation or cross-validation, are not optimal for our purpose, as they rely on interpolation quality. To extrapolate the network, the \"right\" formula must be found, but how can we say? We use the occams shaving principle: the simplest formula is most likely the right one. Intuitively, if we have a choice between cos (x) and its shortened power circuit approximation 1 \u2212 x2 / 2 + x4 / 24, the first one is preferred. We use the number of active hidden units in the network as a substitute for the complexity of the formula, see Appendix A1 for details. One could also think about differentiating between unit types. In any case, this argument is correct only when the model explains the data well."}, {"heading": "Related work", "text": "This year, it is only a matter of time before a decision is reached in which an agreement can be reached."}, {"heading": "Experimental evaluation", "text": "We demonstrate the ability of EQL to learn physically inspired models with good extrapolation quality through experiments on synthetic and real data (81); we demonstrate the ability of EQL to share physically inspired models with good extrapolation quality through experiments on synthetic and real data (81); we implement the network training and evaluation process in Python based on the Theano Development Team, 2016); we will make the code for training and evaluation public after acceptance of the manuscript. We will first present the results of learning the motion equations for a very simple physical system: a pendulum. The state space of a pendulum is X = R \u00b7 R, where the first value of the angle of the pole in radiants and the second value is the angular velocity. In the physical literature, these are usually referred to as (8,000), but for our purposes we call them (x1, x2) to keep the notation between experiments consistent and the following dynamic behavior: \u00b2 The usual two is governed by the following:"}, {"heading": "Conclusions", "text": "We have introduced a new network architecture called EQL, which can learn analytical expressions that typically occur in equations that master physical, especially mechanical systems; the network is fully differentiable, allowing for training with back propagation; by sequencing L1 regularization and defining the L0 standard, we achieve sparse representations with unbiased estimation of factors within the learned equations; we also introduce a model selection process specifically designed for good extrapolation quality through a multi-objective criterion based on validation errors and conciseness; the proposed method is able to learn functional relationships and extrapolate them to invisible portions of data space, as we demonstrate through experiments on synthetic and real data; the approach learns concise functional forms that can provide insights into relationships within the data, as we demonstrate by using physical measurements of local energy ratios rather than local ones."}, {"heading": "Acknowledgments", "text": "GM received funding from the People Programme (Marie Curie Actions) of the Seventh Framework Programme of the European Union (FP7 / 2007-2013) under FGD Funding Agreement No [291734]."}, {"heading": "Appendix", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A1: Model selection details", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "Quantifying sparsity", "text": "We actually want a measure of the complexity of the formula, but since it is not clear what is the right choice of a measure, we use sparseness instead by counting the number of active / used hidden units denoted by s. For a given network, we get (\u03c6) = L \u2211 l = 1 k \u2211 i = 1% (| W (l) i, \u00b7 | \u043a | W (l + 1) \u00b7, i | \u2212 0.01), (14), where \u0443 is the main function and 0.01 is an arbitrary threshold. For the multiplication units, the standard of incoming weights is added for both inputs (omitted to avoid confusion in the formula)."}, {"heading": "Selection criteria", "text": "As explained in the main text, we try to choose the model that is both simple and has good performance in terms of the validation set. As both quantities have different scales, we suggested to choose them based on their rank. If rv (\u03c6) and rs (\u03c6) are the ranks of the network \u03c6 w. r. t. of the validation error or the splitting rate s (\u03c6), then the network with the minimum square rank standard is selected: arg min \u03c6 [rv (\u03c6) 2 + rs (\u03c6) 2] (15) In Figure 7, the extrapolation performance of all considered networks for the kin2D-3 dataset is visualized depending on the validation error and the splitting rate. It becomes obvious that the most powerful networks are both sparse and have a low validation error."}, {"heading": "A2: Dependence on noise and number of data points", "text": "To understand how the method depends on the amount of noise and the number of data points, we scan the two parameters and present the empirical results in Fig. 8. In general, the method is noise-resistant and, as expected, more noise can be compensated by more data."}], "references": [{"title": "Support vector regression", "author": ["D. Basak", "S. Pal", "D.C. Patranabis"], "venue": "Neural Information Processing-Letters and Reviews,", "citeRegEx": "Basak et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Basak et al\\.", "year": 2007}, {"title": "A theory of learning from different domains", "author": ["S. Ben-David", "J. Blitzer", "K. Crammer", "A. Kulesza", "F. Pereira", "J.W. Vaughan"], "venue": "Machine Learning,", "citeRegEx": "Ben.David et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ben.David et al\\.", "year": 2010}, {"title": "Learning deep architectures for AI", "author": ["Y. Bengio"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "Bengio.,? \\Q2009\\E", "shortCiteRegEx": "Bengio.", "year": 2009}, {"title": "Representation learning: A review and new perspectives", "author": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),", "citeRegEx": "Bengio et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2013}, {"title": "Neural networks for pattern recognition", "author": ["C.M. Bishop"], "venue": null, "citeRegEx": "Bishop.,? \\Q1995\\E", "shortCiteRegEx": "Bishop.", "year": 1995}, {"title": "Radial basis functions, multi-variable functional interpolation and adaptive networks", "author": ["D.S. Broomhead", "D. Lowe"], "venue": "Technical report, DTIC Document,", "citeRegEx": "Broomhead and Lowe.,? \\Q1988\\E", "shortCiteRegEx": "Broomhead and Lowe.", "year": 1988}, {"title": "X-ray transition energies: new approach to a comprehensive evaluation", "author": ["R.D. Deslattes", "E.G. Kessler Jr.", "P. Indelicato", "L. De Billy", "E. Lindroth", "J. Anton"], "venue": "Reviews of Modern Physics,", "citeRegEx": "Deslattes et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Deslattes et al\\.", "year": 2003}, {"title": "Product units: A computationally powerful and biologically plausible extension to backpropagation networks", "author": ["R. Durbin", "D.E. Rumelhart"], "venue": "Neural Computation,", "citeRegEx": "Durbin and Rumelhart.,? \\Q1989\\E", "shortCiteRegEx": "Durbin and Rumelhart.", "year": 1989}, {"title": "Nonparametric curve estimation from time series, volume 60", "author": ["L. Gy\u00f6rfi", "W. H\u00e4rdle", "P. Sarda", "P. Vieu"], "venue": null, "citeRegEx": "Gy\u00f6rfi et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Gy\u00f6rfi et al\\.", "year": 2013}, {"title": "Adam: A method for stochastic optimization", "author": ["D. Kingma", "J. Ba"], "venue": "In in Proceedings of ICLR,", "citeRegEx": "Kingma and Ba.,? \\Q2015\\E", "shortCiteRegEx": "Kingma and Ba.", "year": 2015}, {"title": "Predicting time series with support vector machines", "author": ["K.-R. M\u00fcller", "A.J. Smola", "G. R\u00e4tsch", "B. Sch\u00f6lkopf", "J. Kohlmorgen", "V. Vapnik"], "venue": "In Artificial Neural Networks (ICANN),", "citeRegEx": "M\u00fcller et al\\.,? \\Q1997\\E", "shortCiteRegEx": "M\u00fcller et al\\.", "year": 1997}, {"title": "Causal discovery with continuous additive noise models", "author": ["J. Peters", "J. Mooij", "D. Janzing", "B. Sch\u00f6lkopf"], "venue": "Journal of Machine Learning Research (JMLR),", "citeRegEx": "Peters et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Peters et al\\.", "year": 2014}, {"title": "Dataset shift in machine learning", "author": ["J. Quionero-Candela", "M. Sugiyama", "A. Schwaighofer", "N.D. Lawrence"], "venue": null, "citeRegEx": "Quionero.Candela et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Quionero.Candela et al\\.", "year": 2009}, {"title": "Distilling free-form natural laws from experimental data", "author": ["M. Schmidt", "H. Lipson"], "venue": "Science, 324(5923):81\u201385,", "citeRegEx": "Schmidt and Lipson.,? \\Q2009\\E", "shortCiteRegEx": "Schmidt and Lipson.", "year": 2009}, {"title": "The pi-sigma network : An efficient higher-order neural network for pattern classification and function approximation", "author": ["Y. Shin", "J. Ghosh"], "venue": "In in Proceedings of the International Joint Conference on Neural Networks,", "citeRegEx": "Shin and Ghosh.,? \\Q1991\\E", "shortCiteRegEx": "Shin and Ghosh.", "year": 1991}, {"title": "A tutorial on support vector regression", "author": ["A.J. Smola", "B. Sch\u00f6lkopf"], "venue": "Statistics and computing,", "citeRegEx": "Smola and Sch\u00f6lkopf.,? \\Q2004\\E", "shortCiteRegEx": "Smola and Sch\u00f6lkopf.", "year": 2004}, {"title": "A general regression neural network", "author": ["D.F. Specht"], "venue": "IEEE Transactions on Neural Networks (TNN),", "citeRegEx": "Specht.,? \\Q1991\\E", "shortCiteRegEx": "Specht.", "year": 1991}, {"title": "Regression shrinkage and selection via the lasso", "author": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological),", "citeRegEx": "Tibshirani.,? \\Q1996\\E", "shortCiteRegEx": "Tibshirani.", "year": 1996}, {"title": "Extrapolation, interpolation, and smoothing of stationary time series, volume 2", "author": ["N. Wiener"], "venue": null, "citeRegEx": "Wiener.,? \\Q1949\\E", "shortCiteRegEx": "Wiener.", "year": 1949}, {"title": "Gaussian processes for machine learning", "author": ["C.K.I. Williams", "C.E. Rasmussen"], "venue": null, "citeRegEx": "Williams and Rasmussen.,? \\Q2006\\E", "shortCiteRegEx": "Williams and Rasmussen.", "year": 2006}, {"title": "Learning to discover efficient mathematical identities", "author": ["W. Zaremba", "K. Kurach", "R. Fergus"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "Zaremba et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Zaremba et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 5, "context": "Other, predominantly local nonlinearities, in particular radial basis functions [Broomhead and Lowe, 1988] we do not include, since one cannot expect them to extrapolate at all.", "startOffset": 80, "endOffset": 106}, {"referenceID": 7, "context": "Multiplication was introduced into neural networks long ago as product-units [Durbin and Rumelhart, 1989] and Pi-Sigma-unit [Shin and Ghosh, 1991].", "startOffset": 77, "endOffset": 105}, {"referenceID": 14, "context": "Multiplication was introduced into neural networks long ago as product-units [Durbin and Rumelhart, 1989] and Pi-Sigma-unit [Shin and Ghosh, 1991].", "startOffset": 124, "endOffset": 146}, {"referenceID": 17, "context": "We adopt a Lasso-like objective [Tibshirani, 1996],", "startOffset": 32, "endOffset": 50}, {"referenceID": 9, "context": "that is, a linear combination of L2 loss and L1 regularization, and apply a stochastic gradient descent algorithm with mini-batches and Adam [Kingma and Ba, 2015] for calculating the updates:", "startOffset": 141, "endOffset": 162}, {"referenceID": 19, "context": "a reproducing kernel Hilbert space for Gaussian Processes Regression (GPR) [Williams and Rasmussen, 2006] or Support Vector Regression (SVR) [Smola and Sch\u00f6lkopf, 2004], or a multi-layer network of suitable expressive power [Specht, 1991].", "startOffset": 75, "endOffset": 105}, {"referenceID": 15, "context": "a reproducing kernel Hilbert space for Gaussian Processes Regression (GPR) [Williams and Rasmussen, 2006] or Support Vector Regression (SVR) [Smola and Sch\u00f6lkopf, 2004], or a multi-layer network of suitable expressive power [Specht, 1991].", "startOffset": 141, "endOffset": 168}, {"referenceID": 16, "context": "a reproducing kernel Hilbert space for Gaussian Processes Regression (GPR) [Williams and Rasmussen, 2006] or Support Vector Regression (SVR) [Smola and Sch\u00f6lkopf, 2004], or a multi-layer network of suitable expressive power [Specht, 1991].", "startOffset": 224, "endOffset": 238}, {"referenceID": 11, "context": "Recent extensions of causal learning can take a functional view, but typically do not constrain the regression functions to physically plausible ones, but rather constrain the noise distributions [Peters et al., 2014].", "startOffset": 196, "endOffset": 217}, {"referenceID": 18, "context": "predict the next value(s) [Wiener, 1949].", "startOffset": 26, "endOffset": 40}, {"referenceID": 8, "context": "By our nomenclature, this is typically rather an interpolation task, when the prediction is based on the behaviour of the series at earlier time steps but with similar value distribution [Gy\u00f6rfi et al., 2013; M\u00fcller et al., 1997].", "startOffset": 187, "endOffset": 229}, {"referenceID": 10, "context": "By our nomenclature, this is typically rather an interpolation task, when the prediction is based on the behaviour of the series at earlier time steps but with similar value distribution [Gy\u00f6rfi et al., 2013; M\u00fcller et al., 1997].", "startOffset": 187, "endOffset": 229}, {"referenceID": 12, "context": "In particular, since we assume a common labeling function, our setting would fall under the covariate shift setting [Quionero-Candela et al., 2009].", "startOffset": 116, "endOffset": 147}, {"referenceID": 1, "context": "As domain adaptation typically does not make additional assumptions about how the data distribution may change, existing methods need access to some unlabeled data from the test distribution already at training time [Ben-David et al., 2010].", "startOffset": 216, "endOffset": 240}, {"referenceID": 4, "context": "On the technical level, EQL networks are an instance of general feed-forward networks for function approximation [Bishop, 1995].", "startOffset": 113, "endOffset": 127}, {"referenceID": 2, "context": "In contrast to recent trends towards deep learning [Bengio, 2009; Bengio et al., 2013], our goal is not to learn any data representation, but to learn a function which compactly represents the input-output relation and generalizes between different regions of the data space, like a physical formula.", "startOffset": 51, "endOffset": 86}, {"referenceID": 3, "context": "In contrast to recent trends towards deep learning [Bengio, 2009; Bengio et al., 2013], our goal is not to learn any data representation, but to learn a function which compactly represents the input-output relation and generalizes between different regions of the data space, like a physical formula.", "startOffset": 51, "endOffset": 86}, {"referenceID": 14, "context": "Structurally, EQL networks resemble sum-product networks (SPNs) [Poon and Domingos, 2012] and Pi-Sigma networks (PSNs) [Shin and Ghosh, 1991], in the sense that both are based on directed acyclic graphs with computational units that allows for summation and multiplication.", "startOffset": 119, "endOffset": 141}, {"referenceID": 13, "context": "With these techniques it is possible to discover physical laws such as invariants and conserved quantities [Schmidt and Lipson, 2009].", "startOffset": 107, "endOffset": 133}, {"referenceID": 20, "context": "In [Zaremba et al., 2014] this was done using machine learning to overcome the potentially exponential search space.", "startOffset": 3, "endOffset": 25}, {"referenceID": 0, "context": "A second baseline is given by epsilon support vector regression (SVR) [Basak et al., 2007] with two hyperparameters C \u2208 10{\u22123,\u22122,\u22121,0,1,2,3,3.", "startOffset": 70, "endOffset": 90}, {"referenceID": 13, "context": "For that we use recorded trajectories of a real double pendulum [Schmidt and Lipson, 2009].", "startOffset": 64, "endOffset": 90}, {"referenceID": 6, "context": "The data is taken from [Deslattes et al., 2003], where we consider one specific transition, called the K \u03b12 line, because it was measured for all elements.", "startOffset": 23, "endOffset": 47}], "year": 2016, "abstractText": "In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified. Introduction The quality of a model is typically measured by its ability to generalize from a training set to previously unseen data from the same distribution. In regression tasks generalization essentially boils down to interpolation if the training data is sufficiently dense. As long as models are selected correctly, i. e. in a way to not overfit the data, the regression problem is well understood and can \u2013 at least conceptually \u2013 be considered solved. However, when working with data from real-world devices, e. g. controlling a robotic arm, interpolation might not be sufficient. It could happen that future data lies outside of the training domain, e. g. when the arm is temporarily operated outside of its specifications. For the sake of robustness and safety it is desirable in such a case to have a regression model that continues to make good predictions, or at least does not fail catastrophically. This setting, which we call extrapolation generalization, is the topic of the present paper. We are particularly interested in regression tasks for systems that can be described by real-valued analytic expression, e. g. mechanical systems such as a pendulum or a robotic arm. These are typically governed by a highly nonlinear function but it is nevertheless possible, in principle, to infer their behavior on an extrapolation domain from their behavior elsewhere. We make two main contributions: 1) a new type of network that can learn analytical expressions and is able to extrapolate to unseen domains and 2) a model selection strategy tailored to the extrapolation setting. The following section describes the setting of regression and extrapolation. Afterwards we introduce our method and discuss the architecture, its training, and its relation to prior art. We present our results in the Section Experimental evaluation and close with conclusions. Regression and extrapolation We consider a multivariate regression problem with a training set {(x1, y1), . . . , (xN , yN )} with x \u2208 R, y \u2208 R. Because our main interest lies on extrapolation in the context of learning the dynamics of physical systems we assume the data originates from an unknown analytical function (or system of functions), \u03c6 : R \u2192 R with additive zero-mean noise, \u03be, i. e. y = \u03c6(x) + \u03be and E\u03be = 0. The function \u03c6 may, for instance, reflect a system of ordinary differential equations that govern the movements of a robot arm or the like. The general task is to learn a function \u03c8 : R \u2192 R that approximates the true functional relation as well as possible in the squared loss sense, i. e. achieves minimal expected error E\u2016\u03c8(x)\u2212 \u03c6(x)\u20162. In practice, we only have particular examples of the function values available and measure the quality of predicting in terms of the empirical error on training or test data D,", "creator": "LaTeX with hyperref package"}}}