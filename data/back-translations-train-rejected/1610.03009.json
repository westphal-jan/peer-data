{"id": "1610.03009", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Investigation of Synthetic Speech Detection Using Frame- and Segment-Specific Importance Weighting", "abstract": "Speaker verification systems are vulnerable to spoofing attacks which presents a major problem in their real-life deployment. To date, most of the proposed synthetic speech detectors (SSDs) have weighted the importance of different segments of speech equally. However, different attack methods have different strengths and weaknesses and the traces that they leave may be short or long term acoustic artifacts. Moreover, those may occur for only particular phonemes or sounds. Here, we propose three algorithms that weigh likelihood-ratio scores of individual frames, phonemes, and sound-classes depending on their importance for the SSD. Significant improvement over the baseline system has been obtained for known attack methods that were used in training the SSDs. However, improvement with unknown attack types was not substantial. Thus, the type of distortions that were caused by the unknown systems were different and could not be captured better with the proposed SSD compared to the baseline SSD.", "histories": [["v1", "Mon, 10 Oct 2016 18:03:29 GMT  (156kb,D)", "http://arxiv.org/abs/1610.03009v1", null]], "reviews": [], "SUBJECTS": "cs.SD cs.CL", "authors": ["ali khodabakhsh", "cenk demiroglu"], "accepted": false, "id": "1610.03009"}, "pdf": {"name": "1610.03009.pdf", "metadata": {"source": "CRF", "title": "Investigation of Synthetic Speech Detection Using Frame- and Segment-Specific Importance Weighting", "authors": ["Ali Khodabakhsh", "Cenk Demiro\u011flu"], "emails": ["ali.khodabakhsh@ozu.edu.tr,", "cenk.demiroglu@ozyegin.edu.tr"], "sections": [{"heading": null, "text": "This year, as never before in the history of a country in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country and a country, in which is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which it is a country, in which is a country, in which is a country, in which is a country, in"}, {"heading": "II. SYNTHETIC SPEECH DETECTORS", "text": "An overview of the proposed system is shown in Fig. 1. Mel Frequency Cepstral Coefficients (MFCC) are first extracted from the speech expression, then the feature vectors are grouped into J groups. In one approach, vectors aligned with the same Gaussian component of a GMM are described. In another approach, feature vectors belonging to the same phoneme or sound class are grouped into a group. Details of the grouping are described in the next section. After grouping, log-likelihood ratio (LLR) detection is performed for each group of feature vectors. To calculate LLR, a GMM is trained for natural language and a GMM is trained for synthetic language. The same GMMs are used for all J groups. Once the score of each group is calculated, score fusion is performed with a logistical regression function to calculate the end result S (u)."}, {"heading": "A. Duration-based Weighting", "text": "The distribution of the LLR values at frame level follows in most statements approximately a Gaussian distribution. By averaging the LLR values, as done in Eq.3, assuming Gaussian accuracy, an estimate of the mean with maximum probability (ML) is found. Considering that the ML estimate of the mean of a Gaussian has an estimate variance inversely proportional to the number of observations, the reliability of the detector j increases when Nj increases. To take into account the estimate variance, i.e. the uncertainty of the detector values, we propose the duration-weighted ScoreS \u2032 j = ln (Nj + 1) Sj (4), where ln (.) is the natural logarithm."}, {"heading": "III. FEATURE GROUPING METHODS", "text": "In the phoneme-based approach, each phoneme represents a group, so feature vectors occurring within a particular phoneme type were summarized in the utterance. One of the problems with the phoneme-based approach is that some of the utterances included in the challenge were short (2-3 seconds), meaning that many of the phonemes were not observed in these cases. As broad acoustic sound classes share similar acoustic properties, we suspected that if a system performs poorly in synthesizing a phoneme, it will most likely perform poorly for the other acoustically similar phonemes. So, in order to provide more data for each group, a class-based approach is used for grouping in the second approach. In the class-based approach, five sound classes are used: vowels, nasals, sliding wings, stops, and rest."}, {"heading": "IV. EXPERIMENTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Experiment Setup", "text": "Synthetic speech detectors were trained with 19 MFCCs along with the delta and delta delta features. In short-term analysis, image length was 25 msec and frame rate 10 msec. Bigaussian Voice Activity Detection (VAD) was used where the power of speech and sound images is modeled with single gaussies and the probability ratio detector is used to detect speech images. Synthetic speech detector was based on a 512-component GMM to model natural language. Similarly, synthetic language was modeled with a 512-component GMM. For natural language, GMM training was initialized with k-means clustering. GMM for synthetic language was adapted from the GMM of natural language using a maximum a posteriori (MAP) approach. Synthetic language experiments were performed with GMM that were trained independently of natural language."}, {"heading": "B. Results and Discussion", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "We investigated a multi-detector approach to synthetic speech recognition, in which each detector is focused on a specific acoustic segment; the Gaussian detector performed better in speech conversion attacks; phonem- and class-based detectors performed better in HMM-based synthesis attacks; the normalization of endurance-based characteristics improved the Phonemand-based systems, but not the Gaussian system; the proposed systems performed significantly better than the base system in known attack types; in unknown attacks, the improvement was not significant; the merging of the results of the proposed detectors further improved performance in both known and unknown states; and our goal in this paper was to take a commonly used probability ratio based on FCD and use it in a segment-specific manner. The hypothesis here was that different segments should contribute different amounts of information and their results should be weighted accordingly; the results confirmed our hypothesis."}, {"heading": "ACKNOWLEDGMENT", "text": "This work was supported by the TUBITAK 1001 scholarship no. 112E160."}], "references": [{"title": "Spoofing and countermeasures for speaker verification: a survey", "author": ["Z. Wu", "N. Evans", "T. Kinnunen", "J. Yamagishi", "F. Alegre", "H. Li"], "venue": "Speech Communication, vol. 66, pp. 130\u2013153, 2015.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2015}, {"title": "A robust speaker verification system against imposture using an hmm-based speech synthesis system.", "author": ["T. Satoh", "T. Masuko", "T. Kobayashi", "K. Tokuda"], "venue": "INTERSPEECH,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2001}, {"title": "A speech parameter generation algorithm considering global variance for hmm-based speech synthesis", "author": ["T. Tomoki", "K. Tokuda"], "venue": "IEICE TRANSACTIONS on Information and Systems, vol. 90, no. 5, pp. 816\u2013 824, 2007.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2007}, {"title": "Speaker verification against synthetic speech", "author": ["L.-W. Chen", "W. Guo", "L.-R. Dai"], "venue": "Chinese Spoken Language Processing (ISCSLP), 2010 7th International Symposium on. IEEE, 2010, pp. 309\u2013312.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2010}, {"title": "Evaluation of speaker verification security and detection of hmmbased synthetic speech", "author": ["P.L. De Leon", "M. Pucher", "J. Yamagishi", "I. Hernaez", "I. Saratxaga"], "venue": "Audio, Speech, and Language Processing, IEEE Transactions on, vol. 20, no. 8, pp. 2280\u20132290, 2012.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2012}, {"title": "Discrimination method of synthetic speech using pitch frequency against synthetic speech falsification", "author": ["A. Ogihara", "U. Hitoshi", "A. Shiozaki"], "venue": "IEICE transactions on fundamentals of electronics, communications and computer sciences, vol. 88, no. 1, pp. 280\u2013286, 2005.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Synthetic speech discrimination using pitch pattern statistics derived from image analysis.", "author": ["P.L. De Leon", "B. Stewart", "J. Yamagishi"], "venue": "INTERSPEECH,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition.", "author": ["Z. Wu", "C.E. Siong", "H. Li"], "venue": "INTERSPEECH,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2012}, {"title": "A study on spoofing attack in state-of-the-art speaker verification: the telephone speech case", "author": ["Z. Wu", "T. Kinnunen", "E.S. Chng", "H. Li", "E. Ambikairajah"], "venue": "Signal & Information Processing Association Annual Summit and Conference (APSIPA ASC), 2012 Asia-Pacific. IEEE, 2012, pp. 1\u20135.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2012}, {"title": "Spoofing countermeasures to protect automatic speaker verification from voice conversion", "author": ["F. Alegre", "A. Amehraye", "N. Evans"], "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. IEEE, 2013, pp. 3068\u20133072.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "A one-class classification approach to generalised speaker verification spoofing countermeasures using local binary patterns", "author": ["\u2014\u2014"], "venue": "Biometrics: Theory, Applications and Systems (BTAS), 2013 IEEE Sixth International Conference on. IEEE, 2013, pp. 1\u20138.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2013}, {"title": "Joint speaker verification and antispoofing in the i -vector space", "author": ["A. Sizov", "E. Khoury", "T. Kinnunen", "Z. Wu", "S. Marcel"], "venue": "Information Forensics and Security, IEEE Transactions on, vol. 10, no. 4, pp. 821\u2013 832, April 2015.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2015}, {"title": "Phoneme recognizer based on long temporal context", "author": ["P. Schwarz", "P. Matejka", "L. Burget", "O. Glembek"], "venue": "Speech Processing Group, Faculty of Information Technology, Brno University of Technology.[Online]. Available: http://speech. fit. vutbr. cz/en/software, 2006.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2006}, {"title": "The bosaris toolkit: Theory, algorithms and code for surviving the new dcf", "author": ["N. Br\u00fcmmer", "E. de Villiers"], "venue": "arXiv preprint arXiv:1304.2865, 2013.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "Even though verification of speaker identity through human voice has been shown to be successful [1], state-of-the art verification systems have been shown to be vulnerable to spoofing attacks using speech synthesis and voice conversion [2].", "startOffset": 237, "endOffset": 240}, {"referenceID": 1, "context": "Even though HMM-based synthesis can successfully spoof the modern verification systems, it is also easy to detect by exploiting the unnaturally smooth trajectories of the parameters [3][4][5].", "startOffset": 182, "endOffset": 185}, {"referenceID": 2, "context": "Even though HMM-based synthesis can successfully spoof the modern verification systems, it is also easy to detect by exploiting the unnaturally smooth trajectories of the parameters [3][4][5].", "startOffset": 185, "endOffset": 188}, {"referenceID": 3, "context": "Even though HMM-based synthesis can successfully spoof the modern verification systems, it is also easy to detect by exploiting the unnaturally smooth trajectories of the parameters [3][4][5].", "startOffset": 188, "endOffset": 191}, {"referenceID": 4, "context": "Moreover, because the vocoder typically has a minimum-phase filter, phase was also used for detecting HMM-based synthesis since natural speech spectrum is not minimum phase [6].", "startOffset": 173, "endOffset": 176}, {"referenceID": 5, "context": "Existing synthetic speech detectors (SSDs) typically use jumps in fundamental frequency at the concatenation points for detection [7][8].", "startOffset": 130, "endOffset": 133}, {"referenceID": 6, "context": "Existing synthetic speech detectors (SSDs) typically use jumps in fundamental frequency at the concatenation points for detection [7][8].", "startOffset": 133, "endOffset": 136}, {"referenceID": 0, "context": "Voice conversion algorithms can also be used for spoofing [2].", "startOffset": 58, "endOffset": 61}, {"referenceID": 7, "context": "Because they typically use minimum-phase vocoders, phase was used in [9][10] for detecting voice-converted speech.", "startOffset": 69, "endOffset": 72}, {"referenceID": 8, "context": "Because they typically use minimum-phase vocoders, phase was used in [9][10] for detecting voice-converted speech.", "startOffset": 72, "endOffset": 76}, {"referenceID": 9, "context": "Moreover, some voice conversion systems exhibit low parameter variability across an utterance compared to natural speech and that was also exploited for detecting voice conversion [11].", "startOffset": 180, "endOffset": 184}, {"referenceID": 10, "context": "One promising approach is to use a local binary pattern (LBP) analysis for feature extraction [12].", "startOffset": 94, "endOffset": 98}, {"referenceID": 11, "context": "In [13], i-vectors are used both for speaker verification and synthetic speech detection.", "startOffset": 3, "endOffset": 7}, {"referenceID": 12, "context": "The Hungarian phoneme recognizer [14] was trained with WSJ-CAM database and used here for phoneme recognition.", "startOffset": 33, "endOffset": 37}, {"referenceID": 13, "context": "The BOSARIS toolkit [15] was used to train the logistic regression algorithm that was used for fusing the scores of detectors.", "startOffset": 20, "endOffset": 24}], "year": 2016, "abstractText": "Speaker verification systems are vulnerable to spoofing attacks which presents a major problem in their real-life deployment. To date, most of the proposed synthetic speech detectors (SSDs) have weighted the importance of different segments of speech equally. However, different attack methods have different strengths and weaknesses and the traces that they leave may be short or long term acoustic artifacts. Moreover, those may occur for only particular phonemes or sounds. Here, we propose three algorithms that weigh likelihood-ratio scores of individual frames, phonemes, and sound-classes depending on their importance for the SSD. Significant improvement over the baseline system has been obtained for known attack methods that were used in training the SSDs. However, improvement with unknown attack types was not substantial. Thus, the type of distortions that were caused by the unknown systems were different and could not be captured better with the proposed SSD compared to the baseline SSD.", "creator": "LaTeX with hyperref package"}}}