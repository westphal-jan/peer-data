{"id": "1610.03090", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Dynamic Metric Learning from Pairwise Comparisons", "abstract": "Recent work in distance metric learning has focused on learning transformations of data that best align with specified pairwise similarity and dissimilarity constraints, often supplied by a human observer. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we address the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the ground-truth clustering used to generate constraints or changes in the feature subspaces in which the class structure is apparent. We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD), a general adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We apply the OCELAD framework to an ensemble of online learners. Specifically, we create a retro-initialized composite objective mirror descent (COMID) ensemble (RICE) consisting of a set of parallel COMID learners with different learning rates, demonstrate RICE-OCELAD on both real and synthetic data sets and show significant performance improvements relative to previously proposed batch and online distance metric learning algorithms.", "histories": [["v1", "Mon, 10 Oct 2016 20:39:25 GMT  (1870kb,D)", "http://arxiv.org/abs/1610.03090v1", "to appear Allerton 2016. arXiv admin note: substantial text overlap witharXiv:1603.03678"]], "COMMENTS": "to appear Allerton 2016. arXiv admin note: substantial text overlap witharXiv:1603.03678", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["kristjan greenewald", "stephen kelley", "alfred hero iii"], "accepted": false, "id": "1610.03090"}, "pdf": {"name": "1610.03090.pdf", "metadata": {"source": "CRF", "title": "Dynamic metric learning from pairwise comparisons", "authors": ["Kristjan Greenewald"], "emails": [], "sections": [{"heading": null, "text": "In fact, at a time when we are able to assert ourselves, we will be able to change the world, and we will be able to change the world, to change the world. \""}, {"heading": "A. Related Work", "text": "This year it is so far that it will only be a matter of time before it is so far, until it is so far."}, {"heading": "II. NONSTATIONARY METRIC LEARNING", "text": "Metric learning aims to learn a metric that encourages close data points that are labeled as similar and distant data points that are labeled as different.The time-variable Mahalanobis distance at time t is determined by Mt asd2Mt (x, z) = (x \u2212 z) TMt (x \u2212 z) (1) where Mt-Rn \u00b7 n 0. Let's assume a temporal sequence of similarity constraints where each constraint is the triple (xt, zt, yt), xt, and zt data points in Rn, and the designation yt = + 1 if the dots xt are similar, zt the time sequence of similarity constraints are similar, and yt = \u2212 1 if they are dissimilar. Following [5] we introduce the following boundary constraints: t | yt = 1: d2Mt (xt, zt) and zt are data points in Rn, and the label yt = 1 if the xt)."}, {"heading": "III. RETRO-INITIALIZED COMID ENSEMBLE (RICE)", "text": "It is not the first time that the number of learners in the fields of learning, learning, learning, learning, learning, education, education, education, education, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture, culture,"}, {"heading": "IV. OCELAD", "text": "(I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I (I) (I) (I) (I) (I) (I (I) (I) (I (I) (I) (I (I) (I) (I) (I (I) (I) (I) (I (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I) (I (I) (I (I) (I) (I (I) (I) (I) (I (I) (I) (I) (I (I) (I (I) (I (I) (I) (I (I) (I) (I (I) (I (I) (I) (I) (I) (I (I) (I) (I) (I) (I (I (I (I (I) (I (I (I) (I) (I) (I) (I (I (I) (I (I) (I (I) (I) (I) (I) (I (I (I) (I) (I (I (I) (I) (I) (I (I) (I) (I (I) (I (I) (I) (I) (I (I) (I) (I (I) (I) (I) (I) (I"}, {"heading": "V. STRONGLY ADAPTIVE DYNAMIC REGRET", "text": "The default static regret is defined as RB, static (I) = \"I ft\" (I) = \"I ft\" (I) = \"I ft\" (I) = \"I ft\" (I) = \"I ft\" (I) = \"I ft\" (I) = \"I ft\" (I) = \"I\" (I) = \"I\" (I) = \"I\" (I) = \"I\" (I) = \"I\" (I) = \"I\" (I) = \"I\" (I) = \"I\" (I) = \"I\" (I).I) = \"I.\" I \"(I) =\" I \"I.\" I \"(I) =\" I. \"I.\" \"I\" (I) = \"I\" I \".I\" (I) = \"I\" I \"(I) =\" I \"T\" (I) = \"I.\""}, {"heading": "VI. RESULTS", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Synthetic Data", "text": "We perform our metric learning algorithms on a synthetic dataset that goes through various types of simulated metric drift. We create a synthetic 2000-point dataset with 2 independent 50-20-30% clusters (A and B) in which all three dimensional sub-spaces of R25 fail completely. The clusters are formed as 3-D Gaussian blobs, and the remaining 19-dimensional sub-spaces are created with iid Gaussian noise.We create a scenario showing nonstationary drift, combining continuous drifts and shifts between the two clusterings (A and B). To simulate a continuous drift, we perform a small random rotation of the datasets in each step. The drift profile is shown in 3. For the first interval, Partition A is used and the dataset is static, no drift occurs. Then the partition is changed to B, followed by an interval of a moderate, then moderate and then moderate drift."}, {"heading": "B. Clustering Product Reviews", "text": "Dre rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the rfu the"}, {"heading": "VII. CONCLUSION AND FUTURE WORK", "text": "However, if the problem of interest or data distribution is not stationary, the optimal metric may vary over time. We looked at the problem of tracking a non-stationary metric and presented an efficient, highly adaptive online algorithm (OCELAD) that combines the results of any black box learning ensemble (such as RICE) and provides strong theoretical guarantees of remorse. Our algorithm's performance was evaluated on both synthetic and real data sets, demonstrating its ability to learn quickly and adapt to changes in both clustering of interest and underlying data distribution. Potential instructions for future work include learning more meaningful metrics beyond Mahalanobis metrics, incorporating unlabeled data points into a semi-monitored learning framework [24] and incorporating an active learning framework to obtain 25 learning points at any given time."}], "references": [{"title": "The elements of statistical learning: data mining, inference and prediction", "author": ["T. Hastie", "R. Tibshirani", "J. Friedman", "J. Franklin"], "venue": "The Mathematical Intelligencer, vol. 27, no. 2, pp. 83\u201385, 2005.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2005}, {"title": "Nonlinear dimensionality reduction", "author": ["J.A. Lee", "M. Verleysen"], "venue": "Springer Science & Business Media,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2007}, {"title": "Metric learning: A survey.", "author": ["B. Kulis"], "venue": "Foundations and Trends in Machine Learning,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2012}, {"title": "Mirror descent for metric learning: a unified approach", "author": ["G. Kunapuli", "J. Shavlik"], "venue": "Machine Learning and Knowledge Discovery in Databases. Springer, 2012, pp. 859\u2013874.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2012}, {"title": "Distance metric learning with application to clustering with side-information", "author": ["E.P. Xing", "M.I. Jordan", "S. Russell", "A.Y. Ng"], "venue": "Advances in Neural Information Processing Systems, 2002, pp. 505\u2013512.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2002}, {"title": "Distance metric learning for large margin nearest neighbor classification", "author": ["K.Q. Weinberger", "J. Blitzer", "L.K. Saul"], "venue": "Advances in Neural Information Processing System, 2005, pp. 1473\u20131480.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2005}, {"title": "Fast solvers and efficient implementations for distance metric learning", "author": ["K.Q. Weinberger", "L.K. Saul"], "venue": "ICML, 2008, pp. 1160\u20131167.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2008}, {"title": "Neighbourhood components analysis", "author": ["J. Goldberger", "G.E. Hinton", "S.T. Roweis", "R. Salakhutdinov"], "venue": "Advances in neural information processing systems, 2004, pp. 513\u2013520.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2004}, {"title": "Informationtheoretic metric learning", "author": ["J.V. Davis", "B. Kulis", "P. Jain", "S. Sra", "I.S. Dhillon"], "venue": "ICML, 2007, pp. 209\u2013216.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2007}, {"title": "A survey on metric learning for feature vectors and structured data", "author": ["A. Bellet", "A. Habrard", "M. Sebban"], "venue": "arXiv preprint arXiv:1306.6709, 2013.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2013}, {"title": "Distance metric learning: A comprehensive survey", "author": ["L. Yang", "R. Jin"], "venue": "Michigan State Universiy, vol. 2, 2006.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2006}, {"title": "Prediction, learning, and games", "author": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2006}, {"title": "Composite objective mirror descent", "author": ["J.C. Duchi", "S. Shalev-Shwartz", "Y. Singer", "A. Tewari"], "venue": "COLT. Citeseer, 2010, pp. 14\u201326.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2010}, {"title": "Strongly adaptive online learning", "author": ["A. Daniely", "A. Gonen", "S. Shalev-Shwartz"], "venue": "ICML, 2015.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2015}, {"title": "Adaptive bound optimization for online convex optimization", "author": ["H.B. McMahan", "M. Streeter"], "venue": "COLT, 2010.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2010}, {"title": "Adaptive subgradient methods for online learning and stochastic optimization", "author": ["J.C. Duchi", "E. Hazan", "Y. Singer"], "venue": "COLT, 2010.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2010}, {"title": "Analysis techniques for adaptive online learning", "author": ["H.B. McMahan"], "venue": "arXiv preprint arXiv:1403.3465, 2014.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Tracking the best expert", "author": ["M. Herbster", "M.K. Warmuth"], "venue": "Machine Learning, vol. 32, no. 2, pp. 151\u2013178, 1998.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1998}, {"title": "Adaptive algorithms for online decision problems", "author": ["E. Hazan", "C. Seshadhri"], "venue": "Electronic Colloquium on Computational Complexity (ECCC), vol. 14, no. 088, 2007.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2007}, {"title": "Online convex optimization in dynamic environments", "author": ["E. Hall", "R. Willett"], "venue": "Selected Topics in Signal Processing, IEEE Journal of, vol. 9, no. 4, pp. 647\u2013662, June 2015.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2015}, {"title": "From external to internal regret", "author": ["A. Blum", "Y. Mansour"], "venue": "Learning theory. Springer, 2005, pp. 621\u2013636.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2005}, {"title": "Biographies, bollywood, boomboxes and blenders: Domain adaptation for sentiment classification", "author": ["J. Blitzer", "M. Dredze", "F. Pereira"], "venue": "ACL, vol. 7, 2007, pp. 440\u2013447.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2007}, {"title": "Integrating constraints and metric learning in semi-supervised clustering", "author": ["M. Bilenko", "S. Basu", "R.J. Mooney"], "venue": "ICML, 2004, p. 11.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2004}, {"title": "Active learning", "author": ["B. Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning, vol. 6, no. 1, pp. 1\u2013114, 2012.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "Examples of such techniques include Principal Component Analysis [1], Multidimensional Scaling [2], covariance estimation [2], [1], and manifold learning [3].", "startOffset": 95, "endOffset": 98}, {"referenceID": 0, "context": "Examples of such techniques include Principal Component Analysis [1], Multidimensional Scaling [2], covariance estimation [2], [1], and manifold learning [3].", "startOffset": 122, "endOffset": 125}, {"referenceID": 1, "context": "Examples of such techniques include Principal Component Analysis [1], Multidimensional Scaling [2], covariance estimation [2], [1], and manifold learning [3].", "startOffset": 154, "endOffset": 157}, {"referenceID": 2, "context": "Many supervised and semi-supervised distance metric learning approaches have been developed [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 3, "context": "This includes online algorithms [5] with regret guarantees for situations where similarity constraints are received sequentially.", "startOffset": 32, "endOffset": 35}, {"referenceID": 4, "context": "MMC [6], a method for identifying a Mahalanobis metric for clustering with side information, uses semidefinite programming to identify a metric that maximizes the sum of distances between points labeled with different classes subject to the constraint that the sum of distances between all points with similar labels be less than some constant.", "startOffset": 4, "endOffset": 7}, {"referenceID": 5, "context": "Large Margin Nearest Neighbor (LMNN) [7] similarly uses semidefinite programming to identify a Mahalanobis distance.", "startOffset": 37, "endOffset": 40}, {"referenceID": 6, "context": "This method has been shown to be computationally efficient [8] and, in contrast to the similarly motivated Neighborhood Component Analysis [9], is guaranteed to converge to a globally optimal solution.", "startOffset": 59, "endOffset": 62}, {"referenceID": 7, "context": "This method has been shown to be computationally efficient [8] and, in contrast to the similarly motivated Neighborhood Component Analysis [9], is guaranteed to converge to a globally optimal solution.", "startOffset": 139, "endOffset": 142}, {"referenceID": 8, "context": "Information Theoretic Metric Learning (ITML) [10] is another popular Distance Metric Learning technique.", "startOffset": 45, "endOffset": 49}, {"referenceID": 2, "context": "For surveys of the vast metric learning literature, see [4], [11], [12].", "startOffset": 56, "endOffset": 59}, {"referenceID": 9, "context": "For surveys of the vast metric learning literature, see [4], [11], [12].", "startOffset": 61, "endOffset": 65}, {"referenceID": 10, "context": "For surveys of the vast metric learning literature, see [4], [11], [12].", "startOffset": 67, "endOffset": 71}, {"referenceID": 11, "context": "Online learning [13] meets these criteria by efficiently updating the estimate every time a new data point is obtained, instead of solving an objective function formed from the entire dataset.", "startOffset": 16, "endOffset": 20}, {"referenceID": 11, "context": "Many online learning methods have regret guarantees, that is, the loss in performance relative to a batch method is provably small [13], [14].", "startOffset": 131, "endOffset": 135}, {"referenceID": 12, "context": "Many online learning methods have regret guarantees, that is, the loss in performance relative to a batch method is provably small [13], [14].", "startOffset": 137, "endOffset": 141}, {"referenceID": 13, "context": "In practice, however, the performance of an online learning method is strongly influenced by the learning rate, which may need to vary over time in a dynamic environment [15], [16], [17], especially one with changing drift rates.", "startOffset": 170, "endOffset": 174}, {"referenceID": 14, "context": "In practice, however, the performance of an online learning method is strongly influenced by the learning rate, which may need to vary over time in a dynamic environment [15], [16], [17], especially one with changing drift rates.", "startOffset": 176, "endOffset": 180}, {"referenceID": 15, "context": "In practice, however, the performance of an online learning method is strongly influenced by the learning rate, which may need to vary over time in a dynamic environment [15], [16], [17], especially one with changing drift rates.", "startOffset": 182, "endOffset": 186}, {"referenceID": 14, "context": "For learning static parameters, AdaGrad-style methods [16], [17] perform gradient descent steps with the step size adapted based on the magnitude of recent gradients.", "startOffset": 54, "endOffset": 58}, {"referenceID": 15, "context": "For learning static parameters, AdaGrad-style methods [16], [17] perform gradient descent steps with the step size adapted based on the magnitude of recent gradients.", "startOffset": 60, "endOffset": 64}, {"referenceID": 16, "context": "Follow the regularized leader (FTRL) type algorithms adapt the regularization to the observations [18].", "startOffset": 98, "endOffset": 102}, {"referenceID": 13, "context": "SAOL maintains several learners with different learning rates and randomly selects the best one based on recent performance [15].", "startOffset": 124, "endOffset": 128}, {"referenceID": 16, "context": "Several of these adaptive methods have provable regret bounds [18], [19], [20].", "startOffset": 62, "endOffset": 66}, {"referenceID": 17, "context": "Several of these adaptive methods have provable regret bounds [18], [19], [20].", "startOffset": 68, "endOffset": 72}, {"referenceID": 18, "context": "Several of these adaptive methods have provable regret bounds [18], [19], [20].", "startOffset": 74, "endOffset": 78}, {"referenceID": 16, "context": "regret from time 0 to time T ) at every time [18].", "startOffset": 45, "endOffset": 49}, {"referenceID": 13, "context": "SAOL, on the other hand, attempts to have low static regret on every subinterval, as well as low regret overall [15].", "startOffset": 112, "endOffset": 116}, {"referenceID": 3, "context": "Following [5], we introduce the following margin based constraints:", "startOffset": 10, "endOffset": 13}, {"referenceID": 3, "context": "Kunapuli and Shavlik [5] propose using nuclear norm regularization (r(M) = \u2016M\u2016\u2217) to encourage projection of the data onto a low dimensional subspace (feature selection/dimensionality reduction), and we have also had success with the elementwise L1 norm (r(M) = \u2016vec(M)\u20161).", "startOffset": 21, "endOffset": 24}, {"referenceID": 3, "context": "Viewing the acquisition of new data points as stochastic realizations of the underlying distribution [5] suggests the use of composite objective stochastic mirror descent techniques", "startOffset": 101, "endOffset": 104}, {"referenceID": 12, "context": "For the loss (3) and learning rate \u03b7t, COMID [14] gives", "startOffset": 45, "endOffset": 49}, {"referenceID": 3, "context": "In [5] a closedform algorithm for solving the minimization in (4) with", "startOffset": 3, "endOffset": 6}, {"referenceID": 19, "context": "Critically, the optimal learning rate \u03b7t depends on the rate of change of Mt [21], and thus will need to change with time to adapt to nonstationary drift.", "startOffset": 77, "endOffset": 81}, {"referenceID": 13, "context": ", with an arrangement that is a dyadic partition of the temporal axis, as in [15].", "startOffset": 77, "endOffset": 81}, {"referenceID": 20, "context": "The weight update we use is inspired by the multiplicative weight (MW) literature [22], modified to allow for unbounded loss functions.", "startOffset": 82, "endOffset": 86}, {"referenceID": 20, "context": "The weighted average of the ensemble is reasonable here due to our use of a convex loss function (proven in the next section), as opposed to the possibly non-convex losses of [22], necessitating a randomized selection approach.", "startOffset": 175, "endOffset": 179}, {"referenceID": 19, "context": "In [21] the authors derive dynamic regret bounds that hold over all possible sequences w such that \u2211 t\u2208I \u2016\u03b8t+1\u2212\u03b8t\u2016 \u2264 \u03b3, i.", "startOffset": 3, "endOffset": 7}, {"referenceID": 13, "context": "Strongly adaptive regret bounds [15] have claimed that static regret is low on every subinterval, instead of only low in the aggregate.", "startOffset": 32, "endOffset": 36}, {"referenceID": 19, "context": "The proof of the following result is omitted for lack of space, and derives from a result in [21].", "startOffset": 93, "endOffset": 97}, {"referenceID": 3, "context": "Metric tracking performance is computed for RICE-OCELAD (adaptive), nonadaptive COMID [5] (high learning rate), nonadaptive COMID (low learning rate), the batch solution (LMNN) [7], SAOL [15] and online ITML [10], averaged over 3000 random trials.", "startOffset": 86, "endOffset": 89}, {"referenceID": 5, "context": "Metric tracking performance is computed for RICE-OCELAD (adaptive), nonadaptive COMID [5] (high learning rate), nonadaptive COMID (low learning rate), the batch solution (LMNN) [7], SAOL [15] and online ITML [10], averaged over 3000 random trials.", "startOffset": 177, "endOffset": 180}, {"referenceID": 13, "context": "Metric tracking performance is computed for RICE-OCELAD (adaptive), nonadaptive COMID [5] (high learning rate), nonadaptive COMID (low learning rate), the batch solution (LMNN) [7], SAOL [15] and online ITML [10], averaged over 3000 random trials.", "startOffset": 187, "endOffset": 191}, {"referenceID": 8, "context": "Metric tracking performance is computed for RICE-OCELAD (adaptive), nonadaptive COMID [5] (high learning rate), nonadaptive COMID (low learning rate), the batch solution (LMNN) [7], SAOL [15] and online ITML [10], averaged over 3000 random trials.", "startOffset": 208, "endOffset": 212}, {"referenceID": 13, "context": "In our results, we consider RICE-OCELAD, SAOL with COMID [15], nonadaptive COMID [5], LMNN (batch) [7], and online ITML [10].", "startOffset": 57, "endOffset": 61}, {"referenceID": 3, "context": "In our results, we consider RICE-OCELAD, SAOL with COMID [15], nonadaptive COMID [5], LMNN (batch) [7], and online ITML [10].", "startOffset": 81, "endOffset": 84}, {"referenceID": 5, "context": "In our results, we consider RICE-OCELAD, SAOL with COMID [15], nonadaptive COMID [5], LMNN (batch) [7], and online ITML [10].", "startOffset": 99, "endOffset": 102}, {"referenceID": 8, "context": "In our results, we consider RICE-OCELAD, SAOL with COMID [15], nonadaptive COMID [5], LMNN (batch) [7], and online ITML [10].", "startOffset": 120, "endOffset": 124}, {"referenceID": 8, "context": "Online ITML fails due to its bias agains low-rank solutions [10], and the batch method and low learning rate COMID fail due to an inability to adapt.", "startOffset": 60, "endOffset": 64}, {"referenceID": 21, "context": "As an example real data task, we consider clustering Amazon text reviews, using the Multi-Domain Sentiment Dataset [23].", "startOffset": 115, "endOffset": 119}, {"referenceID": 22, "context": "Potential directions for future work include the learning of more expressive metrics beyond the Mahalanobis metric, the incorporation of unlabeled data points in a semi-supervised learning framework [24], and the incorporation of an active learning framework to select which pairs of data points to obtain labels for at any given time [25].", "startOffset": 199, "endOffset": 203}, {"referenceID": 23, "context": "Potential directions for future work include the learning of more expressive metrics beyond the Mahalanobis metric, the incorporation of unlabeled data points in a semi-supervised learning framework [24], and the incorporation of an active learning framework to select which pairs of data points to obtain labels for at any given time [25].", "startOffset": 335, "endOffset": 339}], "year": 2016, "abstractText": "Recent work in distance metric learning has focused on learning transformations of data that best align with specified pairwise similarity and dissimilarity constraints, often supplied by a human observer. The learned transformations lead to improved retrieval, classification, and clustering algorithms due to the better adapted distance or similarity measures. Here, we address the problem of learning these transformations when the underlying constraint generation process is nonstationary. This nonstationarity can be due to changes in either the groundtruth clustering used to generate constraints or changes in the feature subspaces in which the class structure is apparent. We propose Online Convex Ensemble StrongLy Adaptive Dynamic Learning (OCELAD), a general adaptive, online approach for learning and tracking optimal metrics as they change over time that is highly robust to a variety of nonstationary behaviors in the changing metric. We apply the OCELAD framework to an ensemble of online learners. Specifically, we create a retroinitialized composite objective mirror descent (COMID) ensemble (RICE) consisting of a set of parallel COMID learners with different learning rates, demonstrate RICE-OCELAD on both real and synthetic data sets and show significant performance improvements relative to previously proposed batch and online distance metric learning algorithms.", "creator": "LaTeX with hyperref package"}}}