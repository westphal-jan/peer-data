{"id": "1610.03098", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "10-Oct-2016", "title": "Neural Paraphrase Generation with Stacked Residual LSTM Networks", "abstract": "In this paper, we propose a novel neural approach for paraphrase generation. Conventional para- phrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We evaluate our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based and bi- directional LSTM models on BLEU, METEOR, TER and an embedding-based sentence similarity metric.", "histories": [["v1", "Mon, 10 Oct 2016 21:01:00 GMT  (1197kb,D)", "http://arxiv.org/abs/1610.03098v1", "COLING 2016"], ["v2", "Wed, 12 Oct 2016 15:02:02 GMT  (1192kb,D)", "http://arxiv.org/abs/1610.03098v2", "COLING 2016"], ["v3", "Thu, 13 Oct 2016 00:37:33 GMT  (1198kb,D)", "http://arxiv.org/abs/1610.03098v3", "COLING 2016"]], "COMMENTS": "COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["aaditya prakash", "sadid a hasan", "kathy lee", "vivek datla", "ashequl qadir", "joey liu", "oladimeji farri"], "accepted": false, "id": "1610.03098"}, "pdf": {"name": "1610.03098.pdf", "metadata": {"source": "CRF", "title": "Neural Paraphrase Generation with Stacked Residual LSTM Networks", "authors": ["Aaditya Prakash", "Sadid A. Hasan", "Kathy Lee", "Vivek Datla", "Ashequl Qadir", "Joey Liu", "Oladimeji Farri"], "emails": ["aprakash@{brandeis.edu,philips.com}", "aaditya.prakash@{brandeis.edu,philips.com}", "1@philips.com", "vivek.datla@philips.com", "ashequl.qadir@philips.com", "joey.liu@philips.com", "dimeji.farri@philips.com"], "sections": [{"heading": "1 Introduction", "text": "In this book, we focus on the Paraphrase generation to achieve performance improvements in multiple NLP applications, for example by using text variants or pattern alternatives to express the same meaning as the source code."}, {"heading": "2 Model Description", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "2.1 Encoder-Decoder Model", "text": "A neural approach to sequence modeling, as proposed by Sutskever et al. (2014), is a two-component model in which a source sequence is first encoded into a low-dimensional representation (Figure 1), which is later used to reproduce the sequence back to a high-dimensional target sequence (i.e. decoding). In machine translation, an encoder works with text written in the source language and encodes the meaning of the sentence to be translated before the decoder can take this vector (representing the meaning) and generate a sentence in the target language. In these encoder decoder blocks, it can be either a vanilla RNN or its variants. In generating the target sequence, the generation of each new word depends on the model and the last word generated. For the first word, this is done by attaching a special \"EOS\" (end of sentence) token to the source."}, {"heading": "2.2 Deep LSTM", "text": "An LSTM unit takes three inputs xt, ht \u2212 1, ct \u2212 1 in each time step and produces the hidden state, ht and the internal memory state, ct at each time step. The memory cell is controlled by three learned gates: input i, forgot f and output o. These memory cells use the addition of gradients in relation to time and thus minimize the gradient explosion. In most NLP tasks, LSTM outperforms vanilla RNN (Sundermeyer et al., 2012). Therefore, for our model, we only examine LSTM as a basic unit in the encoder and decoder. Here, we describe the basic calculations in an LSTM unit that will provide the grounding to understand the residual connections between stacked LSTM layers later. In the following equations, the learned parameters for x and h or velyb (multiple element) are \u2212 Wh."}, {"heading": "2.3 Stacked Residual LSTM", "text": "Using theoretical and empirical arguments, He et al. (2015) have shown that the explicit addition of residual x to the learned function enables deeper network training without overloading the data. Stacking multiple layers of neurons often causes the network to suffer from a degradation problem (He et al., 2015). The degradation problem arises from the low convergence rate of training errors and differs from the problem of disappearing gradients. Residual connections can help overcome this problem. We experimented with four layers of stacked LSTM for each model. Residual connections are added to layer two as pointer addition (see Figure 3), and therefore require input in the same dimension as the output of ht."}, {"heading": "3 Datasets", "text": "PPDB (Pavlick et al., 2015) is a well-known dataset used for various NLP tasks. It comes in different sizes and the precision of paraphrases degraded with the size of the dataset. We use the sizeL dataset of PPDB 2.0, which comes with over 18M paraphrases, including lexical, phrasial and syntactic types. We have omitted the syntactic paraphrases and the instances that contain numbers, as they significantly increase the vocabulary size without giving the benefit of a larger dataset. This dataset contains relatively short paraphrases (86% of the data is less than four words), making it suitable for synonym generation and phrase substitution to address lexical and phrasistic paraphrases (Madnani and Dorr, 2010)."}, {"heading": "4 Experimental Settings", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "4.1 Data Selection", "text": "For PPDB, we remove phrases that contain numbers, including all syntactical phrases. This step gives us a total of 5.3M paraphrases from which we randomly select 90% instances for the training, and 20K pairs from the remaining 10% data are similarly selected for the test. Although WikiAnswers contains 29, 139, 992 instances, we randomly select 4, 826, 492 for the training to use the special UNK symbol for the words outside the vocabulary (see Table 1). 20K instances are randomly selected from the remaining data for the test. Note that for the WikiAnswers dataset, we had to cut out the vocabulary size 2 to 50K and use the special UNK symbol for the words outside the vocabulary. The MSCOCO dataset contains five captions for each image. This dataset contains separate captions for the training and validation: train 2014 contains over 82K images and Val 2014 contains over 40K images."}, {"heading": "4.2 Models", "text": "For each model, we experimented with two- and four-layer stacked LSTMs, inspired by state-of-the-art speech recognition systems that also use three to four layers of stacked LSTMs (Li and Wu, 2015). In encoder decoder models, the size of the beam size used in the inference is very important. Larger beam sizes always provide higher accuracy, but require more computational effort. We experimented with beam sizes of 5 and 10 to compare the models, as these are the most commonly used beam sizes in literature (Sutskever et al., 2014). The bidirectional model used half the number of layers shown in other models, to ensure similar parameter sizes across the models. 2WikiAnswers datasets showed many spelling errors that yielded a very large vocabulary (about 502K)."}, {"heading": "4.3 Training", "text": "We used a one-hot-vector approach to represent the words in all models, which were trained using a stochastic gradient descent (SGD) algorithm; the learning rate started at 1.0 and halved after every third training period; each network was trained for ten epochs; after each LSTM shift, a standard dropout (Srivastava et al., 2014) of 50% was applied; the number of LSTM units in each shift was fixed at 512 in all models; in order to allow the exploration of a variety of models, training was limited to a limited number of epochs, and no search for hyperparameters was conducted; the training time ranged from 36 hours for WikiAnswers and PPDB to 14 hours for MSCOCO on a Titan X with CuDNN 5 using the Theano version 0.9.0dev1 (Theano Development Team, 2016).A beam search algorithm was used to generate optimum modelling capability by using the SCOxalgorithm as a very good parameter."}, {"heading": "5 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Metrics", "text": "To quantify the performance of our Paraphrase Generation models, we use the well-known Automatic Assessment Metrics 3 for comparing parallel corpora: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007) and Translation Error Rate (TER) (Snover et al., 2006). Although these metrics are designed for machine translation, previous work has shown that they are good for the paraphrase recognition task (Madnani et al., 2012) and correlate well with human judgement in assessing generated paraphrases (Wubben et al., 2010). Although there are a few Automatic Assessment Metrics designed specifically for the Paraphrase Generation, such as PEM (Paraphrase Evaluation Metric), BLU et al., 2010) and PINC (Paraphrase In Ngram Changes and an, 2011), they have significant limitations."}, {"heading": "5.2 Results", "text": "Although we focus on stacked residual values, which are only applicable if there are more than two layers, we still present the values of two-layer LSTM as a starting point, which provides a good comparison with deeper models. Results show that our proposed model outperforms other models on BLEU and TER for all data sets. On Emb Greedy, our model performs better in all data sets than other models except for the attention model when the beam size is 10. On METEOR, our model outperforms other models on MSCOCO and WikiAnswers; however, for PPPDB, the simple sequence on the sequence model shows better values. Note that these results were obtained from the use of individual models and no interaction of models was used. To calculate BLEU and METEOR, four references were used for MSCOCO and five for PDB and WikiAnswers."}, {"heading": "5.3 Analysis", "text": "As shown in Figure 5, more than fifty percent of PPDBs contain two or fewer words. This leads to a big difference between training and validation errors, as shown in Figure 4. Results show that lower LSTMs get better and better than flat ones. At beam size 5, our model beats other models in all data sets. At beam size 10, the attention model has a slightly better Emb Greedy Score than our model. If we look at the qualitative results, we find that the bias in the data set is exploited by the system, which is a side effect of any form of learning based on a limited data set. We can see this effect in Table 5. For example, if we look at an OBJECT metaphor, we can see that the bias in the data set is exploited by the system, which is a side effect of any form of learning based on a limited data set."}, {"heading": "6 Related Work", "text": "This year it has come to the point where it will be able to retaliate, \"he said in an interview with the German Press Agency.\" We have never hesitated so long to find a solution, \"he said.\" But we have never waited so long to find a solution. \""}, {"heading": "7 Conclusion and Future Work", "text": "We have shown that stacking remaining LSTM layers is useful for generating paraphrases, but may not be equally good for machine translation because not every word in a source sequence needs to be replaced by paraphrases. Residual connections help preserve important words in the generated paraphrases. We experimented with three different large-scale datasets and reported on results using various automatic evaluation metrics. We demonstrated the use of the well-known MSCOCO dataset for generating paraphrases and demonstrated that models can be trained effectively without the benefit of reconstructing the generated images. Regardless of this limitation, our model has achieved equally good results on these datasets. We believe that we have set strong bases for generating paraphrases to compare them with future supraphrases."}, {"heading": "Acknowledgment", "text": "The authors would like to thank the anonymous reviewers for their valuable comments and feedback. The first author especially thanks Prof. James Storer, Brandeis University, for his guidance and Nick Moran, Brandeis University, for helpful discussions."}], "references": [{"title": "Neural Machine Translation by Jointly Learning to Align and Translate", "author": ["References D. Bahdanau", "K. Cho", "Y. Bengio."], "venue": "Proceedings of ICLR, pages 1\u201315.", "citeRegEx": "Bahdanau et al\\.,? 2015", "shortCiteRegEx": "Bahdanau et al\\.", "year": 2015}, {"title": "Paraphrasing with Bilingual Parallel Corpora", "author": ["C. Bannard", "C. Callison-Burch."], "venue": "Proceedings of ACL, pages 597\u2013604.", "citeRegEx": "Bannard and Callison.Burch.,? 2005", "shortCiteRegEx": "Bannard and Callison.Burch.", "year": 2005}, {"title": "Learning Long-Term Dependencies with Gradient Descent is Difficult", "author": ["Y. Bengio", "P. Simard", "P. Frasconi."], "venue": "IEEE Transactions on Neural Networks, 5(2):157\u2013166.", "citeRegEx": "Bengio et al\\.,? 1994", "shortCiteRegEx": "Bengio et al\\.", "year": 1994}, {"title": "Collecting Highly Parallel Data for Paraphrase Evaluation", "author": ["D. Chen", "W.B. Dolan."], "venue": "Proceedings of ACL-HLT, pages 190\u2013200.", "citeRegEx": "Chen and Dolan.,? 2011", "shortCiteRegEx": "Chen and Dolan.", "year": 2011}, {"title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "author": ["K. Cho", "B. van Merrienboer", "C. Gulcehre", "D. Bahdanau", "F. Bougares", "H. Schwenk", "Y. Bengio."], "venue": "Proceedings of EMNLP, pages 1724\u20131734.", "citeRegEx": "Cho et al\\.,? 2014", "shortCiteRegEx": "Cho et al\\.", "year": 2014}, {"title": "Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability", "author": ["J.H. Clark", "C. Dyer", "A. Lavie", "N.A. Smith."], "venue": "Proceedings of ACL-HLT, pages 176\u2013181.", "citeRegEx": "Clark et al\\.,? 2011", "shortCiteRegEx": "Clark et al\\.", "year": 2011}, {"title": "Microsoft Research Paraphrase Corpus", "author": ["B. Dolan", "C. Brockett", "C. Quirk."], "venue": "Retrieved March, 29:2008.", "citeRegEx": "Dolan et al\\.,? 2005", "shortCiteRegEx": "Dolan et al\\.", "year": 2005}, {"title": "Paraphrase-Driven Learning for Open Question Answering", "author": ["A. Fader", "L. S Zettlemoyer", "O. Etzioni."], "venue": "ACL, pages 1608\u20131618. ACL.", "citeRegEx": "Fader et al\\.,? 2013", "shortCiteRegEx": "Fader et al\\.", "year": 2013}, {"title": "Learning Hierarchical Features for Scene Labeling", "author": ["C. Farabet", "C. Couprie", "L. Najman", "Y. LeCun."], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1915\u20131929.", "citeRegEx": "Farabet et al\\.,? 2013", "shortCiteRegEx": "Farabet et al\\.", "year": 2013}, {"title": "Hybrid Speech Recognition with Deep Bidirectional LSTM", "author": ["A. Graves", "N. Jaitly", "A. Mohamed."], "venue": "Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on, pages 273\u2013278. IEEE.", "citeRegEx": "Graves et al\\.,? 2013", "shortCiteRegEx": "Graves et al\\.", "year": 2013}, {"title": "Neural Turing Machines", "author": ["A. Graves", "G. Wayne", "I. Danihelka."], "venue": "arXiv:1410.5401.", "citeRegEx": "Graves et al\\.,? 2014", "shortCiteRegEx": "Graves et al\\.", "year": 2014}, {"title": "Generating Sequences with Recurrent Neural Networks", "author": ["A. Graves."], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "UNT: SubFinder: Combining Knowledge Sources for Automatic Lexical Substitution", "author": ["S. Hassan", "A. Csomai", "C. Banea", "R. Sinha", "R. Mihalcea."], "venue": "Proceedings of SemEval, pages 410\u2013413.", "citeRegEx": "Hassan et al\\.,? 2007", "shortCiteRegEx": "Hassan et al\\.", "year": 2007}, {"title": "Deep Residual Learning for Image Recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun."], "venue": "arXiv preprint arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Long Short-Term Memory", "author": ["S. Hochreiter", "J. Schmidhuber."], "venue": "Neural Computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Densely Connected Convolutional Networks", "author": ["G. Huang", "Z. Liu", "K.Q. Weinberger."], "venue": "arXiv preprint arXiv:1608.06993.", "citeRegEx": "Huang et al\\.,? 2016", "shortCiteRegEx": "Huang et al\\.", "year": 2016}, {"title": "Skip-Thought Vectors", "author": ["R. Kiros", "Y. Zhu", "R.R. Salakhutdinov", "R. Zemel", "R. Urtasun", "A. Torralba", "S. Fidler."], "venue": "Advances in Neural Information Processing Systems, pages 3294\u20133302.", "citeRegEx": "Kiros et al\\.,? 2015", "shortCiteRegEx": "Kiros et al\\.", "year": 2015}, {"title": "Generating Natural Language Inference Chains", "author": ["V. Kolesnyk", "T. Rockt\u00e4schel", "S. Riedel."], "venue": "CoRR, abs/1606.01404.", "citeRegEx": "Kolesnyk et al\\.,? 2016", "shortCiteRegEx": "Kolesnyk et al\\.", "year": 2016}, {"title": "Generation of Single-sentence Paraphrases from Predicate/Argument Structure Using Lexico-grammatical Resources", "author": ["R. Kozlowski", "K.F. McCoy", "K. Vijay-Shanker."], "venue": "Proceedings of the 2nd International Workshop on Paraphrasing, pages 1\u20138.", "citeRegEx": "Kozlowski et al\\.,? 2003", "shortCiteRegEx": "Kozlowski et al\\.", "year": 2003}, {"title": "METEOR: An Automatic Metric for MT Evaluation with High Levels of Correlation with Human Judgments", "author": ["A. Lavie", "A. Agarwal."], "venue": "Proceedings of the Second Workshop on Statistical Machine Translation, pages 228\u2013231.", "citeRegEx": "Lavie and Agarwal.,? 2007", "shortCiteRegEx": "Lavie and Agarwal.", "year": 2007}, {"title": "Constructing Long Short-term Memory based Deep Recurrent Neural Networks for Large Vocabulary Speech Recognition", "author": ["X. Li", "X. Wu."], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 4520\u20134524. IEEE.", "citeRegEx": "Li and Wu.,? 2015", "shortCiteRegEx": "Li and Wu.", "year": 2015}, {"title": "Microsoft COCO: Common Objects in Context", "author": ["T. Lin", "M. Maire", "S. Belongie", "J. Hays", "P. Perona", "D. Ramanan", "P. Doll\u00e1r", "C.L. Zitnick."], "venue": "European Conference on Computer Vision, pages 740\u2013755. Springer.", "citeRegEx": "Lin et al\\.,? 2014", "shortCiteRegEx": "Lin et al\\.", "year": 2014}, {"title": "PEM: A Paraphrase Evaluation Metric Exploiting Parallel Texts", "author": ["C. Liu", "D. Dahlmeier", "H.T. Ng."], "venue": "Proceedings of EMNLP, pages 923\u2013932.", "citeRegEx": "Liu et al\\.,? 2010", "shortCiteRegEx": "Liu et al\\.", "year": 2010}, {"title": "Generating Phrasal and Sentential Paraphrases: A Survey of Data-driven Methods", "author": ["N. Madnani", "B.J. Dorr."], "venue": "Computational Linguistics, 36(3):341\u2013387.", "citeRegEx": "Madnani and Dorr.,? 2010", "shortCiteRegEx": "Madnani and Dorr.", "year": 2010}, {"title": "Re-examining Machine Translation Metrics for Paraphrase Identification", "author": ["N. Madnani", "J. Tetreault", "M. Chodorow."], "venue": "Proceedings of NAACL-HLT, pages 182\u2013190.", "citeRegEx": "Madnani et al\\.,? 2012", "shortCiteRegEx": "Madnani et al\\.", "year": 2012}, {"title": "Paraphrasing Questions Using Given and New Information", "author": ["K.R. McKeown."], "venue": "Computational Linguistics, 9(1):1\u201310.", "citeRegEx": "McKeown.,? 1983", "shortCiteRegEx": "McKeown.", "year": 1983}, {"title": "Word2Vec", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean."], "venue": "https://code.google.com/p/word2vec. Online; accessed 2014-04\u201315.", "citeRegEx": "Mikolov et al\\.,? 2014", "shortCiteRegEx": "Mikolov et al\\.", "year": 2014}, {"title": "BLEU: A Method for Automatic Evaluation of Machine Translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W. Zhu."], "venue": "Proceedings of ACL, pages 311\u2013318.", "citeRegEx": "Papineni et al\\.,? 2002", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "How to Construct Deep Recurrent Neural Networks", "author": ["R. Pascanu", "C. Gulcehre", "K. Cho", "Y. Bengio."], "venue": "arXiv preprint arXiv:1312.6026.", "citeRegEx": "Pascanu et al\\.,? 2013", "shortCiteRegEx": "Pascanu et al\\.", "year": 2013}, {"title": "LSTM Implementation Explained: apaszke.github.io/lstm-explained.html", "author": ["A. Paszke"], "venue": null, "citeRegEx": "Paszke.,? \\Q2015\\E", "shortCiteRegEx": "Paszke.", "year": 2015}, {"title": "PPDB 2.0: Better paraphrase ranking, fine-grained entailment relations, word embeddings, and style classification", "author": ["E. Pavlick", "P. Rastogi", "J. Ganitkevitch", "B. Van Durme", "C. Callison-Burch"], "venue": "In Proceedings of ACLIJCNLP),", "citeRegEx": "Pavlick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Pavlick et al\\.", "year": 2015}, {"title": "Monolingual Machine Translation for Paraphrase Generation", "author": ["C. Quirk", "C. Brockett", "W. Dolan."], "venue": "Proceedings of EMNLP, pages 142\u2013149.", "citeRegEx": "Quirk et al\\.,? 2004", "shortCiteRegEx": "Quirk et al\\.", "year": 2004}, {"title": "A Comparison of Greedy and Optimal Assessment of Natural Language Student Input using Word-to-Word Similarity Metrics", "author": ["V. Rus", "M. Lintean."], "venue": "Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 157\u2013162. ACL.", "citeRegEx": "Rus and Lintean.,? 2012", "shortCiteRegEx": "Rus and Lintean.", "year": 2012}, {"title": "Multiresolution Recurrent Neural Networks: An Application to Dialogue Response Generation", "author": ["I.V. Serban", "T. Klinger", "G. Tesauro", "K. Talamadupula", "B. Zhou", "Y. Bengio", "A. Courville."], "venue": "arXiv preprint arXiv:1606.00776.", "citeRegEx": "Serban et al\\.,? 2016", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "author": ["K. Simonyan", "A. Zisserman."], "venue": "arXiv preprint arXiv:1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "A Study of Translation Edit Rate with Targeted Human Annotation", "author": ["M. Snover", "B. Dorr", "R. Schwartz", "L. Micciulla", "J. Makhoul."], "venue": "Proceedings of Association for Machine Translation in the Americas, pages 223\u2013231.", "citeRegEx": "Snover et al\\.,? 2006", "shortCiteRegEx": "Snover et al\\.", "year": 2006}, {"title": "Dynamic Pooling and Unfolding Recursive Autoencoders for Paraphrase Detection", "author": ["R. Socher", "E.H. Huang", "J. Pennington", "A.Y. Ng", "C.D. Manning."], "venue": "Advances in Neural Information Processing Systems, pages 1\u20139.", "citeRegEx": "Socher et al\\.,? 2011", "shortCiteRegEx": "Socher et al\\.", "year": 2011}, {"title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "author": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov."], "venue": "Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "End-to-End Memory Networks", "author": ["S. Sukhbaatar", "J. Weston", "R. Fergus"], "venue": "In Advances in neural information processing systems,", "citeRegEx": "Sukhbaatar et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Sukhbaatar et al\\.", "year": 2015}, {"title": "LSTM Neural Networks for Language Modeling", "author": ["M. Sundermeyer", "R. Schl\u00fcter", "H. Ney."], "venue": "Interspeech, pages 194\u2013197.", "citeRegEx": "Sundermeyer et al\\.,? 2012", "shortCiteRegEx": "Sundermeyer et al\\.", "year": 2012}, {"title": "Generating Text with Recurrent Neural Networks", "author": ["I. Sutskever", "J. Martens", "G.E. Hinton."], "venue": "Proceedings of ICML, pages 1017\u20131024.", "citeRegEx": "Sutskever et al\\.,? 2011", "shortCiteRegEx": "Sutskever et al\\.", "year": 2011}, {"title": "Sequence to Sequence Learning with Neural Networks", "author": ["I. Sutskever", "O. Vinyals", "Q.V. Le."], "venue": "Annual Conference on Neural Information Processing Systems, pages 3104\u20133112.", "citeRegEx": "Sutskever et al\\.,? 2014", "shortCiteRegEx": "Sutskever et al\\.", "year": 2014}, {"title": "Theano: A Python Framework for Fast Computation of Mathematical Expressions", "author": ["Theano Development Team."], "venue": "arXiv e-prints, abs/1605.02688, May.", "citeRegEx": "Team.,? 2016", "shortCiteRegEx": "Team.", "year": 2016}, {"title": "Full Resolution Image Compression with Recurrent Neural Networks", "author": ["G. Toderici", "D. Vincent", "N. Johnston", "S.J. Hwang", "D. Minnen", "J. Shor", "M. Covell."], "venue": "arXiv preprint arXiv:1608.05148.", "citeRegEx": "Toderici et al\\.,? 2016", "shortCiteRegEx": "Toderici et al\\.", "year": 2016}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan."], "venue": "CoRR, abs/1411.4555.", "citeRegEx": "Vinyals et al\\.,? 2014", "shortCiteRegEx": "Vinyals et al\\.", "year": 2014}, {"title": "Grammar as a Foreign Language", "author": ["O. Vinyals", "L. Kaiser", "T. Koo", "S. Petrov", "I. Sutskever", "G. Hinton."], "venue": "Advances in Neural Information Processing Systems, pages 2773\u20132781.", "citeRegEx": "Vinyals et al\\.,? 2015", "shortCiteRegEx": "Vinyals et al\\.", "year": 2015}, {"title": "From Paraphrase Database to Compositional Paraphrase Model and Back", "author": ["J. Wieting", "M. Bansal", "K. Gimpel", "K. Livescu."], "venue": "Transactions of the ACL (TACL).", "citeRegEx": "Wieting et al\\.,? 2015", "shortCiteRegEx": "Wieting et al\\.", "year": 2015}, {"title": "Paraphrase Generation As Monolingual Translation: Data and Evaluation", "author": ["S. Wubben", "A. van den Bosch", "E. Krahmer."], "venue": "Proceedings of INLG, pages 203\u2013207.", "citeRegEx": "Wubben et al\\.,? 2010", "shortCiteRegEx": "Wubben et al\\.", "year": 2010}, {"title": "Convolutional Neural Network for Paraphrase Identification", "author": ["W. Yin", "H. Sch\u00fctze."], "venue": "Proceedings of NAACL-HLT, pages 901\u2013911.", "citeRegEx": "Yin and Sch\u00fctze.,? 2015", "shortCiteRegEx": "Yin and Sch\u00fctze.", "year": 2015}, {"title": "Combining Multiple Resources to Improve SMT-based Paraphrasing Model", "author": ["S. Zhao", "C. Niu", "M. Zhou", "T. Liu", "S. Li."], "venue": "Proceedings of ACL-HLT, pages 1021\u20131029.", "citeRegEx": "Zhao et al\\.,? 2008", "shortCiteRegEx": "Zhao et al\\.", "year": 2008}, {"title": "Application-driven Statistical Paraphrase Generation", "author": ["S. Zhao", "X. Lan", "T. Liu", "S. Li."], "venue": "Proceedings of ACL-IJCNLP, pages 834\u2013842.", "citeRegEx": "Zhao et al\\.,? 2009", "shortCiteRegEx": "Zhao et al\\.", "year": 2009}, {"title": "Leveraging Multiple MT Engines for Paraphrase Generation", "author": ["S. Zhao", "H. Wang", "X. Lan", "T. Liu."], "venue": "Proceedings of COLING, pages 1326\u20131334.", "citeRegEx": "Zhao et al\\.,? 2010", "shortCiteRegEx": "Zhao et al\\.", "year": 2010}], "referenceMentions": [{"referenceID": 23, "context": "to generate a reference paraphrase given a source textual unit) (Madnani and Dorr, 2010).", "startOffset": 64, "endOffset": 88}, {"referenceID": 23, "context": "Paraphrase generation has been used to gain performance improvements in several NLP applications, for example, by generating query variants or pattern alternatives for information retrieval, information extraction or question answering systems, by creating reference paraphrases for automatic evaluation of machine translation and document summarization systems, and by generating concise or simplified information for sentence compression or sentence simplification systems (Madnani and Dorr, 2010).", "startOffset": 475, "endOffset": 499}, {"referenceID": 25, "context": "Traditional paraphrase generation methods exploit hand-crafted rules (McKeown, 1983) or automatically learned complex paraphrase patterns (Zhao et al.", "startOffset": 69, "endOffset": 84}, {"referenceID": 50, "context": "Traditional paraphrase generation methods exploit hand-crafted rules (McKeown, 1983) or automatically learned complex paraphrase patterns (Zhao et al., 2009), use thesaurus-based (Hassan et al.", "startOffset": 138, "endOffset": 157}, {"referenceID": 12, "context": ", 2009), use thesaurus-based (Hassan et al., 2007) or semantic analysis driven natural language generation approaches (Kozlowski et al.", "startOffset": 29, "endOffset": 50}, {"referenceID": 18, "context": ", 2007) or semantic analysis driven natural language generation approaches (Kozlowski et al., 2003), or leverage statistical machine learning theory (Quirk et al.", "startOffset": 75, "endOffset": 99}, {"referenceID": 31, "context": ", 2003), or leverage statistical machine learning theory (Quirk et al., 2004; Wubben et al., 2010).", "startOffset": 57, "endOffset": 98}, {"referenceID": 47, "context": ", 2003), or leverage statistical machine learning theory (Quirk et al., 2004; Wubben et al., 2010).", "startOffset": 57, "endOffset": 98}, {"referenceID": 41, "context": "Recently, techniques like sequence to sequence learning (Sutskever et al., 2014) have been applied to various NLP tasks with promising results, for example, in the areas of machine translation (Cho et al.", "startOffset": 56, "endOffset": 80}, {"referenceID": 4, "context": ", 2014) have been applied to various NLP tasks with promising results, for example, in the areas of machine translation (Cho et al., 2014; Bahdanau et al., 2015), speech recognition (Li and Wu, 2015), language modeling (Vinyals et al.", "startOffset": 120, "endOffset": 161}, {"referenceID": 0, "context": ", 2014) have been applied to various NLP tasks with promising results, for example, in the areas of machine translation (Cho et al., 2014; Bahdanau et al., 2015), speech recognition (Li and Wu, 2015), language modeling (Vinyals et al.", "startOffset": 120, "endOffset": 161}, {"referenceID": 20, "context": ", 2015), speech recognition (Li and Wu, 2015), language modeling (Vinyals et al.", "startOffset": 28, "endOffset": 45}, {"referenceID": 45, "context": ", 2015), speech recognition (Li and Wu, 2015), language modeling (Vinyals et al., 2015), and dialogue systems (Serban et al.", "startOffset": 65, "endOffset": 87}, {"referenceID": 33, "context": ", 2015), and dialogue systems (Serban et al., 2016).", "startOffset": 30, "endOffset": 51}, {"referenceID": 36, "context": "There are several works on paraphrase recognition (Socher et al., 2011; Yin and Sch\u00fctze, 2015; Kiros et al., 2015), but those employ classification techniques and do not attempt to generate paraphrases.", "startOffset": 50, "endOffset": 114}, {"referenceID": 48, "context": "There are several works on paraphrase recognition (Socher et al., 2011; Yin and Sch\u00fctze, 2015; Kiros et al., 2015), but those employ classification techniques and do not attempt to generate paraphrases.", "startOffset": 50, "endOffset": 114}, {"referenceID": 16, "context": "There are several works on paraphrase recognition (Socher et al., 2011; Yin and Sch\u00fctze, 2015; Kiros et al., 2015), but those employ classification techniques and do not attempt to generate paraphrases.", "startOffset": 50, "endOffset": 114}, {"referenceID": 17, "context": "(LSTM) networks for textual entailment generation (Kolesnyk et al., 2016); however, paraphrase generation is a type of bidirectional textual entailment generation and no prior work has proposed a deep learning-based formulation of this task.", "startOffset": 50, "endOffset": 73}, {"referenceID": 13, "context": "This is inspired by the recent success of such connections in a deep Convolutional Neural Network (CNN) for the image recognition task (He et al., 2015).", "startOffset": 135, "endOffset": 152}, {"referenceID": 40, "context": "RNNs differ from normal perceptrons as they allow gradient propagation in time to model sequential data with variable-length input and output (Sutskever et al., 2011).", "startOffset": 142, "endOffset": 166}, {"referenceID": 2, "context": "In practice, RNNs often suffer from the vanishing/exploding gradient problems while learning long-range dependencies (Bengio et al., 1994).", "startOffset": 117, "endOffset": 138}, {"referenceID": 14, "context": "LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al.", "startOffset": 5, "endOffset": 39}, {"referenceID": 4, "context": "LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014) are known to be successful remedies to these problems.", "startOffset": 48, "endOffset": 66}, {"referenceID": 34, "context": "It has been observed that increasing the depth of a deep neural network can improve the performance of the model (Simonyan and Zisserman, 2014; He et al., 2015) as deeper networks learn better representations of features (Farabet et al.", "startOffset": 113, "endOffset": 160}, {"referenceID": 13, "context": "It has been observed that increasing the depth of a deep neural network can improve the performance of the model (Simonyan and Zisserman, 2014; He et al., 2015) as deeper networks learn better representations of features (Farabet et al.", "startOffset": 113, "endOffset": 160}, {"referenceID": 8, "context": ", 2015) as deeper networks learn better representations of features (Farabet et al., 2013).", "startOffset": 68, "endOffset": 90}, {"referenceID": 20, "context": "For tasks like speech recognition (Li and Wu, 2015) and also in machine translation, it is useful to stack layers of LSTM or any other variants of RNN.", "startOffset": 34, "endOffset": 51}, {"referenceID": 40, "context": "1 Encoder-Decoder Model A neural approach to sequence to sequence modeling proposed by Sutskever et al. (2014) is a twocomponent model, where a source sequence is first encoded into some low dimensional representation (Figure 1) that is later used to reproduce the sequence back to a high dimensional target sequence (i.", "startOffset": 87, "endOffset": 111}, {"referenceID": 40, "context": "1 Encoder-Decoder Model A neural approach to sequence to sequence modeling proposed by Sutskever et al. (2014) is a twocomponent model, where a source sequence is first encoded into some low dimensional representation (Figure 1) that is later used to reproduce the sequence back to a high dimensional target sequence (i.e. decoding). In machine translation, an encoder operates on text written in the source language and encodes the meaning of the sentence to be translated before the decoder can take that vector (which represents the meaning) and generate a sentence in the target language. These encoder-decoder blocks can be either a vanilla RNN or its variants. While producing the target sequence, the generation of each new word depends on the model and the last word generated. For the first word, this is done by appending a special \u2018EOS\u2019 (end-of-sentence) token to the source. The training objective is to maximize the log probability of the target sequence given the source sequence. Therefore, the best possible decoded target is the one that has the maximum score over the length of the sequence. To find this, a small set of hypotheses (candidate set) called beam size is used and the total score for all these hypotheses are computed. In the original work by Sutskever et al. (2014), they observe that even beam size of 1 gives a very good result but a higher beam size always does better.", "startOffset": 87, "endOffset": 1298}, {"referenceID": 29, "context": "Figure 2: LSTM cell (Paszke, 2015)", "startOffset": 20, "endOffset": 34}, {"referenceID": 39, "context": "In most NLP tasks, LSTM outperforms vanilla RNN (Sundermeyer et al., 2012).", "startOffset": 48, "endOffset": 74}, {"referenceID": 10, "context": "Graves (2013) explored the advantages of deep LSTMs for handwriting recognition and text generation.", "startOffset": 0, "endOffset": 14}, {"referenceID": 10, "context": "Graves (2013) explored the advantages of deep LSTMs for handwriting recognition and text generation. There are multiple ways of combining one layer of LSTM with another. For example, Pascanu et al. (2013) explored multiple ways of combining them and discussed various difficulties in training deep LSTMs.", "startOffset": 0, "endOffset": 205}, {"referenceID": 10, "context": "Graves (2013) explored the advantages of deep LSTMs for handwriting recognition and text generation. There are multiple ways of combining one layer of LSTM with another. For example, Pascanu et al. (2013) explored multiple ways of combining them and discussed various difficulties in training deep LSTMs. In this work, we employ vertical stacking where only the output of the previous layer of LSTM is fed to the input, as compared to the stacking technique used by Sutskever et al. (2014), where hidden states of all LSTM layers are fully connected.", "startOffset": 0, "endOffset": 490}, {"referenceID": 2, "context": "This is similar to stacked RNN proposed by Bengio et al. (1994) but with LSTM units.", "startOffset": 43, "endOffset": 64}, {"referenceID": 13, "context": "3 Stacked Residual LSTM We take inspiration from a very successful deep learning network ResNet (He et al., 2015) with regard to adding residue for the purpose of learning.", "startOffset": 96, "endOffset": 113}, {"referenceID": 13, "context": "3 Stacked Residual LSTM We take inspiration from a very successful deep learning network ResNet (He et al., 2015) with regard to adding residue for the purpose of learning. With theoretical and empirical reasoning, He et al. (2015)", "startOffset": 97, "endOffset": 232}, {"referenceID": 13, "context": "When stacking multiple layers of neurons, often the network suffers through a degradation problem (He et al., 2015).", "startOffset": 98, "endOffset": 115}, {"referenceID": 13, "context": "When stacking multiple layers of neurons, often the network suffers through a degradation problem (He et al., 2015). The degradation problem arises due to the low convergence rate of training error and is different from the vanishing gradient problem. Residual connections can help overcome this issue. We experimented with four-layers of stacked LSTM for each of the model. Residue connections is added at layer two as the pointwise addition (see Figure 3), and thus it requires the input to be in the same dimension as the output of ht. Principally because of this reason, we use a simple last hidden unit stacking of LSTM instead of a more intricate way as shown by Sutskever et al. (2014). This allowed us to clip the ht to match the dimension of xt\u22122 where they were not the same.", "startOffset": 99, "endOffset": 693}, {"referenceID": 30, "context": "PPDB (Pavlick et al., 2015) is a well known dataset used for various NLP tasks.", "startOffset": 5, "endOffset": 27}, {"referenceID": 23, "context": "This dataset contains relatively short paraphrases (86% of the data is less than four words), which makes it suitable for synonym generation and phrase substitution to address lexical and phrasal paraphrasing (Madnani and Dorr, 2010).", "startOffset": 209, "endOffset": 233}, {"referenceID": 7, "context": "WikiAnswers (Fader et al., 2013) is a large question paraphrase corpus created by crawling the WikiAnswers website1, where users can post questions and answers about any topic.", "startOffset": 12, "endOffset": 32}, {"referenceID": 21, "context": "MSCOCO (Lin et al., 2014) dataset contains human annotated captions of over 120K images.", "startOffset": 7, "endOffset": 25}, {"referenceID": 44, "context": "In fact, this is the main reason why neural networks for generating captions obtain very good BLEU scores (Vinyals et al., 2014), which allows us to use this dataset for the paraphrase generation task.", "startOffset": 106, "endOffset": 128}, {"referenceID": 44, "context": "Because of the free form nature of the caption generation task (Vinyals et al., 2014), some captions were very long.", "startOffset": 63, "endOffset": 85}, {"referenceID": 41, "context": "Table 1: Dataset details Models Reference Sequence to Sequence (Sutskever et al., 2014) With Attention (Bahdanau et al.", "startOffset": 63, "endOffset": 87}, {"referenceID": 0, "context": ", 2014) With Attention (Bahdanau et al., 2015) Bi-directional LSTM (Graves et al.", "startOffset": 23, "endOffset": 46}, {"referenceID": 9, "context": ", 2015) Bi-directional LSTM (Graves et al., 2013) Residual LSTM Our proposed model", "startOffset": 28, "endOffset": 49}, {"referenceID": 20, "context": "This was motivated from the state-of-the-art speech recognition systems that also use three to four layers of stacked LSTMs (Li and Wu, 2015).", "startOffset": 124, "endOffset": 141}, {"referenceID": 41, "context": "We experimented with beam sizes of 5 and 10 to compare the models, as these are the most common beam sizes used in the literature (Sutskever et al., 2014).", "startOffset": 130, "endOffset": 154}, {"referenceID": 37, "context": "A standard dropout (Srivastava et al., 2014) of 50% was applied after every LSTM layer.", "startOffset": 19, "endOffset": 44}, {"referenceID": 41, "context": "A beam search algorithm was used to generate optimal paraphrases by exploiting the trained models in the testing phase (Sutskever et al., 2014).", "startOffset": 119, "endOffset": 143}, {"referenceID": 27, "context": "1 Metrics To quantitatively evaluate the performance of our paraphrase generation models, we use the well-known automatic evaluation metrics3 for comparing parallel corpora: BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and Translation Error Rate (TER) (Snover et al.", "startOffset": 179, "endOffset": 202}, {"referenceID": 19, "context": ", 2002), METEOR (Lavie and Agarwal, 2007), and Translation Error Rate (TER) (Snover et al.", "startOffset": 16, "endOffset": 41}, {"referenceID": 35, "context": ", 2002), METEOR (Lavie and Agarwal, 2007), and Translation Error Rate (TER) (Snover et al., 2006).", "startOffset": 76, "endOffset": 97}, {"referenceID": 24, "context": "Even though these metrics were designed for machine translation, previous works have shown that they can perform well for the paraphrase recognition task (Madnani et al., 2012) and correlate well with human judgments in evaluating generated paraphrases (Wubben et al.", "startOffset": 154, "endOffset": 176}, {"referenceID": 47, "context": ", 2012) and correlate well with human judgments in evaluating generated paraphrases (Wubben et al., 2010).", "startOffset": 84, "endOffset": 105}, {"referenceID": 22, "context": "Although there exists a few automatic evaluation metrics that are specifically designed for paraphrase generation, such as PEM (Paraphrase Evaluation Metric) (Liu et al., 2010) and PINC (Paraphrase In Ngram Changes) (Chen and Dolan, 2011), they have certain limitations.", "startOffset": 158, "endOffset": 176}, {"referenceID": 3, "context": ", 2010) and PINC (Paraphrase In Ngram Changes) (Chen and Dolan, 2011), they have certain limitations.", "startOffset": 47, "endOffset": 69}, {"referenceID": 3, "context": "Although PINC correlates well with human judgments in lexical dissimilarity assessment, BLEU have been shown to correlate better for semantic equivalence agreements at the sentence-level when a sufficiently large number of reference sentences are available for each source sentence (Chen and Dolan, 2011).", "startOffset": 282, "endOffset": 304}, {"referenceID": 5, "context": "As suggested in (Clark et al., 2011), we used a stratified approximate randomization (AR) test.", "startOffset": 16, "endOffset": 36}, {"referenceID": 26, "context": "In our experiments, we used Word2Vec embeddings pre-trained on the Google News Corpus (Mikolov et al., 2014).", "startOffset": 86, "endOffset": 108}, {"referenceID": 5, "context": "As suggested in (Clark et al., 2011), we used a stratified approximate randomization (AR) test. AR calculates the probability of a metric score providing the same reference sentence by chance. We report our p-values at 95% Confidence Intervals (CI). The major limitation of these evaluation metrics is that they do not consider the meaning of the paraphrases, and hence, are not able to capture paraphrases of entities. For example, these metrics will not reward the paraphrasing of Coke to Pepsi. However, in Word2Vec embeddings they will be considered as similar entities. Therefore, we also evaluate our models on a sentence similarity metric4 proposed by Rus et al. (2012). This metric uses word embeddings to compare the phrases.", "startOffset": 17, "endOffset": 677}, {"referenceID": 5, "context": "This is calculated using bootstrap re-sampling for each optimizer run (Clark et al., 2011).", "startOffset": 70, "endOffset": 90}, {"referenceID": 23, "context": "The knowledge-driven approaches for paraphrase generation (Madnani and Dorr, 2010) generally utilize hand-crafted rules (McKeown, 1983) or automatically learned complex paraphrase patterns (Zhao et al.", "startOffset": 58, "endOffset": 82}, {"referenceID": 25, "context": "The knowledge-driven approaches for paraphrase generation (Madnani and Dorr, 2010) generally utilize hand-crafted rules (McKeown, 1983) or automatically learned complex paraphrase patterns (Zhao et al.", "startOffset": 120, "endOffset": 135}, {"referenceID": 50, "context": "The knowledge-driven approaches for paraphrase generation (Madnani and Dorr, 2010) generally utilize hand-crafted rules (McKeown, 1983) or automatically learned complex paraphrase patterns (Zhao et al., 2009).", "startOffset": 189, "endOffset": 208}, {"referenceID": 12, "context": "Other related work uses thesaurus-based (Hassan et al., 2007) or semantic analysis-driven natural language We used the software available at https://github.", "startOffset": 40, "endOffset": 61}, {"referenceID": 18, "context": "generation approaches (Kozlowski et al., 2003) to generate paraphrases.", "startOffset": 22, "endOffset": 46}, {"referenceID": 50, "context": "Other models generate application-specific paraphrases (Zhao et al., 2009), leverage bilingual parallel corpora (Bannard and Callison-Burch, 2005) or apply a multi-pivot approach to output candidate paraphrases (Zhao et al.", "startOffset": 55, "endOffset": 74}, {"referenceID": 1, "context": ", 2009), leverage bilingual parallel corpora (Bannard and Callison-Burch, 2005) or apply a multi-pivot approach to output candidate paraphrases (Zhao et al.", "startOffset": 45, "endOffset": 79}, {"referenceID": 51, "context": ", 2009), leverage bilingual parallel corpora (Bannard and Callison-Burch, 2005) or apply a multi-pivot approach to output candidate paraphrases (Zhao et al., 2010).", "startOffset": 144, "endOffset": 163}, {"referenceID": 6, "context": "Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2005) is another widely used dataset for paraphrase detection.", "startOffset": 44, "endOffset": 64}, {"referenceID": 15, "context": "A Variant of residual network called DenseNet (Huang et al., 2016), which uses dense connections over every layer, has been shown to be effective for image recognition tasks setting state-of-the-art results in CIFAR and SVHN datasets.", "startOffset": 46, "endOffset": 66}, {"referenceID": 15, "context": "generation approaches (Kozlowski et al., 2003) to generate paraphrases. In contrast, Quirk et al., (2004) show the effectiveness of SMT techniques for paraphrase generation given adequate monolingual parallel corpus extracted from comparable news articles.", "startOffset": 23, "endOffset": 106}, {"referenceID": 15, "context": "generation approaches (Kozlowski et al., 2003) to generate paraphrases. In contrast, Quirk et al., (2004) show the effectiveness of SMT techniques for paraphrase generation given adequate monolingual parallel corpus extracted from comparable news articles. Wubben et al., (2010) propose a phrase-based SMT framework for sentential paraphrase generation by using a large aligned monolingual corpus of news headlines.", "startOffset": 23, "endOffset": 279}, {"referenceID": 15, "context": "generation approaches (Kozlowski et al., 2003) to generate paraphrases. In contrast, Quirk et al., (2004) show the effectiveness of SMT techniques for paraphrase generation given adequate monolingual parallel corpus extracted from comparable news articles. Wubben et al., (2010) propose a phrase-based SMT framework for sentential paraphrase generation by using a large aligned monolingual corpus of news headlines. Zhao et al., (2008) propose a combination of multiple resources to learn phrase-based paraphrase tables and corresponding feature functions to devise a log-linear SMT model.", "startOffset": 23, "endOffset": 436}, {"referenceID": 1, "context": ", 2009), leverage bilingual parallel corpora (Bannard and Callison-Burch, 2005) or apply a multi-pivot approach to output candidate paraphrases (Zhao et al., 2010). Not much work has been done with regard to applications of deep learning for paraphrase generation. We explored several sources as potential large datasets. Recently, Weiting et al. (2015) took the PPDB dataset (sizeXL) and annotated phrases based on their paraphrasability.", "startOffset": 46, "endOffset": 354}, {"referenceID": 1, "context": ", 2009), leverage bilingual parallel corpora (Bannard and Callison-Burch, 2005) or apply a multi-pivot approach to output candidate paraphrases (Zhao et al., 2010). Not much work has been done with regard to applications of deep learning for paraphrase generation. We explored several sources as potential large datasets. Recently, Weiting et al. (2015) took the PPDB dataset (sizeXL) and annotated phrases based on their paraphrasability. This dataset is called AnnotatedPPDB and contains 3000 pairs in total. In the same work, they also introduced another dataset called ML-Paraphrase for the purpose of evaluating bigram paraphrases. This dataset contains 327 instances. Microsoft Research Paraphrase Corpus (MSRP) (Dolan et al., 2005) is another widely used dataset for paraphrase detection. MSRP contains 5800 pairs of sentences (obtained from various news sources) accompanied with human annotations. These datasets are too small and therefore not suitable for deep learning models. To the best of our knowledge this is the first work on using residual connections with recurrent neural networks. However, we recently noticed that Toderici et al. (2016) used residual GRU to show an improvement in image compression rates for a given quality over JPEG.", "startOffset": 46, "endOffset": 1160}, {"referenceID": 38, "context": "Recent advances in neural network with regard to learnable memory (Sukhbaatar et al., 2015; Graves et al., 2014) have enabled models to get one step closer to learn comprehension.", "startOffset": 66, "endOffset": 112}, {"referenceID": 10, "context": "Recent advances in neural network with regard to learnable memory (Sukhbaatar et al., 2015; Graves et al., 2014) have enabled models to get one step closer to learn comprehension.", "startOffset": 66, "endOffset": 112}], "year": 2016, "abstractText": "In this paper, we propose a novel neural approach for paraphrase generation. Conventional paraphrase generation methods either leverage hand-written rules and thesauri-based alignments, or use statistical machine learning principles. To the best of our knowledge, this work is the first to explore deep learning models for paraphrase generation. Our primary contribution is a stacked residual LSTM network, where we add residual connections between LSTM layers. This allows for efficient training of deep LSTMs. We experiment with our model and other state-of-the-art deep learning models on three different datasets: PPDB, WikiAnswers and MSCOCO. Evaluation results demonstrate that our model outperforms sequence to sequence, attention-based and bi-directional LSTM models on BLEU, METEOR, TER and an embedding-based sentence similarity metric.", "creator": "TeX"}}}