{"id": "1610.03147", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "Context-Aware Online Learning for Course Recommendation of MOOC Big Data", "abstract": "The Massive Open Online Course (MOOC) has expanded significantly in recent years. With the widespread of MOOC, the opportunity to study the fascinating courses for free has attracted numerous people of diverse educational backgrounds all over the world. In the big data era, a key research topic for MOOC is how to mine the needed courses in the massive course databases in cloud for each individual (course) learner accurately and rapidly as the number of courses is increasing fleetly. In this respect, the key challenge is how to realize personalized course recommendation as well as to reduce the computing and storage costs for the tremendous course data. In this paper, we propose a big data-supported, context-aware online learning-based course recommender system that could handle the dynamic and infinitely massive datasets, which recommends courses by using personalized context information and historical statistics. The context-awareness takes the personal preferences into consideration, making the recommendation suitable for people with different backgrounds. Besides, the algorithm achieves the sublinear regret performance, which means it can gradually recommend the mostly preferred and matched courses to learners. Unlike other existing algorithms, ours bounds the time complexity and space complexity linearly. In addition, our devised storage module is expanded to the distributed-connected clouds, which can handle massive course storage problems from heterogenous sources. Our experiment results verify the superiority of our algorithms when comparing with existing works in the big data setting.", "histories": [["v1", "Tue, 11 Oct 2016 01:02:15 GMT  (3402kb)", "http://arxiv.org/abs/1610.03147v1", null], ["v2", "Sun, 16 Oct 2016 03:34:37 GMT  (1062kb,D)", "http://arxiv.org/abs/1610.03147v2", null]], "reviews": [], "SUBJECTS": "cs.LG cs.CY cs.IR", "authors": ["yifan hou", "pan zhou", "ting wang", "li yu", "yuchong hu", "dapeng wu"], "accepted": false, "id": "1610.03147"}, "pdf": {"name": "1610.03147.pdf", "metadata": {"source": "CRF", "title": "Context-Aware Online Learning for Course Recommendation of MOOC Big Data", "authors": ["Yifan Hou"], "emails": ["panzhou@hust.edu.cn"], "sections": [{"heading": null, "text": "This year, it has come to the point where one feels able to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to move, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight, to fight."}, {"heading": "II. RELATED WORKS", "text": "A wealth of previous work on algorithm recommendations is insufficient. As far as MOOCs are concerned, the two main tactics for updating the algorithm are filter-based approaches and online learning methods [11]. Besides filter-based approaches, there are some branches such as collaborative filtering [12], content-based filtering [13] and hybrid approaches [15]. The collaborative filtering approach collects students \"learning records and then classifies them based on the properties provided, recommending a course from the learning records for new students [12]. Content-based filtering recommends a course for the learner that is relevant to the learning records. [13] The hybrid approach is the combination of the two methods. Filter-based approaches may initially be better than the online learning records [12]."}, {"heading": "III. PROBLEM FORMULATION", "text": "In this section, we present the system model, context model, course model and regret definition, as well as some relevant notations and preliminary definitions.Fig.1 illustrates the functional model. Older users, such as professors or networking platforms, upload MOOC course resources, including pre-recorded videos for open classes, live videos for online courses and class tests, etc., into the course cloud. The system then gathers key information from the learners via features such as professors \"reputation, course language, level of education for courses with appropriate difficulties, quality of presentations, etc. When the system receives the information, it recommends courses that users have not previously learned to the learners as alternative choices. Then, the learners give the system satisfaction services based on their preferences, such as some learners prefer computer science and other classical music."}, {"heading": "A. System Model", "text": "In each time frame, there are three current states: (1) a learner with an exclusive context information comes into our model; (2) the model recommends a course from the current course cluster to the learner; (3) the learner offers a reward based on the newly recommended course to the system. Afterwards, the model will learn how to better fit into the next recommended procedure taking into account the rewards received. We give concepts of three essential elements: learners: We refer to the set of incoming learners as S = 1, 2, \u00b7 \u00b7) during the course of the algorithm, the system will propose a course based on the rewards and context information received. Contexts: The context is presented as Set X, with the space being continuous. In addition, we assume that the context space is a dX dimension space, meaning that the context X area is a vector."}, {"heading": "B. Context Model for Individualization", "text": "Since the context space is a dX-dimensional vector space, we normalize all dimensions of the context area from 0 to 1. The realistic meaning of the dimensions denotes dX-independent characteristics such as age, cultural background, nationalities, etc., which represent the student's preferences. We define the number of context units as nT, which indicates the number of notes in the dimension. To have a better formulation, PT = {P1, P2, \u00b7 \u00b7, P (nT) dX} is used to denote the sliced chronological sub-hypercube. As shown in Fig. 3, we leave dX = 3 and nT = 2. We divide each axis into 2 parts and the number of sub-hypercube (nT) dX = 8. For simplicity, we use the center in the sub-hypercube to represent the specific contexts in the cube. With this context model, we divide the user types X (T) into different types."}, {"heading": "C. Course Set Model for Recommendation", "text": "For the course model, we use the binary tree to index the course dates. Courses are collected together and then divided into two parts by the algorithm, based (on the regret information), which explore the process of the tree model. The number of nodes at depth h is 2h, so we let Nh, i (1 \u2264 i \u2264 2h) represent the node at depth h. The root node of the binary course tree is a set of the entire course N0.1 = C. And with the exploration of the tree, the two child nodes contain all the courses from the parent node, and they never intersect each other, Nh, i = Nh \u2212 1, Nh \u2212 1 \u2212 1 gap of the entire course N0.1 = C. And with the exploration of the tree, the two child nodes contain all the courses from the parent node, and they never intersect each other, Nh, i = Nh \u2212 1 \u2212 1 gap from the entire course N0.1 = C. And with the exploration of the tree, they never contain both the parent nodes and the child."}, {"heading": "D. The Regret of Learning", "text": "Simply put, regret means the loss of accuracy in the recommended approach due to unknown dynamics. We assume that the reward rt [0, 1] is the indicator of accuracy. According to the reward, we define regret asR (T) = T-T-T = 1 rPt-E [T-T = 1 (I (r-t = rt)], (2) where the rt represents the reward in time and the subscript \u0439 is the optimal solution, i.e. r-T is the best reward. Regret shows the convergence rate of the optimal recommended option. If regret is sublinear R (T) = O (T-T), where 0 < B < < 1, the algorithm finally converges to the optimal solution. In the following section, we propose our algorithms with sublinear regret."}, {"heading": "IV. REPARTIMIENTO HIERARCHICAL TREE", "text": "In this section, we propose our most important online learning algorithm to dismantle a course in MOOC Big Data."}, {"heading": "A. Algorithm", "text": "The algorithmic procedures are described in Fig 4. When the reward we receive for the nodes, we receive for the nodes, we receive for the nodes, we receive for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes, for the nodes for the nodes"}, {"heading": "B. Regret Analyze", "text": "We look at the regret in a sub-hypercube Pt and get the sum of it at the end. Since the regret is the deviation from the optimal courses, we must define the optimal course in the first place. To find the node in an affirmative manner, we must precisely define the minimum sub-optimality gap, which indicates the difference between the optimal course in this node and the overall optimal course, in order to define the optimal 3. The minimum sub-optimality gap and the context gap areDPth, i = f (for Pt) \u2212 f."}, {"heading": "V. DISTRIBUTED STORED COURSE RECOMMENDATION TREE", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A. Distributed Algorithm for Multiple Course Storages", "text": "In this section, we turn to a new algorithm called Distributed Storage Repartimiento Hierarchical Trees (DSRHT), which we can use to improve memory pressure by using distributed units to store the price data in clouds. The practical situation can be very complex, so we tie the number of distributed nd units to our model of tree 2z \u2212 1 \u2264 nd \u2264 2z, where z is the depth of the tree and 2z is the number of nodes at this depth. In the next figure, we replace the number nd with 2z.In algorithm 4, we still find the context sub-hypercube. Since there are then 2z distributed units, we first determine these top nodes. Based on the information obtained, the algorithm can begin to find the course by using the Bound Estimation.The determination of the number of distributed units is necessary, as we assume that there are 2z memory nodes, and the number satisfies2z (lnT + 1dT) \u2212 stimation."}, {"heading": "B. Regret Analyze", "text": "In this subsection we prove the regret result in DSRHT can be limited sublinear problem. (Since we use the model to determine the threshold of the number of distributed units, we recommend the algorithms 4 Distributed course recommendation Tree Require: The constants k1 > 0, m \u00b2 (0, 1), the number of memory unit 2z and the context of the learner to divide. (Auxiliary function: Exploration and Bound Updating Initialization: For all contexts sub-hypercues belonging to PT, 1, NPtz, 2... NPtz, 2... EPtz, 2} EPtz, i = \u221e for i = 1... 2z1: for t = 1.2, T do 2: for dt = 0, 1, 2... dX do 3: Find context in dt dimension 4: end for 5: Get the context sub-hypercubity Pt 7: for j center of Pt 7: j = 1.2, T do 2: for dt = 1, dX do 3: Find context in dt dimension 4: end for 5: Get the context sub-hypercubity Pt."}, {"heading": "VI. STORAGE COMPLEXITY", "text": "In this section, we analyze the space complexity of the two algorithms mathematically. We use S (T) to represent the space complexity, but for the RHT algorithm, we take the number of memory units emanating from the root node, and it is relatively easy to know the space complexity is the linear complexity O (E [S (T)]) = O (T).Theorem 3. In the optimal state, we take the number of memory units that 2z = (T lnT) dX + dC + dC \u2212 1dC \u2212 1dC \u2212 2, then we can get the space complex E [S (T)] = O (T).Theorem 3. In the optimal state, we take the number of memory units that 2z = (T lnT) dX + dC + dC \u2212 1dC \u2212 1dC \u2212 2, then we get the space complexityE [S (T)] = O (T) + dC + dC \u2212 dC \u2212 dC \u2212 1."}, {"heading": "VII. NUMERICAL RESULTS", "text": "In this section, we present: (1) the source of the data set; (2) the sum of regrets is sublinear and the average regret eventually converges to 0; (3) we compare the limits of regret of our algorithms with other similar work; (4) distributed storage methods can reduce space complexity. Fig. 5 illustrates the MOOC operation pattern in edX [27]. On the right is the teaching window and learning resources, and on the left are lesson content, homepage, forums and other functionality options."}, {"heading": "A. Description of the Dataset", "text": "We take the data set, the feedback information and course details from the edX [27] and the intermedi-TABLE I contains: Theoretical comparison algorithms context Infinity Time Complexity Space Complexity RegretACR [29] O (E + dC + 1 dC + 2 dC + 1 dC + 2) RHT Yes O (T + 2 lnT) O (T) O (T) O (T) O (T) D (T) D) D) D (D) D (D) D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D D\" D \"D D\" D \"D\" D D \"D D\" D \"D\" D D \"D\" D \"D\" D D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\" D \"D\""}, {"heading": "B. Experimental Setup", "text": "As for our algorithm, the final training number of data is over 6 \u00b7 106 and the number of courses is about 5 \u00b7 108, which for the training number means that we can consider the number of courses as infinite. Three works are introduced as follows. \u2022 Adaptive Clustering Recommendation Algorithm (ACR) [29]: The algorithm injects contextual factors that are able to adapt to more learners, however, if the course database is relatively large, ergodic process in this model cannot handle the data set well. \u2022 We consider both infinity and context, so our model can better adapt to future DSOC situation. In DSRHT, we sacrifice some immediate interests in order to obtain better long-term performance. To practically verify the conclusions, we divide experiments into the following three steps: 1) We compare this data with two steps."}, {"heading": "C. Results and Analysis", "text": "We analyze our algorithm from two different angles: comparing it with other two works and comparing it with other parameters, for example. In each direction, we compare the regret first and analyze the average regret. And then, we discuss the accuracy based on the average regret. Finally, we can better compare the storage conditions of other algorithms. HCT algorithms are better than ACR and HCT."}, {"heading": "VIII. CONCLUSION", "text": "In this paper, we have presented RHT and DSRHT algorithms for price recommendation in MOOC Big Data. In order to better fit into the changeable data set in the future, we are expanding the number of objects to infinity. We pay attention to individualization in the recommendation system and inject context awareness into our algorithm. In addition, we use distributed memory to reduce memory pressure and make it more suitable for big data. We have also tested our algorithm and compared it with two similar algorithms. As for our ongoing work, we intend to further reduce space complexity through the combined method of HCT [31] and distributed storage in order to further improve the storage state."}, {"heading": "APPENDIX A PROOF OF LEMMA 1", "text": "The proof: We assume that the path is outside the best depth of k, so we can know the estimate at the depth k + 1 (the deviation of the actual value), which means that EPtk + 1, i (k + 1) \u2032 \u2264 EPtk + 1, i (the first is the best path and the second is the node selected at the depth of k + 1. According to the definition of Bound and and and estimation we can know that with the decreasing depth either the estimate of the node expires. So we can know that EPtk + 1, i (k + 1) \u2032 n (E Pt k + 1, i (t), i (k), i (k), i (k), i (k), k (k), k (k), k (k), i (i), i (t)."}, {"heading": "APPENDIX B PROOF OF LEMMA 2", "text": "The proof: After supposition 3, we let c1, c2, selected from the same context, in the same optimal node and let c1 be the best course so that we can change the context with a dot, we must inject the context (rPtc2) \u2212 f (rPtc2) h (mPt) h (mPt) h (mPt) h (n) h (n) h (n) h (n) h (n) h (n) h (n) h (n) h (n) h (n) h (n) h (h h h h h h h (n) h (n) n n n n n n n n n (n) h (n) h (n) h (h) h (h h h h h) h h h (h h h h h) h (n) n (n) n n n n (h) n (h) h (n) h (h) h (h h h) h (h h h h h) h (n) n, n n (n) n (h) n (n) n (h (h) h (h) h (h) h (h h h) h (h h h h) h (h) h (n) h (n) n n n n n n n), n (n (n) n (n) n (n) n (n) n (h (n) h (n) h (n) h (n) h (n) h (h (h (h) h (h i i i i i i i) h (n) h (h h h (n) h h h (h h h h h) h h h h (n) h h h (n) h (n) h (n) h h h (n) h (n) h (n) h (n) h (n) h (n n n), n n n n n n) n (n) n n (n n n) n (h (h (n) h (h (h) h (h (h) h (h (h h h h h) h h (h h h h h h h) h h h h (h h h h h) h h h h (h h) h h"}, {"heading": "APPENDIX C PROOF OF LEMMA 3", "text": "The proof: With the help of the assumption q + 4k2 lnT [D > q h, i \u2212 k1 (mPt) h \u2212 LX (n) h (n) h \u2212 k1 (mPt) h, i \u2212 k1 (mPt) h \u2212 LX (n) h \u2212 k1 (n) h (n) h \u2212 k1 (n) h \u2212 k1 (n) h \u2212 k1 (mPt) h (n) h (n) h (n) h (n) h (n) h (n) h (n) h (n) h (n) h (h) h (h) h (n) h (h) h (n) h (h) h (h) h (h) h (n) h (h) h (h) h (h) h (n) n (n) n (n) h (n) h (n) h (h) h (h) h (h) h (h) h (h) h (n), n n (n) h (n) h (n) h (n) h (h) h (h) h (h) h (h) h (h) h (h) h (n), n (n) n (n) n (n) h (n) h (n), n (n) h (n) h (n) h (n), n (n) h (n) h (h (n) h (n) h (h) h (h) h (h (h) h (h) h (h) h (h) h (h (h) h (h) h (h) h (h) h (n) h (h) h (h (h) h (h) h (n) h (h (h) h (h) h (h (h) h (n) h) h (h (h) h (h) h (h) h (n n n n n n n n n (n), n n n n (n) h (n) h (n) h (n), n (n) h (n) h (n) h (h (n) h (h (n) h (n) h (n) h (n) h ("}, {"heading": "APPENDIX D PROOF OF THEOREM 2", "text": "The proof: Since we replace the j with j, the term is still valid for distributed stored algorithms m. Based on the segmentation, the regret can be represented with E [R (T)] = E [R1 (T)] + E [R2 (T)] + E [R3 (T) + E [R4 (T) + E [R1 (T)]], since it is the same as the algorithm 1, so that we can assume the first term asE [R1 (T)]] \u2264 4 [k1 (m Pt) H + LX (D) H + LX (D).D The depth is from z to H, so that we can assume the 2H (T)."}], "references": [{"title": "The Year of the MOOC", "author": ["L. Pappano"], "venue": "The New York Times, 2014.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2014}, {"title": "Universities Abroad Join Partnerships on the Web", "author": ["T. Lewin"], "venue": "New York Times, 2013.", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2013}, {"title": "MOOCs make their move", "author": ["A. Brown"], "venue": "The Bent, vol. 104, no. 2, pp. 13-17, 2013.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2013}, {"title": "Big data for development: a review of promises and challenges", "author": ["M. Hilbert"], "venue": "Development Policy Review, pp. 135-174, 2016.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2016}, {"title": "Managing open educational resources on the web of data", "author": ["G. Paquette", "A. Miara"], "venue": "International Journal of Advanced Computer Science and Applications (IJACSA), vol. 5, no. 8, 2014.", "citeRegEx": "7", "shortCiteRegEx": null, "year": 2014}, {"title": "Competencybased personalization for Massive Online Learning", "author": ["G. Paquette", "O. Marino", "D. Rogozan", "M. Lonard"], "venue": "Avaliable at: http;//r-libre.teluq.ca/491/1/SLE-Competency-based%20personalisationrevised%2013.08.2014.pdf.2014.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2013}, {"title": "MOOC performance prediction via clickstream data and social learning networks", "author": ["C.G. Brinton", "M. Chiang"], "venue": "IEEE Conference on Computer Communications (INFOCOM), pp. 2299-2307, 2015.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2015}, {"title": "X-armed bandits", "author": ["S. Bubeck", "R. Munos", "G. Stoltz"], "venue": "Journal of Machine Learning Research pp. 1655-1695, 2011.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2011}, {"title": "Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions", "author": ["G. Adomavicius", "A. Tuzhilin"], "venue": "IEEE Transactions on Knowledge and Data Engineering, vol. 17, no. 6, pp. 734-749, 2005.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2005}, {"title": "A group recommender system for online course study", "author": ["D. Yanhui", "W. Dequan", "Z. Yongxin"], "venue": "International Conference on Information Technology in Medicine and Education, pp. 318-320, 2015.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Content-based recommendation over a customer network for ubiquitous shopping", "author": ["M.J. Pazzani", "D. Billsus"], "venue": "IEEE Transactions on Services Computing, vol. 2, no. 2, pp. 140-151, 2009.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2009}, {"title": "Hybrid recommender systems: Survey and experiments", "author": ["R. Burke"], "venue": "User Modeling and User-adapted Interaction, vol. 16, no. 2, pp. 325- 341. 2007.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2007}, {"title": "An efficient hybrid music recommender system using an incrementally trainable probabilistic generative model", "author": ["K. Yoshii", "M. Goto", "K. Komatani", "T. Ogata", "H.G. Okuno"], "venue": "IEEE Transactions on Audio, Speech, Language Processing, vol. 16, no. 2, pp. 435-447, 2008.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2008}, {"title": "Make adaptive learning of the MOOC: The CML model", "author": ["L. Yanhong", "Z. Bo", "G. Jianhou"], "venue": "International Conference on Computer Science and Education (ICCSE), pp. 1001-1004, 2015.", "citeRegEx": "16", "shortCiteRegEx": null, "year": 2015}, {"title": "A proposed framework for an adaptive learning of Massive Open Online Courses (MOOCs)", "author": ["A. Alzaghoul", "E. Tovar"], "venue": "International Conference on Remote Engineering and Virtual Instrumentation (REV), pp. 127-132, 2016.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2016}, {"title": "A model of adaptation in online learning environments (LMSs and MOOCs)", "author": ["C. Cherkaoui", "A. Qazdar", "A. Battou", "A. Mezouary", "A. Bakki", "D. Mamass", "A. Qazdar", "B. Er-Raha"], "venue": "International Conference on Intelligent Systems: Theories and Applications (SITA), pp. 1-6, 2015.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2015}, {"title": "Online learning with prior knowledge", "author": ["E. Hazan", "N. Megiddo"], "venue": "Learning Theory. Berlin, Germany: Springer-Verlag, pp. 499C513, 2007.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 2007}, {"title": "Contextual bandits with similarity information", "author": ["A. Slivkins"], "venue": "J. Mach. Learn. Res., vol. 15, no. 1, pp. 2533C2568, Jan. 2014.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 2014}, {"title": "The EpochCGreedy algorithm for contextual multi-armed bandits", "author": ["J. Langford T. Zhang"], "venue": "Proc. NIPS, pp. 1096C1103, 2007.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2007}, {"title": "Contextual bandits with linear payoff functions", "author": ["W. Chu", "L. Li", "L. Reyzin", "R.E. Schapire"], "venue": "Proc. AISTATS, pp. 208C214, 2011.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2011}, {"title": "Contextual multi-armed bandits", "author": ["T. Lu", "D. Pl", "M. Pl"], "venue": "Proc. AISTATS, pp. 485C492, 2010.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2010}, {"title": "X-armed bandits[J", "author": ["S Bubeck", "R Munos", "G Stoltz"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "24", "shortCiteRegEx": "24", "year": 2011}, {"title": "Distributed online big data classification using context information", "author": ["C. Tekin", "M. van der Schaar"], "venue": "IEEE Annual Allerton Conference: Communication, Control, and Computing, pp. 1435-1442, 2013.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2013}, {"title": "Probability inequalities for sums of bounded random variables", "author": ["W. Hoeffding"], "venue": "Journal of the American Statistical Association, pp. 13-30, 1963.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 1963}, {"title": "Barycentric lagrange interpolation", "author": ["J.P. Berrut", "L.N. Trefethen"], "venue": "pp. 501-517, 2004.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2004}, {"title": "Online learning in large-scale contextual recommender systems", "author": ["L. Song", "C. Tekin", "M. van der Schaar"], "venue": "vol. pp, no. 99, 2015.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2015}, {"title": "Contextual bandits with similarity information[J", "author": ["A. Slivkins"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2014}, {"title": "Online Stochastic Optimization under Correlated Bandit Feedback", "author": ["M.G. Azar", "A. Lazaric", "E. Brunskill"], "venue": "pp. 1557-1565, 2014. 13", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2014}], "referenceMentions": [{"referenceID": 0, "context": "INTRODUCTION MOOC is a concept first proposed in 2008 and known to the world in 2012 [1] [2].", "startOffset": 85, "endOffset": 88}, {"referenceID": 1, "context": "INTRODUCTION MOOC is a concept first proposed in 2008 and known to the world in 2012 [1] [2].", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "Course recommender system helps the learner to find the requisite course directly in the course ocean of numerous MOOC platforms such like Coursera, edX, Udacity and so on [4].", "startOffset": 172, "endOffset": 175}, {"referenceID": 3, "context": "One key challenge in future MOOC course recommendation are processing tremendous data that bears the feature of volume, variety, velocity, variability and veracity [5] of big data.", "startOffset": 164, "endOffset": 167}, {"referenceID": 4, "context": "Hence, appending context information to the model of handling the courses is ineluctable [7] [8].", "startOffset": 89, "endOffset": 92}, {"referenceID": 5, "context": "Hence, appending context information to the model of handling the courses is ineluctable [7] [8].", "startOffset": 93, "endOffset": 96}, {"referenceID": 25, "context": "Previous context-aware algorithms such as [29] only perform well with the known scale of recommendation datasets.", "startOffset": 42, "endOffset": 46}, {"referenceID": 25, "context": "Specifically, the algorithm in [29] would rank all courses in MOOC as leaf nodes, then it clusters some relevance courses together as course nodes to build their parent nodes based on the historical information and current users\u2019 features.", "startOffset": 31, "endOffset": 35}, {"referenceID": 25, "context": "As for the MOOC big data, since the number of courses keeps increasing and becoming fairly tremendous, algorithms in [29] are prohibitive to be applied.", "startOffset": 117, "endOffset": 121}, {"referenceID": 25, "context": "Most previous works [29] [30] could only realize the linear space complexity, however it\u2019s not promising for MOOC big data.", "startOffset": 20, "endOffset": 24}, {"referenceID": 26, "context": "Most previous works [29] [30] could only realize the linear space complexity, however it\u2019s not promising for MOOC big data.", "startOffset": 25, "endOffset": 29}, {"referenceID": 25, "context": "On the other hand, we prove the space complexity can be bounded sublinearly under the optimal condition which is better than [29].", "startOffset": 125, "endOffset": 129}, {"referenceID": 25, "context": "In Section VII, we verify the algorithms by experiment results and compare with relevant previous algorithms [29] [31].", "startOffset": 109, "endOffset": 113}, {"referenceID": 27, "context": "In Section VII, we verify the algorithms by experiment results and compare with relevant previous algorithms [29] [31].", "startOffset": 114, "endOffset": 118}, {"referenceID": 8, "context": "As for MOOC, the two major tactics to actualize the algorithm are filtering-based approaches and online learning methods [11].", "startOffset": 121, "endOffset": 125}, {"referenceID": 9, "context": "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].", "startOffset": 97, "endOffset": 101}, {"referenceID": 6, "context": "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].", "startOffset": 102, "endOffset": 105}, {"referenceID": 10, "context": "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].", "startOffset": 131, "endOffset": 135}, {"referenceID": 11, "context": "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].", "startOffset": 158, "endOffset": 162}, {"referenceID": 12, "context": "Apropos of filtering-based approaches, there are some branches such like collaborative filtering [12] [9], content-based filtering [13] and hybrid approaches [14] [15].", "startOffset": 163, "endOffset": 167}, {"referenceID": 9, "context": "The collaborative filtering approach gathers the students\u2019 learning records together and then classifies them into groups based on the characteristics provided, recommending a course from the group\u2019s learning records to new students [12] [9].", "startOffset": 233, "endOffset": 237}, {"referenceID": 6, "context": "The collaborative filtering approach gathers the students\u2019 learning records together and then classifies them into groups based on the characteristics provided, recommending a course from the group\u2019s learning records to new students [12] [9].", "startOffset": 238, "endOffset": 241}, {"referenceID": 10, "context": "Content-based filtering recommends a course to the learner which is relevant to the learning records before [13].", "startOffset": 108, "endOffset": 112}, {"referenceID": 13, "context": "Most of the previous works of recommending a course utilize the adaptive learning [16] [17] [18].", "startOffset": 82, "endOffset": 86}, {"referenceID": 14, "context": "Most of the previous works of recommending a course utilize the adaptive learning [16] [17] [18].", "startOffset": 87, "endOffset": 91}, {"referenceID": 15, "context": "Most of the previous works of recommending a course utilize the adaptive learning [16] [17] [18].", "startOffset": 92, "endOffset": 96}, {"referenceID": 13, "context": "In [16], the CML model was presented.", "startOffset": 3, "endOffset": 7}, {"referenceID": 16, "context": "The similar works are widely distributed in [19]\u2013[24] as contextual bandit problems.", "startOffset": 44, "endOffset": 48}, {"referenceID": 21, "context": "The similar works are widely distributed in [19]\u2013[24] as contextual bandit problems.", "startOffset": 49, "endOffset": 53}, {"referenceID": 21, "context": "Our work is motivated from [24] for big data support, however we consider the contextaware online learning for the first time with delicately devised context partition schemes for MOOC big data.", "startOffset": 27, "endOffset": 31}, {"referenceID": 0, "context": "With the normalization of context information, we suppose the context space is X = [0, 1]X which is a unit hypercube.", "startOffset": 83, "endOffset": 89}, {"referenceID": 0, "context": "We assume the reward rt \u2208 [0, 1] is the indication of accuracy.", "startOffset": 26, "endOffset": 32}, {"referenceID": 25, "context": "Since the ACR [29] has a ergodic process, the time complexity and regret bound is relative high compared with others.", "startOffset": 14, "endOffset": 18}, {"referenceID": 27, "context": "As for HCT [31], having no context information makes the algorithm has a very low space complexity and regret bound compared with our algorithms.", "startOffset": 11, "endOffset": 15}, {"referenceID": 25, "context": "Algorithm Context Infinity Time Complexity Space Complexity Regret ACR [29] Yes No O (", "startOffset": 71, "endOffset": 75}, {"referenceID": 27, "context": "HCT [31] No Yes O(T lnT ) O (", "startOffset": 4, "endOffset": 8}, {"referenceID": 25, "context": "5: MOOC Learning Model \u2022 Adaptive Clustering Recommendation Algorithm (ACR) [29]: The algorithm injects contextual factors capable of adapting to more learners, however, when the course database is fairly large, ergodic process in this model can\u2019t handle the dataset well.", "startOffset": 76, "endOffset": 80}, {"referenceID": 27, "context": "\u2022 High Confidence Tree algorithm (HCT) [31]: The algorithm supports unlimited dataset however large it is, but there is only one learner for the recommendation model since it doesn\u2019t take context into consideration.", "startOffset": 39, "endOffset": 43}, {"referenceID": 25, "context": ": In this step we compare our RHT algorithm with the two previous work which are ACR [29] and HCT [31] with different size of training data.", "startOffset": 85, "endOffset": 89}, {"referenceID": 27, "context": ": In this step we compare our RHT algorithm with the two previous work which are ACR [29] and HCT [31] with different size of training data.", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "Number \u00d710 Algorithm ACR [29] HCT [31] RHT 1 65.", "startOffset": 25, "endOffset": 29}, {"referenceID": 27, "context": "Number \u00d710 Algorithm ACR [29] HCT [31] RHT 1 65.", "startOffset": 34, "endOffset": 38}, {"referenceID": 25, "context": "TABLE IV: Average Storage Cost ACR [29] HCT [31] RHT DSRHT (z=10) Storage Cost (TB) 12573 2762 4123 2132 Storage Ratio 24.", "startOffset": 35, "endOffset": 39}, {"referenceID": 27, "context": "TABLE IV: Average Storage Cost ACR [29] HCT [31] RHT DSRHT (z=10) Storage Cost (TB) 12573 2762 4123 2132 Storage Ratio 24.", "startOffset": 44, "endOffset": 48}, {"referenceID": 25, "context": "From table 4 we know that ACR [29] algorithm is not suitable for real big data since the storage ratio reaches 24.", "startOffset": 30, "endOffset": 34}, {"referenceID": 27, "context": "HCT [31] algorithm performs well in space complexity which is better than RHT.", "startOffset": 4, "endOffset": 8}, {"referenceID": 27, "context": "As for our ongoing work, we intend to reduce space complexity by the combined method of HCT [31] and distributed storage to improve storage condition further.", "startOffset": 92, "endOffset": 96}, {"referenceID": 23, "context": "With the help of Hoeffding-Azuma inequality [26], we get the conclusion.", "startOffset": 44, "endOffset": 48}], "year": 2017, "abstractText": "The Massive Open Online Course (MOOC) has expanded significantly in recent years. With the widespread of MOOC, the opportunity to study the fascinating courses for free has attracted numerous people of diverse educational backgrounds all over the world. In the big data era, a key research topic for MOOC is how to mine the needed courses in the massive course databases in cloud for each individual (course) learner accurately and rapidly as the number of courses is increasing fleetly. In this respect, the key challenge is how to realize personalized course recommendation as well as to reduce the computing and storage costs for the tremendous course data. In this paper, we propose a big data-supported, contextaware online learning-based course recommender system that could handle the dynamic and infinitely massive datasets, which recommends courses by using personalized context information and historical statistics. The context-awareness takes the personal preferences into consideration, making the recommendation suitable for people with different backgrounds. Besides, the algorithm achieves the sublinear regret performance, which means it can gradually recommend the mostly preferred and matched courses to learners. Unlike other existing algorithms, ours bounds the time complexity and space complexity linearly. In addition, our devised storage module is expanded to the distributed-connected clouds, which can handle massive course storage problems from heterogenous sources. Our experiment results verify the superiority of our algorithms when comparing with existing works in the big data setting.", "creator": "LaTeX with hyperref package"}}}