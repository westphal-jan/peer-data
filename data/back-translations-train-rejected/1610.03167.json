{"id": "1610.03167", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "An Empirical Exploration of Skip Connections for Sequential Tagging", "abstract": "In this paper, we empirically explore the effects of various kinds of skip connections in stacked bidirectional LSTMs for sequential tagging. We investigate three kinds of skip connections connecting to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states and (c) skip connections to the cell outputs. We present comprehensive experiments showing that skip connections to cell outputs outperform the remaining two. Furthermore, we observe that using gated identity functions as skip mappings works pretty well. Based on this novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-of-the-art results on CCG supertagging and comparable results on POS tagging.", "histories": [["v1", "Tue, 11 Oct 2016 03:02:38 GMT  (159kb)", "http://arxiv.org/abs/1610.03167v1", "Accepted at COLING 2016"]], "COMMENTS": "Accepted at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["huijia wu", "jiajun zhang", "chengqing zong"], "accepted": false, "id": "1610.03167"}, "pdf": {"name": "1610.03167.pdf", "metadata": {"source": "CRF", "title": "An Empirical Exploration of Skip Connections for Sequential Tagging", "authors": ["Huijia Wu", "Jiajun Zhang", "Chengqing Zong"], "emails": ["huijia.wu@nlpr.ia.ac.cn", "jjzhang@nlpr.ia.ac.cn", "cqzong@nlpr.ia.ac.cn"], "sections": [{"heading": null, "text": "ar Xiv: 161 0.03 167v 1 [cs.C L] 11 Oct 201 6In this paper, we examine empirically the effects of different types of skip connections in stacked bidirectional LSTMs for sequential tagging. We examine three types of skip connections that connect to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states, and (c) skip connections to the cell outputs. We present comprehensive experiments that show that skip connections to the cell outputs outperform the remaining two. Furthermore, we observe that the use of gated identity functions as skip mappings works quite well. On the basis of these novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-of-the-art results on CCG supertagging and comparable results on POS tagging."}, {"heading": "1 Introduction", "text": "This year, it has reached the stage where it will be able to take the lead."}, {"heading": "2 Related Work", "text": "For recursive neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) suggested the use of stacked RNs. However, researchers have paid less attention to the analysis of different types of skip connections, which is our focus in this paper. The works that are closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016) and Zilly et al. (2016). These works are all based on the design of additional connections between different layers. Srivastava et al. (2015b) and Kalet al. (2015)."}, {"heading": "3 Recurrent Neural Networks for Sequential Tagging", "text": "Consider a recursive neural network applied to the sequential marker: Faced with a sequence x = (x1,.., xT), the RNN calculates the hidden state h = (h1,. \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 \u2212 hT) and the output y = (y1,.., yT) by iterating the following equations: ht = f (xt, ht \u2212 1; \u03b8h) (1) yt = g (ht; \u03b8o) (2), where t \u00b2 {1,.,. T} represents time. xt represents the input to time t, ht \u2212 1 and ht the previous and current hidden state, respectively the time sequence. \u2212 f \u2212 g are the transition function and the output function, respectively the network parameters."}, {"heading": "4 Various kinds of Skip Connections", "text": "rE \"s tis rf\u00fc eid rf\u00fc ide rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rf\u00fc the rfu the rfu the rfu the rfu the rfu the r"}, {"heading": "5 Neural Architecture for Sequential Tagging", "text": "Sequential marking can be formulated as P (t | w; \u03b8), where w = [w1,..., wT] denotes the T-words in a sentence, and t = [t1,..., tT] denotes the corresponding T-tags. In this section, we will introduce a neural architecture for P (\u00b7) that includes an input layer, a stacked hidden layer, and an output layer. As the stacked hidden layers have already been introduced, we will only introduce the input and output levels here."}, {"heading": "5.1 Network Inputs", "text": "There are many types of symbol representations, such as the use of a single word embedded in a local window, or a combination of word and letter representation. Here, our input includes the concatenation of word representations, letter representations, and uppercase representations. The entire sentence is then represented by a matrix with column vector, initialized with random initializations or pre-formed embeddings. Each word in a sentence can be converted to an embed vector, and the entire sentence is then represented by a matrix with column vector. \u2212 wT] We use a context window of size d to get its context information."}, {"heading": "5.2 Network Outputs", "text": "For the sequential marking we use a Softmax activation function g (\u00b7) in the output level: yt = g (W hy [\u2212 \u2192 ht; \u2190 \u2212 ht]) (17), where yt is a probability distribution over all possible tags. yk (t) = exp (hk) \u2211 k \u2032 exp (h k \u2032) is the k-th dimension of yt, which corresponds to the k-th markers in the tag set. W hy is the hidden output weight."}, {"heading": "6 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Combinatory Category Grammar Supertagging", "text": "Combinatory Category Grammar (CCG) Supertagging is a sequential tagging problem when processing natural language. The task is to assign supertags to each word in a sentence. In CCG, supertags stand for the lexical categories, which are composed of the basic categories such as N, NP and PP and complex categories that represent the combination of the basic categories based on a set of rules. Detailed explanations of CCG refer to (Steedman, 2000; Steedman and Baldridge, 2011).The training set for this task consists of only 39604 sentences, which is too small to train a deep model and can lead to over-parameterization, but we opt for this as it has already been proven by many authors that a bi-directional recurring network fits the task (Lewis et al., 2016; Vaswani et al., 2016)."}, {"heading": "6.1.1 Dataset and Pre-processing", "text": "Our experiments are conducted with CCGBank (Hockenmaier and Steedman, 2007), a translation from Penn Treebank (Marcus et al., 1993) to CCG with a coverage of 99.4%. We follow the standard splits and use Sections 02-21 for training, Section 00 for development, and Section 23 for testing. We use a complete category set of 1285 keywords. All digits are mapped into the same digit \"9,\" and all words are lowercase."}, {"heading": "6.1.2 Network Configuration", "text": "There are two types of weights in our experiments: recurring and non-recurring weights. For non-recurring weights, we initialize word embedding using the upstream 200-dimensional GolVe vectors (Pennington et al., 2014). Other weights, we initialize using the random orthogonal matrices by a factor of 0.1, where fan-in is the number of units in the input layer. For recurring weight matrices, according to Saxe et al. (2013) we initialize using random orthogonal matrices through SVD to avoid unstable gradients. Orthogonal initialization for recurring weights is important, which is about 2% more powerful than other methods such as Xavier cell initialization (Glorot and Bengio, 2010)."}, {"heading": "6.1.3 Results", "text": "Table 1 shows comparisons with other models for supertagging. The comparisons do not include externally marked data and POS labels. We use stacked bidirectional LSTMs with gated skip connections for comparisons and report the highest 1-best supertagging accuracy on the development kit for the final test. Our model presents up-to-date results compared to existing systems. Character-level (+ 3% relative accuracy) and drop-out (+ 8% relative accuracy) information is needed to improve performance.1http: / / deeplearning.net / software / theano /"}, {"heading": "6.1.4 Experiments on Skip Connections", "text": "We are experimenting with a 7-layer model of CCGbank to compare different types of skip connections introduced in Section 4. Our analysis mainly focuses on the identity function and the gating mechanism. The comparisons (Table 2) are summarized as follows: No skip connections. If the number of stacked layers is large, performance without skip connections is impaired. Accuracy in a 7-layer stacked model without skip connections is 93.94% (Table 2), which is lower than the method with skip connections proposed in Zilly et al. (2016). We are experimenting with the gated identity connections between internal states introduced in Zhang et al. (2016), but the network does not work well (Table 2, 93.14%). We are also implementing the method proposed in Zilly et al. (2016), where we are using a single bidirectional RNH layer to provide the layer with a recursive connection of a modified depth of 3."}, {"heading": "6.1.5 Experiments on Number of Layers", "text": "Table 3 compares the effect of depth in the stacked models. We can observe that performance improves with the increased number of layers, but if the number of layers exceeds 9, performance is impaired. In the experiments, we found that the number of stacked layers between 7 and 9 is the best choice when using skip connections. Note that we do not have layer-by-layer pretraining2Our original implementation of Zilly et a. (2016) cannot converge with a recurring depth of 3. The reason for this could be the explosion of stL in addition. To avoid this, we replace s t L in the last recurring step with ot \u0445 tanh (s t L). (Bengio et al., 2007; Simonyan and Zisserman, 2014), which is an important technique for training deep networks. Further improvements could be achieved with this method to build a deeper network with improved performance."}, {"heading": "3 94.21 94.35", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5 94.51 94.57", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "7 94.51 94.67", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.2 Part-of-speech Tagging", "text": "Part-of-speech tagging is another sequential tagging task, which consists of assigning POS tags to each word within a set. It is very similar to the task of supertagging, so these two tasks can be solved in a single architecture. POS tagging uses the same network configurations as supertagging, except for word vocabulary size and tag set size. We conduct experiments on the Wall Street Journal of the Penn Treebank dataset using the standard splits (sections 0-18 for the train, sections 19-21 for the validation, and sections 22-24 for the test).Although the POS tagging result shown in Table 4 is slightly below state-of-the-art, we do not perform parameter tuning or changes to network architectures, but simply use the one that achieves the best development accuracy in the supertagging task. This demonstrates the generalization of the model of redesigning and avoiding heavy work."}, {"heading": "7 Conclusions", "text": "This paper examines different types of skip connections in stacked bidirectional LSTM models. We present a deeply stacked network (7 or 9 layers) that is easy to train and provides improved accuracy in CCG supertagging and POS tagging. Our experiments show that skip connections to cell outputs work best with the gated identity function. Our research could easily be applied to other sequential processing problems that can be modeled with RNN architectures."}, {"heading": "Acknowledgements", "text": "The research was funded by the Natural Science Foundation of China under grant number 61333018. We thank the anonymous reviewers for their useful comments, which have significantly improved the manuscript."}], "references": [{"title": "Greedy layer-wise training of deep networks. Advances in neural information processing", "author": ["Yoshua Bengio", "Pascal Lamblin", "Dan Popovici", "Hugo Larochelle"], "venue": null, "citeRegEx": "Bengio et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Bengio et al\\.", "year": 2007}, {"title": "Wide-coverage efficient statistical parsing with ccg and log-linear models", "author": ["Stephen Clark", "James R Curran."], "venue": "Computational Linguistics, 33(4):493\u2013552.", "citeRegEx": "Clark and Curran.,? 2007", "shortCiteRegEx": "Clark and Curran.", "year": 2007}, {"title": "Hierarchical recurrent neural networks for long-term dependencies", "author": ["Salah El Hihi", "Yoshua Bengio."], "venue": "NIPS, volume 400, page 409. Citeseer.", "citeRegEx": "Hihi and Bengio.,? 1995", "shortCiteRegEx": "Hihi and Bengio.", "year": 1995}, {"title": "Evaluating word embeddings and a revised corpus for part-of-speech tagging in portuguese", "author": ["Erick R Fonseca", "Jo\u00e3o Lu\u0131\u0301s G Rosa", "Sandra Maria Alu\u0131\u0301sio"], "venue": "Journal of the Brazilian Computer Society,", "citeRegEx": "Fonseca et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Fonseca et al\\.", "year": 2015}, {"title": "Learning to forget: Continual prediction with lstm", "author": ["Felix A Gers", "J\u00fcrgen Schmidhuber", "Fred Cummins."], "venue": "Neural computation, 12(10):2451\u20132471.", "citeRegEx": "Gers et al\\.,? 2000", "shortCiteRegEx": "Gers et al\\.", "year": 2000}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "author": ["Xavier Glorot", "Yoshua Bengio."], "venue": "Aistats, volume 9, pages 249\u2013256.", "citeRegEx": "Glorot and Bengio.,? 2010", "shortCiteRegEx": "Glorot and Bengio.", "year": 2010}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Alex Graves", "J\u00fcrgen Schmidhuber."], "venue": "Neural Networks, 18(5):602\u2013610.", "citeRegEx": "Graves and Schmidhuber.,? 2005", "shortCiteRegEx": "Graves and Schmidhuber.", "year": 2005}, {"title": "Generating sequences with recurrent neural networks", "author": ["Alex Graves."], "venue": "arXiv preprint arXiv:1308.0850.", "citeRegEx": "Graves.,? 2013", "shortCiteRegEx": "Graves.", "year": 2013}, {"title": "Deep residual learning for image recognition", "author": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun."], "venue": "arXiv preprint arXiv:1512.03385.", "citeRegEx": "He et al\\.,? 2015", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Michiel Hermans", "Benjamin Schrauwen."], "venue": "Advances in Neural Information Processing Systems, pages 190\u2013198.", "citeRegEx": "Hermans and Schrauwen.,? 2013", "shortCiteRegEx": "Hermans and Schrauwen.", "year": 2013}, {"title": "Lstm can solve hard long time lag problems", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Advances in neural information processing systems, pages 473\u2013479.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "Ccgbank: a corpus of CCG derivations and dependency structures extracted from the penn treebank", "author": ["Julia Hockenmaier", "Mark Steedman."], "venue": "Computational Linguistics, 33(3):355\u2013396.", "citeRegEx": "Hockenmaier and Steedman.,? 2007", "shortCiteRegEx": "Hockenmaier and Steedman.", "year": 2007}, {"title": "Grid long short-term memory", "author": ["Nal Kalchbrenner", "Ivo Danihelka", "Alex Graves."], "venue": "arXiv preprint arXiv:1507.01526.", "citeRegEx": "Kalchbrenner et al\\.,? 2015", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2015}, {"title": "Adam: A method for stochastic optimization", "author": ["Diederik Kingma", "Jimmy Ba."], "venue": "arXiv preprint arXiv:1412.6980.", "citeRegEx": "Kingma and Ba.,? 2014", "shortCiteRegEx": "Kingma and Ba.", "year": 2014}, {"title": "Neural architectures for named entity recognition", "author": ["Guillaume Lample", "Miguel Ballesteros", "Sandeep Subramanian", "Kazuya Kawakami", "Chris Dyer."], "venue": "arXiv preprint arXiv:1603.01360.", "citeRegEx": "Lample et al\\.,? 2016", "shortCiteRegEx": "Lample et al\\.", "year": 2016}, {"title": "Improved CCG parsing with semi-supervised supertagging", "author": ["Mike Lewis", "Mark Steedman."], "venue": "Transactions of the Association for Computational Linguistics, 2:327\u2013338.", "citeRegEx": "Lewis and Steedman.,? 2014", "shortCiteRegEx": "Lewis and Steedman.", "year": 2014}, {"title": "Lstm ccg parsing", "author": ["Mike Lewis", "Kenton Lee", "Luke Zettlemoyer."], "venue": "Proceedings of the 15th Annual Conference of the North American Chapter of the Association for Computational Linguistics.", "citeRegEx": "Lewis et al\\.,? 2016", "shortCiteRegEx": "Lewis et al\\.", "year": 2016}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation. arXiv preprint arXiv:1508.02096", "author": ["Wang Ling", "Tiago Lu\u0131\u0301s", "Lu\u0131\u0301s Marujo", "Ram\u00f3n Fernandez Astudillo", "Silvio Amir", "Chris Dyer", "Alan W Black", "Isabel Trancoso"], "venue": null, "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Building a large annotated corpus of english: The penn treebank", "author": ["Mitchell P Marcus", "Mary Ann Marcinkiewicz", "Beatrice Santorini."], "venue": "Computational linguistics, 19(2):313\u2013330.", "citeRegEx": "Marcus et al\\.,? 1993", "shortCiteRegEx": "Marcus et al\\.", "year": 1993}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning."], "venue": "EMNLP, volume 14, pages 1532\u201343.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Experiments on learning by back propagation", "author": ["David C Plaut"], "venue": null, "citeRegEx": "Plaut,? \\Q1986\\E", "shortCiteRegEx": "Plaut", "year": 1986}, {"title": "Deep learning made easier by linear transformations in perceptrons", "author": ["Tapani Raiko", "Harri Valpola", "Yann LeCun."], "venue": "AISTATS, volume 22, pages 924\u2013932.", "citeRegEx": "Raiko et al\\.,? 2012", "shortCiteRegEx": "Raiko et al\\.", "year": 2012}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "author": ["Andrew M Saxe", "James L McClelland", "Surya Ganguli."], "venue": "arXiv preprint arXiv:1312.6120.", "citeRegEx": "Saxe et al\\.,? 2013", "shortCiteRegEx": "Saxe et al\\.", "year": 2013}, {"title": "Learning complex, extended sequences using the principle of history compression", "author": ["J\u00fcrgen Schmidhuber."], "venue": "Neural Computation, 4(2):234\u2013242.", "citeRegEx": "Schmidhuber.,? 1992", "shortCiteRegEx": "Schmidhuber.", "year": 1992}, {"title": "Very deep convolutional networks for large-scale image recognition", "author": ["Karen Simonyan", "Andrew Zisserman."], "venue": "arXiv preprint arXiv:1409.1556.", "citeRegEx": "Simonyan and Zisserman.,? 2014", "shortCiteRegEx": "Simonyan and Zisserman.", "year": 2014}, {"title": "Semisupervised condensed nearest neighbor for part-of-speech tagging", "author": ["Anders S\u00f8gaard."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 48\u201352. Association for Computational Linguistics.", "citeRegEx": "S\u00f8gaard.,? 2011", "shortCiteRegEx": "S\u00f8gaard.", "year": 2011}, {"title": "Dropout: A simple way to prevent neural networks from overfitting", "author": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov."], "venue": "The Journal of Machine Learning Research, 15(1):1929\u20131958.", "citeRegEx": "Srivastava et al\\.,? 2014", "shortCiteRegEx": "Srivastava et al\\.", "year": 2014}, {"title": "Training very deep networks", "author": ["Rupesh K Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "Advances in neural information processing systems, pages 2377\u20132385.", "citeRegEx": "Srivastava et al\\.,? 2015a", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Highway networks", "author": ["Rupesh Kumar Srivastava", "Klaus Greff", "J\u00fcrgen Schmidhuber."], "venue": "arXiv preprint arXiv:1505.00387.", "citeRegEx": "Srivastava et al\\.,? 2015b", "shortCiteRegEx": "Srivastava et al\\.", "year": 2015}, {"title": "Combinatory categorial grammar", "author": ["Mark Steedman", "Jason Baldridge."], "venue": "Non-Transformational Syntax: Formal and Explicit Models of Grammar. Wiley-Blackwell.", "citeRegEx": "Steedman and Baldridge.,? 2011", "shortCiteRegEx": "Steedman and Baldridge.", "year": 2011}, {"title": "The syntactic process, volume 24", "author": ["Mark Steedman."], "venue": "MIT Press.", "citeRegEx": "Steedman.,? 2000", "shortCiteRegEx": "Steedman.", "year": 2000}, {"title": "Theano: A python framework for fast computation of mathematical expressions", "author": ["Theano Development Team", "Rami Alrfou", "Guillaume Alain", "Amjad Almahairi", "Christof Angermueller", "Dzmitry Bahdanau", "Nicolas Ballas", "Frdric Bastien", "Justin Bayer", "Anatoly Belikov"], "venue": null, "citeRegEx": "Team et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Team et al\\.", "year": 2016}, {"title": "Supertagging with lstms", "author": ["Ashish Vaswani", "Yonatan Bisk", "Kenji Sagae", "Ryan Musa."], "venue": "Proceedings of the Human Language Technology Conference of the NAACL.", "citeRegEx": "Vaswani et al\\.,? 2016", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Part-of-speech tagging with bidirectional long short-term memory recurrent neural network", "author": ["Peilu Wang", "Yao Qian", "Frank K Soong", "Lei He", "Hai Zhao."], "venue": "arXiv preprint arXiv:1510.06168.", "citeRegEx": "Wang et al\\.,? 2015", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "A dynamic window neural network for ccg supertagging", "author": ["Huijia Wu", "Jiajun Zhang", "Chengqing Zong."], "venue": "arXiv preprint arXiv:1610.02749.", "citeRegEx": "Wu et al\\.,? 2016", "shortCiteRegEx": "Wu et al\\.", "year": 2016}, {"title": "CCG supertagging with a recurrent neural network", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Volume 2: Short Papers, page 250.", "citeRegEx": "Xu et al\\.,? 2015", "shortCiteRegEx": "Xu et al\\.", "year": 2015}, {"title": "Expected f-measure training for shift-reduce parsing with recurrent neural networks", "author": ["Wenduan Xu", "Michael Auli", "Stephen Clark."], "venue": "Proceedings of NAACL-HLT, pages 210\u2013220.", "citeRegEx": "Xu et al\\.,? 2016", "shortCiteRegEx": "Xu et al\\.", "year": 2016}, {"title": "Depth-gated lstm", "author": ["Kaisheng Yao", "Trevor Cohn", "Katerina Vylomova", "Kevin Duh", "Chris Dyer."], "venue": "Presented at Jelinek Summer Workshop on August, volume 14, page 1.", "citeRegEx": "Yao et al\\.,? 2015", "shortCiteRegEx": "Yao et al\\.", "year": 2015}, {"title": "Adadelta: an adaptive learning rate method", "author": ["Matthew D Zeiler."], "venue": "arXiv preprint arXiv:1212.5701.", "citeRegEx": "Zeiler.,? 2012", "shortCiteRegEx": "Zeiler.", "year": 2012}, {"title": "Highway long short-term memory rnns for distant speech recognition", "author": ["Yu Zhang", "Guoguo Chen", "Dong Yu", "Kaisheng Yaco", "Sanjeev Khudanpur", "James Glass."], "venue": "2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5755\u20135759. IEEE.", "citeRegEx": "Zhang et al\\.,? 2016", "shortCiteRegEx": "Zhang et al\\.", "year": 2016}, {"title": "Recurrent highway networks. arXiv preprint arXiv:1607.03474", "author": ["Julian Georg Zilly", "Rupesh Kumar Srivastava", "Jan Koutn\u0131\u0301k", "J\u00fcrgen Schmidhuber"], "venue": null, "citeRegEx": "Zilly et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Zilly et al\\.", "year": 2016}], "referenceMentions": [{"referenceID": 6, "context": "Bidirectional LSTMs (Graves and Schmidhuber, 2005) become dominant in sequential tagging problems due to the superior performance (Wang et al.", "startOffset": 20, "endOffset": 50}, {"referenceID": 33, "context": "Bidirectional LSTMs (Graves and Schmidhuber, 2005) become dominant in sequential tagging problems due to the superior performance (Wang et al., 2015; Vaswani et al., 2016; Lample et al., 2016).", "startOffset": 130, "endOffset": 192}, {"referenceID": 32, "context": "Bidirectional LSTMs (Graves and Schmidhuber, 2005) become dominant in sequential tagging problems due to the superior performance (Wang et al., 2015; Vaswani et al., 2016; Lample et al., 2016).", "startOffset": 130, "endOffset": 192}, {"referenceID": 14, "context": "Bidirectional LSTMs (Graves and Schmidhuber, 2005) become dominant in sequential tagging problems due to the superior performance (Wang et al., 2015; Vaswani et al., 2016; Lample et al., 2016).", "startOffset": 130, "endOffset": 192}, {"referenceID": 21, "context": "Skip connections (or shortcut connections) enable unimpeded information flow by adding direct connections across different layers (Raiko et al., 2012; Graves, 2013; Hermans and Schrauwen, 2013).", "startOffset": 130, "endOffset": 193}, {"referenceID": 7, "context": "Skip connections (or shortcut connections) enable unimpeded information flow by adding direct connections across different layers (Raiko et al., 2012; Graves, 2013; Hermans and Schrauwen, 2013).", "startOffset": 130, "endOffset": 193}, {"referenceID": 9, "context": "Skip connections (or shortcut connections) enable unimpeded information flow by adding direct connections across different layers (Raiko et al., 2012; Graves, 2013; Hermans and Schrauwen, 2013).", "startOffset": 130, "endOffset": 193}, {"referenceID": 10, "context": "Furthermore, following the gate design of LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) and highway networks (Srivastava et al.", "startOffset": 47, "endOffset": 100}, {"referenceID": 4, "context": "Furthermore, following the gate design of LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2000) and highway networks (Srivastava et al.", "startOffset": 47, "endOffset": 100}, {"referenceID": 27, "context": ", 2000) and highway networks (Srivastava et al., 2015a; Srivastava et al., 2015b), we observe that adding a multiplicative gate to the identity function will help to improve performance.", "startOffset": 29, "endOffset": 81}, {"referenceID": 28, "context": ", 2000) and highway networks (Srivastava et al., 2015a; Srivastava et al., 2015b), we observe that adding a multiplicative gate to the identity function will help to improve performance.", "startOffset": 29, "endOffset": 81}, {"referenceID": 17, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other.", "startOffset": 31, "endOffset": 50}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other.", "startOffset": 54, "endOffset": 77}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs.", "startOffset": 54, "endOffset": 166}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs.", "startOffset": 54, "endOffset": 181}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs.", "startOffset": 54, "endOffset": 211}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al.", "startOffset": 54, "endOffset": 468}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al.", "startOffset": 54, "endOffset": 486}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al.", "startOffset": 54, "endOffset": 514}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al.", "startOffset": 54, "endOffset": 533}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016), and Zilly et al.", "startOffset": 54, "endOffset": 554}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016), and Zilly et al. (2016). These works are all based on the design of extra connections between different layers.", "startOffset": 54, "endOffset": 579}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016), and Zilly et al. (2016). These works are all based on the design of extra connections between different layers. Srivastava et al. (2015b) and He et al.", "startOffset": 54, "endOffset": 693}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016), and Zilly et al. (2016). These works are all based on the design of extra connections between different layers. Srivastava et al. (2015b) and He et al. (2015) mainly focus on feed-forward neural network, using well-designed skip connections across different layers to make the information pass more easily.", "startOffset": 54, "endOffset": 714}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016), and Zilly et al. (2016). These works are all based on the design of extra connections between different layers. Srivastava et al. (2015b) and He et al. (2015) mainly focus on feed-forward neural network, using well-designed skip connections across different layers to make the information pass more easily. The Grid LSTM proposed by Kalchbrenner et al. (2015) extends the one dimensional LSTMs to many dimensional LSTMs, which provides a more general framework to construct deep LSTMs.", "startOffset": 54, "endOffset": 915}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016), and Zilly et al. (2016). These works are all based on the design of extra connections between different layers. Srivastava et al. (2015b) and He et al. (2015) mainly focus on feed-forward neural network, using well-designed skip connections across different layers to make the information pass more easily. The Grid LSTM proposed by Kalchbrenner et al. (2015) extends the one dimensional LSTMs to many dimensional LSTMs, which provides a more general framework to construct deep LSTMs. Yao et al. (2015) and Zhang et al.", "startOffset": 54, "endOffset": 1059}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016), and Zilly et al. (2016). These works are all based on the design of extra connections between different layers. Srivastava et al. (2015b) and He et al. (2015) mainly focus on feed-forward neural network, using well-designed skip connections across different layers to make the information pass more easily. The Grid LSTM proposed by Kalchbrenner et al. (2015) extends the one dimensional LSTMs to many dimensional LSTMs, which provides a more general framework to construct deep LSTMs. Yao et al. (2015) and Zhang et al. (2016) propose highway LSTMs by introducing gated direct connections between internal states in adjacent layers and do not use skip connections, while we propose gated skip connections across cell outputs.", "startOffset": 54, "endOffset": 1083}, {"referenceID": 2, "context": "For recurrent neural networks, Schmidhuber (1992); El Hihi and Bengio (1995) introduced deep RNNs by stacking hidden layers on top of each other. Raiko et al. (2012); Graves (2013); Hermans and Schrauwen (2013) proposed the use of skip connections in stacked RNNs. However, the researchers have paid less attention to the analyzing of various kinds of skip connections, which is our focus in this paper. The works closely related to ours are Srivastava et al. (2015b), He et al. (2015), Kalchbrenner et al. (2015), Yao et al. (2015), Zhang et al. (2016), and Zilly et al. (2016). These works are all based on the design of extra connections between different layers. Srivastava et al. (2015b) and He et al. (2015) mainly focus on feed-forward neural network, using well-designed skip connections across different layers to make the information pass more easily. The Grid LSTM proposed by Kalchbrenner et al. (2015) extends the one dimensional LSTMs to many dimensional LSTMs, which provides a more general framework to construct deep LSTMs. Yao et al. (2015) and Zhang et al. (2016) propose highway LSTMs by introducing gated direct connections between internal states in adjacent layers and do not use skip connections, while we propose gated skip connections across cell outputs. Zilly et al. (2016) introduce recurrent highway networks (RHN) which use a single recurrent layer to make RNN deep in a vertical direction, in contrast to our stacked models.", "startOffset": 54, "endOffset": 1302}, {"referenceID": 8, "context": "He et al. (2015) also use an identity mapping to train a very deep convolution neural network with improved performance.", "startOffset": 0, "endOffset": 17}, {"referenceID": 33, "context": "Following Wu et al. (2016), we add logistic gates to each token in the context window.", "startOffset": 10, "endOffset": 27}, {"referenceID": 3, "context": "Inspired by Fonseca et al. (2015) et al, which uses a character prefix and suffix with length from 1 to 5 for part-of-speech tagging, we concatenate character embeddings in a word to get the character-level representation.", "startOffset": 12, "endOffset": 34}, {"referenceID": 30, "context": "Detailed explanations of CCG refers to (Steedman, 2000; Steedman and Baldridge, 2011).", "startOffset": 39, "endOffset": 85}, {"referenceID": 29, "context": "Detailed explanations of CCG refers to (Steedman, 2000; Steedman and Baldridge, 2011).", "startOffset": 39, "endOffset": 85}, {"referenceID": 16, "context": "But we choose it since it has been already proved that a bidirectional recurrent net fits the task by many authors (Lewis et al., 2016; Vaswani et al., 2016).", "startOffset": 115, "endOffset": 157}, {"referenceID": 32, "context": "But we choose it since it has been already proved that a bidirectional recurrent net fits the task by many authors (Lewis et al., 2016; Vaswani et al., 2016).", "startOffset": 115, "endOffset": 157}, {"referenceID": 11, "context": "1 Dataset and Pre-processing Our experiments are performed on CCGBank (Hockenmaier and Steedman, 2007), which is a translation from Penn Treebank (Marcus et al.", "startOffset": 70, "endOffset": 102}, {"referenceID": 18, "context": "1 Dataset and Pre-processing Our experiments are performed on CCGBank (Hockenmaier and Steedman, 2007), which is a translation from Penn Treebank (Marcus et al., 1993) to CCG with a coverage 99.", "startOffset": 146, "endOffset": 167}, {"referenceID": 19, "context": "For non-recurrent weights, we initialize word embeddings with the pre-trained 200-dimensional GolVe vectors (Pennington et al., 2014).", "startOffset": 108, "endOffset": 133}, {"referenceID": 1, "context": "Model Dev Test Clark and Curran (2007) 91.", "startOffset": 15, "endOffset": 39}, {"referenceID": 1, "context": "Model Dev Test Clark and Curran (2007) 91.5 92.0 Lewis et al. (2014) 91.", "startOffset": 15, "endOffset": 69}, {"referenceID": 1, "context": "Model Dev Test Clark and Curran (2007) 91.5 92.0 Lewis et al. (2014) 91.3 91.6 Lewis et al. (2016) 94.", "startOffset": 15, "endOffset": 99}, {"referenceID": 1, "context": "Model Dev Test Clark and Curran (2007) 91.5 92.0 Lewis et al. (2014) 91.3 91.6 Lewis et al. (2016) 94.1 94.3 Xu et al. (2015) 93.", "startOffset": 15, "endOffset": 126}, {"referenceID": 1, "context": "Model Dev Test Clark and Curran (2007) 91.5 92.0 Lewis et al. (2014) 91.3 91.6 Lewis et al. (2016) 94.1 94.3 Xu et al. (2015) 93.1 93.0 Xu et al. (2016) 93.", "startOffset": 15, "endOffset": 153}, {"referenceID": 1, "context": "Model Dev Test Clark and Curran (2007) 91.5 92.0 Lewis et al. (2014) 91.3 91.6 Lewis et al. (2016) 94.1 94.3 Xu et al. (2015) 93.1 93.0 Xu et al. (2016) 93.49 93.52 Vaswani et al. (2016) 94.", "startOffset": 15, "endOffset": 187}, {"referenceID": 5, "context": "Orthogonal initialization for recurrent weights is important in our experiments, which takes about 2% relative performance enhancement than other methods such as Xavier initialization (Glorot and Bengio, 2010).", "startOffset": 184, "endOffset": 209}, {"referenceID": 21, "context": "For recurrent weight matrices, following Saxe et al. (2013) we initialize with random orthogonal matrices through SVD to avoid unstable gradients.", "startOffset": 41, "endOffset": 60}, {"referenceID": 38, "context": "We also tried other optimization methods, such as momentum (Plaut and others, 1986), Adadelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014), but none of them perform as well as SGD.", "startOffset": 94, "endOffset": 108}, {"referenceID": 13, "context": "We also tried other optimization methods, such as momentum (Plaut and others, 1986), Adadelta (Zeiler, 2012), or Adam (Kingma and Ba, 2014), but none of them perform as well as SGD.", "startOffset": 118, "endOffset": 139}, {"referenceID": 31, "context": "We trained the 7-layer network for roughly 2 to 3 days on one NVIDIA TITAN X GPU using Theano 1 (Team et al., 2016).", "startOffset": 96, "endOffset": 115}, {"referenceID": 26, "context": "Dropout (Srivastava et al., 2014) is the only regularizer in our model to avoid overfitting.", "startOffset": 8, "endOffset": 33}, {"referenceID": 39, "context": "We experiment with the gated identity connections between internal states introduced in Zhang et al.(2016), but the network performs not good (Table 2, 93.", "startOffset": 88, "endOffset": 107}, {"referenceID": 39, "context": "We experiment with the gated identity connections between internal states introduced in Zhang et al.(2016), but the network performs not good (Table 2, 93.14%). We also implement the method proposed in Zilly et al. (2016), which we use a single bidirectional RNH layer with a recurrent depth of 3 with a slightly modification2.", "startOffset": 88, "endOffset": 222}, {"referenceID": 39, "context": "Case Variant Dev Test H-LSTM, Zhang et al.(2016) 93.", "startOffset": 30, "endOffset": 49}, {"referenceID": 39, "context": "Case Variant Dev Test H-LSTM, Zhang et al.(2016) 93.14 93.52 RHN, Zilly et al. (2016) L = 3, with output gates 94.", "startOffset": 30, "endOffset": 86}, {"referenceID": 0, "context": "(Bengio et al., 2007; Simonyan and Zisserman, 2014), which is an important technique in training deep networks.", "startOffset": 0, "endOffset": 51}, {"referenceID": 24, "context": "(Bengio et al., 2007; Simonyan and Zisserman, 2014), which is an important technique in training deep networks.", "startOffset": 0, "endOffset": 51}, {"referenceID": 24, "context": "Model Test S\u00f8gaard (2011) 97.", "startOffset": 11, "endOffset": 26}, {"referenceID": 17, "context": "5 Ling et al. (2015) 97.", "startOffset": 2, "endOffset": 21}, {"referenceID": 17, "context": "5 Ling et al. (2015) 97.36 Wang et al. (2015) 97.", "startOffset": 2, "endOffset": 46}, {"referenceID": 17, "context": "5 Ling et al. (2015) 97.36 Wang et al. (2015) 97.78 Vaswani et al. (2016) 97.", "startOffset": 2, "endOffset": 74}], "year": 2016, "abstractText": "In this paper, we empirically explore the effects of various kinds of skip connections in stacked bidirectional LSTMs for sequential tagging. We investigate three kinds of skip connections connecting to LSTM cells: (a) skip connections to the gates, (b) skip connections to the internal states and (c) skip connections to the cell outputs. We present comprehensive experiments showing that skip connections to cell outputs outperform the remaining two. Furthermore, we observe that using gated identity functions as skip mappings works pretty well. Based on this novel skip connections, we successfully train deep stacked bidirectional LSTM models and obtain state-ofthe-art results on CCG supertagging and comparable results on POS tagging.", "creator": "LaTeX with hyperref package"}}}