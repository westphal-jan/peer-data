{"id": "1610.03256", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training", "abstract": "Recently, attempts have been made to remove Gaussian mixture models (GMM) from the training process of deep neural network-based hidden Markov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two problems, namely the initial alignment of the frame-level state labels and the creation of context-dependent states. Although flat-start training via iteratively realigning and retraining the DNN using a frame-level error function is viable, it is quite cumbersome. Here, we propose to use a sequence-discriminative training criterion for flat start. While sequence-discriminative training is routinely applied only in the final phase of model training, we show that with proper caution it is also suitable for getting an alignment of context-independent DNN models. For the construction of tied states we apply a recently proposed KL-divergence-based state clustering method, hence our whole training process is GMM-free. In the experimental evaluation we found that the sequence-discriminative flat start training method is not only significantly faster than the straightforward approach of iterative retraining and realignment, but the word error rates attained are slightly better as well.", "histories": [["v1", "Tue, 11 Oct 2016 09:52:57 GMT  (23kb)", "http://arxiv.org/abs/1610.03256v1", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["g\\'abor gosztolya", "tam\\'as gr\\'osz", "l\\'aszl\\'o t\\'oth"], "accepted": false, "id": "1610.03256"}, "pdf": {"name": "1610.03256.pdf", "metadata": {"source": "CRF", "title": "GMM-Free Flat Start Sequence-Discriminative DNN Training", "authors": ["G\u00e1bor Gosztolya"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 0.03 256v 1 [cs.C L] 11 Oct 201 6"}, {"heading": "1. Introduction", "text": "In fact, it is such that most of them will be able to move into another world, in which they are able to move, in which they move, in which they move, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they, in which they live, in which they, in which they live, in which they live, in which they, in which they live, in which they live, in which they live."}, {"heading": "2. Flat-start training of HMM/DNN", "text": "Typically, the formation of an HMM / DNN system is initiated by the formation of an HMM / GMM, just to obtain timely training labels. At this point, we compare two approaches aimed at removing GMM from this process. As a starting point, we apply a simple solution that iterates the cycle of CE-DNN training and realignment, and then propose an approach that creates timely transcriptions for the training data by training a DNN with a sequence training criterion. Out of the wide variety of sequence training methods, we have opted for MMI training (Maximum Mutual Information) [5]."}, {"heading": "2.1. Iterative CE Training and Realignment", "text": "For comparison, we are also testing what may be the simplest solution for flat-start DNN training, namely the use of the CE training criterion and the iterating DNN training and realignment, using the following algorithm based on the description provided by Zhang et al. [4]: 1. Train a DNN using uniformly segmented sound files. 2. Use the current DNN to realign the labels. 3. Train a randomly initialized DNN using the new alignments. 4. Repeat steps 2-3 several times. The last DNN was used to timed labels for the training set.The main advantage of this method is that it only requires an implementation of the CE training for the DNN, and the realignment step can also be easily performed with standard ASR toolkits. The downside is that the process of retraining and realignment tends to be quite time consuming, which is also confirmed by our experiments (see section 6)."}, {"heading": "2.2. Sequence-Discriminative Training Using MMI", "text": "Several sequence discriminating training criteria for HMM / GMMs [16] have been developed - and adapted to HMM / DNNs [5, 6, 12, 17] - of which the criterion of maximum mutual information (MMI) is the oldest and simplest. The MMI function measures the mutual information between the distribution of the observation and the phoneme sequence. It denotes the sequence of all observations according to Ou = ou1,.. ouTu and the labelling sequence for uttering u according to Wu, the MMI criterion can be defined by the formula FMMI = \u2211 ulog p (Ou | Su) \u03b1p (Wu) \u2211 W p (Ou | S) \u03b1p (W), (1) where Su = su1,. suTu is the sequence of the states according to WMMI (\u03b1\u03b1) and \u03b1 the acoustic scaling factor."}, {"heading": "3. Performing DNN Flat Start with MMI", "text": "This means that in order to be able to perform a flat start of random DNNs with sequence training, we have made some minor changes in the standard MMI process that we will describe. First, we use the numerator training in the flat start phase (r) in Eq. (2) instead of the sut values that we use with the rough targets during the DNN training. Another advantage of the training is that we use the values in Eq. (2) Instead of the sut values, we can work with the rough targets that are commonly used during the DNN training."}, {"heading": "4. KL divergence-based CD state tying", "text": "After aligning the cochlear implant phone models by means of flat start training, the next step is the construction of CD models. Currently, the prevailing solution to this is the decision tree-based state binding method [19]. This technique bundles all the context variants of a state and then builds a decision tree by gradually splitting this sentence into two, according to one of the predefined questions. For each step, it matches the Gaussians to the distribution of states and selects the question that leads to the highest probability increase. However, modelling the state binding of states with a Gaussian function could be suboptimal if we use DNNs in the final acoustic model. To this end, we have decided to first form an auxiliary neuronal network on the CI target marks and then perform the CD state binding based on the output of this network. Such a frame level output can be treated as a discrete probability distribution, and a natural distance function for such distributions we have replaced by the culler and the KL is the backup distribution."}, {"heading": "5. Experimental Setup", "text": "This year it has come to the point where we will be able to retaliate, \"he said.\" We are able to retaliate, \"he said,\" but we are not yet able to retaliate. \""}, {"heading": "6. Results and Discussion", "text": "This year it has come to the point that it is a purely reactionary project, in which it is a reactionary project, in which it is a reactionary project."}, {"heading": "7. Conclusions", "text": "Here we showed how to start with sequence-discriminating DNN training flat. We applied the standard MMI sequence training method, for which we introduced some minor modifications. Our results showed that compared to the standard procedure of iterative CE-DNN training and realignment, we were not only able to reduce the WER values, but also achieved a significant reduction in training times. By also proposing Kullback-Leibler divergence-based CD state binding, we made the entire training procedure of context-dependent HMM / DNN GMM-free."}, {"heading": "8. References", "text": "[1] G. Gosztolya, T. Gro \u0301 sz acoustic models, L. To \u0301 th, and D. Imseng, \"Buildingcontext-dependent DNN acousitc models using Kullback-Leibler divergence-based state tying,\" in Proceedings of ICASSP, 2015, pp. 4570-4574. [2] S. Young, G. Evermann, M. J. F. Gales, T. Hain, D. Kershaw, G. Moore, J. Odell, D. Povey, V. Valtchev, and P. Woodland, The HTK Book. Cambridge, UK: Cambridge University Engineering Department, 2006. [3] A. Senior, G. Heigold, M. Bacchiani, and H. Liao \"GMM-free DNN acoustic model training,\" in Proceedings of ICASSP, 2014, pp. 5639-5643. [4] C. Zhang and P. Woodland, Netalone training of contextdependent Deep network acoustic models, in \"Proceedings of ASSP, 2014."}], "references": [{"title": "Building context-dependent DNN acousitc models using Kullback-Leibler divergence-based state tying", "author": ["G. Gosztolya", "T. Gr\u00f3sz", "L. T\u00f3th", "D. Imseng"], "venue": "Proceedings of ICASSP, 2015, pp. 4570\u20134574.", "citeRegEx": "1", "shortCiteRegEx": null, "year": 2015}, {"title": "GMM-free DNN acoustic model training", "author": ["A. Senior", "G. Heigold", "M. Bacchiani", "H. Liao"], "venue": "Proceedings of ICASSP, 2014, pp. 5639\u20135643.", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2014}, {"title": "Standalone training of contextdependent Deep Neural Network acoustic models", "author": ["C. Zhang", "P. Woodland"], "venue": "Proceedings of ICASSP, 2014, pp. 5634\u20135638.", "citeRegEx": "4", "shortCiteRegEx": null, "year": 2014}, {"title": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling", "author": ["B. Kingsbury"], "venue": "Proceedings of ICASSP, 2009, pp. 3761\u20133764.", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2009}, {"title": "Sequencediscriminative training of deep neural networks", "author": ["K. Vesel\u00fd", "A. Ghoshal", "L. Burget", "D. Povey"], "venue": "Proceedings of Interspeech, 2013, pp. 2345\u20132349.", "citeRegEx": "6", "shortCiteRegEx": null, "year": 2013}, {"title": "A sequence training method for Deep Rectifier Neural Networks in speech recognition.", "author": ["T. Gr\u00f3sz", "G. Gosztolya", "L. T\u00f3th"], "venue": "Proceedings of SPECOM, Novi Sad, Serbia,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Sequence training of multiple Deep Neural Networks for better performance and faster training speed", "author": ["P. Zhou", "L. Dai", "H. Jiang"], "venue": "Proceedings of ICASSP, 2014, pp. 5664\u20135668.", "citeRegEx": "8", "shortCiteRegEx": null, "year": 2014}, {"title": "Asynchronous stochastic optimization for sequence training of Deep Neural Networks: Towards big data", "author": ["E. McDermott", "G. Heigold", "P. Moreno", "A. Senior", "M. Bacchiani"], "venue": "Proceedings of Interspeech, 2014, pp. 1224\u20131228.", "citeRegEx": "9", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigation of full-sequence training of Deep Belief Networks for speech recognition", "author": ["A. Mohamed", "D. Yu", "L. Deng"], "venue": "Proceedings of Interspeech, 2010, pp. 2846\u20132849.", "citeRegEx": "10", "shortCiteRegEx": null, "year": 2010}, {"title": "A comparison of two optimization techniques for sequence discriminative training of Deep Neural Networks", "author": ["G. Saon", "H. Soltau"], "venue": "Proceedings of ICASSP, 2014, pp. 5604\u20135608.", "citeRegEx": "11", "shortCiteRegEx": null, "year": 2014}, {"title": "Investigations on sequence training of neural networks", "author": ["S. Wiesler", "P. Golik", "R. Sch\u00fcter", "H. Ney"], "venue": "Proceedings of ICASSP, 2015, pp. 4565\u20134569.", "citeRegEx": "12", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint sequence training of phone and grapheme acoustic model based on multi-task learning Deep Neural Networks", "author": ["D. Chen", "B. Mak", "S. Sivadas"], "venue": "Proceedings of Interspeech, 2014, pp. 1083\u20131087.", "citeRegEx": "13", "shortCiteRegEx": null, "year": 2014}, {"title": "Speech recognition with Deep Recurrent Neural Networks", "author": ["A. Graves", "A.-R. Mohamed", "G.E. Hinton"], "venue": "Proceedings of ICASSP, 2013, pp. 6645\u20136649.", "citeRegEx": "14", "shortCiteRegEx": null, "year": 2013}, {"title": "Flat start training of CD-CTC- SMBR LSTM RNN acoustic models", "author": ["K. Rao", "A. Senior", "H. Sak"], "venue": "Proceedings of ICASSP, Shanghai, China, 2016, pp. 5405\u20135409.", "citeRegEx": "15", "shortCiteRegEx": null, "year": 2016}, {"title": "Discriminative Learning for Speech Recognition", "author": ["X. He", "L. Deng"], "venue": "San Rafael,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2008}, {"title": "Chapter 8: Deep neural network sequencediscriminative training", "author": ["D. Yu", "L. Deng"], "venue": "Automatic Speech Recognition \u2014 A Deep Learning Approach. Springer, October 2014.", "citeRegEx": "17", "shortCiteRegEx": null, "year": 2014}, {"title": "Annealed dropout training of deep networks", "author": ["S.J. Rennie", "V. Goel", "S. Thomas"], "venue": "Proceedings of SLT, South Lake Tahoe, NV, USA, 2014, pp. 159\u2013164.", "citeRegEx": "18", "shortCiteRegEx": null, "year": 2014}, {"title": "Tree-based state tying for high accuracy acoustic modelling", "author": ["S.J. Young", "J.J. Odell", "P.C. Woodland"], "venue": "Proceedings of HLT, 1994, pp. 307\u2013312.", "citeRegEx": "19", "shortCiteRegEx": null, "year": 1994}, {"title": "On information and sufficiency", "author": ["S. Kullback", "R. Leibler"], "venue": "Ann. Math. Statist., vol. 22, no. 1, pp. 79\u201386, 1951.", "citeRegEx": "20", "shortCiteRegEx": null, "year": 1951}, {"title": "Decision tree clustering for KL-HMM", "author": ["D. Imseng", "J. Dines"], "venue": "IDIAP Research Institute, Tech. Rep. Idiap-Com-01-2012, 2012.", "citeRegEx": "21", "shortCiteRegEx": null, "year": 2012}, {"title": "Comparing different acoustic modeling techniques for multilingual boosting", "author": ["D. Imseng", "J. Dines", "P. Motlicek", "P. Garner", "H. Bourlard"], "venue": "Proceedings of Interspeech, 2012.", "citeRegEx": "22", "shortCiteRegEx": null, "year": 2012}, {"title": "Deep sparse rectifier networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "Proceedings of AISTATS, 2011, pp. 315\u2013323.", "citeRegEx": "23", "shortCiteRegEx": null, "year": 2011}, {"title": "Convolutional deep maxout networks for phone recognition", "author": ["L. T\u00f3th"], "venue": "Proceedings of Interspeech, 2014, pp. 1078\u20131082.", "citeRegEx": "24", "shortCiteRegEx": null, "year": 2014}, {"title": "Assessing the degree of nativeness and Parkinson\u2019s condition using Gaussian Processes and Deep Rectifier Neural Networks", "author": ["T. Gr\u00f3sz", "R. Busa-Fekete", "G. Gosztolya", "L. T\u00f3th"], "venue": "Proceedings of Interspeech, Sep 2015, pp. 1339\u20131343.", "citeRegEx": "25", "shortCiteRegEx": null, "year": 2015}, {"title": "Detecting the intensity of cognitive and physical load using AdaBoost and Deep Rectifier Neural Networks", "author": ["G. Gosztolya", "T. Gr\u00f3sz", "R. Busa-Fekete", "L. T\u00f3th"], "venue": "Proceedings of Interspeech, Singapore, Sep 2014, pp. 452\u2013456.", "citeRegEx": "26", "shortCiteRegEx": null, "year": 2014}, {"title": "Automatic detection of mild cognitive impairment from spontaneous speech using ASR", "author": ["L. T\u00f3th", "G. Gosztolya", "V. Vincze", "I. Hoffmann", "G. Szatl\u00f3czki", "E. Bir\u00f3", "F. Zsura", "M. P\u00e1k\u00e1ski", "J. K\u00e1lm\u00e1n"], "venue": "Proceedings of Interspeech, Dresden, Germany, Sep 2015, pp. 2694\u20132698.", "citeRegEx": "27", "shortCiteRegEx": null, "year": 2015}, {"title": "Joint optimization of spectro-temporal features and Deep Neural Nets for robust automatic speech recognition", "author": ["Gy. Kov\u00e1cs", "L. T\u00f3th"], "venue": "Acta Cybernetica, vol. 22, no. 1, pp. 117\u2013134, 2015.", "citeRegEx": "28", "shortCiteRegEx": null, "year": 2015}, {"title": "A comparison of Deep Neural Network training methods for Large Vocabulary Speech Recognition", "author": ["T. Gr\u00f3sz", "L. T\u00f3th"], "venue": "Proceedings of TSD, Pilsen, Czech Republic, 2013, pp. 36\u201343.", "citeRegEx": "29", "shortCiteRegEx": null, "year": 2013}, {"title": "Asynchronous, online, GMM-free training of a context dependent acoustic model for speech recognition", "author": ["M. Bacchiani", "A. Senior", "G. Heigold"], "venue": "Proceedings of Interspeech, Singapore, Singapore, Sep 2014, pp. 1900\u20131904.", "citeRegEx": "30", "shortCiteRegEx": null, "year": 2014}, {"title": "Error back propagation for sequence training of context-dependent deep networks for conversational speech transcription", "author": ["H. Su", "G. Li", "D. Yu", "F. Seide"], "venue": "Proceedings of ICASSP, 2013, pp. 6664\u20136668.", "citeRegEx": "31", "shortCiteRegEx": null, "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "We proposed a GMM-free solution for state clustering earlier [1], and in this study we will focus on the issue of obtaining the initial state alignment for training the DNN.", "startOffset": 61, "endOffset": 64}, {"referenceID": 1, "context": "randomly initialized their neural network [3], while Zhang et al.", "startOffset": 42, "endOffset": 45}, {"referenceID": 2, "context": "trained their first model on equal-sized segments for each state [4].", "startOffset": 65, "endOffset": 68}, {"referenceID": 3, "context": "Within the framework of HMM/GMM systems, several sequence-discriminative training methods have been developed, and these have now been adapted to HMM/DNN hybrids as well [5, 6, 7].", "startOffset": 170, "endOffset": 179}, {"referenceID": 4, "context": "Within the framework of HMM/GMM systems, several sequence-discriminative training methods have been developed, and these have now been adapted to HMM/DNN hybrids as well [5, 6, 7].", "startOffset": 170, "endOffset": 179}, {"referenceID": 5, "context": "Within the framework of HMM/GMM systems, several sequence-discriminative training methods have been developed, and these have now been adapted to HMM/DNN hybrids as well [5, 6, 7].", "startOffset": 170, "endOffset": 179}, {"referenceID": 6, "context": "[8, 9, 10]) or just to provide frame-level state labels (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 7, "context": "[8, 9, 10]) or just to provide frame-level state labels (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 8, "context": "[8, 9, 10]) or just to provide frame-level state labels (e.", "startOffset": 0, "endOffset": 10}, {"referenceID": 3, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 4, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 9, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 10, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 11, "context": "[5, 6, 11, 12, 13]).", "startOffset": 0, "endOffset": 18}, {"referenceID": 12, "context": "The Connectionist Temporal Classification (CTC) approach has recently become very popular for training DNNs without an initial time alignment being available [14].", "startOffset": 158, "endOffset": 162}, {"referenceID": 13, "context": "proposed a flat start training procedure which is built on CTC [15].", "startOffset": 63, "endOffset": 67}, {"referenceID": 12, "context": "Second, the CTC algorithm is not a sequence-discriminative training method, so for the best performance it has to be combined with techniques like sMBR training [14, 15].", "startOffset": 161, "endOffset": 169}, {"referenceID": 13, "context": "Second, the CTC algorithm is not a sequence-discriminative training method, so for the best performance it has to be combined with techniques like sMBR training [14, 15].", "startOffset": 161, "endOffset": 169}, {"referenceID": 2, "context": "[4], and we find that our method is faster and gives slightly lower word error rates.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "Furthermore, we can combine sequence-discriminative flat start training with the KullbackLeibler divergence-based state clustering method we proposed recently [1].", "startOffset": 159, "endOffset": 162}, {"referenceID": 3, "context": "From the wide variety of sequence training methods, we opted for MMI (Maximum Mutual Information) training [5].", "startOffset": 107, "endOffset": 110}, {"referenceID": 2, "context": "[4]:", "startOffset": 0, "endOffset": 3}, {"referenceID": 14, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 83, "endOffset": 87}, {"referenceID": 3, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 114, "endOffset": 128}, {"referenceID": 4, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 114, "endOffset": 128}, {"referenceID": 10, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 114, "endOffset": 128}, {"referenceID": 15, "context": "Several sequence-discriminative training criteria have been developed for HMM/GMMs [16] \u2013 and adapted to HMM/DNNs [5, 6, 12, 17] \u2013 from which the maximum mutual information (MMI) criterion is the oldest and simplest.", "startOffset": 114, "endOffset": 128}, {"referenceID": 4, "context": "However, all authors initialize their networks using CE training, and apply the sequencediscriminative criterion only in the final phase of the training procedure, to fine-tune their models [6, 12], which makes it necessary to use some method (HMM/GMM or iterative CE training) to provide frame-level state targets.", "startOffset": 190, "endOffset": 197}, {"referenceID": 10, "context": "However, all authors initialize their networks using CE training, and apply the sequencediscriminative criterion only in the final phase of the training procedure, to fine-tune their models [6, 12], which makes it necessary to use some method (HMM/GMM or iterative CE training) to provide frame-level state targets.", "startOffset": 190, "endOffset": 197}, {"referenceID": 4, "context": "[6, 17]), but we only found Zhou et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 15, "context": "[6, 17]), but we only found Zhou et al.", "startOffset": 0, "endOffset": 7}, {"referenceID": 6, "context": "[8] actually doing this.", "startOffset": 0, "endOffset": 3}, {"referenceID": 16, "context": "[18]).", "startOffset": 0, "endOffset": 4}, {"referenceID": 3, "context": "This strategy can be readily adapted to sequence DNN training [5], and we found it to be essential for the stability of our flat-start MMI DNN training method.", "startOffset": 62, "endOffset": 65}, {"referenceID": 17, "context": "Currently, the dominant solution for this is the decision tree-based state tying method [19].", "startOffset": 88, "endOffset": 92}, {"referenceID": 18, "context": "Such a frame-level output can be treated as a discrete probability distribution, and a natural distance function for such distributions is the KullbackLeibler (KL) divergence [20].", "startOffset": 175, "endOffset": 179}, {"referenceID": 19, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 20, "context": "[21, 22].", "startOffset": 0, "endOffset": 8}, {"referenceID": 0, "context": "For details, see [1].", "startOffset": 17, "endOffset": 20}, {"referenceID": 0, "context": "Our experimental setup is essentially the same as that of our previous study [1].", "startOffset": 77, "endOffset": 80}, {"referenceID": 21, "context": "We employed a DNN with 5 hidden layers, each containing 1000 rectified neurons [23], while the softmax activation function was applied in the output layer.", "startOffset": 79, "endOffset": 83}, {"referenceID": 22, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 23, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 24, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 25, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 26, "context": "[24, 25, 26, 27, 28]).", "startOffset": 0, "endOffset": 20}, {"referenceID": 27, "context": "The 28 hour-long speech corpus of Hungarian broadcast news [29] was collected from eight TV channels.", "startOffset": 59, "endOffset": 63}, {"referenceID": 0, "context": "(In our previous study we found that using a deep network for this re-alignment setup did not give any significant improvement [1].", "startOffset": 127, "endOffset": 130}, {"referenceID": 2, "context": "performed 20 such iterations [4], while we employed only 4 iterations.", "startOffset": 29, "endOffset": 32}, {"referenceID": 0, "context": "The reason is that for iterative CE flat-start training we used a mini-batch of 100 frames (which we found optimal previously [1]), while for MMI whole utterances (usually more than 1000 frames) were used to update the weights, and this allowed better parallelization on the GPU.", "startOffset": 126, "endOffset": 129}, {"referenceID": 28, "context": "[30].", "startOffset": 0, "endOffset": 4}, {"referenceID": 29, "context": "As regards stability, a known drawback of sequence training methods is that the same process is responsible both for aligning and training the DNN, which often leads to the \u201crun-away silence model\u201d issue [31].", "startOffset": 204, "endOffset": 208}], "year": 2016, "abstractText": "Recently, attempts have been made to remove Gaussian mixture models (GMM) from the training process of deep neural network-based hidden Markov models (HMM/DNN). For the GMM-free training of a HMM/DNN hybrid we have to solve two problems, namely the initial alignment of the frame-level state labels and the creation of context-dependent states. Although flat-start training via iteratively realigning and retraining the DNN using a frame-level error function is viable, it is quite cumbersome. Here, we propose to use a sequencediscriminative training criterion for flat start. While sequencediscriminative training is routinely applied only in the final phase of model training, we show that with proper caution it is also suitable for getting an alignment of context-independent DNN models. For the construction of tied states we apply a recently proposed KL-divergence-based state clustering method, hence our whole training process is GMM-free. In the experimental evaluation we found that the sequence-discriminative flat start training method is not only significantly faster than the straightforward approach of iterative retraining and realignment, but the word error rates attained are slightly better as well.", "creator": "LaTeX with hyperref package"}}}