{"id": "1610.03321", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "Keystroke dynamics as signal for shallow syntactic parsing", "abstract": "Keystroke dynamics have been extensively used in psycholinguistic and writing research to gain insights into cognitive processing. But do keystroke logs contain actual signal that can be used to learn better natural language processing models?", "histories": [["v1", "Tue, 11 Oct 2016 13:20:52 GMT  (489kb,D)", "http://arxiv.org/abs/1610.03321v1", "In COLING 2016"]], "COMMENTS": "In COLING 2016", "reviews": [], "SUBJECTS": "cs.CL", "authors": ["barbara plank"], "accepted": false, "id": "1610.03321"}, "pdf": {"name": "1610.03321.pdf", "metadata": {"source": "CRF", "title": "Keystroke dynamics as signal for shallow syntactic parsing", "authors": ["Barbara Plank"], "emails": ["b.plank@rug.nl"], "sections": [{"heading": null, "text": "We postulate that keystroke dynamics contains information about the syntactic structure that can be incorporated into a flat syntactic parsing. To test this hypothesis, we are investigating labels derived from keyboard loggers as auxiliary tasks in a multi-task bi-directional long-term memory (bi-LSTM). Our results show promising results for two flat syntactic parsing tasks, chunking and CCG supertagging. Our model is simple, has the advantage that the data can come from different sources, and produces models that are significantly better than models trained solely on text annotations."}, {"heading": "1 Introduction", "text": "The fact is that we are able to hide in the way that we have done in the past: in the way that we have done it, in the way that we have done it, in the way that we have done it, in the way that we have done it, in the way that we have done it, in the way that we have done it. \""}, {"heading": "2 Keystroke dynamics", "text": "We consider keystroke dynamics to be a complementary view of the data beyond the linguistic signal, which is easy to capture and particularly attractive to build robust models for external configurations. Keystroke logs can be considered as an instance of random data (Plank, 2016); it is a side benefit of the behavior we are trying to exploit here. However, keystroke logs are raw data and therefore need to be refined before they can be used. Our idea is to treat the duration of pauses before words as a simple sequence marking problem. we first describe the process of obtaining automatically labeled data from raw keystroke logs and then provide background and motivation for this choice. Section 3 then describes our model, i.e. by solving the problem of keystroke labeling together with flat syntactical parsing tasks (chunking and CCG supertagging)."}, {"heading": "2.1 From keystroke logs to auxiliary labels", "text": "While key dynamics take into account a number of timing parameters, such as holding time and time and automatically releasing the time between each key press (p in Figure 1), this study is only concerned with the pause before a word (i.e. the third p in Figure 1).1 We are using a simple tokenization scheme here. However, if we take an arbitrary threshold of 500 ms, the fractions indicated by the brackets are derived as sentence boundaries, confirming that pre-word pauses are arbitrary information. However, the typing behavior of users differs, as shown in Figure 2."}, {"heading": "2.2 Background", "text": "In fact, most people are able to decide for themselves what they want and what they want."}, {"heading": "3 Tagging with bi-LSTMs", "text": "We build on the recent success of bidirectional relapsing neural networks (bi-RNNs) (Graves and Schmidhuber, 2005), in particular Long Short Term Memory (LSTM) models (Hochreiter and Schmidhuber, 1997), which read the input sequences twice in both directions. Bi-LSTM has recently been successfully used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015)."}, {"heading": "3.1 Bidirectional Long-Short Term Memory Models", "text": "Our model is a hierarchical bi-LSTM as illustrated in Figure 5. It takes as input word embeddings ~ w associated with embeddings of characters from the last two states (forward, backward) of operation of a lower level bi-LSTM on the characters. In detail, however, our model is a context bi-LSTM as additional information has proven effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016). Our model is a context bi-LSTM as input word embeddings ~ w. Note: Character embeddings ~ c are made via a hierarchical bi-LSTM using a sequence bi-LSTM on the lower level (Ballesteros et al., 2015; Plank et al., 2016). Character representation is associated with (learned) word embeddings ~ to form the input into the STana context."}, {"heading": "4 Experiments", "text": "We implement our model in CNN / pycnn.4 For all experiments, we use the same hyperparameters set on a reserved portion of the CoNLL 2000 data, i.e., SGD with cross-entropy loss, no mini-batches, 30 epochs, default learning rate (0.1), 64 dimensions for word embedding, 100 for character embedding, random initialization for all embedding, 100 hidden states, h = 3 stacked layers, Gaussian noise with \u03c3 = 0.2. As the training is poke-elastic, we use a fixed seed (selected and fixed embedding), no further unlabeled data is taken into account. Datasets An overview of the syntactic datasets considered in this paper is in Table 3. For chunking, we use the original CoNLL data (Tjong Kim Sang and Buchholz, 2000) from test data from WJ-36 test data, we use the test data for section 39K-18 and 39K-36 as training data."}, {"heading": "4.1 Results", "text": "The results are summarized in the table. We do not use POS information."}, {"heading": "5 Discussion", "text": "To get a better idea of what the model has learned, Table 6 provides the per-label breakdown for chunking, Table 7 for CCG tagging. Most of the improvements come from noun phrases (NP) chunks. From manual inspection, we note that the model is improving, especially compared to unconventional spelling and fragmented noun phrases typical of Twitter, see examples from Table 8. As Table 6 shows, keystroke data also helps for verb phrases on a record. The current encoding is not so advantageous for PPs. Pauses before prepositions are short, as illustrated in Figure 4, and pauses often fall into auxiliary annotation segments, while prepositions are separate tokens in chunking. Therefore, it is not surprising that the model performs worse for PPs. We believe that our pause, which mainly gathers structural information between words, is less morphosyntactic information itself, i.e. pauses are more intuitive than pauses for small tactical structures in the language."}, {"heading": "6 Related Work", "text": "Keystroke logging has become a promising tool for research into writing (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), since time measurements can provide insights into cognitive processes associated with writing (Nottbusch et al., 2007) or translation studies. In fact, most previous work using keystroke logs has focused on experimental research. For example, Hanoulle et al. (2015) investigated whether a bilingual glossary reduces the work time of professional translators. They looked at pauses before keystroke logs and found that a bilingual glossary in the translation process of documentary films reduces the workload of translators. Other translation research combined eye-tracking data with keystroke logs to study the translation process (Carl et al al al, 2016). An analysis of typing behavior by users was conducted by Babtrua Bintrutic and Suzuki (2012)."}, {"heading": "7 Conclusions", "text": "Our model, a Bi-LSTM, integrates key data as an auxiliary task and surpasses models trained solely on the linguistic signal. We get promising results for two syntactic tasks, chunking and CCG supertagging. This justifies many directions for future research, such as using information from the nonlinear writing process that we have left out here (e.g. revisions), evaluating other languages, and completing the full parsing task."}, {"heading": "Acknowledgements", "text": "I would like to thank Veerle Baaijen for the insightful discussions and the three anonymous reviewers, as well as He'ctor Mart\u00ed \"nez Alonso, Maria Barrett and Zeljko Agic\" for their comments on earlier drafts of this paper. I would like to thank the Centre for Information Technology at the University of Groningen for their support and for providing access to the Peregrine High Performance Computing (HPC) cluster, as well as NVIDIA Corporation for supporting my research."}], "references": [{"title": "Keystroke analysis: Reflections on procedures and measures", "author": ["David Galbraith", "Kees de Glopper"], "venue": "Written Communication,", "citeRegEx": "Baaijen et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baaijen et al\\.", "year": 2012}, {"title": "How are spelling errors generated and corrected?: a study of corrected and uncorrected spelling errors using keystroke logs", "author": ["Baba", "Suzuki2012] Yukino Baba", "Hisami Suzuki"], "venue": "In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume", "citeRegEx": "Baba et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Baba et al\\.", "year": 2012}, {"title": "Improved transition-based parsing by modeling characters instead of words with lstms", "author": ["Chris Dyer", "Noah A. Smith"], "venue": null, "citeRegEx": "Ballesteros et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ballesteros et al\\.", "year": 2015}, {"title": "Using reading behavior to predict grammatical functions", "author": ["Barrett", "S\u00f8gaard2015] Maria Barrett", "Anders S\u00f8gaard"], "venue": "In Workshop on Cognitive Aspects of Computational Language Learning", "citeRegEx": "Barrett et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2015}, {"title": "Part-of-speech induction from eye-tracking data", "author": ["Joachim Bingel", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Barrett et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Barrett et al\\.", "year": 2016}, {"title": "Part-of-speech induction from fmri", "author": ["Maria Barrett", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Bingel et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Bingel et al\\.", "year": 2016}, {"title": "Measuring the translation process", "author": ["Carl et al.2016] Michael Carl", "Isabel Lacruz", "Masaru Yamada", "Akiko Aizawa"], "venue": "In The 22nd Annual Meeting of the Association for Natural Language Processing", "citeRegEx": "Carl et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Carl et al\\.", "year": 2016}, {"title": "Multitask learning", "author": ["Rich Caruana"], "venue": "In Learning to learn,", "citeRegEx": "Caruana.,? \\Q1998\\E", "shortCiteRegEx": "Caruana.", "year": 1998}, {"title": "Natural language understanding with distributed representation. ArXiv, abs/1511.07916", "author": ["Kyunghyun Cho"], "venue": null, "citeRegEx": "Cho.,? \\Q2015\\E", "shortCiteRegEx": "Cho.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Transition-based dependency parsing with stack long short-term memory", "author": ["Dyer et al.2015] Chris Dyer", "Miguel Ballesteros", "Wang Ling", "Austin Matthews", "Noah A. Smith"], "venue": null, "citeRegEx": "Dyer et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Dyer et al\\.", "year": 2015}, {"title": "From news to comments: Resources and benchmarks for parsing the language of Web 2.0", "author": ["Ozlem Cetinoglu", "Joachim Wagner", "Josef Le Roux", "Joakim Nivre", "Deirde Hogan", "Josef van Genabith"], "venue": "In IJCNLP", "citeRegEx": "Foster et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Foster et al\\.", "year": 2011}, {"title": "Multilingual language processing from bytes", "author": ["Gillick et al.2015] Dan Gillick", "Cliff Brunk", "Oriol Vinyals", "Amarnag Subramanya"], "venue": null, "citeRegEx": "Gillick et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Gillick et al\\.", "year": 2015}, {"title": "A primer on neural network models for natural language processing. ArXiv, abs/1510.00726", "author": ["Yoav Goldberg"], "venue": null, "citeRegEx": "Goldberg.,? \\Q2015\\E", "shortCiteRegEx": "Goldberg.", "year": 2015}, {"title": "Muddying the multiword expression waters: How cognitive demand affects multiword expression production", "author": ["Goodkind", "Rosenberg2015] Adam Goodkind", "Andrew Rosenberg"], "venue": "In Proceedings of MWE", "citeRegEx": "Goodkind et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Goodkind et al\\.", "year": 2015}, {"title": "Framewise phoneme classification with bidirectional lstm and other neural network architectures", "author": ["Graves", "Schmidhuber2005] Alex Graves", "J\u00fcrgen Schmidhuber"], "venue": "Neural Networks,", "citeRegEx": "Graves et al\\.,? \\Q2005\\E", "shortCiteRegEx": "Graves et al\\.", "year": 2005}, {"title": "The translation of documentaries: Can domain-specific, bilingual glossaries reduce the translators\u2019 workload? an experiment involving professional translators", "author": ["V\u00e9ronique Hoste", "Aline Remael"], "venue": "New Voices in Translation Studies,", "citeRegEx": "Hanoulle et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Hanoulle et al\\.", "year": 2015}, {"title": "Long short-term memory", "author": ["Hochreiter", "Schmidhuber1997] Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "venue": "Neural computation,", "citeRegEx": "Hochreiter et al\\.,? \\Q1997\\E", "shortCiteRegEx": "Hochreiter et al\\.", "year": 1997}, {"title": "Bidirectional lstm-crf models for sequence tagging", "author": ["Huang et al.2015] Zhiheng Huang", "Wei Xu", "Kai Yu"], "venue": "arXiv preprint arXiv:1508.01991", "citeRegEx": "Huang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Huang et al\\.", "year": 2015}, {"title": "Humans have idiosyncratic and task-specific scanpaths for judging faces", "author": ["Dina NF Bseiso", "Nicholas A Ray", "Janet H Hsiao", "Garrison W Cottrell"], "venue": "Vision research,", "citeRegEx": "Kanan et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Kanan et al\\.", "year": 2015}, {"title": "Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations. ArXiv e-prints", "author": ["Kiperwasser", "Goldberg2016] E. Kiperwasser", "Y. Goldberg"], "venue": null, "citeRegEx": "Kiperwasser et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Kiperwasser et al\\.", "year": 2016}, {"title": "Improving sentence compression by learning to predict gaze", "author": ["Klerke et al.2016] Sigrid Klerke", "Yoav Goldberg", "Anders S\u00f8gaard"], "venue": null, "citeRegEx": "Klerke et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Klerke et al\\.", "year": 2016}, {"title": "Detecting outliers: Do not use standard deviation around the mean, use absolute deviation around the median", "author": ["Leys et al.2013] Christophe Leys", "Christophe Ley", "Olivier Klein", "Philippe Bernard", "Laurent Licata"], "venue": "Journal of Experimental Social Psychology,", "citeRegEx": "Leys et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Leys et al\\.", "year": 2013}, {"title": "Wiki-ly supervised part-of-speech tagging", "author": ["Li et al.2012] Shen Li", "Jo\u00e3o Gra\u00e7a", "Ben Taskar"], "venue": null, "citeRegEx": "Li et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Li et al\\.", "year": 2012}, {"title": "Finding function in form: Compositional character models for open vocabulary word representation", "author": ["Ling et al.2015] Wang Ling", "Chris Dyer", "Alan W Black", "Isabel Trancoso", "Ramon Fermandez", "Silvio Amir", "Luis Marujo", "Tiago Luis"], "venue": "In EMNLP", "citeRegEx": "Ling et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ling et al\\.", "year": 2015}, {"title": "Fine-grained opinion mining with recurrent neural networks and word embeddings", "author": ["Liu et al.2015] Pengfei Liu", "Shafiq Joty", "Helen Meng"], "venue": null, "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Continuous authentication with cognition-centric text production and revision features", "author": ["Sathya Govindarajan", "Zdenka Sitova", "Adam Goodkind", "David Guy Brizan", "Andrew Rosenberg", "Vir V Phoha", "Paolo Gasti", "Kiran S Balagani"], "venue": "In Biometrics (IJCB),", "citeRegEx": "Locklear et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Locklear et al\\.", "year": 2014}, {"title": "Multi-task sequence to sequence learning", "author": ["Quoc V. Le", "Ilya Sutskever", "Oriol Vinyals", "Lukasz Kaiser"], "venue": "In ICLR", "citeRegEx": "Luong et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Luong et al\\.", "year": 2016}, {"title": "Behavioral biometric verification of student identity in online course assessment and authentication of authors in literary works", "author": ["Monaco et al.2013] John V Monaco", "John C Stewart", "Sung-Hyuk Cha", "Charles C Tappert"], "venue": "In Biometrics: Theory, Applications and Systems (BTAS),", "citeRegEx": "Monaco et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Monaco et al\\.", "year": 2013}, {"title": "Computer-intensive methods for testing hypotheses: an introduction", "author": ["Eric Noreen"], "venue": null, "citeRegEx": "Noreen.,? \\Q1989\\E", "shortCiteRegEx": "Noreen.", "year": 1989}, {"title": "From written word to written sentence production", "author": ["Rdiger Weingarten", "Said Sahel"], "venue": "Writing and cognition: Research and applications.,", "citeRegEx": "Nottbusch et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Nottbusch et al\\.", "year": 2007}, {"title": "Learning part-of-speech taggers with inter-annotator agreement loss", "author": ["Plank et al.2014] Barbara Plank", "Dirk Hovy", "Anders S\u00f8gaard"], "venue": "In EACL", "citeRegEx": "Plank et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2014}, {"title": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss", "author": ["Plank et al.2016] Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg"], "venue": null, "citeRegEx": "Plank et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Plank et al\\.", "year": 2016}, {"title": "What to do about non-standard (or non-canonical) language in NLP", "author": ["Barbara Plank"], "venue": "In KONVENS", "citeRegEx": "Plank.,? \\Q2016\\E", "shortCiteRegEx": "Plank.", "year": 2016}, {"title": "Extracting Knowledge from Twitter and The Web", "author": ["Alan Ritter"], "venue": "Ph.D. thesis,", "citeRegEx": "Ritter.,? \\Q2011\\E", "shortCiteRegEx": "Ritter.", "year": 2011}, {"title": "Deep multi-task learning with low level tasks supervised at lower layers", "author": ["S\u00f8gaard", "Goldberg2016] Anders S\u00f8gaard", "Yoav Goldberg"], "venue": null, "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2016\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2016}, {"title": "What\u2019s in a p-value in nlp? In CoNLL", "author": ["Anders Johannsen", "Barbara Plank", "Dirk Hovy", "H\u00e9ctor Mart\u0131\u0301nez Alonso"], "venue": null, "citeRegEx": "S\u00f8gaard et al\\.,? \\Q2014\\E", "shortCiteRegEx": "S\u00f8gaard et al\\.", "year": 2014}, {"title": "Keystroke logging: an introduction", "author": ["Spelman Miller", "Kirk PH Sullivan"], "venue": null, "citeRegEx": "Miller et al\\.,? \\Q2006\\E", "shortCiteRegEx": "Miller et al\\.", "year": 2006}, {"title": "An investigation of keystroke and stylometry traits for authenticating online test takers", "author": ["John V Monaco", "Sung-Hyuk Cha", "Charles C Tappert"], "venue": "In Biometrics (IJCB),", "citeRegEx": "Stewart et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Stewart et al\\.", "year": 2011}, {"title": "Computer keystroke logging and writing: Methods and applications", "author": ["Eva Lindgren"], "venue": null, "citeRegEx": "Sullivan and Lindgren,? \\Q2006\\E", "shortCiteRegEx": "Sullivan and Lindgren", "year": 2006}, {"title": "Semi-supervised sequential labeling and segmentation using giga-word scale unlabeled data", "author": ["Suzuki", "Isozaki2008] Jun Suzuki", "Hideki Isozaki"], "venue": null, "citeRegEx": "Suzuki et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Suzuki et al\\.", "year": 2008}, {"title": "Introduction to the conll2000 shared task: Chunking", "author": ["Tjong Kim Sang", "Sabine Buchholz"], "venue": "In CoNLL", "citeRegEx": "Sang et al\\.,? \\Q2000\\E", "shortCiteRegEx": "Sang et al\\.", "year": 2000}, {"title": "Keystroke logging in writing research: Observing writing processes with inputlog. GFL-German as a foreign language, 2(3):41\u201364", "author": ["Mari\u00eblle Leijten", "Daphne Van Weijen"], "venue": null, "citeRegEx": "Waes et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Waes et al\\.", "year": 2009}, {"title": "Supertagging with lstms", "author": ["Yonatan Bisk", "Kenji Sagae", "Ryan Musa"], "venue": null, "citeRegEx": "Vaswani et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Vaswani et al\\.", "year": 2016}, {"title": "Part-of-speech tagging with bidirectional long short-term memory recurrent neural network. pre-print, abs/1510.06168", "author": ["Wang et al.2015] Peilu Wang", "Yao Qian", "Frank K. Soong", "Lei He", "Hai Zhao"], "venue": null, "citeRegEx": "Wang et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Wang et al\\.", "year": 2015}, {"title": "Aligning context-based statistical models of language with brain activity", "author": ["Wehbe et al.2014] Leila Wehbe", "Ashish Vaswani", "Kevin Knight", "Tom Mitchell"], "venue": null, "citeRegEx": "Wehbe et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Wehbe et al\\.", "year": 2014}, {"title": "Ccg supertagging with a recurrent neural network", "author": ["Xu et al.2015] Wenduan Xu", "Michael Auli", "Stephen Clark"], "venue": null, "citeRegEx": "Xu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Xu et al\\.", "year": 2015}], "referenceMentions": [{"referenceID": 21, "context": "For instance, eye tracking data can inform sentence compression (Klerke et al., 2016) and gaze is predictive for part-of-speech (Barrett and S\u00f8gaard, 2015; Barrett et al.", "startOffset": 64, "endOffset": 85}, {"referenceID": 4, "context": ", 2016) and gaze is predictive for part-of-speech (Barrett and S\u00f8gaard, 2015; Barrett et al., 2016).", "startOffset": 50, "endOffset": 99}, {"referenceID": 33, "context": "Keystroke logging data can be seen as an instance of fortuitous data (Plank, 2016); it is side benefit of behavior that we want to exploit here.", "startOffset": 69, "endOffset": 82}, {"referenceID": 22, "context": "(2016) use a related encoding scheme to discretize fixation durations obtained from eye tracking data, however, in contrast to them we here use median-based measures which are better suited for such highly skewed data (Leys et al., 2013).", "startOffset": 218, "endOffset": 237}, {"referenceID": 21, "context": "Klerke et al. (2016) use a related encoding scheme to discretize fixation durations obtained from eye tracking data, however, in contrast to them we here use median-based measures which are better suited for such highly skewed data (Leys et al.", "startOffset": 0, "endOffset": 21}, {"referenceID": 30, "context": "Keystroke logging has developed to a promising tool in writing research (Sullivan et al., 2006; Nottbusch et al., 2007; Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), where time measurements\u2014pauses, bursts and revisions (described below)\u2014are studied as traces of the recursive nature of the writing process.", "startOffset": 72, "endOffset": 180}, {"referenceID": 0, "context": "Keystroke logging has developed to a promising tool in writing research (Sullivan et al., 2006; Nottbusch et al., 2007; Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), where time measurements\u2014pauses, bursts and revisions (described below)\u2014are studied as traces of the recursive nature of the writing process.", "startOffset": 72, "endOffset": 180}, {"referenceID": 0, "context": "Such a cutoff is rather arbitrary (Baaijen et al., 2012), and from our own experience results in long chunks.", "startOffset": 34, "endOffset": 56}, {"referenceID": 0, "context": "2 In contrast, writing research (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012) defines pauses as the start time of a keystroke until the start time of the next keystroke.", "startOffset": 32, "endOffset": 93}, {"referenceID": 0, "context": "Such a cutoff is rather arbitrary (Baaijen et al., 2012), and from our own experience results in long chunks. Taking writing research as a starting point, we postulate that keystrokes contain further fine-grained information that help identify syntactic chunks. We aim at a finer-grained representation, and transform user-based average statistics into automatically derived labels (cf. above). We notice that the literature defines different ways to define a pause. Goodkind and Rosenberg (2015), coming from a stylometry background, use the difference between release time of the previous key and the timepress of the current key to calculate pre-word pause duration.", "startOffset": 35, "endOffset": 497}, {"referenceID": 0, "context": "Such a cutoff is rather arbitrary (Baaijen et al., 2012), and from our own experience results in long chunks. Taking writing research as a starting point, we postulate that keystrokes contain further fine-grained information that help identify syntactic chunks. We aim at a finer-grained representation, and transform user-based average statistics into automatically derived labels (cf. above). We notice that the literature defines different ways to define a pause. Goodkind and Rosenberg (2015), coming from a stylometry background, use the difference between release time of the previous key and the timepress of the current key to calculate pre-word pause duration.2 In contrast, writing research (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012) defines pauses as the start time of a keystroke until the start time of the next keystroke. We experimented with both types of pause definitions, and found the former slightly more robust, hence we use that to calculate pauses throughout this paper. In order to get a better feel of word pause durations, we examine various properties of them. First, do we need to normalize pauses for word length? Goodkind and Rosenberg (2015) found a linear relationship between pre-word pauses and word length in their dataset.", "startOffset": 35, "endOffset": 1192}, {"referenceID": 38, "context": "Second, keystroke logs are presumably idiosyncratic, can we still use it? In fact, user keystroke biometrics are successfully used for author stylometry and verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014).", "startOffset": 200, "endOffset": 266}, {"referenceID": 28, "context": "Second, keystroke logs are presumably idiosyncratic, can we still use it? In fact, user keystroke biometrics are successfully used for author stylometry and verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014).", "startOffset": 200, "endOffset": 266}, {"referenceID": 26, "context": "Second, keystroke logs are presumably idiosyncratic, can we still use it? In fact, user keystroke biometrics are successfully used for author stylometry and verification in computer security research (Stewart et al., 2011; Monaco et al., 2013; Locklear et al., 2014).", "startOffset": 200, "endOffset": 266}, {"referenceID": 19, "context": "However, also eye tracking data like scanpaths (the resulting series of fixations and saccades in eye tracking) are known to be idiosyncratic (Kanan et al., 2015).", "startOffset": 142, "endOffset": 162}, {"referenceID": 21, "context": "Nevertheless it has been shown that gaze patterns help to inform NLP (Barrett and S\u00f8gaard, 2015; Klerke et al., 2016).", "startOffset": 69, "endOffset": 117}, {"referenceID": 9, "context": "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).", "startOffset": 68, "endOffset": 245}, {"referenceID": 24, "context": "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).", "startOffset": 68, "endOffset": 245}, {"referenceID": 44, "context": "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).", "startOffset": 68, "endOffset": 245}, {"referenceID": 18, "context": "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).", "startOffset": 68, "endOffset": 245}, {"referenceID": 10, "context": "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).", "startOffset": 68, "endOffset": 245}, {"referenceID": 2, "context": "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).", "startOffset": 68, "endOffset": 245}, {"referenceID": 25, "context": "Bi-LSTM have recently successfully been used for a variety of tasks (Collobert et al., 2011; Ling et al., 2015; Wang et al., 2015; Huang et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015).", "startOffset": 68, "endOffset": 245}, {"referenceID": 2, "context": ", 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015).", "startOffset": 8, "endOffset": 126}, {"referenceID": 2, "context": ", 2015; Ballesteros et al., 2015; Kiperwasser and Goldberg, 2016; Liu et al., 2015). For further details, see Goldberg (2015) and Cho (2015).", "startOffset": 8, "endOffset": 141}, {"referenceID": 2, "context": "Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 143, "endOffset": 211}, {"referenceID": 12, "context": "Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 143, "endOffset": 211}, {"referenceID": 32, "context": "Adding character representations as additional information has been shown to be effective for a number of tasks, including parsing and tagging (Ballesteros et al., 2015; Gillick et al., 2015; Plank et al., 2016).", "startOffset": 143, "endOffset": 211}, {"referenceID": 2, "context": "Character embeddings~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016).", "startOffset": 111, "endOffset": 157}, {"referenceID": 32, "context": "Character embeddings~c are incorporated via a hierarchical bi-LSTM using a sequence bi-LSTM at the lower level (Ballesteros et al., 2015; Plank et al., 2016).", "startOffset": 111, "endOffset": 157}, {"referenceID": 23, "context": "POS annotations were obtained by looking up the possible tag of a token in English wiktionary (Li et al., 2012).", "startOffset": 94, "endOffset": 111}, {"referenceID": 7, "context": "Predicting all tasks at the outermost layer is the most commonly used form of multi-task learning in neural networks (Caruana, 1998; Collobert et al., 2011).", "startOffset": 117, "endOffset": 156}, {"referenceID": 9, "context": "Predicting all tasks at the outermost layer is the most commonly used form of multi-task learning in neural networks (Caruana, 1998; Collobert et al., 2011).", "startOffset": 117, "endOffset": 156}, {"referenceID": 31, "context": "(2011) (250 sentences), converted to chunks (Plank et al., 2014).", "startOffset": 44, "endOffset": 64}, {"referenceID": 30, "context": "For chunking we use Twitter data from Ritter (2011) (all, 2364 tweets) and Foster et al.", "startOffset": 38, "endOffset": 52}, {"referenceID": 11, "context": "For chunking we use Twitter data from Ritter (2011) (all, 2364 tweets) and Foster et al. (2011) (250 sentences), converted to chunks (Plank et al.", "startOffset": 75, "endOffset": 96}, {"referenceID": 38, "context": "The keystroke logging data stems from students taking an actual test on spreadsheet modeling in a university course (Stewart et al., 2011; Monaco et al., 2013).", "startOffset": 116, "endOffset": 159}, {"referenceID": 28, "context": "The keystroke logging data stems from students taking an actual test on spreadsheet modeling in a university course (Stewart et al., 2011; Monaco et al., 2013).", "startOffset": 116, "endOffset": 159}, {"referenceID": 29, "context": "Statistical significance is computed using the approximate randomization test (Noreen, 1989) using i = 1000 iterations and p-values are reported (S\u00f8gaard et al.", "startOffset": 78, "endOffset": 92}, {"referenceID": 36, "context": "Statistical significance is computed using the approximate randomization test (Noreen, 1989) using i = 1000 iterations and p-values are reported (S\u00f8gaard et al., 2014).", "startOffset": 145, "endOffset": 167}, {"referenceID": 18, "context": "64 (Huang et al., 2015), however, additionally uses https://github.", "startOffset": 3, "endOffset": 23}, {"referenceID": 38, "context": "Disregarding users due to issues with logging (Stewart et al., 2011).", "startOffset": 46, "endOffset": 68}, {"referenceID": 46, "context": "88 Xu et al. (2015) 93.", "startOffset": 3, "endOffset": 20}, {"referenceID": 43, "context": "(Vaswani et al., 2016), however, in this exploratory paper we are interested in examining whether we find signal in keystroke data, and are not interested in beating the latest state-of-the-art.", "startOffset": 0, "endOffset": 22}, {"referenceID": 45, "context": "41, compared to the more complex model by Xu et al. (2015) achieving an accuracy of 93.", "startOffset": 42, "endOffset": 59}, {"referenceID": 27, "context": "In fact, similar effects have been shown in a multitask machine translation and parsing setup (Luong et al., 2016), where mixing coefficients were used to downplay the importance of the auxiliary parsing data that otherwise swamped the main task data.", "startOffset": 94, "endOffset": 114}, {"referenceID": 0, "context": "6 Related Work Keystroke logging has developed into a promising tool for research into writing (Wengelin, 2006; Van Waes et al., 2009; Baaijen et al., 2012), as time measurements can give insights into cognitive processes involved in writing (Nottbusch et al.", "startOffset": 95, "endOffset": 156}, {"referenceID": 30, "context": ", 2012), as time measurements can give insights into cognitive processes involved in writing (Nottbusch et al., 2007) or translation studies.", "startOffset": 93, "endOffset": 117}, {"referenceID": 0, "context": ", 2009; Baaijen et al., 2012), as time measurements can give insights into cognitive processes involved in writing (Nottbusch et al., 2007) or translation studies. In fact, most prior work that uses keystroke logs focuses on experimental research. For example, Hanoulle et al. (2015) study whether a bilingual glossary reduces the working time of professional translators.", "startOffset": 8, "endOffset": 284}, {"referenceID": 6, "context": "Other translation research has combined eye-tracking data with keystroke logs to study the translation process (Carl et al., 2016).", "startOffset": 111, "endOffset": 130}, {"referenceID": 21, "context": "A recent related line of work explores eye tracking data to inform sentence compression (Klerke et al., 2016) and induce part-of-speech (Barrett and S\u00f8gaard, 2015).", "startOffset": 88, "endOffset": 109}, {"referenceID": 45, "context": "Similarly, there are recent studies that predict fMRI activation from reading (Wehbe et al., 2014) or use fMRI data for POS induction (Bingel et al.", "startOffset": 78, "endOffset": 98}, {"referenceID": 5, "context": ", 2014) or use fMRI data for POS induction (Bingel et al., 2016).", "startOffset": 43, "endOffset": 64}, {"referenceID": 5, "context": "Other translation research has combined eye-tracking data with keystroke logs to study the translation process (Carl et al., 2016). An analysis of users\u2019 typing behavior was studied by Baba and Suzuki (2012). They collect keystroke logs of online users describing images to measure spelling difficulty.", "startOffset": 112, "endOffset": 208}, {"referenceID": 5, "context": "Other translation research has combined eye-tracking data with keystroke logs to study the translation process (Carl et al., 2016). An analysis of users\u2019 typing behavior was studied by Baba and Suzuki (2012). They collect keystroke logs of online users describing images to measure spelling difficulty. They analyzed corrected and uncorrected spelling mistakes in Japanese and English and found that spelling errors related to phonetic problems remain mostly unnoticed. Goodkind and Rosenberg (2015) is the only study prior to us that use keystroke loggings in NLP.", "startOffset": 112, "endOffset": 500}], "year": 2016, "abstractText": "Keystroke dynamics have been extensively used in psycholinguistic and writing research to gain insights into cognitive processing. But do keystroke logs contain actual signal that can be used to learn better natural language processing models? We postulate that keystroke dynamics contain information about syntactic structure that can inform shallow syntactic parsing. To test this hypothesis, we explore labels derived from keystroke logs as auxiliary task in a multi-task bidirectional Long Short-Term Memory (bi-LSTM). Our results show promising results on two shallow syntactic parsing tasks, chunking and CCG supertagging. Our model is simple, has the advantage that data can come from distinct sources, and produces models that are significantly better than models trained on the text annotations alone.", "creator": "LaTeX with hyperref package"}}}