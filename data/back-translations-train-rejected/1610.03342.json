{"id": "1610.03342", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "abstract": "We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning.", "histories": [["v1", "Tue, 11 Oct 2016 14:00:28 GMT  (252kb,D)", "http://arxiv.org/abs/1610.03342v1", "Accepted at COLING 2016"]], "COMMENTS": "Accepted at COLING 2016", "reviews": [], "SUBJECTS": "cs.CL cs.LG", "authors": ["lieke gelderloos", "grzegorz chrupa{\\l}a"], "accepted": false, "id": "1610.03342"}, "pdf": {"name": "1610.03342.pdf", "metadata": {"source": "CRF", "title": "From phonemes to images: levels of representation in a recurrent neural model of visually-grounded language learning", "authors": ["Lieke Gelderloos", "Grzegorz Chrupa\u0142a"], "emails": ["liekegelderloos@gmail.com", "g.chrupala@uvt.nl"], "sections": [{"heading": null, "text": "We present a model of visually based language learning processes based on stacked recursive neural networks that learn to predict visual characteristics from an image description in the form of a sequence of sounds. The learning task is similar to that of learners of human language who have to discover structure and meaning from loud and ambiguous data across different modalities. We show that our model actually learns to predict characteristics of the visual context from phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of layers: lower layers in the stack are comparatively more form sensitive, while higher layers are more sensitive to meaning."}, {"heading": "1 Introduction", "text": "In fact, most people who stand up for people's rights are standing up for people's rights, violating the rights and duties of others, \"he said.\" I don't think they feel able to respect people's rights. \"He added,\" I don't think people are able to respect people's rights. \"He added,\" I don't think people are able to respect people's rights. \"He added,\" I don't think people are able to respect people's rights that I don't respect. \"He added,\" I don't think people are able to respect people's rights. \"He added,\" I don't think people are able to respect people's rights that I don't respect. \""}, {"heading": "2 Related work", "text": "In fact, it is the case that most of them are able to survive themselves by embarking on the search for new paths that they are following. (...) Most of them are able to embark on the search for new paths. (...) Most of them are able to embark on the search for new paths. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves. (...) Most of them are able to identify themselves."}, {"heading": "3 Models", "text": "Take a student who sees a person pointing at a scene and pronounces the unfamiliar phrase, \"Look, the monkeys are playing.\" We can assume that the student will update her speech comprehension model in such a way that the subsequent utterance of this sentence evokes in her mind something similar to the impression of this visual scene. Our model is a special instance of this simple idea."}, {"heading": "3.1 Phon GRU", "text": "The architecture of our main model of interest, PHON GRU, is schematically shown in Figure 1 and consists of a phoneme coding layer followed by a stack of K gated recurrent neural networks, followed by a tightly connected layer that maps the last hidden state of the top recurrent layer to a vector of visual features (gated recurrent units (gated recurrent units) were introduced in Cho et al (2014) and Chung et al. (2014) as an attempt to mitigate the problem of disappearing gradients in standard simple recurrent networks as known since the work of Elman (1990)."}, {"heading": "3.2 Word GRU", "text": "The architecture of this model is the same as PHON GRU, except that we use words instead of phonemes as input symbols, use learnable word embeddings instead of fixed phoneme encodings, and reduce the number of layers in the GRU stack. See section 4 for details."}, {"heading": "3.3 Word Sum", "text": "The second model we use for comparison is a word-based non-sequential model, consisting of a word embedding matrix, a vector sum operator, and an assignment to the image attribute vector: i \u0441 = I n \u2211 t = 1E [:, wt] (10), where wt is the word at position t in the input expression. This model simply learns word embedding, which is then grouped into a single vector and projected onto the target image vector. Therefore, this model has no access to word sequence information and is a distributed analogue of a bag-of-words model."}, {"heading": "4 Experiments", "text": "For all experiments, the models were trained on the training set of MS-COCO. MS-COCO contains over 163,000 images accompanied by at least five captions of human commentators. At an average of 7.7 labeled object instances per image, the images typically contain more objects than the captions mention, with the reference to the scene being ambiguous. Textual inputs for the PHON-GRU models were automatically entered using the graph-to-phoneme functionality with the default English voice of the eSpeak speech synthesis toolkit.1 Stress and pause markers were removed, as well as word boundaries (after storing their position for use in experiments), so that only phoneme symbols are used. See Figure 2 for a sample transcription. Visual input for all models was done by forwarding images through the 16-layer Convolutionary Neural Networks described in Simonyan and Zisserman (2014)."}, {"heading": "4.1 Prediction of visual features", "text": "The models are trained and evaluated on predicting visual characteristics vectors from captions. While our goal is not to develop an image retrieval method, we use this task as it reflects the ability to extract visually highlighted semantic information from the language.Figure 3 shows the value of validating average cosmic distance between the predicted visual vector and the target vector for three random initializations of each of the model types.For validation and test data, we used a random sample of 5000 images, each from the MS-COCO validation set.Figure 3 shows the value of validating average cosmic distance between the predicted visual vector and the target vector for three random initializations of each model types.The phonetic GRU model is more sensitive to initialization: one can clearly distinguish three separate trajectories. The word-level models are much less randomly affected by the overall initialization of the model based on the GRD."}, {"heading": "4.2 Word boundary prediction", "text": "To investigate the sensitivity of the PHON-GRU model to the linguistic structure at the level of subwords 71, we investigated the coding of information about word boundaries in the hidden layers. Logistic regression models were trained on activation patterns of the hidden layers in all time windows, with the aim of identifying phonemes that preceded a word boundary. For comparison, we also trained logistic regression models on n-gram data to perform the same tasks, with positional phonemes n-grams in the range 1-n. The location of the word boundaries was taken from eSpeak transcriptions, which largely correspond to the location of word boundaries according to conventional English spelling. eSpeak models perform some coarticulation effects, which sometimes lead to word boundaries that disappear from transcription. For example, bank of a flow is used as transcription [baNk @ ning @ Iv @.Learning @] All models were implemented with logistics regression learning."}, {"heading": "4.3 Word similarity", "text": "To understand this, we must follow the rules we have set ourselves."}, {"heading": "4.4 Position of shared substrings", "text": "Here we quantify the time scale on which information is stored in the various layers of PHON GRU. We investigated the position of phonemstrings divided by sentences and their closest neighbors in the 5,000-image validation sample. We determined the closest neighbor of each sentence for each hidden layer in PHON GRU. The closest neighbor is the sentence for which the activation vector at the end of the sentence symbol has the smallest cosmic distance to the activation vector of the original sentence. The MEN dataset can be found at http: / / clic.cimec.unitn.it / \u02dc elia.bruni / MEN 4Note that in the MEN dataset meaning and word form are also (weakly) correlated: human similarity judgments and edit distance correlate at \u2212 0.08 (p < 1e \u2212 5). The position of matching substrings is the average position of symbols in substrings shared by neighboring sentences."}, {"heading": "5 Future work", "text": "Although our analyses show a clear pattern of short-term information in the lower layers and greater dependencies in the higher layers, the third layer still encodes information about the phonetic form: its activation patterns were predictions of word boundaries, and similarities between word pairs at this level correlated more strongly with the processing distance than human similarity judgments. It would be interesting to examine exactly what information this is and to what extent it corresponds to the representation of language in the mind of human speakers. In humans, both the phonological word form and the word meaning can function as primes, somewhat reminiscent of the behavior of our model. Finally, we would like to take the next step toward a grounded learning of language from raw perceptual input, applying models similar to the one described here, to acoustic speech signals coupled with visual input. We expect this to be a challenging but essential endeavor."}], "references": [{"title": "Nicolas Bouchard", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian J. Goodfellow", "Arnaud Bergeron"], "venue": "and Yoshua Bengio.", "citeRegEx": "Bastien et al.2012", "shortCiteRegEx": null, "year": 2012}, {"title": "Adrian Muscat", "author": ["Raffaella Bernardi", "Ruket Cakici", "Desmond Elliott", "Aykut Erdem", "Erkut Erdem", "Nazli IkizlerCinbis", "Frank Keller"], "venue": "and Barbara Plank.", "citeRegEx": "Bernardi et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Nam-Khanh Tran", "author": ["Elia Bruni"], "venue": "and Marco Baroni.", "citeRegEx": "Bruni et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Dzmitry Bahdanau", "author": ["Kyunghyun Cho", "Bart van Merri\u00ebnboer"], "venue": "and Yoshua Bengio.", "citeRegEx": "Cho et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Akos K\u00e1d\u00e1r", "author": ["Grzegorz Chrupa\u0142a"], "venue": "and Afra Alishahi.", "citeRegEx": "Chrupa\u0142a et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "KyungHyun Cho", "author": ["Junyoung Chung", "Caglar Gulcehre"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Kyunghyun Cho", "author": ["Junyoung Chung"], "venue": "and Yoshua Bengio.", "citeRegEx": "Chung et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Estela C\u00e0mara", "author": ["Toni Cunillera", "Matti Laine"], "venue": "and Antoni Rodr\u0131\u0301guez-Fornells.", "citeRegEx": "Cunillera et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "Finding structure in time", "author": ["Jeffrey L Elman"], "venue": "Cognitive science,", "citeRegEx": "Elman.,? \\Q1990\\E", "shortCiteRegEx": "Elman.", "year": 1990}, {"title": "Afra Alishahi", "author": ["Afsaneh Fazly"], "venue": "and Suzanne Stevenson.", "citeRegEx": "Fazly et al.2010", "shortCiteRegEx": null, "year": 2010}, {"title": "and Joshua B", "author": ["Michael C. Frank", "Noah D. Goodman"], "venue": "Tenenbaum.", "citeRegEx": "Frank et al.2007", "shortCiteRegEx": null, "year": 2007}, {"title": "The role of cross-modal associations in statistical learning", "author": ["Glicksohn", "Cohen2013] Arit Glicksohn", "Asher Cohen"], "venue": "Psychonomic Bulletin & Review,", "citeRegEx": "Glicksohn et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Glicksohn et al\\.", "year": 2013}, {"title": "Deep multimodal semantic embeddings for speech and images. arXiv:1511.03690", "author": ["Harwath", "Glass2015] David Harwath", "James Glass"], "venue": null, "citeRegEx": "Harwath et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Harwath et al\\.", "year": 2015}, {"title": "Shaoqing Ren", "author": ["Kaiming He", "Xiangyu Zhang"], "venue": "and Jian Sun.", "citeRegEx": "He et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Training and analysing deep recurrent neural networks", "author": ["Hermans", "Schrauwen2013] Michiel Hermans", "Benjamin Schrauwen"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "Hermans et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Hermans et al\\.", "year": 2013}, {"title": "Language acquisition through a human\u2013robot interface by combining speech, visual, and behavioral information", "author": ["Naoto Iwahashi"], "venue": "Information Sciences,", "citeRegEx": "Iwahashi.,? \\Q2003\\E", "shortCiteRegEx": "Iwahashi.", "year": 2003}, {"title": "Grzegorz Chrupa\u0142a", "author": ["\u00c1kos K\u00e1d\u00e1r"], "venue": "and Afra Alishahi.", "citeRegEx": "K\u00e1d\u00e1r et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Justin Johnson", "author": ["Andrej Karpathy"], "venue": "and Fei-Fei Li.", "citeRegEx": "Karpathy et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Adam: A method for stochastic optimization. CoRR, abs/1412.6980", "author": ["Kingma", "Ba2014] Diederik P. Kingma", "Jimmy Ba"], "venue": null, "citeRegEx": "Kingma et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Kingma et al\\.", "year": 2014}, {"title": "Antonio Torralba", "author": ["Ryan Kiros", "Yukun Zhu", "Ruslan R Salakhutdinov", "Richard Zemel", "Raquel Urtasun"], "venue": "and Sanja Fidler.", "citeRegEx": "Kiros et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Peter W Foltz", "author": ["Thomas K Landauer"], "venue": "and Darrell Laham.", "citeRegEx": "Landauer et al.1998", "shortCiteRegEx": null, "year": 1998}, {"title": "Nghia The Pham", "author": ["Angeliki Lazaridou"], "venue": "and Marco Baroni.", "citeRegEx": "Lazaridou et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Raquel Fern\u00e1ndez", "author": ["Angeliki Lazaridou", "Grzegorz Chrupa\u0142a"], "venue": "and Marco Baroni.", "citeRegEx": "Lazaridou et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Piotr Doll\u00e1r", "author": ["Tsung-Yi Lin", "Michael Maire", "Serge Belongie", "James Hays", "Pietro Perona", "Deva Ramanan"], "venue": "and C Lawrence Zitnick.", "citeRegEx": "Lin et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Tiago Lu\u0131\u0301s", "author": ["Wang Ling"], "venue": "Lu\u0131\u0301s Marujo, Ram\u00f3n Fernandez Astudillo, Silvio Amir, Chris Dyer, Alan W Black, and Isabel Trancoso.", "citeRegEx": "Ling et al.2015", "shortCiteRegEx": null, "year": 2015}, {"title": "Nal Kalchbrenner", "author": ["Aaron van den Oord"], "venue": "and Koray Kavukcuoglu.", "citeRegEx": "Oord et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Matthieu Perrot", "author": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg", "Jake Vanderplas", "Alexandre Passos", "David Cournapeau", "Matthieu Brucher"], "venue": "and \u00c9douard Duchesnay.", "citeRegEx": "Pedregosa et al.2011", "shortCiteRegEx": null, "year": 2011}, {"title": "2016", "author": ["Barbara Plank", "Anders S\u00f8gaard", "Yoav Goldberg"], "venue": "Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss. In Proceedings of ACL", "citeRegEx": "Plank et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Learning words from sights and sounds: a computational model", "author": ["Roy", "Pentland2002] Deb K Roy", "Alex P Pentland"], "venue": "Cognitive Science,", "citeRegEx": "Roy et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Roy et al\\.", "year": 2002}, {"title": "Berg", "author": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C"], "venue": "and Li Fei-Fei.", "citeRegEx": "Russakovsky et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556", "author": ["Simonyan", "Zisserman2014] Karen Simonyan", "Andrew Zisserman"], "venue": null, "citeRegEx": "Simonyan et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Simonyan et al\\.", "year": 2014}, {"title": "A computational study of cross-situational techniques for learning wordto-meaning mappings. Cognition, 61(1-2):39\u201391", "author": ["Jeffrey M. Siskind"], "venue": null, "citeRegEx": "Siskind.,? \\Q1996\\E", "shortCiteRegEx": "Siskind.", "year": 1996}, {"title": "Maarten Versteegh", "author": ["Gabriel Synnaeve"], "venue": "and Emmanuel Dupoux.", "citeRegEx": "Synnaeve et al.2014", "shortCiteRegEx": null, "year": 2014}, {"title": "Effects of visual information on adults\u2019 and infants\u2019 auditory statistical learning", "author": ["Erik D Thiessen"], "venue": "Cognitive Science,", "citeRegEx": "Thiessen.,? \\Q2010\\E", "shortCiteRegEx": "Thiessen.", "year": 2010}, {"title": "Sanja Fidler", "author": ["Ivan Vendrov", "Ryan Kiros"], "venue": "and Raquel Urtasun.", "citeRegEx": "Vendrov et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Dan Jurafsky", "author": ["Ziang Xie", "Anand Avati", "Naveen Arivazhagan"], "venue": "and Andrew Y Ng.", "citeRegEx": "Xie et al.2016", "shortCiteRegEx": null, "year": 2016}, {"title": "Chen Yu", "author": ["Daniel Yurovsky"], "venue": "and Linda B Smith.", "citeRegEx": "Yurovsky et al.2012", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [], "year": 2016, "abstractText": "We present a model of visually-grounded language learning based on stacked gated recurrent neural networks which learns to predict visual features given an image description in the form of a sequence of phonemes. The learning task resembles that faced by human language learners who need to discover both structure and meaning from noisy and ambiguous data across modalities. We show that our model indeed learns to predict features of the visual context given phonetically transcribed image descriptions, and show that it represents linguistic information in a hierarchy of levels: lower layers in the stack are comparatively more sensitive to form, whereas higher layers are more sensitive to meaning.", "creator": "LaTeX with hyperref package"}}}