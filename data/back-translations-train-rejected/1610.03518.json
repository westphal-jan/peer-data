{"id": "1610.03518", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "11-Oct-2016", "title": "Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model", "abstract": "Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesn't work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation.", "histories": [["v1", "Tue, 11 Oct 2016 20:24:31 GMT  (1151kb,D)", "http://arxiv.org/abs/1610.03518v1", null]], "reviews": [], "SUBJECTS": "cs.RO cs.AI cs.LG cs.SY", "authors": ["paul christiano", "zain shah", "igor mordatch", "jonas schneider", "trevor blackwell", "joshua tobin", "pieter abbeel", "wojciech zaremba"], "accepted": false, "id": "1610.03518"}, "pdf": {"name": "1610.03518.pdf", "metadata": {"source": "CRF", "title": "Transfer from Simulation to Real World through Learning Deep Inverse Dynamics Model", "authors": ["Paul Christiano", "Zain Shah", "Igor Mordatch", "Jonas Schneider", "Trevor Blackwell", "Joshua Tobin", "Pieter Abbeel", "Wojciech Zaremba"], "emails": [], "sections": [{"heading": null, "text": "In fact, it is such that most of us are able to move into another world, in which they are able, in which they are able, in which they are able, in which they are able, in which they are able, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they live, in which they"}, {"heading": "II. RELATED WORK", "text": "This year, it is only a matter of time before an agreement is reached."}, {"heading": "A. Setting", "text": "We study the transfer from a source environment to a target environment. Typically, the source environment would be a simulator, and the target environment would be a physical robot. However, to validate our method, we start with a simulator in both the source and the target domain. This setup has merit in developing an experimental understanding of our approach, since we can control the degree of deviation between the source and target environments. Our final experiments are in transferring from a simulator to the physical robot.For each environment, we designate the state space with S, the action space with A, and the observation space with O. Points s, a, o, o, O, are states, actions, and observations. The state is not assumed to be slightly overloaded, the agent makes loud and incomplete observations of the underlying system, o = o (s). O, which typically do not expose some latent factors (e.g. fluctuating temperature or counter-reaction)."}, {"heading": "B. Transfer to the target environment", "text": "Instead of executing the dynamics directly in the target environment, we try to transfer the high-level characteristics of the dynamics to the target environment in order to reuse them in the target environment, but not their specific characteristics at a lower level. Our approach is illustrated in Figure 1. Throughout the execution, we repeat the following at all times: Consider the recent history of the observations. \u2212 k: Calculate the action that our source policy dictates for the source environment. Let us simulate what observation o: next = o (Tsource - k: asource)) would be achieved at the next step in the source environment, and then calculate Atarget = \u03c6:, o: next). It is a learned inverse dynamic model for the target environment that takes place in the recent history of actions and observations, as well as the desired next observation, and generates the action in the target domain that is as close as possible to the desired observation o."}, {"heading": "C. Training of the inverse dynamics model", "text": "We propose to collect trajectories in the physical environment and train a neural network that represents the inverse dynamic model, i.e., that can (approximately) predict the action that will lead to the next observation. For a section of a trajectory: \u03c4H: H + K and the next observation oH + k + 1, we train a neural network \u03c6 to perform the previous action aH + k: \u03c6: (oH, aH, oH + 1,..., ak + H \u2212 1, ok + H, ok + H + 1) 7 \u2192 ak + HWe incorporate the story into our model and select the time window parameter H so large that we can (implicitly) derive all the important latent factors or time dependencies present in the dynamics."}, {"heading": "D. Data collection / Exploration", "text": "At each point during the training, we have a preliminary inverse dynamic model \u03c6 that we can use to implement a preliminary strategic \u03c0target. To collect training data for our model, we execute this preliminary strategic goal. We add noise to the prescribed measures for exploration, i.e. to ensure that we have sufficiently diverse training data. Too much noise leads to data being collected too far from the target paths, too little noise leads to insufficient exploration, and the inverse dynamic model will improve very slowly. In our experiments, we describe our noise settings. We found it helpful not to add noise at every time step. Too often, adding noise distracts the data collection too far from the relevant parts of the space for the task. In the simulation, we can collect training samples very efficiently by adjusting the simulator to the states that occur along a path; in a neural system, the efficiency of the training data depends on the efficiency of the training system."}, {"heading": "IV. EXPERIMENTS", "text": "The purpose of our method is to adapt a policy from a source environment to a target environment, with the most important application being the adaptation from the simulation to the real world. First, we measure the adaptability between two simulators IV-A, as this allows us to quantify the differences between the source environment and the target environment most directly. Afterwards, we present results for the adaptation from a simulation to a physical environment."}, {"heading": "A. Simulated Environments \u2013 Sim1 to Sim2 transfer", "text": "We test our method on several simulated models in the robot simulation MuJoCo [38] with OpenAI Gym environment [3]. Therefore, both source and target environments are in the simulation. We conduct experiments in the following standard OpenAI Gym environments (Figure 2). In any case, the observation space consists of positions and speeds of all degrees of freedom. The two-dimensional models of a robot with a single \"foot\" moving through hopping, with a 12-dimensional observation room and 3 actuators. Two-dimensional model of a two-dimensional robot with a two-dimensional observation room. \u2022 Humanoid. Three-dimensional model of a humanoid observation room with a 376-dimensional observation room and 17-dimensional observation room. We have a two-dimensional model of a two-dimensional robot with a 17-dimensional observation room and 6 actuators."}, {"heading": "B. Physical interaction \u2013 Sim to Real transfer", "text": "We test our method of transferring trajectories from a simulated source domain to the target domain, the physical fetch robot [42]. We control the robot using position control and bearing firmware based on ROS in 10 Hz frequency. The tasks take into account the control of the arm and our metric measurements normalize the distance between observations made in the simulator through the trajectory and observations made on the physical robot. The task is an agile oscillation of an arm, where the center of the arm is pulled by a Bungee cord. Our action adaptation method is able to adapt to this state by adjusting and exerting the necessary torque. As a baseline, we use PD controllers where the targets in the simulator are experienced states. Table 5 summarizes our results."}, {"heading": "V. DISCUSSION AND FUTURE WORK", "text": "We have presented a general method for adapting strategies developed in one area, such as simulation, to another area, such as the physical world, by learning a deep inverse dynamic model that is trained on the behaviour of the physical robot. Our method is successfully able to adapt complex control policies for aggressive reach and locomotion to scenarios that include touches, hysteresis effects in the form of time-correlated noise and significant differences between environments. However, in order to create robots that truly generalise in the physical world, in addition to adapting to actions, it is also necessary to adapt states and observations between simulation and the physical world. We currently assume that the observations generated by our simulator closely correspond to physical observations, which is reasonable when we view sensors as common positions, but is it not reasonable to expect that simulated visual or depth sensors correspond to the high accuracy of the real world."}], "references": [{"title": "Transfer learning for reinforcement learning on a physical robot", "author": ["Samuel Barrett", "Matt E. Taylor", "Peter Stone"], "venue": "In Ninth International Conference on Autonomous Agents and Multiagent Systems - Adaptive Learning Agents Workshop (ALA),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2010}, {"title": "Approximate real-time optimal control based on sparse gaussian process models", "author": ["Joschka Boedecker", "Jost Tobias Springenberg", "Jan Wlfing", "Martin Riedmiller"], "venue": "In Adaptive Dynamic Programming and Reinforcement Learning (ADPRL),", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning inverse dynamics models with contacts", "author": ["R. Calandra", "S. Ivaldi", "M. Deisenroth", "E. R\u00fcckert", "J. Peters"], "venue": "In IEEE International Conference on Robotics and Automation,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2015}, {"title": "Real-world reinforcement learning via multifidelity simulators", "author": ["Mark Cutler", "Thomas J Walsh", "Jonathan P How"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "System identification and modelling of a high performance hydraulic actuator. In Eds.), Lecture Notes in Control and Information Sciences", "author": ["Benoit Boulet Laeeque Daneshmend"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 1992}, {"title": "Pilco: A modelbased and data-efficient approach to policy search", "author": ["Marc Peter Deisenroth", "Carl Edward Rasmussen"], "venue": "Proceedings of the International Conference on Machine Learning,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2011}, {"title": "Decaf: A deep convolutional activation feature for generic visual recognition", "author": ["Jeff Donahue", "Yangqing Jia", "Oriol Vinyals", "Judy Hoffman", "Ning Zhang", "Eric Tzeng", "Trevor Darrell"], "venue": "In ICML,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2014}, {"title": "Rigid Body Dynamics Algorithms", "author": ["Roy Featherstone"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2007}, {"title": "Guided cost learning: Deep inverse optimal control via policy optimization", "author": ["Chelsea Finn", "Sergey Levine", "Pieter Abbeel"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2016}, {"title": "One-shot learning of manipulation skills with online dynamics adaptation and neural network", "author": ["Justin Fu", "Sergey Levine", "Pieter Abbeel"], "venue": "priors. CoRR,", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2015}, {"title": "Literature survey of contact dynamics modelling", "author": ["G. Gilardi", "I. Sharf"], "venue": "Mechanism and Machine Theory,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2002}, {"title": "Learning dexterous manipulation for a soft robotic hand from human", "author": ["Abhishek Gupta", "Clemens Eppner", "Sergey Levine", "Pieter Abbeel"], "venue": "demonstration. CoRR,", "citeRegEx": "13", "shortCiteRegEx": "13", "year": 2016}, {"title": "ABAQUS/CAE User\u2019s Manual", "author": ["Karlsson Hibbitt", "Sorensen"], "venue": "Hibbitt, Karlsson & Sorensen, Incorporated,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2002}, {"title": "Generative adversarial imitation learning", "author": ["Jonathan Ho", "Stefano Ermon"], "venue": "arXiv preprint arXiv:1606.03476,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2016}, {"title": "Efficient learning of domain-invariant image representations", "author": ["Judy Hoffman", "Erik Rodner", "Jeff Donahue", "Trevor Darrell", "Kate Saenko"], "venue": "arXiv preprint arXiv:1301.3224,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2013}, {"title": "Nonlinear identification of backlash in robot transmissions", "author": ["GE Hovland", "S Hanssen", "E Gallestey", "S Moberg", "T Brogardh", "S Gunnarsson", "M Isaksson"], "venue": "In Proceedings of the 33rd ISR (International Symposium on Robotics),", "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2002}, {"title": "System identification of unmanned aerial vehicles", "author": ["Thomas Ingebretsen"], "venue": null, "citeRegEx": "18", "shortCiteRegEx": "18", "year": 2012}, {"title": "Gp-bayesfilters: Bayesian filtering using gaussian process prediction and observation models", "author": ["Jonathan Ko", "Dieter Fox"], "venue": "Auton. Robots,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2009}, {"title": "Imagenet classification with deep convolutional neural networks", "author": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E. Hinton"], "venue": "Advances in Neural Information Processing Systems", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2012}, {"title": "Locomotion control for many-muscle humanoids", "author": ["Yoonsang Lee", "Moon Seok Park", "Taesoo Kwon", "Jehee Lee"], "venue": "ACM Trans. Graph.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 2014}, {"title": "Deepmpc: Learning deep latent features for model predictive control", "author": ["Ian Lenz", "Ross Knepper", "Ashutosh Saxena"], "venue": "In RSS,", "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2015}, {"title": "Modeling, System Identification, and Control for Dynamic Locomotion of the LittleDog Robot on Rough Terrain", "author": ["Michael Yurievich Levashov"], "venue": "PhD thesis, Citeseer,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 2012}, {"title": "System Identification: Theory for the User. Prentice Hall information and system sciences series", "author": ["L. Ljung"], "venue": null, "citeRegEx": "24", "shortCiteRegEx": "24", "year": 1999}, {"title": "Towards robust online inverse dynamics learning", "author": ["Franziska Meier", "Daniel Kappler", "Nathan Ratliff", "Stefan Schaal"], "venue": "In Proceedings of the IEEE/RSJ Conference on Intelligent Robots and Systems", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2016}, {"title": "System identification of small-size unmanned helicopter dynamics", "author": ["Bernard Mettler", "Mark B. Tischler", "Takeo Kanade"], "venue": "In Presented at the American Helicopter Society 55th Forum,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 1999}, {"title": "Ensemblecio: Full-body dynamic motion planning that transfers to physical humanoids", "author": ["Igor Mordatch", "Kendall Lowrey", "Emanuel Todorov"], "venue": "In Intelligent Robots and Systems (IROS),", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 2015}, {"title": "Combining model-based policy search with online model learning for control of physical humanoids", "author": ["Igor Mordatch", "Nikhil Mishra", "Clemens Eppner", "Pieter Abbeel"], "venue": "In Proceedings of the IEEE International Conference on Robotics and Automation,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2016}, {"title": "Using model knowledge for learning inverse dynamics", "author": ["D. Nguyen-Tuong", "J. Peters"], "venue": null, "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2010}, {"title": "Actuator control for the nasa-jsc valkyrie humanoid robot: A decoupled dynamics approach for torque control of series elastic robots", "author": ["Nicholas Paine", "Joshua S. Mehling", "James Holley", "Nicolaus A. Radford", "Gwendolyn Johnson", "Chien-Liang Fok", "Luis Sentis"], "venue": "Journal of Field Robotics,", "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2015}, {"title": "Robot motion planning for pouring liquids", "author": ["Zherong Pan", "Chonhyon Park", "Dinesh Manocha"], "venue": "In Proceedings of the Twenty-Sixth International Conference on Automated Planning and Scheduling,", "citeRegEx": "31", "shortCiteRegEx": "31", "year": 2016}, {"title": "Deep learning helicopter dynamics models", "author": ["Ali Punjani", "Pieter Abbeel"], "venue": "IEEE International Conference on Robotics and Automation (ICRA),", "citeRegEx": "32", "shortCiteRegEx": "32", "year": 2015}, {"title": "Trust region policy optimization", "author": ["John Schulman", "Sergey Levine", "Philipp Moritz", "Michael I Jordan", "Pieter Abbeel"], "venue": "CoRR, abs/1502.05477,", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2015}, {"title": "Articulated swimming creatures", "author": ["Jie Tan", "Yuting Gu", "Greg Turk", "C. Karen Liu"], "venue": "In ACM SIGGRAPH 2011 papers,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2011}, {"title": "Soft body locomotion", "author": ["Jie Tan", "Greg Turk", "C. Karen Liu"], "venue": "ACM Trans. Graph.,", "citeRegEx": "35", "shortCiteRegEx": "35", "year": 2012}, {"title": "Modeling and identification of pneumatic actuators", "author": ["Yuval Tassa", "Tingfan Wu", "Javier Movellan", "Emanuel Todorov"], "venue": "IEEE International Conference on Mechatronics and Automation,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Transfer learning for reinforcement learning domains: A survey", "author": ["Matthew E Taylor", "Peter Stone"], "venue": "Journal of Machine Learning Research,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 2009}, {"title": "Mujoco: A physics engine for model-based control", "author": ["Emanuel Todorov", "Tom Erez", "Yuval Tassa"], "venue": "In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2012}, {"title": "A generalized iterative LQG method for locally-optimal feedback control of constrained nonlinear stochastic systems", "author": ["Emmanuel Todorov", "Weiwei Li"], "venue": "In American Control Conference,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2005}, {"title": "Towards adapting deep visuomotor representations from simulated to real environments", "author": ["Eric Tzeng", "Coline Devin", "Judy Hoffman", "Chelsea Finn", "Xingchao Peng", "Sergey Levine", "Kate Saenko", "Trevor Darrell"], "venue": "CoRR, abs/1511.07111,", "citeRegEx": "40", "shortCiteRegEx": "40", "year": 2015}, {"title": "Simultaneous deep transfer across domains and tasks", "author": ["Eric Tzeng", "Judy Hoffman", "Trevor Darrell", "Kate Saenko"], "venue": "CoRR, abs/1510.02192,", "citeRegEx": "41", "shortCiteRegEx": "41", "year": 2015}, {"title": "Model-Less Feedback Control of Continuum Manipulators in Constrained Environments", "author": ["Michael C. Yip", "David B. Camarillo"], "venue": "IEEE Transactions on Robotics,", "citeRegEx": "43", "shortCiteRegEx": "43", "year": 2014}, {"title": "Essentials of robust control, volume 104", "author": ["Kemin Zhou", "John Comstock Doyle"], "venue": "Prentice hall Upper Saddle River, NJ,", "citeRegEx": "44", "shortCiteRegEx": "44", "year": 1998}], "referenceMentions": [{"referenceID": 36, "context": "Hopper, Cheetah, Humanoid from MuJoCo / OpenAI Gym [38] [3].", "startOffset": 51, "endOffset": 55}, {"referenceID": 22, "context": "We compare our approach with output error control [24] and Gaussian Dynamics Adaptation [11], two established approaches to handle mismatch between simulation and real world.", "startOffset": 50, "endOffset": 54}, {"referenceID": 9, "context": "We compare our approach with output error control [24] and Gaussian Dynamics Adaptation [11], two established approaches to handle mismatch between simulation and real world.", "startOffset": 88, "endOffset": 92}, {"referenceID": 7, "context": "rigid multibody dynamics are especially suited for simulating articulated robots [9].", "startOffset": 81, "endOffset": 84}, {"referenceID": 33, "context": "Flexible or inflatable bodies [35] [13], area contact [12], interaction with fluids [34] [31] are just a few of such examples.", "startOffset": 30, "endOffset": 34}, {"referenceID": 11, "context": "Flexible or inflatable bodies [35] [13], area contact [12], interaction with fluids [34] [31] are just a few of such examples.", "startOffset": 35, "endOffset": 39}, {"referenceID": 10, "context": "Flexible or inflatable bodies [35] [13], area contact [12], interaction with fluids [34] [31] are just a few of such examples.", "startOffset": 54, "endOffset": 58}, {"referenceID": 32, "context": "Flexible or inflatable bodies [35] [13], area contact [12], interaction with fluids [34] [31] are just a few of such examples.", "startOffset": 84, "endOffset": 88}, {"referenceID": 29, "context": "Flexible or inflatable bodies [35] [13], area contact [12], interaction with fluids [34] [31] are just a few of such examples.", "startOffset": 89, "endOffset": 93}, {"referenceID": 12, "context": "as those based on Finite Element Method [14] can be used to more closely match such real world effects, but they can be extremely computationally intensive (requiring days to compute seconds of simulation) and furthermore can be numerically ill-conditioned, which makes them difficult to use within numerical trajectory or policy optimization methods.", "startOffset": 40, "endOffset": 44}, {"referenceID": 36, "context": "Our method allows the use of simple, high-performance, and numerically smooth rigid body simulators (we use MuJoCo [38]) for policy or trajectory optimization, while still being able to adapt to complex effects present in the real world.", "startOffset": 115, "endOffset": 119}, {"referenceID": 21, "context": "focused on identifying these parameters from observations of robots\u2019 behavior in the real world, but tend to require separate specialized identification approaches and models for different robot platforms, such as legged robots [23], helicopters [26], or fixed-wing UAVs [18].", "startOffset": 228, "endOffset": 232}, {"referenceID": 24, "context": "focused on identifying these parameters from observations of robots\u2019 behavior in the real world, but tend to require separate specialized identification approaches and models for different robot platforms, such as legged robots [23], helicopters [26], or fixed-wing UAVs [18].", "startOffset": 246, "endOffset": 250}, {"referenceID": 16, "context": "focused on identifying these parameters from observations of robots\u2019 behavior in the real world, but tend to require separate specialized identification approaches and models for different robot platforms, such as legged robots [23], helicopters [26], or fixed-wing UAVs [18].", "startOffset": 271, "endOffset": 275}, {"referenceID": 15, "context": "Furthermore, individual physical effects also require specialized expert-designed models and parameter identification methods, such as motor backlash [17], hydraulic actuation [6], series elastic actuation [30], or pneumatic actuation [36].", "startOffset": 150, "endOffset": 154}, {"referenceID": 4, "context": "Furthermore, individual physical effects also require specialized expert-designed models and parameter identification methods, such as motor backlash [17], hydraulic actuation [6], series elastic actuation [30], or pneumatic actuation [36].", "startOffset": 176, "endOffset": 179}, {"referenceID": 28, "context": "Furthermore, individual physical effects also require specialized expert-designed models and parameter identification methods, such as motor backlash [17], hydraulic actuation [6], series elastic actuation [30], or pneumatic actuation [36].", "startOffset": 206, "endOffset": 210}, {"referenceID": 34, "context": "Furthermore, individual physical effects also require specialized expert-designed models and parameter identification methods, such as motor backlash [17], hydraulic actuation [6], series elastic actuation [30], or pneumatic actuation [36].", "startOffset": 235, "endOffset": 239}, {"referenceID": 22, "context": "A number of approaches learn forward dynamics models functions mapping current state and action to a next state [24] [32].", "startOffset": 112, "endOffset": 116}, {"referenceID": 30, "context": "A number of approaches learn forward dynamics models functions mapping current state and action to a next state [24] [32].", "startOffset": 117, "endOffset": 121}, {"referenceID": 27, "context": "action that achieves the transition between the two [29], [4], [25].", "startOffset": 52, "endOffset": 56}, {"referenceID": 2, "context": "action that achieves the transition between the two [29], [4], [25].", "startOffset": 58, "endOffset": 61}, {"referenceID": 23, "context": "action that achieves the transition between the two [29], [4], [25].", "startOffset": 63, "endOffset": 67}, {"referenceID": 9, "context": "An alternative is to learn dynamics models in an on-line fashion, constantly adapting the model based on an incoming stream of observed states and actions [11] [28] [43] [22].", "startOffset": 155, "endOffset": 159}, {"referenceID": 26, "context": "An alternative is to learn dynamics models in an on-line fashion, constantly adapting the model based on an incoming stream of observed states and actions [11] [28] [43] [22].", "startOffset": 160, "endOffset": 164}, {"referenceID": 40, "context": "An alternative is to learn dynamics models in an on-line fashion, constantly adapting the model based on an incoming stream of observed states and actions [11] [28] [43] [22].", "startOffset": 165, "endOffset": 169}, {"referenceID": 20, "context": "An alternative is to learn dynamics models in an on-line fashion, constantly adapting the model based on an incoming stream of observed states and actions [11] [28] [43] [22].", "startOffset": 170, "endOffset": 174}, {"referenceID": 5, "context": "Another alternative is to iteratively intertwine data collection and dynamics model learning [7] [10].", "startOffset": 93, "endOffset": 96}, {"referenceID": 8, "context": "Another alternative is to iteratively intertwine data collection and dynamics model learning [7] [10].", "startOffset": 97, "endOffset": 101}, {"referenceID": 26, "context": "A number of options are available for representation of learned dynamics functions, from linear functions [28] [43], to Gaussian processes [2] [19] [7], to deep neural networks [32] [11].", "startOffset": 106, "endOffset": 110}, {"referenceID": 40, "context": "A number of options are available for representation of learned dynamics functions, from linear functions [28] [43], to Gaussian processes [2] [19] [7], to deep neural networks [32] [11].", "startOffset": 111, "endOffset": 115}, {"referenceID": 1, "context": "A number of options are available for representation of learned dynamics functions, from linear functions [28] [43], to Gaussian processes [2] [19] [7], to deep neural networks [32] [11].", "startOffset": 139, "endOffset": 142}, {"referenceID": 17, "context": "A number of options are available for representation of learned dynamics functions, from linear functions [28] [43], to Gaussian processes [2] [19] [7], to deep neural networks [32] [11].", "startOffset": 143, "endOffset": 147}, {"referenceID": 5, "context": "A number of options are available for representation of learned dynamics functions, from linear functions [28] [43], to Gaussian processes [2] [19] [7], to deep neural networks [32] [11].", "startOffset": 148, "endOffset": 151}, {"referenceID": 30, "context": "A number of options are available for representation of learned dynamics functions, from linear functions [28] [43], to Gaussian processes [2] [19] [7], to deep neural networks [32] [11].", "startOffset": 177, "endOffset": 181}, {"referenceID": 9, "context": "A number of options are available for representation of learned dynamics functions, from linear functions [28] [43], to Gaussian processes [2] [19] [7], to deep neural networks [32] [11].", "startOffset": 182, "endOffset": 186}, {"referenceID": 35, "context": "There is a rich body of work focusing on adapting policies, rather than actions in the context of reinforcement learning [37] [1] [5].", "startOffset": 121, "endOffset": 125}, {"referenceID": 0, "context": "There is a rich body of work focusing on adapting policies, rather than actions in the context of reinforcement learning [37] [1] [5].", "startOffset": 126, "endOffset": 129}, {"referenceID": 3, "context": "There is a rich body of work focusing on adapting policies, rather than actions in the context of reinforcement learning [37] [1] [5].", "startOffset": 130, "endOffset": 133}, {"referenceID": 41, "context": "simulator and the real world [44] [27].", "startOffset": 29, "endOffset": 33}, {"referenceID": 25, "context": "simulator and the real world [44] [27].", "startOffset": 34, "endOffset": 38}, {"referenceID": 39, "context": "In addition to actions, adaptation of states and observations between simulation and the real world is another challenging problem [41] [16] [40] [8].", "startOffset": 131, "endOffset": 135}, {"referenceID": 14, "context": "In addition to actions, adaptation of states and observations between simulation and the real world is another challenging problem [41] [16] [40] [8].", "startOffset": 136, "endOffset": 140}, {"referenceID": 38, "context": "In addition to actions, adaptation of states and observations between simulation and the real world is another challenging problem [41] [16] [40] [8].", "startOffset": 141, "endOffset": 145}, {"referenceID": 6, "context": "In addition to actions, adaptation of states and observations between simulation and the real world is another challenging problem [41] [16] [40] [8].", "startOffset": 146, "endOffset": 149}, {"referenceID": 19, "context": "tendons, as in [21].", "startOffset": 15, "endOffset": 19}, {"referenceID": 18, "context": "and variance 1 [20].", "startOffset": 15, "endOffset": 19}, {"referenceID": 36, "context": "We test our method on several simulated models in the robotics simulator MuJoCo [38] using OpenAI Gym environment [3].", "startOffset": 80, "endOffset": 84}, {"referenceID": 31, "context": "The expert policies are obtained from Trust Region Policy Optimization [33] (source code by Ho et.", "startOffset": 71, "endOffset": 75}, {"referenceID": 13, "context": "al [15]).", "startOffset": 3, "endOffset": 7}, {"referenceID": 37, "context": "ing iterative LQR [39].", "startOffset": 18, "endOffset": 22}, {"referenceID": 9, "context": "This is the approach proposed and described in more detail in [11].", "startOffset": 62, "endOffset": 66}, {"referenceID": 39, "context": "we plan to experiment with observation adaptation methods, such as [41] for instance.", "startOffset": 67, "endOffset": 71}], "year": 2016, "abstractText": "Developing control policies in simulation is often more practical and safer than directly running experiments in the real world. This applies to policies obtained from planning and optimization, and even more so to policies obtained from reinforcement learning, which is often very data demanding. However, a policy that succeeds in simulation often doesnt work when deployed on a real robot. Nevertheless, often the overall gist of what the policy does in simulation remains valid in the real world. In this paper we investigate such settings, where the sequence of states traversed in simulation remains reasonable for the real world, even if the details of the controls are not, as could be the case when the key differences lie in detailed friction, contact, mass and geometry properties. During execution, at each time step our approach computes what the simulation-based control policy would do, but then, rather than executing these controls on the real robot, our approach computes what the simulation expects the resulting next state(s) will be, and then relies on a learned deep inverse dynamics model to decide which real-world action is most suitable to achieve those next states. Deep models are only as good as their training data, and we also propose an approach for data collection to (incrementally) learn the deep inverse dynamics model. Our experiments shows our approach compares favorably with various baselines that have been developed for dealing with simulation to real world model discrepancy, including output error control and Gaussian dynamics adaptation.", "creator": "LaTeX with hyperref package"}}}