{"id": "1610.03577", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Minimax Filter: Learning to Preserve Privacy from Inference Attacks", "abstract": "Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differential privacy, which provides a more formal definition of privacy, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform the optimization. In addition, the paper proposes an extension that combines minimax filter and differentially-private noisy mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or better target task accuracy and lower inference accuracy, often significantly lower than previous methods.", "histories": [["v1", "Wed, 12 Oct 2016 01:57:20 GMT  (117kb,D)", "http://arxiv.org/abs/1610.03577v1", null], ["v2", "Thu, 14 Sep 2017 18:40:30 GMT  (148kb,D)", "http://arxiv.org/abs/1610.03577v2", "Revision 1. Minor changes and fixes"]], "reviews": [], "SUBJECTS": "cs.LG", "authors": ["jihun hamm"], "accepted": false, "id": "1610.03577"}, "pdf": {"name": "1610.03577.pdf", "metadata": {"source": "CRF", "title": "Minimax Filter: Learning to Preserve Privacy from Inference Attacks", "authors": ["Jihun Hamm"], "emails": ["hammj@cse.ohio-state.edu"], "sections": [{"heading": null, "text": "Keywords: inference attack, empirical risk minimization, minimax optimization, differential privacy, k-anonymity"}, {"heading": "1. Introduction", "text": "In fact, it is only a matter of time before most of them will be able to decide whether or not they want to stay in the US; in the US, they will be able to choose whether or not to stay in the US."}, {"heading": "2. Related work", "text": "In contrast, this paper is one of the main questions in privacy research. Utilityprivacy tradeoff is a direct measure of privacy risks under different assumptions about privacy (Dwork and Nissim, 2004; Dwork et al., 2006; Dwork, 2006), in the context of statistical estimation (Smith, 2011; Alvim et al., 2012; Duchi et al., 2013) and learning ability (Kasiviswanathan et al., 2011).Other measures of privacy and utility have also been proposed. Information theoretical quantities have been analyzed by Sankar et al. (2010); Rebollo-Monedero et al. (2010); du Pin Calmon and Fawaz et al., privacy in terms of distortion theory in communication. One problem with the use of mutual information or related quantity is that it is difficult to estimate mutual information about high-dimensional and continuous variables in practice without a simple distribution."}, {"heading": "3. Minimax Filter", "text": "In this section, the Minimax filter is formulated and discussed in detail and its generalization performance is analyzed."}, {"heading": "3.1 Formulation", "text": "In fact, it is so that it is a matter of a way in which it is a matter of a way in which people move in the most diverse areas of life. (...) In fact, it is as if people are able to put themselves in the world. (...) It is as if they were able to put themselves in the world. (...) It is as if they were able to put themselves in the world. (...) It is as if they were able to put themselves in the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world of the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world, the world in the world in the world, the world in the world in the world, the world in the world in the world, the world in the world in the world in the world, the world in the world in the world in the world, the world in the world in the world in the world in the world, the world in the world in the world in the world, the world in the world in the world in the world, the world in the world in the world in the world in the world, in the world in the world in the world in the world, in the world in the world in the world in the world, in the world in the world in the world in the world, in the world in the world in the world in the world in the world, in the world in the world in the world in the world, in the world in the world in the world, in the world in the world in the world in the world, in the world in the world in the world, in the world in the world in the world in the world in the world, in the world in the world in the world in the world, in the world in the world in the world in the world in the world, in the world in the world in the world in the world in the, in the world in the world in the world in the world in the world"}, {"heading": "3.2 Notes on private and utility tasks", "text": "The private variable y can be any attribute considered sensitive or identifying. For example, y can be any number or string uniquely assigned to a person in the data set. Such identifiers are bijective with {1,..., S}, where S is the total number of participants, so assume y {1,..., S}. The private task of an opponent is then to predict the identity y based on the filtered data g (x), the inaccuracy of which is measured by the expected risk. That is, the higher the risk, the more anonymous is the filtered output. The identity variable can also be group identifiers, e.g. y is a demographic grouping based on age, gender, ethnicity, etc., another example of private tasks is to filter a specific subject individually among the rest, in which case y is binary: y = \u2212 1 means \"not the target subject\" and thus \"the subject = 1 means the target subject is anonymous with the rest of the anonymous."}, {"heading": "3.3 Multiple tasks", "text": "Suppose there are Np private tasks f1priv (u, vi) = E [l i p (y (x; u); vi), y i)]. Suppose there are Nu target tasks f1util (u, w1),..., f Nu util (u, wNu) = E [l i p (y (x; u); vi), y i)]. Suppose there are Nu target tasks f1util (u, w1),..., f Nu util (u, wNu) = E [l i p (y (x; u); vi), y i). Suppose there are the coefficients that represent the relative importance of private tasks, and \u03c1Nu are the coefficients for utility tasks, then the common goal is to solve the following problem: u Np \u00b2 i = 1,..."}, {"heading": "3.4 Generalization performance of minimax filter", "text": "The proposed data protection mechanism is an educational approach. An optimum filter is one that solves the expected risk problem (u = ED). In reality, however, an optimum filter (u = ED) must be assessed (u = ED). (u = ED). (u = ED). (u = ED (u = ED). (u = ED). (u = ED). (u = ED). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u. (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u). (u (u). (u). (u) (u). (u) (u) (u). (u). (u) (u). (u (u). (u). (u) (u) (u) (u) (u) (u) (u). (u (u). (u) (u). (u (u) (u). (u) (u). (u). (u). (u"}, {"heading": "4. Minimax Optimization", "text": "This section presents theoretical and numerical solutions to the common problem (7), which is a variant of unlimited continuous minimax problems (see Rustem and Howe (2009) for a review.) The problem (7) can be written in an equivalent form: u) = min u [\u03a6priv (u) \u2212 \u03c1 \u03a6util (u)] (44) = min u [max v \u2212 fpriv (u, v) \u2212 \u03c1 max w \u2212 futil (u, w)] (45) The optimization above is a min-diff-max problem and can be considered a simultaneous solution of two sub-problems minu [maxv \u2212 fpriv (u, v) and minu [\u2212 maxw \u2212 futil (u, w)], but is obviously not the same as the summation of individual solutions in u (u) maxi (u) 6 = min u \u2212 fpriv (u, v)] + min u \u2212 futil (u, w), problem w] (in general)."}, {"heading": "4.1 Simple case: eigenvalue problem", "text": "The filter class is a linear dimensionality reduction (g (x; u) = UTx), parameterized by the matrix U-RD \u00b7 d, and the private and target tasks are the least square regressions parameterized by the matrices V and W. (48) In this case, the quantity (U) = maxxV \u2212 fp (U, V) and maxW \u2212 fu (U, W) are both constant problems with closed form solutions V = arg min Vfpriv = (U TCxxU) \u2212 fp (U, V)."}, {"heading": "4.2 Saddle-point problem", "text": "There is a subclass of continuous minimax problems that are easier to solve than others. Saddle point problems are minimax problems for which f (u, v) in u is convex and concave in v, such as asmin u max v (u, v) = min u max v [u2 \u2212 v2]. (63) Analogous to convex problems, f (u, v) has a global optimum (u, v), the f (u, v) \u2264 f (u, v) \u2264 f (u, v). (64) The presence of global optima in saddle point problems makes numerical optimizations practicable. Convergence rate of a simple subgradient descent method for saddle point problems (u, v) is not yet expected (Uvl)."}, {"heading": "4.3 General problem", "text": "A general, numerical solution approach to optimization (44) is described in this section. (u) It is as if it is a real function (u). (u) It is as if it is a real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real) real (u) real (u) real (real) real (u) real (real) real (u) real (real) real (u) real (real) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real) real (u) real (u) real (u) real) real (u) real (u) real (u) real (u) real) real (u) real (u) real (u) real (u) real (u) real) real (u) real (u) real (u) real (u) real (u) real (u) real) real (u) real (u) real (u) real (u) real (u) real (u) real) real (u) real (u) real (u) real (u) real (u) real (u) real) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real (u) real) real ("}, {"heading": "5. Noisy Minimax Filter", "text": "The privacy warranty that Minimax filters offer is very different from that of differentiated private mechanisms. As the filter is learned from data, its privacy warranty is given only in expectation / probability. Furthermore, it is a deterministic mechanism that cannot provide differentiated privacy. In this section, noisy Minimax filters are presented that combine Minimax filters with additive noise mechanisms to meet differential privacy criteria."}, {"heading": "5.1 Differential privacy", "text": "& & # 252; for & # 252; for all measurable S & # 246; T of the output range and for & # 252; for all datasets D & # 160; and D & # 160; which differ in a single item, for & # 252; for the non-private function for & # 252; that is, even if an opponent knows the entire data set for & # 252; for a single item, he cannot derive much more about the unknown item from the output of the algorithm. & # 160; A known mechanism for the conversion of a non-private function for & # 160; & # 160; & & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; & 160; 160; & 160; & 160; & 160; & 160; 160; & 160; & 160; 160; & 160; & 160; 160; & 160; 160; & 160; 160 & 160; 160; & 160; 160; & 160; 160; & 160; 160; & 160; 160; & 160; 160; & 160; 160; & 160; 160 & 160; & 160; 160; & 160; & 160; & 160; 160; & 160; & 160; 160; & 160 & 160; 160; 160 & 160; 160 & 160; 160 & 160; 160 & 160; 160; 160; 160 & 160; 160 & 160; 160; 160 & 160; 160; & 160; 160 & 160; 160; 160 & 160; 160 & 160; 160 & 160; 160; 160 & 160; 160; 160 & 160 & 160; 160; 160 & 160; 160 & 160; 160 & 160; 160; 160 & 160; 160 & 160; 160; 160 & 160; 160 & 160; 160; 160 & 160; 160 & 160; 160; 160; 160 & 160; 160 & 160; 160; 160 & 160; 160; 160 & 160; 160 & 160; 160; 160 & 160; 160; 160 & 160; 160 & 160; 160; 160; 160 & 160; 160; 160 & 160; 160; 160; 160 & 160; 160 & 160; 160 & 160; 160 & 160; 160 & 160; 160; 160;"}, {"heading": "5.2 Preprocessing vs postprocessing", "text": "The paper suggests two approaches: filtering is performed before interference (so-called preprocessing) and after interference (so-called postprocessing) 4. fig. 2 shows the signal chains of the two approaches. In pre-processing, the original property x is filtered first by g (x) and then - differentially private by boundary overruns and interference (g (x)) and potentially - the original property x is done first -differentially private by4. Pre-processing and post-processing is similar, but not exactly the same as output interference and input interference (Sarwate and Chaudhuri, 2013). Bounding and perturbing b (x) followed by filtering g (x) and filtering g (x). By adding an appropriate amount of noise, both approaches are locally different, regardless of the distribution of the data."}, {"heading": "6. Experiments", "text": "In this section, the algorithms proposed in the paper are evaluated against three real-world data sets: facial data for classifying gender and expression, language data for classifying emotions, and motion data for classifying activities. First, Minimax filters are compared with nonminimax methods for privacy and benefit violations, measured against the accuracy of home and target classifiers for test data. Second, noisy Minimax filters are tested under different conditions using the same data sets."}, {"heading": "6.1 Methods", "text": "Filters. The following minimax and not minimax filters are compared: \u2022 Edge: random subspace projection with g (x; U) = UTx, where U is a random full-fledged D \u00b7 d matrix. \u2022 PCA: main component analysis with g (x; U) = UTx, where U is the eigenvector score corresponded to the largest eigenvalues of Cov (x). \u2022 PPLS: private partial smallest squares, using algorithm 1 from Enev et al. (2012). \u2022 DDD: discriminatory decreasing discriminability (DDD) from Whitehill and Movellan (2012) with a mask-type filter from Code5. \u2022 Minimax 1: linear filter g (x; U) = UTx, where U is calculated from Alg. (1). \u2022 Minimax 2: non-linear filter from the database, non-linear filter (relevant)."}, {"heading": "6.2 Data sets", "text": "The GENKI database (Whitehill and Movellan, 2012) consists of facial images with different poses and facial expressions; the original data set is used unchanged and does not overlap with the training set; the dimensionality of the original data is D = 256, and the filters are tested with d = 10, 50, 100; the data set has gender and expression labels, but no subject labels; consequently, the gender classification is used as the target task; and the emotion classification from the language is tested with d = 10, 50, 100."}, {"heading": "6.3 Result 1: Minimax filters", "text": "The dotted lines are level sets of utility-privacy-trade-off (i.e., target task accuracy - private task accuracy) that are shown as a reference. Minimax 2 achieves the best benefit (i.e., the most accurate expression classification) and Minimax 1 (linear) achieves the best privacy (i.e., the least precise gender classification). Minimax 1 achieves the highest accuracy of benefit and privacy in all dimensions (i.e., Minimax 2 and DDD perform very well). In terms of data protection risk, Minimax 1 achieves almost the probability level of accuracy (0.5), which implies a strong preservation of privacy. DDD comes close to Minimax 1, while another private method PPLS is not very successful in reducing privacy risk."}, {"heading": "6.4 Result 2: Noisy minimax filters", "text": "The same data sets from the previous section are used to demonstrate the effect of the noise mechanism on Minimax filters. Four types of noise filters are compared: PCA-pre, PCApost, Minimax-pre and Minimax-post. PCA is actually selected as a non-minimax reference filter that preserves the original signal in the sense of the least average square error. PCA-pre / post means that PCA is applied before / after the fault in a similar way to Minimax-pre / post from Fig. 2. For Minimax-pre / post, a linear filter of the same dimension as PCA-pre-post / post is used. Tests are performed for d = 20. Optimization of 6 is performed similarly to the previous section. All tests are repeated 10 times for different noise samples of (80), for each of 10 random training / test splits. Fig. 7 shows the following results. Firstly, within each plot, the data protection level from the left \u2212 1 = 10 is reduced to accuracy."}, {"heading": "7. Conclusion", "text": "This paper presents a new privacy-preserving mechanism for preventing inferences to continuous and high-dimensional data. In this mechanism, a filter transforms continuous and high-dimensional raw data into a dimensionally reduced representation of data. After filtering, information about target tasks remains, but information about identifying / sensitive attributes is removed, making it difficult for an adversary to derive these attributes accurately from the released filtered output. Minimax filters are designed to achieve the optimal benefit-privacy compromise in terms of expected risk. The paper proves that a filter learned from empirical risks is not far from an ideal filter, as the number of samples increases. This property and their dependence on the task make this mechanism very different from previous mechanisms, including syntactical anonymization and differential privacy. Algorithms to locate minimax filters are presented and analyzed to demonstrate subjective data attacks that are available to the public, in order to demonstrate the subjective nature of data attacks."}, {"heading": "Acknowledgments", "text": "It is not the first time that the EU Commission has taken such a step."}], "references": [], "referenceMentions": [], "year": 2016, "abstractText": "Preserving privacy of continuous and/or high-dimensional data such as images, videos and audios, can be challenging with syntactic anonymization methods which are designed for discrete attributes. Differential privacy, which provides a more formal definition of privacy, has shown more success in sanitizing continuous data. However, both syntactic and differential privacy are susceptible to inference attacks, i.e., an adversary can accurately infer sensitive attributes from sanitized data. The paper proposes a novel filter-based mechanism which preserves privacy of continuous and high-dimensional attributes against inference attacks. Finding the optimal utility-privacy tradeoff is formulated as a min-diff-max optimization problem. The paper provides an ERM-like analysis of the generalization error and also a practical algorithm to perform the optimization. In addition, the paper proposes an extension that combines minimax filter and differentially-private noisy mechanism. Advantages of the method over purely noisy mechanisms is explained and demonstrated with examples. Experiments with several real-world tasks including facial expression classification, speech emotion classification, and activity classification from motion, show that the minimax filter can simultaneously achieve similar or better target task accuracy and lower inference accuracy, often significantly lower than previous methods.", "creator": "LaTeX with hyperref package"}}}