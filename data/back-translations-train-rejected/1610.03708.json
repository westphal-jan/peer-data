{"id": "1610.03708", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Generating captions without looking beyond objects", "abstract": "This paper explores new evaluation perspectives for image captioning and introduces a noun translation task that achieves comparative image caption generation performance by translating from a set of nouns to captions. This implies that in image captioning, all word categories other than nouns can be evoked by a powerful language model without sacrificing performance on the precision-oriented metric BLEU. The paper also investigates lower and upper bounds of how much individual word categories in the captions contribute to the final BLEU score. A large possible improvement exists for nouns, verbs, and prepositions.", "histories": [["v1", "Wed, 12 Oct 2016 13:42:03 GMT  (3074kb,D)", "https://arxiv.org/abs/1610.03708v1", "This paper is accepted to the ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)"], ["v2", "Tue, 18 Oct 2016 09:35:03 GMT  (3076kb,D)", "http://arxiv.org/abs/1610.03708v2", "This paper was presented at the ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)"]], "COMMENTS": "This paper is accepted to the ECCV2016 2nd Workshop on Storytelling with Images and Videos (VisStory)", "reviews": [], "SUBJECTS": "cs.CV cs.CL", "authors": ["hendrik heuer", "christof monz", "arnold w m smeulders"], "accepted": false, "id": "1610.03708"}, "pdf": {"name": "1610.03708.pdf", "metadata": {"source": "CRF", "title": "Generating captions without looking beyond objects", "authors": ["Hendrik Heuer", "Christof Monz", "Arnold W.M. Smeulders"], "emails": ["h.heuer@uva.nl", "c.monz@uva.nl", "a.w.m.smeulders@uva.nl"], "sections": [{"heading": null, "text": "Keywords: caption, review, machine translation"}, {"heading": "1 Introduction", "text": "The purpose of the caption is to automatically generate grammatical descriptions of images that represent the meaning of a single image. Figure 1 compares different captions for an image from the MSCOCO [2] dataset. Describing Figure 1 as \"woman and dog with frisbee on grass near the fence.\" merely transforms the results of object recognition into fluent English sentences. A caption such as \"a woman playing tug of war with a dog over a white frisbee.\" shows an understanding that the dog bites a frisbee, that the woman is playing with the dog and, more specifically, that the woman and the dog are playing a game called tug of war. Captions like this are more informative and relevant. They could also be useful in a assistive technological context where captions help visually impaired people, such as on Facebook. This paper motivates why it is important to separate the caption from the object identification task."}, {"heading": "2 Background", "text": "In fact, it is so that most people who are able to determine for themselves what they want and what they want to do \u3002 Most of them are not able to move \u3002 Most of them are not able to move \u3002 Most of them are not able to move \u3002 Most of them are not able to move \u3002 Most of them are not able to move themselves \u3002 Most of them are not able to move themselves \u3002 Most of them are not able to move themselves \u3002 Most of them are not able to move themselves \u3002 Most of them are not able to move themselves \u3002 Most of them are not able to move themselves \u3002 Most of them are able to move themselves \u3002"}, {"heading": "3 Methodology", "text": "We introduce a blind noun translation task, which demonstrates the linguistic modeling capabilities of state-of-the-art language models such as LSTMs and practically shows how much a language model can derive from a set of nouns. For the blind noun translation task, a state-of-the-art neural machine translation model [1] is trained to translate nouns into complete noun descriptions, which are compared with the Karpathy et al system. [3] Image captioning system. Nouns added to the translation model are not taken from the basic truth of the data set, but are automatically extracted from the capabilities generated by Karpathy et al. to increase comparability of results. Stanford log linear part-of-speech tagger [6.5] is used to extract all noun days. The model for the blind COnoun-day task 2014 is trained by the translation task MCO."}, {"heading": "4 Results of the blind noun (theoretical) experiment", "text": "This year, it has come to the point that it will only be a matter of time before it is ready, until it is ready."}, {"heading": "5 Discussion", "text": "The results show that a blind noun translation system can generate captions comparable to state-of-the-art captioning systems, underscoring how strong the LSTM's language modeling capabilities are. It also shows how important it is to critically evaluate the contribution of the LSTM and its intrinsic language modeling capabilities, which should motivate a more rigorous evaluation of the captioning outcomes. While we recognize the limitations of BLEU metrics for the overall evaluation task as a precision-based metric, it is nevertheless useful to investigate how much individual word categories contribute to the captioning performance. Results show that it is possible to perform a more qualitative analysis of the contribution of certain linguistic phenomena to the captioning task. Analysis shows that a significant improvement in BLEU precision metrics and certain word categories can be performed, especially when substituting automatically for certain nouns, and automated predictions can be performed."}], "references": [{"title": "Neural machine translation by jointly learning to align and translate", "author": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "venue": "CoRR abs/1409.0473 (2014),", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Microsoft COCO captions: Data collection and evaluation", "author": ["X. Chen", "H. Fang", "T. Lin", "R. Vedantam", "S. Gupta", "P. Doll\u00e1r", "C.L. Zitnick"], "venue": null, "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2015}, {"title": "Deep visual-semantic alignments for generating image descriptions", "author": ["A. Karpathy", "L. Fei-Fei"], "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3128\u20133137", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2015}, {"title": "Bleu: A method for automatic evaluation of machine translation", "author": ["K. Papineni", "S. Roukos", "T. Ward", "W.J. Zhu"], "venue": "Proceedings of the 40th Annual Meeting on Association for Computational Linguistics. pp. 311\u2013318", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2002}, {"title": "Feature-rich part-of-speech tagging with a cyclic dependency network", "author": ["K. Toutanova", "D. Klein", "C.D. Manning", "Y. Singer"], "venue": "In Proceedings of HLT-NAACL 2003. pp. 252\u2013259", "citeRegEx": "5", "shortCiteRegEx": null, "year": 2003}, {"title": "Enriching the knowledge sources used in a maximum entropy part-of-speech tagger", "author": ["K. Toutanova", "C.D. Manning"], "venue": "EMNLP \u201900,", "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2000}, {"title": "Show and tell: A neural image caption generator", "author": ["O. Vinyals", "A. Toshev", "S. Bengio", "D. Erhan"], "venue": "CoRR abs/1411.4555", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Image captioning with an intermediate attributes layer", "author": ["Q. Wu", "C. Shen", "A. van den Hengel", "L. Liu", "A.R. Dick"], "venue": null, "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2015}, {"title": "Show, attend and tell: Neural image caption generation with visual attention", "author": ["K. Xu", "J. Ba", "R. Kiros", "K. Cho", "A.C. Courville", "R. Salakhutdinov", "R.S. Zemel", "Y. Bengio"], "venue": null, "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2015}, {"title": "Trainable performance upper bounds for image and video captioning", "author": ["L. Yao", "N. Ballas", "K. Cho", "J.R. Smith", "Y. Bengio"], "venue": null, "citeRegEx": "10", "shortCiteRegEx": "10", "year": 2015}], "referenceMentions": [{"referenceID": 1, "context": "Figure 1 compares different captions for an image from the MSCOCO [2] dataset.", "startOffset": 66, "endOffset": 69}, {"referenceID": 2, "context": "[3] system, the Vinyals et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] system, and the Xu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].", "startOffset": 208, "endOffset": 217}, {"referenceID": 6, "context": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].", "startOffset": 208, "endOffset": 217}, {"referenceID": 8, "context": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].", "startOffset": 208, "endOffset": 217}, {"referenceID": 7, "context": "Recent approaches in automated caption creation have addressed the problem of generating grammatical descriptions of images as a sequence modeling problem using recurrent neural network (RNN) language models [3,7,9,8].", "startOffset": 208, "endOffset": 217}, {"referenceID": 2, "context": "[3] and Vinyals et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7] combine a vision CNN with a language generating RNN.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] extended this architecture by adding an attention mechanism, which learns not only a distribution over the words in the vocabulary but also a distribution over the locations in the image based on the last convolutional layer of a CNN pretrained on ImageNet.", "startOffset": 0, "endOffset": 3}, {"referenceID": 7, "context": "[8] introduced a method of incorporating explicit high-level concepts such as bag, eating, and red, which is remarkable as it covers a noun, a verb, and an adjective.", "startOffset": 0, "endOffset": 3}, {"referenceID": 9, "context": "This work is complementary to previous work that disentangled the contribution of visual model and language model [10].", "startOffset": 114, "endOffset": 118}, {"referenceID": 3, "context": "BLEU is a precision-oriented machine translation evaluation metric that measures the N-gram overlap between sentences in the output of a machine translation system and one or more reference translations [4].", "startOffset": 203, "endOffset": 206}, {"referenceID": 0, "context": "For the blind noun translation task, a state-of-the-art neural machine translation model [1] is trained to translate nouns into full captions.", "startOffset": 89, "endOffset": 92}, {"referenceID": 2, "context": "[3] image captioning system.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For this, the Stanford log-linear part-of-speech tagger [6,5] is used to extract all noun tags.", "startOffset": 56, "endOffset": 61}, {"referenceID": 4, "context": "For this, the Stanford log-linear part-of-speech tagger [6,5] is used to extract all noun tags.", "startOffset": 56, "endOffset": 61}, {"referenceID": 1, "context": "The model for the blind noun translation task is trained on the MSCOCO 2014 [2] training set, which includes over 80,000 images with 5 captions per image.", "startOffset": 76, "endOffset": 79}, {"referenceID": 1, "context": "For the evaluation of the blind noun translation experiment, the MSCOCO 2014 [2] validation set is used, which consists of more than 40,000 captions.", "startOffset": 77, "endOffset": 80}, {"referenceID": 2, "context": "[3], Vinyals et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 6, "context": "[7], and Xu et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "system [9] was obtained and analyzed.", "startOffset": 7, "endOffset": 10}, {"referenceID": 5, "context": "Using the Stanford log-linear part-of-speech tagger [6,5], for each word in the caption, the word category based on its syntactic function was assigned (noun, verb, adjective).", "startOffset": 52, "endOffset": 57}, {"referenceID": 4, "context": "Using the Stanford log-linear part-of-speech tagger [6,5], for each word in the caption, the word category based on its syntactic function was assigned (noun, verb, adjective).", "startOffset": 52, "endOffset": 57}, {"referenceID": 2, "context": "[3] compared to a noun translation system, that translates the nouns extracted from the Karpathy et al.", "startOffset": 0, "endOffset": 3}, {"referenceID": 0, "context": "image captioning system into captions using a state-of-the-art machine translation system [1].", "startOffset": 90, "endOffset": 93}, {"referenceID": 2, "context": "[3]) 63.", "startOffset": 0, "endOffset": 3}, {"referenceID": 2, "context": "[3] system on the MSCOCO dataset showing possible improvement respectively loss for different word categories.", "startOffset": 0, "endOffset": 3}, {"referenceID": 8, "context": "[9] system on the MSCOCO dataset showing possible improvement respectively loss for different word categories.", "startOffset": 0, "endOffset": 3}], "year": 2016, "abstractText": "This paper explores new evaluation perspectives for image captioning and introduces a noun translation task that achieves comparative image caption generation performance by translating from a set of nouns to captions. This implies that in image captioning, all word categories other than nouns can be evoked by a powerful language model without sacrificing performance on n-gram precision. The paper also investigates lower and upper bounds of how much individual word categories in the captions contribute to the final BLEU score. A large possible improvement exists for nouns, verbs, and prepositions.", "creator": "LaTeX with hyperref package"}}}