{"id": "1610.03759", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Language Models with Pre-Trained (GloVe) Word Embeddings", "abstract": "In this work we implement a training of a Language Model (LM), using Recurrent Neural Network (RNN) and GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2], but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].", "histories": [["v1", "Wed, 12 Oct 2016 15:53:02 GMT  (115kb,D)", "https://arxiv.org/abs/1610.03759v1", null], ["v2", "Sun, 5 Feb 2017 11:24:05 GMT  (119kb,D)", "http://arxiv.org/abs/1610.03759v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["victor makarenkov", "bracha shapira", "lior rokach"], "accepted": false, "id": "1610.03759"}, "pdf": {"name": "1610.03759.pdf", "metadata": {"source": "CRF", "title": "Language Models with Pre-Trained (GloVe) Word Embeddings", "authors": ["Victor Makarenkov", "Lior Rokach", "Bracha Shapira"], "emails": ["makarenk@post.bgu.ac.il,", "liorrk@bgu.ac.il,", "bshapira@bgu.ac.il"], "sections": [{"heading": null, "text": "Voice models with pre-trained (GloVe) word embeddings Victor Makarenkov, Lior Rokach, Bracha Shapira makarenk @ post.bgu.ac.il, liorrk @ bgu.ac.il, bshapira @ bgu.ac.ilDepartment of Software and Information Systems Engineering Ben-Gurion University of the Negev"}, {"heading": "1 Introduction", "text": "In this paper, we present a step-by-step implementation of the training of a language model (LM) introduced by Pennigton et al. in [1] using Recurrent Neural Network (RNN) and pre-trained GloVe Word Embeddings. Implementation follows the general idea of training RNs for LM tasks presented in [2], but uses the Gated Recurrent Unit (GRU) [3] for a memory cell rather than the more commonly used LSTM [4]. The implementation presented is based on the use of Keras1 [5]."}, {"heading": "2 Motivation", "text": "Language modeling is an important task in many applications of Natural Language Processing (NLP), which includes clustering, information gathering, machine translation, spelling, and grammatical error correction. Generally, a language model is defined as a function that sets a probability measure over strings extracted from a vocabulary. In this thesis, we consider an RNN-based language model task that aims to predict the next nth word in a sequence, given the preceding n \u2212 1 words. In other words, finding the word with the maximum value for P (wn | w1,..., wn \u2212 1). The n parameter is the ContextWindowSize argument in the algorithm described above. To maximize the effectiveness and performance of the model, we use word embedding in a continuous vector space. The model of embedding that we use is the Working WindowSize argument in the aforementioned algorithm."}, {"heading": "3 Short Description", "text": "In this work, we use 300-dimensional and 50-dimensional, GloVe word embeddings. To embed the words in a vector space, the GloVe model is trained by examining the word coexistence matrix Xij within a huge text corpus. Despite the enormous size of the Common Crawl corpus, some words cannot exist with the embeddings, so we put these words on random vectors and use the same embeddings consistently when we come across the same invisible word in the text again. RNN is further trained to predict the next word in its embeddedness form, that is, the next n-dimensional vector, considering the previous words of ContextWindowSize. We share the TextF file for training and testing purposes in 70% and 30%."}, {"heading": "4 Pseudo Code", "text": "Input: Input: Glove vectors: Pre-Trained-Word-Embeddings, Text-File, ContextWindowSize = 10, hidden-unites = 300 Output: A language model trained on text file with word-embeddings representation for w-text-file if w-OOV-File thentokenized-file.append (OOV-File.get-Vector (w)) end if w-glove-vectors thentokenized-file.append (glove-vectors.get-vector (w) end elsevector \u2190 random-vector () tokenized-file.append (vector) OOV-file.append (vector) end NN-Vectors.get-vector (w) end elsevector-vectors.get-vector (w) end elsevector (w) end elsevector \u2190 random-vector () tokenized-file.append (vector) OOOV-file.append (vector) Nutzed-Nutrized-Nutrized-Nutrized-Nutrized-Nutrized-vector (w) Nutrized-Nutrized-Nutrized-end (w) Nutrized-Nutrized-Nutrized-vector (w) Nutrized-Nutrized-Nutrized-Nutrized-vector (w) Nutrized-Nutrized-end (Nutrized-Nutrized-vector) Nutrized-end (Nutrized-Nutrized-Nutrized-end) Nutrized-Nutrized-end (Nutrized-Nutrized-Nutrized-Nutrized-vector) (Nutrized-Nutrized-end) (Nutrized-Nutrized-Nutrized-Nutrized-Nutrized-vector) Nutrized-Nutrized-Nutrized-end (Nutrized-Nutrized-"}, {"heading": "5 Detailed Explanation", "text": "As already mentioned, the GloVe model is trained by examining the word equality matrix of two words i and j: Xij within a huge text corpus. During the training, wiTwj + bi + bj = log (Xij), where wi and wj are the trained vectors, bi and bj are the scalar biases associated with the words i and j. The most important parts of the training process in GloVe are: 1) A weighting function f to eliminate very common words (such as stop words) that add noise and are not overrated, 2) rare words are not overweighted, 3) the co-event strength when modeled as distance should be smoothed with a log function. Thus, the final loss function for a GloVe model is J = i, j \u00b2 V f (Xij) (wij Twj + bj \u2212 log \u2212 Xix (max = 7) and (Vj = 7) is a complete part (x)."}, {"heading": "6 Evaluation", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "6.1 Pre-trained Vector Models", "text": "In order to evaluate our implementation of the language model, we train several different language models and compare the predicted error distribution with the prediction of a standard word. The error is measured with a cosine space3 between two vectors: 1 \u2212 x \u00b7 y | | x | | | | y |. In Figure 2, we see the error distribution of the RNN with 30 hidden units. The training was performed on a 5000 character text file, the first entry in Wikipedia, Anarchism. The machine used for the evaluation showed the following features: 1.7 GHz, Core i7 with 8 GB of memory, OS X version 10.11.13. The time it took to train the model with 30 epochs was 125 seconds. The time it took to make predictions on a test set was 0.5 seconds.In Figure 3, we see the error distribution of the RNN with 300 hidden units. The time it took to train the model with 300 epochs was 1298 seconds."}, {"heading": "6.2 Self Trained Vector Model", "text": "We used ICTIR-2015 and SIGIR2015 conference materials as corpora and produced 50-dimensional vectors. The vector model is based on a total of 1,500,000 tokens and resulted in 17,000 long vocabulary. The language model for the evaluation is based on a paper published in ICTIR-2015 Proceedings [7]. Consider Figure 4. The error distribution of the predicted words differs even more than in the general case where the vectors were trained on the common crawl corpora. That is, the performance of the language model for the task of word prediction is higher. The time required to build this model is 10 seconds. The time to calculate the predictions for the evaluations is 0.04 seconds."}, {"heading": "7 Instructions for running the code", "text": "The implementation of the model training was written in this thesis in the Python language, version 2.7. The library used is Keras, which was based on Theano Framework in the course of this implementation. Instead of Theano, the Tensorflow of Google can also be used behind the scenes of the Keras in this implementation. In order to train the model itself, you must follow the next steps: 1. Download pre-formed GloVe vectors from http: / / nlp.stanford.edu / projects / glove / 2. Get a text to train the model on it. In our example, we use a WikipdiaAnarchism entry. 3. Open and adjust the LM _ RNN _ GloVe.py file parameters within the main function: (a) File _ 2 _ tokenize _ name (example = \"/ Users / macbook _ corportoid.4c _ file _ itoid.4c _ (example / vsers / totbook / kenizkenizze / kenizkenizkenizze _ corportfile) (2.tokenizkenizze _ file).corporto.x.ib (.corporto.ib)."}, {"heading": "8 Discussion", "text": "To highlight the strength of such an approach, we have chosen one of the most powerful and prominent techniques for text embedding - the GloVe embedding. Although there are other approaches, such as the popular word2vec [8] technique, GloVe embedding has been shown to outperform it for several tasks [1], partly for the reasons described in Section 5. By training the model with two different settings, one of which is order of magnitude more complex than the other, we show the power of such an LM. The distributions shown in Figures 2 and 3 show significantly smaller errors in the task of the next word prediction. The most important limitation of this implementation is the fixed window size, the prefix in the LM. This approach does not fully display the full power of an RNN-based LM. For dynamic size selection LM, see the DyNet [6] package for example. DyNet supports dynamic calculation and divides the parameters across multiple learnable lengths."}, {"heading": "9 The source at GitHub", "text": "s GitHub repository and is available at vicmak username, Proofseer project4.4 https: / / github.com / vicmak / ProofSeer."}], "references": [{"title": "Glove: Global vectors for word representation", "author": ["J. Pennington", "R. Socher", "C.D. Manning"], "venue": "Empirical Methods in Natural Language Processing (EMNLP)", "citeRegEx": "1", "shortCiteRegEx": "1", "year": 2014}, {"title": "Recurrent neural network regularization", "author": ["W. Zaremba", "I. Sutskever", "O. Vinyals"], "venue": "CoRR abs/1409.2329", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 2014}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "author": ["K. Cho", "B. van Merrienboer", "\u00c7. G\u00fcl\u00e7ehre", "F. Bougares", "H. Schwenk", "Y. Bengio"], "venue": null, "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2014}, {"title": "Long short-term memory", "author": ["S. Hochreiter", "J. Schmidhuber"], "venue": "Neural Comput", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "DyNet: The Dynamic Neural Network Toolkit arXiv preprint arXiv:1701.03980", "author": ["Graham Neubig", "Chris Dyer", "Yoav Goldberg", "Austin Matthews", "Waleed Ammar", "Antonios Anastasopoulos", "Miguel Ballesteros", "David Chiang", "Daniel Clothiaux", "Trevor Cohn", "Kevin Duh", "Manaal Faruqui", "Cynthia Gan", "Dan Garrette", "Yangfeng Ji", "Lingpeng Kong", "Adhiguna Kuncoro", "Gaurav Kumar", "Chaitanya Malaviya", "Paul Michel", "Yusuke Oda", "Matthew Richardson", "Naomi Saphra", "Swabha Swayamdipta", "Pengcheng Yin"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2017}, {"title": "Theoretical categorization of query performance predictors", "author": ["V. Makarenkov", "B. Shapira", "L. Rokach"], "venue": "Proceedings of the 2015 International Conference on The Theory of Information Retrieval. ICTIR \u201915,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2015}, {"title": "Efficient estimation of word representations in vector space", "author": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "venue": "CoRR abs/1301.3781", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2013}], "referenceMentions": [{"referenceID": 0, "context": "in [1].", "startOffset": 3, "endOffset": 6}, {"referenceID": 1, "context": "The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].", "startOffset": 92, "endOffset": 95}, {"referenceID": 2, "context": "The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].", "startOffset": 145, "endOffset": 148}, {"referenceID": 3, "context": "The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4].", "startOffset": 204, "endOffset": 207}, {"referenceID": 0, "context": "The model of embedding we use is the GloVe [1] model, with dimensionality size equal to 300 or 50.", "startOffset": 43, "endOffset": 46}, {"referenceID": 1, "context": "It was shown lately, that RNNs outperform most language modeling based tasks [2, 3] when tuned and trained correctly.", "startOffset": 77, "endOffset": 83}, {"referenceID": 2, "context": "It was shown lately, that RNNs outperform most language modeling based tasks [2, 3] when tuned and trained correctly.", "startOffset": 77, "endOffset": 83}, {"referenceID": 5, "context": "The language model for the evaluation was built on a paper published in the ICTIR-2015 proceedings [7].", "startOffset": 99, "endOffset": 102}, {"referenceID": 6, "context": "Although there are other approaches, such as the popular word2vec [8] technique, the GloVe embeddings was shown to outperform it on several tasks [1], partially because of the reasons described in section 5.", "startOffset": 66, "endOffset": 69}, {"referenceID": 0, "context": "Although there are other approaches, such as the popular word2vec [8] technique, the GloVe embeddings was shown to outperform it on several tasks [1], partially because of the reasons described in section 5.", "startOffset": 146, "endOffset": 149}, {"referenceID": 4, "context": "For dynamic size prefix LM please consider the DyNet [6] package for example.", "startOffset": 53, "endOffset": 56}], "year": 2017, "abstractText": "In this work we present a step-by-step implementation of training a Language Model (LM) , using Recurrent Neural Network (RNN) and pre-trained GloVe word embeddings, introduced by Pennigton et al. in [1]. The implementation is following the general idea of training RNNs for LM tasks presented in [2] , but is rather using Gated Recurrent Unit (GRU) [3] for a memory cell, and not the more commonly used LSTM [4]. The implementation presented is based on using keras1 [5].", "creator": "LaTeX with hyperref package"}}}