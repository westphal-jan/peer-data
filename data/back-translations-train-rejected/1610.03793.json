{"id": "1610.03793", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Introduction to the \"Industrial Benchmark\"", "abstract": "A novel reinforcement learning benchmark, called Industrial Benchmark, is introduced. The Industrial Benchmark aims at being be realistic in the sense, that it includes a variety of aspects that we found to be vital in industrial applications. It is not designed to be an approximation of any real system, but to pose the same hardness and complexity.", "histories": [["v1", "Wed, 12 Oct 2016 17:18:01 GMT  (12kb)", "http://arxiv.org/abs/1610.03793v1", "11 pages"], ["v2", "Thu, 28 Sep 2017 11:28:26 GMT  (97kb)", "http://arxiv.org/abs/1610.03793v2", "11 pages"]], "COMMENTS": "11 pages", "reviews": [], "SUBJECTS": "cs.LG", "authors": ["daniel hein", "alexander hentschel", "volkmar sterzing", "michel tokic", "steffen udluft"], "accepted": false, "id": "1610.03793"}, "pdf": {"name": "1610.03793.pdf", "metadata": {"source": "CRF", "title": null, "authors": ["Daniel Hein", "Alexander Hentschel", "Volkmar Sterzing", "Michel Tokic", "Steffen Udluft"], "emails": [], "sections": [{"heading": null, "text": "ar Xiv: 161 0.03 793v 1 [cs.L G] 12 Oct 2"}, {"heading": "1 Introduction", "text": "The scientific method requires that a hypothesis be tested by experiments, in the field of machine learning, when algorithms are developed or improved. Such a test can be to run the algorithm on a real system to observe its performance, or to run it on a real system implemented as a computer program. The latter has several advantages: it is usually faster, cheaper and, of course, safer to test the algorithm on a simulation. Furthermore, simulation can be handled much more freely than a real system. Internal states can be observed, restored, and the freedom to test specific aspects with little effort."}, {"heading": "2 Industrial Benchmark", "text": "The Industrial Benchmark aims to be realistic in the sense that it encompasses a multitude of aspects that we have considered indispensable in industrial applications; it is not designed to be an approximation to a real system, but to have the same hardness and complexity; the state and action space are continuous, the state space is rather high-dimensional and only partially observable; the measures consist of three continuous components and produce three controls; there are delayed effects; the optimization task is multicriteria in the sense that there are two reward components that exhibit opposing dependencies on the measures; the dynamic behavior is heteroscedic with state-dependent observation noise and state-dependent probability distributions, based on latent variables; the dynamic behavior depends on an external driver that cannot be influenced by the measures; the industrial benchmark is designed so that the optimal policy does not approach any fixed operational point in the three."}, {"heading": "3 Detailed description", "text": "At any point in time, the reinforcement learning agent can influence the environment (industrial benchmark) through actions ~ a (t), the three-dimensional vectors in [\u2212 1,1] 3. Each action can be interpreted as three proposed changes to the three observable state variables, which are called rewards. These current control mechanisms are called velocity v, gain g and shift s. Each of these actions is limited to [0,100]. ~ a (t) = (4) rewards are called rewards (0, max (100, v + 0), (2) g (t + 1) = min (0,100, g + 10), (3) s (t) s (0, max) s (0, max), s (s + 1) s (0, max (0, s) s (4) s (4) s (4)."}, {"heading": "4 Description of the dynamical behaviour", "text": "The dynamic behaviour of the industrial benchmark is determined by the three steering speeds v, gain g and shift s, the external driver setpoint p and five latent variables. Dynamics can be broken down into three different sub-dynamics called operating costs, miscalibration and fatigue."}, {"heading": "4.1 Dynamics of operational cost", "text": "The subdynamics of the operating costs are influenced by the external driver setpoint p and two of the three steering systems, namely velocity v and profit g. The current operating costs o (t) are calculated aso (t) = exp (2p (t) + 4v (t) + 2.5 g (t) 100). (6) The current operating costs o (t) cannot be observed, the observation is calculated by a konvolutionoc (t) = 0o (t) + 0o (t \u2212 1) + 0o (t \u2212 2) + 0o (t \u2212 2) + 0o (t \u2212 3) + 0o (t \u2212 4) + 19 o (t \u2212 5) + 2 9 o (t \u2212 6) + 3 9 o (t \u2212 7) + 2 9 o (t \u2212 8) + 1 9 o (t \u2212 9) (t \u2212 7) The convoluted operating costs oc (t) cannot yet be directly observed, they are modified by the second subdynamics, which is called industrial linearity, which are subject to the dynamics, and all of these are ultimately for our observation."}, {"heading": "4.2 Dynamics of mis-calibration", "text": "The subdynamics of the miscalibration are influenced by the external driver, the setpoint p and only one steering, namely displacement s. The setpoint p and displacement s are combined into an effective displacement sese = min (1.5, max (\u2212 1.5, s / 20 \u2212 p / 50 \u2212 1.5), (8), which affects the three latent variables ml1, m l 2 and m l 3. The resulting miscalibration m is a function of the effective displacement and the latent variables m = f (se, ml1, m l 2, m l 3). (9) The resulting miscalibration m (t) is added to the confused operating costs oc (t), whereby c, c = oc (t) + 25m (t), (10) Before it can be observed as consumption c, the modified operating costs c (t) are subject to heteroscedastic observation noisec = c + gauss (0.02 + 0.02) with an isian value of 0.011 and 0.011."}, {"heading": "4.3 Dynamics of fatigue", "text": "The subdynamics of fatigue are influenced by the same variables as the sub-dynamics of operating costs, i.e. set point p, velocity v and profit g. The industrial benchmark is designed in such a way that when the control speed v > gain g changes to reduce operating costs, fatigue is increased, leading to the desired multi-criterion task, with two reward components having opposite dependencies to the actions. Basic fatigue fb is calculated (13), which is an amplification. Amplification depends on two latent variables hv and hg, an effective velocity ve, an effective gain and is influenced by noise."}, {"heading": "5 State definitions", "text": "To give an overview of possible state definitions, a small summary is given."}, {"heading": "5.1 The observation vector", "text": "This observation vector is also called an observable state, but it must be remembered that it does not meet the Markov property. Observation vector ~ O (t) at time t includes current values of velocity v (t), gain g (t), displacement s (t), setpoint p (t), consumption c (t) and fatigue f (t)."}, {"heading": "5.2 The preferred minimal markovian state", "text": "The preferred minimal Markovian state fulfills the Markov property with the minimum number of variables. It contains 20 values. These are the observation vector (velocity v (t), gain g (t), displacement s (t), setpoint p (t), consumption c (t) and fatigue f (t) plus some latent variables of partial dynamics. The partial dynamics of operating costs adds a list of previous operating costs o (t \u2212 i) with i-1, \u00b7 \u00b7, 9. Note that the current operating costs o (t) are not part of this state definition. It would be redundant as it can be calculated by v (t), gain g (t) and setpoint p. The subdynamics of the miscalibration requires 3 additional latent variables hv and hv (22 and Eq)."}, {"heading": "5.3 The extended state", "text": "The extended state, also called internal markovian state, contains all variables of the preferred minimum markovian state as well as some variables that are useful for data analysis."}, {"heading": "6 Experimental setup", "text": "To test the algorithms in an initial batch mode outside the guidelines, the data is generated by the maximum entropy policy dP (a | s) da = const (23).The benchmark is initialized for ten different setpoints p-10.20, \u00b7 \u00b7 \u00b7, 90.100 with the latent variables in their default values and the three controls of 50 each. Subsequently, for each setpoint, the maximum entropy policy is applied to the benchmark for 1000 time steps, resulting in 10,000 data points. This data can then be used to train identification models and / or strategies of the system. The goal is to develop a policy \u03c0 that maximizes the average reward on the same basis, using the \u03c0 policy instead of the maximum entropy policy."}, {"heading": "7 Interfaces", "text": "The most important interfaces are defined in com.siemens.rl.interfaces: DataVector, Environment and ExternalDriver."}, {"heading": "7.1 Interface DataVector", "text": "This interface lists all the necessary methods to implement a data vector, which could be a state or action vector.Modifier and Type Method and Description DataVector clone () Returns a copy of the data vector.List < String > getKeys () Returns a list with the names of the data vector dimensions.Double getValue (String _ key) Returns the value for a given data vector dimension.double [] getValuesArray () Returns a double [] array with the values.void setValue (String _ key, double _ value) Returns the current value of a given data vector dimension."}, {"heading": "7.2 Interface Environment", "text": "This interface describes all the relevant methods for implementing the dynamics of an environment.Modifier and Type Method and Description void addExternalDriver (ExternalDriver _ extDriver) This function adds an external driver to the environment that affects state variables during the call to step ().DataVector getInternalMarkovState () Returns the internal Markovian state.double getReward () Returns the rewarded.DataVector getState () Returns the observable state.void reset () function to reset the environment.double step (DataVector _ action) Performs an action within the environment and returns the reward."}, {"heading": "7.3 Interface ExternalDriver", "text": "Abstract interface for connecting external drivers to the environment that affect certain state variables / filters (e.g. setpoint).Modifier and Type Method and Description void filter (DataVector _ state) Applies the external drivers \"in place\" to the given data vector.DataVector getState () Returns the current configuration.void setConfiguration () Sets the external driver configuration within the given data vector.void setSeed (long _ seed) Sets the random seed."}, {"heading": "8 Example usage", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "8.1 Class ExampleMain", "text": "An example is implemented in com.siemens.industrialsim.ExampleMain.Modifier and Type Method and Description static void main (String [] _ args). Run an example simulation with random actions."}, {"heading": "9 First results", "text": "Initial results with 10,000 data vectors, as described in Section 6, indicate that the reward (called RewardTotal) can be estimated on the basis of current and past values of velocity, gain, displacement and setpoint through a relapsing neural network with a mean relative absolute deviation (MRABD) of about 10%. Consumption c can be estimated with an MRABD of about 3.6% and fatigue f with an MRABD of about 24% (for f > 1).The average reward of the maximum entropy policy in the environment described in Section 6 is -290.8 \u00b1 0.6 with a standard deviation of 20. Initial results with the policy gradient Neural Reward Regression (PGNRR) [4] and with an extension to other measures of neurally adjusted Q-iteration (NFQ) [5] lead to average rewards of about -270."}], "references": [{"title": "Reinforcement Learning: An Introduction", "author": ["Richard S. Sutton", "Andrew G. Barto"], "venue": null, "citeRegEx": "1", "shortCiteRegEx": "1", "year": 1998}, {"title": "Is learning the n-th thing any easier than learning the first", "author": ["S. Thrun"], "venue": "Advances in Neural Information Processing Systems (NIPS)", "citeRegEx": "2", "shortCiteRegEx": "2", "year": 1996}, {"title": "A survey on transfer learning", "author": ["S.J. Pan", "Q. Yang"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 2010}, {"title": "Improving optimality of neural rewards regression for data-efficient batch near-optimal policy identification", "author": ["Daniel Schneega\u00df", "Steffen Udluft", "Thomas Martinetz"], "venue": "Artificial Neural Networks \u2013 ICANN 2007: 17th International Conference,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 2007}, {"title": "Neural fitted Q-iteration - first experiences with a data efficient neural reinforcement learning method", "author": ["Martin Riedmiller"], "venue": "In Proceedings of the 16th European Conference on Machine Learning,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2005}], "referenceMentions": [{"referenceID": 0, "context": "This task will be described in the theoretical framework of reinforcement learning [1].", "startOffset": 83, "endOffset": 86}, {"referenceID": 1, "context": "Learning to act optimal for any set point in the Constant Set Point Setting can also be seen as a multi-task or transfer learning task [2, 3].", "startOffset": 135, "endOffset": 141}, {"referenceID": 2, "context": "Learning to act optimal for any set point in the Constant Set Point Setting can also be seen as a multi-task or transfer learning task [2, 3].", "startOffset": 135, "endOffset": 141}, {"referenceID": 0, "context": "05, \u03b7 v b and \u03b7 b are drawn from binominial distributions Binom(1, v ) and Binom(1, g), respectively, \u03b7 u and \u03b7 u are drawn from an uniform distribution in [0,1].", "startOffset": 156, "endOffset": 161}, {"referenceID": 3, "context": "First results with the policy gradient neural rewards regression (PGNRR) [4] and with an extension to continues actions of the neural fitted Q-iteration (NFQ) [5] lead to average rewards of roughly -270.", "startOffset": 73, "endOffset": 76}, {"referenceID": 4, "context": "First results with the policy gradient neural rewards regression (PGNRR) [4] and with an extension to continues actions of the neural fitted Q-iteration (NFQ) [5] lead to average rewards of roughly -270.", "startOffset": 159, "endOffset": 162}], "year": 2016, "abstractText": "A novel reinforcement learning benchmark, called Industrial Benchmark, is introduced. The Industrial Benchmark aims at being be realistic in the sense, that it includes a variety of aspects that we found to be vital in industrial applications. It is not designed to be an approximation of any real system, but to pose the same hardness and complexity.", "creator": "LaTeX with hyperref package"}}}