{"id": "1610.03807", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "12-Oct-2016", "title": "Question Generation from a Knowledge Base with Web Exploration", "abstract": "Question generation has been a research topic for a long time, where a big challenge is how to generate deep and natural questions. To tackle this challenge, we propose a system to generate natural language questions from a domain-specific knowledge base (KB) by utilizing rich web information. A small number of question templates are first created based on the KB and instantiated into questions, which are used as seed set and further expanded through the web to get more question candidates. A filtering model is then applied to select candidates with high grammaticality and domain relevance. The system is able to generate large amount of in-domain natural language questions with considerable semantic diversity and is easily applicable to other domains. We evaluate the quality of the generated questions by human judgments and the results show the effectiveness of our proposed system.", "histories": [["v1", "Wed, 12 Oct 2016 17:55:56 GMT  (51kb,D)", "http://arxiv.org/abs/1610.03807v1", null], ["v2", "Wed, 1 Feb 2017 23:31:47 GMT  (96kb,D)", "http://arxiv.org/abs/1610.03807v2", null]], "reviews": [], "SUBJECTS": "cs.CL", "authors": ["linfeng song", "lin zhao"], "accepted": false, "id": "1610.03807"}, "pdf": {"name": "1610.03807.pdf", "metadata": {"source": "CRF", "title": "Domain-specific Question Generation from a Knowledge Base", "authors": ["Linfeng Song", "Lin Zhao"], "emails": [], "sections": [{"heading": "1 Introduction", "text": "This year, it is only a matter of time before there is a result in which there is a result."}, {"heading": "2 Related Work", "text": "In recent years, it has been shown that the problem is not only a problem, but also a problem that needs to be solved. (...) In recent years, it has been shown that it is not only a problem, but also a problem. (...) In recent years, it has been shown that the problem cannot be solved. (...) In recent years, it has been shown that it is a problem. (...) It is not the case that it can be solved. (...) It is not the case that it can be solved. (...) In recent years, it has been shown that the problem can be solved. (...) In the last ten years, it has not been solved. (...) In the last ten years, it has been shown that it is a problem. (...) (...)... (...)... (...)... (...) \"(...)... (...)\" (...) \"(...)\" (...) In the last ten years, it has not been solved. \"(...) (...\" (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...) (...)) (...) (...)) (...)) (...)) (...) (...)) (...)) (...)) (...)) (...)) (...) (...) (...) (...) (...)) (...)...))) (...) (...) (...) (...) (...)...)...)...) (...) (...) (...) (...)) (...) (...)...)...)...)...) (...) (...) (...)...) (...)...) (...) (...) (...) (...) (...) (...)...)...)...)...) (...) (...) (...) (...) (...) (...)) (...) (...)...)...)...) (...)...)...)...)...)...)...) (...) (...)...)...)...)... (... (... (...)...)...)...)...)...)...)...)"}, {"heading": "3 Domain-specific Knowledge Base", "text": "In fact, most of them are able to play by the rules that they have imposed on themselves, and they are able to play by the rules that they have imposed on themselves."}, {"heading": "4 Framework", "text": "The framework shown in Figure 2 contains 4 subprocesses: KB progressively, question template creation, seed question creation, and question extension. If you consider a KB as a list of triples, KB processing removes useless triples for creating questions. Generally, we remove two types of triples: One type of triples contains abstract subjects or objects such as \"(tool, performsActivity, Activity),\" where both \"tool\" and \"activity\" are abstract, and the other type of triples is used to define relationships. For example, \"(bearingDiameter, type, FunctionalProperty)\" defines \"bearingDiameter\" as a kind of \"FunctionalProperty.\" We remove this type of triples because it does not generate natural questions. We manually construct question templates for each predicate. For example, \"Can I # Y # X with #?\" we will use the Ahttps 3.1 to define the question."}, {"heading": "4.1 KB Processing", "text": "Generally, we remove two types of triples: One type contains abstract units (subjects or objects). For example, \"tool\" and \"activity\" are abstract units, but \"saw\" and \"cut\" are not. It is easy for humans to define, but difficult for computers to quantify. To quantify whether a given unit is abstract, we look at the graph with \"rdfs: subClassOf\" edges. We define the depth of units that do not have an outgoing \"rdfs: subClassOf\" edge. Otherwise, the depth corresponds to the depth of the \"flattest parent\" plus 1. Figure 1 shows the depth of \"Saw\" 2, because the depth of the flattest parent is \"Tool\" 1. We consider one unit as abstract if its depth is less than n. The other type contains predicates that define such a \"question framework\":"}, {"heading": "4.2 Question Template Construction", "text": "Generating fluid and relevant questions from a KB is always a difficult problem, as the only input is a triple, while the output is a fluid one, and2https: / / www.google.com / 3https: / / www.w3.org / TR / rdf-schema / relevant question. Recently, Serban et al. (2016) introduced a method to generate 30M factoid questions from a KB. However, they use 100K humancrafted (triple, question) pairs to train their system, and the BLEU score (Papineni et al., 2002) of generated questions is only about 35. Here, we use a template-based method (in Section 4.3), where for each predicate identified in the previous step, a few question templates are created manually. In Table 1, a template is created for predicate \"performsActivity\" that \"can perform\" activities # X #, where # X # the predicate # is limited to the # Y # Subject is # Significant."}, {"heading": "4.3 Seed Question Generation", "text": "To create seed questions, we use a two-step method in which the constructed templates and the triples are used as input. In the first step, a seed question set is generated for each template by filling the templates with values from associated triples. For example, for each template \"Can I run # Y # with # X #?\" and \"Triple\" (jigsaw, performance activity, curve section), we get the question \"Can I perform curve cut with the jigsaw?\" This process is repeated for each template so that all variables are replaced with values. In the second step, we replace a subject or object with a sibling class of the original unit to get a new question. A sibling class has the same parent class with the original class in the domain-specific KB. For example, for the question \"Can I perform curve cut with a jigsaw?\" we replace \"Jigsaw\" with \"Jigsaw\" to get a new question?"}, {"heading": "4.4 Question expansion", "text": "These advanced questions can be sent back to the search engine to retrieve more queries, and this process is carried out iteratively until we receive enough queries; the expansion method has the advantage that the advanced questions represent real user information; the advanced questions may not be fluid or domain relevant, especially as the iteration continues, the domain relevance of newly advanced questions decreases significantly; previous methods either ignore this problem (Serban et al., 2016), or allow human annotation data to learn a classifier (Labutov et al., 2015); our method does not rely on human efforts by using word embedding and language modeling; using the seed question (Section 4.3) as the word contained in the domain; we then filter questions that are either relevant."}, {"heading": "5 Experiments", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "5.1 Setup", "text": "It contains 67 different predicates, 293 different subjects and 279 different objects. For the 67 predicates, we have 163 templates, some examples are shown in Table 1. We first selected 1000 questions from the extended question for development and test sets, which are all used to generate seed questions. After the question, we build up dev and test sets to optimize and test fluctuation and domain relevance. A case is only positive if its score is greater than 2.In addition to the main experiment, we evaluate our question for development and test sets 3 linguistic specialists to name the fluctuation and domain relevance."}, {"heading": "5.2 Results and Analysis", "text": "In the main experiment, we generated 12,228 seed questions, from which 20,000 more questions were augmented with the Google search7. Table 2 presents some extended questions, from which we can see that most of the questions are grammatically and relevant to the area of power tools. In addition, most questions are informative and correspond to a specific answer, except for the one \"Do I need a hammer drill\" that lacks context. Finally, in addition to simple factoid questions, our system generates many complex questions such as \"How to cut a groove into wood without a milling cutter.\" We have tried different values of tf and tr on the Devels and show the curves between precision and retrieval in Figure 3. The currency and test results with matched parameters are explained in Table 3. For relevance, enrichment with full snippet is significantly better than the others, and enrichment with related queries is significantly better than just using questions. This explains the importance of data saving for the calculation."}, {"heading": "5.3 Domain Relevance", "text": "As shown in Table 4, we compare our question relevance evaluation method with previous prior art methods: Phan et al. (2008) first derives latent topics with LDA (Lead et al., 2003) from a series of texts from Wikipedia and then uses the topics as attachments to extend the short text. Chen et al. (2011) extend Phan et al. (2008) further by using multi-granularity topics. Ma et al. (2015) adopts a Bayesian model according to which the probability of a document D belonging to a topic is equal to the previous t, with the probability of each word w in D comes7https: / / www.google.com / from t. Our method (question) first calculates the document embedded in the training set for each test document and each domain, then assigns test documents to the nearest (cosmic similarities) domains."}, {"heading": "5.4 Comparison on 30M Factoid Question Answer Corpus", "text": "Serban et al. (2016) publishes a corpus of 30 million (triple, question) pairs, from which we randomly select 500 triple questions to generate our questions. We first create 53 templates, then generate 991 seed questions from the triple questions, and finally receive an extended set of 1,529 questions from the Google search. We skip the KB processing step, since our entries here are already triple questions. From the extended catalog, we select 500 based on the averaged language model, since it is a cross-domain corpus. In Table 5, we compare our questions with Serban et al. (2016), that questions in the same line describe the same entity. We see that our questions are grammatical, natural, since these questions are what people normally ask on the Internet. On the other hand, questions from Serban et al. (2016) are either ungrammatic (like \"Who was someone involved in leukemia?\" and \"What is the title of a book?\" Who is the theme of a book? \"Who is the title of leukemia?\""}, {"heading": "6 Conclusion", "text": "We presented an approach to generating natural language questions from knowledge graphs. By using rich web information, the system is able to generate relevant questions on a large scale and the human effort can be significantly reduced. We evaluated our approach in terms of grammar and relevance of the questions generated and the results showed its effectiveness. Note that while the work presented in this paper is for a specific area, the workflow is a general framework that could potentially be applied to other areas or CBs."}], "references": [{"title": "Automatic question generation using discourse cues", "author": ["Rakshit Shah", "Prashanth Mannem"], "venue": "In Proceedings of the 6th Workshop on Innovative Use of NLP for Building Educational Applications,", "citeRegEx": "Agarwal et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Agarwal et al\\.", "year": 2011}, {"title": "Automation of question generation from sentences", "author": ["Ali et al.2010] Husam Ali", "Yllias Chali", "Sadid A Hasan"], "venue": "In Proceedings of QG2010: The Third Workshop on Question Generation,", "citeRegEx": "Ali et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Ali et al\\.", "year": 2010}, {"title": "Dbpedia: A nucleus for a web of open data", "author": ["Auer et al.2007] S\u00f6ren Auer", "Christian Bizer", "Georgi Kobilarov", "Jens Lehmann", "Richard Cyganiak", "Zachary Ives"], "venue": "In The semantic web,", "citeRegEx": "Auer et al\\.,? \\Q2007\\E", "shortCiteRegEx": "Auer et al\\.", "year": 2007}, {"title": "Latent dirichlet allocation", "author": ["Blei et al.2003] David M Blei", "Andrew Y Ng", "Michael I Jordan"], "venue": "Journal of machine Learning research,", "citeRegEx": "Blei et al\\.,? \\Q2003\\E", "shortCiteRegEx": "Blei et al\\.", "year": 2003}, {"title": "Freebase: a collaboratively created graph database for structuring human knowledge", "author": ["Colin Evans", "Praveen Paritosh", "Tim Sturge", "Jamie Taylor"], "venue": "In Proceedings of the 2008 ACM SIGMOD,", "citeRegEx": "Bollacker et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Bollacker et al\\.", "year": 2008}, {"title": "Generating questions automatically from informational text", "author": ["Chen et al.2009] W Chen", "G Aist", "J Mostow"], "venue": "In Proceedings of the 2nd Workshop on Question Generation,", "citeRegEx": "Chen et al\\.,? \\Q2009\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2009}, {"title": "Short text classification improved by learning multi-granularity topics", "author": ["Chen et al.2011] Mengen Chen", "Xiaoming Jin", "Dou Shen"], "venue": "In IJCAI,", "citeRegEx": "Chen et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Chen et al\\.", "year": 2011}, {"title": "Question generation based on lexicosyntactic patterns learned from the web", "author": ["Curto et al.2012] S\u00e9rgio Curto", "A Mendes", "Luisa Coheur"], "venue": "Dialogue & Discourse,", "citeRegEx": "Curto et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Curto et al\\.", "year": 2012}, {"title": "Good question! statistical ranking for question generation", "author": ["Heilman", "Smith2010] Michael Heilman", "Noah A Smith"], "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association", "citeRegEx": "Heilman et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Heilman et al\\.", "year": 2010}, {"title": "Deep questions without deep understanding", "author": ["Labutov et al.2015] Igor Labutov", "Sumit Basu", "Lucy Vanderwende"], "venue": "In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics", "citeRegEx": "Labutov et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Labutov et al\\.", "year": 2015}, {"title": "Generating natural language questions to support learning on-line", "author": ["Fred Popowich", "John Nesbit", "Phil Winne"], "venue": null, "citeRegEx": "Lindberg et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Lindberg et al\\.", "year": 2013}, {"title": "An empirical comparison between n-gram and syntactic language models for word ordering", "author": ["Liu", "Zhang2015] Jiangming Liu", "Yue Zhang"], "venue": "In Conference on Empirical Methods in Natural Language Processing", "citeRegEx": "Liu et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Liu et al\\.", "year": 2015}, {"title": "Distributional representations of words for short text classification", "author": ["Ma et al.2015] Chenglong Ma", "Weiqun Xu", "Peijia Li", "Yonghong Yan"], "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing,", "citeRegEx": "Ma et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Ma et al\\.", "year": 2015}, {"title": "Linguistic considerations in automatic question generation", "author": ["Mazidi", "Nielsen2014] Karen Mazidi", "Rodney D Nielsen"], "venue": "In Proceedings of ACL,", "citeRegEx": "Mazidi et al\\.,? \\Q2014\\E", "shortCiteRegEx": "Mazidi et al\\.", "year": 2014}, {"title": "Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems", "author": ["Mikolov", "Dean2013] T Mikolov", "J Dean"], "venue": null, "citeRegEx": "Mikolov et al\\.,? \\Q2013\\E", "shortCiteRegEx": "Mikolov et al\\.", "year": 2013}, {"title": "Question generation from concept maps", "author": ["Olney et al.2012] Andrew M Olney", "Arthur C Graesser", "Natalie K Person"], "venue": "Dialogue and Discourse,", "citeRegEx": "Olney et al\\.,? \\Q2012\\E", "shortCiteRegEx": "Olney et al\\.", "year": 2012}, {"title": "Bleu: a method for automatic evaluation of machine translation", "author": ["Salim Roukos", "Todd Ward", "Wei-Jing Zhu"], "venue": "In Proceedings of the 40th Annual Conference of the Association for Computational Linguis-", "citeRegEx": "Papineni et al\\.,? \\Q2002\\E", "shortCiteRegEx": "Papineni et al\\.", "year": 2002}, {"title": "Learning to classify short and sparse text & web with hidden topics from large-scale data collections", "author": ["Phan et al.2008] Xuan-Hieu Phan", "Le-Minh Nguyen", "Susumu Horiguchi"], "venue": "In Proceedings of the 17th international conference on World Wide Web,", "citeRegEx": "Phan et al\\.,? \\Q2008\\E", "shortCiteRegEx": "Phan et al\\.", "year": 2008}, {"title": "Overview of the first question generation shared task evaluation challenge", "author": ["Rus et al.2010] Vasile Rus", "Brendan Wyse", "Paul Piwek", "Mihai Lintean", "Svetlana Stoyanchev", "Cristian Moldovan"], "venue": "In Proceedings of the Third Workshop on Question Gener-", "citeRegEx": "Rus et al\\.,? \\Q2010\\E", "shortCiteRegEx": "Rus et al\\.", "year": 2010}, {"title": "Generating factoid questions with recurrent neural networks: The 30m factoid question-answer corpus", "author": ["Alberto Garc\u0131\u0301aDur\u00e1n", "Caglar Gulcehre", "Sungjin Ahn", "Sarath Chandar", "Aaron Courville", "Yoshua Bengio"], "venue": null, "citeRegEx": "Serban et al\\.,? \\Q2016\\E", "shortCiteRegEx": "Serban et al\\.", "year": 2016}, {"title": "Generating quiz questions from knowledge graphs", "author": ["Mohamed Yahya", "Klaus Berberich"], "venue": "In Proceedings of the 24th International Conference on World Wide Web,", "citeRegEx": "Seyler et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Seyler et al\\.", "year": 2015}, {"title": "Automatic question generation from text-an aid to independent study", "author": ["John H Wolfe"], "venue": "ACM SIGCSE Bulletin,", "citeRegEx": "Wolfe.,? \\Q1976\\E", "shortCiteRegEx": "Wolfe.", "year": 1976}], "referenceMentions": [{"referenceID": 7, "context": "Most work is focusing on generating questions from a text (Curto et al., 2012; Olney et al., 2012; Mazidi and Nielsen, 2014; Labutov et al., 2015).", "startOffset": 58, "endOffset": 146}, {"referenceID": 15, "context": "Most work is focusing on generating questions from a text (Curto et al., 2012; Olney et al., 2012; Mazidi and Nielsen, 2014; Labutov et al., 2015).", "startOffset": 58, "endOffset": 146}, {"referenceID": 9, "context": "Most work is focusing on generating questions from a text (Curto et al., 2012; Olney et al., 2012; Mazidi and Nielsen, 2014; Labutov et al., 2015).", "startOffset": 58, "endOffset": 146}, {"referenceID": 20, "context": "With the rising of structured knowledge base (KB), some work starts to utilize KB to generate questions (Seyler et al., 2015; Serban et al., 2016).", "startOffset": 104, "endOffset": 146}, {"referenceID": 19, "context": "With the rising of structured knowledge base (KB), some work starts to utilize KB to generate questions (Seyler et al., 2015; Serban et al., 2016).", "startOffset": 104, "endOffset": 146}, {"referenceID": 4, "context": "KB can contain knowledge in either open domain such as Freebase (Bollacker et al., 2008) and DBpedia (Auer et al.", "startOffset": 64, "endOffset": 88}, {"referenceID": 2, "context": ", 2008) and DBpedia (Auer et al., 2007) or closed domain such as those available in many industrial domains (e.", "startOffset": 20, "endOffset": 39}, {"referenceID": 18, "context": "Automatic question generation has received increasing interest from the Natural Language Generation (NLG) community, been further advanced by the Question Generation Shared Task and Evaluation Challenge (QGSTEC) (Rus et al., 2010), which created a common corpus for empirical evaluation of question generation.", "startOffset": 212, "endOffset": 230}, {"referenceID": 21, "context": "Among the work, some utilizes syntactic parsing to identify the targets of questions and construct syntactic transformation rules to generate questions (Wolfe, 1976; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012).", "startOffset": 152, "endOffset": 228}, {"referenceID": 1, "context": "Among the work, some utilizes syntactic parsing to identify the targets of questions and construct syntactic transformation rules to generate questions (Wolfe, 1976; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012).", "startOffset": 152, "endOffset": 228}, {"referenceID": 7, "context": "Among the work, some utilizes syntactic parsing to identify the targets of questions and construct syntactic transformation rules to generate questions (Wolfe, 1976; Ali et al., 2010; Heilman and Smith, 2010; Curto et al., 2012).", "startOffset": 152, "endOffset": 228}, {"referenceID": 5, "context": "Some utilizes semantic information (Chen et al., 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014) where semantic role labeling is applied to identify patterns in the source sentences for question generation.", "startOffset": 35, "endOffset": 103}, {"referenceID": 10, "context": "Some utilizes semantic information (Chen et al., 2009; Lindberg et al., 2013; Mazidi and Nielsen, 2014) where semantic role labeling is applied to identify patterns in the source sentences for question generation.", "startOffset": 35, "endOffset": 103}, {"referenceID": 0, "context": "Agarwal et al. (2011) uses discourse connectives to generate questions from selected text segments for different question types.", "startOffset": 0, "endOffset": 22}, {"referenceID": 0, "context": "Agarwal et al. (2011) uses discourse connectives to generate questions from selected text segments for different question types. Olney et al. (2012) first converts the text into concept maps from which questions are generated.", "startOffset": 0, "endOffset": 149}, {"referenceID": 0, "context": "Agarwal et al. (2011) uses discourse connectives to generate questions from selected text segments for different question types. Olney et al. (2012) first converts the text into concept maps from which questions are generated. Labutov et al. (2015) generate deep open-ended questions from Wikipedia text, where they use crowd-sourcing to construct question templates based on the category of the Wikipedia page, and then apply question ranking to select the final question.", "startOffset": 0, "endOffset": 249}, {"referenceID": 19, "context": "Seyler et al. (2015) generates quiz questions from knowledge graphs, where for each target entity, a SPARQL query is generated as an intermediate representation and turned into a natural language question by a simple predefined template.", "startOffset": 0, "endOffset": 21}, {"referenceID": 19, "context": "Serban et al. (2016) has constructed a corpus of 30M factoid question and an-", "startOffset": 0, "endOffset": 21}, {"referenceID": 4, "context": "Large-scale KBs such as Freebase (Bollacker et al., 2008) and DBpedia (Auer et al.", "startOffset": 33, "endOffset": 57}, {"referenceID": 2, "context": ", 2008) and DBpedia (Auer et al., 2007) are very popular and widely used in many NLP applications.", "startOffset": 20, "endOffset": 39}, {"referenceID": 16, "context": "However, they use 100K humancrafted (triple, question) pairs to train their system, and the BLEU score (Papineni et al., 2002) of generated questions is only around 35.", "startOffset": 103, "endOffset": 126}, {"referenceID": 18, "context": "Recently Serban et al. (2016) introduces a method to generate 30M factoid questions from a KB.", "startOffset": 9, "endOffset": 30}, {"referenceID": 19, "context": "The human effort is significantly lower than Serban et al. (2016).", "startOffset": 45, "endOffset": 66}, {"referenceID": 19, "context": "Previous methods either ignore this problem (Serban et al., 2016), or let human annotate training data to learn a classifier (Labutov et al.", "startOffset": 44, "endOffset": 65}, {"referenceID": 9, "context": ", 2016), or let human annotate training data to learn a classifier (Labutov et al., 2015).", "startOffset": 67, "endOffset": 89}, {"referenceID": 19, "context": "In addition to the main experiment, we also evaluate our framework on the 30M Factoid Question-Answer Corpus4, released by Serban et al. (2016). Each line in the dataset contains a triple (subject, predicate, object) and a question generated by their method.", "startOffset": 123, "endOffset": 144}, {"referenceID": 17, "context": "There has been plenty of results (Phan et al., 2008; Chen et al., 2011; Ma et al., 2015) on the dataset.", "startOffset": 33, "endOffset": 88}, {"referenceID": 6, "context": "There has been plenty of results (Phan et al., 2008; Chen et al., 2011; Ma et al., 2015) on the dataset.", "startOffset": 33, "endOffset": 88}, {"referenceID": 12, "context": "There has been plenty of results (Phan et al., 2008; Chen et al., 2011; Ma et al., 2015) on the dataset.", "startOffset": 33, "endOffset": 88}, {"referenceID": 5, "context": "18 Chen et al. (2011) 85.", "startOffset": 3, "endOffset": 22}, {"referenceID": 5, "context": "18 Chen et al. (2011) 85.31 Ma et al. (2015) 85.", "startOffset": 3, "endOffset": 45}, {"referenceID": 3, "context": "(2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text.", "startOffset": 44, "endOffset": 63}, {"referenceID": 13, "context": "Shown in Table 4, We compare our question relevance evaluation method with previous state of the art methods: Phan et al. (2008) first derives latent topics with LDA (Blei et al.", "startOffset": 110, "endOffset": 129}, {"referenceID": 3, "context": "(2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text. Chen et al. (2011) further extend Phan et al.", "startOffset": 45, "endOffset": 187}, {"referenceID": 3, "context": "(2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text. Chen et al. (2011) further extend Phan et al. (2008) by using multi-granularity topics.", "startOffset": 45, "endOffset": 221}, {"referenceID": 3, "context": "(2008) first derives latent topics with LDA (Blei et al., 2003) from a set of texts from Wikipedia, then uses the topics as appended features to expand the short text. Chen et al. (2011) further extend Phan et al. (2008) by using multi-granularity topics. Ma et al. (2015) adopts a Bayesian model that the probability a document D belongs to a topic t equals to the prior of t times the probability each word w in D comes", "startOffset": 45, "endOffset": 273}, {"referenceID": 19, "context": "Ours Serban et al. (2016)", "startOffset": 5, "endOffset": 26}], "year": 2016, "abstractText": "Question generation has been a research topic for a long time, where a big challenge is how to generate deep and natural questions. To tackle this challenge, we propose a system to generate natural language questions from a domain-specific knowledge base (KB) by utilizing rich web information. A small number of question templates are first created based on the KB and instantiated into questions, which are used as seed set and further expanded through the web to get more question candidates. A filtering model is then applied to select candidates with high grammaticality and domain relevance. The system is able to generate large amount of in-domain natural language questions with considerable semantic diversity and is easily applicable to other domains. We evaluate the quality of the generated questions by human judgments and the results show the effectiveness of our proposed system.", "creator": "LaTeX with hyperref package"}}}