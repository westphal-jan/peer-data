{"id": "1610.03996", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Bank Card Usage Prediction Exploiting Geolocation Information", "abstract": "We describe the solution of team ISMLL for the ECML-PKDD 2016 Discovery Challenge on Bank Card Usage for both tasks. Our solution is based on three pillars. Gradient boosted decision trees as a strong regression and classification model, an intensive search for good hyperparameter configurations and strong features that exploit geolocation information. This approach achieved the best performance on the public leaderboard for the first task and a decent fourth position for the second task.", "histories": [["v1", "Thu, 13 Oct 2016 09:44:03 GMT  (48kb,D)", "http://arxiv.org/abs/1610.03996v1", "Describes the winning solution for the ECML-PKDD 2016 Discovery Challenge on Bank Card Usage Analysis. Final results on the private leaderboard are available here:this https URL"]], "COMMENTS": "Describes the winning solution for the ECML-PKDD 2016 Discovery Challenge on Bank Card Usage Analysis. Final results on the private leaderboard are available here:this https URL", "reviews": [], "SUBJECTS": "cs.LG cs.AI", "authors": ["martin wistuba", "nghia duong-trung", "nicolas schilling", "lars schmidt-thieme"], "accepted": false, "id": "1610.03996"}, "pdf": {"name": "1610.03996.pdf", "metadata": {"source": "CRF", "title": "Bank Card Usage Prediction Exploiting Geolocation Information", "authors": ["Martin Wistuba", "Nghia Duong-Trung", "Nicolas Schilling"], "emails": ["wistuba@ismll.uni-hildesheim.de", "duongn@ismll.uni-hildesheim.de", "schilling@ismll.uni-hildesheim.de", "schmidt-thieme@ismll.uni-hildesheim.de", "cosine@1", "cosine@5", "cosine@k"], "sections": [{"heading": "1 Challenge Description", "text": "The objective of one of this year's ECML-PKDD Discovery Challenges was to predict the behaviour of the customers of the Hungarian otpbank. The challenge was divided into two tasks: the first task was to predict the number of visits for each bank branch for a group of customers, the second task was to predict whether a customer will apply for a credit card in the next six months. For these tasks, anonymised customer information (e.g. age, location, income, gender) and banking activities (e.g. what was purchased, where and when) were provided. A labelled data set was provided for 2014 which can be used for supervised machine learning \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c c cosically, c c c c c c c c c cosmic, c c @ 1 with a cosmic and 5 clients = c c c c @ c c c @ 1 = c c c = c c c c = 1, c c = c = 1, c = 1, c = c \u00b7 i."}, {"heading": "2 Problem Identification", "text": "For the first task, we assumed that there was no correlation between the number of visits a customer made between branches, which enabled us to solve b different regression tasks for each of the b branches. Separately, we trained a regression model for each branch that predicts for a customer how often she will visit that branch based on past information for that branch. This is a classic example of counting data, so we approached this task as a Poisson regression problem. For task 1, we had to select five bank branches for which we wanted to make predictions. We simply chose the five with the highest predicted number of visits, which is the best way to get a good score if the predictor reasonably performs. We considered task 2 as a classification task. We minimized the logistical loss and looked at the class imbalance by using an appropriate class weight.For both tasks, we used felled decision trees [2] as a pre-seasonal model."}, {"heading": "3 Data Preprocessing", "text": "For the feature and hyperparameter selection, we had to split the described data into a training data set Dtrain and a validation data set Dvalid, so that the performance on Dvalid reflected the performance on the hidden test data. The task was to derive from some customers and their activities in 2014 the behaviour of a group of customers in 2015. Only basic customer information and customer activities in the first half of 2015 (excluding store visits) were given to the test customers. So we decided to share the pre-marked data, with 80% uniformed for Dtrain and the remaining 20% for Dvalid. Only the first six months of customer validation activities (excluding store visits) were provided for validation. The only problem is that we actually made predictions from customer data in 2014 for customers, but there was no way to overcome this problem. Very basic information about customers were available, including age, gender and income."}, {"heading": "4 Hyperparameter Tuning and Ensembling", "text": "For both tasks, we optimized the hyperparameters by considering the choice of hyperparameters \u03bb as the black box optimization problem margin \u03bbL (y) (y) (2), where y) is the model that was trained on the training portion of the data Dtrain using the hyperparameter configuration \u03bb and y (Dvalid) of the corresponding predictions for the validation partitionDvalid. Subsequently, the problem of hyperparameter optimization is to find a hyperparameter configuration \u03bb, so that a loss function L is minimized taking into account the predictions and the basic truth. We have addressed this black box optimization problem by means of sequential model-based optimization (SMBO) [3]. Figure 4 shows the progress of the optimization process, which was performed in parallel to 100 cores for our own pull / validation split as well as results on the public order list for task 2."}, {"heading": "5 Conclusions", "text": "Our solution is based on strong ensemble methods, intelligent feature engineering and an intensive search for optimal hyperparameter configurations. In task 1, this led to the best solution in terms of the public leaderboard and a decent result in task 2. Recognition. The authors gratefully acknowledge the co-financing of their work by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under SCHM 2583 / 6-1."}], "references": [{"title": "Xgboost: A scalable tree boosting system", "author": ["T. Chen", "C. Guestrin"], "venue": "The 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, San Francisco, CA, USA - August 13 - 17, 2016", "citeRegEx": "2", "shortCiteRegEx": null, "year": 2016}, {"title": "Practical bayesian optimization of machine learning algorithms", "author": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "venue": "Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States. pp. 2960\u2013 2968", "citeRegEx": "3", "shortCiteRegEx": null, "year": 2012}], "referenceMentions": [{"referenceID": 0, "context": "For both tasks, we used gradient boosted decision trees [2] as the prediction model.", "startOffset": 56, "endOffset": 59}, {"referenceID": 1, "context": "We tackled this black-box optimization problem using Sequential Modelbased Optimization (SMBO) [3].", "startOffset": 95, "endOffset": 98}], "year": 2016, "abstractText": "We describe the solution of team ISMLL for the ECMLPKDD 2016 Discovery Challenge on Bank Card Usage for both tasks. Our solution is based on three pillars. Gradient boosted decision trees as a strong regression and classification model, an intensive search for good hyperparameter configurations and strong features that exploit geolocation information. This approach achieved the best performance on the public leaderboard for the first task and a decent fourth position for the second task. 1 Challenge Description The goal of one of this year\u2019s ECML-PKDD Discovery Challenges was to predict the behaviour of customers of the Hungarian bank otpbank. The challenge was divided into two tasks. The first task was to predict for every bank branch the number of visits for a set of customers, the second task was to predict, whether a customer will apply for a credit card in the next six months. For these tasks, anonymized customer information (e.g. age, location, income, gender) and bank activities (e.g. what has been bought, where and when) were provided. A labeled data set for 2014 was made available which can be used for supervised machine learning to predict the targets for a disjoint set of customers for 2015. The evaluation measure for Task 2 is the area under the ROC curve (AUC), a very common measure for imbalanced classification problems. The evaluation measure for Task 1 is a little bit more exotic. It is the average of cosine@1 and cosine@5 for every customer c where cosine@k := \u2211k i=1 yc,i\u0177c,i \u221a\u2211b i=1 y 2 c,i \u221a\u2211k i=1 \u0177 2 c,i (1) with yc,i being the number of times the customer c has visited bank branch i and \u0177c,i the prediction, respectively. There are b different branches in total. For more information we refer to the challenge website [1]. ar X iv :1 61 0. 03 99 6v 1 [ cs .L G ] 1 3 O ct 2 01 6 2 Problem Identification For the first task, we assumed that there is no relation between the number of visits of a customer among branches. This enabled us to tackle b different regression tasks for each of the b branches. Independently, we trained a regression model for each branch that predicts for a customer how often she will visit that branch based on past information for that branch. This is a classical example for count data and hence, we tackled this task as a Poisson regression problem. For Task 1 we had to select five bank branches for which we wanted to make predictions. We simply chose the five with highest predicted number of visits which is the best way to achieve a good score in case the predictor performs reasonable. We considered Task 2 to be a classification task. We minimized the logistic loss and considered the class imbalance by choosing an appropriate class weight. For both tasks, we used gradient boosted decision trees [2] as the prediction model. 3 Data Preprocessing For the feature and hyperparameter selection we had to split the labeled data set into a training data set Dtrain and a validation data set Dvalid such that the performance on Dvalid will reflect the performance on the hidden test data. The task was to infer from some customers and their activities in 2014 the behaviour of a disjoint set of customers in 2015. Only basic customer information as well as the customer\u2019s activities of the first half of 2015 (excluding branch visits) was given for the test customers. Thus, we decided to split the given labeled data set by customers, selecting 80% for Dtrain and the remaining 20% for Dvalid uniformly at random. Only the first six months of activities of the validation customers (excluding branch visits) was provided for validation purposes. The only problem here is that we are actually predicting from data from 2014 for customers in 2014 but there was no way to overcome this problem. Very basic information of the customers was available including age, location, income and gender. While gender is by nature binary, the other features were already binned into three categories. We employed this information as features after transforming them via one-hot encoding. Furthermore, the internal classification of a bank whether the customer is considered as wealthy or not was given for each month. We distinguished customers of following five categories: customers that have been classified as 1) wealthy in all observed months, 2) not wealthy in all observed months, 3) first wealthy and then changed to not wealthy, 4) first not wealthy and then changed to wealthy, 5) those who changed their classification more than once. Applying one-hot encoding, we added this information as features. Finally, the information in what month the customer possesses a credit card of the bank was provided. Analogously to the five categories of the wealthy classification, we created categories for the credit card time-series information. Besides using basic customer features, we wanted to use the information of the customer\u2019s activities. While we found many features that improved the performance for Task 2 on our internal data split, we saw for many features no improvement on the public leaderboard. Thus, the only feature we used is the number of activities per channel committed by the customer. Figure 1 shows that it is one of the most predictive features. 0.00 0.05 0.10 0.15 0.20 0.25 A ge (< 5) A ge (6\u2212 5) A ge (> 5) Loation (cpital) Loation (ity) Loation (vage) Icom e low ) Icom e (m eium ) Icom e (high) Icom e (o) G eder C rdit C rd (es) C rdit C rd (o) C rdit C rd (ot one) C rdit C rd (ost it) C rdit C rd (vaiable) W elthy N ot w elthy B ecam e w elthy B ecam e ot w elthy V aable w elthy P O S acvities W eshop acvities A civity (cpital) A civity (ity) A civity (vage) A civity 5\u2212 11h) A civity (2\u2212 18h) A civity (9\u2212 h) C rd U sge (debit crd) C rd U sge (cedit crd) R el at iv e R el ev an ce Fig. 1. This plot visualizes the relative number of times a feature was chosen to build a tree in Task 2. The activity features are used in almost every fourth tree. For Task 2 we considered location information about activities, bank branches and customers to be irrelevant and only used aforementioned features. However, for Task 1 this information was one of the most impactful information. One feature we used was the distance between the residence of the customer and a bank branch which is a quite obvious choice. Digging into the data, we saw that there were many customers using bank branches very far away from their residence. We tried to cover this by also adding the maximum, minimum, mean and median distance between a bank branch and the customer\u2019s activities. Finally, we added k-nearest-neighbors predictions for k = 2, 2, . . . , 2 using the Euclidean distance between the residence of customers as the distance function. These features follow the simple assumption that customers that live nearby visit the same bank branches. Figure 2 provides insight into our intermediate feature selection experiments for Task 1 and clearly shows the importance of the location-aware features. Based on this experiment, we used all features but the credit card information for Task 1. Figure 3 shows the relative frequency of a specific feature being taken as a splitting variable. Again, this shows the importance of location-aware features for Task 1. 0.66 0.67 0.68 0.69 0.70 A ll N o activ/branch diance N o ge N o chnnel ativity N o cedit card N o cedit card nd w elthy N o cedit crd, w elthy nd usebranch diance N o gnder N o incom e N o kN N N o kN N or k> 1 N o loation N o residce/branch diance N o w elthy O ly age, kN N nd activ/branch diance E va lu at io n M ea su re Fig. 2. Intermediate feature backward selection results for Task 1. Location-aware features provide huge improvements. 0.000 0.025 0.050 0.075 0.100 A ge (< 5) A ge (6\u2212 5) A ge (> 5) Loation (cpital) Loation (ity) Loation (vage) Icom e low ) Icom e (m eium ) Icom e (high) Icom e (o) G eder W elthy N ot w elthy B ecam e w elthy B ecam e ot w elthy V aable w elthy P O S acvities W eshop acvities A ll acvities R esidee/branch diance A ctiv/branch diance (m n) A ctiv/branch diance (m x) A ctiv/branch diance (m ean) A ctiv/branch diance (m eian) 1\u2212 N N P reiction 2\u2212 N N P reiction 4\u2212 N N P reiction 8\u2212 N N P reiction 6\u2212 N N P reiction 2\u2212 N N P reiction 4\u2212 N N P reiction 128\u2212 N N P reiction 256\u2212 N N P reiction 512\u2212 N N P reiction 124\u2212 N N P reiction R el at iv e R el ev an ce Fig. 3. This plot visualizes the relative relevance of all features used in Task 1. The higher the score, the more often the feature was used for building a tree. Location-aware features prove to be highly predictive. 4 Hyperparameter Tuning and Ensembling For both tasks we tuned the hyperparameters by considering the choice of hyperparameters \u03bb as a black-box optimization problem argmin \u03bb L (\u0177\u03bb (Dvalid) , yvalid) (2) where \u0177\u03bb is the model that was trained on the training partition of the data Dtrain using hyperparameter configuration \u03bb and \u0177\u03bb (Dvalid) the corresponding predictions for the validation partitionDvalid. Then, the problem of hyperparameter tuning is to find a hyperparameter configuration \u03bb such that a loss function L given the predictions and the groundtruth is minimized. We tackled this black-box optimization problem using Sequential Modelbased Optimization (SMBO) [3]. Figure 4 presents the progress of the optimization process that was conducted in parallel on 100 cores for our own train/validation split as well as results on the public leaderboard for Task 1. For Task 2, we tried diverse ways of ensembling using different base models but did not achieve any improvement. In the end, we averaged the predictions of 100 models for the best hyperparameter configuration using different seeds.", "creator": "LaTeX with hyperref package"}}}