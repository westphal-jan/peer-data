{"id": "1610.04032", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Predicting the dynamics of 2d objects with a deep residual network", "abstract": "We investigate how a residual network can learn to predict the dynamics of interacting shapes purely as an image-to-image regression problem.", "histories": [["v1", "Thu, 13 Oct 2016 11:27:07 GMT  (1168kb,D)", "https://arxiv.org/abs/1610.04032v1", null], ["v2", "Thu, 24 Nov 2016 11:12:52 GMT  (1217kb,D)", "http://arxiv.org/abs/1610.04032v2", null]], "reviews": [], "SUBJECTS": "cs.CV cs.LG", "authors": ["fran\\c{c}ois fleuret"], "accepted": false, "id": "1610.04032"}, "pdf": {"name": "1610.04032.pdf", "metadata": {"source": "CRF", "title": "Predicting the dynamics of 2d objects with a deep residual network", "authors": ["Fran\u00e7ois Fleuret"], "emails": ["francois.fleuret@idiap.ch"], "sections": [{"heading": null, "text": "We study how a residual network can learn to predict the dynamics of interacting shapes purely as a picture-to-picture regression task. We use a simple 2D physics simulator to generate short sequences of rectangles that are set in motion by applying a traction force at a randomly selected point. The network is trained with a square loss to predict the image of the resulting configuration, as the image of the start configuration and an image indicate the point of seizure. Experiments show that the network learns to accurately predict the resulting image, which in particular implies that (1) it segments rectangles as different components, (2) it contains the grass point, (3) it correctly models the dynamics of a single rectangle including torque, (4) it detects and handles collisions to a certain degree, and (5) it re-synthesizes the entire scene with offset rectangles."}, {"heading": "1 Problem definition", "text": "We implemented a simple 2d physics simulator to generate short sequences of interacting shapes. The simulation is quite coarse, but still contains an elastic collision model, a correct torque model, and (strong) fluid friction. As illustrated by some examples in Figure 1, each sequence consists of grayscale images of resolution 64 x 64 and is generated as follows: We send 10 rectangles \u0445 francois.fleuret @ idiap.char Xiv: 161 0.04 342 v 2 [cs.C V] 2 fixed size randomly in the unit square so that they do not overlap. Then we randomly select a point that is uniform in the union of rectangle interiors, and we apply a constant force there that pulls up with a constant time delay. This shifts the captured rectangle upward and can cause collisions with other rectangles, and move them."}, {"heading": "2 Network and training", "text": "We train a residual network (He et al., 2015) with 18 layers and 16 channels to predict Rn, with Gn and Sn given as input. To make it easier to read long compositions of mappings, we leave f B g for g \u041af, since there are two mappings f and g."}, {"heading": "2.1 Structure of the network", "text": "Our network follows the classical structure of the residual networks and concatenates several identical modules with two revolutionary layers. We define \u2022 Cfc, d a standard folding layer (LeCun et al., 1998) with filters of size f \u00b7 f, padding of (f \u2212 1) / 2 to maintain the card size, c channels as input and d channels as output, \u2022 R a ReLU rectifier layer (Glorot et al., 2011), \u2022 B a batch normalization layer (Ioffe and Szegedy, 2015), \u2022 I the identity layer and \u2022 M = (Cfq, q B B B Cfq, q + I) B B B R a two-layer Resnet module (Heet al., 2015) with the second batch normalization and nonlinearity applied after summing the identity layer. The structure of the complete network has a value of 0 = Cf2, B B B = total B = 2 B B = 104 B D."}, {"heading": "2.2 Loss, initialization and training", "text": "We minimize the square loss between the predicted and targeted training patterns, L = \u2211 n \u0441 (Sn, Gn) \u2212 Rn \u0445 22 (2) and train with 32, 768 samples. We use a standard stochastic gradient descent, randomizing the training quantity for each epoch using mini-batches of size 128 and a constant learning rate of 0.1. The initialization of weights is the standard torch rule, which for the folding layers is a centered uniform distribution of the width twice the square root of the number of weights (i.e. total number of filter coefficients), and for batch normalization the target standard deviation uniformly selects in [0, 1] and sets the target value to zero. We have not adjusted the network structure, all results obtained here are with the first attempt. A run with half of the channels (i.e. q = 8) shows that it significantly deteriorates performance."}, {"heading": "3.1 Prediction", "text": "The resulting network makes an accurate prediction of the final configuration. On Figure 3, we provide five examples selected to illustrate the strengths and weaknesses of the prediction, and on Figure 4, some examples taken according to the ranking of their individual losses in order to obtain a better intuition of the overall performance. We observe that the network: \u2022 recognizes the detected rectangle and moves it while leaving the others undisturbed if there is no collision. \u2022 models the transmission and torque. \u2022 propagates the dynamics in the event of collisions to a certain degree (Figure 3 (d)). \u2022 models the hard boundaries around the area, albeit with some deformations (Figure 3 (b)). \u2022 implements the synthesis of the disturbed scene, which includes in particular the segmentation of the moving vs. stationary parts, and the synthesis of the edges in multiple orientations (Figure 4)."}, {"heading": "3.2 Inner representation", "text": "In order to shed light on the processing going on in the network, in Figure 5, for two examples, we show the processing from top to bottom as activation of the input layer, the ReLU layers after each resnet module and the output layer. The top row contains two activation cards corresponding to the two input channels, respectively the start configuration Sn and the breakpoint Gn, the bottom row contains a single card corresponding to the output of the network (Sn, Gn). The 9 other lines correspond to the ReLU layer located after the initial convolution layer Cf2, q, which converts the 2 input channels into the q-internal channels, followed by the ReLU layers placed at the output of each of the D = 8 resnet modules. The 16 columns in these 9 rows correspond to the q = 16 layer for the internal coding. We observe a homogeneity \"per channel,\" which here translates into \"per column.\""}, {"heading": "509/1024 510/1024 511/1024 512/1024 513/1024 514/1024", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "1/1024 2/1024 3/1024 4/1024 5/1024 6/1024", "text": "It is not the first time that the EU Commission has taken such a step."}, {"heading": "A Torch network structure", "text": "(2) - > (14) - > (16) - > (16) - > (16) - > (17) - > (18) - > (18) - > (19) - > (19) - > (21) - > (22) - > (25) - > (16) - > (17) - > (18) - > (21) - > (21) - > (21) - > (25) - > (25) - > (26) - > (27) - > (27) - > (31) - > (21) - > (21) - > (21) - > (25) - > (26) - > (25) - > (26) - > (21) - > (27) - > (31) - > (21) - > (21) - 21 - 21 - 21 - 21 - 21 - 21 - 21 - 21 - 21 (21) (21) - 21) - 21 - 21 - 21 - 21 (21) - 21 - 21 - 21 - 21 - 21 - 21 - 21 - 21 - 21 - 21 - 21) - (21) > (21) > (21) - > (21) - > (21) - > (21) - > (21) - > (21) - > (22) - > (22) - > (22) - > (22) - > (22) - > (22) - > (25) - > (16) - > (16) - > (16) - > (17) - > (17) - > (18) - > (18) - > (21) - > (21) - > (21) - > (21) - > (21) - > (21) - > (21) - > (21) - > (21) - > (21) > (25) > (25) - > (25) - > (25) - > (25) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) - > (26) -"}], "references": [{"title": "Torch7: A Matlab-like environment for machine learning", "author": ["R. Collobert", "K. Kavukcuoglu", "C. Farabet"], "venue": "In Proceedings of the BigLearn NIPS Workshop,", "citeRegEx": "Collobert et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep sparse rectifier neural networks", "author": ["X. Glorot", "A. Bordes", "Y. Bengio"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "citeRegEx": "Glorot et al\\.,? \\Q2011\\E", "shortCiteRegEx": "Glorot et al\\.", "year": 2011}, {"title": "Deep residual learning for image recognition", "author": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": null, "citeRegEx": "He et al\\.,? \\Q2015\\E", "shortCiteRegEx": "He et al\\.", "year": 2015}, {"title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "author": ["S. Ioffe", "C. Szegedy"], "venue": null, "citeRegEx": "Ioffe and Szegedy.,? \\Q2015\\E", "shortCiteRegEx": "Ioffe and Szegedy.", "year": 2015}, {"title": "Gradient-based learning applied to document recognition", "author": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "venue": "Proceedings of the IEEE,", "citeRegEx": "LeCun et al\\.,? \\Q1998\\E", "shortCiteRegEx": "LeCun et al\\.", "year": 1998}], "referenceMentions": [{"referenceID": 2, "context": "We train a residual network (He et al., 2015) with 18 layer and 16 channels to predict Rn, given Gn and Sn as input.", "startOffset": 28, "endOffset": 45}, {"referenceID": 4, "context": "\u2022 C c,d a standard convolution layer (LeCun et al., 1998) with filters of size f \u00d7 f , padding of (f \u2212 1)/2 to maintain the map size, c channels as input and d channels as output, \u2022 R a ReLU rectifier layer (Glorot et al.", "startOffset": 37, "endOffset": 57}, {"referenceID": 1, "context": ", 1998) with filters of size f \u00d7 f , padding of (f \u2212 1)/2 to maintain the map size, c channels as input and d channels as output, \u2022 R a ReLU rectifier layer (Glorot et al., 2011), \u2022 B a batch-normalization layer (Ioffe and Szegedy, 2015), \u2022 I the identity layer, and \u2022 M = ( C q,q B B B R B C q,q + I ) B B B R a two-layer resnet module (He et al.", "startOffset": 157, "endOffset": 178}, {"referenceID": 3, "context": ", 2011), \u2022 B a batch-normalization layer (Ioffe and Szegedy, 2015), \u2022 I the identity layer, and \u2022 M = ( C q,q B B B R B C q,q + I ) B B B R a two-layer resnet module (He et al.", "startOffset": 41, "endOffset": 66}, {"referenceID": 2, "context": ", 2011), \u2022 B a batch-normalization layer (Ioffe and Szegedy, 2015), \u2022 I the identity layer, and \u2022 M = ( C q,q B B B R B C q,q + I ) B B B R a two-layer resnet module (He et al., 2015) with the second batch normalization and non-linearity applied after summing the identity.", "startOffset": 166, "endOffset": 183}, {"referenceID": 0, "context": "We implemented the simulator in C++ and the network processing and performance evaluation in the Torch framework (Collobert et al., 2011).", "startOffset": 113, "endOffset": 137}], "year": 2016, "abstractText": "We investigate how a residual network can learn to predict the dynamics of interacting shapes purely as an image-to-image regression task. With a simple 2d physics simulator, we generate short sequences composed of rectangles put in motion by applying a pulling force at a point picked at random. The network is trained with a quadratic loss to predict the image of the resulting configuration, given the image of the starting configuration and an image indicating the point of grasping. Experiments show that the network learns to predict accurately the resulting image, which implies in particular that (1) it segments rectangles as distinct components, (2) it infers which one contains the grasping point, (3) it models properly the dynamic of a single rectangle, including the torque, (4) it detects and handles collisions to some extent, and (5) it re-synthesizes properly the entire scene with displaced rectangles. 1 Problem definition We implemented a simple 2d physics simulator to generate short sequences of interacting shapes. The simulation is quite crude but still includes an elastic collision model, a proper torque model, and (strong) fluid frictions. As illustrated with a few examples on Figure 1, each sequence is composed of grayscale images of resolution 64\u00d764, and is created as follows: We dispatch 10 rectangles \u2217francois.fleuret@idiap.ch 1 ar X iv :1 61 0. 04 03 2v 2 [ cs .C V ] 2 4 N ov 2 01 6 of fixed size at random in the unit square, so that they do not overlap. Then we pick at random a point uniformly in the union of the rectangle interiors, and we apply there a constant force pulling upward for a constant time delay. This moves the grasped rectangle upward and may induce collisions with other rectangles, and make them move. The borders of the square area are impenetrable, hence rectangles grabbed near the top may have their motion constrained accordingly. While the grasping point location is randomized for every sequence, the characteristics of the force and its duration are common to all the sequences. Gn Sn Rn Figure 1: Each row corresponds to one sequence of our data-set. It is composed of six gray-scale images of size 64 \u00d7 64: the \u201cgrasping point image\u201d, followed by five frames. In each sequence, the rectangle originally containing the grasping point is pulled upward and moves accordingly. It may collide with and push other rectangles. We show several frames of each sequence for clarity here, but use only the two leftmost images (Sn, Gn) and the rightmost one Rn from each sequence in our experiments, in which we try to predict the latter from the former. As illustrated on Figure 1, from each generated sequence we produce three images: Gn, Sn, Rn which correspond, respectively, to the grasping point image (all white, with a dot at the location of the grasp, as shown in the leftmost column of Figure 1), the starting configuration, which is the first image of the sequence, and the resulting configuration, which is the last image of the sequence.", "creator": "LaTeX with hyperref package"}}}