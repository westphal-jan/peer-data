{"id": "1610.04120", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding", "abstract": "This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER).", "histories": [["v1", "Thu, 13 Oct 2016 15:11:40 GMT  (46kb,D)", "http://arxiv.org/abs/1610.04120v1", null]], "reviews": [], "SUBJECTS": "cs.AI cs.CL cs.NE", "authors": ["lina m rojas barahona", "milica gasic", "nikola mrk\\v{s}i\\'c", "pei-hao su", "stefan ultes", "tsung-hsien wen", "steve young"], "accepted": false, "id": "1610.04120"}, "pdf": {"name": "1610.04120.pdf", "metadata": {"source": "CRF", "title": "Exploiting Sentence and Context Representations in Deep Neural Models for Spoken Language Understanding", "authors": ["Lina M. Rojas-Barahona", "Milica Ga\u0161i\u0107", "Nikola Mrk\u0161i\u0107", "Pei-Hao Su", "Stefan Ultes", "Tsung-Hsien Wen", "Steve Young"], "emails": ["sjy@cam.ac.uk"], "sections": [{"heading": "1 Introduction", "text": "In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence that identifies problems with models trained and tested on datasets with word-level annotations (Tu Tuer et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014). Understanding language from unaligned data, in which utterances are commented with an abstract semantic, faces the additional challenge of not knowing which specific words are relevant to the extraction of semantics. This problem was addressed in (Zhou and Er, 2011) by using conditional random fields (CRFs) driven by finely tuned crafted feature. Other discriminatory approaches that deal with non-aligned data (Henderson), or deleterization of some known concepts."}, {"heading": "2 Related Work", "text": "However, it is costly to use these word levels and sequence neural networks as they require specialized data collection. (U) It is very interesting that we understand this spoken language. (U) It is very important that we understand this language. (RBM) It is a combination of neural networks and triangular CRFs in (Celikyilmaz and Hakkani-Tur, 2010) in which a Convolutionary Neural Network is used to extract the input functions of a triangular CRF to perform common endings and slot fillings. All of these models use word levels of semantic annotation. But providing these word level semantic annotations is costly as it requires specialized CRF data to perform common endings and slot fillings."}, {"heading": "3 Deep Learning Semantic Decoder", "text": "We divide the task of semantic decoding into two steps: (i) training a common model to predict the dialog act and the presence or absence of slots, and (ii) predicting the values for the most likely slots found in (i). As shown in Figure 2, in both steps we use the same deep learning architecture to combine sentence and context representations to create the last hidden unit that feeds one or many Softmax layers. In the first step, as shown in Figure 2, there are different Softmax layers to jointly optimize the dialog act and each possible slot. In the second step, there is a single Softmax layer that predicts the value of each specific slot. Below, we explain this architecture in detail."}, {"heading": "3.1 Sentence Representation", "text": "CNN is a variant of (Kim, 2014), in which the inputs are the word vectors in each ASR hypothesis. Let xi be a k \u2212 dimensional word embedded in a hypothesis for the i \u2212 th word. A hypothesis of length m is presented as follows: x1: m = x1 x 2 x... x m, in which the concatenation is operative. A revolutionary operation is applied to a window l words to generate a new characteristic.ci = f (w \u00b7 xi: i + l \u2212 1 + b) (1), where f is the hyperbolic tangent function; w \u00b2 Rlk is a filter applied to a window l words, and b \u00b2 R is a bias term. The filter is applied to each window of words in the sentence to generate a characteristic."}, {"heading": "3.2 Context Representation", "text": "A LSTM (Hochreiter and Schmidhuber, 1997) is used to track the context implied by previous dialog system actions; the top layer of this LSTM network then provides the context representation for decoding the current input expression; an LSTM is a sequence model that uses a memory cell that is able to preserve states for long periods of time; this cell is always connected to itself and has three multiplication units, an input gate, a forget gate and an output gate; these gating vectors are located in [0.1]; the cell makes selective decisions about what information should be preserved, and when access to units should be allowed, about gates that open and close; the LSTM transition equations are as follows: they (W (i) \u00b7 xt + U (i) \u00b7 1 + b (i) \u00b7 b (i) (i), H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, H, and Schmidhuber, and Schmidhuber."}, {"heading": "3.3 Combining Sentence and Context", "text": "In this thesis we examine two ways of combining the sentence st and the contextual representations ht: The first simple way is to apply a nonlinear function to its weighted sum: h-t = tanh (Ws \u00b7 st + Wc \u00b7 ht) (5) The second way is to allow the sentence representation to be the last input into the LSTM network, then h-t = ht. For the classification, a Softmax layer is used for each prediction: P (Y = k | h, W, b) = e (Wkh + bk) \u2211 k \"e (Wk \u2032 h + bk \u2032) (6), where k is the index of the initial neuron representing a class. For the dialogue act classification k is one of the possible values: inform, request, offer,... etc. For the slot prediction k is either 0 for absent or 1 for present. For the slot value prediction k is one of the possible values for each slot."}, {"heading": "4 Experimental Evaluation", "text": "In this section we present the corpora and describe the experiments carried out and the evaluation metrics used."}, {"heading": "4.1 Corpora", "text": "For the experimental evaluation, two similar data sets were used: DSTC2 (Henderson et al., 2014b) and In-Car (Tsiakoulis et al., 2012), both of which were collected using a spoken dialogue system that provides a restaurant information system for the City of Cambridge. Users can enter restaurant suggestions by area, price category and food type, and then query the system for additional restaurant-specific information such as phone number, zip code and address. The first dialogue corpus was released for dialogue tracking and we use the semantic annotations that were also provided 2. The train has 2118 dialogs and 15611 rotations in total, while the test set has a total of 1117 dialogues and 9890 rotations. The second corpus contains dialogs collected under various noisy conditions in the car. In a stationary car with climate fan on and off, in a moving car, and in a car simulator the total of 41 times Tsiakoulis is set (the total error corpus is 15408 and 1043. higher)."}, {"heading": "4.2 Hyperparameters and Training", "text": "The models were implemented in Theano (Bastien et al., 2012). We2The DSTC2 corpus is publicly available at: http: / / camdial.org / \u02dc mh521 / dstc / 3This corpus was obtained in an industry-funded project and is therefore not available for public use. Filter windows of 3, 4 and 5 were used, each with 100 function boards for CNN. A drop-out rate of 0.5 and a batch size of 50 was used, 10% of the training kit was used as validation set and an early stop was introduced. Training is performed by stochastic gradient departure via mixed mini-batches with Adadelta updating rule (we used a word adadelta decay of 0.95)."}, {"heading": "4.3 Experiments", "text": "Step I: Common classification of dialog files and slots: We evaluated five different model configurations for the common classification of dialog files and the presence or absence of slots. \u2022 CNN: The Softmax levels for the common classification of dialog files and slots are directly connected to the CNN sentence representation without context. \u2022 CNN + LSTM: We examine the influence of the context taking into account the previous system actions (Section 3.2, Eq. 5), here we examine the different context lengths by using a context window of 1, 4 and all previous system actions, namely CNN + LSTM w1, CNN + LSTM w4 or CNN + LSTM w. \u2022 LSTM all: Finally, we examine the effects of long-distance dependencies by mainly using the LSTM model, using the previous system actions as input, but predicting the slot representation as the last LSTM input. \u2022 Step II: We select the pre-classification of each slot by predicting the best value pairing for the present time in the model."}, {"heading": "4.4 Evaluation Metrics", "text": "We evaluate the performance of our models using traditional classification metrics, namely accuracy, precision, retrieval and F-measurement (F1 score). Additionally, we used the ICE score (Equation 8) between hypotheses and reference semantics (i.e. ground truth) to measure the overall quality of the distribution obtained from the models (Thomson et al., 2008). Let U be the number of utterances and W the number of available semantic items. Give u = 1.. U and w = 1... W, let us: cuw = {p, the reliability assigned to the hypothesis that the semantic growth element is part of the utterance u, 0 if none has been assigned."}, {"heading": "5 Results and Discussion", "text": "In this section we report on the results on DSTC2 and In-car dialogue corpora.Step I: Joint classification of dialogue-acts and slots: For this step, the classifiers must say together 14 dialogue acts and 5 slots for the DSTC2 dataset as as 14 dialogue acts and 7 slots for the in-car dataset. We rate both (i) with 10 fold cross-validation of the trainsets and (ii) on the corpora'testsets.Table 1 shows the 10 fold cross-validation results on both corpora. These results suggest that for DTSC2, the context representation is not significantly impacting the prediction of the model with a window of 4, CNN + LSTM w4, improved slightly the accuracy and f1-score. On the in-car dataset, including the context does help to disambiguate the illformed hypotheses."}, {"heading": "6 Conclusion and Future Work", "text": "We compared different models to combine sentence and context representations and found that context representations significantly influence the ASR hypotheses generated under very loud conditions. Combining sentence and context representations, with a context window of 4 words, significantly exceeds all baseline in terms of the ICE score. In terms of F1 scores, our model does not exceed the baseline on the DSTC2 corpus and the baseline without manually designed features on the in-car corpus. Although the F score of our model does not exceed the baseline enriched by context features on the in-car corpus, the proposed model remains competitive, especially considering that our model does not require manually designed features or application-specific semantic dictionaries."}, {"heading": "7 Future Work", "text": "Semantic distributed vector representations can be used to identify similarities between domains. As a future work, we want to investigate the adoption of the set and the contex representations generated in Step I (i.e. the common prediction of dialog files and slots) within a topic management in multi-domain dialog systems. Topic Manager is responsible for recognizing the domain and the intention behind users \"statements. Furthermore, it would be interesting to examine these embedding for domain adaptations on potentially open domains."}, {"heading": "Acknowledgments", "text": "This research was partly funded by the EP-SRC grant EP / M018946 / 1 Open Domain Statistical Spoken Dialogue Systems. Data used in this paper was produced in an industry-funded project and is not available to the public."}], "references": [{"title": "Theano: new features and speed improvements", "author": ["Fr\u00e9d\u00e9ric Bastien", "Pascal Lamblin", "Razvan Pascanu", "James Bergstra", "Ian Goodfellow", "Arnaud Bergeron", "Nicolas Bouchard", "Yoshua Bengio."], "venue": "nips workshop on deep learning and unsupervised feature learning.", "citeRegEx": "Bastien et al\\.,? 2012", "shortCiteRegEx": "Bastien et al\\.", "year": 2012}, {"title": "Convolutional neural network based semantic tagging with entity embeddings", "author": ["Asli Celikyilmaz", "Dilek Hakkani-Tur."], "venue": "genre.", "citeRegEx": "Celikyilmaz and Hakkani.Tur.,? 2010", "shortCiteRegEx": "Celikyilmaz and Hakkani.Tur.", "year": 2010}, {"title": "Learning bidirectional intent embeddings by convolutional deep structured semantic models for spoken language understanding", "author": ["Yun-Nung Chen", "Xiaodong He."], "venue": "Extended Abstract of The 29th Annual Conference on Neural Information Processing Systems\u2013Machine Learning for Spoken Language Understanding and Interactions Workshop (NIPS-SLU).", "citeRegEx": "Chen and He.,? 2015", "shortCiteRegEx": "Chen and He.", "year": 2015}, {"title": "Natural language processing (almost) from scratch", "author": ["Ronan Collobert", "Jason Weston", "L\u00e9on Bottou", "Michael Karlen", "Koray Kavukcuoglu", "Pavel Kuksa."], "venue": "Journal of Machine Learning Research, 12(Aug):2493\u2013 2537.", "citeRegEx": "Collobert et al\\.,? 2011", "shortCiteRegEx": "Collobert et al\\.", "year": 2011}, {"title": "Deep belief network based semantic taggers for spoken language understanding", "author": ["Anoop Deoras", "Ruhi Sarikaya."], "venue": "INTERSPEECH, pages 2713\u20132717.", "citeRegEx": "Deoras and Sarikaya.,? 2013", "shortCiteRegEx": "Deoras and Sarikaya.", "year": 2013}, {"title": "Online adaptative zero-shot learning spoken language understanding using word-embedding", "author": ["Emmanuel Ferreira", "Bassam Jabaian", "Fabrice Lef\u00e8vre."], "venue": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5321\u20135325. IEEE.", "citeRegEx": "Ferreira et al\\.,? 2015", "shortCiteRegEx": "Ferreira et al\\.", "year": 2015}, {"title": "Discriminative Spoken Language Understanding Using Word Confusion Networks", "author": ["Matthew Henderson", "Milica Ga\u0161i\u0107", "Blaise Thomson", "Pirros Tsiakoulis", "Kai Yu", "Steve Young."], "venue": "Spoken Language Technology Workshop, 2012. IEEE.", "citeRegEx": "Henderson et al\\.,? 2012", "shortCiteRegEx": "Henderson et al\\.", "year": 2012}, {"title": "Word-based Dialog State Tracking with Recurrent Neural Networks", "author": ["M. Henderson", "B. Thomson", "S.J. Young."], "venue": "Proceedings of SIGdial.", "citeRegEx": "Henderson et al\\.,? 2014a", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "The second dialog state tracking challenge", "author": ["Matthew Henderson", "Blaise Thomson", "Jason Williams."], "venue": "15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, volume 263.", "citeRegEx": "Henderson et al\\.,? 2014b", "shortCiteRegEx": "Henderson et al\\.", "year": 2014}, {"title": "Improving neural networks by preventing co-adaptation of feature detectors", "author": ["Geoffrey E Hinton", "Nitish Srivastava", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan R Salakhutdinov."], "venue": "arXiv preprint arXiv:1207.0580.", "citeRegEx": "Hinton et al\\.,? 2012", "shortCiteRegEx": "Hinton et al\\.", "year": 2012}, {"title": "Long short-term memory", "author": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber."], "venue": "Neural computation, 9(8):1735\u20131780.", "citeRegEx": "Hochreiter and Schmidhuber.,? 1997", "shortCiteRegEx": "Hochreiter and Schmidhuber.", "year": 1997}, {"title": "A convolutional neural network for modelling sentences", "author": ["Nal Kalchbrenner", "Edward Grefenstette", "Phil Blunsom."], "venue": "arXiv preprint arXiv:1404.2188.", "citeRegEx": "Kalchbrenner et al\\.,? 2014", "shortCiteRegEx": "Kalchbrenner et al\\.", "year": 2014}, {"title": "Convolutional neural networks for sentence classification", "author": ["Yoon Kim."], "venue": "arXiv preprint arXiv:1408.5882.", "citeRegEx": "Kim.,? 2014", "shortCiteRegEx": "Kim.", "year": 2014}, {"title": "Using recurrent neural networks for slot filling in spoken language understanding", "author": ["Gr\u00e9goire Mesnil", "Yann Dauphin", "Kaisheng Yao", "Yoshua Bengio", "Li Deng", "Dilek Hakkani-Tur", "Xiaodong He", "Larry Heck", "Gokhan Tur", "Dong Yu"], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,", "citeRegEx": "Mesnil et al\\.,? \\Q2015\\E", "shortCiteRegEx": "Mesnil et al\\.", "year": 2015}, {"title": "Multi-domain Dialog State Tracking using Recurrent Neural Networks", "author": ["Nikola Mrk\u0161i\u0107", "Diarmuid \u00d3 S\u00e9aghdha", "Blaise Thomson", "Milica Ga\u0161i\u0107", "Pei-Hao Su", "David Vandyke", "Tsung-Hsien Wen", "Steve Young."], "venue": "Proceedings of ACL.", "citeRegEx": "Mrk\u0161i\u0107 et al\\.,? 2015", "shortCiteRegEx": "Mrk\u0161i\u0107 et al\\.", "year": 2015}, {"title": "Glove: Global vectors for word representation", "author": ["Jeffrey Pennington", "Richard Socher", "Christopher D. Manning."], "venue": "Empirical Methods in Natural Language Processing (EMNLP), pages 1532\u20131543.", "citeRegEx": "Pennington et al\\.,? 2014", "shortCiteRegEx": "Pennington et al\\.", "year": 2014}, {"title": "Deep belief nets for natural language call-routing", "author": ["R. Sarikaya", "G.E. Hinton", "B. Ramabhadran."], "venue": "2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5680\u20135683, May.", "citeRegEx": "Sarikaya et al\\.,? 2011", "shortCiteRegEx": "Sarikaya et al\\.", "year": 2011}, {"title": "Application of deep belief networks for natural language understanding", "author": ["Ruhi Sarikaya", "Geoffrey E Hinton", "Anoop Deoras."], "venue": "IEEE/ACM Transactions on Audio, Speech, and Language Processing, 22(4):778\u2013 784.", "citeRegEx": "Sarikaya et al\\.,? 2014", "shortCiteRegEx": "Sarikaya et al\\.", "year": 2014}, {"title": "Evaluating semantic-level confidence scores with multiple hypotheses", "author": ["Blaise Thomson", "Kai Yu", "Milica Gasic", "Simon Keizer", "Francois Mairesse", "Jost Schatzmann", "Steve J Young."], "venue": "INTERSPEECH, pages 1153\u2013 1156.", "citeRegEx": "Thomson et al\\.,? 2008", "shortCiteRegEx": "Thomson et al\\.", "year": 2008}, {"title": "Statistical methods for building robust spoken dialogue systems in an automobile", "author": ["Pirros Tsiakoulis", "Milica Ga\u0161ic", "Matthew Henderson", "Joaquin Planells-Lerma", "Jorge Prombonas", "Blaise Thomson", "Kai Yu", "Steve Young", "Eli Tzirkel."], "venue": "Proceedings of the 4th applied human factors and ergonomics.", "citeRegEx": "Tsiakoulis et al\\.,? 2012", "shortCiteRegEx": "Tsiakoulis et al\\.", "year": 2012}, {"title": "Semantic parsing using word confusion networks with conditional random fields", "author": ["G\u00f6khan T\u00fcr", "Anoop Deoras", "Dilek Hakkani-T\u00fcr."], "venue": "INTERSPEECH, pages 2579\u20132583.", "citeRegEx": "T\u00fcr et al\\.,? 2013", "shortCiteRegEx": "T\u00fcr et al\\.", "year": 2013}, {"title": "Web-style ranking and slu combination for dialog state tracking", "author": ["Jason D Williams."], "venue": "Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 282\u2013291.", "citeRegEx": "Williams.,? 2014", "shortCiteRegEx": "Williams.", "year": 2014}, {"title": "Recurrent neural networks for language understanding", "author": ["Kaisheng Yao", "Geoffrey Zweig", "Mei-Yuh Hwang", "Yangyang Shi", "Dong Yu."], "venue": "INTERSPEECH, pages 2524\u20132528.", "citeRegEx": "Yao et al\\.,? 2013", "shortCiteRegEx": "Yao et al\\.", "year": 2013}, {"title": "Learning conditional random fields from unaligned data for natural language understanding", "author": ["Deyu Zhou", "Yulan He."], "venue": "European Conference on Information Retrieval, pages 283\u2013288. Springer.", "citeRegEx": "Zhou and He.,? 2011", "shortCiteRegEx": "Zhou and He.", "year": 2011}, {"title": "Semantic parser enhancement for dialogue domain extension with little data", "author": ["Su Zhu", "Lu Chen", "Kai Sun", "Da Zheng", "Kai Yu."], "venue": "Spoken Language Technology Workshop (SLT), 2014 IEEE, pages 336\u2013341. IEEE.", "citeRegEx": "Zhu et al\\.,? 2014", "shortCiteRegEx": "Zhu et al\\.", "year": 2014}], "referenceMentions": [{"referenceID": 20, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 13, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 22, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 16, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 4, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 17, "context": "1 Introduction In most existing work on Spoken Language Understanding (SLU), semantic decoding is usually seen as a sequence tagging problem with models trained and tested on datasets with word-level annotations (T\u00fcr et al., 2013; Mesnil et al., 2015; Yao et al., 2013; Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 212, "endOffset": 342}, {"referenceID": 23, "context": "This problem was tackled in (Zhou and He, 2011), by using conditional random fields (CRFs) driven by finely-tuned hand-crafted features.", "startOffset": 28, "endOffset": 47}, {"referenceID": 6, "context": "Other discriminative approaches that deal with unaligned data use some form of delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012; Henderson et al., 2014a).", "startOffset": 150, "endOffset": 199}, {"referenceID": 7, "context": "Other discriminative approaches that deal with unaligned data use some form of delexicalisation or mapping of the input to known ontological concepts (Henderson et al., 2012; Henderson et al., 2014a).", "startOffset": 150, "endOffset": 199}, {"referenceID": 3, "context": "A convolutional neural network (CNN) (Collobert et al., 2011) is used to generate the sentence representation, while a longshort term memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to generate the context representation.", "startOffset": 37, "endOffset": 61}, {"referenceID": 10, "context": ", 2011) is used to generate the sentence representation, while a longshort term memory (LSTM) network (Hochreiter and Schmidhuber, 1997) is used to generate the context representation.", "startOffset": 102, "endOffset": 136}, {"referenceID": 8, "context": "Our models are evaluated on two datasets DSTC2 (Henderson et al., 2014b) and In-car (Tsiakoulis et al.", "startOffset": 47, "endOffset": 72}, {"referenceID": 19, "context": ", 2014b) and In-car (Tsiakoulis et al., 2012) using accuracy, f-measure and the Item Cross Entropy (ICE) score (Thomson et al.", "startOffset": 20, "endOffset": 45}, {"referenceID": 18, "context": ", 2012) using accuracy, f-measure and the Item Cross Entropy (ICE) score (Thomson et al., 2008).", "startOffset": 73, "endOffset": 95}, {"referenceID": 22, "context": "For instance, Recurrent Neural Networks have been proposed in (Yao et al., 2013; Mesnil et al., 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al.", "startOffset": 62, "endOffset": 101}, {"referenceID": 13, "context": "For instance, Recurrent Neural Networks have been proposed in (Yao et al., 2013; Mesnil et al., 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al.", "startOffset": 62, "endOffset": 101}, {"referenceID": 16, "context": ", 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 132, "endOffset": 205}, {"referenceID": 4, "context": ", 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 132, "endOffset": 205}, {"referenceID": 17, "context": ", 2015) and generative Deep Neural Networks consisting of a composition of Restricted Boltzmann Machines (RBM) have been studied by (Sarikaya et al., 2011; Deoras and Sarikaya, 2013; Sarikaya et al., 2014).", "startOffset": 132, "endOffset": 205}, {"referenceID": 1, "context": "A combination of neural networks and triangular CRFs is presented in (Celikyilmaz and Hakkani-Tur, 2010), in which a convolutional neural network is used for extracting the input features of a triangular CRF in order to perform joint intent detection and slot filling.", "startOffset": 69, "endOffset": 104}, {"referenceID": 23, "context": "(Zhou and He, 2011) has proposed learning CRFs from unaligned data, however they use manually tuned lexical or syntactic features.", "startOffset": 0, "endOffset": 19}, {"referenceID": 12, "context": "Convolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation.", "startOffset": 86, "endOffset": 124}, {"referenceID": 11, "context": "Convolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation.", "startOffset": 86, "endOffset": 124}, {"referenceID": 2, "context": "(Chen and He, 2015) proposed a CNN for generating intent embeddings in SLU, which uses tri-letter input vectors.", "startOffset": 0, "endOffset": 19}, {"referenceID": 15, "context": "Instead, in this paper the models are initialised with GloVe word embeddings (Pennington et al., 2014).", "startOffset": 77, "endOffset": 102}, {"referenceID": 1, "context": "A combination of neural networks and triangular CRFs is presented in (Celikyilmaz and Hakkani-Tur, 2010), in which a convolutional neural network is used for extracting the input features of a triangular CRF in order to perform joint intent detection and slot filling. All these models use word-level semantic annotations. However, providing these word-level semantic annotations is costly since it requires specialised annotators. (Zhou and He, 2011) has proposed learning CRFs from unaligned data, however they use manually tuned lexical or syntactic features. In this work we avoid the need for word-level annotation by exploiting distributed word embeddings and using deep learning for feature representation. Convolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation.", "startOffset": 70, "endOffset": 916}, {"referenceID": 1, "context": "A combination of neural networks and triangular CRFs is presented in (Celikyilmaz and Hakkani-Tur, 2010), in which a convolutional neural network is used for extracting the input features of a triangular CRF in order to perform joint intent detection and slot filling. All these models use word-level semantic annotations. However, providing these word-level semantic annotations is costly since it requires specialised annotators. (Zhou and He, 2011) has proposed learning CRFs from unaligned data, however they use manually tuned lexical or syntactic features. In this work we avoid the need for word-level annotation by exploiting distributed word embeddings and using deep learning for feature representation. Convolutional Neural Networks (CNNs) have been used previously for sentiment analysis (Kim, 2014; Kalchbrenner et al., 2014) and in this work we explore a similar CNN to the one presented by Kim (2014) for generating a sentence representation. However unlike Kim (2014), the input in not a single well formed sentence but a set of ill-formed ASR hypotheses.", "startOffset": 70, "endOffset": 984}, {"referenceID": 5, "context": "Approaches for adaptive SLU have been proposed in (Ferreira et al., 2015; Zhu et al., 2014), however they focused more on domain adaptation on top of an existing SLU component.", "startOffset": 50, "endOffset": 91}, {"referenceID": 24, "context": "Approaches for adaptive SLU have been proposed in (Ferreira et al., 2015; Zhu et al., 2014), however they focused more on domain adaptation on top of an existing SLU component.", "startOffset": 50, "endOffset": 91}, {"referenceID": 7, "context": "Recently, some researchers have focused on mapping word level hypotheses directly to beliefs without using an explicit semantic decoder step (Henderson et al., 2014a; Mrk\u0161i\u0107 et al., 2015).", "startOffset": 141, "endOffset": 187}, {"referenceID": 14, "context": "Recently, some researchers have focused on mapping word level hypotheses directly to beliefs without using an explicit semantic decoder step (Henderson et al., 2014a; Mrk\u0161i\u0107 et al., 2015).", "startOffset": 141, "endOffset": 187}, {"referenceID": 12, "context": "The CNN is a variant of (Kim, 2014), in which the inputs are the word vectors in each ASR hypothesis.", "startOffset": 24, "endOffset": 35}, {"referenceID": 10, "context": "2 Context Representation An LSTM (Hochreiter and Schmidhuber, 1997) is used for tracking the context implied by previous dialogue system actions.", "startOffset": 33, "endOffset": 67}, {"referenceID": 8, "context": "1 Corpora Experimental evaluation used two similar datasets: DSTC2 (Henderson et al., 2014b) and In-car (Tsiakoulis et al.", "startOffset": 67, "endOffset": 92}, {"referenceID": 19, "context": ", 2014b) and In-car (Tsiakoulis et al., 2012).", "startOffset": 20, "endOffset": 45}, {"referenceID": 19, "context": "In a stationary car with the air conditioning fan on and off, in a moving car and in a car simulator (Tsiakoulis et al., 2012) 3.", "startOffset": 101, "endOffset": 126}, {"referenceID": 9, "context": "2 Hyperparameters and Training Dropout was used on the penultimate layers of both the CNN and the LSTM networks to prevent coadaptation of hidden units by randomly dropping out a proportion of the hidden units during forward propagation (Hinton et al., 2012).", "startOffset": 237, "endOffset": 258}, {"referenceID": 0, "context": "The models were implemented in Theano (Bastien et al., 2012).", "startOffset": 38, "endOffset": 60}, {"referenceID": 15, "context": "To initialise the models, GloVE word vectors were used (Pennington et al., 2014) with a dimension d = 100.", "startOffset": 55, "endOffset": 80}, {"referenceID": 18, "context": "ground-truth) to measure the overall quality of the distribution returned by the models(Thomson et al., 2008).", "startOffset": 87, "endOffset": 109}, {"referenceID": 6, "context": "Overall performance A baseline for assessing overall performance is provided by the model presented in (Henderson et al., 2012), in which the vector representation is obtained by summing up the frequency of n-grams extracted from the 10-best hypotheses, weighted by their confidence scores.", "startOffset": 103, "endOffset": 127}, {"referenceID": 21, "context": "A similar model, namely SLU1, was evaluated on DSTC2 in (Williams, 2014).", "startOffset": 56, "endOffset": 72}, {"referenceID": 21, "context": "Corpus Model F1 ICE DSTC2 SLU1 (Williams, 2014) 80.", "startOffset": 31, "endOffset": 47}, {"referenceID": 6, "context": "In-car WNGRAMS (Henderson et al., 2012) 70.", "startOffset": 15, "endOffset": 39}, {"referenceID": 6, "context": "WNGRAMS+Ctxt (Henderson et al., 2012) 74.", "startOffset": 13, "endOffset": 37}], "year": 2016, "abstractText": "This paper presents a deep learning architecture for the semantic decoder component of a Statistical Spoken Dialogue System. In a slot-filling dialogue, the semantic decoder predicts the dialogue act and a set of slot-value pairs from a set of n-best hypotheses returned by the Automatic Speech Recognition. Most current models for spoken language understanding assume (i) word-aligned semantic annotations as in sequence taggers and (ii) delexicalisation, or a mapping of input words to domain-specific concepts using heuristics that try to capture morphological variation but that do not scale to other domains nor to language variation (e.g., morphology, synonyms, paraphrasing ). In this work the semantic decoder is trained using unaligned semantic annotations and it uses distributed semantic representation learning to overcome the limitations of explicit delexicalisation. The proposed architecture uses a convolutional neural network for the sentence representation and a long-short term memory network for the context representation. Results are presented for the publicly available DSTC2 corpus and an In-car corpus which is similar to DSTC2 but has a significantly higher word error rate (WER).", "creator": "TeX"}}}