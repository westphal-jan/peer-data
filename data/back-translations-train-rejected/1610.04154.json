{"id": "1610.04154", "review": {"conference": "arxiv", "VERSION": "v1", "DATE_OF_SUBMISSION": "13-Oct-2016", "title": "An Information Theoretic Feature Selection Framework for Big Data under Apache Spark", "abstract": "With the advent of extremely high dimensional datasets, dimensionality reduction techniques are becoming mandatory. Among many techniques, feature selection has been growing in interest as an important tool to identify relevant features on huge datasets --both in number of instances and features--. The purpose of this work is to demonstrate that standard feature selection methods can be parallelized in Big Data platforms like Apache Spark, boosting both performance and accuracy. We thus propose a distributed implementation of a generic feature selection framework which includes a wide group of well-known Information Theoretic methods. Experimental results on a wide set of real-world datasets show that our distributed framework is capable of dealing with ultra-high dimensional datasets as well as those with a huge number of samples in a short period of time, outperforming the sequential version in all the cases studied.", "histories": [["v1", "Thu, 13 Oct 2016 16:17:07 GMT  (417kb,D)", "https://arxiv.org/abs/1610.04154v1", null], ["v2", "Wed, 19 Oct 2016 16:46:28 GMT  (417kb,D)", "http://arxiv.org/abs/1610.04154v2", null]], "reviews": [], "SUBJECTS": "cs.AI cs.DC cs.LG", "authors": ["sergio ram\\'irez-gallego", "h\\'ector mouri\\~no-tal\\'in", "david mart\\'inez-rego", "ver\\'onica bol\\'on-canedo", "jos\\'e manuel ben\\'itez", "amparo alonso-betanzos", "francisco herrera"], "accepted": false, "id": "1610.04154"}, "pdf": {"name": "1610.04154.pdf", "metadata": {"source": "CRF", "title": "An Information Theoretic Feature Selection Framework for Big Data under Apache Spark", "authors": ["Sergio Ram\u0131\u0301rez-Gallego", "H\u00e9ctor Mouri\u00f1o-Ta\u013a\u0131n", "David Mart\u0301\u0131nez-Rego", "Ver\u00f3nica Bol\u00f3n-Canedo", "Jos\u00e9 Manuel Be\u0144\u0131tez", "Amparo Alonso-Betanzos", "Francisco Herrera"], "emails": ["sramirez@decsai.ugr.es,", "j.m.benitez@decsai.ugr.es,", "herrera@decsai.ugr.es", "h.mtalin@udc.es,", "dmartinez@udc.es,", "veronica.bolon@udc.es,", "ciamparo@udc.es"], "sections": [{"heading": null, "text": "Index Terms - High Dimensional, Filtering Methods, Feature Selection, Apache Spark, Big Data."}, {"heading": "1 Introduction", "text": "This year is the highest in the history of the country."}, {"heading": "2 Background", "text": "In this section, we will give a brief introduction to FS, followed by a discussion on the advent of big data and its impact on this area. Finally, we will outline the specifics of the MapReduce framework and the models derived from it."}, {"heading": "2.1 Feature Selection", "text": "FS is a method for reducing dimensionality that attempts to remove irrelevant and redundant features from the original data. It aims to obtain a subset of features that correctly describes the given problem with a minimal deterioration of performance, in order to obtain simpler and more accurate schemes [17]. Formally, we can define the feature selection as follows: Let us be an instance ei = (ei1,.., one, eiy) in which heirs correspond to the r-th characteristic value of the i-th sample and eiy to the value of the output class Y. Let us take a training set D with examples whose instances ei are formed by a setX of n features or characteristics, and a test set Dt exist. Let us then define S\u03b8 X as a subset of selected characteristics produced by an FS algorithm. FS methods can generally be categorized as [4] a general selection function based on it."}, {"heading": "2.2 Big Data: a two-sided coin", "text": "As the Internet continues to generate quantities of data, the problem of the large amounts of data in our world is becoming more and more urgent. For example, in 2012 2.5 exabytes of daily data were created, which became one of the most important and complex challenges in data analysis, and this situation has led to the transformation of many of these data into obsolete systems."}, {"heading": "2.3 MapReduce Programming Model and Frameworks: Hadoop and Spark", "text": "In fact, most people are able to decide whether they will be able to play by the rules, or whether they will be able to break the rules."}, {"heading": "3 Filtering Feature Selection for Big Data", "text": "In [8] an information theoretical framework has been proposed that includes many common FS filtering algorithms. In their work, the authors prove that algorithms are such as a minimum of redundancy-maximum relevance (mRMR) and other special cases of conditional mutual information when some specific assumptions of independence are made both across class and characteristics (see details below).At this point, we show that these criteria are not only a solid theoretical formulation, but also fit well into modern big data platforms and allow us to distribute multiple FS methods and their complexity across a cluster of machines.In this work, we describe, among other things, how we have redesigned this framework for a distributed paradigm. This version includes a generic implementation of several information theoretical FS methods such as: mRMR, Conditional Mutual Information Maximization (CMIM) or Joint Mutual Information (JMI), which can be incorporated in addition to the Spark Big Library, which can be designed in this framework."}, {"heading": "3.1 Filter methods based on Information Theory", "text": "In anticipatory learning, we associate the message with the output mark in the classification. A frequently used uncertainty function is the mutual information (MI) [10], which measures the amount of information containing one random variable over another. This is the reduction of the uncertainty of one random variable over another: I (A; B) \u2212 H (A | B) = A [A).B [B).B).B (a).B).B (a).D (a).D (a).B).C (a).B).B (a).B).C (a).B).C (a).B).B (.B).B).B (a).B (a).B (a).B (.B).D) (.D) (.D).D (.D).D A (A).D) (.D).D (.D) (.D) (.D).D (.D) (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D) (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D).D (.D (.D).D (.D).D (.D (.D).D (.D).D (.D).D (.D).D (.D) (.D (.D).D (.D) (.D (.D) (.D) (.D) (.D) (.D (.D).D) (.D) (.D) (.D) (.D) (.D) (."}, {"heading": "3.2 Filter FS Framework for Big Data", "text": "In fact, most of them are able to play by the rules that they need for their work, and they are able to play by the rules that they need for their work in order to play by the rules."}, {"heading": "3.2.1 Main FS Algorithm", "text": "In algorithm 1, the main algorithm for selecting the characteristics is presented. This method is responsible for deciding which characteristic should be selected sequentially. Roughly, it calculates the initial relevance for all characteristics and iterates the selection of the best characteristics according to Equation 5 and the underlying MI- and CMI values. Input: npart Number of partitions to be set. Input: cindex Index of the output characteristics. Output: S\u03b8 List of selected characteristics Dc \u2190 Column transmission (D, ns, npart) ni nrows Number of characteristics to be selected. Input: npart Number of partitions to be set Number of characteristics to be set. Input: cindex Index of output characteristics Dc \u2190 Column transmission (D, npart) ni nrows Number of characteristics to be selected; nf \u2190 D.ncols REL \u2190 computeRelevances (Dc, index, ni) CRIT-Index (REL) inbest."}, {"heading": "3.2.2 Distributed Operations: Columnar Transformation and MI Computations", "text": "This year, the number of persons recorded by it has increased in the first half of the year in the second half of the year in the second half of the year in the first half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the first half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the year in the second half of the second half of the year in the first half of the second half of the year in the second half of the year in the second half of the year."}, {"heading": "3.2.3 High-dimensional and sparse version", "text": "In this case, the algorithms affected by this change are detailed: the column transformation (algorithm 2) and the process of creating histograms (algorithm 5).The rest of the code remains unchanged except the structure of the data, which is reduced to a single vector (in the form of index, vector).The block index is therefore removed from this structure as just a histogram."}, {"heading": "3.2.4 Complexity of the algorithms", "text": "As we mentioned earlier, the FS algorithm performs a greedy search, which stops when the condition defined as input is reached. Furthermore, this sequential algorithm is affected by the distributed algorithms / operations presented in the previous section. Distributed primitives used in these algorithms must be analyzed to verify the complexity of the entire proposal. Note that the first operation (column-by-column transformation) is quite time consuming, since it makes heavy use of network and memory when buffering all data (wide dependency). However, once data has a known partitioning, it can be reused in the following phases (utilization of the data locality property). In any case, this transformation is performed once at startup and can be omitted if the data is already in a column-like format."}, {"heading": "4 Experimental Framework and Analysis", "text": "This section describes the experiments that have been carried out to evaluate the usefulness of FS against a number of huge problems in the real world - both in its characteristics and in its examples - against the proposed framework."}, {"heading": "4.1 Datasets and methods", "text": "In fact, the number of people who are able to get to the top of the leaderboard is very high. \"This is the biggest challenge we have to face,\" he said. \"We have to be able to be able to be able to be,\" he said. \"We have to put ourselves at the top of the leaderboard.\""}, {"heading": "4.2 Cluster configuration", "text": "For all experiments, we used a cluster consisting of eighteen computing nodes and one master node. These computing nodes have the following features: 2 processors x Intel Xeon CPU E5-2620, 6 cores per processor, 2.00 GHz, 15 MB cache, QDR InfiniBand Network (40 Gbps), 2 TB HDD, 64 GB RAM. Regarding the software, we used the following configuration: Hadoop 2.5.0-cdh5.3.1 from Cloudera's open source Apache Hadoop Distribution9, HDFS replication factor: 2, HDFS default block size: 128 MB, Apache Spark and MLlib 1.2.0, 432 cores (24 cores / nodes), 864 RAM GB (48 GB / nodes).Both HDFS and Spark master processes (the HDFS NameNode and the Spark Master) are hosted on the main node."}, {"heading": "4.3 Analysis of selection results", "text": "Here we evaluate the time taken by our implementation to rank the most relevant features. Table 4 represents the temporal results achieved by our algorithm using different ranking thresholds (number of features selected); as shown in Table 4, our algorithm delivers competitive results in all cases, regardless of the number of iterations used (represented by the threshold); for those sets of data with the highest data volume: kddb (ultra-high-dimensional data) and ECBDL14 (huge number of samples), it is important to note that our method is capable of classifying 100 features in less than an hour; in addition, a comparative study comparing our distributed version and the sequential version developed by Brown's Laborato.10 The samples from dna were generated with different ratios of instances to examine the scalability of our approach to the sequential version."}, {"heading": "4.4 Analysis of classification results", "text": "Figures 6 and 7 show the accuracy results for SVM and Naive Bayes using different FS schemes. All of the data sets described in Table 2 were used in this study except kddb, as the above classifiers are not designed for such large dimensionality. Figure 7 shows an important improvement over the use of FS over Url and Epsilon, while their application to dna and ECBDL14 appears to have a negligible impact on AUC, explained by the fact that their high imbalance ratio and / or small number of features. Figure 6 shows similar results to the previous case, but in this case the improvement of the Url data sets is much smaller. Beyond AUC, the time spent on creating a classification model is quite important in many large-scale cases."}, {"heading": "5 Conclusions", "text": "In this paper, we have discussed the problem of processing huge data, especially from the perspective of dimensionality. We have seen the impact of correctly identifying relevant characteristics on these datasets, as well as the difficulty of this task due to the combinatorial effects as incoming data grows - both in instances and in properties. Despite the growing interest in dimensionality reduction for big data, few FS methods have been developed to deal with high-dimensional problems. As a result, we have redesigned a generic FS framework for big data based on information theory and adapted an earlier one by Brown et al. in [8]. The framework contains implementations of many state-of-the-art FS algorithms such as mRMR or JMI. However, the adaptation carried out has involved a profound redesign of Brown et al. \"s framework to adapt to the distributed paradigm."}, {"heading": "Acknowledgment", "text": "This work is supported by the Spanish national research project TIN201237954, TIN2013-47210-P and TIN2014-57251-P as well as the Andalusian research plan P10-TIC-6858, P11-TIC-7765 and P12-TIC-2958 and by the Xunta de Galicia within the framework of the research project GRC 2014 / 035 (all projects are partly financed by FEDER funds of the European Union). S. Rami'rez-Gallego is a recipient of an FPU scholarship from the Spanish Ministry of Education and Science (FPU13 / 00047). D. Mart'\u0131nez-Rego and V. Bolo'n-Canedo acknowledge the support of Xunta de Galicia within the framework of the postdoctoral codes POS-A / 2013 / 196 and ED481B 2014 / 164- 0."}], "references": [{"title": "Using mutual information for selecting features in supervised neural net learning", "author": ["Roberto Battiti"], "venue": "Neural Networks, IEEE Transactions on,", "citeRegEx": "3", "shortCiteRegEx": "3", "year": 1994}, {"title": "Selection of relevant features and examples in machine learning", "author": ["Avrim L. Blum", "Pat Langley"], "venue": "Artif. Intell.,", "citeRegEx": "4", "shortCiteRegEx": "4", "year": 1997}, {"title": "Recent advances and emerging challenges of feature selection in the context of big data", "author": ["V. Bol\u00f3n-Canedo", "N. S\u00e1nchez-Marono", "A. Alonso-Betanzos"], "venue": "Knowledge-Based Systems,", "citeRegEx": "5", "shortCiteRegEx": "5", "year": 2015}, {"title": "Feature Selection for High-Dimensional Data. Artificial Intelligence: Foundations, Theory, and Algorithms", "author": ["Ver\u00f3nica Bol\u00f3n-Canedo", "Noelia S\u00e1nchez-Maro\u00f1o", "Amparo Alonso- Betanzos"], "venue": null, "citeRegEx": "6", "shortCiteRegEx": "6", "year": 2015}, {"title": "A review of microarray datasets and applied feature selection methods", "author": ["V. Bol\u00f3n-Canedo", "N. S\u00e1nchez-Maro\u00f1o", "A. Alonso-Betanzos", "J.M. Be\u0144\u0131tez", "F. Herrera"], "venue": "Information Sciences,", "citeRegEx": "7", "shortCiteRegEx": "7", "year": 2014}, {"title": "Conditional likelihood maximisation: A unifying framework for information theoretic feature selection", "author": ["Gavin Brown", "Adam Pocock", "Ming-Jie Zhao", "Mikel Luj\u00e1n"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "8", "shortCiteRegEx": "8", "year": 2012}, {"title": "LIBSVM: A library for support vector machines", "author": ["Chih-Chung Chang", "Chih-Jen Lin"], "venue": "ACM Transactions on Intelligent Systems and Technology,", "citeRegEx": "9", "shortCiteRegEx": "9", "year": 2011}, {"title": "Mapreduce: Simplified data processing on large clusters", "author": ["Jeffrey Dean", "Sanjay Ghemawat"], "venue": "OSDI", "citeRegEx": "11", "shortCiteRegEx": "11", "year": 2004}, {"title": "On the use of mapreduce for imbalanced big data using random forest", "author": ["Sara del \u0154\u0131o", "Victoria L\u00f3pez", "Jos\u00e9 Manuel Be\u0144\u0131tez", "Francisco Herrera"], "venue": "Information Sciences,", "citeRegEx": "12", "shortCiteRegEx": "12", "year": 2014}, {"title": "Pattern classification and scene analysis, volume", "author": ["Richard O. Duda", "Peter E. Hart"], "venue": null, "citeRegEx": "13", "shortCiteRegEx": "13", "year": 1973}, {"title": "Big data with cloud computing: an insight on the computing environment, mapreduce, and programming frameworks", "author": ["Alberto Fern\u00e1ndez", "Sara del \u0154\u0131o", "Victoria L\u00f3pez", "Abdullah Bawakid", "Ma\u0155\u0131a Jos\u00e9 del Jes\u00fas", "Jos\u00e9 Manuel Be\u0144\u0131tez", "Francisco Herrera"], "venue": "Wiley Interdisc. Rew.: Data Mining and Knowledge Discovery,", "citeRegEx": "14", "shortCiteRegEx": "14", "year": 2014}, {"title": "Fast binary feature selection with conditional mutual information", "author": ["Fran\u00e7ois Fleuret"], "venue": "The Journal of Machine Learning Research,", "citeRegEx": "15", "shortCiteRegEx": "15", "year": 2004}, {"title": "An introduction to variable and feature selection", "author": ["Isabelle Guyon", "Andr\u00e9 Elisseeff"], "venue": "J. Mach. Learn. Res.,", "citeRegEx": "16", "shortCiteRegEx": "16", "year": 2003}, {"title": "Feature Extraction: Foundations and Applications (Studies in Fuzziness and Soft Computing)", "author": ["Isabelle Guyon", "Steve Gunn", "Masoud Nikravesh", "Lotfi A. Zadeh"], "venue": null, "citeRegEx": "17", "shortCiteRegEx": "17", "year": 2006}, {"title": "Scholkopf. Support vector machines", "author": ["Marti A. Hearst", "Susan T. Dumais", "Edgar Osman", "John Platt", "Bernhard"], "venue": "Intelligent Systems and their Applications, IEEE,", "citeRegEx": "18", "shortCiteRegEx": "18", "year": 1998}, {"title": "Attribute Interactions in Machine Learning", "author": ["Aleks Jakulin"], "venue": "PhD thesis, University of Ljubljana,", "citeRegEx": "19", "shortCiteRegEx": "19", "year": 2003}, {"title": "Learning Spark: Lightning-Fast Big Data Analytics", "author": ["Holden Karau", "Andy Konwinski", "Patrick Wendell", "Matei Zaharia"], "venue": "O\u2019Reilly Media, Incorporated,", "citeRegEx": "20", "shortCiteRegEx": "20", "year": 2015}, {"title": "Wrappers for feature subset selection", "author": ["Ron Kohavi", "George H. John"], "venue": "Artif. Intell.,", "citeRegEx": "21", "shortCiteRegEx": "21", "year": 1997}, {"title": "3d data management: Controlling data volume, velocity and variety", "author": ["Doug Laney"], "venue": null, "citeRegEx": "22", "shortCiteRegEx": "22", "year": 2001}, {"title": "Feature selection and feature extraction for text categorization", "author": ["David D. Lewis"], "venue": "In Proceedings of the workshop on Speech and Natural Language,", "citeRegEx": "23", "shortCiteRegEx": "23", "year": 1992}, {"title": "Mapreduce is good enough? if all you have is a hammer, throw away everything that\u2019s not a nail", "author": ["Jimmy Lin"], "venue": "CoRR, abs/1209.2191,", "citeRegEx": "25", "shortCiteRegEx": "25", "year": 2012}, {"title": "Efficient multitemplate learning for structured prediction", "author": ["Qi Mao", "Ivor Wai-Hung Tsang"], "venue": "IEEE Trans. Neural Netw. Learning Syst.,", "citeRegEx": "26", "shortCiteRegEx": "26", "year": 2013}, {"title": "Experimental study of information measure and inter-intra class distance ratios on feature selection and orderings", "author": ["Mark Michael", "Wen-Chun Lin"], "venue": "Systems, Man and Cybernetics, IEEE Transactions on,", "citeRegEx": "27", "shortCiteRegEx": "27", "year": 1973}, {"title": "Feature selection based on mutual information criteria of max-dependency, max-relevance, and minredundancy", "author": ["Hanchuan Peng", "Fulmi Long", "Chris Ding"], "venue": "Pattern Analysis and Machine Intelligence, IEEE Transactions on,", "citeRegEx": "28", "shortCiteRegEx": "28", "year": 2005}, {"title": "TR10: Peering into Video\u2019s Future", "author": ["Wade Roush"], "venue": "MIT Technology Review March", "citeRegEx": "29", "shortCiteRegEx": "29", "year": 2013}, {"title": "A review of feature selection techniques in bioinformatics", "author": ["Yvan Saeys", "I\u00f1aki Inza", "Pedro Larra\u00f1aga"], "venue": null, "citeRegEx": "30", "shortCiteRegEx": "30", "year": 2007}, {"title": "An improvement to feature selection of random forests on spark", "author": ["Ke Sun", "Wansheng Miao", "Xin Zhang", "Ruonan Rao"], "venue": "In IEEE 17th International Conference on Computational Science and Engineering (CSE),", "citeRegEx": "33", "shortCiteRegEx": "33", "year": 2014}, {"title": "Object recognition with informative features and linear classification", "author": ["Michel Vidal-Naquet", "Shimon Ullman"], "venue": "In ICCV,", "citeRegEx": "34", "shortCiteRegEx": "34", "year": 2003}, {"title": "Realistic human action recognition with multimodal feature selection and fusion. Systems, Man, and Cybernetics: Systems", "author": ["Qiuxia Wu", "Zhiyong Wang", "Feiqi Deng", "Zheru Chi", "D.D. Feng"], "venue": "IEEE Transactions on,", "citeRegEx": "36", "shortCiteRegEx": "36", "year": 2013}, {"title": "Data visualization and feature selection: New algorithms for nongaussian data", "author": ["Howard Hua Yang", "John E Moody"], "venue": "In Advances in Neural Information Processing Systems,", "citeRegEx": "37", "shortCiteRegEx": "37", "year": 1999}, {"title": "The emerging \u201cbig dimensionality", "author": ["Yiteng Zhai", "Yew-Soon Ong", "Ivor W. Tsang"], "venue": "IEEE Comp. Int. Mag.,", "citeRegEx": "38", "shortCiteRegEx": "38", "year": 2014}, {"title": "Spectral feature selection for data mining", "author": ["Zheng Alan Zhao", "Huan Liu"], "venue": "Chapman & Hall/CRC,", "citeRegEx": "39", "shortCiteRegEx": "39", "year": 2011}], "referenceMentions": [{"referenceID": 6, "context": "In fact, if we analyze the datasets posted in the popular libSVM Database [9], we can observe that in the 1990s, the maximum dimensionality of the data was about 62 000; in the 2000s, this number increased to more than 16 million; and in the 2010s it further increased to more than 29 million.", "startOffset": 74, "endOffset": 77}, {"referenceID": 13, "context": "Dimensionality reduction techniques can be applied to reduce the dimensionality of the original data and even to improve learning performance [17, 39, 6].", "startOffset": 142, "endOffset": 153}, {"referenceID": 31, "context": "Dimensionality reduction techniques can be applied to reduce the dimensionality of the original data and even to improve learning performance [17, 39, 6].", "startOffset": 142, "endOffset": 153}, {"referenceID": 3, "context": "Dimensionality reduction techniques can be applied to reduce the dimensionality of the original data and even to improve learning performance [17, 39, 6].", "startOffset": 142, "endOffset": 153}, {"referenceID": 28, "context": "Due to the fact that FS maintains the original features, it is especially useful for applications where the original features are important for model interpretation and knowledge extraction [36, 7], and so this model will be the focus of this paper.", "startOffset": 190, "endOffset": 197}, {"referenceID": 4, "context": "Due to the fact that FS maintains the original features, it is especially useful for applications where the original features are important for model interpretation and knowledge extraction [36, 7], and so this model will be the focus of this paper.", "startOffset": 190, "endOffset": 197}, {"referenceID": 2, "context": "On the other hand, existing FS methods are not expected to scale well when dealing with Big Data due to the fact that their efficiency may significantly deteriorate or even become inapplicable [5].", "startOffset": 193, "endOffset": 196}, {"referenceID": 7, "context": "The first programming model was MapReduce [11] along with its open-source implementation Apache Hadoop [35, 1].", "startOffset": 42, "endOffset": 46}, {"referenceID": 16, "context": "Recently, Apache Spark [20, 31], a new distributed framework, was presented as a fast and general engine for large-scale data processing, popular among machine learning researchers due to its suitability for iterative procedures.", "startOffset": 23, "endOffset": 31}, {"referenceID": 26, "context": "Only a simple approach based on Chi-Squared, and an improvement to FS on Random Forest [33] have been proposed in the literature to deal with this problem.", "startOffset": 87, "endOffset": 91}, {"referenceID": 5, "context": "Here, we propose a new distributed design for a FS generic framework based on Information Theory [8], which has been implemented using", "startOffset": 97, "endOffset": 100}, {"referenceID": 13, "context": "Its goal is to obtain a subset of features that describes properly the given problem with a minimum degradation of performance, in order to obtain simpler and more accurate schemes [17].", "startOffset": 181, "endOffset": 185}, {"referenceID": 1, "context": "FS methods can be broadly categorized as [4]:", "startOffset": 41, "endOffset": 44}, {"referenceID": 17, "context": "Wrapper methods, which use an evaluation function dependent on a learning algorithm [21].", "startOffset": 84, "endOffset": 88}, {"referenceID": 12, "context": "They only consider the general characteristics of the dataset, being independent of any predictor [16].", "startOffset": 98, "endOffset": 102}, {"referenceID": 25, "context": "Embedded methods, which use a search procedure which is implicit in the classifier/regressor [30].", "startOffset": 93, "endOffset": 97}, {"referenceID": 18, "context": "Gartner [22] introduced the 3Vs concept by defining Big Data as high volume, velocity and variety information that require a new large-scale processing.", "startOffset": 8, "endOffset": 12}, {"referenceID": 30, "context": "An under-explored but not less important topic is the \u201cBig Dimensionality\u201d in Big Data [38].", "startOffset": 87, "endOffset": 91}, {"referenceID": 6, "context": "It has been captured in many of the most famous dataset repositories in computational intelligence (like UCI or libSVM) [24, 9], where the majority of the new added datasets present a huge dimensionality [38] (e.", "startOffset": 120, "endOffset": 127}, {"referenceID": 30, "context": "It has been captured in many of the most famous dataset repositories in computational intelligence (like UCI or libSVM) [24, 9], where the majority of the new added datasets present a huge dimensionality [38] (e.", "startOffset": 204, "endOffset": 208}, {"referenceID": 24, "context": "For instance, on the Internet, all multimedia content represents about 60% of total traffic [29], transmitted in thousands of different formats (audio, video, images, etc.", "startOffset": 92, "endOffset": 96}, {"referenceID": 21, "context": ", are simultaneously employed so as to produce comprehensible and reliable models [26].", "startOffset": 82, "endOffset": 86}, {"referenceID": 7, "context": "The MapReduce framework [11] was born in 2003 as a revolutionary tool in Big Data, designed by Google for processing and generating large-scale datasets.", "startOffset": 24, "endOffset": 28}, {"referenceID": 10, "context": "2For a exhaustive review of MapReduce and others programming frameworks, please check [14].", "startOffset": 86, "endOffset": 90}, {"referenceID": 20, "context": "Despite being the most popular opensource implementation of MapReduce, Hadoop is not suitable in many cases, such as online and/or iterative computing, high inter-process communication paradigms or in-memory computing, among others [25].", "startOffset": 232, "endOffset": 236}, {"referenceID": 16, "context": "In recent years, Apache Spark has been introduced in the Hadoop Ecosystem [20, 31].", "startOffset": 74, "endOffset": 82}, {"referenceID": 5, "context": "In [8], an Information Theoretic framework that includes many common FS filter algorithms was proposed.", "startOffset": 3, "endOffset": 6}, {"referenceID": 22, "context": "Information measures tell us how much information has been acquired by the receiver when he/she gets a message [27].", "startOffset": 111, "endOffset": 115}, {"referenceID": 0, "context": "For instance, redundant features can be discarded (those variables that carry similar information) using the Mutual Information criterion [3]:", "startOffset": 138, "endOffset": 141}, {"referenceID": 5, "context": "[8] proposed a generic expression that allows to ensemble multiple information theoretic criteria into a unique FS framework.", "startOffset": 0, "endOffset": 3}, {"referenceID": 5, "context": "For a detailed description of the transformation processes, please see [8].", "startOffset": 71, "endOffset": 74}, {"referenceID": 5, "context": "[8] is:", "startOffset": 0, "endOffset": 3}, {"referenceID": 19, "context": "Mutual Information Maximisation (MIM) [23]", "startOffset": 38, "endOffset": 42}, {"referenceID": 0, "context": "Mutual Information FS (MIFS) [3]", "startOffset": 29, "endOffset": 32}, {"referenceID": 29, "context": "Joint Mutual Information (JMI) [37]", "startOffset": 31, "endOffset": 35}, {"referenceID": 23, "context": "Minimum-Redundancy Maximum-Relevance (mRMR) [28]", "startOffset": 44, "endOffset": 48}, {"referenceID": 11, "context": "Conditional Mutual Information Maximization (CMIM) [15]", "startOffset": 51, "endOffset": 55}, {"referenceID": 27, "context": "Informative Fragments (IF) [34] (equivalent to CMIM)", "startOffset": 27, "endOffset": 31}, {"referenceID": 15, "context": "Interaction Capping (ICAP) [19]", "startOffset": 27, "endOffset": 31}, {"referenceID": 8, "context": "For this imbalanced problem, the MapReduce version of the Random OverSampling (ROS) algorithm presented in [12] was applied (henceforth we will use ECBDL14 to refer to the ROS version).", "startOffset": 107, "endOffset": 111}, {"referenceID": 6, "context": "The rest of datasets (epsilon, url and kddb) come from the LibSVM dataset repository [9].", "startOffset": 85, "endOffset": 88}, {"referenceID": 23, "context": "As an FS benchmark method, we have used mRMR algorithm [28] since it is one of the most relevant and cited selectors in the literature.", "startOffset": 55, "endOffset": 59}, {"referenceID": 14, "context": "In order to carry out a comparison study, the following classifiers were chosen: Support Vector Machines (SVM) [18], and Naive Bayes [13].", "startOffset": 111, "endOffset": 115}, {"referenceID": 9, "context": "In order to carry out a comparison study, the following classifiers were chosen: Support Vector Machines (SVM) [18], and Naive Bayes [13].", "startOffset": 133, "endOffset": 137}, {"referenceID": 5, "context": "in [8].", "startOffset": 3, "endOffset": 6}], "year": 2016, "abstractText": "With the advent of extremely high dimensional datasets, dimensionality reduction techniques are becoming mandatory. Among many techniques, feature selection has been growing in interest as an important tool to identify relevant features on huge datasets \u2013both in number of instances and features\u2013. The purpose of this work is to demonstrate that standard feature selection methods can be parallelized in Big Data platforms like Apache Spark, boosting both performance and accuracy. We thus propose a distributed implementation of a generic feature selection framework which includes a wide group of well-known Information Theoretic methods. Experimental results on a wide set of real-world datasets show that our distributed framework is capable of dealing with ultra-high dimensional datasets as well as those with a huge number of samples in a short period of time, outperforming the sequential version in all the cases studied.", "creator": "LaTeX with hyperref package"}}}